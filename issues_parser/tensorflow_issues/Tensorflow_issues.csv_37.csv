Issue Number,Issue Title,Issue Body
31907,please add tensor factorization,"**System information**
- TensorFlow version (you are using): 2b1
- Are you willing to contribute it (Yes/No): I want to but I don't think I'm qualified

**Describe the feature and the current behavior/state.**
tensorflow is about tensors but there's no built-in tensor factorization. 
i do believe this should be a core feature of the framework...what do you think?

**Will this change the current api? How?**
tf.linalg (i guess) would have something like Tucker Decomposition, higher order SVD
would this be a good 2.0 feature?

**Who will benefit with this feature?**
people who need to compress / factorize big tensors

**Any Other info.**
http://tensorly.org/stable/user_guide/quickstart.html#tensor-decomposition
https://github.com/hottbox/hottbox"
31905,mnist_cnn.py major performance loss after switching to TF 1.14,"**System information**
- Windows 10 Enterprise 64-bit
- TensorFlow GPU installed via Anaconda
- TensorFlow GPU version: 1.14
- Python version: 3.7.3
- CUDA version: 10.0.130
- cuDNN version: 7.6.0
- GPU model and memory: NVIDIA Quadro P600 (2 GB)

**Describe the current behavior**
The mnist.py script achieves a test accuracy of <85% after 12 epochs of training when using tensorflow-gpu=1.14.

**Describe the expected behavior**
The mnist.py script is supposed to to achieve >99% test accuracy after 12 epochs of training. Doing a clean install of tensorflow-gpu=1.13 and running the script achieves this result.

- TensorFlow GPU version: 1.13
- Python version: 3.7.3
- CUDA version: 10.0.130
- cuDNN version: 7.6.0

This discrepancy between versions exists even when setting the numpy and TensorFlow random seeds before training. There are potentially major implications for the reproducibility of any work done in TensorFlow across these versions. Why is this happening and/or how can it be fixed?

**Code to reproduce the issue**
Use [mnist.py](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) and replace keras with tf.keras

**Other info / logs**
None.
"
31903,Fix Tensorflow Lite Documentation,"Hi,

On https://www.tensorflow.org/lite/guide/python the sample Interpreter code is wrong 

![image](https://user-images.githubusercontent.com/2943831/63538358-9b562f80-c4cc-11e9-8cac-d81786a1acad.png)

""from tflite_runtime import Interpreter""  should be changed to

""from tflite_runtime.interpreter import Interpreter""


Thanks

Hakan

"
31902,[tflite] Support INT8 quantisation for UNPACK with TFLITE_BUILTINS_INT8 OpsSet,"**System information**
- TensorFlow version (you are using): 1.14 and built from sources, master branch
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
The new TFLiteConverter post-training quantisation flow, as described in https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations, does not support quantisation of UNPACK/UNSTACK operation when only integer operations are requested in the output model. When such conversion is attempted the following error is reported: 

> RuntimeError: Quantization not yet supported for op: UNPACK

For example, the script below

```
import tensorflow as tf
import numpy as np

def representative_dataset_gen():
	input = np.ones([2, 10],dtype=np.float32)
	for _ in range(10):
		yield [input]

# tf Graph Input
in_stacked = tf.compat.v1.placeholder(""float32"", [2, 10])
out_unstacked = tf.unstack(in_stacked, axis=0)

with tf.compat.v1.Session() as sess:
	tf.io.write_graph(tf.compat.v1.get_default_graph(), '.','unpack.pb', as_text=False)

input_name = [""Placeholder""]
output_name = [""unstack"", ""unstack:1""]

tflite_model_name = ""int8_unpack.tflite""
converter = tf.lite.TFLiteConverter.from_frozen_graph(""unpack.pb"", input_name, output_name)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.representative_dataset = representative_dataset_gen
tflite_model = converter.convert()
open(tflite_model_name, ""wb"").write(tflite_model)

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(tflite_model_name)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
```

produces
```

2019-08-22 17:44:45.800226: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3492095000 Hz
2019-08-22 17:44:45.800687: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c7b280 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-08-22 17:44:45.800705: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-08-22 17:44:45.805127: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-08-22 17:44:45.805220: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
Traceback (most recent call last):
  File ""TF_Tests/test.py"", line 24, in <module>
    tflite_model = converter.convert()
  File ""/home/antkac01/lab/mlt/venv-runtime/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py"", line 993, in convert
    inference_output_type)
  File ""/home/antkac01/lab/mlt/venv-runtime/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py"", line 239, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File ""/home/antkac01/lab/mlt/venv-runtime/lib/python3.5/site-packages/tensorflow_core/lite/python/optimize/calibrator.py"", line 78, in calibrate_and_quantize
    np.dtype(output_type.as_numpy_dtype()).num, allow_float)
  File ""/home/antkac01/lab/mlt/venv-runtime/lib/python3.5/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 115, in QuantizeModel
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
RuntimeError: Quantization not yet supported for op: UNPACK

```

We propose adding the required support into the TFLiteConverter as it seems to be a natural extension of the existing functionality and will make the int8 quantisation process consistent across all supported  conversion options.

It appears that the UNPACK operator for kTfLiteInt8 type is already implemented (see [unpack.cc](https://github.com/tensorflow/tensorflow/blob/186e794d71c17b52deb52ace151ec5add8525f2c/tensorflow/lite/kernels/unpack.cc#L93)). The converstion fails on the [check of ""quantizable"" property](https://github.com/tensorflow/tensorflow/blob/186e794d71c17b52deb52ace151ec5add8525f2c/tensorflow/lite/tools/optimize/quantize_model.cc#L561) because [GetOperatorProperty()](https://github.com/tensorflow/tensorflow/blob/186e794d71c17b52deb52ace151ec5add8525f2c/tensorflow/lite/tools/optimize/operator_property.cc#L20) does not contain a case statement for BuiltinOperator_UNPACK.

It looks like modifying GetOperatorProperty() could unlock this feature. The corresponding unit test(s) will need to be added as well.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Anyone who attempts full 8-bit fixed point quantisation of models containinig UNPACK/UNSTACK operation, e.g. some versions of DeepSpeech.

**Any Other info.**
"
31901,Handle AddV2 and FusedBatchNormV3 in tflite_convert,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.0.0b1


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONCATENATION, CONV_2D, EXPAND_DIMS, LOGISTIC, MAX_POOL_2D, MEAN, MIRROR_PAD, MUL, PACK, RELU, RELU6, RESIZE_NEAREST_NEIGHBOR, RSQRT, SHAPE, SQUARED_DIFFERENCE, SQUEEZE, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: AddV2, FusedBatchNormV3.
```

Also, please include a link to a GraphDef or the model if possible.

https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31900,'is_final' is not a member of 'std',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RedHat 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: master branch
- Python version: N/A
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): cc version 8.2.1 20180905 (Red Hat 8.2.1-3)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

Compilation of LLVM is failing because '-std=c++0x' is being passed, but current LLVM uses C++14 features.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --verbose_failures --config opt --config mkl //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Note ""-std=c++0x"" option. I could probably fix this myself if I could figure out where it is coming from...
```
  /opt/rh/devtoolset-8/root/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/tensorflow/compiler/mlir/lite/quantization/_objs/op_quant_spec_getters_gen/op_quant_spec_getters_gen.d '-frandom-seed=bazel-out/host/bin/tensorflow/compiler/mlir/lite/quantization/_objs/op_quant_spec_getters_gen/op_quant_spec_getters_gen.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote . -iquote bazel-out/host/bin -iquote external/llvm -iquote bazel-out/host/bin/external/llvm -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_mlir -iquote bazel-out/host/bin/external/local_config_mlir -iquote external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -isystem external/llvm/include -isystem bazel-out/host/bin/external/llvm/include -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_mlir/include -isystem bazel-out/host/bin/external/local_config_mlir/include -g0 '-march=native' -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DINTEL_MKL=1' -DEIGEN_USE_VML -DENABLE_MKL -fopenmp -msse3 -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/compiler/mlir/lite/quantization/tools/op_quant_spec_getters_gen.cc -o bazel-out/host/bin/tensorflow/compiler/mlir/lite/quantization/_objs/op_quant_spec_getters_gen/op_quant_spec_getters_gen.o)
Execution platform: @bazel_tools//platforms:host_platform
In file included from external/llvm/include/llvm/TableGen/Record.h:27,
                 from tensorflow/compiler/mlir/lite/quantization/tools/op_quant_spec_getters_gen.cc:21:
external/llvm/include/llvm/Support/TrailingObjects.h: In static member function 'static void llvm::TrailingObjects<BaseTy, TrailingTys>::verifyTrailingObjectsAssertions()':
external/llvm/include/llvm/Support/TrailingObjects.h:252:24: error: 'is_final' is not a member of 'std'
     static_assert(std::is_final<BaseTy>(), ""BaseTy must be final."");
                        ^~~~~~~~
```"
31896,tf-2.0.0rc0-gpu tf.dataset bug,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: yes 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): docker image
- TensorFlow version (use command below): tensorflow/tensorflow:2.0.0rc0-gpu-py3
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10
- GPU model and memory:

**Describe the current behavior**
I am using tf.keras for training with the dataset api and TFrecords. At the beginning of the training the dataset api tries to load and shuffle the whole training dataset so the memory fills up and the process dies.  I cannot train my model at all. I am not using any .shuffle() calls in my dataset pipeline.

**Describe the expected behavior**
The dataset should not load and shuffle the records unless the .shuffle() call is issued.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
import os
import cv2
import tensorflow.keras.layers as K

def extract_fn(data_record):
    features = {
        'data': tf.io.FixedLenFeature([], tf.string)
    }
    sample = tf.io.parse_single_example(data_record, features)
    data = tf.image.decode_image(sample['data'])

    return data, 1.


class DataGenerator(tf.keras.utils.Sequence):

    def __init__(self, dataset_iterator, len):
        self.dataset_iterator = dataset_iterator
        self.len = len

    def __len__(self):
        # number of batches per epoch
        return self.len

    def __getitem__(self, index):
        # Generate one batch of data

        next_element = next(self.dataset_iterator)
        x = next_element[0]
        y = next_element[1]

        return x, y


with tf.io.TFRecordWriter(""dummy_dataset.tfrecords"") as writer:
    data = np.float32(np.random.random(size=(1000, 1000, 3)) * 255)
    data = cv2.imencode("".png"", data)[1].tostring()
    example = tf.train.Example(features=tf.train.Features(
        feature={'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[data]))}))
    for i in range(10000):
        writer.write(example.SerializeToString())

dataset = tf.data.TFRecordDataset([""dummy_dataset.tfrecords""])
dataset = dataset.map(extract_fn)
n_batch = 3
dataset = dataset.batch(batch_size=n_batch, drop_remainder=True)

dataset = dataset.repeat(5)
dataset_iterator = iter(dataset)
next_element = next(dataset_iterator)

data_generator = DataGenerator(dataset_iterator, int(10000/n_batch))

input = K.Input(shape=(1000, 1000, 3), name='input')
net = K.Conv2D(1, 3, activation='sigmoid')(input)
output = K.GlobalAveragePooling2D()(net)
model = tf.keras.models.Model(inputs=input, outputs=output)

model.compile(loss='mse', optimizer='sgd')

model.fit(x=data_generator, epochs=10)
```
**Other info / logs**
note that code generates about 28GB dummy data in a tfrecord file since I am having this issue when I am trying to use the dataset api with tf.keras.utils.Sequence on a tfrecord file.

EDIT(robieta): code formatting."
31895,How to fix the ConcatOp dimension error when trying to compile a model using tensorflow XLA AOT?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version: 3.6
- Installed using virtualenv? pip? conda?: Built using source
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**
I have a LSTM model which I need to compile using XLA AOT. I am getting the ConcatOp dimensionality error when building using tensorflow. 
**ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1**
I am not able to understand the problem here with the dimensionality as the model works fine for inference.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
BUILD File

load('@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl', 'tf_library')

tf_library(
    name = 'graph',
    config = 'graph.config.pbtxt',
    cpp_class = 'Graph',
    graph = 'graph.pb',
)

command to compile model
:~bazel build --show_progress_rate_limit=600 @org_tensorflow//:graph --verbose_failures

**Any other info / logs**
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/e7f4ecf01a350c19085daa7be2d44aba/external/org_tensorflow/BUILD:3:1: Executing genrule @org_tensorflow//:gen_graph failed (Exit 1): bash failed: error executing command
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/e7f4ecf01a350c19085daa7be2d44aba/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/: \
    PATH=/home/ubuntu/anaconda3/bin/:/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/anaconda3/bin/:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ubuntu/src/cntk/bin:/usr/local/mpi/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; CUDA_VISIBLE_DEVICES='\'''\'' bazel-out/host/bin/external/org_tensorflow/tensorflow/compiler/aot/tfcompile --graph=external/org_tensorflow/graph.pb --config=external/org_tensorflow/graph.config.pbtxt --entry_point=__xla___graph --cpp_class=Graph --target_triple=x86_64-pc-linux --out_header=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph.h --out_metadata_object=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph_tfcompile_metadata.o --out_function_object=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph_tfcompile_function.o  ')
Execution platform: @bazel_tools//platforms:host_platform
2019-08-21 16:38:57.472380: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
INVALID ARGUMENTS: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1
         [[{{node rnn/basic_lstm_cell/concat}}]]"
31894,tf.keras.layers.BatchNormalization() throws TypeError: Incompatible types: <dtype: 'resource'> vs. int64. Value is 0 ,"```
import tensorflow as tf
batch_size = 20
inp = tf.placeholder(tf.float32, [batch_size, 19, 64, 64, 3])
out = tf.placeholder(tf.float32, [batch_size, 19, 60, 60, 16])
def model(inp):

  enc = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(128, activation='relu', kernel_size=3,kernel_initializer='glorot_uniform'))(inp)
  enc = tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization())(enc)
  enc = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(16, activation='relu',kernel_size=3,kernel_initializer='glorot_uniform'))(enc)
  return enc

pred = model(inp)

loss = tf.reduce_mean(tf.keras.backend.binary_crossentropy(out, pred))
lr = 0.0001
train_op = tf.train.AdamOptimizer(lr).minimize(loss)
```

Throws error::

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-43-c9408a385d78> in <module>()
      1 lr = 0.0001
----> 2 train_op = tf.train.AdamOptimizer(lr).minimize(reconstuction_loss)

7 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
    401         aggregation_method=aggregation_method,
    402         colocate_gradients_with_ops=colocate_gradients_with_ops,
--> 403         grad_loss=grad_loss)
    404 
    405     vars_with_grad = [v for g, v in grads_and_vars if g is not None]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)
    510         gate_gradients=(gate_gradients == Optimizer.GATE_OP),
    511         aggregation_method=aggregation_method,
--> 512         colocate_gradients_with_ops=colocate_gradients_with_ops)
    513     if gate_gradients == Optimizer.GATE_GRAPH:
    514       grads = control_flow_ops.tuple(grads)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)
    156         ys, xs, grad_ys, name, colocate_gradients_with_ops,
    157         gate_gradients, aggregation_method, stop_gradients,
--> 158         unconnected_gradients)
    159   # pylint: enable=protected-access
    160 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    718               # issue here because of zeros.
    719               if loop_state:
--> 720                 out_grads[i] = loop_state.ZerosLike(op, i)
    721               else:
    722                 out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in ZerosLike(self, op, index)
   1229       # If the shape is known statically, just create a zero tensor with
   1230       # the right shape in the grad loop context.
-> 1231       result = constant_op.constant(0, shape=shape.dims, dtype=val.dtype)
   1232       if dead_branch:
   1233         # op is a cond switch. Guard the zero tensor with a switch.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    244   """"""
    245   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 246                         allow_broadcast=True)
    247 
    248 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    282       tensor_util.make_tensor_proto(
    283           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--> 284           allow_broadcast=allow_broadcast))
    285   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    286   const_tensor = g.create_op(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    499                             dtype.base_dtype != numpy_dtype.base_dtype):
    500     raise TypeError(""Incompatible types: %s vs. %s. Value is %s"" %
--> 501                     (dtype, nparray.dtype, values))
    502 
    503   # If shape is not given, get the shape from the numpy array.

TypeError: Incompatible types: <dtype: 'resource'> vs. int64. Value is 0
```
UPDATE:: 
this works but if you set the trainable boolean to True, it throws the same error
```
  enc = tf.keras.layers.TimeDistributed(tf.layers.BatchNormalization(trainable = False))(enc)
```

"
31893,[TF2] tf.saved_model.save fails on models re-using other models,"**System information**
- Have I written custom code: Yes.
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0.0-dev20190821
- Python version: 3.7.3
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1080 Ti

This bug might be related to the behavior observed by @cysmnl in https://github.com/tensorflow/tensorflow/issues/28923#issuecomment-514736847 . However I think the bug present is distinct from the original bug described in https://github.com/tensorflow/tensorflow/issues/28923.

**Describe the current behavior**
The minimal working example below fails with the following exception:
```
Traceback (most recent call last):
  File ""mwe.py"", line 33, in <module>
    tf.saved_model.save(second_convolution_model, '/tmp/model2')  # does NOT work
  File "".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 860, in save
    meta_graph_def, saveable_view, signatures)
  File "".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 590, in _fill_meta_graph_def
    signatures = _generate_signatures(signature_functions, resource_map)
  File "".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 464, in _generate_signatures
    function, mapped_inputs, resource_map)
  File "".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 416, in _call_function_with_mapped_captures
    function.graph.captures, resource_map)
  File "".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 338, in _map_captures_to_created_tensors
    .format(interior))
AssertionError: Tried to export a function which references untracked object Tensor(""StatefulPartitionedCall/args_1:0"", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
```

As a side note, the error message misses a space after the period: https://github.com/tensorflow/tensorflow/blob/d1583caf72add28ad997616fe4aa08b5b5181b71/tensorflow/python/saved_model/save.py#L334 

**Describe the expected behavior**
Possibility to save models with `tf.saved_model.save` as they would be saveable with `model.save`.

**Code to reproduce the issue**
(This is example is nonsensical, but shows the problem. I've boiled down the problem to a MWE.)
```python
import tensorflow as tf
from tensorflow.python.keras import Model
from tensorflow.python.keras.layers import Input, Conv1D, Lambda

first_input = Input(shape=(1,))
first_result = Conv1D(filters=1, kernel_size=1)(first_input[..., tf.newaxis])
first_convolution_model = Model(inputs=[first_input], outputs=[first_result])


def inner_loop(tensor):
    the_len = tf.shape(tensor)[0]
    collector = tf.TensorArray(tf.float32, size=the_len)

    _, collector = tf.while_loop(
        cond=lambda i, _: i < the_len,
        body=lambda i, c_: (i+1, c_.write(i, first_convolution_model(tensor[i:i + 1]))),
        loop_vars=(0, collector)
    )

    return collector.stack()


second_input = Input(shape=(1,))
second_result = Lambda(inner_loop)(second_input)[..., 0]
second_convolution_model = Model(inputs=[second_input], outputs=[second_result])

print(first_convolution_model.predict([1, 2, 3]))
first_convolution_model.save('/tmp/model1.h5')  # works
tf.saved_model.save(first_convolution_model, '/tmp/model1')  # works

print(second_convolution_model.predict([1, 2, 3]))
first_convolution_model.save('/tmp/model2.h5')  # works
tf.saved_model.save(second_convolution_model, '/tmp/model2')  # does NOT work
print(""(not reached)"")
```

"
31892,java.lang.IllegalArgumentException: Expects arg[0] to be float but string is provided (java),"I have own trained model and use label image demo code.

```
try (Graph graph = new Graph();
        Session session = new Session(graph)) {
        graph.importGraphDef(fileContent);
        
        try (Tensor<String> input = Tensors.create(imageBytes);
            Tensor output =
                session
                    .runner()
                    .feed(""Placeholder"", input)
                    .fetch(""final_result"")
                    .run()
                    .get(0)) {
          if (probabilities == null) {
            probabilities = new float[(int) output.shape()[0]];
          }
          output.copyTo(probabilities);
          int label = argmax(probabilities);
          System.out.printf(
              ""%-30s --> %-15s (%.2f%% likely)\n"",
              filename, labels.get(label), probabilities[label] * 100.0);
        }
```
I see error `Exception in thread ""main"" java.lang.IllegalArgumentException: Expects arg[0] to be float but string is provided`. This model works perfect in python, but i have issue in java "
31891,TF2.0   'NoneType' object has no attribute 'shape',"windows 10
anaconda python 3.7 



**Describe the current behavior**
![image](https://user-images.githubusercontent.com/27112868/63515144-ba7cad80-c51c-11e9-93cb-167363519e4f.png)

Problem 
Line 32 input_x[:,1] occurs the error.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import time
import math
import tensorflow as tf
import numpy as np
import tensorflow as keras
from tensorflow.keras import layers
from tensorflow import keras
import tensorflow.keras.backend as K


from tensorflow.keras.utils import plot_model
from scipy . stats import multivariate_normal as normal

tf.keras.backend.set_floatx('float64')


def test_tf (x):
    z = x[:,0]** 3 + x[:,1]**3
    return z


def grad (y, x):
    z = layers.Lambda ( lambda z : K.gradients(z[0], z[1]) ) ([y, x])
    return z

input_x  = keras.Input( shape = (2))
y =  test_tf (input_x)

print(""y = "", y )
print (""input_x[0] = "", input_x[:1])

z0 = grad(y, input_x[:,1])

print(""z0 = "", z0)

model =  keras.Model(inputs = [input_x], outputs = [z0] )

print (""gradient  = "", model.predict(  tf.constant( [[2,3]]  )   ))

```



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31889,how to add new label(say mobilephone) to the existing Image classification pre trained model :android,"
I'm working on Android project to use Image classification , my requirement is to add new label (say mobilephone )to the existing Image classification(detection) pre trained model , is that possible ? If yes how to do that . I see when we train new category it does not add to the existing pre trained model ,example referred is mentioned below 

https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#8

"
31887,//tensorflow/contrib/distributions/python/kernel_tests/independent_test.py fails  with assertion error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 s390x
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.15
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
```
python tensorflow/contrib/distributions/python/kernel_tests/independent_test.py
======================================================================
FAIL: testMnistLikeDynamicShape (__main__.ProductDistributionTest)
testMnistLikeDynamicShape (__main__.ProductDistributionTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor
    yield
  File ""/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py"", line 162, in run
    testMethod()
  File ""tensorflow/contrib/distributions/python/kernel_tests/independent_test.py"", line 275, in testMnistLikeDynamicShape
    self._testMnistLike(static_shape=False)
  File ""tensorflow/contrib/distributions/python/kernel_tests/independent_test.py"", line 269, in _testMnistLike
    rtol=1e-6, atol=0.)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 1073, in decorated
    return f(*args, **kwds)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2303, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2272, in _assertAllCloseRecursive
    (path_str, path_str, msg)))
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2207, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=""\n"".join(msgs), equal_nan=True)
  File ""/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py"", line 1501, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py"", line 827, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=1e-06, atol=0
Mismatched value: a is different from b.
not close where = (array([1, 2, 2, 2, 3]), array([3, 1, 2, 4, 4]), array([6, 1, 1, 8, 6]))
not close lhs = [-463.41407785 -467.87059292 -444.45405599 -469.33429804 -457.64799854]
not close rhs = [-463.41458 -467.87006 -444.45358 -469.3348  -457.6486 ]
not close dif = [0.00050345 0.00053677 0.00047323 0.00051031 0.00059155]
not close tol = [0.00046341 0.00046787 0.00044445 0.00046933 0.00045765]
dtype = float64, shape = (4, 5, 10)
Mismatch: 2.5%
Max absolute difference: 0.00059155
Max relative difference: 1.29257756e-06
 x: array([[[-465.912459, -448.916315, -457.207675, -486.805523,
         -456.784984, -448.14827 , -453.583166, -486.295655,
         -468.533898, -481.740375],...
 y: array([[[-465.9126 , -448.9159 , -457.20764, -486.8053 , -456.7849 ,
         -448.1483 , -453.5835 , -486.2955 , -468.53412, -481.74048],
        [-472.38965, -483.41187, -464.7721 , -467.14288, -478.4115 ,...

======================================================================
FAIL: testMnistLikeStaticShape (__main__.ProductDistributionTest)
testMnistLikeStaticShape (__main__.ProductDistributionTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor
    yield
  File ""/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py"", line 162, in run
    testMethod()
  File ""tensorflow/contrib/distributions/python/kernel_tests/independent_test.py"", line 272, in testMnistLikeStaticShape
    self._testMnistLike(static_shape=True)
  File ""tensorflow/contrib/distributions/python/kernel_tests/independent_test.py"", line 269, in _testMnistLike
    rtol=1e-6, atol=0.)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 1073, in decorated
    return f(*args, **kwds)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2303, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2272, in _assertAllCloseRecursive
    (path_str, path_str, msg)))
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2207, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=""\n"".join(msgs), equal_nan=True)
  File ""/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py"", line 1501, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py"", line 827, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=1e-06, atol=0
Mismatched value: a is different from b.
not close where = (array([1, 2, 2, 2, 3]), array([3, 1, 2, 4, 4]), array([6, 1, 1, 8, 6]))
not close lhs = [-463.41407785 -467.87059292 -444.45405599 -469.33429804 -457.64799854]
not close rhs = [-463.41458 -467.87006 -444.45358 -469.3348  -457.6486 ]
not close dif = [0.00050345 0.00053677 0.00047323 0.00051031 0.00059155]
not close tol = [0.00046341 0.00046787 0.00044445 0.00046933 0.00045765]
dtype = float64, shape = (4, 5, 10)
Mismatch: 2.5%
Max absolute difference: 0.00059155
Max relative difference: 1.29257756e-06
 x: array([[[-465.912459, -448.916315, -457.207675, -486.805523,
         -456.784984, -448.14827 , -453.583166, -486.295655,
         -468.533898, -481.740375],...
 y: array([[[-465.9126 , -448.9159 , -457.20764, -486.8053 , -456.7849 ,
         -448.1483 , -453.5835 , -486.2955 , -468.53412, -481.74048],
        [-472.38965, -483.41187, -464.7721 , -467.14288, -478.4115 ,...

----------------------------------------------------------------------
Ran 10 tests in 1.036s

FAILED (failures=2)

```
**Describe the expected behavior**
```
The test should pass on s390x.
```
**Code to reproduce the issue**
```
python tensorflow/contrib/distributions/python/kernel_tests/independent_test.py
```

**Other info / logs**

The following solution resolves the issue.
``` diff
--- a/tensorflow/contrib/distributions/python/kernel_tests/independent_test.py
+++ b/tensorflow/contrib/distributions/python/kernel_tests/independent_test.py
@@ -20,7 +20,7 @@ from __future__ import print_function

 import importlib
 import numpy as np
-
+import sys
 from tensorflow.contrib.distributions.python.ops import independent as independent_lib
 from tensorflow.contrib.distributions.python.ops import mvn_diag as mvn_diag_lib
 from tensorflow.python.framework import dtypes
@@ -231,9 +231,15 @@ class ProductDistributionTest(test.TestCase):
     def expected_log_prob(x, logits):
       return (x * logits - np.log1p(np.exp(logits))).sum(-1).sum(-1).sum(-1)

-    with self.cached_session() as sess:
-      logits_ph = array_ops.placeholder(
-          dtypes.float32, shape=logits.shape if static_shape else None)
+      with self.cached_session() as sess:
+
+       if sys.byteorder == ""big"":
+            logits_ph = array_ops.placeholder(
+            dtypes.float64, shape=logits.shape if static_shape else None)
+       else:
+            logits_ph = array_ops.placeholder(
+            dtypes.float32, shape=logits.shape if static_shape else None)
+
       ind = independent_lib.Independent(
           distribution=bernoulli_lib.Bernoulli(logits=logits_ph))
       x = ind.sample(sample_shape, seed=42)
```
The test passes with the above change on s390x as well as x86.
Could you please comment if this change is appropriate?"
31885,Enabling model training on Android ,"The current support of tensorflow for Android does not include the binaries required for training. Is there a way by which we can include the required function's files ( for training basic image recognition model) in the libtensorflow-core.a?
Or any other method by which we can enable training?"
31884,declared output erros while building,"How can I fix these errors with tensorflow 1.14.0 and bazel 1.24

```
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/python/BUILD:3469:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/python/BUILD:102:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/contrib/BUILD:12:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/mh.naderan/.cache/bazel/_bazel_mh.naderan/a6a33b5ca42465d31b51832fb008b42b/external/local_config_cuda/cuda/BUILD:168:1: declared output 'external/local_config_cuda/cuda/cuda/include/thrust/detail/dispatch/is_trivial_copy.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: /home/mh.naderan/.cache/bazel/_bazel_mh.naderan/a6a33b5ca42465d31b51832fb008b42b/external/local_config_cuda/cuda/BUILD:168:1: declared output 'external/local_config_cuda/cuda/cuda/include/thrust/iterator/detail/is_trivial_iterator.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: /home/mh.naderan/.cache/bazel/_bazel_mh.naderan/a6a33b5ca42465d31b51832fb008b42b/external/local_config_cuda/cuda/BUILD:168:1: declared output 'external/local_config_cuda/cuda/cuda/include/thrust/system/detail/generic/type_traits.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: /home/mh.naderan/.cache/bazel/_bazel_mh.naderan/a6a33b5ca42465d31b51832fb008b42b/external/local_config_cuda/cuda/BUILD:168:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3.481s, Critical Path: 2.85s
INFO: 15 processes: 15 local.
FAILED: Build did NOT complete successfully

```"
31883,Can TF add a C++ kernel for PRelu? ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-16
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.4, master branch, commit id is c188ffcc9e8769e5c6f767b7fb1977e54cad4040
- Python version: 2.7
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc 6.3
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
Some models(MTCC) uses PRelu as an activation layer, and it will be split to small ops including Relu, Sub(https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/advanced_activations.py#L141). It's hard to fuse and will impact the performance .

**Describe the expected behavior**
Hope to have a TF C++ kernel for this op,  then it could call the kernel directly to get good cache locality on CPU or reduce memory access on GPU.

**Code to reproduce the issue**
NA



**Other info / logs**
NA

"
31882,no attribute python,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
31881,no attribute python,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
31880,Converting .pb to tflite GetOpWithOutput-Error,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
tf-nightly 1.15.0.dev20190819
- Python version:
2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I trained a object-detection model with my own dataset and I'm trying to convert the output to a tflite file for my mobile application

My code to convert the checkpoint to .pb:
python export_tflite_ssd_graph.py 
--pipeline_config_path /tensorflow/models/research/object_detection/training/ssd_mobilenet_v1.config 
--trained_checkpoint_prefix /tensorflow/models/research/object_detection/dataset/model.ckpt-50000 
--output_directory /tensorflow/models/research/object_detection/dataset/tflite --add_postprocessing_op=true

My code to convert .pb to tflite 

!tflite_convert --output_file /tensorflow/models/research/object_detection/dataset/tflite/tflite_graph.tflite \
                --graph_def_file /tensorflow/models/research/object_detection/dataset/tflite/tflite_graph.pb \
                --output_format TFLITE \
                --inference_type FLOAT \
                --input_arrays image_tensor \
                --input_shapes 1,300,300,3 \
                --output_arrays raw_detection_scores \
                --mean_values=128 \
                --std_dev_values=128 \
                --change_concat_input_ranges=false \
                --allow_nudging_weights_to_use_fast_gemm_kernel=true \
                --allow_custom_ops

The error from console:
2019-08-22 08:59:26.271136: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-08-22 08:59:26.275691: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-08-22 08:59:26.277466: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56240f206dc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-08-22 08:59:26.277520: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/tflite_convert.py"", line 515, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/tflite_convert.py"", line 511, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/tflite_convert.py"", line 199, in _convert_tf1_model
    output_data = converter.convert()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/lite.py"", line 989, in convert
    **converter_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/convert.py"", line 412, in toco_convert_graph_def
    enable_mlir_converter=enable_mlir_converter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
WARNING: Logging before flag parsing goes to stderr.
W0822 08:59:27.571232 139982160922432 __init__.py:689] 

  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.

  Please upgrade your code to TensorFlow 2.0:
    * https://www.tensorflow.org/beta/guide/migration_guide

  Or install the latest stable TensorFlow 1.X release:
    * `pip install -U ""tensorflow==1.*""`

  Otherwise your code may be broken by the change.

  
2019-08-22 08:59:27.633571: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TFLite_Detection_PostProcess
2019-08-22 08:59:27.640703: F tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array ""raw_detection_scores"" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.
Aborted (core dumped)

**Describe the expected behavior**

**Code to reproduce the issue**
[tflite_graph.zip](https://github.com/tensorflow/tensorflow/files/3528754/tflite_graph.zip)




Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31879,"Lite: is the ""MinimumRuntimeVersion"" list still under maintenance? ","*This is an issue ""compiled by eyes in original source"", so there is no templeted issue questions*.

Hi, I notice that the ""[GetMinimumRuntimeVersionForModel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/op_version.cc#L26)"" doesn't include **a complete list** of supported Ops of TFLite when I am adding new op. I am curious that if that list is still under maintenance? It seems that the MinimumRuntimeVersion is simply a tip string that won't block model to run on TFLite of any version. And [the min version searching logic](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/op_version.cc#L198) seems restricted to regex `[0-9]\.[0-9]\.[0-9]`, which means `1.9.0` is a larger version than `1.14.0`.

As there seems no runtime check to enforce this min version, maybe we can remove it to lease the op versioning handling? Or we can improve it and enforce it (include PRs)?

Ping people who may know, sorry for the spam ;)
@haozha111 @jianlijianli @bjacob @aselle @suharshs "
31877,graph_transforms/transform_graph failed with shared backbone and multiple input node,"**I try to optimize my model with tf transform_graph tool, and failed**

bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=inference.pb \
--out_graph=inference_opt.pb \
--inputs='input_image_1:0,input_image2:0,input_image_3:0 \
--outputs='output_node_1:0,output_node_2:0 \
--transforms='
  remove_nodes(op=Identity, op=CheckNumerics)
  fold_batch_norms
  fold_old_batch_norms
  sort_by_execution_order'

**Then long log print occur and never stop, something like this:**

2019-08-22 10:56:11.108570: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/read/_135__cf__135 to be preserved.
2019-08-22 10:56:11.108594: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/read/_136__cf__136 to be preserved.
2019-08-22 10:56:11.108604: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/moving_mean/read/_137__cf__137 to be preserved.
2019-08-22 10:56:11.108614: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/moving_variance/read/_138__cf__138 to be preserved.
2019-08-22 10:56:11.108623: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.
2019-08-22 10:56:11.120159: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/read/_140__cf__140 to be preserved.
2019-08-22 10:56:11.120181: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/read/_141__cf__141 to be preserved.
2019-08-22 10:56:11.120192: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/project/BatchNorm/moving_mean/read/_142__cf__142 to be preserved.
2019-08-22 10:56:11.120201: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/project/BatchNorm/moving_variance/read/_143__cf__143 to be preserved.
2019-08-22 10:56:11.120211: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.
2019-08-22 10:56:11.138263: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/Conv_1/BatchNorm/beta/read/_10__cf__10 to be preserved.
2019-08-22 10:56:11.138286: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/Conv_1/BatchNorm/gamma/read/_11__cf__11 to be preserved.
2019-08-22 10:56:11.138294: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/Conv_1/BatchNorm/moving_mean/



**And after I remove  transform option  **remove_nodes(op=Identity, op=CheckNumerics)**,
It seems succeed, but when I open optimized pb fie, bn node is still there, which not be fold.**
I test other classification model,  transform_graph just woks well. But in my model, it just can not work. **By the way, my model has multiple input and shares backbone mobilenet_v2**.

bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=inference.pb \
--out_graph=inference_opt.pb \
--inputs='input_image_1:0,input_image2:0,input_image_3:0 \
--outputs='output_node_1:0,output_node_2:0 \
--transforms='
  fold_batch_norms
  fold_old_batch_norms
  sort_by_execution_order'"
31876,Invalid Link for TensorFlow Keras Official Documentation,"## URL with the issue:
https://www.tensorflow.org/guide/keras
https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras.ipynb
https://github.com/tensorflow/docs/blob/master/site/en/guide/keras.ipynb

## Description of issue (what needs changing):
The last two links are directed to 404, which means they don't exist with valid colab files or gitlab files."
31875,Aanconda python 3.7  could not find tf-nightly-2.0-preview,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
Anaconda 
python 3.7
conda 4.7.10







**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
![image](https://user-images.githubusercontent.com/27112868/63479254-1bc26380-c4c0-11e9-9cb7-37e00538778d.png)

![image](https://user-images.githubusercontent.com/27112868/63481925-862bd180-c4c9-11e9-9b4e-218e3db603e4.png)




![image](https://user-images.githubusercontent.com/27112868/63480959-7a8adb80-c4c6-11e9-94fe-e292af574574.png)


**Problem**  
_pip install  tf-nightly-2.0-preview_  did  not work
however _pip search  tf-nightly-2.0-preview_  works

Further,    I found      _pip install tf-nightly_       works.


![image](https://user-images.githubusercontent.com/27112868/63483541-51bb1400-c4cf-11e9-87d8-9d7c103fbbc8.png)


could you solve this problems ?
thanks 





**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31874,Compute library for hardware acceleration ,"Does google has their own compute library for hardware(arm cpu, gpu, fpga, etc.) acceleration? If google has it, where is it located at? which directory? Thanks!"
31873,Parts of graphs are missing during tf.lite.TFLiteConverter,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow version (use command below): r1.14
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version : gcc 5.4.0 
- CUDA/cuDNN version: 10.0 / 7
- GPU model and memory: Nvidia GeForce GTX TITAN 



This is a little bit tricky to explain but I found that TFLiteConverter, which is shown above, drops a bunch of nodes in the tensorflow computation graph. 

Here is a part of my converting code.
```
    converter = tf.lite.TFLiteConverter.from_saved_model(latest,
                input_arrays=[""Placeholder"",""Placeholder_1""],
                input_shapes={""Placeholder"":[1,144000,1], ""Placeholder_1"":[1,500]},
                output_arrays=[output_node_names])

    # graph dump options
    converter.dump_graphviz_dir=""graph_dir"" # outputs graph visualization file (.dot)
    tflite_model = converter.convert()
    open(""base_model.tflite"", ""wb"").write(tflite_model)

```

The problem part of graph can be described as follows with input and output node names.
The original part of the graph looks like
```
...
Squeeze, Reshape_2 -> add (Add op)
Reshape_5, add -> add_1 (Add op)
...
```
, but the converter gave me 
```
...
add,Squeeze -> add_1 (Add op)
...
```
. The paths containing Reshape_5 and Reshapes_2 are completely missing in the converted graph. The paths containing Reshape_5 and Reshape_2 start from Const op nodes, not input nodes, so that the wrong converted graph still gives me output array in the same shape, but the output value is wrong. 

When I inspected the dumped graph such as toco_AT_IMPORT.dot, toco_AFTER_TRANSFORMATIONS.dot and toco_AFTER_ALLOCATION.dot, to find out the cause.
I found that toco_AT_IMPORT.dot, which is dumped at the importing the saved_model stage, already dropped the nodes. Thus I think that the problem occurred at the 'importing' stage, which reads the saved_model as GraphDef.

I modified the converter code to see intermediate tflite graph dump files in *.dot, as follows:
(https://github.com/tensorflow/tensorflow/blob/b431fcc28d6f905fd868d979121b8e6c80902731/tensorflow/lite/toco/import_tensorflow.cc#L2642)
```
std::unique_ptr<Model> ImportTensorFlowGraphDef(
    const ModelFlags& model_flags, const TensorFlowImportFlags& tf_import_flags,
    const GraphDef& tf_graph) {
  LogDumpGraphDef(kLogLevelModelChanged, ""AT IMPORT"", tf_graph);

  GraphDef inlined_graph(tf_graph);
  if (InlineAllFunctions(&inlined_graph)) {
    LogDumpGraphDef(kLogLevelModelChanged, ""AFTER INLINING"", inlined_graph);
  }

  // Check input and output specification.
  for (const auto& specified_input_array : model_flags.input_arrays()) {
    CHECK(!absl::EndsWith(specified_input_array.name(), "":0""))
        << ""Unsupported explicit zero output index: ""
        << specified_input_array.name();
  }
  for (const string& specified_output_array : model_flags.output_arrays()) {
    CHECK(!absl::EndsWith(specified_output_array, "":0""))
        << ""Unsupported explicit zero output index: "" << specified_output_array;
  }

  Model* model = new Model;
  internal::ConverterMapType converter_map;

  // This is used for the TFLite ""Full Flex Mode"" conversion. All the ops are
  // imported as `TensorFlowUnsupportedOperator`, and later all these ops are
  // converted to TFLite Flex ops.
  if (!tf_import_flags.import_all_ops_as_unsupported) {
    converter_map = internal::GetTensorFlowNodeConverterMap();
  } else {
    converter_map = internal::GetTensorFlowNodeConverterMapForFlex();
  }

  for (auto node : inlined_graph.node()) {
    StripZeroOutputIndexFromInputs(&node);
    auto status = internal::ImportTensorFlowNode(node, tf_import_flags, model,
                                                 converter_map);
    CHECK(status.ok()) << status.error_message();
  }
  //LogDump(kLogLevelModelChanged, ""IN IMPORT 0"", *model); // fail
  ResolveModelFlags(model_flags, model);

  //LogDump(kLogLevelModelChanged, ""IN IMPORT 1"", *model); // fail
  StripCaretFromArrayNames(model);
  //LogDump(kLogLevelModelChanged, ""IN IMPORT 2"", *model); // fail
  AddExtraOutputs(model);
  //LogDump(kLogLevelModelChanged, ""IN IMPORT 3"", *model); // fail
  FixNoMissingArray(model);
  LogDump(kLogLevelModelChanged, ""IN IMPORT 4"", *model); //
  FixNoOrphanedArray(model);
  LogDump(kLogLevelModelChanged, ""IN IMPORT 5"", *model); //
  FixOperatorOrdering(model);
  LogDump(kLogLevelModelChanged, ""IN IMPORT 6"", *model); //
  CheckInvariants(*model);
  LogDump(kLogLevelModelChanged, ""IN IMPORT 7"", *model); //

  // if rnn state arrays are constant, make them transient
  for (const auto& rnn_state : model->flags.rnn_states()) {
    model->GetArray(rnn_state.state_array()).buffer = nullptr;
  }

  return std::unique_ptr<Model>(model);
}
```
I want to find out the exact stage that causes the problem and fix it. The code gave me toco_IN_IMPORT_4.dot, toco_IN_IMPORT_5.dot, toco_IN_IMPORT_6.dot and toco_IN_IMPORT_7.dot, however, they were all same to toco_AT_IMPORT.dot.

I really need helps for this problem. Here are my questions.
1. What are possible causes of this problem? Is it in my tensorflow model or the converter?
2. The 'LogDumpGraphDef' function seems to dump outputs showing the graph definition, but how can I actually get its output?
"
31872,"Compiling from source, error with finding cuda ","
- OS Platform and Distribution: Centos 7
- TensorFlow installed from: source
- TensorFlow version: 1.15
- Python version: 2.7.5
- Installed using virtualenv: virtualenv
- Bazel version: 0.26.0
- GCC/Compiler version: 7.3.0
- CUDA version: CUDA 10.1
-cuDNN version: 7.6.2
- TensorRT: 5.1.5
- GPU: Tesla T4

I'm trying to install tensorflow from source and tried v1.14 and v1.15. When running ./configure, I select yes for CUDA and TensorRT support. I initially get an error saying no cuda.h is found, so I point the path to /usr/include/linux. The installation fails with the following error:

`Traceback (most recent call last):
  File ""third_party/gpus/find_cuda_config.py"", line 500, in <module>
    main()
  File ""third_party/gpus/find_cuda_config.py"", line 492, in main
    for key, value in sorted(find_cuda_config().items()):
  File ""third_party/gpus/find_cuda_config.py"", line 457, in find_cuda_config
    result.update(_find_cuda_config(cuda_paths, cuda_version))
  File ""third_party/gpus/find_cuda_config.py"", line 251, in _find_cuda_config
    get_header_version)
  File ""third_party/gpus/find_cuda_config.py"", line 238, in _find_header
    required_version, get_version)
  File ""third_party/gpus/find_cuda_config.py"", line 227, in _find_versioned_file
    actual_version = get_version(file)
  File ""third_party/gpus/find_cuda_config.py"", line 244, in get_header_version
    version = int(_get_header_version(path, ""CUDA_VERSION""))
ValueError: invalid literal for int() with base 10: ''
Asking for detailed CUDA configuration...
`

Full log:

WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.26.0 installed.
Please specify the location of python. [Default is /home/mltlocal/envs/Resnet50GPU/bin/python]:


Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'getsitepackages'
Found possible Python library paths:
  /home/mltlocal/envs/Resnet50GPU/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/home/mltlocal/envs/Resnet50GPU/lib/python2.7/site-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Could not find any cuda.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
        '/lib64'
        '/opt/rh/devtoolset-4/root/usr/lib64/dyninst'
        '/usr'
        '/usr/lib64/atlas'
        '/usr/lib64/dyninst'
        '/usr/lib64/mysql'
        '/usr/local/cuda-10.1/targets/x86_64-linux/lib'
Asking for detailed CUDA configuration...

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 10.1


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.6.2


Please specify the TensorRT version you want to use. [Leave empty to  default to TensorRT 5]:


Please specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]:


Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /usr/include/linux


Traceback (most recent call last):
  File ""third_party/gpus/find_cuda_config.py"", line 500, in <module>
    main()
  File ""third_party/gpus/find_cuda_config.py"", line 492, in main
    for key, value in sorted(find_cuda_config().items()):
  File ""third_party/gpus/find_cuda_config.py"", line 457, in find_cuda_config
    result.update(_find_cuda_config(cuda_paths, cuda_version))
  File ""third_party/gpus/find_cuda_config.py"", line 251, in _find_cuda_config
    get_header_version)
  File ""third_party/gpus/find_cuda_config.py"", line 238, in _find_header
    required_version, get_version)
  File ""third_party/gpus/find_cuda_config.py"", line 227, in _find_versioned_file
    actual_version = get_version(file)
  File ""third_party/gpus/find_cuda_config.py"", line 244, in get_header_version
    version = int(_get_header_version(path, ""CUDA_VERSION""))
ValueError: invalid literal for int() with base 10: ''
Asking for detailed CUDA configuration...
"
31871,TF 2.0 dev20190820 consumes more memory than dev20190504 when training keras model using train_on_batch(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1903
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-dev20190820
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10, cuDNN 7.6.2.24
- GPU model and memory: Geforce RTX 2080 ti 11GB

**Describe the current behavior**
Program crashes due to insufficient memory issue. But with same code, dev20190504 can train the model without any problem.

**Describe the expected behavior**
Can train large model using dev20190820 without memory issue, like dev20190504.

**Code to reproduce the issue**
```
import os
import tensorflow as tf
import numpy as np

os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'


def conv(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal'):
    c = tf.keras.layers.Conv2D(
        filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=initializer, use_bias=False)(x)

    return c


def conv_bn(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal', bn_gamma_initializer='ones'):
    c = conv(x, filters=filters, kernel_size=kernel_size,
             strides=strides, padding=padding, initializer=initializer)

    c_bn = tf.keras.layers.BatchNormalization(
        gamma_initializer=bn_gamma_initializer)(c)

    return c_bn


def conv_bn_relu(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal', bn_gamma_initializer='ones'):
    c_bn = conv_bn(x, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,
                   initializer=initializer, bn_gamma_initializer=bn_gamma_initializer)

    return tf.keras.layers.Activation('relu')(c_bn)


def conv_gap(x, output_filters, kernel_size=(1, 1)):
    x = conv(x, filters=output_filters, kernel_size=kernel_size)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)

    return x


def my_block(x, output_filters, inter_filters):
    c1 = conv_bn_relu(x, inter_filters, (1, 1))
    c2 = conv_bn_relu(c1, inter_filters, (3, 3))
    c3 = conv_bn(
        c2, output_filters, (1, 1), bn_gamma_initializer='zeros')

    p = tf.keras.layers.add([c3, x])

    return tf.keras.layers.Activation('relu')(p)


def my_block_inc(x, output_filters, inter_filters, strides1x1=(1, 1), strides2x2=(2, 2)):
    c1 = conv_bn_relu(
        x, inter_filters, (1, 1), strides=strides1x1)
    c2 = conv_bn_relu(
        c1, inter_filters, (3, 3), strides=strides2x2)
    c3 = conv_bn(
        c2, output_filters, (1, 1), bn_gamma_initializer='zeros')

    strides = np.multiply(strides1x1, strides2x2)
    s = conv_bn(
        x, output_filters, (1, 1), strides=strides)  # shortcut

    p = tf.keras.layers.add([c3, s])

    return tf.keras.layers.Activation('relu')(p)


def repeat_blocks(x, block_delegate, count, **kwargs):
    assert count >= 0

    for _ in range(count):
        x = block_delegate(x, **kwargs)
    return x


# This line makes trick!
tf.keras.backend.set_learning_phase(1)
shape = (299, 299, 3)

inputs = tf.keras.Input(shape, dtype='float32')
outputs = conv_bn_relu(inputs, 256 // 4, (7, 7), strides=(2, 2))
outputs = tf.keras.layers.MaxPool2D(
    (3, 3), strides=(2, 2), padding='same')(outputs)
outputs = my_block_inc(outputs, 256, 256 // 4, strides2x2=(1, 1))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=2,
                        output_filters=256,
                        inter_filters=256 // 4)

outputs = my_block_inc(outputs, 512, 512 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=7,
                        output_filters=512,
                        inter_filters=512 // 4)

outputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=40,
                        output_filters=1024,
                        inter_filters=1024 // 4)

outputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=16,
                        output_filters=1024,
                        inter_filters=1024 // 4)

outputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=16,
                        output_filters=1024,
                        inter_filters=1024 // 4)

outputs = my_block_inc(outputs, 2048, 2048 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=6,
                        output_filters=2048,
                        inter_filters=2048 // 4)

outputs = conv_gap(outputs, 1024)
outputs = tf.keras.layers.Activation('sigmoid')(outputs)
model = tf.keras.models.Model(
    inputs=inputs, outputs=outputs, name='resnet_custom_v2')

optimizer = tf.optimizers.Adam(0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy',
              metrics=['mean_absolute_error'])


def make_batch(_):
    x = np.ones((shape[0], shape[1], shape[2]), dtype='float32')
    y = np.ones((1024), dtype='float32')

    return (x, y)


dataset = tf.data.Dataset.from_tensor_slices(
    ['0'] * 40)
dataset = dataset.map(lambda x: tf.py_function(
    make_batch, (x,), (tf.float32, tf.float32)))
dataset = dataset.batch(20)

for x, y in dataset:
    step_results = model.train_on_batch(x=x, y=y)
    print(f'loss={step_results[0]}')

print('press input key')
input()

```
"
31870,TF_CPP_MIN_LOG_LEVEL does not work with TF2.0 dev20190820,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1903
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-dev20190820
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10, cuDNN 7.6.2.24
- GPU model and memory: Geforce RTX 2080 ti 11GB

**Describe the current behavior**
Setting TF_CPP_MIN_LOG_LEVEL does not work lateset TF 2.0. If I set TF_CPP_MIN_LOG_LEVEL to 2, sill TF shows information and warning logs (including libpng warnings)

With dev20190504, this issue does not occurred.

**Describe the expected behavior**
Set TF_CPP_MIN_LOG_LEVEL=2 should prevent showing information/warning logs including libpng warnings.

**Code to reproduce the issue**
```
import os
import tensorflow as tf
import numpy as np

os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""2""

a = tf.Variable(np.array([0, 1, 2]))

print(a)
```
You can see many [I] log using dev20190820. But with dev20190504, no logs."
31867,Bug on last batch of first epoch using TPU with pretrained-model on google colab,"Hi,

I am trying to train a mobileNetV2 coming from tf.keras but the training stops on the last batch of the first epoch: 
Epoch 1/15
1/2 [==============>...............] - ETA: 10s - loss: 2.7432 - acc: 0.1406
Don't mind about the number of batch, adding some more  changes nothing

Here's the error :
Compilation failure: Asked to propagate a dynamic dimension from hlo %convert.283 = f32[8,80,80,32]{3,2,1,0} convert(f32[8,80,80,32]{3,2,1,0} %add.1), metadata={op_type=""FusedBatchNorm"" op_name=""bn_Conv1_2/FusedBatchNorm""}@{}@0 to hlo %clamp.288 = f32[8,80,80,32]{3,2,1,0} clamp(f32[8,80,80,32]{3,2,1,0} %broadcast.286, f32[8,80,80,32]{3,2,1,0} %convert.283, f32[8,80,80,32]{3,2,1,0} %broadcast.287), metadata={op_type=""Relu6"" op_name=""Conv1_relu_2/Relu6""}, which is not implemented.
	TPU compilation failed
	 [[{{node TPUReplicateMetadata_1}}]]

image of the error for more clarity:
https://drive.google.com/open?id=1Gkzow3mrUJPACYfjz6E7V3Efj8eO261F

Here's my configuration below:

Python 3.6.8 (default, Jan 14 2019, 11:02:34) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux

tensorflow=1.14.0

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130

Here's the link to my ipynb : https://drive.google.com/open?id=1w2VJAH1gIv6-tJms3qrQg8IxSoKS1aw0

Any idea of what's going on ?"
31866,error: 'is_final' is not a member of 'std',"With 
  bazel 0.26.1
  gcc 7.4
  cuda 10.1 update 2

and the latest git clone of tensorflow, I hit the following error at the bazel build command

```
ERROR: /home/mh.naderan/.cache/bazel/_bazel_mh.naderan/dacf7a124fc721f30ac789c201b3b139/external/llvm/BUILD.bazel:201:1: C++ compilation of rule '@llvm//:llvm-tblgen' failed (Exit 1)
In file included from external/llvm/include/llvm/TableGen/Record.h:27:0,
                 from external/llvm/utils/TableGen/SubtargetFeatureInfo.h:13,
                 from external/llvm/utils/TableGen/SubtargetFeatureInfo.cpp:9:
external/llvm/include/llvm/Support/TrailingObjects.h: In static member function 'static void llvm::TrailingObjects<BaseTy, TrailingTys>::verifyTrailingObjectsAssertions()':
external/llvm/include/llvm/Support/TrailingObjects.h:252:24: error: 'is_final' is not a member of 'std'
     static_assert(std::is_final<BaseTy>(), ""BaseTy must be final."");
                        ^~~~~~~~
external/llvm/include/llvm/Support/TrailingObjects.h:252:24: note: suggested alternative: 'is_heap'
     static_assert(std::is_final<BaseTy>(), ""BaseTy must be final."");
                        ^~~~~~~~
                        is_heap
external/llvm/include/llvm/Support/TrailingObjects.h:252:39: error: expected primary-expression before '>' token
     static_assert(std::is_final<BaseTy>(), ""BaseTy must be final."");
                                       ^
external/llvm/include/llvm/Support/TrailingObjects.h:252:41: error: expected primary-expression before ')' token
     static_assert(std::is_final<BaseTy>(), ""BaseTy must be final."");
                                         ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 442.171s, Critical Path: 25.05s
INFO: 1254 processes: 1254 local.
FAILED: Build did NOT complete successfully

```"
31859,Tensorflow lite: micro_vision missing model data,"On the latest commit (eb7b5e6) on master branch, the mico_vision example is still missing `person_detect.tflite` as well as `person_detect_model_data.cc`. This problem is mentioned in issues #29792 and #29524. The only resolution mentioned in these closed issues are mentions that it ""should be fixed now"" without mention to a commit that fixes it or a pointer to the files existing in a branch.

If these files do exist in some history and branch, where can I find them?
"
31855,Building TensorFlow 1.14 with local GRPC via TF_SYSTEM_LIBS,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.14
- Python version: 3.6
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc 6.3.0
- CUDA/cuDNN version: 10.1, 7.5.1
- GPU model and memory: N/A

**Describe the problem**

I'm trying to build TF 1.14.0 from source using the setup described above. We have a patched version of GRPC that I'm trying to build against, so I'm trying to use TF_SYSTEM_LIBS=""grpc"". I've copied the GRPC binaries and headers into a folder called ""prefix"" and set the PREFIX variable before calling configure. Here's the resulting .tf_configure.bazelrc:

```
build --action_env PYTHON_BIN_PATH=""/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build_bin/python3.6""
build --action_env PYTHON_LIB_PATH=""/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/build/lib/python3.6""
build --python_path=""/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build_bin/python3.6""
build:xla --define with_xla_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env TF_NEED_TENSORRT=""0""
build --action_env TF_CUDA_VERSION=""10.1""
build --action_env TF_CUDNN_VERSION=""7.5""
build --action_env TF_NCCL_VERSION=""2.4""
build --action_env TF_CUDA_PATHS=""/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/build/cuda,/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/build/cudnn,/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/nccl/2/4/2/dist""
build --action_env CUDA_TOOLKIT_PATH=""/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/build/cuda""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.0""
build --action_env LD_LIBRARY_PATH=""/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/cuda/10/1/local/cuda/lib64/stubs:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/bazel/skylib/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/jq/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/oniguruma/5/9/6/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/hdf5/1/8/19/c/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/vendor/intel/mkl/2019/2/187/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/nccl/2/4/2/dist/lib/x86_64-linux-gnu:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/cudnn/7/5/1/dist/lib/x86_64-linux-gnu:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/cuda/10/1/dist/lib64:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/cares/1/15/0/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/openjdk/11/0/4/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/google/protobuf/3/6/1/c/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/libffi/3/2/1/dist/lib64/:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/libuuid/1/0/3/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/zlib/1/2/8/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/xz/5/2/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gnu/libs/gcc/x/dist/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gnu/gcc/5/4/0/dist/exported_libs/:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gnu/gdb/7/8/c/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/bz2/1/0/6/c/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/sqlite/3/15/1/dist/lib:/opt/openssl/1.0/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gdbm/1/11/c/lib:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/db/4/7/25/c/lib""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build --config=cuda
build:opt --copt=-march=sandybridge
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --action_env TF_SYSTEM_LIBS=""grpc""
build --define=PREFIX=/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/prefix
build --define=LIBDIR=/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/prefix/lib
build --define=INCLUDEDIR=/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/prefix/include
build:v2 --define=tf_api_version=2
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial
test --build_tag_filters=-benchmark-test,-no_oss
test --test_tag_filters=-no_gpu
test --build_tag_filters=-no_gpu
test --test_env=LD_LIBRARY_PATH
build --action_env TF_CONFIGURE_IOS=""0""
```

Here are the contents of prefix:
```
/home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/prefix
 include
  grpc -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/include/grpc
  grpc++ -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/include/grpc++
  grpcpp -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/include/grpcpp
 lib
     libaddress_sorting.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libaddress_sorting.so
     libaddress_sorting.so.7 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libaddress_sorting.so.7
     libaddress_sorting.so.7.0.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libaddress_sorting.so.7.0.0
     libgpr.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgpr.a
     libgpr.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgpr.so
     libgpr.so.7 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgpr.so.7
     libgpr.so.7.0.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgpr.so.7.0.0
     libgrpc.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc.a
     libgrpc++.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++.a
     libgrpc_cronet.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_cronet.a
     libgrpc++_cronet.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_cronet.a
     libgrpc_cronet.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_cronet.so
     libgrpc++_cronet.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_cronet.so
     libgrpc++_cronet.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_cronet.so.1
     libgrpc++_cronet.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_cronet.so.1.18.0
     libgrpc_cronet.so.7 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_cronet.so.7
     libgrpc_cronet.so.7.0.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_cronet.so.7.0.0
     libgrpc++_error_details.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_error_details.a
     libgrpc++_error_details.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_error_details.so
     libgrpc++_error_details.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_error_details.so.1
     libgrpc++_error_details.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_error_details.so.1.18.0
     libgrpcpp_channelz.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpcpp_channelz.a
     libgrpcpp_channelz.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpcpp_channelz.so
     libgrpcpp_channelz.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpcpp_channelz.so.1
     libgrpcpp_channelz.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpcpp_channelz.so.1.18.0
     libgrpc++_reflection.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_reflection.a
     libgrpc++_reflection.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_reflection.so
     libgrpc++_reflection.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_reflection.so.1
     libgrpc++_reflection.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_reflection.so.1.18.0
     libgrpc.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc.so
     libgrpc++.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++.so
     libgrpc++.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++.so.1
     libgrpc++.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++.so.1.18.0
     libgrpc.so.7 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc.so.7
     libgrpc.so.7.0.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc.so.7.0.0
     libgrpc_unsecure.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_unsecure.a
     libgrpc++_unsecure.a -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_unsecure.a
     libgrpc_unsecure.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_unsecure.so
     libgrpc++_unsecure.so -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_unsecure.so
     libgrpc++_unsecure.so.1 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_unsecure.so.1
     libgrpc++_unsecure.so.1.18.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc++_unsecure.so.1.18.0
     libgrpc_unsecure.so.7 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_unsecure.so.7
     libgrpc_unsecure.so.7.0.0 -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/libgrpc_unsecure.so.7.0.0
     pkgconfig -> /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/lib/pkgconfig
```
and /home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/dist/include/grpcpp contains impl/codegen/async_generic_service.h.

At build time, bazel complains that it can't find grpcpp/impl/codegen/async_generic_service.h:

```
ERROR: /home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/tensorflow/tensorflow/core/debug/BUILD:49:1: C++ compilation of rule '//tensorflow/core/debug:debug_service_proto_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/adamson/tsgit/tf-114-gpu/ext/public/google/tensorflow/1/14/0/python/build/tensorflow/python3.6/bazel_cache/python3.6/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/grpc/1/18/0/bin:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gnu/gcc/5/4/0/dist/libexec/gcc/x86_64-unknown-linux-gnu/5.4.0:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/nvidia/cuda/10/1/dist/bin:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/google/bazelversions/0/24/1/bin:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/google/protobuf/x/dist/bin:/home/adamson/tsgit/tf-114-gpu/.base_universe/glibc-2.24-x86_64/ext/public/gnu/gcc/x/bin:/bin:/usr/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/debug/_objs/debug_service_proto_cc/debug_service.grpc.pb.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/debug/_objs/debug_service_proto_cc/debug_service.grpc.pb.pic.o' -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/grpc -iquote bazel-out/host/genfiles/external/grpc -iquote bazel-out/host/bin/external/grpc -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -Wno-unknown-warning-option -Wno-unused-but-set-variable -Wno-sign-compare -c bazel-out/host/genfiles/tensorflow/core/debug/debug_service.grpc.pb.cc -o bazel-out/host/bin/tensorflow/core/debug/_objs/debug_service_proto_cc/debug_service.grpc.pb.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
In file included from bazel-out/host/genfiles/tensorflow/core/debug/debug_service.grpc.pb.cc:6:0:
bazel-out/host/genfiles/tensorflow/core/debug/debug_service.grpc.pb.h:26:55: fatal error: grpcpp/impl/codegen/async_generic_service.h: No such file or directory
 #include <grpcpp/impl/codegen/async_generic_service.h>
                                                       ^
```

The provided prefix directory isn't showing up in the include options based to gcc (but -iquote external/grpc is). What do I need to do to get the build to look in the prefix directory for the grpc headers?"
31853,Cannot load saved model with tf.keras.models.load_model,"**System information**
- Wrote custom code for simple model
- Ubuntu 16.04.6 LTS
- Tensorflow 2.0.0-beta1 installed from binary using pip
- Python 3.7, no CUDA or GPU

Loading a saved model results in error: 
__init__() got an unexpected keyword argument 'reduction'

**Code to reproduce the issue**

```
model = tf.keras.Model(inputs=[input_x], outputs=[logits])
loss = tf.keras.losses.MeanSquaredError()
model.compile(optimizer=tf.keras.optimizers.Adam(),  # Optimizer
              # Loss function to minimize; Emphasize not getting false positives with pos_weight
              loss=loss, # tf.nn.weighted_cross_entropy_with_logits(logits,labels,pos_weight=1) # tf.keras.losses.MeanSquaredError()
              # tf.keras.losses.mean_squared_error
              # List of metrics to monitor
              metrics=[tf.keras.losses.MeanSquaredError()])
checkpointer = tf.keras.callbacks.ModelCheckpoint(session_name + '_backup.h5', save_best_only=True, monitor = 'acc', verbose = 0)
early_stopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3, verbose=1,min_delta=0.005)
history = model.fit(data_train, roi_zoom_train,
                    batch_size=batch_size,
                    epochs = 1,
                    # We pass some validation for
                    # monitoring validation loss and metrics
                    # at the end of each epoch
                    validation_data=(data_val, roi_zoom_val),callbacks=[checkpointer,early_stopper]) #
model.save(session_name + '.h5')
model = tf.keras.models.load_model(session_name + '.h5')
```

**Other info / logs**
Traceback (most recent call last):
  File ""/home/m047659/Programs/pycharm/helpers/pydev/_pydevd_bundle/pydevd_exec2.py"", line 3, in Exec
    exec(exp, global_vars, local_vars)
  File ""<input>"", line 1, in <module>
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 137, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 178, in load_model_from_hdf5
    training_config, custom_objects))
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 458, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 320, in compile
    self._cache_output_metric_attributes(metrics, weighted_metrics)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1735, in _cache_output_metric_attributes
    metrics, self.output_names, output_shapes, self.loss_functions)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 755, in collect_per_output_metric_info
    metric_name = get_metric_name(metric, is_weighted)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 929, in get_metric_name
    metric = metrics_module.get(metric)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py"", line 2833, in get
    return deserialize(identifier)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py"", line 2827, in deserialize
    printable_module_name='metric function')
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 194, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 451, in from_config
    return cls(**config)
TypeError: __init__() got an unexpected keyword argument 'reduction'
"
31852,Tensorflow2.0 distributed training gives error :- A non-DistributedValues value 8 cannot be reduced with the given reduce op ReduceOp.SUM.,"<em> **Getting the error when i am running distributed training**  as described in this tutorial (https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/guide/distribute_strategy.ipynb#scrollTo=IlYVC0goepdk)</em>

**System information**
- OS Platform: - Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0-beta
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.01
- GPU model and memory: p100, 16GB

```
@tf.function
def train_step(dist_inputs):
    def step_fn(inputs):
        features, labels = inputs

        with tf.GradientTape() as tape:
            logits = model(features)
            #print(logits)
            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(
                        logits=logits, labels=labels)
            loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)
        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))
        return cross_entropy

    per_example_losses = mirrored_strategy.experimental_run_v2(
            step_fn, args=(dist_inputs,))
    mean_loss = mirrored_strategy.reduce(
            tf.compat.v2.distribute.ReduceOp.MEAN, per_example_losses, axis=0)
    return mean_loss
```
I am getting this error while running abobe distributed code.
`ValueError: A non-DistributedValues value 8 cannot be reduced with the given reduce op ReduceOp.SUM.

**But it works when I use tf.distribute.ReduceOp.SUM instead of tf.distribute.ReduceOp.MEAN** 
`"
31851,"When subclassing, keras converts SparseTensors to Tensors","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04

Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No

TensorFlow installed from (source or binary):
Binary

TensorFlow version (use command below):
1.14.0

**Describe the current behavior**

keras.Model.fit converts SparseTensors to tensors. 

**Describe the expected behavior**

keras.Model.fit passes SparseTensors as is.

**Code to reproduce the issue**

```
import numpy as np
import tensorflow.compat.v2 as tf

def dummy_parse_fn(iterable):
  features = {}
  # the input is always constant
  features['sparse_tensor'] = tf.SparseTensor(
    indices=tf.constant([[0,0],[1,1]], dtype=tf.int64),
    values=tf.constant([1.0, 1.0], dtype=tf.float32),
    dense_shape=tf.constant([2, 2], dtype=tf.int64))
  labels = tf.constant([1.0, 1.0], dtype=tf.float32)
  return features, labels


def get_dummy_dataset():
  iterable =  np.random.random((128, 1)).astype(np.float32)
  return (
    tf.data.Dataset
    .from_tensor_slices(iterable)
    .map(dummy_parse_fn)
    .take(1024)
  )


class SparseModel(tf.keras.Model):
  def call(self, features):
    sparse_tensor = features['sparse_tensor']
    assert isinstance(sparse_tensor, tf.sparse.SparseTensor), type(sparse_tensor)
    return sparse_tensor.values


if __name__ == ""__main__"":

  print(tf.__version__)

  model = SparseModel()
  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)
  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)

  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
  model.fit(get_dummy_dataset(), epochs=2)
```
"
31850,"Using Tensorflow 2.0 loss functions that require inputs gives ""`tf.Tensor` as a Python `bool` is not allowed""  error","** System information **

- Wrote custom code for simple model
- Ubuntu 16.04.6 LTS
- Tensorflow 2.0.0-beta1 installed from binary using pip
- Python 3.7, no CUDA or GPU
- Loading a saved model results in error:
init() got an unexpected keyword argument 'reduction'

** Code to reproduce the issue **

Relevant architecture: 

```
labels = tf.keras.Input(batch_shape=(batch_size, xsze, ysze), name='labels')
logits = tf.keras.layers.Conv2D(1, [1, 1], activation='linear', name='output_x')(drop11)
model = tf.keras.Model(inputs=[input_x], outputs=[logits]
model.compile(optimizer=tf.keras.optimizers.Adam(),  
              loss=loss, 
              metrics=[tf.keras.losses.MeanSquaredError()])
```

If I use a keras loss class that doesn't require explicit input arguments, it works fine: 
`loss = tf.keras.losses.MeanSquaredError() `

But I get the error with any loss that requires inputs: 
`loss = tf.keras.losses.sparse_categorical_crossentropy(labels,logits)`

Including custom functions which don't seem to be mixing numpy and tensor operations, nor do they check for if not None: 

```
def dice_loss_per_image(labels, logits):
     smooth = tf.constant(1e-17,dtype=float)

    numerator = tf.constant(2, dtype=float) * tf.reduce_sum(labels * logits, axis=(1, 2))
    denominator = tf.reduce_sum(labels + logits, axis=(1, 2))

    loss = tf.constant(1.0,dtype=float) - (numerator + smooth)/(denominator + smooth)
    return loss
```

** Error traceback ** 

Traceback (most recent call last):
  File ""/home/m047659/Programs/pycharm/helpers/pydev/_pydevd_bundle/pydevd_exec2.py"", line 3, in Exec
    exec(exp, global_vars, local_vars)
  File ""<input>"", line 1, in <module>
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 137, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 178, in load_model_from_hdf5
    training_config, custom_objects))
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 458, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 320, in compile
    self._cache_output_metric_attributes(metrics, weighted_metrics)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1735, in _cache_output_metric_attributes
    metrics, self.output_names, output_shapes, self.loss_functions)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 755, in collect_per_output_metric_info
    metric_name = get_metric_name(metric, is_weighted)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 929, in get_metric_name
    metric = metrics_module.get(metric)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py"", line 2833, in get
    return deserialize(identifier)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py"", line 2827, in deserialize
    printable_module_name='metric function')
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 194, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""/home/m047659/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 451, in from_config
    return cls(**config)
TypeError: __init__() got an unexpected keyword argument 'reduction' 

Any way to fix this? 
Thank you. "
31848,Eager execution prevents using while loops on Keras symbolic tensors,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Mint 19.1
- TensorFlow installed from: binary (using pip)
- TensorFlow version: 2.0.0-beta1 & 2.0.0-dev10290731 (tried on both)
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: Nvidia Quadro P1000 - 4 GB GDDR5

**Describe the current behavior**

When Eager execution is enabled, `tf.while_loop` uses a backend implementation suited for Eager tensors only, which practically disallows the use of while loops with Keras symbolic tensors. Specifically, the line `while cond(*loop_vars):` ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L2713) as of this date) is only valid when `loop_vars` is a list of `EagerTensor` instances, thus enabling the `while` check on the `numpy` attribute of the results.

Now, my issue is that I actually need to implement a function that works on Keras symbolic tensors and uses a `tf.while_loop`, which seemingly proves impossible (apart from disabling Eager execution, which is a workaround I am comfortable to use in the long term, but i does not feel like an actual solution).

Intuitively, I would think a way to fix the issue would be to follow an alternative route within `tf.while_loop`'s source code when using symbolic tensors (_e.g._ that used when Eager is disabled), and keeping the Eager-suited one otherwise. I tried a nasty fix which consisted in adding `and all(isinstance(tensor, ops.EagerTensor) for tensor in loop_vars)` to the `executing_eagerly = context.executing_eagerly()` line at the beginning of `while_loop`'s body in the source code, but this results in raising `AttributeError: Tensor.name is meaningless when eager execution is enabled.` within the loop constructor (only with symbolic tensors - when using Eager ones, everything goes fine). I would be happy to dig deeper and contribute a fix if I can find one, but first it would be nice to know whether a solution already exist (I might just be missing something, perhaps from the keras backend submodule).

**Describe the expected behavior**

I would like to be able to run symbolic tensors through a while loop without disabling Eager execution (basically, I would like Eager execution not to take away practical functionalities which are very useful in designing models without having to put up small hacks of the framework, which are bound to decrease readability and stability).

**Code to reproduce the issue**

Base code defining the function I want to implement and two minimalist tests (in practice my symbolic tensors are not mere inputs, but the issue is strictly similar):
```python
import tensorflow as tf

def pred_in_top_k(y_true, y_pred, k=5):
    """"""Check whether targets are in top K predictions, for batched samples.

    Extension of `tf.keras.metrics.sparse_top_k_categorical_accuracy`
    to batched sequences of targets and predictions.

    y_true : true labels; tf.int32 or tf.int64 Tensor of shape
             (batch_len, max_seq_len)
    y_pred : predicted probabilities; tf.float32 Tensor of shape
             (batch_len, max_seq_len, n_labels)
    """"""
    # Define the loop's body.
    def body(i, matches):
        """"""Compute matches for a given sample in the batch.""""""
        matching = tf.nn.in_top_k(y_true[i], y_pred[i], k=k)
        matching = tf.expand_dims(tf.cast(matching, tf.float32), 0)
        updated = tf.concat([matches[:i], matching, matches[i + 1:]], axis=0)
        updated.set_shape(matches.shape)
        return i + 1, updated
    # Define the loop's stopping condition.
    def cond(i, _):
        """"""Stop when the entire batch has been processed.""""""
        return tf.less(i, tf.shape(y_true)[0])
    # Run the loop and return the results.
    loop_vars = [tf.constant(0), tf.zeros_like(y_true, dtype=tf.float32)]
    _, matches = tf.while_loop(cond, body, loop_vars)
    return matches


def test_random_tensors():
    """"""""Run pred_in_top_k on random tensors.""""""
    y_true = tf.random.uniform(shape=(4, 10), maxval=20, dtype=tf.int64)
    y_pred = tf.nn.softmax(tf.random.normal(shape=(4, 10, 20))) 
    return pred_in_top_k(y_true, y_pred)


def test_symbolic_tensors():
    """"""Run pred_in_top_k on symbolic tensors.""""""
    y_true = tf.keras.Input((None,), dtype=tf.int64)
    y_pred = tf.keras.Input((None, 20), dtype=tf.float32)
    return pred_in_top_k(y_true, y_pred)
```

`test_random_tensors()` works both with and without having executed `tf.compat.v1.disable_eager_execution()` first.

`test_symbolic_tensors()` fails when Eager is left enabled, with the following error messages depending on the tensorflow 2.0 installation used:

2.0b1:
```python
TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.
```

2.0 nightly:
```python
OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
```

Note that decoration with `@tf.function` does not solve the issue, as functions wrapped this way do not accept Keras symbolic tensors as inputs."
31844,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.","**System information**
- Windows 10:
- TensorFlow installed using PIP:
- TensorFlow version 1.14:


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.

```

Also, please include a link to a GraphDef or the model if possible.
ssd_mobilenet_v1_coco

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Using following code to convert the graph file to tflite:

import tensorflow as tf
graph_def_file = ""C:/tensorflow1/models/research/object_detection/inference_graph/tflite_graph.pb""
input_arrays = [""normalized_input_image_tensor""]
input_shaps = {""normalized_input_image_tensor"":[1, 300, 300, 3]}
output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']
#output_shaps = {""raw_outputs/box_encodings"":[1, 10, 4]}
converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shaps)
tflite_model = converter.convert()
open(""detact.tflite"", ""wb"").write(tflite_model)
"
31843,Attempt to build with NDK only in configure.py results in invalid android.bzl,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (docker)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.14.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip yes
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): Clang from NDK r17b
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**
I'm attempting to build tensorflow light using android NDK compiler, without plugging in SDK.
Therefore I set `TF_SET_ANDROID_WORKSPACE` to 0, while I have `ANDROID_NDK_HOME` pointing to location of valid NDK installation.

Attempting to perform such build results in error:
```
ERROR: /root/.cache/bazel/_bazel_root/9deba62a2f90185d207a955a930b77dc/external/local_config_android/android.bzl:10:2: indentation error
ERROR: error loading package '': Extension 'android.bzl' has errors
ERROR: error loading package '': Extension 'android.bzl' has errors
```

This happens because configure.py adds ndk location (specified in env variable) by appending to android.bzl that has only pass in body of function.
But it is added with 2 spaces, while rest of android.bzl was with 4 tabs.

This is the first problem, I have to manually remove `pass` and leave only NDK setup
Probably there should not be such difference in tabulation when generating `android.bzl`

The second issue, is whether it should be possible to build `//tensorflow/lite:libtensorflowlite.so` with NDK only.
I was able to do so after manually correcting `android.bzl`

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Environment variables:

```
PYTHON_BIN_PATH = <valid location python>
USE_DEFAULT_PYTHON_LIB_PATH = '1'
TF_ENABLE_XLA = '0'
TF_NEED_ROCM = '0'
TF_NEED_CUDA = '0'
TF_NEED_MPI = '0'
TF_DOWNLOAD_CLANG = '0'
TF_SET_ANDROID_WORKSPACE = '0'
TF_CONFIGURE_IOS = '0'
ANDROID_NDK_HOME = <valid location of NDK>
ANDROID_NDK_API_LEVEL = '18'
TF_NEED_OPENCL_SYCL = '1'
TF_NEED_COMPUTECPP = '0'
TRISYCL_INCLUDE_DIR = '<path to cloned repo of TrySYCL>/include'
HOST_CXX_COMPILER = '<NDK path>/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++'
HOST_C_COMPILER = '<NDK path>/toolchains/llvm/prebuilt/linux-x86_64/bin/clang'
CC_OPT_FLAGS = '-Wno-sign-compare'
```

- Run `python configure.py`
- Rim `bazel shutdown`
- Run build with `bazel build --config=opt --config=v2 --cxxopt='--std=c++11' --define=no_tensorflow_py_deps=true --config=android_arm64 //tensorflow/lite:libtensorflowlite.so --verbose_failures`

**Any other info / logs**
Manual correction of `android.bzl` results in successful build, but it seems there are no symbols related to GPU delegate (I guess a separate target is required?)
"
31842,Tensorflow2.0 Training OOM when tf.function retraces excessively,"<em>When I am training my model using **tf.function** CPU memory is leaking and after some steps of training it is getting killed.</em>

**System information**
- OS Platform: - Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0-beta
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.01
- GPU model and memory: p100, 16GB

```
@tf.function
def train_step(self, x, step, grad_clip=True, clip_value=1.0):
    with tf.name_scope(""input_data""):
        inputs = x[:, :-1]
        targets = x[:, 1:]

     with tf.GradientTape() as tape:
          predictions, _ = model(inputs, training=True)
          loss = self.get_loss(targets, predictions)

      with tf.name_scope(""gradients""):
          gradients = tape.gradient(loss, self.trainable_variables)
          if grad_clip:
              gradients = [(tf.clip_by_value(grad, -clip_value, clip_value))
                             for grad in gradients]
          self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))

        accuracy = self.get_padded_accuracy(targets, predictions)
        assert self.train_writer is not None
        with tf.name_scope(""summary_writer""):
            with self.train_writer.as_default():
                tf.summary.scalar(""loss"", loss, step=step)
                tf.summary.scalar(""accuracy"", accuracy, step=step)
       return loss, accuracy

**Stats of CPU memory
After 100 steps - 5.08 GB
After 500 steps -  14.8 GB
After 1000 steps - 27 GB**
```
        


**When is graph mode of training using tf.function CPU memory gets leaked and after some steps, it gets killed but when I train my model using eager mode it works fine, I am using above code for graph mode training as recommended by TensorFlow 2.0 community.**

"
31840,TFLite benchmark tool doesn't show complete Op profiling when using use_nnapi=true flag,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: All devices
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): Release 1.14.0
- Python version: 2.7.15+
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0

**Describe the current behavior**
I am testing the benchmark tool found on this link: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark
I am building and running in Android to benchmark MobileNet V1 model on some devices. I am interested in seeing the profiling information for each of the layers/operations and I am able to do that by adding the enable_op_profiling=true flag. However, I can't see the information when I also add the flag ""--use_nnapi=true"".

Example with NNAPI switched off:
![image](https://user-images.githubusercontent.com/8417115/63438560-2ab00400-c435-11e9-9c11-cb518270a54e.png)


Example with NNAPI switched on:
![image](https://user-images.githubusercontent.com/8417115/63438606-41eef180-c435-11e9-9299-08dee2ad6f9f.png)

**Describe the expected behavior**
I expect that using the --use_nnapi=true flag will also display op profiling information

**Code to reproduce the issue**
Just enable --enable_op_profiling=true and --use_nnapi=true flags when running the benchmark on any model. Is it not implemented yet?
"
31839,TF 1.14 RNN Variables does not appear in variables(),"Hi,

I have a model that inherits ``tf.keras.Model`` (M) that contains a custom multi-layered RNN that inherits ``tf.keras.layers.Layer`` (R). I use the eager mode, so I call the ``build`` function of M before I start training. Inside the ``build`` function of M there are 2 variables that gets initialized. After initializing those variables, M's ``build`` function also calls the ``build`` function of the RNN cells inside R.

The problem is, while I was using Tensorflow 1.13.1 all of the variables appeared in the ``M.variables`` list but after I've updated to Tensorflow 1.14 RNN variables are no longer in the list!

Am I missing something or is there a bug?

- I am using Python 3.6
- Code runs on CPU

Kind regards"
31838,Simple Audio Recognition sample couldn't be run in iOS swift - TFLite,"I have successfully run the simple audio recognition sample from the examples folder and successfully generated the tensorflow model output_graph.pb. Then i successfully converted the model to TFLite using the following code.
```
import tensorflow as tf

frozen_graph = '/tmp/graph.pb'

input_arrays = [""decoded_sample_data""]
output_arrays = [""labels_softmax""]
supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]

convertor = tf.lite.TFLiteConverter
convertor = convertor.from_frozen_graph(graph_def_file=frozen_graph,input_arrays=input_arrays,output_arrays=output_arrays)
convertor.target_ops = supported_ops

tflite_model = convertor.convert()
open(""converted.tflite"", ""wb"").write(tflite_model)
```

I have a sample iOS app and i am using the Firebase ML kit's MLModelInterpreter to run the model. I successfully complied and ran the project but at run time during the run i am get the error that ""Unsupported ops"". If the Decode_wave, Audiospectrogram and MFCC ops are not supported then how can the conversion be successfully? How can i run the tflite model in the iOS swift project successfully.

For iOS implementation refer- https://firebase.google.com/docs/ml-kit/ios/use-custom-models
OS - OSX - Mojave - 10.14.5
Tensorflow version - 1.14.0 stable 
Environment - Conda
Swift version - 4.2 and 5
Xcode - 10.2

[PB and Lite graphs.zip](https://github.com/tensorflow/tensorflow/files/3525438/PB.and.Lite.graphs.zip)
"
31837,KeyError: 'BatchMatMulV2',"KeyError: 'BatchMatMulV2'

train and save model in tf 1.14 environment,
but get  --- KeyError: 'BatchMatMulV2'  ---- in tf 1.13 environment,
how to solve this problem in tf 1.13 environment"
31836,[lite]  cc_library or cc_binary not including libs to build .so,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**so+.out rans ok in adb ,but DllNotFoundException in apk from unity.**
08-22 10:09:17.697 26322 26345 D Unity   : Unable to load library '/data/app/com.abc.cba-ZQdutbR1M-hRhdbPDZ25OA==/lib/arm/libProphet.so', native render plugin support disabled: java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""_ZTVN6tflite17MutableOpResolverE"" referenced by ""/data/app/com.abc.cba-ZQdutbR1M-hRhdbPDZ25OA==/lib/arm/libProphet.so""...


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):buid
cc_library(
    name = ""Prophet"",
    srcs = [
        ""Prophet.cc"",
        ""minimal.h"",
    ],
    linkopts = tflite_linkopts() + select({
        ""//tensorflow:android"": [
            ""-pie"",  # Android 5.0 and later supports only PIE
            ""-lm"",  # some builtin ops, e.g., tanh, need -lm
        ],
        ""//conditions:default"": [],
    }),
    deps = [
        ""//tensorflow/lite:framework"",
        ""//tensorflow/lite/c:c_api_internal"",
        ""//tensorflow/lite/kernels:builtin_ops"",
    ],
    linkstatic = False,
)

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu18x64->win10x64 unity-
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:arm7
- TensorFlow installed from (source or binary):source 
- TensorFlow version (use command below):14
- Python version: 3.7
- Bazel version (if compiling from source):0.26
- GCC/Compiler version (if compiling from source):7.4
- CUDA/cuDNN version: none
- GPU model and memory: none

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
execute libc++_shared.so+example.so+example.out ok!
execute unity17+libc.so+libdl.so+ibc++_shared.so+example.so->apkcannot find dll.
c++filt shows vtable for tflite::MutableOpResolver
**Describe the expected behavior**
apk ok!
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 bazel build --config monolithic --cxxopt=-std=c++11 \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  --cpu=armeabi-v7a  --verbose_failures\
  //tensorflow/lite/examples/l17:example(so)
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

`
08-21 21:11:24.373 16862 16884 D Unity   : UnloadTime: 6.400000 ms
08-21 21:11:24.381 16862 16884 D Unity   : UUID: a9c3ed14008066f6 => d958add90437df1f9b9cbdba44b84779
08-21 21:11:24.487 16862 16884 D Unity   : Sensor :        Accelerometer ( 1) ; 0.002396 / 0.00s ; ICM20690 Accelerometer / InvenSense 
08-21 21:11:24.512 16862 16884 D Unity   : Choreographer available: Enabling VSYNC timing
08-21 21:11:24.520 16862 16884 D Unity   : Unable to load library '/data/app/com.abc.cba-gPQU27o5p_2Y5PClNwV8OQ==/lib/arm/libProphet.so', native render plugin support disabled: java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""_ZTVN6tflite17MutableOpResolverE"" referenced by ""/data/app/com.abc.cba-gPQU27o5p_2Y5PClNwV8OQ==/lib/arm/libProphet.so""...
08-21 21:11:24.522 16862 16884 E Unity   : Unable to find Prophet
08-21 21:11:24.633 16862 16884 E Unity   : DllNotFoundException: Prophet
08-21 21:11:24.633 16862 16884 E Unity   :   at (wrapper managed-to-native) rlsdk.RLSDKAPI:init (byte[],int)
08-21 21:11:24.633 16862 16884 E Unity   :   at helloworld.Start () [0x00017] in D:\tflite\0821\L17online\Assets\helloworld.cs:18 
08-21 21:11:24.633 16862 16884 E Unity   :  
08-21 21:11:24.633 16862 16884 E Unity   : (Filename: D Line: 0)
`"
31835,"Don't know what to do next,help me please!","**System information**
- OS Platform and Distribution:windows10
- TensorFlow installed from (source or binary):
git clone https://github.com/tensorflow/tensorflow.git
- TensorFlow version:r1.13
- Python version:3.6
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):0.21.0
- GCC/Compiler version (if compiling from source):9.1.0
- CUDA/cuDNN version:not using
- GPU model and memory:not using



**Describe the problem**
I want to build a dll for c++ ,and build is successful.But I couldn't find .h file anywhere,so I try to run sh build_all_linux.sh in msys as the tutorial said.But it doesn't work at all,and there is noone ask for this. 
Then I also try to build by cmake,it doesn't work too.And the only anser is ""We encourage you to use Bazel instead of Cmake as we no longer support cmake for Windows.
Also please refer this link for more information on Building Tensorflow on Windows.""
I'm so lost. Can you provid some completely tutorial for people like me.(I have to make a demo with windows)
**Provide the exact sequence of commands / steps that you executed before running into the problem**

**bazel head file problem:**
$ sh build_all_linux.sh
PROTOC = ""protoc""
CC_PREFIX = """"
rm -rf /c/programing/tensorflow-r1.13/tensorflow/contrib/makefile/gen
rm -rf tensorflow/core/util/version_info.cc
downloading https://bitbucket.org/eigen/eigen/get/9f48e814419e.tar.gz
tar.exe: Error opening archive: Failed to open '\\.\tape0'

**cmake problem:**
CMake Error at tf_core_framework.cmake:332 (add_library):
Cannot find source file:

E:/work_space/projectcpp/tensorflow/tensorflow/tensorflow/core/util/version_info.cc

Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
       .hpp .hxx .in .txx
      Call Stack (most recent call first):
       CMakeLists.txt:587 (include)
"
31832,[TF 1.14] [TPU] Errors while training LSTM model on Colab TPU,"I worked a bit with TF2.0 but new to TF1.x working. I want o use Google TPUs and for that I am making a LSTM model in TF1.14 without eager execution. I am reusing code from one of the Tensorflow [tutorials](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches). Below is the code I am executing in Colab while setting the runtime environment to **Python 2** and **TPU**.
On running it in colab, I am getting error `KeyError: u'flat_filenames'`
and if I run it again (without restarting runtime) it throws following error `InvalidArgumentError: Graph is invalid, contains a cycle with 1 nodes, including: bidirectional/while/Merge, bidirectional/while/Merge_1, bidirectional/while/Merge_2`
THANK YOU!

````
import tensorflow as tf
import tensorflow_datasets as tfds
import os
import pprint
import tensorflow as tf
import time
import numpy as np
from tensorflow import keras

TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']

DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'
FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']

for name in FILE_NAMES:
  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)
  
parent_dir = os.path.dirname(text_dir)

print (parent_dir)
def labeler(example, index):
  return example, tf.cast(index, tf.int32)  

labeled_data_sets = []

for i, file_name in enumerate(FILE_NAMES):
  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))
  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))
  labeled_data_sets.append(labeled_dataset)

BUFFER_SIZE = 50000
BATCH_SIZE = 64
TAKE_SIZE = 5000
all_labeled_data = labeled_data_sets[0]
for labeled_dataset in labeled_data_sets[1:]:
  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)
  
all_labeled_data = all_labeled_data.shuffle(
    BUFFER_SIZE, reshuffle_each_iteration=False)
print type(all_labeled_data)
tokenizer = tfds.features.text.Tokenizer()

vocabulary_set = set()
my_iterator = all_labeled_data.make_initializable_iterator()
text_tensor, _ = my_iterator.get_next()
with tf.Session() as sess1: 
  sess1.run(my_iterator.initializer)
  try:
    while True:
      text_string = sess1.run(text_tensor)
      #print text_string
      some_tokens = tokenizer.tokenize(text_string)
      vocabulary_set.update(some_tokens)
  except tf.errors.OutOfRangeError:
    pass
vocab_size = len(vocabulary_set)
print 'Length of Vocab',len(vocabulary_set)
print 'Done Tokenizing!'

encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)
with tf.Session() as sess2:
  sess2.run(my_iterator.initializer)
  try:
    while True:
      text_string = sess2.run(text_tensor)
      encoded_example = encoder.encode(text_string)
      #print encoded_example
  except tf.errors.OutOfRangeError:
    pass

def encoder_fn(text_tensor, label):
  #print type(text_tensor)
  #print text_tensor
  encoded_text = encoder.encode(text_tensor)
  padded_text = encoded_text[:4]# limiting the length to 4
  padded_text = np.asarray(padded_text, dtype=np.int32)# chnaging type for tensor type compatibility with TF
  return padded_text, label

def encode_map_fn(text, label):
  return tf.py_func(encoder_fn, inp = [text, tf.reshape(label, [])], Tout = [tf.int32, tf.int32])

all_encoded_data= all_labeled_data.map(encode_map_fn)
train_data = all_encoded_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder= True)
my_iterator1 = train_data.make_initializable_iterator()
text_tensor, lab = my_iterator1.get_next()
with tf.Session() as sess2:
  sess2.run(my_iterator1.initializer)
  text_string = sess2.run(text_tensor)
  lab = sess2.run(lab)
  print text_string
  print lab
print 'Printing an example data done!.'

all_encoded_data = all_encoded_data.repeat()
train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)
train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder= True)
#train_data= train_data.batch(64)
train_data = train_data.prefetch(BATCH_SIZE)
test_data = all_encoded_data.take(TAKE_SIZE)
test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder = True)
test_data = test_data.prefetch(BATCH_SIZE)



vocab_size += 1

tf.keras.backend.clear_session()
resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)
tf.contrib.distribute.initialize_tpu_system(resolver)
strategy = tf.contrib.distribute.TPUStrategy(resolver)

with strategy.scope():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Embedding(vocab_size, 64))
  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation= 'tanh', recurrent_activation= 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias= True)))
  # One or more dense layers.
  # Edit the list in the `for` line to experiment with layer sizes.
  for units in [64, 64]:
    model.add(tf.keras.layers.Dense(units, activation='relu'))

  # Output layer. The first argument is the number of labels.
  model.add(tf.keras.layers.Dense(3, activation='softmax'))
  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
###Strategy  end###
steps = 50000/64
print model.summary()
print (steps)

model.fit(train_data, steps_per_epoch = steps,epochs=10)
````
"
31831,Missing `person_detect.tflite` file,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 19.2 Tina
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 80c04b80ad66bf95aa3f41d72a6bba5e84a99622

Files `person_detect_model_data.h/.cc` in `downloads` directory (downloaded from [here](https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale.zip)) mention `person_detect.tflite` file:

```c
// Automatically created from a TensorFlow Lite flatbuffer using a command like:
// xxd -i person_detect.tflite > person_detect_model_data.cc
```

Is this file available somewhere?
Is the full TF model available?
"
31830,fit_generator: number of calls to the validation generator does not match validation_steps,"Installed: pip install tf-nightly-2.0-preview 
in a clean environment. Same bug was in tf 1.14 and 2.0-beta1

The validation generator is called more often than validation_steps in tensorflow.keras.Model.fit_generator(). If one adds workers=0 as a call parameter everything is correct.
I tracked down the issue to starting the threads (parameter max_queue_size changes the number of calls before the calls seem to be used for validation).

Reproduce the bug:
```
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
import numpy as np

train_epoch_size = 10
test_epoch_size = 20

class KerasBatchGenerator(object):
    def generate(self, phase='train'):
        while True:
            if phase == 'train':
                for i in range(train_epoch_size):
                    yield [np.array([0])],[np.array([0])]
            else:
                for i in range(test_epoch_size):
                    print('   i',i)
                    yield [np.array([0])],[np.array([0])]

keras_gen_train = KerasBatchGenerator()

inputs = Input(shape=(1,))
x = Dense(1)(inputs)

model = Model(inputs=inputs, outputs=x)

model.compile(loss='mean_squared_error', optimizer='SGD')
print(model.summary())

model.fit_generator(keras_gen_train.generate(), train_epoch_size, 2,
                           validation_data=keras_gen_train.generate('test'), validation_steps=test_epoch_size)#, workers = 0)
```

the output is
 ```
1/10 [==>...........................] - ETA: 1s - loss: 0.0000e+00   i 0
   i 1
   i 2
   i 3
   i 4
   i 5
   i 6
   i 7
   i 8
   i 9
   i 10
   i 11
   i 12
   i 13
   i 14
   i 15
   i 16
   i 17
   i 18
   i 19
   i 0
   i 1
   i 2
   i 3
   i 4
   i 5
   i 6
   i 7
   i 8
   i 9
   i 10
10/10 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00
```

There are 31 calls to the validation generator (validation_steps + max_queue_size + 1). This might usually be no problem, but it is, if you have to control which validation data match which training data. I use it for transfer learning.

if you uncomment: ,workers=0) you get the expected result:
 ```
1/10 [==>...........................] - ETA: 1s - loss: 0.0000e+00   i 0
   i 1
   i 2
   i 3
   i 4
   i 5
   i 6
   i 7
   i 8
   i 9
   i 10
   i 11
   i 12
   i 13
   i 14
   i 15
   i 16
   i 17
   i 18
   i 19
10/10 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00
```

I tracked down the issue to starting the threads in training_generator.py: enqueuer.start
```
def _make_enqueued_generator(generator,
                             workers=1,
                             use_multiprocessing=False,
                             max_queue_size=10,
                             shuffle=False):
  """"""Create a buffered queue of next elements of the generator.""""""
  is_sequence = isinstance(generator, data_utils.Sequence)
  enqueuer = None
  if workers > 0:
    if is_sequence:
      enqueuer = data_utils.OrderedEnqueuer(
          generator, use_multiprocessing=use_multiprocessing, shuffle=shuffle)
    else:
      enqueuer = data_utils.GeneratorEnqueuer(
          generator, use_multiprocessing=use_multiprocessing)
    enqueuer.start(workers=workers, max_queue_size=max_queue_size) #here the additional calls happen!
    output_generator = enqueuer.get()
  else:
    if is_sequence:
      output_generator = data_utils.iter_sequence_infinite(generator)
    else:
      output_generator = generator
  return output_generator, enqueuer

```
but I am too unfamiliar with python threading to supply a patch, sorry. 

At the moment the workaround is workers=0! "
31829,Using libtensorflow-core.a to enable training on Android ,"I followed : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/README.md

to build the tensorflow library for Android. Does the lib file contain all the functionalities to enable training on Android by using c++ to call the library. 

"
31827,TfLite is going die with serious and ridiculous bug for a long time ...,https://github.com/tensorflow/tensorflow/issues/31359
31826, TensorFlow Lite GPU on Android slower than CPU,"**System information**
Android 9.0, XiaoMi9, snapdragon855
- TensorFlow installed from (source or binary):
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo
- TensorFlow version (or github SHA if from source):
dependencies {
    ...
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'
}

I run a app from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo, and it worked well, the run time of  default model(mobilenet) on GPU is half of CPU, which is normal I think. And I replace the model with a special model which structure like YOLOv2-Tiny, trained by keras and convert to tflite(Implemented by traditional conv2D and pooling), I found the speed on GPU is slower than CPU float(CPU is less than 500ms but GPU is more than 600ms),  is it normal?
is there anyone help me?
"
31825,KeyError while loading image data,"Error:
File ""DataCollection.py"", line 38, in caption_image
return ""Image (CC BY 2.0) "" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])
KeyError: 'sunflowers\6166888942_7058198713_m.jpg'

Function:
def caption_image(image_path):
image_rel = pathlib.Path(image_path).relative_to(data_root)
return ""Image (CC BY 2.0) "" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])

It gives a KeyError while returning the parameter.
And every time it gives a KeyError with different image name and path.

Any idea on this?"
31824,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 64x  
- TensorFlow version: 1.14.0
- Python version: 3.7.3
- Installed using : pip


Tensoreflow import error 



Traceback (most recent call last):
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Vikas rathod\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

"
31823,AutoGraph Map Infinite Loop,"**System information**
I used google colab. [Here's the link to reproduce](https://colab.research.google.com/drive/13g1AMmLsCK3dcmuUdYiDhyv9PqajFRlc)

Given:
```
import tensorflow as tf
from tensorflow.data import Dataset
tf = tf.compat.v2
tf.enable_v2_behavior()

@tf.function
def test1(ds):
  for val in ds.map(lambda x: x+1):
    print(val)
    
@tf.function
def test2(ds):
  for val in map(lambda x: x+1, ds):
    print(val)

ds = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])
```
**Describe the current behavior**
```
print('start')
test1(ds) # NOT HANG
print('end')

print('start')
test2(ds) # HANG
print('end')
```

**Describe the expected behavior**
It should not hang.

**Other info / logs**
I've been thinking about a fix much like [overriding python built-in zip function](https://github.com/tensorflow/tensorflow/pull/31290). The problem is, [tf.data.Dataset's map only applies to one dataset](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#map) while python built-in map function can accept more than 1 data. For example:
```
numbers1 = [1, 2, 3] 
numbers2 = [4, 5, 6] 
  
result = map(lambda x, y: x + y, numbers1, numbers2) 
print(list(result)) 
```

What's the solution? 
Please correct me if I'm wrong or I'm missing something. Thank you."
31821,"EmptyTensorList, RandomUniform, TensorListFromTensor, TensorListReserve, TensorListStack, While ops have not be supported","**System information**
- Linux Ubuntu 16.04:
- TensorFlow installed from anaconda:
- TensorFlow version 1.14.0:

hi,it seems that EmptyTensorList, RandomUniform, TensorListFromTensor, TensorListReserve, TensorListStack, While ops have not be supported .

output from tflite_convert:

Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, FULLY_CONNECTED, GATHER, GREATER_EQUAL, MAX_POOL_2D, MUL, RESHAPE, REVERSE_V2, SHAPE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: EmptyTensorList, RandomUniform, TensorListFromTensor, TensorListReserve, TensorListStack, While.

```
import os
os.environ[""TF_ENABLE_CONTROL_FLOW_V2""] = ""1""
import tensorflow as tf


graph_def_file = ""./vgg/frozen_graph_meta2.pb""
input_arrays = [""images_pre""]#image_widths_pre
output_arrays = [""fully_connected/transpose_time_major""]

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
#converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```



"
31820,tflite output different result with pbfile when using only one convolutional layer ?,https://github.com/tensorflow/tensorflow/issues/31359
31819,keras.model.fit does not work with SparseTensor inputs with functional API,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No

- TensorFlow installed from (source or binary):
Binary

- TensorFlow version (use command below):
1.14.0 

**Describe the current behavior**

The program exits with the following error: 

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'sparse_tensor/indices' with dtype int64 and shape [?,2]
	 [[{{node sparse_tensor/indices}}]]

```

**Describe the expected behavior**

No error occurs and training continues.

**Code to reproduce the issue**

```
import numpy as np
import tensorflow.compat.v2 as tf

def dummy_parse_fn(iterable):
  features = {}
  # the input is always constant
  features['sparse_tensor'] = tf.SparseTensor(
    indices=tf.constant([[0,0],[1,1]], dtype=tf.int64),
    values=tf.constant([1.0, 1.0], dtype=tf.float32),
    dense_shape=tf.constant([2, 2], dtype=tf.int64))
  features['sparse_tensor/indices'] = features['sparse_tensor'].indices
  features['sparse_tensor/values'] = features['sparse_tensor'].values
  features['sparse_tensor/dense_shape'] = features['sparse_tensor'].dense_shape
  labels = tf.constant([1.0, 1.0], dtype=tf.float32)
  return features, labels


def get_dummy_dataset():
  iterable =  np.random.random((128, 1)).astype(np.float32)
  return (
    tf.data.Dataset
    .from_tensor_slices(iterable)
    .map(dummy_parse_fn)
    .take(1024)
  )


if __name__ == ""__main__"":

  print(tf.__version__)

  inputs = tf.keras.layers.Input(shape=(2, ), sparse=True, name=""sparse_tensor"")
  weights = tf.Variable(name='weights', shape=(2, 1), initial_value=[[1.0], [1.0]])
  output = tf.sparse.sparse_dense_matmul(inputs, weights)
  model = tf.keras.Model([inputs], output)

  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)
  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)

  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
  model.fit(get_dummy_dataset(), epochs=2)
```
"
31817,"The tensorflow code on jupyter notebook can run successfully, but there is a lot of warning information","code:
import tensorflow as tf
a = 2
b = 3
c = tf.add(a, b, name='Add')
print(c)

**Warning information**
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])


G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])




What should I do to get rid of the warning message?
"
31816,Processing batches with different sequence lengths using stacked LSTM layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.4
- TensorFlow installed from (source or binary):  Source
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.65

**Describe the current behavior**

An exception is raised when trying to stack multiple `tf.keras.layers.LSTM`, while the sequence length changes across batches. This behavior occurs if the `tf.keras.Model` is built with model subclassing. On the other hand, if the model is built using the functional API, everything works as intended.

**Describe the expected behavior**

Because of the identical implementations, besides the difference in the way the model is built (subclassing and functional API), I would expect the results to be the same. In other words, I am confused why an exception is raised at all if using model subclassing.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

def train_generator():
    while True:
        sequence_length = np.random.randint(10, 100)
        x_train = np.random.random((1000, sequence_length, 5))
        y_train = np.random.random((1000, sequence_length, 2))

        yield x_train, y_train

# Works as intended
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(None, 5)))
model.add(tf.keras.layers.LSTM(8, return_sequences=True))
model.add(tf.keras.layers.Dense(2))
model.compile(optimizer=""adam"", loss=""mse"")
model.fit_generator(train_generator(), steps_per_epoch=2, epochs=2, verbose=1)

# Throws an exception
class LSTMModel(tf.keras.Model):
    def __init__(self):
        super(LSTMModel, self).__init__()
        self._lstm_0 = tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(None, 5)) 
        self._lstm_1 = tf.keras.layers.LSTM(8, return_sequences=True)
        self._dense = tf.keras.layers.Dense(2)

    def call(self, inputs, training=False):
        output = self._lstm_0(inputs)
        output = self._lstm_1(output)
        output = self._dense(output)

        return output

model = LSTMModel()
model.compile(optimizer=""adam"", loss=""mse"")
model.fit_generator(train_generator(), steps_per_epoch=2, epochs=2, verbose=1)
```

**Other info / logs**
>InvalidArgumentError:  [_Derived_]  Operation expected a list with 58 elements but got a list with 88 elements.
	 [[{{node gradients/TensorArrayUnstack/TensorListFromTensor_grad/TensorListStack}}]]
	 [[Adam/gradients_24/lstm_model_22/lstm_56/StatefulPartitionedCall_grad/StatefulPartitionedCall]] [Op:__inference_keras_scratch_graph_75269]"
31814,How can I convert image to arrays using Java code?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: Samsung A3 2016 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0/7.4
- GPU model and memory: Nvidia GeForce 840m 3 Go 


**Describe the current behavior**
I'm trying to set up an MLKit detection project on Android using Tensorflow library, I have got false values on the output using Tensorflow lite ( different values than Frozen model inference ).

I doubt that the problem is with the input ( image ), So I want to compare the two same images that I have.

To do this task, I have used PIL image and numpy libraries on python to get bytes arrays, and I have converted the drawable image to bitmap and from bitmap to bytes arrays.

I don't know if the function np.asarray should give the same value as those two functions below:


**Code to reproduce the issue**

+ **Java code:**

```java
private float[][][][] bitmapToInputArray() {
    // [START mlkit_bitmap_input]
    Bitmap bitmap= getYourInputImage();
    int batchNum = 0;
    float[][][][] input = new float[1][112][112][3];
    for (int x = 0; x < 112; x++) {
        for (int y = 0; y < 112; y++) {
            int pixel = bitmap.getPixel(x, y);
            // Normalize channel values to [-1.0, 1.0]. This requirement varies by
            // model. For example, some models might require values to be normalized
            // to the range [0.0, 1.0] instead.
            input[batchNum][x][y][0] = (Color.red(pixel))/ 255.0f;
            input[batchNum][x][y][1] = (Color.green(pixel)) / 255.0f;
            input[batchNum][x][y][2] = (Color.blue(pixel))/ 255.0f;
            Log.i(""Input"",""input""+input[batchNum][x][y][0]);
            Log.i(""input"",""input""+input[batchNum][x][y][1]);

        }
    }
    // [END mlkit_bitmap_input]

    return input;
}
public byte[] convertBitmapToByteArray(Bitmap bitmap) {
    ByteArrayOutputStream stream = null;
    try {
        stream = new ByteArrayOutputStream();
        bitmap.compress(Bitmap.CompressFormat.JPEG, 100, stream);

        return stream.toByteArray();
    }finally {
        if (stream != null) {
            try {
                stream.close();
            } catch (IOException e) {
                Log.e(ThemedSpinnerAdapter.Helper.class.getSimpleName(), ""ByteArrayOutputStream was not closed"");
            }
        }
    }
}
private Bitmap getYourInputImage() {
    // This method is just for show
    BitmapDrawable drawable = (BitmapDrawable) image2.getDrawable();
    Bitmap bitmap = drawable.getBitmap();
    Bitmap bitmapp=Bitmap.createScaledBitmap(bitmap,112,112,true);
    Bitmap bitmap2= bitmapp.copy(Bitmap.Config.ARGB_8888, true);
    return bitmap2;
}

    byte[] bytes=convertBitmapToByteArray(bitmap1);
    Log.i(""byte"",""""+ Arrays.toString(bytes));
    float[][][][] inp = new float[1][112][112][3];
    inp=bitmapToInputArray();
    Log.i(""byte2"",""""+Arrays.toString(inp[0]));
```

+ **Python code:**

```python
img = Image.open(""irisdata-300VW_Dataset_2015_12_14-017-000880.jpg"")
img.load()
img = img.resize((112, 112), PIL.Image.ANTIALIAS)
print(str(image_to_byte_array(img)))


# Normalize to [0, 1]
data = np.asarray( img, dtype=""float32"")
print(data)
```
**Other info / logs**

+ Java output:

>2019-08-20 19:01:13.589 1513-1513/com.example.irisdetection I/byte: [-1, -40, -1, -32, 0, 16, 74, 70, 73, 70, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, -1, -37, 0, 67, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -37, 0, 67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -64, 0, 17, 8, 0, 112, 0, 112, 3, 1, 34, 0, 2, 17, 1, 3, 17, 1, -1, -60, 0, 29, 0, 0, 2, 2, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 6, 9, 4, 5, 10, 3, 2, 1, -1, -60, 0, 69, 16, 0, 1, 3, 3, 1, 5, 5, 4, 7, 5, 6, 5, 5, 0, 0, 0, 3, 1, 2, 4, 5, 6, 17, 33, 0, 7, 18, 19, 49, 8, 34, 65, 81, 97, 20, 35, 113, -127, 9, 21, 50, 51, -111, -95, -79, 36, 66, -63, -47, -16, 22, 23, 67, 82, 83, -31, 37, 52, 98, 114, -15, 99, 115, -126, -125, -94, -1, -60, 0, 28, 1, 0, 2, 2, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 7, 4, 5, 2, 3, 8, 0, 1, -1, -60, 0, 55, 17, 0, 0, 4, 3, 6, 3, 8, 1, 3, 4, 3, 1, 0, 0, 0, 0, 1, 2, 3, 4, 5, 17, 33, 0, 6, 49, 65, 81, 97, 18, 19, 113, 7, 20, 35, -127, -111, -95, -79, -16, -47, 34, -63, -31, 21, 36, 67, -15, 8, 22, 50, 51, -1, -38, 0, 12, 3, 1, 0, 2, 17, 3, 17, 0, 63, 0, -28, -105, 115, 116, 26, 45, 98, -97, 117, 74, -71, -31, -57, -88, 40, -73, 93, 88, -83, 83, 89, 39, 41, -20, -43, 40, -7, 28, 115, 12, -100, -52, -116, -88, 78, 53, -45, 84, -49, -53, 102, 114, 124, 34, -52, -20, -3, -72, 91, -115, 6, -58, -120, -76, -5, -102, -126, 71, 36, -107, 43, -97, -11, 61, 93, -17, 78, 96, 63, 115, -35, -52, 8, -3, -30, -89, -40, -26, 117, -40, 29, -70, 107, 106, 21, 112, 86, -36, 7, 73, 36, 120, -9, 30, -19, -18, -48, -56, 86, -79, -88, -31, -106, -105, 36, -53, -53, 103, 30, -92, 121, 24, -120, 76, 47, -7, -15, -99, 54, 103, 44, -88, -107, 42, -57, 98, 107, 90, 83, -98, 105, 20, -21, 43, 126, 53, -117, 106, 58, -71, -116, 84, -114, 58, -11, 22, 53, 77, -84, 119, 47, -34, -14, -52, 72, 102, 127, -68, -57, -61, 77, -78, 126, -35, 18, -79, 112, 9, 38, 41, 40, 50, 3, 104, 63, -7, -64, 66, 97, -100, -66, -115, -115, 59, 43, -119, 21, 43, -50, -111, 79, 62, 82, -55, -72, 64, 70, -76, -92, -6, 5, 119, -54, -42, 15, -40, -126, -36, -81, 86, 109, 8, 55, 69, -62, 114, 31, -39, -30, 125, 65, 65, 9, 120, -47, -111, -24, -48, 36, -55, 56, -58, 49, -12, 103, 52, -14, 77, -24, -68, 12, -41, 24, -38, -49, 45, -6, 122, -80, -104, -106, -88, 32, -76, 92, 72, -67, 51, -13, 95, 92, -2, 105, -90, 112, -85, -1, 0, 101, -21, 110, -105, 72, -35, 117, -93, 21, -22, -48, 56, 84, 56, 50, 9, -62, -33, -74, 105, 35, -25, -99, 126, 42, 71, -89, -126, -86, -2, 91, 77, 119, -59, -67, 56, 86, 101, 29, -50, -116, -41, 28, -83, 78, 1, 2, 59, 85, 78, 103, -82, -120, -120, -119, -16, 84, -8, -81, -82, -43, -119, 2, 73, -74, -32, 34, 124, -95, -91, 103, -120, -56, 48, -108, -3, 55, 10, -128, 72, 44, 54, -11, 68, -43, -117, 63, 85, 41, 75, -67, -81, -115, 104, 11, -128, 98, 30, 84, -99, -90, 117, -103, -47, 13, 32, -111, 99, 59, 8, -30, 120, -85, 59, -38, -8, 42, 116, -50, -67, 51, -25, -116, 109, -17, 6, -128, 85, 123, 72, 34, 46, 60, 28, -72, -8, 39, 69, -49, -97, -13, -38, -77, -28, -17, 107, 125, -75, -103, -113, -97, 108, 110, -34, -19, -88, 0, 69, -53, 80, 84, -87, 74, -114, 69, 34, -82, -124, 81, -94, 97, 124, -4, -70, 109, 40, -94, 118, -112, -19, 67, 109, 76, -114, -54, -2, -25, 36, -118, -104, -30, 35, -122, -39, -111, -90, 10, 79, 39, 61, 73, -53, 69, -41, 25, 93, 116, -12, -38, -87, 70, -4, -59, 84, -83, 100, 30, 67, 47, -65, 103, 51, 120, 107, -94, 36, 68, -124, -63, -31, -56, 39, 44, -58, -127, -112, -53, 110, -125, -106, 118, 97, 30, -117, 46, 58, 100, -68, 46, 110, 123, -51, -41, -89, 84, -4, -4, -68, 126, 91, 123, -44, -30, 3, -39, -107, 80, 92, 92, 89, -18, -89, -34, 105, -29, -4, 83, -11, -38, 47, -70, -19, -17, 10, -4, -89, 8, -43, 90, 49, 40, -14, -36, 1, -72, -111, -56, -113, 86, 101, 90, -103, 31, 125, 62, -13, -29, -32, -102, -90, -60, -55, -43, 90, 28, 126, 23, 56, 106, -118, -115, -30, 114, 47, -106, 5 2019-08-20 19:01:13.590 1513-1513/com.example.irisdetection I/byte2: [[[F@8bc81c1, [[F@4123d66, [[F@d5d68a7, [[F@ed2a154, [[F@7ce78fd, [[F@c74c9f2, [[F@66de843, [[F@627ec0, [[F@50ea7f9, [[F@201933e, [[F@57cc59f, [[F@250c6ec, [[F@f88cab5, [[F@7ffa54a, [[F@d721cbb, [[F@1f965d8,

+ Python output:

![](https://i.stack.imgur.com/785vr.png)

How can I get the same output of the python code with Java code using bitmap as an input?


"
31813,please add more activation functions,"**System information**
- TensorFlow version (you are using): 2.0b1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
activation functions are easy to add and quite handy

**Will this change the current api? How?**
something like this:
```
import tensorflow as tf

K = tf.keras
B, L = K.backend, K.layers

RRELU_MIN, RRELU_MAX = 0.123, 0.314
HARD_MIN, HARD_MAX = -1., 1.
SOFT_ARGMAX_BETA = 1e10

fn_string = [""swish"", ""gaussian"", ""sin"", ""cos"", ""hard_tanh"", ""lisht"", 'rrelu'] # etc etc
activators = [L.Activation(fn) for fn_string in fn_strings]  

def clean_activation(activation):
    if callable(activation):
        return activation
    elif activation == 'soft_argmax':
        fn = soft_argmax
    elif activation == 'gaussian':
        fn = gaussian
    elif activation == 'swish':
        fn = swish
    elif activation == 'lisht':
        fn = lisht
    elif activation == 'sin':
        fn = tf.math.sin
    elif activation == 'cos':
        fn = tf.math.cos
    else:
        fn = activation
    return fn


def swish(x):
    """"""
    Searching for Activation Functions
    https://arxiv.org/abs/1710.05941
    """"""
    return (B.sigmoid(x) * x)


def soft_argmax(x, beta=SOFT_ARGMAX_BETA):
    """"""
    https://stackoverflow.com/questions/46926809/getting-around-tf-argmax-which-is-not-differentiable
    https://lucehe.github.io/differentiable-argmax/
    """"""
    x_range = tf.range(x.shape.as_list()[-1], dtype=x.dtype)
    return tf.math.reduce_sum(
        tf.nn.softmax(x * beta) * x_range, axis=-1)


def gaussian(x):
    return B.exp(-B.pow(x, 2))


def hard_tanh(x, min=HARD_MIN, max=HARD_MAX):
    if x > max:
        return max
    elif x < min:
        return min
    else:
        return x


def lisht(x):
    """"""
    LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent
    https://github.com/swalpa/LiSHT
    """"""
    return (B.tanh(x) * x)


def rrelu(x, min=RRELU_MIN, max=RRELU_MAX):
    return x if x >= 0 else tf.random.uniform(min, max) * x


def tanhshrink(x):
    return x - B.tanh(x)


def hardshrink(x, min=HARD_MIN, max=HARD_MAX):
    if x > max:
        return x
    elif x < min:
        return min
    else:
        return 0
```

**Who will benefit with this feature?**
anybody who likes to play around
it would also be cool if we could set activations on a per-neuron basis instead of a per-layer basis. or per-channel, potentially... shrug!

**Any Other info.**
might be better suited for addons... cc @seanpmorgan "
31808,Import Tensorflow Could not find 'cudart64_100.dll' - Model repository incompatible with CUDA 10.1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- TensorFlow installed from (source or binary): piwheels
- TensorFlow version: 1.14
- Python version: 3.7
- Installed using virtualenv? pip? conda?: Conda
- CUDA/cuDNN version: CUDA 10.1 / cuDNN 10.1
- GPU model and memory: Nvidia P3200


>  80               ""environment variable. Download and install CUDA %s from ""
>      81               ""this URL: https://developer.nvidia.com/cuda-90-download-archive""
> ---> 82               % (build_info.cudart_dll_name, build_info.cuda_version_number))
>      83 
>      84       if hasattr(build_info, ""cudnn_dll_name"") and hasattr(
> 
> ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive
> **Provide the exact sequence of commands / steps that you executed before running into the problem**

It seems like  C:\...\.conda\envs\tensorflow1\Lib\site-packages\tensorflow\python\platform\self_check.py runs with "".cuda_version_number"" that is locked to CUDA 10.0 looking for cudart64_100.dll when the latest CUDA version is 10.1. The Tensorflow github model repository (https://github.com/tensorflow/models) is still looking for CUDA version 10.0's  cudart64_100.dll while the CUDA 10.1 has  cudart64_101.dll. Because of this code below, Import Tensorflow will fail the self check each time.

```
        try:
          ctypes.WinDLL(build_info.cudart_dll_name)
        except OSError:
          raise ImportError(
              ""Could not find %r. TensorFlow requires that this DLL be ""
              ""installed in a directory that is named in your %%PATH%% ""
              ""environment variable. Download and install CUDA %s from ""
              ""this URL: https://developer.nvidia.com/cuda-90-download-archive""
              % (build_info.cudart_dll_name, build_info.cuda_version_number))
```

"
31807,tf.load_op_library unable to load manylinux2010 repaired custom ops,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No -- using https://github.com/tensorflow/custom-op (But it breaks for addons too)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu16.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tf-nightly & tf-nighty-2.0-preview

**Describe the current behavior**
Currently when I build a custom op in the `tensorflow/tensorflow:custom-op-ubuntu16` docker image using the defined steps I get an install-able pip package `tensorflow_zero_out-0.0.1-cp27-cp27mu-linux_x86_64.whl` 

This works fine, however if I repair that wheel to be manylinux2010 compliant, then `tf.load_op_library` will fail to find the custom-op.
```
python -c ""import tensorflow as tf; print(dir(tf.load_op_library('manylinux/tensorflow_zero_out/python/ops/_zero_out_ops.so')))""

['LIB_HANDLE', 'OP_LIST', 'ZeroOut', '_IS_TENSORFLOW_PLUGIN', 
'_InitOpDefLibrary', '__builtins__', '__doc__', '__name__', '__package__', 
'_collections', '_common_shapes', '_context', '_core', '_dispatch', '_doc_controls', 
'_dtypes', '_errors', '_execute', '_kwarg_only', '_op_def_lib', '_op_def_library', 
'_op_def_pb2', '_op_def_registry', '_ops', '_pywrap_tensorflow', '_six', 
'_tensor_shape', 'deprecated_endpoints', 'tf_export', 'zero_out',
 'zero_out_eager_fallback']
```

```
python -c ""import tensorflow as tf;print(dir(tf.load_op_library('manylinux2010/tensorflow_zero_out/python/ops/_zero_out_ops.so')))""

['LIB_HANDLE', 'OP_LIST', '_IS_TENSORFLOW_PLUGIN', 
'_InitOpDefLibrary', '__builtins__', '__doc__', '__name__', '__package__', 
'_collections', '_common_shapes', '_context', '_core', 
'_dispatch', '_doc_controls', '_dtypes', '_errors', '_execute', '_kwarg_only', 
'_op_def_lib', '_op_def_library', '_op_def_pb2', '_op_def_registry', '_ops', 
'_pywrap_tensorflow', '_six', '_tensor_shape', 'deprecated_endpoints', 'tf_export']
```

Notice  `'zero_out'` &  `'zero_out_eager_fallback'` are not found in the loaded library for manylinux2010


**Code to reproduce the issue**
```
git clone https://github.com/tensorflow/custom-op.git && cd custom-op
docker run -it --rm -v ${PWD}:/workspace -w /workspace tensorflow/tensorflow:custom-op-ubuntu16 /bin/bash

pip install tf-nightly
./configure.sh
bazel build build_pip_pkg
bazel-bin/build_pip_pkg artifacts

# Installed auditwheel is too old for manylinux2010
pip3 install --upgrade auditwheel

# Libtensorflow framework needs to be on LD path
export LD_LIBRARY_PATH=""/usr/local/lib/python2.7/dist-packages/tensorflow_core""

# Repair logs look more or less okay
auditwheel -v repair --plat manylinux2010_x86_64 artifacts/tensorflow_zero_out-0.0.1-cp27-cp27mu-linux_x86_64.whl &> repair.txt
```
**Other info / logs**
Here are the auditwheel repair logs: 
[repair.txt](https://github.com/tensorflow/tensorflow/files/3521714/repair.txt)

Here are the readelf inspections of the so files:
[readelf.txt](https://github.com/tensorflow/tensorflow/files/3521717/readelf.txt)
[readelf-manylinux2010.txt](https://github.com/tensorflow/tensorflow/files/3521718/readelf-manylinux2010.txt)

Here are the so files:
[so-files.zip](https://github.com/tensorflow/tensorflow/files/3521726/so-files.zip)

cc @perfinion @gunan @yifeif 


--------------------------EDIT--------------------
Here are the extracted whl directories which will work with the python `tf.load_op_library` commands from above. (Manylinux2010 repair makes it so the custom op depends on a newly copied libtensorflow_framework.so which is part of the new whl):
[custom-op-dirs.zip](https://github.com/tensorflow/tensorflow/files/3522649/custom-op-dirs.zip)

"
31805,Bidirectional does not merge RNN outputs if return_state is True,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Pre-installed on Google Colab GPU Python3 instance
- TensorFlow version (use command below): `v1.14.0-0-g87989f6959 1.14.0`
- Python version: Python 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0
- GPU model and memory: NVIDIA Tesla T4

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When using GRU with Bidirectional wrapper, setting, `return_state=True, return_sequences=False` in GRU parameters and `merge_mode=""sum""` in Bidirectional parameters does not merge outputs. This makes it impossible to use in a Sequential model.
**Describe the expected behavior**
This issue was resolved upstream in Keras in keras-team/keras#8977
If `merge_mode` is used in Bidirectional, it should merge the returned state from each wrapped GRU even if `return_state=True, return_sequences=False` is set.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```py
from tensorflow.keras import layers, Sequential
from tensorflow.keras.layers import Bidirectional, GRU
model = Sequential()
model.add(Bidirectional(
    GRU(512, return_sequences=False, return_state=True),
    merge_mode='sum',
    input_shape=(None,13)
))
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```py
ValueError                                Traceback (most recent call last)
<ipython-input-26-a9c310b7c128> in <module>()
      6     GRU(512, return_sequences=False, return_state=True),
      7     merge_mode='sum',
----> 8     input_shape=(None,13)
      9 ))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)
    178         # If an input layer (placeholder) is available.
    179         if len(nest.flatten(layer._inbound_nodes[-1].output_tensors)) != 1:
--> 180           raise ValueError('All layers in a Sequential model '
    181                            'should have a single output tensor. '
    182                            'For multi-output layers, '

ValueError: All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.
```"
31804,build tensorflow from source -> screen freeze,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.2
- Python version: 3.7
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): 9.1
- CUDA/cuDNN version: 10.1.168 / 7.6.1.34
- GPU model and memory: GTX 850M / 2G

**Describe the problem**
I launched bazel to build tensorflow so that it can support CUDA and CUDNN for deep learning. But at around 20 minutes, my screen freezed, I could do nothing and it seemed that my pc was not really busy (as opposed to the first 20 minutes). I wait for 3 more hours but nothing happened. I had to shut down my computer via the power-on button. I maybe ran out of RAM !??

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

**What I tried**
I followed the indications from : https://www.tensorflow.org/install/source
and add --local_ram_resources=2048 flag but bazel did not recognized that flag.

Do you thing it is indeed a RAM problem or something else ?
"
31802,RuntimeError: Quantization not yet supported for op: FAKE_QUANT,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Docker image latest-gpu-py3
- TensorFlow version (use command below): 1.14
- Python version: 3.6
- GPU model and memory: RTX 2080 Ti

**Describe the current behavior**

I have trained an autoencoder and want to convert it to a tflite model. I successfully froze the graph and was able to convert the non-quantized 32-bit-float version. When trying to convert the very same frozen graph file with the uint8 option, I get an error:

RuntimeError: Quantization not yet supported for op: FAKE_QUANT

**Describe the expected behavior**

The frozen model does not seem to be corrupted because I was able to deploy the other version successfully. FAKE_QUANT should inherently be supported.

**Code to reproduce the issue**

Conversion was attempted with:

```
converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, inputs, outputs)
converter.representative_dataset = representative_dataset_gen
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()
```

**Other info / logs**

The error traceback:
```

INFO: Initialized TensorFlow Lite runtime.
Traceback (most recent call last):
  File ""train_and_save.py"", line 289, in <module>
    vae.create_tflite_model()
  File ""train_and_save.py"", line 249, in create_tflite_model
    tflite_model = converter.convert()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 908, in convert
    inference_output_type)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 200, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py"", line 78, in calibrate_and_quantize
    np.dtype(output_type.as_numpy_dtype()).num, allow_float)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 115, in QuantizeModel
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
RuntimeError: Quantization not yet supported for op: FAKE_QUANT
```

If it helped, I could provide the frozen graph file."
31800,FIPS enable computers fail due to md5 use,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Enterprise Linux Workstation release 7.7
```
Python 2.7.5 (default, Jun 11 2019, 14:33:56)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.version.GIT_VERSION
'v1.14.0-rc1-22-gaf24dc91b5'
```
- Installed using virtualenv? pip? conda?: pipenv install tensorflow


tflearn fails to import due to tensorflows use of md5 on an fips enabled machine
```
Python 2.7.5 (default, Jun 11 2019, 14:33:56)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tflearn
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../lib/python2.7/site-packages/tflearn/__init__.py"", line 4, in <module>
    from . import config
  File "".../lib/python2.7/site-packages/tflearn/config.py"", line 5, in <module>
    from .variables import variable
  File "".../lib/python2.7/site-packages/tflearn/variables.py"", line 7, in <module>
    from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope
  File "".../lib/python2.7/site-packages/tensorflow/contrib/__init__.py"", line 31, in <module>
    from tensorflow.contrib import cloud
  File "".../lib/python2.7/site-packages/tensorflow/contrib/cloud/__init__.py"", line 28, in <module>
    from tensorflow.contrib.bigtable.python.ops.bigtable_api import BigtableClient
  File "".../lib/python2.7/site-packages/tensorflow/contrib/bigtable/__init__.py"", line 29, in <module>
    from tensorflow.contrib.bigtable.python.ops.bigtable_api import BigtableClient
  File "".../lib/python2.7/site-packages/tensorflow/contrib/bigtable/python/ops/bigtable_api.py"", line 44, in <module>
    resource_loader.get_path_to_datafile(""_bigtable.so""))
  File "".../lib/python2.7/site-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File "".../lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 73, in load_op_library
    module_name = hashlib.md5(wrappers).hexdigest()
ValueError: error:060800A3:digital envelope routines:EVP_DigestInit_ex:disabled for fips
```
replacing all md5 calls with sha1 calls should work?
"
31799,Build tensorflow lite for aarch64: Error in script download_dependencies.sh ,"**System information**
- OS Platform :Linux Ubuntu 16.04
- TensorFlow version: Release 1.14.0
When I run the script ./tensorflow/lite/tools/make/download_dependencies.sh  I am getting following error:

downloading http://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz
./tensorflow/lite/tools/make/download_dependencies.sh: line 64: curl: command not found

gzip: stdin: unexpected end of file
tar: Child returned status 1
tar: Error is not recoverable: exiting now
"
31797,Deep Learning Image: TensorFlow 1.14.0 m33 on Google Cloud produces wrong and non deterministic loss after backpropagation ,"**System information**
- Have I written custom code: Yes, the code is attached.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux thomas-tf-14 4.9.0-9-amd64 #1 SMP Debian 4.9.168-1+deb9u4 (2019-07-19) x86_64 GNU/Linux`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Tested on google cloud
- TensorFlow installed from (source or binary): Binary: `Deep Learning Image: TensorFlow 1.14.0 m33` on Google Cloud
- TensorFlow version: v1.14.0-0-g87989f6 1.14.0
- Python version: 2.7.13 / 3.5.3
- Bazel version (if compiling from source): Not compiled from source
- GCC/Compiler version (if compiling from source): Not compiled from source
- CUDA/cuDNN version: Not used 
- GPU model and memory: None

**Describe the current behavior**
Current behaviour on Google Cloud using `Deep Learning Image: TensorFlow 1.14.0 m33` is non deterministic. During multiple runs the loss after performing backpropagation and updating a variable is different for different runs and does not match the loss of the non optimized standard tensorflow installation. This behaviour exists when using python2 and when using python3.

Created instance with:
`gcloud compute instances create ""tf-1-14-cpu"" --zone=""us-west1-b"" --image-family=""tf-1-14-cpu"" --image-project=deeplearning-platform-release`

Run 1:
```
Loss during step 0: -0.41999998688697815
Loss during step 1: -3.698721931466375e+19
Loss during step 2: -7.38836981337104e+19
```
Run 2:
```
Loss during step 0: -0.41999998688697815
Loss during step 1: -0.41999998688697815
Loss during step 2: -0.41999998688697815
```
Run 3:
```
Loss during step 0: -0.41999998688697815
Loss during step 1: 9.46872814455392e+21
Loss during step 2: 1.8914226722229864e+22
```

**Describe the expected behavior**
On the the same machine using a virtualenv to force the use of non optimized tensorflow as follows:
```
virtualenv -p python3 test
source test/bin/activate
pip3 install tensorflow==1.14.0
```

Run 1:
```
Loss during step 0: -0.41999998688697815
Loss during step 1: -1.4199999570846558
Loss during step 2: -2.4200000762939453
```
Run 2:
```
Loss during step 0: -0.41999998688697815
Loss during step 1: -1.4199999570846558
Loss during step 2: -2.4200000762939453
```
Run 3:
```
Loss during step 0: -0.41999998688697815
Loss during step 1: -1.4199999570846558
Loss during step 2: -2.4200000762939453
```

**Code to reproduce the issue**
```
import tensorflow as tf


if __name__ == ""__main__"":
    session = tf.Session()
    image = tf.get_variable(name=""image"", shape=[1, 1, 1, 1], initializer=tf.constant_initializer(0.42))
    session.run(tf.global_variables_initializer())
    kernel = tf.constant(1.0, shape=[1, 1, 1, 1], dtype=tf.float32)

    conv_out = tf.nn.conv2d(image, kernel, strides=[1, 1, 1, 1], padding=""SAME"")
    max_conv_out = tf.math.reduce_max(conv_out, axis=2)
    loss = -tf.reduce_sum(max_conv_out)
    opt = tf.train.GradientDescentOptimizer(learning_rate=1.0, name=""sgd"")
    optimizer = opt.minimize(loss, var_list=[image], name=""sgd_minimize"")

    for step in range(3):
        loss_value, _ = session.run([loss, optimizer])
        print(""Loss during step {}: {}"".format(step, loss_value))
```


**Other info / logs**
[python-code.txt](https://github.com/tensorflow/tensorflow/files/3520927/python-code.txt)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3520903/tf_env.txt)

"
31796,Learning Rate Scheduler with multi GPU,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14
- Python version: 3.6
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1080Ti

**Describe the current behavior**
When using single GPU model and Training the LearningRateScheduler Callback works fine and everything runs as expected.
If I change to multi gpu training I get the following error:
```
Traceback (most recent call last):
  File ""train.py"", line 165, in <module>
    workers=min(multiprocessing.cpu_count()//2, 32)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 1433, in fit_generator
    steps_name='steps_per_epoch')
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py"", line 208, in model_iteration
    callbacks.on_epoch_begin(epoch, epoch_logs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py"", line 295, in on_epoch_begin
    callback.on_epoch_begin(epoch, logs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py"", line 1351, in on_epoch_begin
    raise ValueError('Optimizer must have a ""lr"" attribute.')
ValueError: Optimizer must have a ""lr"" attribute.
```

Only thing change is single to multi gpu."
31795,Call tf.Session() twice causes fatal error: failed to get device attribute 13 for device 0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise 64-bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda_10.0.130_411.31_win10 / cudnn-10.0-windows10-x64-v7.6.2.24
- GPU model and memory: Nvidia GeForce 940MX 2GB

**Describe the current behavior**
Python stopped working
2019-08-20 18:38:59.811455: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed to get device attribute 13 for device 0: CUDA_ERROR_UNKNOWN: unknown error

**Describe the expected behavior**
should print 'Hello, TensorFlow-GPU!'

**Code to reproduce the issue**
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))

hello_gpu = tf.constant('Hello, TensorFlow-GPU!')
sess_gpu = tf.Session()
print(sess_gpu.run(hello_gpu))

**Other info / logs**
The first print statement generates b'Hello, TensorFlow!'. But the second tf.session() in the same jupyter notebook crashes python

2019-08-20 18:44:31.855812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.189
pciBusID: 0000:01:00.0
2019-08-20 18:44:31.863667: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-08-20 18:44:31.868460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-08-20 18:44:31.870987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-20 18:44:31.875292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-08-20 18:44:31.877960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-08-20 18:44:31.881525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1391 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
2019-08-20 18:45:07.339418: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed to get device attribute 13 for device 0: CUDA_ERROR_UNKNOWN: unknown error
"
31794,libtensorflow_framework.so.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31793,libtensorflow_framework.so.1'],"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31790,Can not use Tokenizer() to tokenize the input words in text classification in TF1.14 without eager execution for TPU..,"I am trying to write my code for using TPUs. However, I can not simply just tokenize the texts. I have tried many things to make it work but it just can not. There is NO documentation on Tensorflow on how to do it without enabling eager execution in TF1.14. 
````
tokenizer = tfds.features.text.Tokenizer()
vocabulary_set = set()
for text_tensor, _ in all_labeled_data:
  some_tokens = tokenizer.tokenize(text_tensor)
  vocabulary_set.update(some_tokens)
````
I am receiving following error
`TypeError: Expected binary or unicode string, got <tf.Tensor 'IteratorGetNext_5:0' shape=() dtype=string>`
I can not get the valiue of this iterator even by using `session.run`. THere is literally no help from Tensorflow on how to tokenize inputs for simple text classification without eager mode."
31789,[tflite] why not dynamic library for arm?,
31788,In reference with issue https://github.com/tensorflow/tensorflow/issues/31533,"For training model we are using
ssd_mobilenet_v2_coco

I have converted my model using following code: 

import tensorflow as tf
graph_def_file = ""C:/tensorflow1/models/research/object_detection/inference_graph/tflite_graph.pb""
input_arrays = [""normalized_input_image_tensor""]
input_shaps = {""normalized_input_image_tensor"":[1, 300, 300, 3]}
output_arrays = [""raw_outputs/box_encodings""]
converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shaps)
tflite_model = converter.convert()
open(""detact.tflite"", ""wb"").write(tflite_model)


As suggested by @jdduke 

Here is the link of my question on stack
https://stackoverflow.com/questions/57393407/java-lang-illegalargumentexception-cannot-copy-between-a-tensorflowlite-tensor"
31787,The current session will be closed and a new session will be created. Error: Stream IDs exhausted,"
**System information**
TensorFlow:
- OS Platform and Distribution (e.g., centos7):
- TensorFlow installed from ( binary):
- TensorFlow version (1.8):

in Distrubuted TensorFlow, train 38 days, we get the error "" **INFO:tensorflow:An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: Stream IDs exhausted** "" , 
any better to solve this quesution?
"
31786,cann't covert my model to tflite,"**System information**
- Ubuntu 16.04
- TensorFlow installed from anaconda:
- TensorFlow version 1.14.0:

when I covert my model to tflile, I get a error.  

my covert code :
```
import os
os.environ[""TF_ENABLE_CONTROL_FLOW_V2""] = ""1""
import tensorflow as tf


graph_def_file = ""./pretrained/saved_model.pb""
input_arrays = [""images_pre"",""image_widths_pre""]
output_arrays = [""output""]

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```

the error log shows below:

```
0820 16:35:04.585085 139937177536256 deprecation.py:323] From /root/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/lite/python/util.py:202: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0820 16:35:04.589805 139937177536256 deprecation.py:323] From /root/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
I0820 16:35:04.844114 139937177536256 graph_util_impl.py:311] Froze 185 variables.
I0820 16:35:04.878954 139937177536256 graph_util_impl.py:364] Converted 185 variables to const ops.
W0820 16:35:05.132165 139937177536256 deprecation.py:323] From /root/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/lite/python/util.py:204: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.remove_training_nodes`
Traceback (most recent call last):
  File ""train.py"", line 35, in <module>
    tf.app.run()
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""train.py"", line 31, in main
    models_path=crnn_config.models_path)
  File ""/mnt/e/ocr/crnnt/crnn/modules.py"", line 295, in train
    tflite_model = converter.convert()
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 898, in convert
    **converter_kwargs)
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 404, in toco_convert_impl
    input_data.SerializeToString())
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-08-20 16:35:08.179718: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-08-20 16:35:08.179786: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-08-20 16:35:08.179825: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorListReserve
2019-08-20 16:35:08.189538: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.189687: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorListReserve
2019-08-20 16:35:08.189707: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198165: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198192: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198208: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198222: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198239: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198254: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198271: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198285: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198301: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198315: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198331: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198345: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198361: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198374: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198390: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198403: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198417: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198430: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198446: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198460: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198476: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198490: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198504: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198517: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198531: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198546: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198561: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198576: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198592: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198604: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198618: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198632: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198647: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198662: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198676: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198691: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198705: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198718: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198732: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198745: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198760: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198773: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198789: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198802: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198817: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198830: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198846: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198861: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198877: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198891: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198908: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198921: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198936: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198950: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198965: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.198979: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.198994: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199006: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199020: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199034: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199052: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199065: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199080: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199094: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199108: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199122: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199138: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199150: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199169: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199183: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199198: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199210: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199224: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199239: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199254: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199268: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199282: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199295: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199309: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199323: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199338: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199351: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199365: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199379: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199395: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199410: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199425: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199439: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199455: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199469: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199485: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199499: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199515: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199530: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199547: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199561: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199577: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199591: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199606: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199620: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199636: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199650: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199666: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199681: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199697: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199712: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199728: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199742: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199758: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199773: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199788: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199802: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199818: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199833: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199849: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199863: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199879: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199894: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199911: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199925: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199942: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199956: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.199971: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.199985: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.200001: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.200016: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.200033: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.200047: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.200063: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.200076: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.200090: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.200102: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.200119: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.200133: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.200149: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: EmptyTensorList
2019-08-20 16:35:08.200163: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-08-20 16:35:08.200178: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2019-08-20 16:35:08.200192: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCGreedyDecoder
2019-08-20 16:35:08.202251: F tensorflow/lite/toco/tooling_util.cc:1041] Check failed: array->has_shape()
Fatal Python error: Aborted

Current thread 0x00007ff732710700 (most recent call first):
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/absl/app.py"", line 251 in _run_main
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/absl/app.py"", line 300 in run
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/root/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/root/anaconda3/envs/tf/bin/toco_from_protos"", line 11 in <module>
Aborted (core dumped)
```
my model pb file shows below:
Also, please include a link to a GraphDef or the model if possible.
[model.zip](https://github.com/tensorflow/tensorflow/files/3519545/model.zip)
]()



"
31784,how to convert tflite model to float 16?,"i run tf v1.14

well, i can hardly find some easy, usable codes to convert my tflite model to fp16(int8 is easy)

i read tf official post training quantization [docs](https://www.tensorflow.org/lite/performance/post_training_quantization), but i can not run this

```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.lite.constants.FLOAT16]
tflite_quant_model = converter.convert()
```
there no FLOAT16 in tensorflow._api.v1.lite.constants, just 
```
FLOAT = dtypes.float32
INT32 = dtypes.int32
INT64 = dtypes.int64
STRING = dtypes.string
QUANTIZED_UINT8 = dtypes.uint8
INT8 = dtypes.int8
```

and i found this [article,](https://medium.com/@fanzongshaoxing/post-training-quantization-of-tensorflow-model-to-fp16-8d66b9dfa77f) which might be help, but too complicated.

if i miss something, remind me please.
"
31783,"how to check model restore ok, tf.train.Saver.restore()","Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
31781,"use multi gpu come this error, but single gpu no this error","hello,
I have come with some problem when I use multi gpu to train below, but when i use single gpu there is no error, could you please help me? thanks very much!
`2019-08-20 09:52:53.646694: E tensorflow/stream_executor/cuda/cuda_driver.cc:981] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-08-20 09:52:53.646771: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x6e470f0: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-08-20 09:52:53.646720: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-08-20 09:52:53.646815: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x6e470f0: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-08-20 09:52:53.646833: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1
2019-08-20 09:52:53.646858: F tensorflow/stream_executor/cuda/cuda_dnn.cc:231] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.`
"
31779,failed to build tensorflow from source,"![Screen Shot 2019-08-19 at 4 29 26 PM](https://user-images.githubusercontent.com/36496141/63307405-1fc56a00-c2a3-11e9-857f-591e054547ad.png)
![Screen Shot 2019-08-19 at 4 30 07 PM](https://user-images.githubusercontent.com/36496141/63307414-26ec7800-c2a3-11e9-9453-59ceccc56f03.png)
Tried building tensorflow from source but getting this error.This tensorflow version-1.10,bazel-0.15,aarm64 bit architecture,gcc version-7.4.Could anyone please help?"
31774,TF 1.14: meta optimizer Invalid argument:,"Hello guys,

For my school paper I am doing fine tuning of some neural networks and one neural network I am using is tiny-yolo from [here](https://github.com/qqwweee/keras-yolo3), when I start the training script it loads the GPU but when it starts this is what I get 

![](https://user-images.githubusercontent.com/54294015/63305631-f549d000-c2e7-11e9-8b32-b45368309ebf.PNG)

The code is using Adam optimizer. From what I see while tracking the GPU/CPU/RAM usage of the application Cuda cores are used every few seconds and only 10% of them. I am not sure if this is because of the errors or what (since on different networks the ones I used for learning it uses 60% but only since I capped it at that)

Is this a known bug and how can I fix it? Or is the slowness of the training not related to this?"
31771,TensorFlow Install 'protoc' is not recognized ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip wheel
- TensorFlow version: 1.14
- Python version: 3.7
- Installed using virtualenv? pip? conda?: PIP

I am following Tensorflow install guide, but at the step where I need to run Protobuf  i get an error.
`protoc object_detection/protos/anchor_generator.proto --python_out=.`

> 'protoc' is not recognized as an internal or external command, operable program or batch file.

I am using Python 3.7 stock (not Anaconda) with Virtualenv "
31770,Missing input file '@curl//:lib/pipeline.h',"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.7.3
- Bazel version (if compiling from source): 0.25.3
- GCC/Compiler version (if compiling from source): 7.4.0



**Problem Description**
While building the pip package from source, I encounter the below error.
```ERROR: missing input file '@curl//:lib/pipeline.h'
ERROR: /users/arung/tensorflow/tensorflow/python/BUILD:4855:1: //tensorflow/python:pywrap_tensorflow_internal_py_wrap: missing input file '@curl//:lib/pipeline.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /users/arung/tensorflow/tensorflow/python/BUILD:4855:1 1 input file(s) do not exist
INFO: Elapsed time: 82.408s, Critical Path: 15.68s
INFO: 675 processes: 675 local.
FAILED: Build did NOT complete successfully
```

**Sequence of commands**
`./configure`
`bazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package`"
31767,Installing tf-nightly-2.0-preview with pipenv fails due to manylinux2010 wheels,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 but should be independent of platform
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): wheels (binaries) from PyPI
- TensorFlow version: Some versions of tf-nightly-2.0 preview newer than [2.0.0.dev20190712](https://pypi.org/project/tf-nightly-2.0-preview/2.0.0.dev20190712/)
- Python version: 3.5/3.6/3.7
- Installed using virtualenv? pip? conda?: [`pipenv`](https://github.com/pypa/pipenv)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

Installing [`tf-nightly-2.0-preview`](https://pypi.org/project/tf-nightly-2.0-preview/) fails when _all_ of the following things are true:
 - [`pipenv`](https://github.com/pypa/pipenv) is being used to install the `tf-nightly-2.0-preview`
 - `tf-nightly-2.0-preview` is constrained to a version or versions that are *newer than [2.0.0.dev20190712](https://pypi.org/project/tf-nightly-2.0-preview/2.0.0.dev20190712/)* that happen to specify `functools32` as a dependency on PyPI
 - PyPI is being used as the source
 - Python 3 is being used (3.5, 3.6, or 3.7)

When the above are all true, installation fails during pipenv's locking step due to an exception raised by functools32 (it correctly complains about being installed in an environment that isn't using Python 2.7).

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Here's an example Pipfile to reproduce the problem:
```TOML
[[source]]
name = ""pypi""
url = ""https://pypi.org/simple""
verify_ssl = true

[packages]
tf-nightly-2-0-preview = ""==2.0.0.dev20190802""

[requires]
python_version = ""3.7""

[pipenv]
allow_prereleases = true
```

Running `pipenv update` while in a folder containing the above Pipfile results in an error during locking.

**Any other info / logs**

The reason this happens is because of how `pipenv` resolves the dependencies of packages that come from PyPI.

`pipenv` uses [PyPI's JSON API](https://warehouse.pypa.io/api-reference/json/) to [get the dependencies of a package to speed up locking](https://github.com/pypa/pipenv/blob/9c1378a9928fbadbd7482867916e7482d5a7fd7f/pipenv/patched/piptools/repositories/pypi.py#L207-L247) (sidenote: the comment in that function checks that the dependency is pinned to a specific versioni.e. `tf-nightly-2-0-preview==2.0.0.dev20190802`but I believe this isn't referring to the dependency as it's listed in the `Pipfile`; by the time that function is invoked for `tf-nightly-2-0-preview`, it will have been 'pinned' to a specific version). IIUC, PyPI is able to expose the dependencies for wheels through it's JSON API by reading the metadata bundled inside wheels.

My understanding is that for packages that PyPI can't provide dependency information for (like source dist packages) and for packages not in PyPI, `pipenv` falls back on running `setup.py egg_info`. When `pipenv` tries to do this for [functools32](https://pypi.org/project/functools32/) (which is distributed as an `sdist` package) it arrives at the exception (functools32 also chooses to throw an Exception instead of using the [requires-python](https://packaging.python.org/specifications/core-metadata/#requires-python) field which causes the cryptic error from `pipenv`).

The [setup.py](https://github.com/tensorflow/tensorflow/blob/82c46d74d9ece4f6b5832f27a7ae580d95e1a312/tensorflow/tools/pip_package/setup.py) for the tensorflow wheel [correctly only adds `functools32` as a dependency if Python 2.x is being targeted](https://github.com/tensorflow/tensorflow/blob/82c46d74d9ece4f6b5832f27a7ae580d95e1a312/tensorflow/tools/pip_package/setup.py#L84-L92), so it doesn't make sense that `pipenv` tries to do anything with `functools32` at all.

Looking at what PyPI's JSON API provides for [tf-nightly-2.0-preview](https://pypi.org/pypi/tf-nightly-2.0-preview/2.0.0.dev20190818/json) reveals why: `functools32` is listed under `requires_dist` and this is because the `requires_dist` field exists on releases but *not* on the various distributions that are part of a release. In other words, the PyPI JSON API cannot communicate that distributions within a release have different dependencies.

Also, I'm not completely sure how PyPI chooses which distribution's `requires_dist` metadata entries represent the entire release, but I think it might have something to do with the order in which the wheels are uploaded; some of the releases since `functools32` was added as a dependency don't have it listed under their `requires_dist` entries (i.e. [August 17th's build](https://pypi.org/pypi/tf-nightly-2.0-preview/2.0.0.dev20190817/json)).

Since July 12th, the following releases have `functools32` listed as a dependency on PyPI:
 - 2.0.0.dev20190713
 - 2.0.0.dev20190715
 - 2.0.0.dev20190719
 - 2.0.0.dev20190720
 - 2.0.0.dev20190721
 - 2.0.0.dev20190722
 - 2.0.0.dev20190729
 - 2.0.0.dev20190803
 - 2.0.0.dev20190807
 - 2.0.0.dev20190808
 - 2.0.0.dev20190813
 - 2.0.0.dev20190816
 - 2.0.0.dev20190818

I realize this is not a bug in tensorflow and should probably be fixed in pipenv/PyPI but I thought I'd file an issue anyways in case other people are having this issue.

Edit: `pipenv` appears to [no longer use PyPI's JSON API](https://github.com/pypa/pipenv/commit/54dd40473c1f07ab6ba275c4c472a3087d46665c); I'm not sure what's causing the above..."
31766,bazel configure fails. can't build tensorflow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): .Linux Manjaro 
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version: 3.7
- Installed using virtualenv? pip? conda?: no virtual, noconda (only spyder installed)
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 9.1.0
- CUDA/cuDNN version: 10.1 / 7.6.1
- GPU model: i7 4700 HQ


**Describe the problem**
I currently have tensorflow 1.14 installed with pip3, but I need to build it in order to support CUDA for deep learning. Thus, I first installed CUDA 10.1.168 and CUDNN 7.6.1.34. Then, I downloaded bazel 0.24.1 installer and launched it. Then, I finally downloaded tensorflow to build it, but I got issues when launching ./configure.


**Provide the exact sequence of commands / steps that you executed before running into the problem**

 ````
sudo pacman -S cuda cudnn
./bazel_0.24.1_installer_linux_x86_64.sh --user
export PATH='$PATH:$HOME/bin'
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r1.14
./configure
````

then ./configure ends like this : 
````
    WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
    You have bazel 0.24.1 installed.
    Please specify the location of python. [Default is /usr/bin/python]: 


    Found possible Python library paths:
      /usr/lib/python3.7/site-packages
    Please input the desired Python library path to use.  Default is [/usr/lib/python3.7/site-packages]

    Do you wish to build TensorFlow with XLA JIT support? [Y/n]: 
    XLA JIT support will be enabled for TensorFlow.

    Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
    No OpenCL SYCL support will be enabled for TensorFlow.

    Do you wish to build TensorFlow with ROCm support? [y/N]: 
    No ROCm support will be enabled for TensorFlow.

    Do you wish to build TensorFlow with CUDA support? [y/N]: y
    CUDA support will be enabled for TensorFlow.

    Do you wish to build TensorFlow with TensorRT support? [y/N]: 
    No TensorRT support will be enabled for TensorFlow.

    Could not find any cuda.h matching version '' in any subdirectory:
            ''
            'include'
            'include/cuda'
            'include/*-linux-gnu'
            'extras/CUPTI/include'
            'include/cuda/CUPTI'
    of:
            '/opt/cuda/extras/CUPTI/lib64'
            '/opt/cuda/lib64'
            '/opt/cuda/nvvm/lib64'
            '/usr'
            '/usr/lib'
            '/usr/lib/libfakeroot'
            '/usr/lib32'
    Asking for detailed CUDA configuration...

    Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 10.1.168


    Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.6.1.34


    Please specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: 1.3


    Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: 5.0


    Could not find any cuda.h matching version '10.1.168' in any subdirectory:
            ''
            'include'
            'include/cuda'
            'include/*-linux-gnu'
            'extras/CUPTI/include'
            'include/cuda/CUPTI'
    of:
    Asking for detailed CUDA configuration...

    Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:
````

So, I can't carry on the configuration. Is there something I have to do before?
"
31765,Docker images for 1.12 version without jupyter,"**System information**
-  Linux Ubuntu 16.04 - Docker + docker-nvidia
- TensorFlow installed from Docker tensorflow
- TensorFlow version: 1.12.0 - 1.12.3
- Python version: py 2 and py3
- GPU: 2x1080



**Problem Description**

While trying to install older version of tensorflow for testing I was met with unexpected behaviour. It seems that the usual non-jupyter docker tags point to jupyter version. 

`sudo docker run -it --rm tensorflow/tensorflow:1.12.0-gpu-py3`
returns : 

```
Unable to find image 'tensorflow/tensorflow:1.12.0-gpu-py3' locally
1.12.0-gpu-py3: Pulling from tensorflow/tensorflow
18d680d61657: Already exists 
0addb6fece63: Already exists 
78e58219b215: Already exists 
eb6959a66df2: Already exists 
e3eb30fe4844: Already exists 
852c9b7a4425: Already exists 
0a298bf31111: Already exists 
f43ecd71dda8: Already exists 
9f554feaeba1: Already exists 
abf1fc85d970: Already exists 
3e67c4ad17bb: Already exists 
c60e8159f45c: Already exists 
2b01db739666: Already exists 
1553de0cb9ac: Already exists 
1ed5c01b0218: Already exists 
9913722703a5: Already exists 
442335dc9a85: Already exists 
Digest: sha256:84f0820e151b129c63ac15c6d9c1c5336a834070dca22a271c7de091d490a17f
Status: Downloaded newer image for tensorflow/tensorflow:1.12.0-gpu-py3
[I 12:58:14.924 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret
[I 12:58:14.946 NotebookApp] Serving notebooks from local directory: /notebooks
[I 12:58:14.946 NotebookApp] The Jupyter Notebook is running at:
[I 12:58:14.946 NotebookApp] http://(8c019c13c8ce or 127.0.0.1):8888/?token=f940ba4e239d161fcd710a013b9755eaedef83dff5e090d5
[I 12:58:14.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 12:58:14.946 NotebookApp] 
    
    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://(8c019c13c8ce or 127.0.0.1):8888/?token=f940ba4e239d161fcd710a013b9755eaedef83dff5e090d5
^C[I 12:59:06.586 NotebookApp] interrupted
Serving notebooks from local directory: /notebooks
0 active kernels
The Jupyter Notebook is running at:
http://(8c019c13c8ce or 127.0.0.1):8888/?token=f940ba4e239d161fcd710a013b9755eaedef83dff5e090d5
```

And not the expected: 


```
34667c7e4631: Already exists 
d18d76a881a4: Already exists 
119c7358fbfc: Already exists 
2aaf13f3eff0: Already exists 
643564d518c8: Already exists 
1fea03e629a4: Already exists 
45402f4cf61d: Already exists 
45f7c407b07b: Already exists 
00e5163fe3e0: Already exists 
7d071071ef98: Pull complete 
5119bdada1e4: Pull complete 
64a9355aa772: Pull complete 
21f5ce47fe21: Pull complete 
5566cd4bac12: Pull complete 
58c608a4c711: Pull complete 
Digest: sha256:0f949ccc690d9c50e9b46b16d9030c2c6845af621c2e7e82c4bf59803edc69b5
Status: Downloaded newer image for tensorflow/tensorflow:1.12.0-gpu-py3

________                               _______________                
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / 
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/


WARNING: You are running this container as root, which can cause new files in
mounted volumes to be created as the root user on your host machine.

To avoid this, run the container by specifying your user's userid:

$ docker run -u $(id -u):$(id -g) args...

root@9f4ce2da7d6a:/# 
```


Running the exact same tags on 1.13 seems to work as expected according to the documentation. 
"
31764,"Protocol ""https"" not supported or disabled in libcurl","Hi Guys,
 
can anyone help me with the following issue

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from source.
- TensorFlow version: r1.11
- Python version: 2.7.12
- GCC/Compiler version (if compiling from source): GNU 5.4.0
**Describe the problem**

from top directory of tensorflow iam running the script as follows
./tensorflow/tools/ci_build/linux/cmake/run.sh 
 iam stuck with the following error

error: downloading 'https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz' failed
         status_code: 1
         status_string: ""Unsupported protocol""
         log:
         --- LOG BEGIN ---
         Protocol ""https"" not supported or disabled in libcurl
like above error there are number of  files that are being not downloaded because of that iam getting build error.

this is my curl version iam using 

curl -V
curl 7.66.0-DEV (x86_64-pc-linux-gnu) libcurl/7.66.0-DEV OpenSSL/1.0.2g zlib/1.2.8
Release-Date: [unreleased]
Protocols: dict file ftp ftps gopher http https imap imaps pop3 pop3s rtsp smb smbs smtp smtps telnet tftp 
Features: AsynchDNS HTTPS-proxy IPv6 Largefile libz NTLM NTLM_WB SSL TLS-SRP UnixSockets


Thank you
Snathick





"
31763,ImportError: DLL load failed with error code -1073741795,"I follow installation instructions from tensorflow.org. I use CPU version.

I have seen many similar problems, but all of them were due to unsupported AVX. 
My CPU supports it, but tensorflow still doesn't work.

**System information**
- OS Platform and Distribution: Wisnows 7 64
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: pip in virtualenv
- CUDA/cuDNN version: CPU version
- GPU model and memory: CPU version

**Describe the problem**
After installation I try to run:

`python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""`

and I get error:
```
Traceback (most recent call last):
  File ""C:\Users\S1614414\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\S1614414\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\S1614414\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\S1614414\venv\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\S1614414\venv\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\S1614414\venv\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\S1614414\venv\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\S1614414\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\S1614414\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\S1614414\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\S1614414\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\S1614414\venv\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\S1614414\venv\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.
```
What could be the issue?

Edit: I have found reason - I had AVX disabled on my system.
This issue can be closed.
"
31762,"Create GPU Delegate failed,  dequantize error","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10+
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): r1.14.0
- Python version: 3.6.3
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I converted my tflite model with F16 quantilization, then I create GPU Delegate to run my model,
the source code version of tensorflow is r1.14.0. I got the following error when running the inference:
Initialized TensorFlow Lite runtime.
Internal error: Failed to apply delegate: tensorflow/lite/kernels/dequantize.cc:62 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 was not true.Node number 1 (DEQUANTIZE) failed to prepare.

**Describe the expected behavior**
Tensorflow Lite can run Float16 model with GPU Delegate correctly.

**Code to reproduce the issue**
gpuDelegate = new GpuDelegate();
tfliteOptions.addDelegate(gpuDelegate);
tfliteOptions.setAllowFp16PrecisionForFp32(true);
tflite = new Interpreter(tfliteModel,tfliteOptions);

**Other info / logs**
The code and cmd to convert the model:
python freeze_graph.py --input_graph=2d-3d-match-pointnet/model_test.pb --input_checkpoint=2d-3d-match-pointnet/model_test.ckpt --output_graph=2d-3d-match-pointnet/tmp/model.pb --output_node_names=moutput

import tensorflow as tf

graph_def_file = ""./model.pb""
input_arrays = [""minput""]
output_arrays = [""moutput""]

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.lite.constants.FLOAT16]
tflite_model = converter.convert()
open(""model_quant_f16.tflite"", ""wb"").write(tflite_model)

 I would appreciate it if you could help check this issue. thanks again."
31761,"[tf.data] Listing files, sampling a subset and repeating iterates over each file when shuffling is enabled","**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X Mojave 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.7.3
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

Testing the tf.data setup, I want to select a subset of the data to read from disc. Doing that with `tf.data.Dataset.list_files(""*"", shuffle=True).take(2)` iterates over the entire folder despite the explicit `take`.

**Describe the expected behavior**

When shuffling is enabled in `tf.data.Dataset.list_files`, I should be able to repeat the random sample indefinitely.

**Code to reproduce the issue**

First, create dummy data.

```shell
mkdir scratchpad
cd scratchpad
for i in {0..100}; do touch $i; done
```

Then, in IPython,

```python
import os
import tensorflow as tf

sess = tf.InteractiveSession()
tf.global_variables_initializer().run()

it = (
    tf.data.Dataset.list_files(os.path.join(""*""))
    .take(2)
    .repeat(None)
    .make_one_shot_iterator()
    .get_next()
)

[sess.run(it) for _ in range(12)]
```

The above works as expected with shuffling disabled:

```python
import os
import tensorflow as tf

sess = tf.InteractiveSession()
tf.global_variables_initializer().run()

it = (
    tf.data.Dataset.list_files(os.path.join(""*""), shuffle=False)
    .take(2)
    .repeat(None)
    .make_one_shot_iterator()
    .get_next()
)

[sess.run(it) for _ in range(12)]
```"
31760,Build failure seems related with C++14 support of GCC 4.8 w.r.t. MLIR dependency,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.6
- Installed using virtualenv? pip? conda?: NA, building
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 4.8
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**

Failed to build latest TF from source with command (which works fine on branch `r1.14`, and works on `master` maybe one month ago):
```sh
bazel build --jobs=8 -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka --config=nonccl //tensorflow/tools/pip_package:build_pip_package
```

Seems c++14 support is required when building TF (with MLIR dependency introduced), while I am using GCC 4.8, which doesn't include C++14 support. I am wondering what is the recommended GCC version? I see the official builds are marked with GCC 4.8. Do you guys encounter this issue?

The failed log:
```
ERROR: /home/wzh/.cache/bazel/_bazel_wzh/7830c7af24b3349eb6de8dd12d5e8378/external/local_config_mlir/BUILD:418:1: C++ compilation of rule '@local_config_mlir//:Support' failed (Exit 1)
gcc-4.8: error: unrecognized command line option '-std=c++14'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```
"
31759,Upsampling to a odd number,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary 
- TensorFlow version (use command below): 1.13
- Python version: 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: V100 32GB


Current Behaviour:
Raises a Value Error: ambiguous dimension hence affecting workflow

Expected:
Upsample the tensor to desired size using single side padding

Code to reproduce the issue:

a = tf.random.uniform(shape=[1,45,60,3], dtype=tf.dtypes.float32)
kernel_size = (45/2,60/2)
b = AveragePooling2D(pool_size=kernel_size, strides=kernel_size, padding='same')(a)
c = UpSampling2D(size=kernel_size, data_format='channels_last', interpolation='bilinear')(b)


"
31758,dataset as input to estimator is broken.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Does not depend on OS.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-0-g87989f6959, 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Dataset input to estimator is broken

**Describe the expected behavior**

Passing the a dataset object to `estimator.fit` method results in the following  error: 

```
ValueError                                Traceback (most recent call last)

<ipython-input-6-1ed4dbb580ca> in <module>()
     14 est.fit(
     15     steps=1000,
---> 16     input_fn=lambda : csv_input_fn(train_path, batch_size))

2 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _train_model(self, input_fn, hooks)
   1036       random_seed.set_random_seed(self._config.tf_random_seed)
   1037       global_step = training_util.create_global_step(g)
-> 1038       features, labels = input_fn()
   1039       self._check_inputs(features, labels)
   1040       training_util._get_or_create_global_step_read()  # pylint: disable=protected-access

ValueError: too many values to unpack (expected 2)
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Following example from the docs can be used to reproduce the problem. Check  the definition of the main function.
```
import pandas as pd
import tensorflow as tf

TRAIN_URL = ""http://download.tensorflow.org/data/iris_training.csv""
TEST_URL = ""http://download.tensorflow.org/data/iris_test.csv""

CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',
                    'PetalLength', 'PetalWidth', 'Species']
SPECIES = ['Setosa', 'Versicolor', 'Virginica']

def maybe_download():
    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)
    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)

    return train_path, test_path

def load_data(y_name='Species'):
    """"""Returns the iris dataset as (train_x, train_y), (test_x, test_y).""""""
    train_path, test_path = maybe_download()

    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)
    train_x, train_y = train, train.pop(y_name)

    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)
    test_x, test_y = test, test.pop(y_name)

    return (train_x, train_y), (test_x, test_y)


def train_input_fn(features, labels, batch_size):
    """"""An input function for training""""""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)

    # Return the dataset.
    return dataset


def eval_input_fn(features, labels, batch_size):
    """"""An input function for evaluation or prediction""""""
    features=dict(features)
    if labels is None:
        # No labels, use only features.
        inputs = features
    else:
        inputs = (features, labels)

    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices(inputs)

    # Batch the examples
    assert batch_size is not None, ""batch_size must not be None""
    dataset = dataset.batch(batch_size)

    # Return the dataset.
    return dataset


# The remainder of this file contains a simple example of a csv parser,
#     implemented using the `Dataset` class.

# `tf.parse_csv` sets the types of the outputs to match the examples given in
#     the `record_defaults` argument.
CSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]

def _parse_line(line):
    # Decode the line into its fields
    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)

    # Pack the result into a dictionary
    features = dict(zip(CSV_COLUMN_NAMES, fields))

    # Separate the label from the features
    label = features.pop('Species')

    return features, label


def csv_input_fn(csv_path, batch_size):
    # Create a dataset containing the text lines.
    dataset = tf.data.TextLineDataset(csv_path).skip(1)

    # Parse each line.
    dataset = dataset.map(_parse_line)

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)

    # Return the dataset.
    return dataset
  
  
def main():
  train_path, test_path = maybe_download()
  batch_size = 100
  CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',
                      'PetalLength', 'PetalWidth', 'Species']

  est = tf.contrib.learn.LinearClassifier(
      CSV_COLUMN_NAMES,
      n_classes=3
  )

  est.fit(
      steps=1000,
      input_fn=lambda : csv_input_fn(train_path, batch_size))
  
main()
```



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31756,tflite: incorrect quantisation scale application in unit test utils,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, the bug was discovered when evaluating new quantisation modes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): discoverd on Linux Ubuntu 16.04, but should be applicable to any platform where the tests are run
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master branch
- Python version: Python3.6
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): GCC 5.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
The quantization scale in [PerChannelQuantizeBias()](https://github.com/tensorflow/tensorflow/blob/1f61f13f8715dc26dabe46a2686216674026d812/tensorflow/lite/kernels/test_util.h#L239) in [tensorflow/lite/kernels/test_util.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/test_util.h) appears to be applied incorrectly. The floating point input data are multiplied by the scale value whereas they should be divided by it. The corresponding tests in [conv_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/conv_test.cc) and [depthwise_conv_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/depthwise_conv_test.cc), e.g. [SimplePerChannelTest](https://github.com/tensorflow/tensorflow/blob/1f61f13f8715dc26dabe46a2686216674026d812/tensorflow/lite/kernels/conv_test.cc#L1343) appear to contain the wrong expected values.

**Describe the expected behavior**
The division operation should be used to convert the floating point to quantized fixed point value and the corresponding tests changes respectively.

**Code to reproduce the issue**
N/A

**Other info / logs**
N/A
"
31755,tf.function decorator not found,"Hi,

in the current nightly build, tf.function cannot be found.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview 2.0.0.dev20190818
- Python version: 3.6

**Code to reproduce the issue**
```
import tensorflow as tf

@tf.function
def my_func():
   print(tf.__version__)
```

**Other info / logs**
AttributeError: module 'tensorflow' has no attribute 'function'
"
31754,No tf.lite.experimental.nn.bidirectional_dynamic_rnn ops is finded,"**System information**
- Linux Ubuntu 16.04:
- TensorFlow installed from anaconda:
- TensorFlow version 1.14:

when i from tensorflow._api.v1.lite.experimental.nn import bidirectional_dynamic_rnn, i get the error:
AttributeError: module 'tensorflow._api.v1.lite.experimental.nn' has no attribute 'bidirectional_dynamic_rnn'

but i find in https://tensorflow.google.cn/lite/convert/rnn that the ops has be supported.

by the way, tf.lite.experimental.nn.dynamic_rnn can be import as https://tensorflow.google.cn/lite/convert/rnn shows.


**Provide the text output from tflite_convert**

```
module 'tensorflow._api.v1.lite.experimental.nn' has no attribute 'bidirectional_dynamic_rnn'
```

"
31753,How to profile in tensorflow 2.0,I can't find the profile method of tensorflow 2.0 in google or github. How to use profiler and timeline in tensorflow 2.0. Is there any other way profile tensorflow 2.0;
31752,Getting 404 in XLA JIT page,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/xla/jit

## Description of issue (what needs changing):
Getting not found when opening

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
31751,"ValueError: ('Input has undefined rank:', TensorShape(None))","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
windows 10, anaconda python 3.7
tensorflow 2.0b1

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
![image](https://user-images.githubusercontent.com/27112868/63238990-9babbc00-c27b-11e9-9f3f-be7fd9165bdd.png)

Problem : ValueError: ('Input has undefined rank:', TensorShape(None))
when I used line 62-63 instead of 64,  there will be no errors.
I am not sure why  line 64 is not working correctly.



**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import time
import math
import tensorflow as tf
import numpy as np
import tensorflow as keras
from tensorflow.keras import layers
from tensorflow import keras
from tensorflow.keras.utils import plot_model
from scipy . stats import multivariate_normal as normal
tf.keras.backend.set_floatx('float64')

d = 10
T = 0.1
n_time = 5
n_sample = 10
batch_size = 10
n_maxstep = 400
h = (T + 0.0) / n_time
t_stamp = np.arange (0, n_time) * h

def f_tf (t, X, Y, Z):
    V =  Y - tf.math.pow (Y,3)
    return V

def g_tf (t, X):
    V =  0.5 / (1 + 0.2*tf.math.reduce_sum (X**2, 1, keepdims=True))
    return V

def sample_path ( n_sample ):
    dW_sample = np.zeros ([ n_sample, d, n_time  ], dtype = np.float64)
    X_sample  = np.zeros ([ n_sample, d, n_time+1], dtype = np.float64)
    for i in range (n_time):
        dW_sample [:, :, i  ] = np.reshape ( normal.rvs ( mean = np.zeros(d,dtype = np.float64),\
                                      cov =1, size = n_sample ) * np.sqrt(h), ( n_sample, d))

        X_sample  [:, :, i+1] =  X_sample  [:, :, i] + np.sqrt(2) * dW_sample [:, :, i]
    return dW_sample, X_sample

def nn_tf(x):
    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)
    x = keras.layers.Dense(d+10, batch_size = n_sample)(x)

    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)
    x = keras.layers.Dense(d+10, batch_size = n_sample)(x)

    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)
    x = keras.layers.Dense(d, batch_size = n_sample)(x)

    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)
    return x

dW = keras.Input(shape = (d, n_time  ), batch_size=n_sample, dtype = tf.float64, name = 'dW')
XX = keras.Input(shape = (d, n_time+1), batch_size=n_sample, dtype = tf.float64, name = 'X' )
X = XX
Y = tf.ones([n_sample, 1], dtype = tf.float64)
Z = tf.ones([n_sample, d], dtype = tf.float64)

for it in range(n_time-1):
    with tf.name_scope(str(it+1)):
        Y = Y - f_tf(t_stamp[it], X[:,:,it], Y, Z) * h
        Y = Y +  tf.math.reduce_sum( Z * dW[:,:,it],  1, keepdims=True)
        #subX = tf.reshape(X[:,:,it], shape = [n_sample, d])
        #Z = nn_tf(subX) / d
        Z = nn_tf(X[:,:,it]) / d

Y = Y - f_tf(t_stamp[n_time-1], X[:,:,n_time-1], Y, Z) * h
Y = Y + tf.math.reduce_sum (Z * dW [:, :, n_time-1], 1, keepdims=True)
model = keras.Model(inputs=[XX,dW], outputs=[Y])

optimizer = keras.optimizers.Adam(learning_rate=1e-3)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

for epoch in range (100):
    dW_train, X_train = sample_path ( n_sample )

    with tf.GradientTape() as tape:
        predictions = model( [X_train, dW_train] )
        label = g_tf (T, X_train[:, :, n_time])
        #print(""label = "", label)
        #print(""predictions = "", predictions)
        #loss_value = tf.reduce_sum( tf.keras.losses.MSE (label, predictions ) )
        loss_value = ( tf.keras.losses.MeanSquaredError() (label, predictions ) )
        #print(""label = "", label)
        #print(""predictions = "", predictions)
        print(""lss = "", loss_value )
    grads = tape.gradient(loss_value,  model.trainable_variables)
    optimizer.apply_gradients( zip(grads, model.trainable_variables) )
    accuracy = train_accuracy(label, predictions)
    print(""step "", epoch, ""loss = "", loss_value.numpy(), ""accuracy = "", accuracy.numpy())
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

runfile('C:/Users/DELL/Desktop/DP_PDE_PY/PDE_DP_1.py', wdir='C:/Users/DELL/Desktop/DP_PDE_PY')
Traceback (most recent call last):

  File ""<ipython-input-42-59f0f0a64686>"", line 1, in <module>
    runfile('C:/Users/DELL/Desktop/DP_PDE_PY/PDE_DP_1.py', wdir='C:/Users/DELL/Desktop/DP_PDE_PY')

  File ""D:\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 826, in runfile
    execfile(filename, namespace)

  File ""D:\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/DELL/Desktop/DP_PDE_PY/PDE_DP_1.py"", line 64, in <module>
    Z = nn_tf(X[:,:,it]) / d

  File ""C:/Users/DELL/Desktop/DP_PDE_PY/PDE_DP_1.py"", line 40, in nn_tf
    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)

  File ""D:\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 616, in __call__
    self._maybe_build(inputs)

  File ""D:\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1951, in _maybe_build
    self.build(input_shapes)

  File ""D:\Anaconda3\lib\site-packages\tensorflow\python\keras\layers\normalization.py"", line 270, in build
    raise ValueError('Input has undefined rank:', input_shape)

ValueError: ('Input has undefined rank:', TensorShape(None))
"
31749,tf.keras: Error in BinaryCrossentropy loss when output shape is determined by inputs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v2.0.0-beta1-4983-g65e6355ad9 2.0.0-rc0
- Python version:
- Bazel version (if compiling from source): bazel 26.1
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Shape mismatch for binary cross-entropy with logits for compiled Keras  when output shape is determined by model inputs (no error when using eager-evaluation):
```
ValueError: logits and labels must have the same shape ((None,) vs (None, 1))
```
**Describe the expected behavior**
No shape mismatch

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
import numpy as np

inputs = tf.keras.Input(shape=(1,), dtype=tf.int64)
outputs = tf.random.normal(shape=(inputs[0, 0],))

model = tf.keras.Model(inputs, outputs)
model.compile(optimizer='sgd', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))

dummy = np.zeros((10, 1))
shapes = np.random.randint(0, 11, 10).reshape(10, 1, 1)
dataset = tf.data.Dataset.from_tensor_slices(shapes)
dataset = dataset.map(lambda x: (x, tf.random.uniform(minval=0, maxval=2, shape=(x[0,0],), dtype=tf.int32)))

# eagar mode, works
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
for inputs, true in dataset:
    pred = model(inputs)
    print(true)
    print(pred)
    print(loss(true, pred))
    break

# compiled model, fails
model.fit(dataset, epochs = 1, steps_per_epoch=1)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 928, in merge_with
    self.assert_same_rank(other)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 983, in assert_same_rank
    (self, other))
ValueError: Shapes (None, 1) and (None,) must have the same rank

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py"", line 168, in sigmoid_cross_entropy_with_logits
    labels.get_shape().merge_with(logits.get_shape())
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 934, in merge_with
    raise ValueError(""Shapes %s and %s are not compatible"" % (self, other))
ValueError: Shapes (None, 1) and (None,) are not compatible

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 25, in <module>
    model.fit(dataset, epochs = 1, steps_per_epoch=1)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 736, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 307, in fit
    total_epochs=epochs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 121, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 84, in execution_function
    for out in distributed_function(input_fn)]
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 427, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 370, in _initialize
    *args, **kwds))
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1834, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2134, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2025, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 884, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 320, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 71, in distributed_function
    per_replica_function, args=(model, x, y, sample_weights))
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 760, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1787, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 2132, in _call_for_each_replica
    return fn(*args, **kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 292, in wrapper
    return func(*args, **kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 299, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 307, in train_on_batch
    output_loss_metrics=output_loss_metrics))
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 252, in _process_single_batch
    training=training))
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 166, in _model_loss
    per_sample_losses = loss_fn.call(targets[i], outs[i])
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py"", line 216, in call
    return self.fn(y_true, y_pred, **self._fn_kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py"", line 989, in binary_crossentropy
    K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py"", line 4449, in binary_crossentropy
    return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py"", line 171, in sigmoid_cross_entropy_with_logits
    (logits.get_shape(), labels.get_shape()))
ValueError: logits and labels must have the same shape ((None,) vs (None, 1))
```"
31747,"Only the first 30 ops will run on the GPU, and the remaining 1 on the CPU","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS10.14.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:iPhone7 & iPhone6
- TensorFlow installed from (source or binary):binary
- TensorFlow version: tensorflow-lite-gpu:0.0.0
- Python version:3.7
- Installed using virtualenv? pip? conda?:cocoapod
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:16G memory

**Describe the problem**

I am building my codes with tensorflow_lite_gpu.framework on iPhone 7. The codes is just like Google recommends:

```
_model = FlatBufferModel::BuildFromFile(modelPathCString);
        ops::builtin::BuiltinOpResolver resolver;
        InterpreterBuilder(*_model, resolver)(&_interpreter);
        _delegate = NewGpuDelegate(nullptr);  // default config
        _interpreter->ModifyGraphWithDelegate(_delegate);
...//Other codes
```
When I use the model mobilenet_v1_1.0_224.tflite which Google provides, I get warning: `WARNING: 25 cannot be handled by this delegate.  Only the first 30 ops will run on the GPU, and the remaining 1 on the CPU` from console after excuting `_interpreter->ModifyGraphWithDelegate(_delegate)`. But the model deeplabv3_257_mv_gpu.tflite is good which Google provides too. As for my own model, the result is like mobilenet_v1_1.0_224.tflite does. Someone please help me.

PS: Here is my podfile: 
```
platform :ios, '10.0'
target ""SpeechExample"" do
pod 'TensorFlowLiteGpuExperimental'
end
```
"
31744,Raspberry Pie 4,"Raspberry Pie 4

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
pi@raspberrypi:/xiaolu/u/soft/tensorflow $ sudo ./tensorflow/lite/tools/make/build_rpi_lib.sh
+ set -e
+++ dirname ./tensorflow/lite/tools/make/build_rpi_lib.sh
++ cd ./tensorflow/lite/tools/make
++ pwd
+ SCRIPT_DIR=/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make
+ TENSORFLOW_DIR=/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/../../../..
+ make -j 4 TARGET=rpi -C /xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/../../../.. -f tensorflow/lite/tools/make/Makefile -latomic
make: /xiaolu/u/soft/tensorflow
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/ -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/ruy/kernel_arm32.cc -o /xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/ruy/kernel_arm32.o
/tmp/ccIPb9CY.s: Assembler messages:
/tmp/ccIPb9CY.s:451: Neon quad precision register expected -- `vld1.32 q11,[r2]'
make: *** [tensorflow/lite/tools/make/Makefile:244/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/ruy/kernel_arm32.o]  1
make: /xiaolu/u/soft/tensorflow

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31743,ModuleNotFoundError: No module named 'tensorflow',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
It cannot import tensor flow in jupyterlab
**Describe the expected behavior**
It should import tensorflow
**Code to reproduce the issue**
import tensorflow as tf
**Other info / logs**
![Screen Shot 2019-08-18 at 5 37 12 PM](https://user-images.githubusercontent.com/20175738/63232913-1a9fe680-c1e1-11e9-9acf-e51d45600d05.png)

"
31741,Seg Fault with Custom Op built with nightly (works fine with v1.14 and 2.0 Beta),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from : binary
- TensorFlow version: Nightly
- Python version: 3.7
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: 1080Ti

I have a series of custom ops. They build and operate perfectly fine on a Windows 10 (with v1.14, v2.0 beta and nightly).

I have just tried to build them on Linux, and they again work fine with v1.14 and v2.0 Beta. However with nightly they instantly segfault. As per the instructions I am using -D_GLIBCXX_USE_CXX11_ABI=0 flag due to using gcc 7.4.0

I have also tried building the op with g++ 4.8 and still have the same issue."
31739,Guide is needed for transform of models suitable for tensorflow lite with gpu delegate,"Hello,
Where one can find how to transform models to be suitable for working with tensorflow lite for gpu?
What should i do to transfer images from 3 components to 4 components.
I am trying to work with mtcnn. I have converted it successfully to tflite, works fine on the cpu,

ERROR: Next operations are not supported by GPU delegate:
NEG: Operation is not supported.
First 2 operations will run on the GPU, and the remaining 40 on the CPU.
WARN: compileToBinary(256):
C:\fakepath(86,169-182): warning X3556: integer divides may be much slower, try using uints if possible.
C:\fakepath(86,246-259): warning X3556: integer modulus may be much slower, try using uints if possible.

ERROR: TfLiteGpuDelegate Invoke: ConvertToPHWC4: Input data size does not match expected size: 12288000 != 6912
ERROR: Node number 27 (TfLiteGpuDelegate) failed to invoke."
31738,delete and append to Tfrecords: Tensorflow,"
I am currently working on a research in which I have to append my training set after some epochs and delete some samples from test set after evaluation. Currently there is no way with which I can access the records (placed at specific indexes) in tfrecord file. Since tfrecords offer very fast training so I avoid using generators. Any chance of adding such feature?

"
31737,Unable to learn model weights using `tf.nn.nce_loss`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): KDE Neon 5.16 (based on Ubuntu 18.04)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0 (GIT: v1.12.0-0-ga6d8ffae09)
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): GCC 8.0.1 20180414 (experimental) [trunk revision 259383]
- CUDA/cuDNN version: CUDA=10.0, cuDNN=7.4.2
- GPU model and memory: NVIDIA GeForce GTX 1050 Ti Max-Q, with 4GB of memory

**Describe the current behavior**
I was trying to build a word2vec model by following [this TensorFlow tutorial](https://www.tensorflow.org/tutorials/representation/word2vec). I've changed the code little bit, mainly in the parts where I have to load training data. The problem is that the model won't learn anything. The loss keeps fluctuating up and down, and the weights of my network stay constant throughout the training process (I've checked this using TensorBoard). I am convinced there's something wrong with either the code posted in the tutorial or with the `tf.nn.nce_loss` function, as I don't see any problem with the code I wrote.

**Describe the expected behavior**
I'm expecting the model to learn _something_ - which doesn't mean that I'm expecting it to achieve a specific accuracy. In other words, I'm expecting the network's weights to be updated after a training step.

**Code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np

vocabulary_size = 13046
embedding_size = 256
num_noise = 1
learning_rate = 1e-3
batch_size = 1024
epochs = 10

def make_hparam_string(embedding_size, num_noise, learning_rate, batch_size, epochs):
    return f'es={embedding_size}_nn={num_noise}_lr={learning_rate}_bs={batch_size}_e={epochs}'

# These are the hidden layer weights
embeddings = tf.get_variable(name='embeddings', initializer=tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable=True)

# 'nce' stands for 'Noise-contrastive estimation' and represents a particular loss function.
# Check https://www.tensorflow.org/tutorials/representation/word2vec for more details.
# 'nce_weights' and 'nce_biases' are simply the output weights and biases.
# NOTE: for some reason, even though output weights will have shape (embedding_size, vocabulary_size),
#       we have to initialize them with the shape (vocabulary_size, embedding_size)
nce_weights = tf.get_variable(name='output_weights',
                              initializer=tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / np.sqrt(embedding_size)), 
                              trainable=True)
nce_biases = tf.get_variable(name='output_biases', initializer=tf.constant_initializer(0.1), shape=[vocabulary_size], trainable=True)

# Placeholders for inputs
train_inputs = tf.placeholder(tf.int32, shape=[None])    # [batch_size]
train_labels = tf.placeholder(tf.int32, shape=[None, 1]) # [batch_size, 1]

# This allows us to quickly retrieve the corresponding word embeddings for each word in 'train_inputs'
matched_embeddings = tf.nn.embedding_lookup(embeddings, train_inputs)

# Compute the NCE loss, using a sample of the negative labels each time.
loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,
                                     biases=nce_biases,
                                     labels=train_labels,
                                     inputs=matched_embeddings,
                                     num_sampled=num_noise,
                                     num_classes=vocabulary_size))

# Use the SGD optimizer to minimize the loss function
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)

# Add some summaries for TensorBoard
loss_summary = tf.summary.scalar('nce_loss', loss)
input_embeddings_summary = tf.summary.histogram('input_embeddings', embeddings)
output_embeddings_summary = tf.summary.histogram('output_embeddings', nce_weights)

################################################################################

# Load data
target_words = np.genfromtxt('target_words.txt', dtype=int, delimiter='\n').reshape((-1, 1))
context_words = np.genfromtxt('context_words.txt', dtype=int, delimiter='\n').reshape((-1, 1))

# Convert to tensors
target_words_tensor = tf.convert_to_tensor(target_words)
context_words_tensor = tf.convert_to_tensor(context_words)

# Create a tf.data.Dataset object representing our dataset
dataset = tf.data.Dataset.from_tensor_slices((target_words_tensor, context_words_tensor))
dataset = dataset.shuffle(buffer_size=target_words.shape[0])
dataset = dataset.batch(batch_size)

# Create an iterator to iterate over the dataset
iterator = dataset.make_initializable_iterator()
next_batch = iterator.get_next()

# Train the model
with tf.Session() as session:

    # Initialize variables
    session.run( tf.global_variables_initializer() )

    merged_summary = tf.summary.merge_all()

    # File writer for TensorBoard
    hparam_string = make_hparam_string(embedding_size, num_noise, learning_rate, batch_size, epochs)
    loss_writer = tf.summary.FileWriter(f'./tensorboard/{hparam_string}')

    global_step = 0
    for epoch in range(epochs):

        session.run(iterator.initializer)
        while True:
            try:
                inputs, labels = session.run(next_batch)

                feed_dict = {train_inputs: inputs[:, 0], train_labels: labels}
                _, cur_loss, all_summaries = session.run([optimizer, loss, merged_summary], feed_dict=feed_dict)

                # Write sumaries to disk
                loss_writer.add_summary(all_summaries, global_step=global_step)
                global_step += 1

                print(f'Current loss: {cur_loss}')

            except tf.errors.OutOfRangeError:
                print(f'Finished epoch {epoch}.')
                break

```

**Other info / logs**
I'm attaching the training samples (target_words.txt) and the corresponding labels (context_words.txt) in case you wanted to reproduce this issue.
[target_words.txt](https://github.com/tensorflow/tensorflow/files/3512860/target_words.txt)
[context_words.txt](https://github.com/tensorflow/tensorflow/files/3512859/context_words.txt)

"
31736,GPU placing issue with slurm,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 42.3
- TensorFlow installed from (source or binary): binary/pip
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: Python 3.6.9 :: Anaconda, Inc.
- CUDA/cuDNN version: CUDA 10.1
- GPU model and memory: 4x GeForce RTX 2080 11Gb

**Describe the current behavior**

- If I do not change anything then the training crashes with the traceback below.
- If I set `allow_soft_placement=True` the framework begins to place everything on CPU.

**Describe the expected behavior**

The training starts on 4 GPUs.

**Code to reproduce the issue**

Slurm command `srun -N 1 --gres=gpu:4 -c 32 --mem=96G python train.py`

[PGAN repo](https://github.com/tkarras/progressive_growing_of_gans)

**Other info / logs**

Traceback


```
Traceback (most recent call last):
  File ""train.py"", line 285, in <module>
    tfutil.call_func_by_name(**config.train)
  File "".../pgan-tf/tfutil.py"", line 236, in call_func_by_name
    return import_obj(func)(*args, **kwargs)
  File "".../pgan-tf/train.py"", line 161, in train_progressive_gan
    G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)
  File "".../pgan-tf/tfutil.py"", line 433, in __init__
    self.reset_vars()
  File "".../pgan-tf/tfutil.py"", line 495, in reset_vars
    run([var.initializer for var in self.vars.values()])
  File "".../pgan-tf/tfutil.py"", line 21, in run
    return tf.get_default_session().run(*args, **kwargs)
  File "".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File "".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File "".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File "".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation G/lod: node G/lod (defined at .../pgan-tf/networks.py:176) was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1, /job:localhost/replica:0/task:0/device:XLA_GPU:2, /job:localhost/replica:0/task:0/device:XLA_GPU:3 ]. Make sure the device specification refers to a valid device.
         [[G/lod]]
srun: error: nvidia99: task 0: Exited with exit code 120
```"
31733,Error when subclassing for stock example: Expected D2 of index to be 2 got 3 at position 1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes. The code is derived from one of the integration test cases.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Stock example in Google Colab https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/keras/feature_columns.ipynb
- TensorFlow version (use command below):
tf.version.GIT_VERSION = v2.0.0-beta0-16-g1d91213fe7
tf.version.VERSION = 2.0.0-beta1
- Python version:
3.6

**Describe the current behavior**
The following `feature_column` creates the issue when subclassing `keras.models.Model`, which works alright with `sequential` API.
```python
crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)
crossed_feature = feature_column.indicator_column(crossed_feature)
```

**Describe the expected behavior**
Not failing with `feature_column.crossed_column` when subclassing. The integration test here, https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/feature_columns_integration_test.py#L161-L183, didn't catch this problem.

**Code to reproduce the issue**
Running the following at the end of the notebook will reproduce the error.
```python
class FeatureColumnDsModel(tf.keras.models.Model):
    def __init__(self, feature_columns, **kwargs):
        super().__init__(name=None, **kwargs)
        self._input_layer = tf.keras.layers.DenseFeatures(feature_columns)
        self._dense1 = tf.keras.layers.Dense(128, activation='relu')
        self._dense2 = tf.keras.layers.Dense(128, activation='relu')
        self._output_layer = tf.keras.layers.Dense(1, activation='sigmoid')
        
    def call(self, features):
        x = self._input_layer(features)
        x = self._dense1(x)
        x = self._dense2(x)
        y = self._output_layer(x)
        return y
    
model = FeatureColumnDsModel(feature_columns)
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'],)

model.fit(train_ds,
          validation_data=val_ds,
          epochs=5)
```

**Other info / logs**
```bash
Epoch 1/5
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-27-fa952c9e276b> in <module>()
     21 model.fit(train_ds,
     22           validation_data=val_ds,
---> 23           epochs=5)

9 frames
/tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    641         max_queue_size=max_queue_size,
    642         workers=workers,
--> 643         use_multiprocessing=use_multiprocessing)
    644 
    645   def evaluate(self,

/tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/engine/training_generator.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    692         shuffle=shuffle,
    693         initial_epoch=initial_epoch,
--> 694         steps_name='steps_per_epoch')
    695 
    696   def evaluate(self,

/tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
    262 
    263       is_deferred = not model._is_compiled
--> 264       batch_outs = batch_function(*batch_data)
    265       if not isinstance(batch_outs, list):
    266         batch_outs = [batch_outs]

/tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
    916       self._update_sample_weight_modes(sample_weights=sample_weights)
    917       self._make_train_function()
--> 918       outputs = self.train_function(ins)  # pylint: disable=not-callable
    919 
    920     if reset_metrics:

/tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3508         value = math_ops.cast(value, tensor.dtype)
   3509       converted_inputs.append(value)
-> 3510     outputs = self._graph_fn(*converted_inputs)
   3511 
   3512     # EagerTensor.numpy() will often make a copy to ensure memory safety.

/tensorflow-2.0.0b1/python3.6/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
    570       raise TypeError(""Keyword arguments {} unknown. Expected {}."".format(
    571           list(kwargs.keys()), list(self._arg_keywords)))
--> 572     return self._call_flat(args)
    573 
    574   def _filtered_call(self, args, kwargs):

/tensorflow-2.0.0b1/python3.6/tensorflow/python/eager/function.py in _call_flat(self, args)
    669     # Only need to override the gradient in graph mode and when we have outputs.
    670     if context.executing_eagerly() or not self.outputs:
--> 671       outputs = self._inference_function.call(ctx, args)
    672     else:
    673       self._register_gradient()

/tensorflow-2.0.0b1/python3.6/tensorflow/python/eager/function.py in call(self, ctx, args)
    443             attrs=(""executor_type"", executor_type,
    444                    ""config_proto"", config),
--> 445             ctx=ctx)
    446       # Replace empty list with None
    447       outputs = outputs or None

/tensorflow-2.0.0b1/python3.6/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError:  Expected D2 of index to be 2 got 3 at position 1
	 [[node feature_column_ds_model/dense_features_7/age_bucketized_X_thal_indicator/SparseCross (defined at <ipython-input-27-fa952c9e276b>:23) ]] [Op:__inference_keras_scratch_graph_32547]

Function call stack:
keras_scratch_graph
```"
31732,Tensorflow serving Batching configuration docs has bunch of broken links,"
Broken URL : https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration

All the link under batching configuration section return 404. 

Example links : 

1. https://www.tensorflow.org/tfx/batching/README#servers_with_multiple_models_model_versions_or_subtasks

2. https://www.tensorflow.org/tfx/batching/README

3. https://www.tensorflow.org/tfx/batching/README#batch_scheduling_parameters_and_tuning

"
31731,tf.linalg.sqrtm() returns nan for invertible matrix,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **windows 10**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **r1.14**
- Python version: **3.6.8**
- CUDA/cuDNN version: **No**

**Describe the current behavior**
tf.linalg.sqrtm() returns nan, even though input matrix is invertible. 
When the input matrix is constructed using numpy operations instead and then converted to tensor, the  behavior is as expected.

The issue can be reproduced with the following code, both .csv's can be downloaded from: https://www.kaggle.com/gemartin/world-bank-data-1960-to-2016/downloads/world-bank-data-1960-to-2016.zip/1


```
import numpy as np
import pandas as pd
import tensorflow as tf


def load_word_bank_dataset():
    """"""
    This function loads the World Bank Data and return it as NxD numpy arrays
    """"""
    # Load and drop rows with missing values
    fert_rate = pd.read_csv(r'./fertility_rate.csv').dropna()
    life_exp = pd.read_csv(r'./life_expectancy.csv').dropna()

    # Keep only countries that appears in both tables
    valid_countries = list(set(fert_rate['Country Code']) & set(life_exp['Country Code']))
    fert_rate = fert_rate[fert_rate['Country Code'].isin(valid_countries)]
    life_exp = life_exp[life_exp['Country Code'].isin(valid_countries)]

    # Create numpy arrays from the years columns
    years_str_list = [str(year) for year in range(1960, 2017)]
    fert_rate = fert_rate[years_str_list].to_numpy()
    life_exp = life_exp[years_str_list].to_numpy()

    return life_exp, fert_rate

def cca_loss(x1, x2, r1=0.0, r2=0.0):
    # Extract dimensions
    N, D1 = x1.shape
    _, D2 = x2.shape
    scale_mat = np.eye(N) - 1.0 / N * np.ones((N, N))
    scale_factor = tf.cast(1. / N, dtype=tf.float32)

    # This part doesn't work
    scale_mat_tensor = tf.convert_to_tensor(scale_mat, dtype=tf.float32)
    x1_T_tensor = tf.convert_to_tensor(x1.T, dtype=tf.float32)
    h1_bar_1_tensor = tf.matmul(x1_T_tensor, scale_mat_tensor)
    cov_11_1_tensor = scale_factor * tf.matmul(h1_bar_1_tensor, h1_bar_1_tensor, adjoint_b=True)
    cov_11_1_sqrt = tf.linalg.sqrtm(cov_11_1_tensor)  # Returns nan's

    # This part works
    h1_bar_np = np.matmul(x1.T, scale_mat)
    h1_bar_2_tensor = tf.convert_to_tensor(h1_bar_np, dtype=tf.float32)
    cov_11_2_tensor = scale_factor * tf.matmul(h1_bar_2_tensor, h1_bar_2_tensor, adjoint_b=True)
    cov_11_2_sqrt = tf.linalg.sqrtm(cov_11_2_tensor)  # Returns real valued matrix

    return -1

if __name__ == '__main__':
    tf.compat.v1.enable_eager_execution()
    # Load data
    fertility_rate, life_expectancy = load_word_bank_dataset()
    loss = cca_loss(fertility_rate, life_expectancy, r1=0., r2=0.)
```"
31729,Make ConcreteFunction Compatible with Pickle,"**System information**
- TensorFlow version (you are using): `2.0.0-beta1`
- Are you willing to contribute it (Yes/No): No 


**Describe the feature and the current behavior/state.**
See [issue 31421](https://github.com/tensorflow/tensorflow/issues/31421#issue-478107883) for more details. 

A `ConcreteFunction`, specifically, a `SavedModel`'s default serving `ConcreteFunction` that is used for `SavedModel` predictions. 

```python
saved_model = tf.saved_model.load(export_dir, tags=['serve'])

# infer is not serializable (it cannot be pickled)
infer = saved_model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
```

The serialization error is 
```sh
_pickle.PickleError: can't pickle repeated message fields, convert to list first
```

**Will this change the current api? How?**
Not to my knowledge 

**Who will benefit with this feature?**
Anyone who wants to deploy a TF 2.0 `SavedModel` or use a `ConcreteFunction` in (Py)Spark or any other distributed system that relies on pickling user-defined functions 

**Any Other info.**

https://github.com/tensorflow/tensorflow/issues/31421#issuecomment-522273057"
31728,New Version of NVIDIA CUDA (10.1 Update 2),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): master
- Are you willing to contribute it (Yes/No): Yes, I have a system that supports CUDA 10.1u2 and am willing to test issues with it.



**Describe the feature and the current behavior/state.** NVIDIA has released the newest version of CUDA, 10.1u2, and I would like to know if anyone familiar with NVIDIA/CUDA programming knows of any major changes that we would need to make in the future. Otherwise, I am available to compile and test on a system with a newer NVIDIA card to see how it works.

**Will this change the current api? How?** Not applicable.

**Who will benefit with this feature?** Newer hardware support.

**Any Other info.**
"
31725,Error loading tensorflow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.14.0
- **Python version**: 3.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.1 - 10.1
- **GPU model and memory**: Nvidia Quadro K5000 - 4Gb
- **Exact command to reproduce**: import tensorflow as tf

### Describe the problem
Error loading Tensorflow. Does not provide a clue about which DLL is failing to initialize.

### Source code / logs
> import tensorflow as tf
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Error en una rutina de inicializacin de biblioteca de vnculos dinmicos (DLL).
Failed to load the native TensorFlow runtime.

Calling from a program:
>python style.py
Traceback (most recent call last):
  File ""C:\Users\Javier\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Javier\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Javier\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Error en una rutina de inicializacin de biblioteca de vnculos dinmicos (DLL).
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""style.py"", line 9, in <module>
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Error en una rutina de inicializacin de biblioteca de vnculos dinmicos (DLL).
Failed to load the native TensorFlow runtime."
31724,Tensorflow distribute strategy with custom layers gives an error when using Adam optimizer,"`<em>Please` make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below):1.14.0
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am getting an error when I use a model with custom layers with trainable weights. It works fine without the distribute strategy, and also works fine if the custom layer is replaced by  an existing tf.Keras layer. The error appears to be related to Adam optimizer finding an empty var list when it tries to colocate variables. 

**Describe the expected behavior**

**Code to reproduce the issue**
`
      
     

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31722,Get prediction values from tensorflow lite android,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: Samsung A3 2016
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):1.13.1
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0/7.4
- GPU model and memory: Nvidia Geforce 840m 3 Go



**Describe the current behavior**
I want to get values from output tensor with input an image to predict the eye region landmarks.
I have predicted image from frozen graph model and Tensorflow lite using python script and it works fine.
The problem is with the Android platform, I got false and different values compared to values from python script

**Describe the expected behavior**

**Code to reproduce the issue**
Python script

```python
data = np.asarray( img, dtype=""float32"" ) 

# Inference on input data normalized to [0, 1]
inputImg = np.expand_dims(data,0).astype(np.float32)
input_details = interpreter.get_input_details()
interpreter.set_tensor(input_details[0]['index'], inputImg)

interpreter.invoke()

output_details = interpreter.get_output_details()
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)

```

Java code ( Android Environment )
```java
private float[][][][] bitmapToInputArray() {
        // [START mlkit_bitmap_input]
        Bitmap bitmap= getYourInputImage();
        bitmap = Bitmap.createScaledBitmap(bitmap, 112, 112, true);

        int batchNum = 0;
        float[][][][] input = new float[1][112][112][3];
        for (int x = 0; x < 112; x++) {
            for (int y = 0; y < 112; y++) {
                int pixel = bitmap.getPixel(x, y);
                // Normalize channel values to [-1.0, 1.0]. This requirement varies by
                // model. For example, some models might require values to be normalized
                // to the range [0.0, 1.0] instead.
                input[batchNum][x][y][0] = (Color.red(pixel) - 127) / 128.0f;
                input[batchNum][x][y][1] = (Color.green(pixel) - 127) / 128.0f;
                input[batchNum][x][y][2] = (Color.blue(pixel) - 127) / 128.0f;
                Log.i(""Input"",""input""+input[batchNum][x][y]);

            }
        }
        // [END mlkit_bitmap_input]

      return input;
    }

 private void useInferenceResult(float[] probabilities) throws IOException {
        String[] result=new String[80];
        String x="""";
        String y="""";
        ArrayList<Point> listpoint= new ArrayList<Point>();

        double viewWidth = canvas.getWidth();
        double viewHeight = canvas.getHeight();
        double imageWidth = mutableBitmap.getWidth();
        double imageHeight = mutableBitmap.getHeight();
        Log.i(""viewWidth"",""viewwidth ""+viewWidth);
        Log.i(""viewHeight"",""viewheight ""+viewHeight);
        Log.i(""imagewidth"",""imagewidth ""+imageWidth);
        Log.i(""imaageHeigh"",""imageheigh ""+imageHeight);
        double scale = Math.min(viewWidth / imageWidth, viewHeight / imageHeight);
        Log.i(""Scale"",""Scale""+scale);
        try {
            for (int i = 0; i < probabilities.length; i++) {

                Log.i(""MLKit"", String.format(""%1.8f"", probabilities[i]));
                float i1 = probabilities[i];
                Log.i(""floaaat"", """" + i1);
            }
        }
```



**Other info / logs**
+ That what I should get : ( this is from python script using Tensorflow lite model ):

>[[0.3323875  0.19654518 0.3430611  0.17367488 0.3671013  0.16478491
  0.39022946 0.17491257 0.399131   0.19814822 0.3900888  0.22294888
  0.36448467 0.23006389 0.34123936 0.21979642 0.30286688 0.20466375
  0.31792122 0.19202346 0.33909258 0.18460245 0.3652809  0.18118384
  0.39127237 0.18584773 0.40995973 0.19605562 0.42233318 0.21089557
  0.40651116 0.2182863  0.38454503 0.22073016 0.3601514  0.22152397
  0.3375371  0.21898884 0.31742495 0.2137229  0.628366   0.21445222
  0.63888156 0.19114998 0.6626687  0.18333182 0.6856534  0.1955274
  0.69444335 0.22068903 0.6838143  0.2438955  0.66011566 0.2518409
  0.63507193 0.2414121  0.60496974 0.22573914 0.6209617  0.21241865
  0.6422507  0.20397455 0.6683734  0.20254172 0.6917013  0.2092008
  0.7084733  0.21974358 0.7189961  0.23459744 0.70730895 0.24100597
  0.69019526 0.24282111 0.6691431  0.24188581 0.6451422  0.23857626
  0.6218671  0.2344214 ]]

+ Those values are from Android Studio ( java using Log.i) :


>2019-08-17 14:47:50.617 21349-21349/com.example.irisdetection I/MLKit: 0,23961355
2019-08-17 14:47:50.620 21349-21349/com.example.irisdetection I/MLKit: 0,25104424
2019-08-17 14:47:50.621 21349-21349/com.example.irisdetection I/MLKit: 0,28179651
2019-08-17 14:47:50.622 21349-21349/com.example.irisdetection I/MLKit: 0,31467810
2019-08-17 14:47:50.623 21349-21349/com.example.irisdetection I/MLKit: 0,33257431
2019-08-17 14:47:50.624 21349-21349/com.example.irisdetection I/MLKit: 0,32645294
2019-08-17 14:47:50.625 21349-21349/com.example.irisdetection I/MLKit: 0,29138848
2019-08-17 14:47:50.626 21349-21349/com.example.irisdetection I/MLKit: 0,25581932
2019-08-17 14:47:50.627 21349-21349/com.example.irisdetection I/MLKit: 0,19593856
2019-08-17 14:47:50.628 21349-21349/com.example.irisdetection I/MLKit: 0,21698779
2019-08-17 14:47:50.631 21349-21349/com.example.irisdetection I/MLKit: 0,24266151
2019-08-17 14:47:50.632 21349-21349/com.example.irisdetection I/MLKit: 0,27562365
2019-08-17 14:47:50.633 21349-21349/com.example.irisdetection I/MLKit: 0,30823168
2019-08-17 14:47:50.635 21349-21349/com.example.irisdetection I/MLKit: 0,33465266
2019-08-17 14:47:50.636 21349-21349/com.example.irisdetection I/MLKit: 0,35355449
2019-08-17 14:47:50.637 21349-21349/com.example.irisdetection I/MLKit: 0,34009647
2019-08-17 14:47:50.638 21349-21349/com.example.irisdetection I/MLKit: 0,31358159
2019-08-17 14:47:50.640 21349-21349/com.example.irisdetection I/MLKit: 0,28156102
2019-08-17 14:47:50.642 21349-21349/com.example.irisdetection I/MLKit: 0,25063315
2019-08-17 14:47:50.643 21349-21349/com.example.irisdetection I/MLKit: 0,21878451
2019-08-17 14:47:50.644 21349-21349/com.example.irisdetection I/MLKit: 0,69623101
2019-08-17 14:47:50.646 21349-21349/com.example.irisdetection I/MLKit: 0,70167470
2019-08-17 14:47:50.646 21349-21349/com.example.irisdetection I/MLKit: 0,73317540
2019-08-17 14:47:50.648 21349-21349/com.example.irisdetection I/MLKit: 0,76974392
2019-08-17 14:47:50.649 21349-21349/com.example.irisdetection I/MLKit: 0,79195201
2019-08-17 14:47:50.651 21349-21349/com.example.irisdetection I/MLKit: 0,78359401
2019-08-17 14:47:50.652 21349-21349/com.example.irisdetection I/MLKit: 0,75674009
2019-08-17 14:47:50.653 21349-21349/com.example.irisdetection I/MLKit: 0,71786618
2019-08-17 14:47:50.654 21349-21349/com.example.irisdetection I/MLKit: 0,66782737
2019-08-17 14:47:50.655 21349-21349/com.example.irisdetection I/MLKit: 0,68930006
2019-08-17 14:47:50.656 21349-21349/com.example.irisdetection I/MLKit: 0,71668541
2019-08-17 14:47:50.657 21349-21349/com.example.irisdetection I/MLKit: 0,75279719
2019-08-17 14:47:50.658 21349-21349/com.example.irisdetection I/MLKit: 0,78872705
2019-08-17 14:47:50.659 21349-21349/com.example.irisdetection I/MLKit: 0,81867975
2019-08-17 14:47:50.661 21349-21349/com.example.irisdetection I/MLKit: 0,83806717
2019-08-17 14:47:50.662 21349-21349/com.example.irisdetection I/MLKit: 0,82371044
2019-08-17 14:47:50.664 21349-21349/com.example.irisdetection I/MLKit: 0,79749656
2019-08-17 14:47:50.665 21349-21349/com.example.irisdetection I/MLKit: 0,76317006
2019-08-17 14:47:50.666 21349-21349/com.example.irisdetection I/MLKit: 0,72700304
2019-08-17 14:47:50.667 21349-21349/com.example.irisdetection I/MLKit: 0,69159627

How can I solve this problem??
"
31721,[lite] failed on  define=with_select_tf_ops=true,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code 
 
cc_binary(
    name = ""Prophet.so"",
    srcs = [""minimal.h"", ""Prophet.cc"",   ],
    linkopts = tflite_linkopts() + select({
        ""//tensorflow:android"": [
            ""-pie"",  # Android 5.0 and later supports only PIE
            ""-lm"",  # some builtin ops, e.g., tanh, need -lm
        ],
        ""//conditions:default"": [],
    }),
    deps = [
        ""//tensorflow/lite:framework"",
        ""//tensorflow/lite/delegates/flex:delegate"", [ on | off]
        ""//tensorflow/lite/c:c_api_internal"",
        ""//tensorflow/lite/kernels:builtin_ops"",
        ""//tensorflow/lite:builtin_op_data"",
        ""//tensorflow/lite/schema:schema_fbs"",
    ],
    linkshared = True,
)
- OS Platform and Distribution (vmware Linux Ubuntu 18.04 x4 on window10x64):
- TensorFlow installed from (source  ):github 20190817
- TensorFlow version (): 1.14.0
- Python version:3.7.4
- Bazel version (if compiling from source):0.26
- GCC/Compiler version (if compiling from source): 7.4 binary
- CUDA/cuDNN version: cpu
- GPU model and memory: cpu

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c

**Describe the current behavior**
Target //tensorflow/lite/examples/l17:Prophet.so failed to build
**Describe the expected behavior**
build successed
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 bazel build --config monolithic --cxxopt=-std=c++11 -c opt  \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
--define=with_select_tf_ops=true   --cpu=armeabi-v7a  --verbose_failures\
  //tensorflow/lite/examples/l17:Prophet.so

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
bazel cmd log
`
ERROR: /root/tensorflow-master/tensorflow/core/kernels/BUILD:6576:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 254): clang failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/510cb3499e6983b92a09020af9102ff3/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=29.0.2 \
    ANDROID_NDK_API_LEVEL=18 \
    ANDROID_NDK_HOME=/root/vender/android-ndk-r17c \
    ANDROID_SDK_API_LEVEL=18 \
    ANDROID_SDK_HOME=/root/vender/android-sdk \
    PATH='~/anaconda3/bin:/root/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin' \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/root/anaconda3/bin/python \
    PYTHON_LIB_PATH=/root/anaconda3/lib/python3.7/site-packages \
    TF_CONFIGURE_IOS=0 \
  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang '-D__ANDROID_API__=18' -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/cwise_op_not_equal_to_2.pic.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/cwise_op_not_equal_to_2.pic.o' -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -iquote . -iquote bazel-out/armeabi-v7a-opt/bin -iquote external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/armeabi-v7a-opt/bin/external/nsync -iquote external/com_google_protobuf -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_protobuf -iquote external/zlib_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/zlib_archive -iquote external/eigen_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-v7a-opt/bin/external/local_config_sycl -iquote external/double_conversion -iquote bazel-out/armeabi-v7a-opt/bin/external/double_conversion -iquote external/farmhash_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/armeabi-v7a-opt/bin/external/fft2d -iquote external/gemmlowp -iquote bazel-out/armeabi-v7a-opt/bin/external/gemmlowp -isystem external/nsync/public -isystem bazel-out/armeabi-v7a-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/armeabi-v7a-opt/bin/external/com_google_protobuf/src -isystem external/zlib_archive -isystem bazel-out/armeabi-v7a-opt/bin/external/zlib_archive -isystem external/eigen_archive -isystem bazel-out/armeabi-v7a-opt/bin/external/eigen_archive -isystem external/double_conversion -isystem bazel-out/armeabi-v7a-opt/bin/external/double_conversion -isystem external/farmhash_archive/src -isystem bazel-out/armeabi-v7a-opt/bin/external/farmhash_archive/src '-std=c++11' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-mfpu=neon' -DTENSORFLOW_MONOLITHIC_BUILD -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-18/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/core/kernels/cwise_op_not_equal_to_2.cc -o bazel-out/armeabi-v7a-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/cwise_op_not_equal_to_2.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
clang: error: unable to execute command: Killed
clang: error: clang frontend command failed due to signal (use -v to see invocation)
Android (4691093 based on r316199) clang version 6.0.2 (https://android.googlesource.com/toolchain/clang 183abd29fc496f55536e7d904e0abae47888fc7f) (https://android.googlesource.com/toolchain/llvm 34361f192e41ed6e4e8f9aca80a4ea7e9856f327) (based on LLVM 6.0.2svn)
Target: armv7-none-linux-android
Thread model: posix
InstalledDir: external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin
clang: note: diagnostic msg: PLEASE submit a bug report to http://llvm.org/bugs/ and include the crash backtrace, preprocessed source, and associated run script.
clang: note: diagnostic msg:
********************

PLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:
Preprocessed source(s) and associated run script(s) are located at:
clang: note: diagnostic msg: /tmp/cwise_op_not_equal_to_2-9cbb9b.cpp
clang: note: diagnostic msg: /tmp/cwise_op_not_equal_to_2-9cbb9b.sh
clang: note: diagnostic msg:

********************
Target //tensorflow/lite/examples/l17:Prophet.so failed to build

`


"
31720,"Only the first 30 ops will run on the GPU, and the remaining 1 on the CPU","I am building my codes with tensorflow_lite_gpu.framework on iPhone 7. The codes is just like Google recommends:

```
_model = FlatBufferModel::BuildFromFile(modelPathCString);
        ops::builtin::BuiltinOpResolver resolver;
        InterpreterBuilder(*_model, resolver)(&_interpreter);
        _delegate = NewGpuDelegate(nullptr);  // default config
        _interpreter->ModifyGraphWithDelegate(_delegate);
...//Other codes
```
When I use the model mobilenet_v1_1.0_224.tflite which Google provides, I get warning: `WARNING: 25 cannot be handled by this delegate.  Only the first 30 ops will run on the GPU, and the remaining 1 on the CPU` from console after excuting `_interpreter->ModifyGraphWithDelegate(_delegate)`. But the model deeplabv3_257_mv_gpu.tflite is good which Google provides too. As for my own model, the result is like mobilenet_v1_1.0_224.tflite does. Someone please help me.

PS: Here is my podfile: 
```
platform :ios, '10.0'
target ""SpeechExample"" do
pod 'TensorFlowLiteGpuExperimental'
end
```"
31719,keras.backend.gradients shows error as tf.gradients,"**System information**
tensorflow 2.0b1
windows 10
anaconda python 3.7

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
![20190817164347](https://user-images.githubusercontent.com/27112868/63209122-87e34700-c10f-11e9-8040-38dbdda1d351.png)


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as layers
import tensorflow.keras.backend as K

tf.keras.backend.set_floatx('float64')



def grad( y, x ):
   V =  tf.keras.layers.Lambda( lambda z: K.gradients( z[ 0 ], z[ 1 ] ), output_shape = [1] )( [ y, x ] )
   return V



fixed_input = keras.Input( shape=(2,) )

a = fixed_input * fixed_input

b = grad( a, fixed_input )


model = keras.Model( inputs = [ fixed_input ], outputs = [ b ] )



c = model( tf.constant(2.0, dtype=tf.float64))
```



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

runfile('F:/2019/temp1.py', wdir='F:/2019')
WARNING: Logging before flag parsing goes to stderr.
W0817 16:46:12.144755  7996 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>>: AssertionError: 
Reloaded modules: tmpd51tclm6
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>>: AssertionError: 
Traceback (most recent call last):

  File ""<ipython-input-2-763868c416b6>"", line 1, in <module>
    runfile('F:/2019/temp1.py', wdir='F:/2019')

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 826, in runfile
    execfile(filename, namespace)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""F:/2019/temp1.py"", line 28, in <module>
    c = model( tf.constant(2.0, dtype=tf.float64))

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 712, in __call__
    outputs = self.call(inputs, *args, **kwargs)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 753, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 895, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 712, in __call__
    outputs = self.call(inputs, *args, **kwargs)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\layers\core.py"", line 785, in call
    return self.function(inputs, **arguments)

  File ""F:/2019/temp1.py"", line 12, in <lambda>
    V =  tf.keras.layers.Lambda( lambda z: K.gradients( z[ 0 ], z[ 1 ] ), output_shape = [1] )( [ y, x ] )

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"", line 3568, in gradients
    loss, variables, colocate_gradients_with_ops=True)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 158, in gradients
    unconnected_gradients)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\ops\gradients_util.py"", line 504, in _GradientsHelper
    raise RuntimeError(""tf.gradients is not supported when eager execution ""

RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.
"
31711,pip3 install tensorflow -> Packages Do Not Match Hashes from Requirements File,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster 7/10
- Raspberry Pi 4
- TensorFlow installed from (source or binary): Installed using wheels
- TensorFlow version: 1.13 or whatever is latest on piwheels
- Python version: 3.7
- Installed using virtualenv? pip? conda?: PIP

> THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
>     h5py from https://www.piwheels.org/simple/h5py/h5py-2.9.0-cp37-cp37m-linux_armv7l.whl#sha256=38657cba8b4689142e203ac4cac0b3f64dfe2f5634067f956dbd3abed7ea3a0a (from keras-applications>=1.0.8->tensorflow):
>         Expected sha256 38657cba8b4689142e203ac4cac0b3f64dfe2f5634067f956dbd3abed7ea3a0a
>              Got        2feb5218ab87862f2367a335f4cc315c87fc397c3534e10c08d723cfaeda3a15
> 

I did a basic PIP install:

`pip3 install tensorflow`

It seems there's a mismatch in the info regarding it installing TensorFlow 1.13 but mentioning Tensorboard 1.14???

> pip3 install tensorflow
> Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple
> Collecting tensorflow
>   Downloading https://www.piwheels.org/simple/tensorflow/tensorflow-1.13.1-cp37-none-linux_armv7l.whl (93.2MB)
>     100% || 93.2MB 5.1kB/s 
> Collecting absl-py>=0.7.0 (from tensorflow)
>   Downloading https://www.piwheels.org/simple/absl-py/absl_py-0.7.1-py3-none-any.whl (117kB)
>     100% || 122kB 238kB/s 
> Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)
>   Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)
>     100% || 3.2MB 162kB/s 
> Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.32.3)
> Collecting keras-preprocessing>=1.0.5 (from tensorflow)
>   Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)
>     100% || 51kB 1.8MB/s 
> Collecting wrapt>=1.11.1 (from tensorflow)
>   Downloading https://www.piwheels.org/simple/wrapt/wrapt-1.11.2-cp37-cp37m-linux_armv7l.whl (68kB)
>     100% || 71kB 205kB/s 
> Collecting grpcio>=1.8.6 (from tensorflow)
>   Downloading https://www.piwheels.org/simple/grpcio/grpcio-1.23.0-cp37-cp37m-linux_armv7l.whl (13.7MB)
>     100% || 13.7MB 38kB/s 
> Collecting protobuf>=3.6.1 (from tensorflow)
>   Downloading https://files.pythonhosted.org/packages/0d/2e/d4b1b67c264ce6722def110f2715461e9b4d49647952750c82c6b247bfa6/protobuf-3.9.1-py2.py3-none-any.whl (432kB)
>     100% || 440kB 644kB/s 
> Collecting astor>=0.6.0 (from tensorflow)
>   Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl
> Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.12.0)
> Collecting keras-applications>=1.0.8 (from tensorflow)
>   Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)
>     100% || 51kB 3.6MB/s 
> Collecting termcolor>=1.1.0 (from tensorflow)
>   Downloading https://www.piwheels.org/simple/termcolor/termcolor-1.1.0-py3-none-any.whl
> Collecting gast>=0.2.0 (from tensorflow)
>   Downloading https://www.piwheels.org/simple/gast/gast-0.2.2-py3-none-any.whl
> Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)
>   Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)
>     100% || 491kB 598kB/s 
> Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.2)
> Collecting google-pasta>=0.1.6 (from tensorflow)
>   Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)
>     100% || 61kB 4.2MB/s 
> Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)
>   Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)
>     100% || 92kB 2.9MB/s 
> Requirement already satisfied: werkzeug>=0.11.15 in /usr/lib/python3/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)
> Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.6.1->tensorflow) (40.8.0)
> Collecting h5py (from keras-applications>=1.0.8->tensorflow)
>   Downloading https://www.piwheels.org/simple/h5py/h5py-2.9.0-cp37-cp37m-linux_armv7l.whl (4.6MB)
>     75% |        | 3.4MB 945kB/s eta 0:00:02
> "
31709,Tensorflow 2.0 is much slower than pytorch for large matrix assignment,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow 2.0 beta
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0/cuDNN 7
- GPU model and memory: GeForce RTX 2080 Ti

**Describe the current behavior**
In one of my research, I need to assign values to large sparse matrices. Since tensorflow does not support value assignment/update to large matrices , I have to use a lot of tf.stack()/tf.concat() functions. 

I compared the same function implemented in tensorflow 2.0 and pytorch 1.1.0, and the execution time for tensorflow was much slower than pytorch. The execution times are listed below:
pytorch: 0.0036 secs
tf 2.0    : 0.1734 secs

**Describe the expected behavior**
Is there a way to optimize the tensorflow codes to have comparable performance?

**Code to reproduce the issue**
```
## Tensorflow 2.0
import tensorflow as tf
import time
import numpy as np

def skew(x):
    x = tf.reshape(x, [-1, 1])
    z1 = tf.stack([tf.zeros(1), -x[2], x[1]], 1)
    z2 = tf.stack([x[2], tf.zeros(1), -x[0]], 1)
    z3 = tf.stack([-x[1], x[0], tf.zeros(1)], 1)
    X = tf.concat([z1, z2, z3], 0)
    return X

def propagate(Rot, v, p, g):
    v_skew_rot = tf.matmul(skew(v), Rot)
    p_skew_rot = tf.matmul(skew(p), Rot)

    F0 = tf.zeros([3, 3])
    F1 = tf.concat([F0, skew(g), F0], 0)
    F2 = tf.concat([F0, F0, tf.eye(3)], 0)
    F3 = tf.zeros([9, 3])
    F4 = tf.concat([-Rot, -v_skew_rot, -p_skew_rot], 0)
    F5 = tf.concat([F0, -Rot, F0], 0)
    F6 = tf.zeros([9, 6])
    F7 = tf.concat([F1, F2, F3, F4, F5, F6], 1)
    F = tf.concat([F7, tf.zeros([12, 21])], 0)

    G0 = tf.zeros([3, 12])
    G1 = tf.concat([Rot, v_skew_rot, p_skew_rot, tf.zeros([12, 3])], 0)
    G2 = tf.concat([F0, Rot, F0, tf.zeros([12, 3])], 0)
    G3 = tf.concat([G0, G0, G0, tf.eye(12)], 0)
    G = tf.concat([G1, G2, G3], 1)
    return F, G

if __name__ == '__main__':
    Rot = tf.eye(3)
    v = np.array([0.5, 0, 0], dtype=np.float32)
    p = np.array([1.5, 0, 0], dtype=np.float32)
    g = np.array([0, 0, -9.80655], dtype=np.float32)
    start = time.time()
    P = propagate(Rot, v, p, g)
    print(""Propagate function takes {} secs"".format(time.time() - start))
```

```
## pytorch 1.1.0
import torch
import time
import numpy as np


def skew(x):
    X = torch.Tensor([[0, -x[2], x[1]],
                      [x[2], 0, -x[0]],
                      [-x[1], x[0], 0]])
    return X

def propagate(Rot_prev, v_prev, p_prev, g):
    F = torch.zeros(21, 21)
    G = torch.zeros(21, 18)
    v_skew_rot = skew(v_prev).mm(Rot_prev)
    p_skew_rot = skew(p_prev).mm(Rot_prev)

    F[:3, 9:12] = -Rot_prev
    F[3:6, :3] = skew(g)
    F[6:9, 3:6] = torch.eye(3)
    F[3:6, 12:15] = -Rot_prev
    F[3:6, 9:12] = -v_skew_rot
    F[6:9, 9:12] = -p_skew_rot

    G[:3, :3] = Rot_prev
    G[3:6, 3:6] = Rot_prev
    G[3:6, :3] = v_skew_rot
    G[6:9, :3] = p_skew_rot
    G[9:12, 6:9] = torch.eye(3)
    G[12:15, 9:12] = torch.eye(3)
    G[15:18, 12:15] = torch.eye(3)
    G[18:21, 15:18] = torch.eye(3)
    return F, G

if __name__ == '__main__':
    Rot = torch.eye(3)
    v = np.array([0.5, 0, 0], dtype=np.float32)
    p = np.array([1.5, 0, 0], dtype=np.float32)
    g = np.array([0, 0, -9.80655], dtype=np.float32)
    start = time.time()
    P = propagate(Rot, v, p, g)
    print(""Propagate function takes {} secs"".format(time.time() - start))
```"
31706,TF 2.0 Dataset.shuffle() is deterministic,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): tf-nightly-gpu-2.0-preview==2.0.0.dev20190815
- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview==2.0.0.dev20190815
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`Dataset.shuffle()` returns results in the same order across invocations and across runs.

**Describe the expected behavior**
Previously, `Dataset.shuffle()` would return randomized results.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

(TF 2.0)
```python
import tensorflow as tf
ds = tf.data.Dataset.from_tensor_slices(list(range(10)))
print([x.numpy() for x in ds])
for i in range(10):
    print([x.numpy() for x in ds.shuffle(10)])
```

```
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]
[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]
[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]
[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]
[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]
[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]
[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]
[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]
[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]
[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]
```

(TF 1.14)
```python
import tensorflow as tf
tf.enable_eager_execution()
ds = tf.data.Dataset.from_tensor_slices(list(range(10)))
print([x.numpy() for x in ds])
for i in range(10):
    print([x.numpy() for x in ds.shuffle(10)])
```

```
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[1, 8, 3, 5, 6, 0, 7, 2, 9, 4]
[7, 0, 8, 5, 1, 6, 4, 3, 9, 2]
[4, 5, 3, 0, 7, 1, 2, 6, 9, 8]
[6, 1, 9, 4, 3, 2, 8, 7, 0, 5]
[3, 0, 4, 6, 5, 2, 8, 9, 7, 1]
[5, 9, 3, 4, 0, 6, 7, 1, 2, 8]
[4, 8, 2, 1, 0, 9, 3, 6, 5, 7]
[3, 5, 7, 0, 2, 4, 8, 6, 9, 1]
[8, 0, 6, 2, 7, 3, 4, 5, 9, 1]
[8, 4, 2, 6, 5, 0, 1, 7, 9, 3]
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

cc @martinwicke"
31705, The arguments and returns of tf.keras.layers.GRUCell.call() make no sense at all,"## URL(s) with the issue: https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1615 https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1718

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell

## Description of issue (what needs changing):
The `states` argument to tf.keras.layers.GRUCell.call() is indexed with `h_tm1 = states[0]  # previous memory`, and the function returns `h` and `[h]`, which is the same value?

### Clear description
Why does this occur? Its inconsistent with the pytorch torch.nn.GRUCell implementation (https://pytorch.org/docs/stable/nn.html#grucell). 

I noticed the `states` issue when I was converting a project from pytorch to tf.keras and the same code, with just the GRUCell swapped from pytorch to tf.keras, did not work. The error message was `tensorflow.python.framework.errors_impl.InvalidArgumentError: In[0] is not a matrix. Instead it has shape [200] [Op:MatMul] name: transition/gru_cell/MatMul/`, and the solution was to replace my `hidden` with `[hidden]` for the states parameter. 

Furthermore, when I got my return types, they were a tuple rather than the output. Upon further inspection, the tuple CONTAINED THE SAME VALUE TWICE.

Is there any reason it does this? In the doc this is not explained AT ALL.

### Correct links
https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1615
https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1718

### Parameters defined
`states` parameter is the confusing one in question.

### Returns defined
`return h, [h]` makes no sense

### Raises listed and defined
Irrelevant

### Usage example
self.rnn = tf.keras.layers.GRUCell(num_units)
self.rnn(input, hidden)

where input, hidden = shape(N, num_units) doesn't work. Needs to be changed to:
self.rnn(input, [hidden]) to execute.

Furthermore, on the LHS of self.rnn, rather than just an x = self.rnn(...), I need to do x, _ = self.rnn(...)
Why?

### Submit a pull request?
I would gladly change this if someone would confirm this is an issue."
31701,ImportError: Could not find 'cudart64_100.dll',"Hi everyone, I installed `tensorflow-gpu==2.0.0-beta1` according official documentation, but catch this error:
`ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive`

I made all things from:
- official documentation: https://www.tensorflow.org/install/gpu
- from here: https://medium.com/@teavanist/install-tensorflow-gpu-on-windows-10-5a23c46bdbc7
- Checked `PATH` variable: `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin`
- also have `CUDA_PATH` : `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0` in variables
- file `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin\cudart64_100.dll` exists
- Did system restart

How can I fix that?

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tensorflow-gpu==2.0.0-beta1
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: CUDA 10.0, cuDNN v7.6.2 (July 22, 2019), for CUDA 10.0
- GPU model and memory: MSI 1070 TI

**### UPD**
Windows reinstall solved the issue"
31698,Docs could mention that Dataset sharding is deterministic,"Hi everyone,
Thanks for your work on maintaining and developing Tensorflow.

I wish to raise a suggestion for the documentation of the `tf.data.Dataset` Python API.
In particular, I am referring to the documentation of the `shard` operation.

## URL(s) with the issue:

r1.14 docs: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard

r2.0 docs: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shard

## Description of issue (what needs changing):

### Clear description

From my understanding of the source code, the `shard` operation is **deterministic**. 
i.e. if we apply `shard` on a Dataset A with some fixed values of `num_shards` and `index`, the operation will always return the same subset of Dataset A.

Perhaps the documentation should mention that this operation is deterministic?
This will help readers understand that the sharding does not involve any randomness.
Currently, the docs do not mention this aspect of `shard`'s behaviour.

### Correct links

Is the link to the source code correct? Yes

### Parameters defined

Are all parameters defined and formatted correctly? Yes

### Returns defined

Are return values defined? Yes

### Raises listed and defined

Are the errors defined? Yes

### Usage example

Is there a usage example? Yes

### Request visuals, if applicable

Are there currently visuals? No, but this issue does not require visuals

### Submit a pull request?

I can submit a PR to update the docs - if this is indeed considered a useful fix.

Thanks for your time."
31697,Tensorflow 2 beta - error save model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190731
- Python version: 3

**Describe the current behavior**

When I try to save a model, I obtain the following errror:
```
AssertionError: Tried to export a function which references untracked object Tensor(""StatefulPartitionedCall/args_2:0"", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
```

**Describe the expected behavior**
I would like to save the model.

**Code to reproduce the issue**

Google Colab link: [https://colab.research.google.com/drive/1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp](https://colab.research.google.com/drive/1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp)

Interesting Code:

```
n = 200
df = pd.DataFrame(data={'a': [x for x in range(n)], 'b': [x for x in range(n+10,n+n+10)], 'labels': [int(x%2==0) for x in range(n)]})
df = df.astype({'b': str})

def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  
  labels = dataframe.pop('labels')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

train, test = train_test_split(df, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
train_ds = df_to_dataset(train)
val_ds = df_to_dataset(val, shuffle=False)
test_ds = df_to_dataset(test, shuffle=False)
feature_columns = []
feature_layer_inputs = {}
for c in df.columns:
  if c == 'labels':
    continue
  elif c == 'b':
    el = feature_column.categorical_column_with_vocabulary_list(c, df[c].unique(), default_value=-10)
    el_one_hot = feature_column.indicator_column(el)
    feature_columns.append(el_one_hot)
    feature_layer_inputs[c] = tf.keras.Input(shape=(1,), name=c, dtype=tf.string)
  elif c == 'a':
    feature_columns.append(feature_column.numeric_column(c, default_value=-10))
    feature_layer_inputs[c] = Input(shape=(1,), name=c)
feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
f_layer = feature_layer(feature_layer_inputs)
input = [v for v in feature_layer_inputs.values()]
x = Dense(2048, activation='relu')(f_layer)
x = Dropout(0.5)(x)
out = Dense(1, activation='sigmoid')(x)
model = Model(inputs=[input], outputs=out)
model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['binary_accuracy', 'AUC'])
model.fit(train_ds, validation_data=val_ds, epochs=10, verbose=0)
model.save('aaa.model')
```

"
31696,tf.gather not supported when both params dtype is int and params is ResourceVariable,"`tf.gather` has both GPU and CPU kernels for params dtype int64 / int32, however, when the params is a tensor wrapped in a variable it does not work when dtype is int32 or int64.

Wrapping the params input in e.g. a `tf.identity` solves the issue, but the behaviour seems like a bug.

Minimal case to reproduce:

```
import tensorflow as tf

print(tf.__version__)

tensor_float = tf.range(10, dtype=tf.float32)
tensor_int = tf.range(10, dtype=tf.int64)

var_float = tf.Variable(tensor_float)
var_int = tf.Variable(tensor_int)

for i in [tensor_float, tensor_int, var_float, var_int]:
    try:
        tf.gather(i, tf.constant([[1]], dtype=tf.int64))
        print('worked for {}'.format(i))
    except tf.errors.NotFoundError as e:
        print('didn\'t work for {}'.format(i))
        print(e)
```

Output:

```
2.0.0-beta1
worked for [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
worked for [0 1 2 3 4 5 6 7 8 9]
worked for <tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)>
didn't work for <tf.Variable 'Variable:0' shape=(10,) dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>
No registered 'ResourceGather' OpKernel for GPU devices compatible with node {{node ResourceGather}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: Tindices=DT_INT64, batch_dims=0, dtype=DT_INT64, validate_indices=true
	.  Registered:  device='XLA_CPU'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_BFLOAT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]
  device='XLA_GPU'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]
  device='XLA_CPU_JIT'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_BFLOAT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]
  device='XLA_GPU_JIT'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]
  device='GPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_QINT32]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_QINT32]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_QUINT8]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_QUINT8]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_QINT8]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_QINT8]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_RESOURCE]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_RESOURCE]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_STRING]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_STRING]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_BFLOAT16]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_BFLOAT16]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_INT8]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_INT8]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_UINT8]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_UINT8]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_INT16]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_INT16]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_UINT16]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_UINT16]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_INT32]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_INT32]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_INT64]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_INT64]; Tindices in [DT_INT32]
 [Op:ResourceGather] name: Gather/
```"
31695,Is it possible to dynamically switch between CPU and GPU?,"I have a code that does training/test together.

However, the eager execution mode seems taking more GPU memory than I have when inference is taken place.

I need to train the data in GPU and do inference in CPU. How can I do this in TensorFlow? Thank you!"
31694,Check failed: array->has_shape(),"Hi,

First, Thanks for all your work! i'm struggling to export my tf model to tflite, seeing the error, i think i got 2 problems : 
- input format
- unsupported ops
I can't find a way to solve this, maybe the issue is to do a more compatible tflite export when freezing the graph (close to something you did for export_tflite_ssd_graph.py but not sure i can make something equivalent for my case) 

if you can help me for documentations and or good workaround...

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): googlecolab Window 10 x64 in conda venv
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.14.0


**Provide the text output from tflite_convert**

ConverterError: TOCO failed. See console for info.
2019-08-16 13:17:37.784025: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794406: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794453: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794480: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794526: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794563: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794592: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794619: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794644: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794669: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794689: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794714: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794740: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794765: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.794790: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.819192: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: RFFT
2019-08-16 13:17:37.819259: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: ComplexAbs
2019-08-16 13:17:37.819678: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-08-16 13:17:37.819727: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-08-16 13:17:37.819759: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-08-16 13:17:37.819787: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-08-16 13:17:37.819814: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.819844: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.819871: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.819896: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.819921: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.819942: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-08-16 13:17:37.819963: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.819983: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-08-16 13:17:37.820005: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3
2019-08-16 13:17:37.820048: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-08-16 13:17:37.820076: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond
2019-08-16 13:17:37.820127: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit
2019-08-16 13:17:37.820214: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3
2019-08-16 13:17:37.820241: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3
2019-08-16 13:17:37.820319: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3
2019-08-16 13:17:37.820434: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3
2019-08-16 13:17:37.829208: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 420 operators, 721 arrays (0 quantized)
2019-08-16 13:17:37.841887: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 401 operators, 686 arrays (0 quantized)
2019-08-16 13:17:37.854995: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 401 operators, 686 arrays (0 quantized)
2019-08-16 13:17:37.897923: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 171 operators, 321 arrays (0 quantized)
2019-08-16 13:17:37.903043: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 153 operators, 287 arrays (0 quantized)
2019-08-16 13:17:37.905901: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 150 operators, 281 arrays (0 quantized)
2019-08-16 13:17:37.908277: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 150 operators, 281 arrays (0 quantized)
2019-08-16 13:17:37.909972: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 150 operators, 281 arrays (0 quantized)
2019-08-16 13:17:37.912697: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 704 bytes, theoretical optimal value: 512 bytes.
2019-08-16 13:17:37.913694: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EQUAL, EXPAND_DIMS, FILL, FLOOR_DIV, FULLY_CONNECTED, GATHER, LESS, LOG, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, REVERSE_V2, SHAPE, SPARSE_TO_DENSE, SPLIT_V, STRIDED_SLICE, SUB, SUM, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, ComplexAbs, Enter, Exit, LoopCond, Merge, RFFT, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.
Traceback (most recent call last):
  File ""/usr/local/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EQUAL, EXPAND_DIMS, FILL, FLOOR_DIV, FULLY_CONNECTED, GATHER, LESS, LOG, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, REVERSE_V2, SHAPE, SPARSE_TO_DENSE, SPLIT_V, STRIDED_SLICE, SUB, SUM, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, ComplexAbs, Enter, Exit, LoopCond, Merge, RFFT, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.


here is the link to the shared notebook : https://colab.research.google.com/drive/1FhMBJHynRVfd_o-q2JH6Wn867YmnomjY

the frozen model .pb is available there : https://we.tl/t-1mwjdnJfxe

Thank you for helping, you're doing a great job

Raphael
"
31693,Error in example of tensorflow lite,"Hello, I got an error in code while tried to run object detection. Let me know how to fix this
![Screenshot (15)](https://user-images.githubusercontent.com/39325207/63173591-d6d0a400-c05d-11e9-807b-42d1c4cc626b.png)
 code"
31692,Rectified Adam in tensorflow,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):
1.13
No 


**Describe the feature and the current behavior/state.**
Pytorch has already Rectified Adam implemented
Can you add it ?


**Will this change the current api? How?**

**Who will benefit with this feature?**
All people in the world.

**Any Other info.**
"
31691,TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. ,"import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
tf.reset_default_graph()
import os
import sys
import shutil
import random
import scipy
from datetime import datetime
import progressbar
from image_data_handler_joint_multimodal import ImageDataHandler
from resnet18 import ResNet
from layer_blocks import *
from utils import flat_shape, count_params, log_file
from imgaug import augmenters as iaa
from keras.objectives import categorical_crossentropy
from keras.layers import GRU, Dense
from keras.metrics import categorical_accuracy
from keras import backend as K
import numpy as np
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
os.environ['CUDA_VISIBLE_DEVICES']='0,2,3'

"""""" Configuration setting """"""
tf.set_random_seed(7)
config=tf.ConfigProto()

# Data-related params
if len(sys.argv) != 3:
    print(""The script requires 2 arguments: (1) the dataset root directory and (2) the parameters root directory."")
#dataset_root_dir = sys.argv[1] #'/mnt/datasets/ocid_dataset'
#params_root_dir = sys.argv[2] #'/mnt/params/models'
dataset_root_dir='/home/users/xdlan/myexperiment/rcfusion_master/ocid_dataset_owncloud'
params_root_dir='/home/users/xdlan/myexperiment/rcfusion_master/resnet18_ocid_params'


dataset_train_dir_rgb = dataset_root_dir + '/ARID20_crops/squared_rgb/'
dataset_val_dir_rgb = dataset_root_dir + '/ARID10_crops/squared_rgb/'
params_dir_rgb = params_root_dir + '/resnet18_ocid_rgb++_params.npy'


dataset_train_dir_depth = dataset_root_dir + '/ARID20_crops/surfnorm++/'
dataset_val_dir_depth = dataset_root_dir + '/ARID10_crops/surfnorm++/'
params_dir_depth = params_root_dir + '/resnet18_ocid_surfnorm++_params.npy'


train_file = dataset_root_dir + '/split_files_and_labels/arid20_clean_sync_instances.txt'
val_file = dataset_root_dir + '/split_files_and_labels/arid10_clean_sync_instances.txt'

# Log params
log_dir = '../log/'
if not os.path.exists(log_dir):
    os.makedirs(log_dir)
log = [""epoch"", ""train_loss"", ""val_loss"", ""val_acc""]
tensorboard_log = '/tmp/tensorflow/'

# Solver params
learning_rate = [[0.0001]]
num_epochs = 50
batch_size = [[16]]
num_neurons = [[100]]
l2_factor = [[0.0]]
maximum_norm = [[4]]
dropout_rate = [[0.4]]

depth_transf = [[256]]
transf_block = transformation_block_v1

# Checkpoint dir
checkpoint_dir = ""/tmp/my_caffenet/""
if not os.path.isdir(checkpoint_dir): os.mkdir(checkpoint_dir)

# Input/Output
num_classes = 49
img_size = [224, 224]
num_channels = 3


"""""" Online data augmentation """"""

def random_choice(start, end , _multiply ):
    start = start * _multiply
    end = end * _multiply
    num = random.randrange(start,end + 1 ,1)
    #print (""il num e'"",num/_multiply)
    return float( num / _multiply)
def x_y_random_image(image):
    width = image.shape[1] 
    hight = image.shape[0]

    border_x = int( ( 256 * 5 ) / 100.0)
    border_y = int( ( 256 * 9 ) / 100.0)

    pos_x = random.randrange(0 + border_x , width - border_x , 1)
    pos_y = random.randrange(0 + border_y, hight - border_y , 1)
    #print (""la pos e' "", pos_x , pos_y)
    return pos_x , pos_y

def data_aug(batch , batch_depth):
    num_img = batch.shape[0]
    list = []
    list_depth = []
    for i in range(num_img):
        val_fliplr = random.randrange(0,2,1) #in questo modo il due non e compreso e restituisce i valori 0 o 1 
        list.extend([iaa.Fliplr( val_fliplr )])
        list_depth.extend([iaa.Fliplr( val_fliplr )])
        
        val_fliplr = random.randrange(0,2,1) #in questo modo il due non e compreso e restituisce i valori 0 o 1 
        list.extend([iaa.Flipud( val_fliplr )])
        list_depth.extend([iaa.Flipud( val_fliplr )])
        
        val_scala = random.randrange(5 , 11 , 1)
        val = float (val_scala / 10.0) 
        list.extend([iaa.Affine( val , mode = 'edge')])
        list.extend([iaa.Affine( 10.0 / val_scala , mode = 'edge')])
        list_depth.extend([iaa.Affine( val , mode = 'edge')])
        list_depth.extend([iaa.Affine( 10.0 / val_scala , mode = 'edge')])
        
        val_rotation = random.randrange( -180, 181 , 90)
        list.extend( [ iaa.Affine( rotate = val_rotation ,mode = 'edge') ] )
        list_depth.extend( [ iaa.Affine( rotate = val_rotation ,mode = 'edge') ] )
        
        augseq = iaa.Sequential(list)
        batch[i] = augseq.augment_image( batch[i] )
        augseq_depth = iaa.Sequential(list)
        batch_depth[i] = augseq_depth.augment_image( batch_depth[i])
        
        list=[]
        list_depth = []


"""""" Loop for gridsearch of hyper-parameters """"""

set_params = [lr+nn+bs+aa+mn+do+dt for lr in learning_rate for nn in num_neurons for bs in batch_size for aa in l2_factor for mn in maximum_norm for do in dropout_rate for dt in depth_transf]

for hp in set_params:

    lr = hp[0]
    nn = hp[1]
    bs = hp[2]
    aa = hp[3]
    mn = hp[4]
    do = hp[5]
    dt = hp[6]

    """""" Data management """"""

    # Place data loading and preprocessing on the cpu
    #with tf.device('/cpu:0'):
    dataset_train_dir = [dataset_train_dir_rgb, dataset_train_dir_depth]
    dataset_val_dir = [dataset_val_dir_rgb, dataset_val_dir_depth]

    tr_data = ImageDataHandler(
        train_file,
        data_dir=dataset_train_dir,
        params_dir=params_root_dir,
        img_size=img_size,
        batch_size=bs,
        num_classes=num_classes,
        shuffle=True,
        random_crops=False)

    val_data = ImageDataHandler(
        val_file,
        data_dir=dataset_val_dir,
        params_dir=params_root_dir,
        img_size=img_size,
        batch_size=bs,
        num_classes=num_classes,
        shuffle=False,
        random_crops=False)

    # Create a re-initializable iterator given the dataset structure
    # no need for two different to deal with training and val data,
    # just two initializers
    iterator = tf.data.Iterator.from_structure(tr_data.data.output_types,
                                       tr_data.data.output_shapes)
    next_batch = iterator.get_next()

    # Ops for initializing the two different iterators
    training_init_op = iterator.make_initializer(tr_data.data)
    validation_init_op = iterator.make_initializer(val_data.data)

    # Get the number of training/validation steps per epoch
    tr_batches_per_epoch = int(np.floor(tr_data.data_size/bs))
    val_batches_per_epoch = int(np.floor(val_data.data_size/bs))

    init_op_rgb = {'training': training_init_op, 'validation': validation_init_op}
    batches_per_epoch = {'training': tr_batches_per_epoch, 'validation': val_batches_per_epoch}

    config.gpu_options.allow_growth=True
    config.gpu_options.per_process_gpu_memory_fraction=0.8
    config.allow_soft_placement=True
    
    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)
  
    # Log vars
    log_epoch = []
    log_train_loss = []
    log_val_loss = []
    log_val_acc = []


    # Start Tensorflow session
    #with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, device_count={'GPU':1})) as sess:
    with tf.Session(config=config) as sess: 
        """""" Network definition """"""        

        ## RGB branch

        # TF placeholder for graph input and output
        x_rgb = tf.placeholder(tf.float32, [None, img_size[0], img_size[1], 3])
        x_depth = tf.placeholder(tf.float32, [None, img_size[0], img_size[1], 3])
        y = tf.placeholder(tf.float32, [None, num_classes])

        x_rgb = tf.reshape(x_rgb, [-1, img_size[0], img_size[1], 3])
        x_depth = tf.reshape(x_depth, [-1, img_size[0], img_size[1], 3])
        y = tf.reshape(y, [-1, num_classes])

        keep_prob = tf.placeholder(tf.float32)
        training_phase = tf.placeholder(tf.bool)

        # Max norm
        ckn = np.inf
        rkc = mn#np.inf
        rrc = mn#np.inf
        dkc = mn#np.inf
        conv_kernel_constraint = tf.keras.constraints.MaxNorm(ckn, axis=[0,1,2])
        rnn_kernel_constraint = tf.keras.constraints.MaxNorm(rkc, axis=[0,1])
        rnn_recurrent_constraint = tf.keras.constraints.MaxNorm(rrc, axis=1)
        dense_kernel_constraint = tf.keras.constraints.MaxNorm(dkc, axis=0)

        # Initialize models
        with tf.variable_scope('rgb', reuse=None):
            model_rgb = ResNet(x_rgb, num_classes, mode='rgb')
        with tf.variable_scope('depth', reuse=None):
            model_depth = ResNet(x_depth, num_classes, mode='depth')

        # Extract features
        res1_rgb = model_rgb.relu1
        inter_res2_rgb = model_rgb.inter_res2
        res2_rgb = model_rgb.res2
        inter_res3_rgb = model_rgb.inter_res3
        res3_rgb = model_rgb.res3
        inter_res4_rgb = model_rgb.inter_res4
        res4_rgb = model_rgb.res4
        inter_res5_rgb = model_rgb.inter_res5
        res5_rgb = model_rgb.res5

        pool2_flat_rgb = model_rgb.pool2_flat

        res1_depth = model_depth.relu1
        inter_res2_depth = model_depth.inter_res2
        res2_depth = model_depth.res2
        inter_res3_depth = model_depth.inter_res3
        res3_depth = model_depth.res3
        inter_res4_depth = model_depth.inter_res4
        res4_depth = model_depth.res4
        inter_res5_depth = model_depth.inter_res5
        res5_depth = model_depth.res5

        pool2_flat_depth = model_depth.pool2_flat

        # Conv1x1
        with tf.variable_scope('conv1x1'):
            
            #depth_transf = 64
            
            relu_conv1x1_res1_rgb = transformation_block(res1_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_res1')
            relu_conv1x1_inter_res2_rgb = transf_block(inter_res2_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_inter_res2')
            relu_conv1x1_res2_rgb = transf_block(res2_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_res2') 
            relu_conv1x1_inter_res3_rgb = transf_block(inter_res3_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_inter_res3')
            relu_conv1x1_res3_rgb = transf_block(res3_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_res3')
            relu_conv1x1_inter_res4_rgb = transf_block(inter_res4_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_inter_res4')
            relu_conv1x1_res4_rgb = transf_block(res4_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_res4')
            relu_conv1x1_inter_res5_rgb = transf_block(inter_res5_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_inter_res5')
            relu_conv1x1_res5_rgb = transf_block(res5_rgb, dt, conv_kernel_constraint, training_phase, 'redux_rgb_res5')
            
            relu_conv1x1_res1_depth = transformation_block(res1_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_res1')
            relu_conv1x1_inter_res2_depth = transf_block(inter_res2_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_inter_res2')
            relu_conv1x1_res2_depth = transf_block(res2_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_res2')
            relu_conv1x1_inter_res3_depth = transf_block(inter_res3_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_inter_res3')
            relu_conv1x1_res3_depth = transf_block(res3_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_res3')
            relu_conv1x1_inter_res4_depth = transf_block(inter_res4_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_inter_res4')
            relu_conv1x1_res4_depth = transf_block(res4_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_res4')
            relu_conv1x1_inter_res5_depth = transf_block(inter_res5_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_inter_res5')
            relu_conv1x1_res5_depth = transf_block(res5_depth, dt, conv_kernel_constraint, training_phase, 'redux_depth_res5')
 
        relu_conv1x1_res1_rgb = tf.reshape(relu_conv1x1_res1_rgb, [-1, flat_shape(relu_conv1x1_res1_rgb)])
        relu_conv1x1_inter_res2_rgb = tf.reshape(relu_conv1x1_inter_res2_rgb, [-1, flat_shape(relu_conv1x1_inter_res2_rgb)])
        relu_conv1x1_res2_rgb = tf.reshape(relu_conv1x1_res2_rgb, [-1, flat_shape(relu_conv1x1_res2_rgb)])
        relu_conv1x1_inter_res3_rgb = tf.reshape(relu_conv1x1_inter_res3_rgb, [-1, flat_shape(relu_conv1x1_inter_res3_rgb)])
        relu_conv1x1_res3_rgb = tf.reshape(relu_conv1x1_res3_rgb, [-1, flat_shape(relu_conv1x1_res3_rgb)])
        relu_conv1x1_inter_res4_rgb = tf.reshape(relu_conv1x1_inter_res4_rgb, [-1, flat_shape(relu_conv1x1_inter_res4_rgb)])
        relu_conv1x1_res4_rgb = tf.reshape(relu_conv1x1_res4_rgb, [-1, flat_shape(relu_conv1x1_res4_rgb)])
        relu_conv1x1_inter_res5_rgb = tf.reshape(relu_conv1x1_inter_res5_rgb, [-1, flat_shape(relu_conv1x1_inter_res5_rgb)])
        relu_conv1x1_res5_rgb = tf.reshape(relu_conv1x1_res5_rgb, [-1, flat_shape(relu_conv1x1_res5_rgb)])

        relu_conv1x1_res1_depth = tf.reshape(relu_conv1x1_res1_depth, [-1, flat_shape(relu_conv1x1_res1_depth)])
        relu_conv1x1_inter_res2_depth = tf.reshape(relu_conv1x1_inter_res2_depth, [-1, flat_shape(relu_conv1x1_inter_res2_depth)])
        relu_conv1x1_res2_depth = tf.reshape(relu_conv1x1_res2_depth, [-1, flat_shape(relu_conv1x1_res2_depth)])
        relu_conv1x1_inter_res3_depth = tf.reshape(relu_conv1x1_inter_res3_depth, [-1, flat_shape(relu_conv1x1_inter_res3_depth)])
        relu_conv1x1_res3_depth = tf.reshape(relu_conv1x1_res3_depth, [-1, flat_shape(relu_conv1x1_res3_depth)])
        relu_conv1x1_inter_res4_depth = tf.reshape(relu_conv1x1_inter_res4_depth, [-1, flat_shape(relu_conv1x1_inter_res4_depth)])
        relu_conv1x1_res4_depth = tf.reshape(relu_conv1x1_res4_depth, [-1, flat_shape(relu_conv1x1_res4_depth)])
        relu_conv1x1_inter_res5_depth = tf.reshape(relu_conv1x1_inter_res5_depth, [-1, flat_shape(relu_conv1x1_inter_res5_depth)])
        relu_conv1x1_res5_depth = tf.reshape(relu_conv1x1_res5_depth, [-1, flat_shape(relu_conv1x1_res5_depth)])

        # RGB and depth pipelines' merge point
        relu_conv1x1_res1 = tf.concat([relu_conv1x1_res1_rgb, relu_conv1x1_res1_depth], axis=1)
        relu_conv1x1_inter_res2 = tf.concat([relu_conv1x1_inter_res2_rgb, relu_conv1x1_inter_res2_depth], axis=1)
        relu_conv1x1_res2 = tf.concat([relu_conv1x1_res2_rgb, relu_conv1x1_res2_depth], axis=1)
        relu_conv1x1_inter_res3 = tf.concat([relu_conv1x1_inter_res3_rgb, relu_conv1x1_inter_res3_depth], axis=1)
        relu_conv1x1_res3 = tf.concat([relu_conv1x1_res3_rgb, relu_conv1x1_res3_depth], axis=1)
        relu_conv1x1_inter_res4 = tf.concat([relu_conv1x1_inter_res4_rgb, relu_conv1x1_inter_res4_depth], axis=1)
        relu_conv1x1_res4 = tf.concat([relu_conv1x1_res4_rgb, relu_conv1x1_res4_depth], axis=1)
        relu_conv1x1_inter_res5 = tf.concat([relu_conv1x1_inter_res5_rgb, relu_conv1x1_inter_res5_depth], axis=1)
        relu_conv1x1_res5 = tf.concat([relu_conv1x1_res5_rgb, relu_conv1x1_res5_depth], axis=1)

        rnn_input = tf.stack([relu_conv1x1_res1, relu_conv1x1_inter_res2, relu_conv1x1_res2, relu_conv1x1_inter_res3, relu_conv1x1_res3, relu_conv1x1_inter_res4, relu_conv1x1_res4, relu_conv1x1_inter_res5, relu_conv1x1_res5], axis=1)


        # Recurrent net
        with tf.variable_scope(""rnn""):
            rnn_h = GRU(nn, activation='tanh', dropout=do, recurrent_dropout=do, name=""rnn_h"", 
                        kernel_constraint=rnn_kernel_constraint, recurrent_constraint=rnn_recurrent_constraint)(rnn_input)
            preds = Dense(num_classes, activation='softmax', kernel_constraint=dense_kernel_constraint)(rnn_h)

        # Include keras-related metadata in the session
        K.set_session(sess)

        # Define trainable variables
        trainable_variables_rnn = [v for v in tf.trainable_variables(scope=""rnn"")]
        trainable_variables_conv1x1 = [v for v in tf.trainable_variables(scope=""conv1x1"")]
        #trainable_variables_rgb = [v for v in tf.trainable_variables(scope=""rgb"")]
        #trainable_variables_depth = [v for v in tf.trainable_variables(scope=""depth"")]

        # Define the training and validation opsi
        global_step = tf.Variable(0, trainable=False)
        increment_global_step = tf.assign(global_step, global_step+1)
        lr_boundaries = [int(num_epochs*tr_batches_per_epoch*0.5)]#, int(num_epochs*tr_batches_per_epoch*0.6)]
        lr_values = [lr, lr/10]
        decayed_lr = tf.train.piecewise_constant(global_step, lr_boundaries, lr_values)

        # L2-regularization
        alpha_rnn = aa
        alpha_conv1x1 = aa
        l2_rnn = tf.add_n([tf.nn.l2_loss(tv_rnn) for tv_rnn in trainable_variables_rnn
                                                    if 'bias' not in tv_rnn.name])*alpha_rnn
        l2_conv1x1 = tf.add_n([tf.nn.l2_loss(tv_conv1x1) for tv_conv1x1 in trainable_variables_conv1x1
                                                    if 'bias' not in tv_conv1x1.name])*alpha_conv1x1
        

        # F2-norm
        #rgb_nodes = [relu_conv1x1_inter_res2_rgb, relu_conv1x1_res2_rgb, relu_conv1x1_inter_res3_rgb, relu_conv1x1_res3_rgb, relu_conv1x1_inter_res4_rgb, relu_conv1x1_res4_rgb, relu_conv1x1_inter_res5_rgb, relu_conv1x1_res5_rgb]
        rgb_nodes = [relu_conv1x1_res1_rgb, relu_conv1x1_inter_res2_rgb, relu_conv1x1_res2_rgb, relu_conv1x1_inter_res3_rgb, relu_conv1x1_res3_rgb, relu_conv1x1_inter_res4_rgb, relu_conv1x1_res4_rgb, relu_conv1x1_inter_res5_rgb, relu_conv1x1_res5_rgb]
        #depth_nodes = [relu_conv1x1_inter_res2_depth, relu_conv1x1_res2_depth, relu_conv1x1_inter_res3_depth, relu_conv1x1_res3_depth, relu_conv1x1_inter_res4_depth, relu_conv1x1_res4_depth, relu_conv1x1_inter_res5_depth, relu_conv1x1_res5_depth]
        depth_nodes = [relu_conv1x1_res1_depth, relu_conv1x1_inter_res2_depth, relu_conv1x1_res2_depth, relu_conv1x1_inter_res3_depth, relu_conv1x1_res3_depth, relu_conv1x1_inter_res4_depth, relu_conv1x1_res4_depth, relu_conv1x1_inter_res5_depth, relu_conv1x1_res5_depth]
        reg = tf.Variable(0.0, trainable=False)
        
        #sigm_reg = 0.0001*tf.sigmoid(((12*tf.cast(global_step,tf.float32))/(num_epochs*tr_batches_per_epoch))-6, name='reg_sigmoid')
        sigm_reg = 1.0
        reg_values = [sigm_reg, 0.0]
        decayed_reg = tf.train.piecewise_constant(global_step, lr_boundaries, reg_values)
        #recompute_reg = tf.assign(reg, sigm_reg)
        reg_lambda = [0.0*decayed_reg, 0.0*decayed_reg, 1e-6*decayed_reg, 1e-5*decayed_reg, 1e-5*decayed_reg, 1e-4*decayed_reg, 1e-4*decayed_reg, 0.0*decayed_reg, 0.0*decayed_reg]
        lr_mult_conv1x1 = decayed_reg
       
        # Loss
        loss_l2 = l2_rnn + l2_conv1x1
        loss_cls = tf.reduce_mean(categorical_crossentropy(y, preds))
        loss = loss_cls #+ loss_l2
        **train_step_rnn = tf.keras.optimizers.RMSprop(lr=decayed_lr).get_updates(loss=loss, params=trainable_variables_rnn)
        train_step_conv1x1 = tf.keras.optimizers.RMSprop(lr=decayed_lr*lr_mult_conv1x1).get_updates(loss=loss, params=trainable_variables_conv1x1)**

        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)                                                                           
        with tf.control_dependencies(update_ops):
            train_step = tf.group(train_step_rnn, train_step_conv1x1, increment_global_step)
            #train_step = tf.group(train_step_rnn, train_step_conv1x1, train_step_rgb, train_step_depth, increment_global_step, recompute_reg)
        accuracy = tf.reduce_mean(categorical_accuracy(y, preds))   

        # Create summaries for Tensorboard
        tf.summary.scalar(""loss_cls"", loss_cls)
        tf.summary.scalar(""loss_l2"", loss_l2)
        tf.summary.scalar(""loss"", loss)
        tf.summary.scalar(""accuracy"", accuracy)
        tf.summary.scalar(""learning_rate"", decayed_lr)
        tf.summary.scalar(""lambda"", decayed_reg)
        summary_op = tf.summary.merge_all()

        name = str(lr) + '_' + str(bs) + '_' +  str(nn)
        train_writer = tf.summary.FileWriter(tensorboard_log + name + '/train/', graph = sess.graph)
        val_writer = tf.summary.FileWriter(tensorboard_log + name + '/val/')

        # Initialize all variables
        sess.run(tf.global_variables_initializer())

        # Load the pretrained weights into the non-trainable layer
        model_rgb.load_params(sess, params_dir_rgb, trainable=False)
        model_depth.load_params(sess, params_dir_depth, trainable=False)

        print(""\nHyper-parameters: lr={}, #neurons={}, bs={}, l2={}, max_norm={}, dropout_rate={}"".format(lr,nn,bs,aa,mn,do))     
        print(""Number of trainable parameters = {}"".format(count_params(trainable_variables_rnn)+count_params(trainable_variables_conv1x1)))    
        print(""\n{} Generate features from training set"".format(datetime.now()))
         
        tb_train_count=0        
        tb_val_count = 0

        # Loop over number of epochs
        num_samples = 0
        # Training set     
        sess.run(training_init_op)

        # Progress bar setting
        bar = progressbar.ProgressBar(maxval=tr_batches_per_epoch, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])
        bar.start()
        train_loss = 0
        for i in range(tr_batches_per_epoch):
            bar.update(i+1)
            tb_train_count+=1
            rgb_batch, depth_batch, label_batch = sess.run(next_batch)

            num_samples += np.shape(rgb_batch)[0]
            feed_dict = {x_rgb: rgb_batch, x_depth: depth_batch, y: label_batch, keep_prob: (1-do), training_phase: True, K.learning_phase(): 1}
            batch_loss, summary = sess.run([loss, summary_op], feed_dict=feed_dict)
            train_loss += batch_loss
            train_writer.add_summary(summary, tb_train_count)

        bar.finish()

        train_loss /= tr_batches_per_epoch #num_samples
        print(""training loss = {}\n"".format(train_loss))

        val_acc = 0
        val_loss = 0
        num_samples = 0  

        sess.run(validation_init_op)
        for i in range(val_batches_per_epoch):

            tb_val_count+=1
            rgb_batch, depth_batch, label_batch = sess.run(next_batch)
            num_samples += np.shape(rgb_batch)[0]

            feed_dict = {x_rgb: rgb_batch, x_depth: depth_batch, y: label_batch, keep_prob: 1.0, training_phase: False, K.learning_phase(): 0}
            batch_loss, batch_acc, summary = sess.run([loss, accuracy, summary_op], feed_dict=feed_dict)

            val_loss+=batch_loss
            val_acc+=batch_acc
            val_writer.add_summary(summary, tb_val_count*(tr_batches_per_epoch/val_batches_per_epoch))

        val_loss /= val_batches_per_epoch #num_samples
        val_acc /= val_batches_per_epoch #num_samples
        print(""\n{} Validation loss : {}, Validation Accuracy = {:.4f}"".format(datetime.now(), val_loss, val_acc))


        # Loop over number of epochs
        for epoch in range(num_epochs):
            num_samples = 0
                 
            # Training set
            print(""\nEpoch: {}/{}"".format(epoch + 1, num_epochs))
            sess.run(training_init_op)

            # Progress bar setting
            bar = progressbar.ProgressBar(maxval=tr_batches_per_epoch, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])
            bar.start()
            train_loss = 0
            for i in range(tr_batches_per_epoch):
                bar.update(i+1)
                tb_train_count+=1
                rgb_batch, depth_batch, label_batch = sess.run(next_batch)

                num_samples += np.shape(rgb_batch)[0] 
                # apply data augmentation
                data_aug(rgb_batch,depth_batch)
                feed_dict = {x_rgb: rgb_batch, x_depth: depth_batch, y: label_batch, keep_prob: (1-do), training_phase: True, K.learning_phase(): 1}
                batch_loss, _, summary = sess.run([loss, train_step, summary_op], feed_dict=feed_dict)
                train_loss += batch_loss                
                train_writer.add_summary(summary, tb_train_count)
                
            bar.finish()
            train_loss /= tr_batches_per_epoch #num_samples
            print(""training loss = {}\n"".format(train_loss))
    
            if (epoch+1)%1 == 0:
                sess.run(validation_init_op)
                num_samples = 0
                val_loss = 0
                val_acc = 0
                tsne_feat_rgb, tsne_feat_depth, tsne_feat_rgbd = None, None, None 
                pred_feat, gt_feat = None, None

                for i in range(val_batches_per_epoch):

                    tb_val_count+=1
                    rgb_batch, depth_batch, label_batch = sess.run(next_batch)
                    num_samples += np.shape(rgb_batch)[0]
                    feed_dict = {x_rgb: rgb_batch, x_depth: depth_batch, y: label_batch, keep_prob: 1.0, training_phase: False, K.learning_phase(): 0}
                    batch_loss, batch_acc, summary, tsne_batch_rgb, tsne_batch_depth, tsne_batch_rgbd, batch_preds = sess.run([loss, 
                        accuracy, summary_op, pool2_flat_rgb, pool2_flat_depth, rnn_h, preds], 
                        feed_dict=feed_dict)

                    if tsne_feat_rgb is None:
                        tsne_feat_rgb = tsne_batch_rgb
                        tsne_feat_depth = tsne_batch_depth
                        tsne_feat_rgbd = tsne_batch_rgbd

                        pred_feat = batch_preds
                        gt_feat = label_batch
                    else:
                        tsne_feat_rgb = np.append(tsne_feat_rgb, tsne_batch_rgb, axis=0)
                        tsne_feat_depth = np.append(tsne_feat_depth, tsne_batch_depth, axis=0)
                        tsne_feat_rgbd = np.append(tsne_feat_rgbd, tsne_batch_rgbd, axis=0)

                        pred_feat = np.append(pred_feat, batch_preds, axis=0)
                        gt_feat = np.append(gt_feat, label_batch, axis=0)

                    val_loss+=batch_loss
                    val_acc+=batch_acc
                    val_writer.add_summary(summary, tb_val_count*(tr_batches_per_epoch/val_batches_per_epoch))                

                val_loss /= val_batches_per_epoch #num_samples
                val_acc /= val_batches_per_epoch #num_samples

                print(""\n{} Validation loss : {}, Validation Accuracy = {:.4f}"".format(datetime.now(), val_loss, val_acc))

                if (epoch + 1) == num_epochs:
                    with open(""../features/tsne_feat_rgb.npy"", ""w+"") as f:
                        np.save(f, tsne_feat_rgb)
                    with open(""../features/tsne_feat_depth.npy"", ""w+"") as f:
                        np.save(f, tsne_feat_depth)
                    with open(""../features/tsne_feat_rgbd.npy"", ""w+"") as f:
                        np.save(f, tsne_feat_rgbd)
                    with open(""../features/feat_preds.npy"", ""w+"") as f:
                        final_preds = np.zeros([pred_feat.shape[0],2])
                        final_preds[:,0] = np.argmax(pred_feat, axis=1)
                        final_preds[:,1] = np.argmax(gt_feat, axis=1)
                        np.save(f, final_preds)

                del tsne_feat_rgb, tsne_feat_depth, tsne_feat_rgbd

                log_epoch.append(epoch)
                log_train_loss.append(train_loss)
                log_val_loss.append(val_loss)
                log_val_acc.append(val_acc)

            # Early stopping for ill-posed params combination
            if ((epoch == 0) and (val_acc < 0.2)) or ((epoch == 9) and (val_acc < 0.5)) or np.isnan(train_loss):
                print(""Training stopped due to poor results or divergence: validation loss = {}"".format(val_acc))
                break
                
        history_callback = {log[0]:log_epoch, log[1]:log_train_loss, log[2]:log_val_loss, log[3]:log_val_acc}
        log_file(history_callback, hp)

    #tf.reset_default_graph()
    shutil.rmtree(tensorboard_log)





IPython 7.5.0 -- An enhanced Interactive Python.

runfile('/home/users/xdlan/myexperiment/rcfusion_master/code/train_and_eval.py', wdir='/home/users/xdlan/myexperiment/rcfusion_master/code')
WARNING: Logging before flag parsing goes to stderr.
W0816 21:23:15.258742 140077442987776 deprecation.py:323] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Using TensorFlow backend.
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 17689183678501733557
, name: ""/device:XLA_CPU:0""
device_type: ""XLA_CPU""
memory_limit: 17179869184
locality {
}
incarnation: 14027451888762908702
physical_device_desc: ""device: XLA_CPU device""
, name: ""/device:XLA_GPU:0""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 11081742533860949395
physical_device_desc: ""device: XLA_GPU device""
, name: ""/device:XLA_GPU:1""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 12973763672644918679
physical_device_desc: ""device: XLA_GPU device""
, name: ""/device:XLA_GPU:2""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 8653398481352517287
physical_device_desc: ""device: XLA_GPU device""
, name: ""/device:XLA_GPU:3""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 4684263568990964685
physical_device_desc: ""device: XLA_GPU device""
, name: ""/device:GPU:0""
device_type: ""GPU""
memory_limit: 11100815360
locality {
  bus_id: 1
  links {
    link {
      device_id: 1
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 2874952132781754303
physical_device_desc: ""device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1""
, name: ""/device:GPU:1""
device_type: ""GPU""
memory_limit: 11970700903
locality {
  bus_id: 1
  links {
    link {
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 16227167641656641306
physical_device_desc: ""device: 1, name: TITAN Xp, pci bus id: 0000:03:00.0, compute capability: 6.1""
, name: ""/device:GPU:2""
device_type: ""GPU""
memory_limit: 2625634304
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      device_id: 3
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 1497904576540092906
physical_device_desc: ""device: 2, name: TITAN Xp, pci bus id: 0000:82:00.0, compute capability: 6.1""
, name: ""/device:GPU:3""
device_type: ""GPU""
memory_limit: 2545942528
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      device_id: 2
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 18157037722769851594
physical_device_desc: ""device: 3, name: TITAN Xp, pci bus id: 0000:83:00.0, compute capability: 6.1""
]
The script requires 2 arguments: (1) the dataset root directory and (2) the parameters root directory.
W0816 21:23:16.917065 140077442987776 deprecation.py:323] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0816 21:23:17.117713 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/train_and_eval.py:177: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(dataset)`.
W0816 21:23:17.118752 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/train_and_eval.py:178: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(dataset)`.
W0816 21:23:17.124696 140077442987776 deprecation.py:323] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(iterator)`.
W0816 21:23:17.125599 140077442987776 deprecation.py:323] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(iterator)`.
W0816 21:23:17.126456 140077442987776 deprecation.py:323] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_classes(iterator)`.
W0816 21:23:17.165664 140077442987776 deprecation.py:506] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0816 21:23:19.061071 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/layer_blocks.py:276: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.AveragePooling2D instead.
W0816 21:23:19.161852 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/layer_blocks.py:286: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0816 21:23:19.317812 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/layer_blocks.py:287: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
W0816 21:23:19.389662 140077442987776 deprecation.py:323] From /home/users/xdlan/myexperiment/rcfusion_master/code/layer_blocks.py:291: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0816 21:23:21.193200 140077442987776 deprecation_wrapper.py:119] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0816 21:23:21.194731 140077442987776 deprecation_wrapper.py:119] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

W0816 21:23:21.272165 140077442987776 deprecation_wrapper.py:119] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

W0816 21:23:21.281805 140077442987776 deprecation.py:506] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0816 21:23:21.627247 140077442987776 deprecation_wrapper.py:119] From /home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.

Traceback (most recent call last):

  File ""<ipython-input-1-aa215b7e30b4>"", line 1, in <module>
    runfile('/home/users/xdlan/myexperiment/rcfusion_master/code/train_and_eval.py', wdir='/home/users/xdlan/myexperiment/rcfusion_master/code')

  File ""/home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 710, in runfile
    execfile(filename, namespace)

  File ""/home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 101, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/home/users/xdlan/myexperiment/rcfusion_master/code/train_and_eval.py"", line 374, in <module>
    train_step_rnn = tf.keras.optimizers.RMSprop(lr=decayed_lr).get_updates(loss=loss, params=trainable_variables_rnn)

  File ""/home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/rmsprop.py"", line 106, in __init__
    super(RMSprop, self).__init__(name, **kwargs)

  File ""/home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 257, in __init__
    if kwargs[k] < 0:

  File ""/home/users/xdlan/.conda/envs/xdlan/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 690, in __bool__
    raise TypeError(""Using a `tf.Tensor` as a Python `bool` is not allowed. ""

TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor."
31690,Import of TF Failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): with pip
- TensorFlow version: 1.14 and 2.0.0-beta
- Python version: 3.7
- Installed using virtualenv? pip? conda?: installed with pip
- Bazel version (if compiling from source): not installed
- GCC/Compiler version (if compiling from source): Not used
- CUDA/cuDNN version: just using CPU
- GPU model and memory:  just using CPU



**Describe the problem** I cant import tensorflow. I guess there have been installation issues. I have this problem with both the old and the new version.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow==2.0.0-beta1
python
import tensorflow as tf



**Any other info / logs**
Traceback (most recent call last):
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Daniel\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.
Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors
for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
31689,"I need to use tensorflow2.0 beta1 GPUs one computer to write a GAN ,but get error as follows. Because of BatchNorm layers, how to solve this problem?","```python
def G_D_fn(epoch, batch_data):
train_data, feature = batch_data
with tf.GradientTape(persistent=True) as tape:
    generates = G( train_data)
    logits_fake = D(generates) #If I use vgg, it works normal. If I use D, it has error.
    logits_real = D(feature)
    d_loss = tl.cost.sigmoid_cross_entropy(logits, tf.ones_like(logits_fake))
    g_gan_loss = 1e-3 * tl.cost.sigmoid_cross_entropy(logits_fake,tf.ones_like(logits_fake)
grad = tape.gradient(g_loss, G.trainable_weights)
g_optimizer.apply_gradients(zip(grad, G.trainable_weights))
    grad = tape.gradient(d_loss, D.trainable_weights)
    d_optimizer.apply_gradients(zip(grad, D.trainable_weights))
    return d_loss, g_gan_loss

print(""Begin training..."")
devices = ['/device:GPU:{}'.format(i) for i in range(num_gpu)] 
strategy = tf.distribute.MirroredStrategy(devices)
with strategy.scope():
    train_log_dir = 'logs/gradient_tape/train'
    train_summary_writer = tf.summary.create_file_writer(train_log_dir)
    # obtain models
    G = get_G((batch_size, None, None, 3))
    D = get_D((batch_size, None, None, 3)) 
    lr_v = tf.Variable(lr_init)
    g_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)
d_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)

    G.train()
D.train()

    train_ds = get_train_data()
dist_train_ds = strategy.experimental_distribute_dataset(train_ds)

    n_batch_epoch = round(n_epoch // batch_size)  
    for epoch in range(n_epoch):    
        total_mse_loss = 0.0
        batch = 0
        for batch_data in dist_train_ds:
            batch += 1                 
            per_d_loss, per_g_gan_loss = strategy.experimental_run_v2(G_D_fn, args=(epoch, batch_data, ))
            total_d_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_d_loss, axis = None)
            total_g_gan_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_g_gan_loss, axis = None)

        d_loss = total_d_loss/batch
        g_gan_loss = total_g_gan_loss/batch

        print(""g_gan:{:.6f}, d_loss: {:.9f}"".format(g_gan_loss, d_loss))

```
I have find the problem.There are BatchNorm layers in my discriminator. When I delete them, the program can work. Or when I didn't use tf.distribute.MirroredStrategy, the program can work. Do tf.distribute.MirroredStrategy permit batch norm ?"
31688,Different result when evaluating tflite model from python and from android,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nokia 7 plus
- TensorFlow installed from (source or binary): don't know (I am using Google Colab)
- TensorFlow version (use command below): 1.14.0 (git version: v1.14.0-0-g87989f6959 - compiler version: 7.4.0)
- Python version: 3.6.8
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): c++ (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: 10.0
- GPU model and memory: NVIDIA-SMI 418.67 - 15079MiB

**Describe the current behavior**

I've trained a model using keras (in google colab), then I've converted the keras h5 file to tflite using `TFLiteConverter` (also in google colab) by using this code:

```python
import tensorflow as tf

tflite_converter = tf.lite.TFLiteConverter.from_keras_model_file(<KERAS_H5_MODEL_PATH>)

tflite_model = tflite_converter.convert()

with open(<KERAS_TFLITE_DEST_PATH>, 'wb') as tflite_model_file:
    tflite_model_file.write(tflite_model)
```

After that, I've run the tflite model (in google colab also):

```python
import numpy as np
import tensorflow as tf

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

input_data = np.ones((1, 256, 256, 3), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

output = [interpreter.get_tensor(output_details[0]['index'])[0], interpreter.get_tensor(output_details[1]['index'])[0]]
```

The tflite model can be found [here](https://drive.google.com/file/d/1cxXHtBGJk5h__6Elf4ZLvtJW8mdRs4_f/view?usp=sharing)

However, when I run the tflite model in an android app (using the same input data) y get different outputs:

(This code is based on [this example](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) from tensorflow)

```java
AssetFileDescriptor fileDescriptor = assets.openFd(<MODEL_TFLITE_FILENAME>);
FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
FileChannel fileChannel = inputStream.getChannel();
long startOffset = fileDescriptor.getStartOffset();
long declaredLength = fileDescriptor.getDeclaredLength();
MappedByteBuffer model = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);

Interpreter.Options interpreterOptions = new Interpreter.Options().setNumThreads(1);
tfLite = new Interpreter(model, interpreterOptions);

float[][][][] floatValues = new float[1][256][256][3];

for (int i = 0; i < 256; i++) {
    for (int j = 0; j < 256; j++) {
        for (int k = 0; k < 3; k++) {
            floatValues[0][i][j][k] = 1.0f;
        }
    }
}

Object[] inputArray = {floatValues};

output1 = new float[1][8][8][18];
output2 = new float[1][16][16][18];

Map<Integer, Object> outputMap = new HashMap<>();
outputMap.put(0, output1);
outputMap.put(1, output2);

tfLite.runForMultipleInputsOutputs(inputArray, outputMap);
```

The output values I get in the android code are not the same as the outputs obtained with python code.

```
// This outputs correspond to output1[0][0][0]

 Python results    |  Android results
 0 >   0.06118933  |  0 >   0.061190486
 1 > - 0.50384498  |  1 > - 0.50384396
 2 > - 0.30500048  |  2 > - 0.30500013
 3 > - 0.18725708  |  3 > - 0.18725668
 4 > -12.56872463  |  4 > -12.568727
 5 > - 3.53239870  |  5 > - 3.5323968
 6 >   0.26756036  |  6 >   0.26756087
 7 > - 0.63708508  |  7 > - 0.6370841
 8 > - 0.11959708  |  8 > - 0.119596675
 9 > - 0.42219949  |  9 > - 0.42219883
10 > -12.26699734  | 10 > -12.266998
11 > - 3.42504168  | 11 > - 3.4250417
12 >   0.29714739  | 12 >   0.2971481
13 > - 0.76750284  | 13 > - 0.7675022
14 > - 0.13859260  | 14 > - 0.13859189
15 > - 0.02096577  | 15 > - 0.020965554
16 > -12.78137684  | 16 > -12.781382
17 > - 3.34643984  | 17 > - 3.3464384
```

Here the results are similar, there is a small difference. However I've also tried with a real input (comming from an image), and in this case, the results in android are totally different.

This is a model to detect objects in images (yolo v3 tiny) and I've constated that the values obtained with python code are the correct ones. When I run the python code with a real input (an image) it detects the objects.

- Why do I get different outputs in android?
- Do I have to do something special to run model in android?
- Do I have to do something different in model conversion from H5 to tflite?
- Does it has something to do with the way as java works with floats?
- Should I use a quantized version of the model?

**Describe the expected behavior**

The expected behavior is to get the same result running the model in android that when I run the model in python

**Code to reproduce the issue**

Already provided in the description of the current behavior

**Other info / logs**

The tflite model can be found [here](https://drive.google.com/file/d/1cxXHtBGJk5h__6Elf4ZLvtJW8mdRs4_f/view?usp=sharing)"
31687,TPUStrategy incompatibility with tf.io.read_file,"**System information**

I am using Colaboratory and Google cloud.

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.9.168-1+deb9u5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): unknown
- TensorFlow version (use command below): 1.14
- Python version: 3.5.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

The script ends with a segmentation fault or abort.

**Describe the expected behavior**

Just a clean run.

**Code to reproduce the issue**

```python
# Importing all necessary libraries
import tensorflow as tf
import cv2
import random as rnd
import os

if int(tf.__version__.split('.')[0]) == 1:
    print(tf.__version__)
    tf.enable_eager_execution()

@tf.function
def _read_test(filename):
    img_raw = tf.io.read_file(tf.squeeze(filename))
    return img_raw

import numpy as np
image = np.array([[rnd.randint(0, 255) for _ in range(936)] for _ in range(1024)])
cv2.imwrite('0.png', image);

raw = _read_test(tf.constant('0.png'))

tf.keras.backend.clear_session()

if 'TPU_NAME' in os.environ:
    TPU_WORKER = 'grpc://' + os.environ['TPU_NAME']
    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER)
    tf.config.experimental_connect_to_host(resolver.master())
    tf.contrib.distribute.initialize_tpu_system(resolver)
    strategy = tf.contrib.distribute.TPUStrategy(resolver)
elif 'COLAB_TPU_ADDR' in os.environ:
    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']
    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER)
    tf.config.experimental_connect_to_host(resolver.master())
    tf.contrib.distribute.initialize_tpu_system(resolver)
    strategy = tf.contrib.distribute.TPUStrategy(resolver)
else:
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

#print(strategy)

with strategy.scope():
    pass

print('success')
```

**Other info / logs**

The previous code is part of an image processing Neural Network. The image files are read while running the code to minimize the disk usage. The code runs smoothly in a CPU or GPU environment, however it crashes in a TPU one."
31686,[TF 2.0] Saving model containing categorical_column_with_vocabulary_list feature_column fails,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.12.1-8794-ge36271a61d 2.0.0-dev20190814
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

Saving a trained keras model containing a `categorical_column_with_vocabulary_list` feature_column results in the error:

 `AssertionError: Tried to export a function which references untracked object Tensor(""StatefulPartitionedCall/args_1:0"", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.`

**Describe the expected behavior**

It should be possible to save a model containing all types of feature_columns.

**Code to reproduce the issue**

```
import tensorflow as tf


category = tf.constant([""A"", ""B"", ""A"", ""C"", ""C"", ""A""])
label = tf.constant([1, 0, 1, 0, 0, 0])

ds = tf.data.Dataset.from_tensor_slices(({""category"": category}, label))
ds = ds.batch(2)

fc_category = tf.feature_column.indicator_column(
    tf.feature_column.categorical_column_with_vocabulary_list(
        ""category"", vocabulary_list=[""A"", ""B"", ""C""]
    )
)
feature_layer = tf.keras.layers.DenseFeatures([fc_category])

model = tf.keras.Sequential(
    [
        feature_layer,
        tf.keras.layers.Dense(10, activation=""relu""),
        tf.keras.layers.Dense(1, activation=""sigmoid""),
    ]
)

model.compile(
    optimizer=""adam"", loss=""binary_crossentropy"", metrics=[""accuracy""]
)

model.fit(ds, epochs=2)

tf.keras.models.save_model(model, ""model"")
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31685,How to append placeholder tf.gradient and things like that to a multiprocess.Manager.list,"I want to share some objects between different process, but my class has some tensorflow object in it ,and triggered an error message

Process Process-7:
Traceback (most recent call last):
  File ""C:\Users\zkx74\Anaconda3\lib\multiprocessing\process.py"", line 297, in _bootstrap
    self.run()
  File ""C:\Users\zkx74\Anaconda3\lib\multiprocessing\process.py"", line 99, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Users\zkx74\PycharmProjects\DDPG_QuantTrade\MADDPG.py"", line 162, in init_nn
    agent_list.append(ACModel(actor, None, env, current_stock_state, current_agent_state))
  File ""<string>"", line 2, in append
  File ""C:\Users\zkx74\Anaconda3\lib\multiprocessing\managers.py"", line 795, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File ""C:\Users\zkx74\Anaconda3\lib\multiprocessing\connection.py"", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""C:\Users\zkx74\Anaconda3\lib\multiprocessing\reduction.py"", line 51, in dumps
    cls(buf, protocol).dump(obj)
TypeError: can't pickle _thread.RLock objects


the code is here: https://paste.ubuntu.com/p/Z54XbRkk8h/"
31684,TimeDistributed does not propagate mask and prevents inner Sequential from propagating mask,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14.0, 2.0.0b
- Python version: 3.6.8

**Code to reproduce the issue**
This fails due to failed assertion:

```
import tensorflow as tf

def check_mask(inputs, mask=None):
    assert mask is not None
    return inputs

model = tf.keras.layers.TimeDistributed(tf.keras.models.Sequential([
    tf.keras.layers.Masking(0),
    tf.keras.layers.Lambda(check_mask)
]))

model(tf.convert_to_tensor([[[0]]]))
```

I would expect the mask to be propagated, as is normal in `Sequential`. The `TimeDistributed` seems to somehow prevent the inner `Sequential` from doing so.

This also fails:
```
import tensorflow as tf

def check_mask(inputs, mask=None):
    assert mask is not None
    return inputs

model = tf.keras.models.Sequential([
    tf.keras.layers.TimeDistributed(tf.keras.layers.Masking(0)),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Lambda(check_mask)),
])

model(tf.convert_to_tensor([[[0]]]))
```

**Other info / logs**

This might be related to another issue I also submitted: https://github.com/tensorflow/tensorflow/issues/31638

"
31683,TFRT Built From Source does not Improve Inference Speed,"[convertTFRT_frozenGraph.zip](https://github.com/tensorflow/tensorflow/files/3509287/convertTFRT_frozenGraph.zip)

<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - 
- TensorFlow installed from (source or binary): built from source 
- TensorFlow version: **1.14**
- Python version: **2.7**
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: **10.0.0 / 7.4.2**
- GPU model and memory: 1080Ti



**Describe the problem**
I have built TensorFlow from Sources, and checked the ""yes"" option when asked about compiling with TensorRT. Graph builds, trt_only nodes exist, inference speed on SSD + FPN model is not optimised (actually slightly worse than the original, pre-optimization frozen_graph.pb

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Build from source tensorFlow 1.14
Create optimised graph from original frozen_graph.pb
Infer over optimised graph ====> **no speedup whatsoever**

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Uploaded conversion script.
Notice I try to get outputs from all FPN levels (not just final feature map), in order to use those in another LSTM-type network classifier.

----------------------------------------------------------------------
Prints from optimization script : 
2019-08-16 12:25:39.310289: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: tf_graph
2019-08-16 12:25:39.310333: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 2836 nodes (-1660), 4183 edges (-1854), time = 728.073ms.
2019-08-16 12:25:39.310337: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 2880 nodes (44), 4255 edges (72), time = 143.931ms.
2019-08-16 12:25:39.310341: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 2880 nodes (0), 4255 edges (0), time = 167.55ms.
2019-08-16 12:25:39.310345: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   TensorRTOptimizer: Graph size after: 1907 nodes (-973), 3218 edges (-1037), time = 7242.89307ms.
graph_size(MB)(native_tf): 123.3
graph_size(MB)(trt): 336.9
num_nodes(native_tf): 4496
num_nodes(tftrt_total): 1907
**num_nodes(trt_only): 94**
time(s) (trt_conversion): 10.6979
**number of TRT ops in the converted graph :  94**


-----------------------------------------------------------------------------------------------------------------
Just tried using tf-nightly-gpu 1.15.0.dev20190816 , with Cuda 10.1 and cudnn 7.6, and the inference time is even worse! (with mode trt_only nodes as shows the print : )

graph_size(MB)(native_tf): 123.3
graph_size(MB)(trt): 159.7
num_nodes(native_tf): 4496
num_nodes(tftrt_total): 1894
num_nodes(trt_only): 102
time(s) (trt_conversion): 8.1412
number of TRT ops in the converted graph :  102

-----------------------------------------------------------------------------------------------------------------

Average [1:100] random images Inference Speeds : 
original .pb inference speed : **0.028s**
tf1.14+trt5 compiled from source inference speed : **0.029**
tf1.15+trt installed with pip : **0.032**

**None** of these had errors when building or optimizing the graph :)



"
31681,Debugging tf.Keras Models with TFDBG get TypeError: Fetch argument None has invalid type <class 'NoneType'>,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution ( Linux Ubuntu 18.04):
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): tensorflow-gpu  1.14.0     
- Python version: python 3.7.1
- CUDA/cuDNN version: CUDA 10.1  cuDNN 7.5
- GPU model and memory: GTX 2060 

**Describe the current behavior**

I want to use debugging tf.keras model, but got an error message like:
```sh
Traceback (most recent call last):
  File ""./debug_keras.py"", line 82, in <module>
    vae.add_loss(loss_fn(x_in, x_out, , ))
  File ""/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 917, in add_loss
    new_layers = base_layer_utils.create_keras_history(symbolic_loss)
  File ""/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 200, in create_keras_history
    _, created_layers = _create_keras_history_helper(tensors, set(), [])
  File ""/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 244, in _create_keras_history_helper
    constants[i] = backend.function([], op_input)([])
  File ""/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py"", line 3292, in __call__
    run_metadata=self.run_metadata)
  File ""/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 628, in wrapped_runner
    callable_runner_args=feed_values)
  File ""/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 569, in run
    run_metadata=run_metadata)
  File ""/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1158, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File ""/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 474, in __init__
    self._fetch_mapper = _FetchMapper.for_fetch(fetches)
  File ""/home/zqh/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 261, in for_fetch
    type(fetch)))
TypeError: Fetch argument None has invalid type <class 'NoneType'>

```

**Describe the expected behavior**

It should be work...

**Code to reproduce the issue**

run this python script, then type in:` run -t 10` will get an error message...
```python
import tensorflow.python as tf
from tensorflow.python import keras as k
from tensorflow.python.keras import layers as kl
from tensorflow.python.keras import activations as ka
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm
from scipy.special import expit
from tensorflow.python import debug as tfdebug

config = tf.ConfigProto()
config.gpu_options.allow_growth = True

k.backend.set_session(tfdebug.LocalCLIDebugWrapperSession(tf.Session(config=config)))
# k.backend.set_session(tf.Session(config=config))

(x_train, y_train), (x_test, y_test) = k.datasets.fashion_mnist.load_data()
x_train = np.expand_dims(x_train, -1) / 255.
x_test = np.expand_dims(x_test, -1) / 255.

image_size = 28
input_shape = (image_size, image_size, 1)
batch_size = 100
kernel_size = 3
filters = 16
latent_dim = 2  # 2
epochs = 30
tf.set_random_seed(9102)


def encoder_fn(inputs, filters):
    x = inputs
    for i in range(2):
        filters *= 2
        x = kl.Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x)
    x = kl.Flatten()(x)
    x = kl.Dense(32, activation='relu')(x)
     = kl.Dense(latent_dim)(x)
     = kl.Dense(latent_dim)(x)
    return , 


def sampling(args):
    """"""  """"""
    ,  = args
     = tf.random_normal(shape=tf.shape())
    return  + tf.exp( / 2) * 


def decoder_fn(z, filters):
    x = kl.Dense(7 * 7 * 32, activation='relu')(z)
    x = kl.Reshape((7, 7, 32))(x)
    for i in range(2):
        x = kl.Conv2DTranspose(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x)
        filters //= 2
    x = kl.Conv2DTranspose(1, kernel_size, activation=None, padding='same')(x)
    return x


def loss_fn(inputs, outputs, , ):
    # 
    xent_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=x_in, logits=x_out), axis=[1, 2, 3])
    # xent_loss = tf.reduce_sum(k.backend.binary_crossentropy(x_in, x_out), axis=[1, 2, 3])
    kl_loss = - 0.5 * tf.reduce_sum(1 +  - tf.square() - tf.exp(), axis=-1)
    vae_loss = tf.reduce_mean(xent_loss + kl_loss)
    return vae_loss


x_in = k.Input(shape=(image_size, image_size, 1))
,  = encoder_fn(x_in, filters)
z = kl.Lambda(sampling, output_shape=(latent_dim,))([, ])

latent_inputs = k.Input(shape=(latent_dim,), dtype=tf.float32)
outputs = decoder_fn(latent_inputs, filters)
decoder = k.Model(latent_inputs, outputs)
x_out = decoder(z)


encoder = k.Model(x_in, )

vae = k.Model(x_in, x_out)
vae.add_loss(loss_fn(x_in, x_out, , ))
vae.compile(k.optimizers.Nadam(0.001))
vae.fit(x=x_train, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_test, None))
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31680,quantization: errror during quantizing bert model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux version 3.10.107-1-tlinux2_kvm_guest-0048, Red Hat 4.8.2-16
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
i want to use quantize_graph.py to quantize bert model under tensorflow/tool/quantization on git branch r1.11, but load generated pb file generated error
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
i use the script to convert the model
```
python quantize_graph.py --input=/root/batch-bert-op/2layer_model.pb --output_node_names=""classifier/predict_probability"" --output=/root/batch-bert-op/eight_bit.pb --mode=eightbit
```
in which 2layer_model.pb is the original pb and eight_bit.pb is the generated pb. 2layer_model.pb can be loaded and get inference result successfully but eight_bit.pb produce error. the code to load the pb file is:
```
with tf.Session() as sess:
    g = tf.Graph().as_default()
    pb_file = sys.argv[1]
    with open(pb_file, ""rb"") as f:
        g_def = tf.GraphDef()
        g_def.ParseFromString(f.read())
        _ = tf.import_graph_def(g_def, name="""")
```
after i remove this line in quantize_graph.py I can load quantized pb successfully but inference take 4s while the original pb takes only 200ms
```
self.set_input_graph(graph_util.remove_training_nodes(self.input_graph, protected_nodes=output_node_names))
```
probability the error is introduced by remove_training_nodes, but without it, inference time is tremendous.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
The full traceback is below:
```
WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

-I/root/anaconda3/envs/py36tf113/lib/python3.6/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1
2019-08-16 19:09:44.358947: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-08-16 19:09:44.384488: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494140000 Hz
2019-08-16 19:09:44.384818: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7f2a68c2d470 executing computations on platform Host. Devices:
2019-08-16 19:09:44.384885: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #213: KMP_AFFINITY: cpuid leaf 11 not supported - decoding legacy APIC ids.
OMP: Info #149: KMP_AFFINITY: Affinity capable, using global cpuid info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-7
OMP: Info #156: KMP_AFFINITY: 8 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #159: KMP_AFFINITY: 1 packages x 8 cores/pkg x 1 threads/core (8 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 4 
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 5 
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 6 
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 7 
OMP: Info #250: KMP_AFFINITY: pid 9830 tid 9830 thread 0 bound to OS proc set 0
2019-08-16 19:09:44.419205: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Traceback (most recent call last):
  File ""/root/anaconda3/envs/py36tf113/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 426, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node bert/embeddings/cond/Slice/begin}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""pb_inference_test.py"", line 122, in <module>
    _ = tf.import_graph_def(g_def, name="""")
  File ""/root/anaconda3/envs/py36tf113/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/root/anaconda3/envs/py36tf113/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 430, in import_graph_def
    raise ValueError(str(e))
ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node bert/embeddings/cond/Slice/begin}}



```
"
31679,"tf.subtract got different result in python , java","code: out = tf.subtract(input, output)
got different result in python vs java

the version is same ,   even the 1.9.0 ,1.13.1, 1.14.0
"
31678,"Error ""swig.exe failed: error executing command"" when build Tensorflow on MSVC","Hi All,

I encountered the following error when building Tensorflow on MSVC, which looks like a configuration issue, but I don't know what was missing. Can you help me? I attached the build log. Thank you very much.

Environment: VS2017 + Windows Server 2016 +Python3

Build log: [Tensorflow_build.log](https://github.com/tensorflow/tensorflow/files/3508725/Tensorflow_build.log)

**Repro steps:**
1. git clone  https://github.com/tensorflow/tensorflow F:\tensorflow\src
2. install msys2 under F:\tensorflow\tools
3. open VS 2017 tools command
4. cd F:\tensorflow\src
5. set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\
6. set PATH=F:\tensorflow\tools;%path%
7. set PATH=F:\tensorflow\tools\msys64\usr\bin;%path%
8. yes """" 2>nul | python ./configure.py
9. bazel build --config=opt --subcommands //tensorflow/tools/pip_package:build_pip_package

**Failures:**
SUBCOMMAND: # //tensorflow/core/kernels:eigen_contraction_kernel_with_mkl [action 'Compiling tensorflow/core/kernels/eigen_contraction_kernel.cc']
cd C:/users/cpptestlocal/_bazel_cpptestlocal/cy65jqax/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\MSBuild\15.0\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\FSharp\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.17763.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\\MSBuild\15.0\bin;C:\windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\Tools\;;C:\windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python/Python37/python.exe
    SET PYTHON_LIB_PATH=C:/Python/Python37/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\CPPTESTLOCAL\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TMP=C:\Users\CPPTESTLOCAL\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.16.27023/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/mkl_dnn /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/mkl_dnn/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/include /Iexternal/mkl_dnn/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src /Iexternal/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/common /Iexternal/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu /Iexternal/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/gemm /Iexternal/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/xbyak /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/eigen_contraction_kernel_with_mkl/eigen_contraction_kernel.obj /c tensorflow/core/kernels/eigen_contraction_kernel.cc
**ERROR: F:/git_projects/tensorflow/tensorflow/tensorflow/lite/python/interpreter_wrapper/BUILD:58:1: SWIGing tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.i failed (Exit -1073741515): swig.exe failed: error executing command 
  cd C:/users/cpptestlocal/_bazel_cpptestlocal/cy65jqax/execroot/org_tensorflow**
bazel-out/x64_windows-opt/bin/external/swig/swig.exe -c++ -python -module tensorflow_wrap_interpreter_wrapper -o bazel-out/x64_windows-opt/bin/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.cc -outdir bazel-out/x64_windows-opt/bin/tensorflow/lite/python/interpreter_wrapper -Iexternal/eigen_archive -Iexternal/swig -Ibazel-out/x64_windows-opt/bin -Ibazel-out/x64_windows-opt/bin/external/local_config_python -Iexternal/gemmlowp -Iexternal/com_google_absl -Iexternal/flatbuffers -Iexternal/arm_neon_2_x86_sse -Iexternal/farmhash_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.i
Execution platform: @bazel_tools//platforms:host_platform
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 599.811s, Critical Path: 126.23s
INFO: 1791 processes: 1791 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully"
31676,Can't import  tokenizer_from_json,"When I  use 'keras.preprocessing.text.tokenizer_from_json', can't find.
I check keras/preprocessing/text.py, find there is no tokenizer_from_json;
Then add ""tokenizer_from_json = text.tokenizer_from_json"", is ok;
![image](https://user-images.githubusercontent.com/54168679/63150263-ec7fa280-c038-11e9-9ac5-63018957ad79.png)
and add ""from tensorflow.python.keras.preprocessing.text import tokenizer_from_json""  in 
""site-packages/tensorflow/_api/v1/keras/preprocessing/text/__init__.py""
![image](https://user-images.githubusercontent.com/54168679/63257680-ebef4200-c2ac-11e9-8c3b-e0e69146e343.png)
"
31671, TF-TRT: Assertion `mParams.k > 0' failed.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.14
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:T4

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Seeing the following error:
```
2019-08-16 03:21:42.299651: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Parameter check failed at: ../builder/Layers.cpp::TopKLayer::2009, condition: k > 0 && k <= MAX_TOPK_K
b_engine-0: ../builder/Layers.cpp:2048: virtual bool nvinfer1::TopKLayer::validate(const std::vector<nvinfer1::TensorForm>&, const nvinfer1::NetworkLayer::ValidationContext&) const: Assertion `mParams.k > 0' failed.

```
**Describe the expected behavior**

What is this error for  ? How to stop this from happening ? 

**Code to reproduce the issue**

Try to create a snippet.

**Other info / logs**

"
31670,Make Tensorflow lite android project using tflite of ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03 model,"Hello, everyone.
Recently I try to make android project of tensorflow lite object detection.
I have successfully converted to tflite file from http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz, which uses 640*640 input image size.
But I have memory overflow error on running my project on the android mobile.
In my previous test, I can run 300*300, 224*224 tflite model on my android mobile.
I use libtensorflowlite_gpu_jni.so and libtensorflowlite_jni.so distributed.
Could anyone tell me the maximum value of tensorflow lite input image size?
If we can run 640*640 tflite model, what should I fix the android code?
Thanks in advance.


 "
31669,Failed to build pip package with syntax error ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version: master branch
- Python version: 3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
when I try to build the tensorflow,with command
```
bazel-bin/tensorflow/tools/pip_package/build_pip_package $build_dir
```
 I got the error 
```
bazel-bin/tensorflow/tools/pip_package/build_pip_package: line 263: syntax error near unexpected token `done'
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel-bin/tensorflow/tools/pip_package/build_pip_package $build_dir
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31668,"Models with tf.keras.layers.BatchNormalisation layers give errors when frozen, or are frozen incorrectly and cannot be loaded","This issue continues on from #31331 with a more wide ranging scope

**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 1903
- TensorFlow installed from: pip
- TensorFlow version: 1.13.1 / 1.14.0 / tf-nightly
- Python version: 3.7.3
- GPU model and memory: RTX 2080 Ti 

**Problem**

Freezing a model with tf.keras.layers.BatchNormalization layers either gives an error, or does not freeze correctly and gives an error when loading, depending on the tensorflow version.

**Method**

- Freezing: Save a model with tf.keras.layers.BatchNormalization layers using `tf.saved_model.simple_save` and then freeze it using `tensorflow.python.tools.freeze_graph.freeze_graph`.
- Loading: Load the frozen model using tf.import_graph_def()

**Results**

*1.13.1*
- Freezing: no error
- Loading: `ValueError: Input 0 of node batch_normalization/cond/ReadVariableOp/Switch was passed float from batch_normalization/gamma:0 incompatible with expected resource`

*1.14.0*
- Freezing: `ValueError: Tensor name 'batch_normalization/cond/ReadVariableOp/Switch:1' is invalid.`
- Loading: n/a

*tf-nightly  1.15.0-dev20190815*
- Freezing: no error
- Loading: `Node 'batch_normalization/cond/ReadVariableOp/Switch_3' has an _output_shapes attribute inconsistent with the GraphDef for output #0: Shapes must be equal rank, but are 1 and 0`

**Code to reproduce**

Colab gist is here: https://colab.research.google.com/gist/geometrikal/da64b13d8a579bc46c005e981d9bc051/tf_31331_freezing_savedmodel.ipynb

```
import tensorflow as tf
import tensorflow.keras.backend as K
import os
import datetime
import shutil
from tensorflow.python.tools import freeze_graph
from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input, Activation, BatchNormalization, Lambda
from tensorflow.keras.models import Model
from tensorflow.python.platform import gfile

# Clear the session
K.clear_session()

# TF version
print(""TF version is "" + tf.__version__)

# Create model
print('# Creating model')
def create_model():
    inputs = Input(shape=(128, 128, 1))
    x = Conv2D(4, (3, 3))(inputs)
    x = BatchNormalization()(x)
    # x = Lambda((lambda x: tf.layers.batch_normalization(x)))(x)
    x = Activation('relu')(x)
    x = Flatten()(x)
    x = Dense(5, activation='softmax')(x)
    model = Model(inputs, x, name='test')
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = create_model()

# Remove old frozen graph directory
if os.path.exists('/content/frozen/'):
  shutil.rmtree('/content/frozen/')

# Create saved model
print('# Saving model')
save_dir = ""/content/frozen/""
tf.saved_model.simple_save(K.get_session(),
                           save_dir,
                           inputs={""input"": model.inputs[0]},
                           outputs={""output"": model.outputs[0]})

# Freeze graph from saved model checkpoint
print('# Freezing graph')
freeze_graph.freeze_graph(None,
                          None,
                          None,
                          None,
                          model.outputs[0].op.name,
                          None,
                          None,
                          os.path.join(save_dir, ""frozen_model.pb""),
                          False,
                          """",
                          input_saved_model_dir=save_dir)

# Try to load frozen graph
print('# Loading graph')
source = ""/content/frozen/frozen_model.pb""
session = K.get_session()
with gfile.Open(source, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    session.graph.as_default()
    tf.import_graph_def(graph_def, name='')
```

**Workaround**

Workaround is to save the weights, clear the session, `tf.keras.backend.set_learning_phase(0)`, recreate the model, restore the weights, and then freeze. https://github.com/tensorflow/tensorflow/issues/31331#issuecomment-518655879"
31666,Call NNAPI cpu implementation on linux machine,"I am trying to implement the example in the following link on my linux machine:
https://developer.android.com/ndk/guides/neuralnetworks
I know if there is no no delegation for nnapi, it will fall back to cpu implementation. So I tried to link the shared library libneuralnetworks.so which is from NDK prebuilt library to my example code. But it does not work. It seems that this .so file is not dynamic link library. Is there anyone know how to run NNAPI cpu implementation on linux machine? 

Thanks very much.




"
31665,tf.math ops do not work on MirroredVariables,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.5
- TensorFlow installed from (source or binary): binary (tf-nightly)
- TensorFlow version (use command below): 1.15.0-dev20190729
- Python version: 3.7.4

**Describe the current behavior**
When I am using the `tf.distribute.MirroredStrategy` with multiple replicas, `tf.math` ops do not work on `MirroredVariables` when inside a cross-replica scope.

**Describe the expected behavior**
The [MirroredVariable class](https://github.com/tensorflow/tensorflow/blob/e19c354920c3b246dda6598229210a582caaa1a9/tensorflow/python/distribute/values.py#L782) is a subclass of the [DistributedDelegate class](https://github.com/tensorflow/tensorflow/blob/e19c354920c3b246dda6598229210a582caaa1a9/tensorflow/python/distribute/values.py#L375), which if I understand correctly, means that `MirroredVariables` are supposed to act like regular tensors that you can perform ops on. This works fine with most standard ops, like multiplication, division, subtraction, etc. However, if you try to use any `tf.math` ops, you get `TypeError: Failed to convert object of type <class 'tensorflow.python.distribute.values.Mirrored'> to Tensor`.

**Code to reproduce the issue**
```Python
import tensorflow as tf

def merge(strategy, var):
  var = strategy.extended.reduce_to(tf.distribute.ReduceOp.SUM, var, var)
  # return var + 1  # this does work
  return tf.math.add(var, 1)  # this doesn't work

def run(var):
  var = var * var
  return tf.distribute.get_replica_context().merge_call(merge, args=(var,))

strategy = tf.distribute.MirroredStrategy(['/cpu:0', '/cpu:1'])
with strategy.scope():
  var = tf.Variable(2, dtype=tf.float32)
  result = strategy.experimental_run_v2(run, args=(var,))
  print(result)
```
"
31664,Building tflite-with-select-ops.aar library in tf v1.12,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14, 1.13, 1.12
- Python version: 2.7, 3.6
- Bazel version (if compiling from source): 0.28, 0.16


**Describe the problem**
Before getting into the problem I need a little intro.
I'm trying to run a tflite model on android. The first little issue that I met was during `.pb` file conversion. One of my ops wasn't supported by tf lite builtins and so I have to export my model using the **SELECT_TF_OPS** tag as bescribed in the [official guide](https://www.tensorflow.org/lite/guide/ops_select#converting_the_model) .
No big deal. **The model was converted using tf v1.14**.
After the successful conversion I needed only to build (from source) my custom select-ops friendly library that should be imported in my project. So I configured the bazel WORKSPACE as explained in [this guide](https://www.tensorflow.org/lite/guide/android#build_tensorflow_lite_locally).
I've chosen ANDROID SDK 23 (also tried 29) and NDK 18.
Then I've built the library using the bazel (v0.28 i guess) build arguments described [here](https://www.tensorflow.org/lite/guide/ops_select#android_aar).
The build took a **long time**, I imported it into my project and I get this Error:
**This was the first real problem**
`java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)`

No bad, I surfed the web and found [this fix](https://github.com/googlecodelabs/tensorflow-for-poets-2/issues/26). They say to downgrade tf back to 1.12.
Honestly I wasn't feeling really well on this, but I have to try.
Changed Gradle version to 4.1 and Plugin Version to 3.0.0.
_Before the fix I decided to try the previous aar build procedure on tf v1.13. But still this `java.lang.UnsatisfiedLinkError` appear._
Ok let's move on **tf v1.12**.
Now **Bazel is v0.16**. The WORKSPACE now was ANDROID SDK 23 and NDK 15.
**Here the second issue will come**, when building with select ops in tf 1.12 the bazel command [given by the official guide](https://www.tensorflow.org/lite/guide/ops_select#android_aar) is not working. Cause _//tensorflow/lite/java:tensorflow-lite-with-select-tf-ops_ cannot be resolved.
So I tried this, taking the idea from [here](https://www.tensorflow.org/lite/guide/ops_select#c):
`bazel build --cxxopt='--std=c++11' `
`-c opt`
`--config=android_arm` 
`--config=monolithic` 
`--jobs=1` 
`--define=with_select_tf_ops=true `
`//tensorflow/contrib/lite/java:tensorflow-lite`

The build was **really fast**. _Don't feel good about this._
Then, imported the model into android studio, run and again : 
`java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)`

I really don't know what I am getting wrong.

What is this error and why does it keeps coming?

Is my bazel build command correct (for building my tflite select-ops friendly aar library in tf v1.12)? 

Should I use ANDROID NDK 17 as [described here](https://www.tensorflow.org/lite/guide/android#install_bazel_and_android_prerequisites)?

I can't find anything that could help me on the web.
Lastly : 
Should I convert the `.pb` model to `.tflite` in tf v1.12?

I will really appreciate any help.
Sorry for bad english I'm not native."
31662,Tensorflow Lite Op Report,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source): 1.14.0


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MIN, CAST, CONCATENATION, EXPAND_DIMS, FULLY_CONNECTED, GATHER, GATHER_ND, LESS, MAXIMUM, MINIMUM, MUL, ONE_HOT, PACK, RANGE, REDUCE_MIN, RESHAPE, REVERSE_V2, SHAPE, SQUEEZE, STRIDED_SLICE, SUM, TANH, TRANSPOSE, UNPACK, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.
```
"
31661,Tensorflow Lite Allow Variable length sequences,"**System information**
- TensorFlow version (you are using): 1.14.9
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.** 

Currently, porting a tensorflow model to tflite does not allow for variable length inputs and instead throws

```
ValueError: None is only supported in the 1st dimension. Tensor 'input_2' has invalid shape '[None, None, 1]'.
```

Many NLP applications require the second axis of the shape to allow for variable length input sequences.

**Will this change the current api? How?** N/A

**Who will benefit with this feature?** NLP/CV applications of tflite"
31660,TOCO fails to handle Dilated Convolution properly with use_bias=False,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
conv1 = tf.layers.conv2d(x, 32, 3, dilation_rate=(2,2), activation=tf.nn.relu, use_bias=False)
TF will wrap this convolution into Space2Batch and Batch2Space pair. When converting to float tflite toco/tflite_convert fails to fold them back into single op. I think that happens because it doesn't match any of the patterns that identify_dilated_conv.cc analyzes.

If use_bias=True then everything works as expected.

**Describe the expected behavior**


**Code to reproduce the issue**
N/A

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31658,Exception has occurred: KeyError in Tensor.py,"Hi there!
I wanna run an example from [this tutorial](https://colab.sandbox.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/text/nmt_with_attention.ipynb)
Firstly I've got an error but that was fixed by adding this line:
`tf.enable_eager_execution()`
I downloaded [French - English dataset](http://www.manythings.org/anki/)
Then script was built successfully but at the end I've got another error:
```
Exception has occurred: KeyError
'desole'
  File ""C:\Users\xxx\Python\Tensor.py"", line 314, in <listcomp>
    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
  File ""C:\Users\xxx\Python\Tensor.py"", line 314, in evaluate
    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
  File ""C:\Users\xxx\Python\Tensor.py"", line 366, in translate
    result, sentence, attention_plot = evaluate(sentence)
  File ""C:\Users\xxx\Python\Tensor.py"", line 377, in <module>
    translate(u'Je suis dsol, mais je ne peux pas rpondre tout de suite.')
```

This error occurs on this line:
`translate(u'Je suis desole, mais je ne peux pas repondre tout de suite .')`
Implementation of this function you can get from tutorial above.

**System information**
OS: latest Windows 10 build;
TensorFlow: 1.14.0;
Python: 3.7.3;
[MSC v.1916 64 bit (AMD64)];
CPU: AMD Ryzen 7 2700x;
GPU: NVidia GeForce GTX 1050 Ti.

If you need some additional info, I will provide it.
Help me please to solve this issue or tell me where I'm doing something wrong.
"
31654,Running tensorflow on GPU is far slower than on CPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1809 & Windows Server 2016
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-beta1, as well as tensorflow-gpu, compared to tensorflow & tensorflow==2.0.0-beta1
- Python version: 3.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: Cuda 10 & cuDNN 7.6.2 for cuda 10.0

### Current behavior:
Im getting a 50%+ performance loss with GPU!
In the below example, the CPU version is even training way faster on a bigger model with slightly bigger epochs.
![time](https://user-images.githubusercontent.com/12736950/63144316-9daf1a00-bff2-11e9-9b39-036fd9291008.png)

Im training on 2 different systems:
My server, without a GPU:

Intel Core i5 6500T (4x@2.5 ghz) Notebook processor
Network attached storage for training data and output
16 Gb DDR 4 Ram
My Desktop PC:

Intel core i7 3770k (4x3,5-4 ghz)
Nvidia GTX 970 @4GB
32gb ddr3 ram
Training Data on local SSD, output to NAS

Now interestingly 3000 epochs, 100000 records each, takes roughly 3h on the server using TF 1.14
The same on my Desktop with GPU takes 8h with TF 2.0
It sits with full Video Ram but at 3% graphical processor use.
The CPU is sometimes at 30% use with tensorflow GPU but 100%  at any time with any CPU build.  
The harddisk is utilized with a whopping 0%.
![system utilisation](https://user-images.githubusercontent.com/12736950/63122624-7f6ffc80-bfa7-11e9-81ee-2b76e4bc5d99.png)

### Expected behavior:
Tensorflow-GPU trains faster than Tensorflow CPU

### Code to reproduce the issue:
My model is a fairly simple keras sequential lstm:
```
for learningrate in learningrates:
	for layerdensity in layerdensitys:
		for layer in amount_of_layers:
			################################
			# generate model               #
			################################
			modelname = f""{layer}-layer_{layerdensity}-nodes_selu-adam_{learningrate}-learningrate_{records_per_epoch}-epochsize_{appendix}""
			model = keras.Sequential()
			model.add(Dense(layerdensity, activation=tf.nn.selu, input_dim=15))
			for i in range(layer-1):
				model.add(Dense(layerdensity, activation=tf.nn.selu))
			model.add(Dense(9,activation=tf.nn.softmax, name = ""Output""))
			# Compile
			optimizer = tf.keras.optimizers.Adam(lr=learningrate)
			model.compile(
				optimizer=optimizer,
				loss='sparse_categorical_crossentropy',
				metrics=['accuracy'])
			model.summary()
			tensorboard = TensorBoard(log_dir=""\\\\drg-fs01\\BigData\\Projects\\Notebooks\\PokerBot\\log\\"" + modelname,
				histogram_freq = 100, write_graph = False)
			#cp_callback = tf.keras.callbacks.ModelCheckpoint(""\\\\drg-fs01\\BigData\\Projects\\Notebooks\\PokerBot\\checkpoints\\"" + modelname, verbose=0)
			################################
			# train model                  #
			################################
			model.fit(trainSet, 
				epochs = epochs, 
				steps_per_epoch = trainSteps, 
				shuffle = True, 
				validation_data = testSet, 
				validation_steps = testSteps, 
				validation_freq = int(epochs/maxTestEpochs),
				verbose = verbose, 
				callbacks = [tensorboard])#,cp_callback])
			model.save(basePath+'saved_models/' + modelname + '.h5')
```
I am having lots of test data which does not fit in memory so I use interleaved datasets:
```
# dataset modeler
def modelDataset(sourcepath, badgesize, repeat = False, repetitions = 10):
    #get all files
    list = os.listdir(sourcepath)
    pathfiles = [sourcepath+x for x in list]
    
    #get metrics
    rows_per_file = count_lines(sourcepath+""0.csv"")
    number_of_files = len(list)
    total_rows = (rows_per_file * number_of_files)
    print(f""records: {total_rows}"")
    # get number of steps per Epoch
    steps_per_epoch = int(rows_per_file / badgesize) # 2000 badges per epoch
    epochs = number_of_files
    if badgesize == 1:
        epochs = 1 
    print(f""number epochs: {epochs}"")
    # model interleaved dataset
    dataset = (tf.data.Dataset.from_tensor_slices(pathfiles).interleave(lambda x:
        tf.data.TextLineDataset(x).map(parse_csv, num_parallel_calls=4),
        cycle_length=4, block_length=16))
    dataset.columns = CSV_COLUMNS
    
    if badgesize != 1:
        dataset = dataset.shuffle(buffer_size=badgesize)
    if repeat:
        dataset = dataset.repeat(repetitions)
        epochs = epochs * repetitions
    dataset = dataset.batch(badgesize)
    dataset = dataset.prefetch(2)  # prefetch one batch
    return dataset, steps_per_epoch, epochs, badgesize
```"
31653,ValueError: Cannot add function 'TRTEngineOp_0_native_segment' because a different function with the same name already exists.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.14 and dev
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:T4

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Not able to call trt.create_inference_graph more than once to create TF-TRT nodes for disjointed sub-graphs. Throws the above error.

**Describe the expected behavior**
Should not throw the above error.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from tensorflow.contrib.slim.nets import resnet_v1
import tensorflow as tf
import tensorflow.contrib.slim as slim
import numpy as np
import time
import os
from PIL import Image
#import tensorflow.contrib.tensorrt as tftrt
from tensorflow.python.compiler.tensorrt import trt_convert as tftrt
import argparse

PATH_TO_CKPT = ""resnet_v1_50.ckpt""
TEST_IMAGE_PATHS = [ ""elephant_small.jpg"", ""tabby_tiger_cat.jpg""]
BATCH_SIZE=25
MAX_BATCH_SIZE=1000
HEIGHT=224
WIDTH=224
CHANNELS=3


def load_image_into_numpy_array(image, batch_size=1):
  (im_width, im_height) = image.size
  x = np.array(image.getdata()).reshape(
      (HEIGHT, WIDTH, CHANNELS)).astype(np.uint8)
  x = np.expand_dims(x, axis=0)
  xsl = list (x.shape)
  xsl[0] = batch_size#MAX_BATCH_SIZE
  x = np.broadcast_to(x[0,:,:,:], xsl) 
  return x

def run_resnet_50():
    # Create graph
    inputs = tf.placeholder(tf.float32, shape=[BATCH_SIZE, HEIGHT, WIDTH, CHANNELS])
    with slim.arg_scope(resnet_v1.resnet_arg_scope()):
            net, end_points = resnet_v1.resnet_v1_50(inputs, is_training=False)
    
    saver = tf.train.Saver()
    
    with tf.Session() as sess:
            saver.restore(sess, PATH_TO_CKPT)
            representation_tensor = sess.graph.get_tensor_by_name('resnet_v1_50/pool5:0') # if you don't know names like these, consider referring to corresponding model file or generate .pbtxt file as mentioned in  @civilman628 's answer above
            img = np.ones((batch_size, height, width, channels))   #load image here with size [1, 224,224, 3]
            features = sess.run(representation_tensor, {'Placeholder:0': img})
            print ( ""features"", features)

def renamed_ckpt_save(name):
    with tf.Session() as sess:# Restore the TF checkpoint

        for var_name, var_shape in tf.contrib.framework.list_variables(PATH_TO_CKPT):
            var = tf.contrib.framework.load_variable(PATH_TO_CKPT, var_name)
            new_name_parts = [name] + var_name.split('/')[1:]
            new_name = '/'.join(new_name_parts)
            var = tf.Variable(var, name=new_name)
            print var_name, var_shape, var.name

        ckpt_dir = ""/tmp/""+name
        if not os.path.exists(ckpt_dir):
            os.mkdir(ckpt_dir)
        sess.run(tf.global_variables_initializer())
        saver = tf.train.Saver()
        saver.save(sess, ckpt_dir)

def renamed_ckpt_mem(sess, name):

        for var_name, var_shape in tf.contrib.framework.list_variables(PATH_TO_CKPT):
            var = tf.contrib.framework.load_variable(PATH_TO_CKPT, var_name)
            new_name_parts = [name] + var_name.split('/')[1:]
            new_name = '/'.join(new_name_parts)
            var = tf.Variable(var, name=new_name)
            print var_name, var_shape, var.name

def build_graph (sess, input_graph, name='graph1'):

    inputs = tf.placeholder(tf.float32, shape=[BATCH_SIZE, HEIGHT, WIDTH, CHANNELS])
    with slim.arg_scope(resnet_v1.resnet_arg_scope()):
            net, end_points = resnet_v1.resnet_v1_50(inputs, is_training=False, scope=name)

    with input_graph.as_default():
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'pool5',
      ]:
        if name == '':
            tensor_name = key + ':0'
        else :
            tensor_name = name+'/'+key + ':0'
        if tensor_name in all_tensor_names:
                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
                tensor_name)
                print ( ""tensor_name"", tensor_name, tensor_dict[key])
        else:
           print( ""tensor_name "", tensor_name, "" not found"")


    # Restore the TF checkpoint
    saver = tf.train.Saver()
    saver.restore(sess, ""/tmp/""+name)
    return tensor_dict

class TfTrt():

    @staticmethod
    def build(sess, graph, name='', import_name='import'):
        tensor_dict = build_graph(sess, graph, name=name, )

        outputl = []
        for i,nname in enumerate(tensor_dict):
            nvalue = tensor_dict[nname].name.split("":"")[0]
            print( i, nname, tensor_dict[nname].name )
            outputl.append(nvalue)
        print ( ""outputl "", outputl)

        #node_names = [n.name for n in graph.as_graph_def().node]
        #import pdb
        #pdb.set_trace()

        # Freeze the graph:
        frozen_graph = tf.graph_util.convert_variables_to_constants(
        sess,
        sess.graph_def,
        output_node_names= outputl)

        # remove training nodes
        frozen_graph = tf.compat.v1.graph_util.remove_training_nodes(frozen_graph)
        # Now you can create a TensorRT inference graph from your frozen graph

        tftrt_graph = tftrt.create_inference_graph(
            input_graph_def=frozen_graph,
            outputs = outputl,
            max_batch_size = MAX_BATCH_SIZE ,
            max_workspace_size_bytes= 1024*1024*1024,
            precision_mode=""FP16"")

        output_nodes = tf.import_graph_def(
            tftrt_graph,
            return_elements = outputl,
            name=import_name
        )

        tensor_dict = {}
        for opname, opnode in zip (outputl, output_nodes):
            tensor_dict[opnode.name] = opnode.outputs[0]

        print ( ""tensor_dict"", tensor_dict)

        return tensor_dict

    @staticmethod
    def run(sess, tensor_dict, image_tensor, image_np_expanded):
        output_dict = sess.run(tensor_dict, feed_dict={image_tensor: image_np_expanded})
        return output_dict

class Tf():

    @staticmethod
    def build(sess, graph, name='', import_name=None):
        return build_graph(sess, graph, name=name)

    @staticmethod
    def run(sess, tensor_dict, image_tensor, image_np_expanded):
        output_dict = sess.run(tensor_dict, feed_dict={image_tensor: image_np_expanded})
        return output_dict

def run (runtime='TF'):

    if runtime=='TF':
        RunClass = Tf
        import_name1= ''
        import_name2= ''
        placeholder_name1 = 'Placeholder:0'
        placeholder_name2 = 'Placeholder_1:0'
    elif runtime=='TFTRT':
        RunClass = TfTrt
        import_name1 = 'import1'
        import_name2 = 'import2'
        placeholder_name1 = 'import1/Placeholder:0'
        placeholder_name2 = 'import2/Placeholder_1:0'

    graph = tf.Graph()

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    #config.gpu_options.per_process_gpu_memory_fraction = 0.33
    config.graph_options.rewrite_options.auto_mixed_precision = 1
    with tf.Session(graph=graph, config=config) as sess:
        #renamed_ckpt_mem(sess, 'resnet_v1_50_1')
        #renamed_ckpt_mem(sess, 'resnet_v1_50_2')
        tensor_dict1 = RunClass.build(sess, graph, name='resnet_v1_50_1', import_name=import_name1)
        tensor_dict2 = RunClass.build(sess, graph, name='resnet_v1_50_2', import_name=import_name2)
        tensorboard_dir = os.environ['TENSORBOARD_DIR']
        file_writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)
        image_path = TEST_IMAGE_PATHS[0]
        print ( ""image_path {}"".format(image_path))
        image = Image.open(image_path)
        image = image.resize((WIDTH, HEIGHT))
        image_np_expanded = load_image_into_numpy_array(image, batch_size=BATCH_SIZE)
        image_tensor1 = graph.get_tensor_by_name(placeholder_name1)
        image_tensor2 = graph.get_tensor_by_name(placeholder_name2)
        time0 = time.time()
        for i in range(1,1001):
            output_dict1 = RunClass.run(sess, tensor_dict1, image_tensor1, image_np_expanded)
            output_dict2 = RunClass.run(sess, tensor_dict2, image_tensor2, image_np_expanded)
            if i % 100 == 0:
                time_taken = (time.time() - time0 )/(i * 1.0)
                print (i, time_taken)
        time_taken = (time.time() - time0 )/(i * 1.0)
        print (""time_taken "", time_taken, ""output_dict"", output_dict1, output_dict2)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--runtime', default='TF', help='TF or TFTRT')
    parser.add_argument('--rewrite_ckpt', action='store_true',  help='rename checkpoints')
    args = parser.parse_args()
    if args.rewrite_ckpt:
        renamed_ckpt_save('resnet_v1_50_1')
        renamed_ckpt_save('resnet_v1_50_2')

    run (args.runtime)



```
To run 
```
PYTHONPATH=$PYTHONPATH:~/models/research/slim  python ./tftrt_resnet2x.py --runtime TFTRT
```
**Other info / logs**
```
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 430, in import_graph_def
    raise ValueError(str(e))
ValueError: Cannot add function 'TRTEngineOp_0_native_segment' because a different function with the same name already exists.
```
"
31652,How to preprocess a dataset with multiple input variables for integration with Tensorflow for binary classification?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: Tensorflow 2.0
- Python version: 3.7.1
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory:

**Describe the problem**

I am trying to setup a minimum viable neural network to train for a binary classification task. I am importing a CSV file with seven total columns, the first column is the true label, 1 or -1. I have divided the dataset into a training set and test set. Additionally, I have been able to import the dataset into python, and normalize the six variables to numbers between zero and one.

I have been spending the past few hours developing ad hoc, layers for a neural network. I have been able to get the network to feedforward, and to make binary predictions. However, I am totally at a loss for how to train the model because I cannot figure out how to update the weights. Generally, I am familiar with back-propagation, SGD, derivates, ADAM, and the perceptron learning rule. The problem is I have been unable to figure out how to implement any of these methods, so my program learns. In short, my goal is to update the weights of the neural network if the predict label and true label are not the same, so the network can learn.

I have tried using Keras because it is a high level API. [Keras](https://www.tensorflow.org/tutorials/keras/basic_classification#build_the_model ) While keras is good for getting started, it is very difficult to use Keras or Tensorflow on the datasets I generated myself. I am having trouble reformatting a neural network for the MNIST dataset to the needs of my present dataset [See Computer Vision]((https://medium.com/@brian.s.haney44/computer-vision-b39256f13fa4)).

In short, I am simply trying to find a way to develop and train a neural network on a dataset I developed. I have gone through the preprocessing and labeling, but I haven't been able load the dataset into python with Tensorflow. Instead, I am only able to load it into python through the standard library through the CSV module.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. I gathered the data and went through pre-processing procedures.
2. I found a way to load the dataset into Python.
3. I found a way to normalize the input layer.
4. I was able to complete the feedforward aspect of the network.

The problem I ran into was updating the weight when the network makes a decision contradicted by the true label.

**Any other info / logs**

This is the code I have been working from. A link to a dummy dataset can be found on my  [Github](https://github.com/Bhaney44/Deep-Neural-Network)
```
import csv
import numpy as np

with open('data.csv', 'r') as data:
    reader = csv.reader(data)
    for row in reader:
        #True Label
        true_label = int(row[0])
        #Input_Variable 1
        var_1_string = (row[1])
        var_1 = float(var_1_string.replace(',',''))
        #Input_Variable 2
        var_2_string = (row[2])
        var_2 = float(var_2_string.replace(',',''))
        #Input_Variable 3
        var_3_string = (row[3])
        var_3 = float(var_3_string.replace(',',''))

        #Input Array
        input_array = (true_label, var_1, var_2, var_3)

        #Input layer with three nodes and a normalization function
        def input_layer():
            x_1 = (input_array[1])/float(100)
            x_2 = (input_array[2])/float(100)
            x_3 = (input_array[2])/float(100)

            #Hidden layer
            def hidden_layer():
                w_0 = 0.1
                w_1 = 0.1
                w_2 = 0.1
                w_3 = 0.1
                x_4 = (x_1 * w_0)+(x_2 * w_1)
                x_5 = (x_2 * w_2)+(x_3 * w_3)

                #Output layer
                def output_layer():
                    w_4 = 0.1
                    w_5 = 0.1
                    x_6 = (x_4 * w_4)+(x_5 * w_5)

                    #Activation function
                    def activation():
                        #True label
                        x_0 = input_array[0]
                        #decision function
                        if x_6 > 0.5:
                            y = 1
                        else:
                            y = -1

                        #activation function
                        if y is x_0:
                            #Here the network predicted right
                            print('correct')
                        else:
                            #Here I want to adjust the weigh parameters
                            #However, I am not sure where to start, ideas include:
                                #calculate loss function
                                #change the way the wights are structured in the program
                                #use matrices instead of linear coefficients
                            print('update weights')
     
                    activation()
                output_layer()
            hidden_layer()
        input_layer()  `
```

As far as the work I have done in keras so far:

```
from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
```
However, this code is difficult to adapt to a different dataset. Indeed, I am unsure of the purpose of the  (x_train, y_train), (x_test, y_test) syntax and my dataset is size very differently than MNIST. Any help, suggestions, or criticisms would be greatly appreciated. 

With thanks,

Brian

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31651,keras.optimizers.Adam cannot apply gradients if ResourceVariable has unknown shape.,"With ResourceVariables we specify unknown shape dimensions for variables which can be resized after initializations similarly to how `validate_shape=False` worked previously.

Applying gradients to ResourceVariables with unknown dimension sizes using `tf.keras.optimizers.Adam` currently crashes with the exception

> ValueError: Cannot convert a partially known TensorShape to a Tensor: (None,)

Attached is a minimal example to reproduce:

```
import tensorflow as tf

print(tf.__version__)

w = tf.Variable([[1.0]], shape=tf.TensorShape([None]))
with tf.GradientTape() as tape:
  loss = w * w

grad = tape.gradient(loss, w)
tf.keras.optimizers.SGD().apply_gradients([(grad, w)])
print('that worked')
tf.keras.optimizers.Adam().apply_gradients([(grad, w)])
print('that crashed')
```

Outputs:

```
2.0.0-beta1
that worked
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in zeros(shape, dtype, name)
   1862         shape = constant_op._tensor_shape_tensor_conversion_function(
-> 1863             tensor_shape.TensorShape(shape))
   1864       except (TypeError, ValueError):

~/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _tensor_shape_tensor_conversion_function(s, dtype, name, as_ref)
    325     raise ValueError(
--> 326         ""Cannot convert a partially known TensorShape to a Tensor: %s"" % s)
    327   s_list = s.as_list()

ValueError: Cannot convert a partially known TensorShape to a Tensor: (None,)

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-174-2eba6f36340c> in <module>
      6 tf.keras.optimizers.SGD().apply_gradients([(grad, w)])
      7 print('that worked')
----> 8 tf.keras.optimizers.Adam().apply_gradients([(grad, w)])
      9 print('that crashed')

~/.local/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)
    433         _ = self.iterations
    434         self._create_hypers()
--> 435         self._create_slots(var_list)
    436 
    437       self._prepare(var_list)

~/.local/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/adam.py in _create_slots(self, var_list)
    143     # Separate for-loops to respect the ordering of slot variables from v1.
    144     for var in var_list:
--> 145       self.add_slot(var, 'm')
    146     for var in var_list:
    147       self.add_slot(var, 'v')

~/.local/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in add_slot(self, var, slot_name, initializer)
    576             dtype=var.dtype,
    577             trainable=False,
--> 578             initial_value=initial_value)
    579       backend.track_variable(weight)
    580       slot_dict[slot_name] = weight

~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    260       return cls._variable_v1_call(*args, **kwargs)
    261     elif cls is Variable:
--> 262       return cls._variable_v2_call(*args, **kwargs)
    263     else:
    264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)

~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)
    254         synchronization=synchronization,
    255         aggregation=aggregation,
--> 256         shape=shape)
    257 
    258   def __call__(cls, *args, **kwargs):

~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)
    235                         shape=None):
    236     """"""Call on Variable class. Useful to force the signature.""""""
--> 237     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
    238     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    239       previous_getter = _make_getter(getter, previous_getter)

~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)
   2549       synchronization=synchronization,
   2550       aggregation=aggregation,
-> 2551       shape=shape)
   2552 
   2553 

~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    262       return cls._variable_v2_call(*args, **kwargs)
    263     else:
--> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    265 
    266 

~/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
    462           synchronization=synchronization,
    463           aggregation=aggregation,
--> 464           shape=shape)
    465 
    466   def __repr__(self):

~/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, shape)
    606           with ops.name_scope(""Initializer""), device_context_manager(None):
    607             initial_value = ops.convert_to_tensor(
--> 608                 initial_value() if init_from_fn else initial_value,
    609                 name=""initial_value"", dtype=dtype)
    610           # Don't use `shape or initial_value.shape` since TensorShape has

~/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops_v2.py in __call__(self, shape, dtype)
     96   def __call__(self, shape, dtype=dtypes.float32):
     97     dtype = dtypes.as_dtype(dtype)
---> 98     return array_ops.zeros(shape, dtype)
     99 
    100 

~/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in zeros(shape, dtype, name)
   1864       except (TypeError, ValueError):
   1865         # Happens when shape is a list with tensor elements
-> 1866         shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)
   1867     if not shape._shape_tuple():
   1868       shape = reshape(shape, [-1])  # Ensure it's a vector

~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
   1098   preferred_dtype = deprecation.deprecated_argument_lookup(
   1099       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
-> 1100   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
   1101 
   1102 

~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1156       name=name,
   1157       preferred_dtype=dtype_hint,
-> 1158       as_ref=False)
   1159 
   1160 

~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)
   1235 
   1236     if ret is None:
-> 1237       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1238 
   1239     if ret is NotImplemented:

~/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _tensor_shape_tensor_conversion_function(s, dtype, name, as_ref)
    324   if not s.is_fully_defined():
    325     raise ValueError(
--> 326         ""Cannot convert a partially known TensorShape to a Tensor: %s"" % s)
    327   s_list = s.as_list()
    328   int64_value = 0

ValueError: Cannot convert a partially known TensorShape to a Tensor: (None,)
```"
31650,tf.keras.backend.function ignores input shapes?,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): conda install/update tensorflow
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.3
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
tf.keras.backend.function seems to ignore explicitly defined input shapes, almost as if the model were defined using None shapes (dynamic reshape).

**Describe the expected behavior**
I expected an error to be thrown, as the wrong input shape was fed to the model. Instead, it seems as if the model was run using a dynamic reshape.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow.keras as K
import tensorflow.keras.backend as Kb

sub_in = K.layers.Input(shape = (1,5,1))
x = K.layers.Conv2D(1, (1,1))(sub_in)
x = K.layers.Flatten()(x)
sub = K.models.Model(inputs=sub_in, outputs=x)

main_in = K.layers.Input(shape = (1,10,1))
main_in1 = K.layers.Lambda(lambda x: x[:,:,:5,:])(main_in)
main_in2 = K.layers.Lambda(lambda x: x[:,:,8:,:])(main_in)
x1 = sub(main_in1)
x2 = sub(main_in2)
x = K.layers.Concatenate()([x1,x2])
main = K.models.Model(inputs=main_in, outputs=x)

arr = np.arange(30).reshape((3,1,10,1))

pred = main.predict(arr)
print(pred.shape)

subOutFunc = Kb.function(sub.input, sub.output)
subOut = subOutFunc(arr)
print(subOut.shape)
```
Running this, you should see output shapes of (3,7) and (3,10), which in my opinion should not be possible. The sub model should only accept inputs of shape (?,1,5,1), yet in the two above examples inputs of shapes (?,1,2,1) and (?,1,10,1) are fed to it, and it works.

The first example (using the .predict method) gives it the (?,1,2,1) input, via the second Lambda split of the main input. No error. The second example calls backend.function directly on the sub model with a (?,1,10,1). Also no error.

Maybe this was/is the desired behavior of backend.function ? It is not what I expected. Hence raising up in case this is a bug. Thank you!"
31649,Keras -> TfLite (AttributeError: 'Node' object has no attribute 'output_masks'),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source): 1.14.0


**Provide the text output from tflite_convert**

```
>>> converter = tf.lite.TFLiteConverter.from_keras_model_file('nlp-lite/app/src/main/python/deeplearning/models/parsing/en/Deep_CRF.h5', custom_objects=custom_objects)
WARNING: Logging before flag parsing goes to stderr.
W0814 17:56:27.203332 4401333696 deprecation.py:506] From /anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0814 17:56:27.470824 4401333696 deprecation_wrapper.py:119] From /anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 747, in from_keras_model_file
    keras_model = _keras.models.load_model(model_file, custom_objects)
  File ""/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py"", line 146, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 212, in load_model_from_hdf5
    custom_objects=custom_objects)
  File ""/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py"", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 89, in deserialize
    printable_module_name='layer')
  File ""/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 192, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1131, in from_config
    process_node(layer, node_data)
  File ""/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1087, in process_node
    layer(flat_input_tensors[0], **kwargs)
  File ""/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/base_layer.py"", line 443, in __call__
    previous_mask = _collect_previous_mask(inputs)
  File ""/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/base_layer.py"", line 1311, in _collect_previous_mask
    mask = node.output_masks[tensor_index]
AttributeError: 'Node' object has no attribute 'output_masks'
```

Model:

```
from keras_contrib.metrics import crf_viterbi_accuracy, crf_accuracy
from keras_contrib.losses import crf_loss
from keras_contrib.layers import CRF


from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer

from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger
from keras.utils import to_categorical
from keras import backend as K


# Defining the network dependencies
from keras.layers import LSTM, Dense, Embedding, Input, TimeDistributed, Flatten, Lambda
from keras.models import load_model, Model
def make_model():
        #layers comes from a pretrained model
        lm_model.layers[1].trainable = False
        lm_model.layers[1].input_length = None
        lm_model.layers[1].batch_input_shape = (None, None, 1)
        lm_model.layers[2].batch_input_shape = (1, None, 1)
        lm_model.layers[2].stateful = False
        crf_input = Input(batch_shape=(None, None, 1))
        crf_emb = lm_model.layers[1](crf_input)
        reshape = Lambda(lambda x: K.squeeze(x, 2))(crf_emb)
        crf_lstm = lm_model.layers[2](reshape)
        crf_dense1 = TimeDistributed(Dense(TD_nodes, activation='relu'))(crf_lstm)
        crf_out = CRF(n_tags, sparse_target=True)(crf_dense1)
        crf_model = Model(inputs=crf_input, outputs=crf_out)
        crf_model.compile(optimizer='rmsprop', loss=crf_loss, metrics=[crf_viterbi_accuracy, crf_accuracy, 'accuracy'])
```

I am using CRF from Keras Contrib
"
31647,Graph visualization of subclassed model/layer,"**System information**
- TensorFlow version (you are using): tensorflow-2.0b1
- Are you willing to contribute it (Yes/No): Yes 

**Describe the feature and the current behavior/state.**
Graph visualization of subclassed model/layer/tf.functions

**Will this change the current api? How?**
Not sure

**Who will benefit with this feature?**
All users

**Details**
I posted this question on stack overflow a week ago and it seems it is completely ignored.  This is an attempt to reach out to experts who are more familiar to the inner working of the graph network implementation, and/or tf.function visualization in Tensorboard.  Please forgive me if this is not the right place, and I hope someone here can point me to the right place, thanks.
  
`https://stackoverflow.com/questions/57421766/plotting-subclassed-model-in-tensorflow-2-0-beta`

I have a subclassed model that instantiates a few custom layers via subclassing.  I tried using `keras.utils.plot_model()` but all it does is print the model block, none of the layers appeared.  

Can a Tensorflow expert comment on this?  Will this feature ever be implemented in the future?  If not, what is the next best alternative to examine the computation graph?  Note that `model.summary()` only gives a summary of the parameters of the custom layer, within which contains two dense layers.  Ideally, I like to see all the computations, if that is not asking too much...

**Update**: I dug into the source, looks like plot_model() first check for the `_is_graph_network` attribute.  Graph Networks are used in Functional and Sequential APIs.  From the source:

> Two types of `Networks` exist: Graph Networks and Subclass Networks. Graph
>  networks are used in the Keras Functional and Sequential APIs. Subclassed
>  networks are used when a user subclasses the `Model` class. In general,
>  more Keras features are supported with Graph Networks than with Subclassed
>  Networks, specifically:

>  - Model cloning (`keras.models.clone()`)
>  - Serialization (`model.get_config()/from_config()`, `model.to_json()/to_yaml()`)
>  - Whole-model saving (`model.save()`)

(**custom graph component**)
Naturally, I like to know if I can build a graph network component, so my subclassed model/layer can work with these features.  Does that involve a lot of effort?

(**tf.function graph visualization**)
Can someone let me know if graph visualization via Tensorboard works with Tensorflow2 tf.functions?  In Tensorflow 1.x, one defines a name scope for a logical group of ops (e.g. generator/discriminator in GAN, encoder/decoder in VAE and loss/metrics), they are then displayed as a high-level block in the graph visualization.  Can I define something similar for tf.functions?

"
31645,i try to run a python code using tensorflow and Mask RCNN but i get that error,"check failed status == cudnn_status_success (7 vs. 0)failed to set cudnn stream

i am using 

tensorflow  1.14.0
cuda  10.0"
31644,tf.keras.models.load_model takes forever to load,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Windows 10 Version 1607
- TensorFlow installed from: pip
- TensorFlow version:
   - Git-Version: v1.12.1-8794-ge36271a61d
   - version: 1.15.0-dev20190814
- CUDA/cuDNN version: 10.0/7
- GPU model and memory: NVIDIA GTX 1050 Ti, 4096 MB

**Describe the current behavior**
The code below produces this output:
```
Save successfull
loading ...
W0815 15:02:11.705600 17844 deprecation.py:506] From C:\tensorflow_anduin\lib\site-packages\tensorflow_core\python\ops\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0815 15:02:11.706604 17844 deprecation.py:506] From C:\tensorflow_anduin\lib\site-packages\tensorflow_core\python\ops\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will
be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
```
even after 30 mins it doesn't print the expected output `Load successfull`

**Describe the expected behavior**
Succesful loading in under 1 min

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.layers import (Dense, Input, Lambda)
from tensorflow.keras.models import Model, Sequential
from scipy import sparse
import numpy as np


def layer_lambda(input_x):
    sparse = input_x[0]
    dense = input_x[1]
    dense = tf.transpose(dense)
    y = tf.sparse.sparse_dense_matmul(sparse, dense)
    return tf.transpose(y)


dense_mat = np.eye(30, 30, dtype=np.float32)
sparse_mat = sparse.coo_matrix(dense_mat)
sparse_indices = np.mat([sparse_mat.row, sparse_mat.col]).transpose()
sparse_tensor = tf.SparseTensor(sparse_indices, sparse_mat.data, sparse_mat.shape)

model = Sequential()
model_input = Input(shape=(20,))
x = Dense(20)(model_input)
x = Dense(30)(x)
x = Lambda(layer_lambda, output_shape=(None, 30, 30))([sparse_tensor, x])
model = Model(model_input, x)

model.predict([[np.ones(20)]])

model.save(""model.h5"")

print(""Save successfull"")
print(""loading ..."")
model_load = tf.keras.models.load_model(""model.h5"", custom_objects={'layer_lambda': layer_lambda})

print(""Load successfull"")

```

This error occurred after fixing [issue 31607](https://github.com/tensorflow/tensorflow/issues/31607).
"
31643,ImportError: DLL load failed: The specified module could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Microsoft Windows 10 Pro
10.0.18362 Build 18362

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14.0
- Python version: 3.6.6 virtual interpreter
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0
- GPU model and memory: NVIDIA GeForce GTX 1050 Ti



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- Note: The usage of -> short for linked to.
- Downloaded and installed CUDA/cuDNN 9.0
- Downloaded and installed 10.0 (needed 10 for cudart64_100.dll) as per error message PyCharm 2019.1.3 (Professional Edition) 'cudart64_100.dll was not found' along those lines.
- Downloaded a copy of cudnn64_7.dll from the cuDNN Library and pasted into ../v9.0/bin folder
- Update windows environment paths as per recommendation https://www.tensorflow.org/install/gpu and as per issue/10033
- Found the issue where a user was recommended to run Scratch.py file via PyCharm 
[stratch-py.txt](https://github.com/tensorflow/tensorflow/files/3504373/stratch-py.txt)
- Installed protobuf 3.6.0 as per issues/28389 -> issues/28389 -> issues/25072
- Unsure if I have non-avx cpu and require non-avx built wheel as per issues/28389 -> issues/28389 -> issues/25072
    - Processor	Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz, 2801 Mhz, 4 Core(s), 8 LogPro's
- Removed CUDA/cuDNN version 10.0 (see log file removed-cuda-10-0) as per issues/22872
    - Copied cudart64_100.dll to ../v9.0/bin folder
    - Re-updated environment paths:
        - C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin
        - C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\extras\CUPTI\libx64
        - C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\include
        - C:\tools\cuda\bin
- Upgraded from 3.6.6 to 3.6.8 as per issues/29575
- Removed tensorflow-gpu and tensorflow-gpu-graphics from pycharm project interpreter settings. 
    - [output-scratch-py-minus-gpu.txt](https://github.com/tensorflow/tensorflow/files/3505544/output-scratch-py-minus-gpu.txt)
    - pip install tensorflow==1.14.0
- Updated the project interpreter to 3.6.8 non virtual interpret via pycharm
    - See output post-updated-pycharm-interp-3.6.8
    - Also current-project-interp-pkgs



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[pycharm-project-interpreter-pkgs.txt](https://github.com/tensorflow/tensorflow/files/3505376/pycharm-project-interpreter-pkgs.txt)
[cli-python-import-tensorflow.txt](https://github.com/tensorflow/tensorflow/files/3504377/cli-python-import-tensorflow.txt)
[tensorflow-log.txt](https://github.com/tensorflow/tensorflow/files/3504340/tensorflow-log.txt)
[pydev-debugger.txt](https://github.com/tensorflow/tensorflow/files/3504349/pydev-debugger.txt)
[removed-cuda-10-0.txt](https://github.com/tensorflow/tensorflow/files/3505369/removed-cuda-10-0.txt)
[stack-trace.pdf](https://github.com/tensorflow/tensorflow/files/3505474/stack-trace.pdf)
[stack-trace-with-debug.pdf](https://github.com/tensorflow/tensorflow/files/3505483/stack-trace-with-debug.pdf)
[post-updated-pycharm-interp-3.6.8.pdf](https://github.com/tensorflow/tensorflow/files/3505572/post-updated-pycharm-interp-3.6.8.pdf)
[current-project-interp-pkgs.txt](https://github.com/tensorflow/tensorflow/files/3505581/current-project-interp-pkgs.txt)


"
31642,How can I use tensorflow2.0 beta1 multiple GPUs one computer to write a GAN code?,"def main():
    def G_D_fn(train_data):  
        with tf.GradientTape() as tape:
            generates = G(train_data)  
            print(generates) 
            print(""!!!!!!!!!!!!!!!!!"")
            logits_fake = D(generates)   #error here, but if I do not use multiple GPUs, the program is normal.

 with strategy.scope():
      # obtain models
      G = get_G((batch_size, None, None, 3)) #(batch_size, 128, 128, 3)
      D = get_D((batch_size, None, None, 3)) 
      train_dataset = get_data()
      train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)
     for epoch in range(10):
          for one_batch in train_dist_dataset:
              per_loss = strategy.experimental_run_v2(G_D_fn, args=(epoch, batch_data, ))

Traceback (most recent call last):
  File ""train.py"", line 680, in <module>
    train()
  File ""train.py"", line 319, in train
    per_d_loss, per_g_loss, per_d_loss1, per_d_loss2, per_g_gan_loss, per_mse_loss, per_vgg_loss, per_style_loss = strategy.experimental_run_v2(G_D_fn, args=(epoch, batch_data, ))
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 708, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1710, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 708, in _call_for_each_replica
    fn, args, kwargs)
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 195, in _call_for_each_replica
    coord.join(threads)
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 189, in _call_for_each_replica
    **merge_kwargs)
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/moving_averages.py"", line 105, in merge_fn
    return update(strategy, v, value)
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/moving_averages.py"", line 96, in update
    return strategy.extended.update(v, update_fn, args=(value,))
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1458, in update
    return self._update(var, fn, args, kwargs, group)
  File ""/home/dongwen/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 758, in _update
    assert isinstance(var, values.DistributedVariable)
AssertionError

"
31641,How can I use multiple GPUs one computer to write a GAN code?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31639,There is a patch in the Eigen dir. Any plan to update it to a new commit?,"I'm compiling a project with TensorFlow and Eigen and I need to use the same version Eigen source code as TensorFlow. But I found there is a git patch in the TensorFlow's Eigen dir.  I don't know which commit of Eigen I should use Bazel to download. Any plan to update the Eigen to a new commit?
Thanks!
"
31638,Mask is not propagated into Sequential keras layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- TensorFlow installed from (source or binary): docker, pip
- TensorFlow version (use command below): 1.14.0, 2.0.0b
- Python version: 3.6.8

**Describe the current behavior**

When a masked input is fed to a `tf.keras.models.Sequential`, the mask is ignored

**Describe the expected behavior**

I would expect the mask to be propagated as if the inner layers were not in a `Sequential` layer.

**Code to reproduce the issue**

This works:
```
import tensorflow as tf

def check_mask(inputs, mask=None):
    assert mask is not None
    return inputs

model = tf.keras.models.Sequential([
    tf.keras.layers.Masking(0, input_shape=[1]),
    tf.keras.layers.Lambda(check_mask)
])
```

This fails due to failed assert:
```
import tensorflow as tf

def check_mask(inputs, mask=None):
    assert mask is not None
    return inputs

model = tf.keras.models.Sequential([
    tf.keras.layers.Masking(0, input_shape=[1]),
    tf.keras.models.Sequential([
        tf.keras.layers.Lambda(check_mask),
    ]),
])
```

**Other info / logs**
I think I have narrowed this down to a cache invalidation issue in `tf.keras.layers.Layer._should_compute_mask`: https://github.com/tensorflow/tensorflow/blob/e19c354920c3b246dda6598229210a582caaa1a9/tensorflow/python/keras/engine/base_layer.py#L2055-L2059

From my debugging, it seems that whenever the issue above appears `_should_compute_mask` is cached as false even though evaluating the actual property expression results in true.

Since this problem is in a base layer, this issue might be affecting other layers as well."
31637,gradient accumulation for very large embeddings,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): yes, but don't know how.



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**
It certainly will add some new apis. Currently, in a distributed environment, we can use `tf.contrib.opt.AGNOptimizer` class to do gradient accumulation. However this optimizer converts all gradients (Tensor or IndexedSlices) to dense types which consumes quite large memory. In certain scenarios, say we've got some very large embedding variables which can't fit into one single machine, this would cause OOM on workers.

**Who will benefit with this feature?**
Anyone who has some very large variables in their models, and want to use gradient accumulation to accelerate training.

**Any Other info.**
None."
31636,Masked GlobalAveragePooling1D fails when size of step axis is not known statically,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14.0, 2.0.0b
- Python version: 3.6.8

**Code to reproduce the issue**

This code:
```
import tensorflow as tf
input = tf.keras.layers.Input([None, 32])
mask = tf.keras.layers.Input([None], dtype=tf.bool)
tf.keras.layers.GlobalAveragePooling1D()(input, mask)
```

Fails with this exception:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 662, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/pooling.py"", line 643, in call
    mask = array_ops.reshape(mask, broadcast_shape)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 7715, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 530, in _apply_op_helper
    raise err
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 527, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1237, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 305, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 246, in constant
    allow_broadcast=True)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 284, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 563, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [-1, None, 1]. Consider casting elements to a supported type.
```

I would expect masked global average pooling to be no different than regular global average pooling (where unknown step axis is supported) 

I think this is caused by the two lines here: https://github.com/tensorflow/tensorflow/blob/e19c354920c3b246dda6598229210a582caaa1a9/tensorflow/python/keras/layers/pooling.py#L641-L642

Instead of finding the broadcast shape using `as_list` it should use the shape tensor directly and `tf.concat` them together (or whatever is an appropriate equivalent in the keras library)."
31634,Custom OP using eigen matrix,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: from source
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: cuda 10.0 cudnn 7.6.2
- GPU model and memory:ROG strix geforce gtx 1080Ti, 11G



**Describe the problem**
I'm trying to develop a Tensorflow custom op using cuda. And inside the op, I need to use Eigen to run some matrix operations, but when I try to calculate the inverse of a size 8*8 matrix, I've encountered several problems.

This function is written in CUDA kernel.
In cuda kernel function file (.cu.cc), I have included
```c++
#include <Eigen\Dense>
#include <Eigen/LU>
#include ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include <stdio.h>
#include <math.h>
```
And in the kernel function, I just tried a simple test:
```c++
typedef Eigen::Matrix<float, 8, 8> Matrix8f;

__global__ void CsmToRcmKernel(Eigen::Map<const Eigen::MatrixXcf> csm_map, Eigen::Map<const Eigen::VectorXf> reg_vec,
    Eigen::Map<const MatrixXi16> offset8,       // (12160, 8, 3)
    Eigen::Map<const MatrixXi16> offset3,       // (9728, 3, 3)
    Eigen::Map<const MatrixXi16> offset4,       // (12160, 4, 3)
    Eigen::Map<const MatrixXi16> offset6,       // (9728, 6, 3)
    Eigen::Map<Eigen::MatrixXcf> rcm_map) {

    Matrix8f A = MatrixXf::Identity(8, 8);
    A(0, 0) = 2;

    // inv_A = inv(A);
    Matrix8f inv_A = A.inverse();
}
```
(You can ignore the input parameters, they aren't used in this test program.)

And when I try to build this op with bazel, I got following errors:
```
c:\users\administrator\_bazel_zhaozixiao\sr6amwum\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/SolveTriangular.h(185): 
error: function ""Eigen::Block<XprType, BlockRows, BlockCols, InnerPanel>::operator=(const Eigen::Block<Eigen::Block<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0>>, -1, -1, false>, -1, -1, false> &) [with XprType=Eigen::Block<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0,Eigen::Stride<0, 0>>, -1, -1, false>, BlockRows=-1, BlockCols=-1, InnerPanel=false]"" (declared implicitly) cannot be referenced -- it is a deleted function

c:\users\administrator\_bazel_zhaozixiao\sr6amwum\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/GenericPacketMath.h(368): 
error: asm operand type size(8) does not match type/size implied by constraint 'r'
```
And if I try to calculate the inverse of a size 4 * 4(or below) matrix, there isn't any problem. 
In fact I found that tensorflow itself can realise big size matrix operation, matrix inverse for example, but it seems like they don't use eigen structure. So I'd like to know if cuda kernel supports big size(bigger than 4 * 4) matrix operations based on eigen? Thank you!

"
31630,cannot import tensorflow ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14.0
- Python version: 3.7.0
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0 / 6.0 *
- GPU model and memory: GeForce 940M (can't seem to find the memory of this card)



**Describe the problem**

importError is raised when I attempt to import tensorflow

**Provide the exact sequence of commands / steps that you executed before running into the problem**

>>>import tensorflow as tf

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\venv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\venv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\venv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\venv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

*: I can't install CUDA 8.0 on my computer, because most of the modules failed to install, so I used 9.0 instead
"
31628,fft2d on CPUs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
It seems like there is a performance issue when running fft2d on data with dimension > 2 (CPU only).
When running fft2d on an array of (40,40,256,256), the function correctly computes fft2d on the inner most 2 dimensions as expected. But it only uses 1 CPU core which results in poor performance.


**Describe the expected behavior**
Because it is equivalent to simultaneously computing 40x40 fft2d, I would expect parallel computation across multiple CPU cores to boost the performance.

**Code to reproduce the issue**
a=tf.cast(tf.Variable(tf.random.uniform([40,40,256,256],dtype=tf.float32)), dtype=tf.complex64)
b=tf.signal.fft2d(a)

Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31627,Encountered error while reading extension file 'protobuf.bzl': ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

Followed these installation instruction
https://www.tensorflow.org/install/source_windows

**Describe the problem**
**Encountered error while reading extension file 'protobuf.bzl': no such package '@com_google_protobuf//': Traceback (most recent call last):
error loading package 'tensorflow': in C:/temp/tensorflow-master/tensorflow/core/platform/default/build_config.bzl: Encountered error while reading extension file** 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


> C:\Temp\tensorflow-master>bazel build --config=opt //tensorflow:tensorflow.dll
> WARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE.
> INFO: Options provided by the client:
>   Inherited 'common' options: --isatty=1 --terminal_columns=120
> INFO: Options provided by the client:
>   'build' options: --python_path=C:/ProgramData/Anaconda3/python.exe
> INFO: Reading rc options for 'build' from c:\temp\tensorflow-master\.bazelrc:
>   'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include
> INFO: Reading rc options for 'build' from c:\temp\tensorflow-master\.tf_configure.bazelrc:
>   'build' options: --action_env PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages --python_path=C:/ProgramData/Anaconda3/python.exe --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --action_env TF_CONFIGURE_IOS=0
> INFO: Found applicable config definition build:monolithic in file c:\temp\tensorflow-master\.bazelrc: --define framework_shared_object=false
> INFO: Found applicable config definition build:opt in file c:\temp\tensorflow-master\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true
> INFO: An error occurred during the fetch of repository 'com_google_protobuf'
> INFO: Call stack for the definition of repository 'com_google_protobuf':
>  - C:/temp/tensorflow-master/tensorflow/workspace.bzl:432:5
>  - C:/temp/tensorflow-master/WORKSPACE:26:1
> ERROR: Skipping '//tensorflow:tensorflow.dll': error loading package 'tensorflow': in C:/temp/tensorflow-master/tensorflow/core/platform/default/build_config.bzl: Encountered error while reading extension file 'protobuf.bzl': no such package '@com_google_protobuf//': Traceback (most recent call last):
>         File ""C:/temp/tensorflow-master/third_party/repo.bzl"", line 104
>                 _apply_patch(ctx, ctx.attr.patch_file)
>         File ""C:/temp/tensorflow-master/third_party/repo.bzl"", line 70, in _apply_patch
>                 _wrap_bash_cmd(ctx, patch_command)
>         File ""C:/temp/tensorflow-master/third_party/repo.bzl"", line 28, in _wrap_bash_cmd
>                 fail(""BAZEL_SH environment variable i..."")
> BAZEL_SH environment variable is not set
> WARNING: Target pattern parsing failed.
> ERROR: error loading package 'tensorflow': in C:/temp/tensorflow-master/tensorflow/core/platform/default/build_config.bzl: Encountered error while reading extension file 'protobuf.bzl': no such package '@com_google_protobuf//': Traceback (most recent call last):
>         File ""C:/temp/tensorflow-master/third_party/repo.bzl"", line 104
>                 _apply_patch(ctx, ctx.attr.patch_file)
>         File ""C:/temp/tensorflow-master/third_party/repo.bzl"", line 70, in _apply_patch
>                 _wrap_bash_cmd(ctx, patch_command)
>         File ""C:/temp/tensorflow-master/third_party/repo.bzl"", line 28, in _wrap_bash_cmd
>                 fail(""BAZEL_SH environment variable i..."")
> BAZEL_SH environment variable is not set
> INFO: Elapsed time: 21.498s
> INFO: 0 processes.
> FAILED: Build did NOT complete successfully (0 packages loaded)
>     currently loading: tensorflow

```"
31625,Strange result when fetching variables under a distributed strategy,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.5
- TensorFlow installed from (source or binary): binary (tf-nightly)
- TensorFlow version (use command below): 1.15.0-dev20190729
- Python version: 3.7.4

**Describe the current behavior**
When I am using any distributed strategy, and I fetch a `tf.Variable`, I get a result that looks like: `[array([( 10,), ( 44,), ( 47,),  ...], dtype=[('resource', 'u1')])]`. Instead, I have to wrap the variable in a `tf.identity` call in order to get its value properly.

**Describe the expected behavior**
The fetched result should be the value of the variable.

**Code to reproduce the issue**
```Python
import tensorflow as tf
import numpy as np
import sys

class RunHook(tf.train.SessionRunHook):
  def before_run(self, run_context):
    return tf.train.SessionRunArgs(fetches=['var:0'])

  def after_run(self, run_context, run_values):
    print(run_values.results)
    sys.exit(0)

def model_fn(features, labels, mode, params):
  var = tf.get_variable(
    initializer=tf.constant([1.0], dtype=tf.float32),
    name=""var"",
    dtype=tf.float32,
    trainable=True,
  )
  loss = tf.identity(var)
  opt = tf.train.AdamOptimizer(0.001)
  global_step = tf.train.get_or_create_global_step()
  train_op = opt.minimize(loss, global_step=global_step)
  return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

strategy = tf.distribute.MirroredStrategy()
session_config = tf.ConfigProto()
config = tf.estimator.RunConfig(train_distribute=strategy, session_config=session_config,
                                log_step_count_steps=1, save_checkpoints_steps=float('inf'))
classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)


x = np.array([1, 2, 3, 4])
y = np.array([5, 6, 7, 8])
train_input_fn = tf.estimator.inputs.numpy_input_fn(x, y, batch_size=1, num_epochs=None, shuffle=True)

tf.estimator.train_and_evaluate(
  classifier,
  train_spec=tf.estimator.TrainSpec(input_fn=lambda: train_input_fn, hooks=[RunHook()]),
  eval_spec=tf.estimator.EvalSpec(input_fn=lambda: train_input_fn)
)
```
"
31622,GPU docker images and pip package have mismatched CuDNN versions,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16
- TensorFlow installed from (source or binary): Docker image
- TensorFlow version: Nightly
- CUDA/cuDNN version: Docker is cudnn 7.5 or 7.4 & pip package is cudnn 7.6

**Describe the problem**
The available GPU docker images (tensorflow/tensorflow:nightly-gpu ; tensorflow/tensorflow:gpu & the others) will fail to intialize cuDNN since TF-nightly is compiled with a higher minor version

**Provide the exact sequence of commands / steps that you executed before running into the problem**
docker run --rm -it --runtime=nvidia tensorflow/tensorflow:nightly-gpu-py3 /bin/bash

```
import tensorflow as tf
import numpy as np

x = tf.placeholder(tf.float32, shape=(2,4,4,3))
layer = tf.keras.layers.Conv2D(5, (2,2), input_shape=(4,4,3))
out = layer(x)

data = np.random.random((2, 4, 4, 3))
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(out, feed_dict={x: data}))

```

Fails with:
```
Loaded runtime CuDNN library: 7.4.1 but source was compiled with: 7.6.0.
Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
```

**Any other info / logs**
Related issue: https://github.com/tensorflow/addons/issues/418

cc @yifeif @av8ramit . As discussed this applies to custom-op repos as well.
"
31621,Support a dictionary of tensors as a CompositeTensor,"- TensorFlow version (you are using): 2.0.0b1
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
The CompositeTensor base class offers a unified way of handling various types of tensors that are themselves composed of tensors. Currently, dictionaries of tensors are not supported; it would be useful if they were.

**Will this change the current api? How?**
Yes, by adding to the types of structures that can be treated as a CompositeTensor.

**Who will benefit with this feature?**
Dictionaries are one of the more convenient ways to manage collections of heterogeneous features. In issue #27679 (Passing a dictionary of tensors to py_function), it was stated that in that case, creating a subclass of CompositeTensor that can represent a dictionary would be one path to ultimately supporting dictionaries of tensors in py_function."
31619,Incorrect value in results when using tf.math.log(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No idea
- TensorFlow installed from (source or binary): binary using pip3 install
- TensorFlow version (use command below): 1.14
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda9.0, cudnn 7.6
- GPU model and memory: RTX 2080Ti

When I run the following code using CPU:
`result= tf.math.log(tf.constant([280.41303540865516]*100, dtype=tf.float32)).numpy()`
I got:
`
[5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634
 5.6362634 5.6362634 5.6362634 5.6362634 5.6362634 5.636264  5.636264
 5.636264  5.636264 ]`

I found result[0] is not equal to result[-1]. the first one is 
5.636263370513916 while the latter one is equal to 5.636263847351074.

I believe it is a bug.


**Code to reproduce the issue**
`
from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf
from random import *
tf.enable_eager_execution()
with tf.device(""/cpu:0""):
    result = tf.math.log(tf.constant([ 280.41303540865516]*100, dtype=tf.float32)).numpy()
print(""{} vs {}"".format(result[0], result[-1]))`


Seems GPU has no such issues."
31618,CUDA installation link leads to 9.0 instead of 10,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64bit
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: none tested
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.14
- **Python version**: 3.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: None installed
- **GPU model and memory**: NVIDIA GeForce GTX 1060 6GB with 16GB RAM
- **Exact command to reproduce**: import tensorflow

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
The CUDA download link that appears when `tensorflow-gpu` is installed but CUDA is missing leads to 9.0

### Source code / logs
```Traceback (most recent call last):
  File ""H:/C33F/Programmieren 4.0/HelloWorldProject/DeepLearning/dqn_main.py"", line 1, in <module>
    from openaigym_cartrack_deeplearning_custom_environments import CarTrackDeepLearningEnvironment
  File ""H:\C33F\Programmieren 4.0\HelloWorldProject\DeepLearning\openaigym_cartrack_deeplearning_custom_environments.py"", line 7, in <module>
    import brains
  File ""H:\C33F\Programmieren 4.0\HelloWorldProject\DeepLearning\brains.py"", line 10, in <module>
    from keras.layers import Dense, Flatten, Input
  File ""C:\Python37\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Python37\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Python37\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Python37\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Python37\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Python37\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Python37\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive
```
"
31617,Accuracy metric not consistent between training and evaluation when using same dataset for both train and test,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES, I made changes based on a script provided by TF
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): PIP install
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6

**Intro**

In order to sanity check a code for custom estimator I wrote, I trained and evaluated it on the same dataset (training set = evaluation set), expecting to get the same metrics. I also set the batch size to the dataset size to avoid having discrepancies due to calculations based on the batch instead of the whole dataset.

While I could verify that the loss was consistent, I haven't managed to get consistent accuracies. Because of that, I grabbed code provided by tensorflow (https://github.com/tensorflow/models/tree/master/samples/core/get_started) and tried to replicate the behavior. I needed to make some changes in order to 1)run it with batch size = whole dataset, 2) run it with training set = test set, and 3) save data to disk so I could inspect it with Tensorboard. The whole pieces of code are shown below.

**Describe the current behavior**

The accuracy calculated during training is different from the one calculated during evaluation, even if training and test set are exactly the same.

**Describe the expected behavior**

I would expect to be able to get the same accuracy both for training and for evaluation, if the same dataset is being used for both phases.

**Code to reproduce the issue**

Scripts based on [custom_estimator.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py) and [iris_data.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py).

My custom_estimator.py:

```
#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
""""""An Example of a custom Estimator for the Iris dataset.""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import argparse
import tensorflow as tf
from tensorflow_estimator import estimator

import iris_data

parser = argparse.ArgumentParser()
parser.add_argument('--batch_size', default=120, type=int, help='batch size')
parser.add_argument('--train_steps', default=10, type=int,
                    help='number of training steps')

def my_model(features, labels, mode, params):
    """"""DNN with three hidden layers and learning_rate=0.1.""""""
    # Create three fully connected layers.
    net = tf.feature_column.input_layer(features, params['feature_columns'])
    for units in params['hidden_units']:
        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)

    # Compute logits (1 per class).
    logits = tf.layers.dense(net, params['n_classes'], activation=None)

    # Compute predictions.
    predicted_classes = tf.argmax(logits, 1)
    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {
            'class_ids': predicted_classes[:, tf.newaxis],
            'probabilities': tf.nn.softmax(logits),
            'logits': logits,
        }
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)

    # Compute loss.
    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

    # Compute evaluation metrics.
    accuracy = tf.metrics.accuracy(labels=labels,
                                   predictions=predicted_classes,
                                   name='acc_op')
    metrics = {'accuracy': accuracy}
    tf.summary.scalar('accuracy', accuracy[1])

    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(
            mode, loss=loss, eval_metric_ops=metrics)

    # Create training op.
    assert mode == tf.estimator.ModeKeys.TRAIN

    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)
    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)


def main(argv):
    args = parser.parse_args(argv[1:])

    # Fetch the data
    (train_x, train_y), (test_x, test_y) = iris_data.load_data()

    # Feature columns describe how to use the input.
    my_feature_columns = []
    for key in train_x.keys():
        my_feature_columns.append(tf.feature_column.numeric_column(key=key))

    # Build 2 hidden layer DNN with 10, 10 units respectively.
    classifier = tf.estimator.Estimator(
        model_fn=my_model,
        params={
            'feature_columns': my_feature_columns,
            # Two hidden layers of 10 nodes each.
            'hidden_units': [10, 10],
            # The model must choose between 3 classes.
            'n_classes': 3,
        },
        config=estimator.RunConfig(
            keep_checkpoint_max=1,
            save_checkpoints_steps=1,
            save_summary_steps=1,
            model_dir=os.path.join(""."", ""model_dir"")))

    # Train the Model.
    estimator.train_and_evaluate(
        estimator=classifier,
        train_spec=estimator.TrainSpec(input_fn=lambda: iris_data.train_input_fn(train_x, train_y, args.batch_size),
                                       max_steps=args.train_steps),
        eval_spec=estimator.EvalSpec(input_fn=lambda: iris_data.eval_input_fn(test_x, test_y, args.batch_size),
                                     start_delay_secs=0,
                                     throttle_secs=0,
                                     steps=None),

    )
    #classifier.train(
    #    input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),
    #    steps=args.train_steps)

    # Evaluate the model.
    #eval_result = classifier.evaluate(
    #    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y, args.batch_size))

    #print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result))

    # Generate predictions from the model
    expected = ['Setosa', 'Versicolor', 'Virginica']
    predict_x = {
        'SepalLength': [5.1, 5.9, 6.9],
        'SepalWidth': [3.3, 3.0, 3.1],
        'PetalLength': [1.7, 4.2, 5.4],
        'PetalWidth': [0.5, 1.5, 2.1],
    }

    predictions = classifier.predict(
        input_fn=lambda:iris_data.eval_input_fn(predict_x,
                                                labels=None,
                                                batch_size=args.batch_size))

    for pred_dict, expec in zip(predictions, expected):
        template = ('\nPrediction is ""{}"" ({:.1f}%), expected ""{}""')

        class_id = pred_dict['class_ids'][0]
        probability = pred_dict['probabilities'][class_id]

        print(template.format(iris_data.SPECIES[class_id],
                              100 * probability, expec))


if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.INFO)
    tf.app.run(main)
```

My iris_data.py:

```
import pandas as pd
import tensorflow as tf

TRAIN_URL = ""http://download.tensorflow.org/data/iris_training.csv""
TEST_URL = TRAIN_URL#""http://download.tensorflow.org/data/iris_test.csv""

CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',
                    'PetalLength', 'PetalWidth', 'Species']
SPECIES = ['Setosa', 'Versicolor', 'Virginica']

def maybe_download():
    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)
    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)

    return train_path, test_path

def load_data(y_name='Species'):
    """"""Returns the iris dataset as (train_x, train_y), (test_x, test_y).""""""
    train_path, test_path = maybe_download()

    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)
    train_x, train_y = train, train.pop(y_name)

    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)
    test_x, test_y = test, test.pop(y_name)

    return (train_x, train_y), (test_x, test_y)


def train_input_fn(features, labels, batch_size):
    """"""An input function for training""""""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)

    # Return the dataset.
    return dataset


def eval_input_fn(features, labels, batch_size):
    """"""An input function for evaluation or prediction""""""
    features=dict(features)
    if labels is None:
        # No labels, use only features.
        inputs = features
    else:
        inputs = (features, labels)

    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices(inputs)

    # Batch the examples
    assert batch_size is not None, ""batch_size must not be None""
    dataset = dataset.batch(batch_size)

    # Return the dataset.
    return dataset


# The remainder of this file contains a simple example of a csv parser,
#     implemented using the `Dataset` class.

# `tf.parse_csv` sets the types of the outputs to match the examples given in
#     the `record_defaults` argument.
CSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]

def _parse_line(line):
    # Decode the line into its fields
    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)

    # Pack the result into a dictionary
    features = dict(zip(CSV_COLUMN_NAMES, fields))

    # Separate the label from the features
    label = features.pop('Species')

    return features, label


def csv_input_fn(csv_path, batch_size):
    # Create a dataset containing the text lines.
    dataset = tf.data.TextLineDataset(csv_path).skip(1)

    # Parse each line.
    dataset = dataset.map(_parse_line)

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)

    # Return the dataset.
    return dataset
```

**Other info / logs**

Some tensorboard images. First, I can always get a consistent loss (loss in step x for eval is equal to loss in step x+1 for training). Example:

![image](https://user-images.githubusercontent.com/42271354/63022114-7d704580-bea2-11e9-9873-19ed0bf715e0.png)

![image](https://user-images.githubusercontent.com/42271354/63022154-8e20bb80-bea2-11e9-8c96-dc2a45d79f76.png)

But the accuracies don't match in any apparent way. I could verify that the evaluation accuracy is indeed correct, when looking at the labels and predicted values, but the accuracy during training is different.

![image](https://user-images.githubusercontent.com/42271354/63022282-d7710b00-bea2-11e9-965b-f59bbf45a4e1.png)

I might be missing some obvious detail here, so please do let me know if you can spot something.
"
31616,"Dataset prefetch not working as expected, not storing data in memory","**System information**
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 LTS
- **TensorFlow installed from (source or binary)**: conda-forge
- **TensorFlow version (use command below)**: unknown 1.14.0
- **Python version**: Python 3.7.3
- **CUDA/cuDNN version**: NVIDIA-SMI 418.67, Driver Version: 418.67, CUDA Version: 10.1
- **GPU model and memory**: Quadro RTX 6000, 24190MiB
- **Exact command to reproduce**:

**Describe the current behavior**

I am training a small LSTM model and until recently, I could use `Dataset.from_tensor_slices()` reading numpy arrays directly because all training data fits into memory. Unfortunately, after adding some new data, I ran into the 2 GB graph memory limitation and was forced to switch to using `TFRecord` and `TFRecordDataset`. However, the actual training data still fits into RAM and I want to make sure it is prefetched even when using the `TFRecordDataset`. Therefore, I tried to use the `Dataset.prefetch()` methodology to achieve this, assuming a buffer will be created and (constantly) filled with data. However, it does not work - in fact, there seems to be little to no difference comparing a version with and without a final `.prefetch(x)` in the data pipeline. See the animated gif below:

![tf_prefetch](https://user-images.githubusercontent.com/53339396/63017447-a985c980-be96-11e9-91ce-0ea3cd23945f.gif)

The actual dataset is filtered in the pipeline and the training stalls whenever a sequence of values that is filtered out is occurring in the data. Only a few values in each data/tfrecord file (of which many exist) are relevant. To illustrate this further, the data layout is similar to this:

    file 1: [---------#####----------------####------]
    file 2: [--------######----------------###-------]
    file 3: [----------####----------------####------]
    file 4: ...
    ...

where `-` denotes irrelevant and `#` denotes relevant data points in a time series. When holding all values in memory (as previously was the case), the filter is rather fast and irrelevant values are skipped unnoticeable.

The data pipeline is set up like this:

    feature_description = { ""features"": tf.FixedLenFeature([132], tf.float32),  ""label"": tf.FixedLenFeature([1], tf.float32) }
    def _parse_function(example_proto):
        return tf.parse_single_example(example_proto, feature_description)
    
    ds = tf.data.TFRecordDataset([f.as_posix() for f in fs_train])
    ds = ds.map(_parse_function)
    ds = ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[""features""][2:], v[""label""])))
    # filter data, only allow Ls[0] and Ls[1]
    ds = ds.filter(
        lambda _, y: tf.reshape(tf.logical_or(
            tf.equal(y, Ls[0]),
            tf.equal(y, Ls[1])
        ), [])
    )
    # relabel and re-map labels to 0 and 1
    ds = ds.flat_map(lambda x, y: tf.data.Dataset.from_tensors((x, tf_relabel(y) - base_label.value)))
    # create sliding window for LSTM
    ds = ds.window(size=window_size, shift=shift, stride=stride, drop_remainder=True)
    ds = ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(window_size), y.batch(window_size))))
    
    # batch and prefetch
    ds = ds.batch(batch_size, drop_remainder=True)
    ds = ds.prefetch(1000000000000000) # tried many values, nothing works

**Describe the expected behavior**
I expect to find some value for `Dataset.prefetch()` that reads all or enough data to memory to allow for fast training without stalling.

**Code to reproduce the issue**
See data pipeline above. I cannot provide the data as it is proprietary.
"
31615,"I have installed tensorflow-gpu 2.0.0 alpha,how can I upgrade it to tensorflow-gpu==2.0.0-beta1,but didn't influence my tensorlayer.","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31614,Licencing of TensorFlow conda distribution in miniconda docker image,"I am using conda distribution of Tensorflow inside miniconda3 docker image.
miniconda3 docker image i have downloaded from below link:- 

https://hub.docker.com/r/continuumio/miniconda3

Installation of tensorflow & other dependencies in docker image:


    FROM continuumio/miniconda3
    RUN apt-get update && \
    apt-get install -y openjdk-8-jre-headless && \
    apt-get install -y ant && \
	apt-get install -y python3-tk && \
    apt-get clean;
    conda install --yes tensorflow==1.12


I created another docker image on the top of miniconda3 docker image.

Scan this final image in whitesource tool for getting information of GPL and AGPL licence. 

I am getting lot's of GPL and AGPL inside tensorflow package folder.

Below is the example of one package:-


    Slic3r-1.41.0-alpha3.1    GPL 2.0AGPL 3.0GPL    opt\conda\lib\python3.6\site-packages\tensorflow\include\Eigen\src\Core\Assign_MKL.h

Now i want to use this in our Enterprise application. is there any other way to use this in application or this the issue of tool to give GPL inside package?

 
"
31613,ImportError: DLL load failed,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution :windows 10 pro 16299
- TensorFlow installed from : conda install
- TensorFlow version (use command below): tensorflow 2.0.0b1
- Python version:3.6.9

log
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-6-64156d691fe5> in <module>
----> 1 import tensorflow as tf

~\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     38 import sys as _sys
     39 
---> 40 from tensorflow.python.tools import module_util as _module_util
     41 
     42 from tensorflow._api.v2 import audio

~\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\BLACK MANTIS\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\BLACK MANTIS\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\BLACK MANTIS\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\BLACK MANTIS\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\BLACK MANTIS\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
31612,tf-lite android so build fail,"**System information**
- OS Platform and Distribution:  (e.g., Linux Ubuntu 18.04)
- TensorFlow installed from (source or binary):  source 
- TensorFlow version: r1.13
- Python version: 2.7
- Bazel version (if compiling from source):  0.24.1
- GCC/Compiler version (if compiling from source):  gcc version 4.8.5
NDK version 14
SDK version 24

*Describe the problem**
`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cxxopt=-std=c++11    --cpu=arm64-v8a`

when i try the build command, it throws clang compile error. so why ndk14 use clang but not gcc g++?

The document might be confused, since some code have changed on git but doc haven't
In fact, I have try many tools version combination: ndk 14 / 12, bazel 0.20.0/ 0.21.0/ 0.24.1. Each combination throws different error

Could you show me a working tool verison combination of bazel, ndk, sdk and tf version, which is tested?

Thank you!

------------------------------------------------
**error log:**
ERROR: /home/ger/opt/tensorflow/tensorflow/core/kernels/BUILD:6575:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 1)
In file included from tensorflow/core/kernels/cwise_op_erf.cc:16:
In file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:
In file included from ./tensorflow/core/kernels/cwise_ops.h:23:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:29:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:415:5: error: static_assert failed ""THIS_TYPE_IS_NOT_SUPPORTED""
    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),
    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'
    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);
                                       ^             ~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2114:44: note: in instantiation of member function 'Eigen::internal::erf_impl<float>::run' requested here
  return EIGEN_MATHFUNC_IMPL(erf, Scalar)::run(x);
                                           ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:248:31: note: in instantiation of function template specialization 'Eigen::numext::erf<float>' requested here
    using numext::erf; return erf(a);
                              ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:434:12: note: in instantiation of member function 'Eigen::internal::scalar_erf_op<float>::operator()' requested here
    return m_functor(m_argImpl.coeff(index));
           ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:156:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here
    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);
                                         ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:218:17: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >, Eigen::ThreadPoolDevice>::evalScalar' requested here
      evaluator.evalScalar(i);
                ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:283:39: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
                           EvalRange::run(&evaluator, firstIdx, lastIdx);
                                      ^
./tensorflow/core/kernels/cwise_ops_common.h:278:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here
  out.device(d) = rhs;
                ^
./tensorflow/core/kernels/cwise_ops_common.h:541:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here
    Assign(d, out, in.unaryExpr(typename Functor::func()));
    ^
./tensorflow/core/kernels/cwise_ops_common.h:249:5: note: in instantiation of member function 'tensorflow::functor::UnaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::erf<float> >::operator()' requested here
    functor::UnaryFunctor<Device, Functor>()(
    ^
./tensorflow/core/kernels/cwise_ops_common.h:234:12: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<float> >::Compute' requested here
  explicit UnaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {
           ^
tensorflow/core/kernels/cwise_op_erf.cc:19:11: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<float> >::UnaryOp' requested here
REGISTER3(UnaryOp, CPU, ""Erf"", functor::erf, float, Eigen::half, double);
          ^
In file included from tensorflow/core/kernels/cwise_op_erf.cc:16:
In file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:
In file included from ./tensorflow/core/kernels/cwise_ops.h:23:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:29:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:415:5: error: static_assert failed ""THIS_TYPE_IS_NOT_SUPPORTED""
    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),
    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'
    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);
                                       ^             ~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2114:44: note: in instantiation of member function 'Eigen::internal::erf_impl<Eigen::half>::run' requested here
  return EIGEN_MATHFUNC_IMPL(erf, Scalar)::run(x);
                                           ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:248:31: note: in instantiation of function template specialization 'Eigen::numext::erf<Eigen::half>' requested here
    using numext::erf; return erf(a);
                              ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:434:12: note: in instantiation of member function 'Eigen::internal::scalar_erf_op<Eigen::half>::operator()' requested here
    return m_functor(m_argImpl.coeff(index));
           ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:156:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here
    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);
                                         ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:218:17: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long>, 16, MakePointer>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long>, 16, MakePointer> > >, Eigen::ThreadPoolDevice>::evalScalar' requested here
      evaluator.evalScalar(i);
                ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:283:39: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
                           EvalRange::run(&evaluator, firstIdx, lastIdx);
                                      ^
./tensorflow/core/kernels/cwise_ops_common.h:278:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long>, 16, MakePointer> > >' requested here
  out.device(d) = rhs;
                ^
./tensorflow/core/kernels/cwise_ops_common.h:541:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long>, 16, MakePointer> > >' requested here
    Assign(d, out, in.unaryExpr(typename Functor::func()));
    ^
./tensorflow/core/kernels/cwise_ops_common.h:249:5: note: in instantiation of member function 'tensorflow::functor::UnaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::erf<Eigen::half> >::operator()' requested here
    functor::UnaryFunctor<Device, Functor>()(
    ^
./tensorflow/core/kernels/cwise_ops_common.h:234:12: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<Eigen::half> >::Compute' requested here
  explicit UnaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {
           ^
tensorflow/core/kernels/cwise_op_erf.cc:19:11: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<Eigen::half> >::UnaryOp' requested here
REGISTER3(UnaryOp, CPU, ""Erf"", functor::erf, float, Eigen::half, double);
          ^
In file included from tensorflow/core/kernels/cwise_op_erf.cc:16:
In file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:
In file included from ./tensorflow/core/kernels/cwise_ops.h:23:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:29:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:415:5: error: static_assert failed ""THIS_TYPE_IS_NOT_SUPPORTED""
    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),
    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'
    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);
                                       ^             ~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2114:44: note: in instantiation of member function 'Eigen::internal::erf_impl<double>::run' requested here
  return EIGEN_MATHFUNC_IMPL(erf, Scalar)::run(x);
                                           ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:248:31: note: in instantiation of function template specialization 'Eigen::numext::erf<double>' requested here
    using numext::erf; return erf(a);
                              ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:434:12: note: in instantiation of member function 'Eigen::internal::scalar_erf_op<double>::operator()' requested here
    return m_functor(m_argImpl.coeff(index));
           ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:156:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<double>, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here
    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);
                                         ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:218:17: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<double>, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >, Eigen::ThreadPoolDevice>::evalScalar' requested here
      evaluator.evalScalar(i);
                ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:283:39: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
                           EvalRange::run(&evaluator, firstIdx, lastIdx);
                                      ^
./tensorflow/core/kernels/cwise_ops_common.h:278:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<double>, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >' requested here
  out.device(d) = rhs;
                ^
./tensorflow/core/kernels/cwise_ops_common.h:541:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<double>, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >' requested here
    Assign(d, out, in.unaryExpr(typename Functor::func()));
    ^
./tensorflow/core/kernels/cwise_ops_common.h:249:5: note: in instantiation of member function 'tensorflow::functor::UnaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::erf<double> >::operator()' requested here
    functor::UnaryFunctor<Device, Functor>()(
    ^
./tensorflow/core/kernels/cwise_ops_common.h:234:12: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<double> >::Compute' requested here
  explicit UnaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {
           ^
tensorflow/core/kernels/cwise_op_erf.cc:19:11: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<double> >::UnaryOp' requested here
REGISTER3(UnaryOp, CPU, ""Erf"", functor::erf, float, Eigen::half, double);
          ^
3 errors generated.
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
"
31611,[issue]Pb convert to tflite :how to calculate output min max of concatenation layer,"I notice that tensorflow source code in quantized_concat_op.cc use the function of **CalculateInputAndOutputRange()** to caculate the minmax value of input and output feature maps, which just let the output min value be the minimum of the  input_mins[i], and max value be the maximum of the  input_maxes[i].
But when i convert pb file to tflite, the minmax value of output feature map of  concat layer not be the value  what it should be.


this is my convet command:
toco  \
--graph_def_file=rpn_190516.pb \
--output_file=rpn_190516_tt.tflite \
--input_shapes=1,2048,2048,3 \
--input_arrays='input_rpn' \
--output_arrays='concat' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_dev_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops


and the reslut screenshot:
![image](https://user-images.githubusercontent.com/13192737/63005315-d7770800-beae-11e9-9372-edd151d909dc.png)

i would appreciate it , if someone can explain the result.
"
31610,"fatal error LNK1201:check for insufficient disk space, invalid path, or insufficient privilege","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
- TensorFlow installed from (source or binary): build from source
- TensorFlow version: 1.12.0
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): VS2015 update3
- CUDA/cuDNN version:9/7
- GPU model and memory:Titan V 11G



**Describe the problem**
When I build debug version Tensorflow on windows 10Linker error 1201 ocoured
LINK : fatal error LNK1201: error writing to program database 'C:\users\dl\_bazel_dl\hd37h63o\execroot\org_tensorflow\bazel-out\x64_windows-dbg\bin\tensorflow\libtensorflow_cc.pdb'; check for insufficient disk space, invalid path, or insufficient privilege


**Any other info / logs**
ERROR: D:/clone/tensorflow-1.12.0/source/tensorflow/BUILD:449:1: Linking of rule '//tensorflow:libtensorflow_cc.so' failed (Exit 1201): link.exe failed: error executing command
  cd C:/users/dl/_bazel_dl/hd37h63o/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/clone/tensorflow-1.12.0/venv/Scripts/python.exe
    SET PYTHON_LIB_PATH=D:/clone/tensorflow-1.12.0/venv/lib/site-packages
    SET TEMP=C:\Users\DL\AppData\Local\Temp
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\DL\AppData\Local\Temp
    SET USE_LINKER=1
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -def:tensorflow/tf_exported_symbols_msvc.lds -defaultlib:advapi32.lib -DEFAULTLIB:advapi32.lib /MACHINE:X64 @bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow_cc.so-2.params /DEBUG:FULL /INCREMENTAL:NO
   Creating library bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow_cc.so.if.lib and object bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow_cc.so.if.exp
libtensorflow_cc.so.if.exp : warning LNK4070: /OUT:tensorflow_cc.dll directive in .EXP differs from output filename 'bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow_cc.so'; ignoring directive
pin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
collective_param_resolver_distributed.lib(collective_param_resolver_distributed.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
batch_kernels.lo.lib(batch_kernels.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
captured_function.lib(captured_function.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
arithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
tfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_NewStatus imported in function ""protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)"" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)
tfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_DeleteStatus imported in function ""protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)"" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)
tfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_GetCode imported in function ""protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)"" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)
tfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_Message imported in function ""protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)"" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : warning LNK4217: locally defined symbol ??0ErrorCode@icu_62@@QEAA@XZ (public: __cdecl icu_62::ErrorCode::ErrorCode(void)) imported in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : warning LNK4217: locally defined symbol ??1ErrorCode@icu_62@@UEAA@XZ (public: virtual __cdecl icu_62::ErrorCode::~ErrorCode(void)) imported in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : warning LNK4217: locally defined symbol ??BErrorCode@icu_62@@QEAAPEAW4UErrorCode@@XZ (public: __cdecl icu_62::ErrorCode::operator enum UErrorCode *(void)) imported in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : warning LNK4217: locally defined symbol ?isSuccess@ErrorCode@icu_62@@QEBACXZ (public: signed char __cdecl icu_62::ErrorCode::isSuccess(void)const ) imported in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : warning LNK4217: locally defined symbol ?reset@ErrorCode@icu_62@@QEAA?AW4UErrorCode@@XZ (public: enum UErrorCode __cdecl icu_62::ErrorCode::reset(void)) imported in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
icuuc.lib(udata.obj) : warning LNK4049: locally defined symbol icudt62_dat imported
arithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
layout_optimizer.lib(layout_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
pin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
LINK : fatal error LNK1201: error writing to program database 'C:\users\dl\_bazel_dl\hd37h63o\execroot\org_tensorflow\bazel-out\x64_windows-dbg\bin\tensorflow\libtensorflow_cc.pdb'; check for insufficient disk space, invalid path, or insufficient privilege"
31609,"""Init node weights/Assign doesn't exist in graph"" happens when use convert in tflite","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):
- TensorFlow version: Tensorflow nightly
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 7/10

**Describe the problem**
When I tried to convert a TensorFlow GraphDef into a TensorFlow Lite FlatBuffer from a tf.Session object, a error happend such like this:
`2019-08-14 16:01:23.946453: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-08-14 16:01:23.947157: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node weights/Assign doesn't exist in graph
`
and my code all showed below:


    def main(_):

      def loss_function(weight, logits, labels):
        labels = tf.one_hot(labels,4)
        labels = tf.cast(labels, tf.float32)
        first = tf.reduce_sum(tf.multiply(-labels, logits),1)
        second_0 = tf.add(tf.exp(logits[:,0]),tf.exp(logits[:,1]))
        second_1 = tf.add(tf.exp(logits[:,2]),tf.exp(logits[:,3]))
        log = tf.log(tf.add(second_1,second_0))
        weight = tf.transpose(tf.reduce_sum(tf.multiply(labels, weight),1))
        output = tf.multiply(weight,tf.add(first,log))

        return output

      def normalize(stft):
        stft_1 = numpy.empty([stft.shape[0],128,128])
        stft_2 = numpy.empty([stft_1.shape[0],stft_1.shape[1],stft_1.shape[2],1])
        for i in range(stft_1.shape[0]):
          image = Image.fromarray(stft[i,:,:])
          image = image.resize([128,128])
          stft_1[i,:,:] = numpy.array(image)

          min = numpy.min(stft_1[i,:,:])
          max = numpy.max(stft_1[i,:,:])
          stft_1[i,:,:] = (stft_1[i,:,:]-min)/(max-min)
          stft_2[i,:,:,:] = stft_1[i,:,:].reshape((stft_1.shape[1],stft_1.shape[2],1))
        return stft_2  
    # Get the data.
    
    stft_training, mfcc_training, labels_training = joblib.load(open(FLAGS.input, mode='rb'))
    stft_test, mfcc_test, labels_test = joblib.load(open(FLAGS.test, mode='rb'))

    stft_test = numpy.array(stft_test)
    mfcc_test = numpy.array(mfcc_test)
    labels_test = numpy.array(labels_test)
    stft_test = normalize(stft_test)
    mfcc_test = normalize(mfcc_test)

    stft_training = numpy.array(stft_training)
    mfcc_training = numpy.array(mfcc_training)
    labels_training = numpy.array(labels_training)
    stft_training = normalize(stft_training)
    mfcc_training = normalize(mfcc_training)

    stft_shape = stft_training.shape
    stft_shape = (None, stft_shape[1], stft_shape[2], 1)

    mfcc_shape = mfcc_training.shape
    mfcc_shape = (None, mfcc_shape[1], mfcc_shape[2], 1)

    labels_shape = labels_training.shape
    labels_shape = (None)

    stft_placeholder = tf.placeholder(stft_training.dtype, stft_shape)
    labels_placeholder = tf.placeholder(labels_training.dtype, labels_shape)
    mfcc_placeholder = tf.placeholder(mfcc_training.dtype, mfcc_shape)
    
    dataset_training = tf.data.Dataset.from_tensor_slices((stft_placeholder, mfcc_placeholder, labels_placeholder))
    dataset_training  = dataset_training.apply(
        tf.data.experimental.shuffle_and_repeat(len(stft_training), None))  
    dataset_training  = dataset_training.batch(BATCH_SIZE)
    dataset_training  = dataset_training.prefetch(1)
    iterator_training = dataset_training.make_initializable_iterator()
    next_element_training = iterator_training.get_next()
    num_epochs = FLAGS.epochs

      train_size = labels_training.shape[0]

      with tf.name_scope('input'):
        stft = tf.placeholder(
            name=""stft"",
            dtype=data_type(),
            shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
        mfcc = tf.placeholder(
            name=""mfcc"",
            dtype=data_type(),
            shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
        labels = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))

      with tf.name_scope('test_input'):
        stft_t = tf.placeholder(
            data_type(),
            shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
        mfcc_t = tf.placeholder(
            data_type(),
            shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))

      model = BRN()
      logits = model.forward(stft, mfcc)
      logits_ = tf.add(0.,logits,name=""logits_"")
      try:
        scalar_summary = tf.scalar_summary
        SummaryWrite = tf.train.SummaryWrite
        merge_summary = tf.merge_summary
      except:
        scalar_summary = tf.summary.scalar
        SummaryWrite = tf.summary.FileWriter
        merge_summary = tf.summary.merge
      with tf.name_scope('loss'):
        weights = [1.0, 1.7, 4.1, 5.7]
         mid = loss_function(weights, logits=logits, labels=labels)
        loss = tf.reduce_sum(mid)
    
        loss_summary = scalar_summary('loss', loss)
        regularizers = (tf.nn.l2_loss(model.conv1_weights) + tf.nn.l2_loss(model.conv2_weights) +
                    tf.nn.l2_loss(model.fc_weights) + tf.nn.l2_loss(model.fc_biases))

        batch = tf.Variable(0, dtype=data_type())
 
      with tf.name_scope('train'):

        optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)
      train_prediction = tf.nn.softmax(logits)
      eval_prediction = tf.nn.softmax(model.forward(stft_t, mfcc_t))
      start_time = time.time()

      def eval_in_batches(stft_data, mfcc_data, sess, type):
        size = stft_data.shape[0]
        if size < EVAL_BATCH_SIZE:
          raise ValueError(""batch size for evals larger than dataset: %d"" % size)
        predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)
        for begin in xrange(0, size, EVAL_BATCH_SIZE):
          end = begin + EVAL_BATCH_SIZE
          if end <= size:
            if type == 'train':
              predictions[begin:end, :] = sess.run(
                  train_prediction,
                  feed_dict={stft: stft_data[begin:end, ...], mfcc: mfcc_data[begin:end, ...]})
            else: 
              predictions[begin:end, :] = sess.run(
                  eval_prediction,
                  feed_dict={stft_t: stft_data[begin:end, ...], mfcc_t: mfcc_data[begin:end, ...]})
          else:
            if type == 'train':
              batch_predictions = sess.run(
                  train_prediction,
                  feed_dict={stft: stft_data[-EVAL_BATCH_SIZE:, ...], mfcc: mfcc_data[-EVAL_BATCH_SIZE:, ...]})
            else:
               batch_predictions = sess.run(
                  eval_prediction,
                  feed_dict={stft_t: stft_data[-EVAL_BATCH_SIZE:, ...], mfcc_t: mfcc_data[-EVAL_BATCH_SIZE:, ...]})
            predictions[begin:, :] = batch_predictions[begin - size:, :]
        return predictions


      config = tf.ConfigProto()
      config.gpu_options.allow_growth = True  

      with tf.Session(config=config) as sess:
   
        tf.global_variables_initializer().run()

        merged = tf.summary.merge_all()
        writer = SummaryWrite(FLAGS.logs + 'train', sess.graph)
        sess.run(iterator_training.initializer, feed_dict={stft_placeholder:stft_training,
                          mfcc_placeholder:mfcc_training,
                         labels_placeholder:labels_training})

        for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):

          batch_stft, batch_mfcc, batch_labels = sess.run(next_element_training)
  
          feed_dict = {stft: batch_stft,
                       mfcc: batch_mfcc,
                       labels: batch_labels}
          sess.run(optimizer, feed_dict=feed_dict)
          if step % EVAL_FREQUENCY == 0:

        summary, l = sess.run([merged, loss],
                                      feed_dict=feed_dict)
            writer.add_summary(summary, step)
            elapsed_time = time.time() - start_time
            start_time = time.time()
            rate, acc = error_rate(eval_in_batches(stft_training, mfcc_training, sess, 'train'), labels_training)
            acc_summary = scalar_summary('accuracy', acc)
            print('Step %d (epoch %.2f), Minibatch loss: %.3f, Minibatch error: %.1f%%, Accuracy:%.4f' %
              (step, float(step) * BATCH_SIZE / train_size,
              l,rate, acc))
            sys.stdout.flush()
            test_error, test_acc = error_rate(eval_in_batches(stft_test, mfcc_test, sess, 'test'), labels_test)
            print('Testset error: %.1f%%, Accuracy:%.4f' % (test_error, test_acc))

    converter = tf.lite.TFLiteConverter.from_session(sess, [stft,mfcc], [logits_])
    tflite_model = converter.convert()
    open(""BRN.tflite"", ""wb"").write(tflite_model)
        
    writer.close()

When I run the official demo of converting a TensorFlow GraphDef into a TensorFlow Lite FlatBuffer from a tf.Session object, the error also happens. Does that ok? I mean, can I use the weight trained in TensorFlow Lite? or the file doesn't save the weight?

"
31608,Auto-Configuration Error: Couldn't find undname.exe under C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\,"**System information**
- OS Platform : Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.13
- Python version:  python3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.21.0
- Compiler version (if compiling from source): VS2019,16.2,MSVC++=14.22

**Describe the problem**
Auto-Configuration Error: Couldn't find undname.exe under C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\, please check your VC installation and set BAZEL_VC environment variable correctly.


**Provide the exact sequence of commands / steps that you executed before running into the problem**

1)conda create -n tensorflow-1.13.1-without-mkl python=3.6 anaconda
2)conda activate tensorflow-1.13.1-without-mkl
3)Install MSYS2
set environmental variable, under PATH -> C:\msys64\usr\bin
4)Install JDK12 and set environmental variable : JAVA_HOME = C:\Program Files\Java\jdk-12
5)pacman -Syu zip unzip
6)pacman -Syuu --noconfirm patch
7)set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC
8) set BAZEL_SH=C:\msys64\usr\bin\bash.exe
9) git clone https://github.com/tensorflow/tensorflow.git -b r1.4
10) python ./configure.py

You have bazel 0.21.0 installed.
Please specify the location of python. [Default is C:\Users\zen2-\Anaconda2\envs\tensorflow-1.13.1-without-mkl\python.exe]:
Found possible Python library paths:
  C:\Users\zen2-\Anaconda2\envs\tensorflow-1.13.1-without-mkl\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\zen2-\Anaconda2\envs\tensorflow-1.13.1-without-mkl\lib\site-packages]
Do you wish to build TensorFlow with XLA JIT support? [y/N]: N
No XLA JIT support will be enabled for TensorFlow.
Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:
Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y

11)bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**

WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
INFO: Invocation ID: ca6515c1-83e0-4ddf-a065-733711b4a9ff
ERROR: C:/users/zen2-/tensorflow/tensorflow/python/BUILD:4172:1: no such package '@local_config_def_file_filter//': Traceback (most recent call last):
        File ""C:/users/zen2-/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl"", line 37
                auto_configure_fail((""Couldn't find undname.exe unde...))
        File ""C:/users/zen2-/_bazel_zen2_microsoft/ldf7kfdv/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 109, in auto_configure_fail
                fail((""\n%sAuto-Configuration Error:%...)))

Auto-Configuration Error: Couldn't find undname.exe under C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\, please check your VC installation and set BAZEL_VC environment variable correctly.
 and referenced by '//tensorflow/python:pywrap_tensorflow_filtered_def_file'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@local_config_def_file_filter//': Traceback (most recent call last):
        File ""C:/users/zen2-/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl"", line 37
                auto_configure_fail((""Couldn't find undname.exe unde...))
        File ""C:/users/zen2-/_bazel_zen2_microsoft/ldf7kfdv/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 109, in auto_configure_fail
                fail((""\n%sAuto-Configuration Error:%...)))

Auto-Configuration Error: Couldn't find undname.exe under C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\, please check your VC installation and set BAZEL_VC environment variable correctly.
INFO: Elapsed time: 0.713s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (5 packages loaded, 20 targets configured)
    currently loading: tensorflow/lite/schema ... (3 packages)
    Fetching @cython; fetching
    Fetching @swig; fetching
    Fetching @local_config_git; Restarting.
    Fetching @grpc; fetching
    Fetching @local_config_def_file_filter; fetching
    Fetching @flatbuffers; fetching
    Fetching @eigen_archive; fetching
    Fetching @icu; fetching ... (9 fetches)

Please correct me in case wrong . "
31607,Keras Models with tf.sparse.sparse_dense_matmul can't be saved - Not JSON Serializable ,"**System information**
- OS Platform and Distribution: Windows 10 Version 1607
- TensorFlow version: 2.0.0-beta1
- Python version: 3.7.3

**Describe the current behavior**
Error:
```
Exception has occurred: TypeError
('Not JSON Serializable:', b'\n\ttranspose\x12\tTranspose\x1a\x10dense_1/Identity\x1a\x0etranspose/perm*\x0b\n\x05Tperm\x12\x020\x03*\x07\n\x01T\x12\x020\x01')
  File ""C:\Visual_Studio_Codes\Sparse_Save.py"", line 30, in <module>
    model.save(""model.h5"")
```

**Describe the expected behavior**
Save without error


**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.layers import (Dense, Input, Lambda)
from tensorflow.keras.models import Model, Sequential
from scipy import sparse
import numpy as np


def layer_lambda(input_x):
    sparse = input_x[0]
    dense = input_x[1]
    dense = tf.transpose(dense)
    y = tf.sparse.sparse_dense_matmul(sparse, dense)
    return tf.transpose(y)


dense_mat = np.eye(30, 30, dtype=np.float32)
sparse_mat = sparse.coo_matrix(dense_mat)
sparse_indices = np.mat([sparse_mat.row, sparse_mat.col]).transpose()
sparse_tensor = tf.SparseTensor(sparse_indices, sparse_mat.data, sparse_mat.shape)

model = Sequential()
model_input = Input(shape=(20,))
x = Dense(20)(model_input)
x = Dense(30)(x)
x = Lambda(layer_lambda, output_shape=(None, 30, 30))([sparse_tensor, x])
model = Model(model_input, x)

model.predict([[np.ones(20)]])

model.save(""model.h5"")
```"
31605,QueueDequeueV2 Operation's device is different between tensorboard and timeline,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source 
- TensorFlow version (use command below):1.12.0
- Python version:3.6
- Bazel version (if compiling from source):0.18
- GCC/Compiler version (if compiling from source):5.0
- CUDA/cuDNN version:9.0
- GPU model and memory:P100


**Describe the current behavior**
On tensorboard QueueDequeueV2 Operation's device is GPU:1.
![image](https://user-images.githubusercontent.com/8842010/62991048-b992ae80-be80-11e9-9dfe-e0661e29d9ae.png)
But on timeline, it is CPU:0
![image](https://user-images.githubusercontent.com/8842010/62991076-c9aa8e00-be80-11e9-9876-0066bdc71261.png)

**Describe the expected behavior**
Which one is correct Set log_device_placement=True, we can see QueueDequeueV2 is on cpu(clone_1/fifo_queue_Dequeue: (QueueDequeueV2)/job:worker/replica:0/task:0/device:CPU:0)
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31604, custom implementations: DEPTH_TO_SPACE.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 14.04):
- TensorFlow installed from (binary):
- TensorFlow version (1.14.0):

  Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, TANH. Here is a list of operators for which you will need custom implementations: DEPTH_TO_SPACE.


```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31603,Lookahead optimizer,"**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No):Yes

**Describe the feature and the current behavior/state.**

https://arxiv.org/abs/1907.08610

This is not a conventional optimizer as in tensorflow API. It observes through k iterations of optimizers(SGD, Adam), and linear interpolates entire weight into direction where fast weight after k iteration has reached. 

I have seen an implementation of this defining two ops for slow weight update and fast weight update. However I see some potential performance benefit if it was defined on backend. 

**Will this change the current api? How?**

I believe not.

**Who will benefit with this feature?**

Tasks with heavy training load will benefit from faster learning rate.

**Any Other info.**
"
31602,How the embedding matrix is optimized using DNNLinearCombinedClassifier,"From the document: https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNLinearCombinedClassifier

We know the parameter includes linear_optimizer and dnn_optimizer that can be used to optimize the linear part and dnn part of the weights. However, it seems that the optimization of the weight of embedding matrix is not involved, such as embedding matrix of the wide and deep model. So my question is that is the optimization of the embedding matrix not involved in reality or is there any default way of optimizing the embedding matrix that I do not know

estimator = DNNLinearCombinedClassifier(
    # common settings
    n_classes=n_classes,
    weight_column_name=weight_column_name,
    # wide settings
    linear_feature_columns=[sparse_feature_a_x_sparse_feature_b],
    linear_optimizer=tf.compat.v1.train.FtrlOptimizer(...),
    # deep settings
    dnn_feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    dnn_hidden_units=[1000, 500, 100],
    dnn_optimizer=tf.compat.v1.train.AdagradOptimizer(...))

"
31600,Issue with model.predict(),"
- **Have I written custom code - Yes I have written some custom code
- **OS Platform and Distribution - Ubuntu Server 18.04
- **TensorFlow installed from - Installed by pip
- **TensorFlow version (use command below)**: v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- **Python version**: 3.6.7
- **Exact command to reproduce**: 
```
        trainarray = np.array([0,0,0,0,0,0,0])
        model = tf.keras.models.Sequential([
          tf.keras.layers.Dense(7, input_shape= trainarray.shape),
          tf.keras.layers.Dense(12),
          tf.keras.layers.Dense(1)
        ])
        model.predict(np.array([0,0,0,0,0,0,0]))
```

### Describe the problem
When I try to predict with the model created above I get the following error: 

> ValueError: Error when checking input: expected dense_input to have shape (7,) but got array with shape (1,)

The input array (np.array([0,0,0,0,0,0,0])) does obviously have the shape 7, but not 1, so to me this looks like a bug. I've tried to go under the hood to see what might be causing this, but I haven't found the cause yet.
"
31598,In 1.14  does using  TF-TRT still show TensorRT nodes in Tensorboard graph ?,I am not seeing any TensorRT related nodes in  Tensorflow 1.14  in Tensorboard graph.   Is that supposed to be that way ?  In that case how do I make sure TF-TRT is in fact used ? 
31596,TFLiteConverter fails with tf.gather when the params argument is a layer attribute,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS 
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2.0.0-dev20190807
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0
- GPU model and memory: 8 x Tesla P100-PCIE-16GB 

**Describe the current behavior**
I am not able to convert a SavedModel to a FlatBuffer using TFLiteConverter when the corresponding tf.keras.Model contains a layer with a tf.gather op for which the params argument comes from a variable that was initialized in the build method of that said layer.

When the params argument is from a locally defined variable, or when using tf.nn.embedding_lookup instead of tf.gather, everything works perfectly fine.

It also applies to tf.gather_nd.

**Describe the expected behavior**
I expect tf.gather to work for the case in which the params argument is an attribute of the tf.keras.layers.Layer, just as it does for the other cases mentioned.

**Code to reproduce the issue**
I wrote a toy example to reproduce the issue, it might be clearer than the description above.
```
import numpy as np
import tensorflow as tf
print(tf.__version__)

class Embedding(tf.keras.layers.Layer):
    def __init__(self, vocab_size, hidden_size):
        super(Embedding, self).__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
    
    def build(self, input_shape):
        self.shared_weights = self.add_weight(
            ""weights"",
            shape=(self.vocab_size, self.hidden_size),
            dtype=tf.float32,
            initializer=tf.random_normal_initializer(
                mean=0.0, 
                stddev=self.hidden_size ** (-0.5)
            )
        )
    
    def call(self, input_):
        # return tf.nn.embedding_lookup(self.shared_weights, input_)
        # return tf.gather(tf.zeros(shape=(self.vocab_size, self.hidden_size)), input_)
        return tf.gather(self.shared_weights, input_)


class SimpleModel(tf.keras.Model):
    def __init__(self, vocab_size, hidden_size):
        super(SimpleModel, self).__init__()
        self.embedding_layer = Embedding(vocab_size, hidden_size)
    
    @tf.function(input_signature=[tf.TensorSpec(shape=(None, ), dtype=tf.int64, name='input')])
    def call(self, input_):
        return self.embedding_layer(input_)

vocab_size = 20000
hidden_size = 300

# Building the model.
model = SimpleModel(vocab_size, hidden_size)
input_ = tf.random.uniform(shape=(20, ), dtype=tf.int64, maxval=100)
model(input_)

# Exporting to SavedModel.
saved_model_dir = 'simple_model/'
tf.saved_model.save(model, saved_model_dir)

# TFLite conversion.
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()
```

**Other info / logs**

```
Traceback (most recent call last):
  File ""/home/michael/.conda/envs/tf20/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: Placeholder statefulpartitionedcall_args_1 should be specied by input_arrays.

```


"
31595,How to create a concrete function for getting coordinates of a masked image and convert it to tensorflow lite model,"How to create concrete function in tensorflow to get semantic segmentation image and calculate co-ordinates.

We are having current code for the same but not able to concrete function as we wanted to convert in tflite file for mobile application.

Existing normal code is given below:
def predict_new_image(img_path, model):
img = load_img(img_path, grayscale=True)
x_img = img_to_array(img)
x_img = resize(x_img, (128, 128, 1), mode='constant', preserve_range=True)

X = np.zeros((1, 128, 128, 1), dtype=np.float32)
y_img = np.zeros((1, 128, 128, 1), dtype=np.float32)
X[0, ..., 0] = x_img.squeeze() / 255

pred = model.predict(X)
preds_img = (pred > 0.5).astype(np.uint8)

img_arr = preds_img[:,:,0]

# get the coordinates where the pixel isn't white (at a threshold)
black_thres = 1
idx = [(i,j) for i,x in enumerate(img_arr) for j,y in enumerate(x) if img_arr[i,j]==black_thres]
return idx"
31593,parallel_for: No converter defined for Cross,"Currently PFor is not registered for tf.cross operation, as evident in [pfor.py](https://github.com/tensorflow/tensorflow/blob/21f5e55ccc42daf14e6386d1d27d0103b29f2c92/tensorflow/python/ops/parallel_for/pfor.py).

With this operation being very fundamental to linear algebra, it seems to me it would be very useful, particularly for enabling vectorization for efficient jacobain computation of functions using the cross product."
31591,tf.signal.stft does not work in eager execution mode,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: GTX 1060

**Describe the current behavior**
tf.signal.stft does not work in eager execution mode, producing the error:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/signal/spectral_ops.py"", line 83, in stft
    signals, frame_length, frame_step, pad_end=pad_end)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/signal/shape_ops.py"", line 120, in frame
    num_outer_dimensions = array_ops.size(outer_dimensions)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 416, in size
    return size_internal(input, name, optimize=True, out_type=out_type)
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 437, in size_internal
    num_elements = np.prod(input._shape_tuple(), dtype=np_out_type)  # pylint: disable=protected-access
  File ""/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 2772, in prod
    initial=initial)
  File ""/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 86, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'
```
**Describe the expected behavior**
tf.signal.stft should work as it does without eager execution

**Code to reproduce the issue**
```
import tensorflow as tf
tf.enable_eager_execution()
input = tf.keras.Input([None])
spec = tf.signal.stft(input, 400, 160)
```"
31588,Performance issue: per-device memory usage increases with number of devices within tf.distribute strategy ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): tf-nightly-gpu-2.0-preview==
- TensorFlow version (use command below): v1.12.1-8566-g207bd43 2.0.0-dev20190812
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: (4) Titan Xp 12 gb 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Tensor memory allocation fails for the same batch size (determined by n_acton_samples)  as I scale the number of GPUs in my strategy.

**Describe the expected behavior**
Without the distributed strategy, I can readily pass a batch size of 300 to a given GPU, within the distributed strategy, this drops to 200 on 2 GPUs, and 70 on 4 GPUs.

**Code to reproduce the issue**
<details>
  <summary>Example code snippet</summary>

```
def tree_search(
    inputs, self_model, world_model, plan_sequence
):
    n_action_samples = 1000
    tree_value = tf.zeros([n_action_samples, 1], dtype=tf.float32)

    for idx, action in enumerate(plan_sequence):
        # print(""idx: "", idx)
        if idx == 0 and action:

            candidate_actions = tf.random.uniform(
                [n_action_samples, 1, num_fingers, 7],
                minval=-1.0,
                maxval=1.0,
                dtype=tf.float32,
            )


        elif idx > 0 and action:
            pass

        elif idx > 0 and not action:
               candidate_actions = tf.zeros(
                [n_action_samples, 1, num_fingers, 7],
                dtype=tf.float32,
            )


        else:
            pass

        tree_value += policy_model(candidate_actions, inputs)
        inputs = world_model(candidate_actions, inputs)

    retval = {""action"": candidate_actions, ""tree_value"": tree_value}

    return retval

@tf.function
def policy(
    inputs,
    policy_model,
    world_model,
    strategy
):

    take_action = [True, False, False]

    distributed_output = strategy.experimental_run_v2(
        tree_search,
        args=(
            inputs,
            self_model,
            world_model,
            plan_sequence,
        ),
    )

     distributed_output[""tree_value""] = tf.concat(
            strategy.experimental_local_results(distributed_output[""tree_value""]), axis=0
        )

     distributed_output[""action""] = tf.concat(
            strategy.experimental_local_results(distributed_output[""action""]), axis=0
        )


    action = choose_max_value_action(
       distributed_output[""action""], distributed_output[""tree_value""]
    )

    return action
```

</details>


**Other info / logs**
On a related note, it seems like there's a regression in the tf.summary.start_trace() based method of profiling models that is required by 2.0. The new tensorboard interface allows visualizing execution time of ops, but there doesn't seem to be a way to show memory usage."
31583,From object detection using Tensorflow,"from utils import label_map_util

from utils import visualization_utils as vis_util 

ImportError                               Traceback (most recent call last)
<ipython-input-4-956de605e8fe> in <module>()
----> 1 from utils import label_map_util
      2 
      3 from utils import visualization_utils as vis_util

C:\Users\hp\Downloads\models\research\object_detection\utils\label_map_util.py in <module>()
     24 import tensorflow as tf
     25 from google.protobuf import text_format
---> 26 from object_detection.protos import string_int_label_map_pb2
     27 
     28 

ImportError: cannot import name 'string_int_label_map_pb2'
"
31582,[TF2] Unhashable variables breaks ExponentialMovingAverage,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tf-nightly-2.0-preview

**Describe the current behavior**
As described in https://github.com/tensorflow/tensorflow/commit/2e1214094b6a78ab72d39051c7fd6e86c682ddf4#diff-ae1a8f7b66539f000615a4ab7e4b2151 

Variables are no longer hashable in TF2. This causes the dictionary tracking of variables to break:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L371
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L448
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L462

**Describe the expected behavior**
We can likely just keep the variable names as the dictionary keys.

**Code to reproduce the issue**
```
import tensorflow as tf

foo = tf.Variable(3.0)
ema = tf.train.ExponentialMovingAverage(0.1)
decayed_foo = ema.apply([foo])
```

```
Traceback (most recent call last):
  File ""break.py"", line 5, in <module>
    decayed_a = ema.apply([a])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/moving_averages.py"", line 425, in apply
    if var not in self._averages:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py"", line 1085, in __hash__
    raise TypeError(""Variable is unhashable if Tensor equality is enabled. ""
TypeError: Variable is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as t
```
"
31579,[TF 2.0] allow tf.function input_signature to be specified by annotations,"**System information**

- TensorFlow version (you are using): 2.0.0-rc0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

`tf.function` has an argument `input_signature` which I have been using to try and make my code a bit safer and ensure I don't keep re-tracing functions. The `input_signature` specifies the tensor type for each of the function arguments. It would be much nicer (I think) to specify these types using python (>=3.5) annotations, where a suitable version of python is available. A very rough example looks like:

``` python
import tensorflow as tf


def function(fn):
    input_signature = list(fn.__annotations__.values())
    return tf.function(fn, autograph=False, input_signature=input_signature)


@function
def foo(
    x: tf.TensorSpec(shape=[None], dtype=tf.float64),
    y: tf.TensorSpec(shape=[None], dtype=tf.float64),
):
    return x + 10.0 + y


vec32 = tf.random.normal([2], dtype=tf.float32)
vec64 = tf.random.normal([2], dtype=tf.float64)


# should pass
foo(vec64, vec64)
foo(y=vec64, x=vec64)

# should fail
foo(vec32, vec64)
```

Which I think is nicer than the current signature:

``` python
@tf.function(
    autograph=False,
    input_signature=[
        tf.TensorSpec(shape=[None], dtype=tf.float64),
        tf.TensorSpec(shape=[None], dtype=tf.float64),
    ],
)
def foo(x, y):
    return x + 10.0 + y
```

I think the main benefit of the annotation approach is that the argument name and type are beside each other, and this syntax is already widely used in python.

In order to enable using annotations as the `input_signature` I think there should be an extra boolean argument to `tf.function` called e.g. `use_annotation_input_signature` which defaults to `False`.

Also note I have set `autograph=False` here to avoid a warning:

> Cause: name 'foo_scope' is not defined

I am guessing a proper implementation inside of `tf.function` would not have this problem.

**Will this change the current api? How?**

It would add an additional argument to `tf.function` which at the default value would not change anything.

**Who will benefit with this feature?**

Anyone using python >= 3.5 who would like to specify the tensor types of their functions.

**Any Other info.**

None"
31577,TensorFlow Lite BroadcastTo,"**System information**
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
TensorFlow installed from (source or binary):source
TensorFlow version (use command below): tf-nightly
Python version: Python 3.6
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: NVIDIA GeForce GTX 1050 Ti



**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DIV, FULLY_CONNECTED, MAXIMUM, MAX_POOL_2D, MEAN, PAD, RESHAPE, SOFTMAX, SPLIT_V, SQRT, SQUARE, STRIDED_SLICE, SUB, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BroadcastTo.
```

[frozen_graph.pb](https://drive.google.com/open?id=1z-TPW-n0OziQ6-5ix1sCMzBMT5qmlbv-)
[full_log.txt](https://github.com/tensorflow/tensorflow/files/3495827/full_log.txt)


**Any other info / logs**
I am trying to convert a person re-identification model to tflite to run on Android. However it seems that the operator BroadcastTo is unsupported. I'm not sure where this operator is even used, because searching on TensorBoard returns nothing. I am aware that it is possible to create [custom operators](https://www.tensorflow.org/lite/guide/ops_custom), however I instantly got lost at the C++ code, I don't even know where to place the new operator if I manage to make it. And even then I'd have to compile a new AAR and use JNI in order to use it on Android.

The command I used is:
toco --graph_def_file=output/frozen_graph.pb --output_file=output/model.tflite --input_shapes=2,1,160,60,3 --input_arrays=images --output_arrays=Softmax --output_format=TFLITE --inference_type=FLOAT --input_data_type=FLOAT
"
31576,AttributeError: module 'tensorflow' has no attribute '__version__',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 1.10
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I installed tensorflow 1.10, and it worked fine. After I installed tensorflow-serving-api 1.10, error occured.   
```
Original exception was:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute '__version__'

```
And the result of dir(tf) is :
```
>>> dir(tf)
['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']
```
It seems that the ""__init__.py"" under tensorflow has been cleaned.   
Before installation of tf-serving-api: 
```
[root@eac5952e1443 /]# cat /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py 
# Copyright 2015 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");

...
_names_with_underscore = ['__version__', '__git_version__', '__compiler_version__', '__cxx11_abi_flag__', '__monolithic_build__']
__all__ = [_s for _s in dir() if not _s.startswith('_')]
__all__.extend([_s for _s in _names_with_underscore])
__all__.remove('print_function')


from tensorflow.python.util.lazy_loader import LazyLoader  # pylint: disable=g-import-not-at-top
contrib = LazyLoader('contrib', globals(), 'tensorflow.contrib')
del LazyLoader

from tensorflow.python.platform import flags  # pylint: disable=g-import-not-at-top
app.flags = flags  # pylint: disable=undefined-variable

del absolute_import
del division
del print_function

# These symbols appear because we import the python package which
# in turn imports from tensorflow.core and tensorflow.python. They
# must come from this module. So python adds these symbols for the
# resolution to succeed.
# pylint: disable=undefined-variable
del python
del core
# pylint: enable=undefined-variable
```
After installation of tf-serving-api:
```
[root@eac5952e1443 /]# cat /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py 
[root@eac5952e1443 /]# 
```

**Describe the expected behavior**
```
>>> print(tf.__version__)
1.10.0
```
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
pip install tensorflow==1.10
pip install tensorflow-serving-api==1.10
python
>> import tensorflow as tf
>> print(tf.__version__)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Of course I know that it would work fine if I reinstall tensorflow after tf-serving-api installation. But I need to install all of these into a docker image from Dockerfile. Reinstall seems not to be a good choice. Anyone has any suggestions? Thanks."
31573,use estimator got InvalidArgumentError Cannot assign a device for operation and allow_soft_placement: true doesn't work,"I create estimator and preditor based on tensor2tensor following code

```
hp = create_hparams()
decode_hp = create_decode_hparams()
run_conf = t2t_trainer.create_run_config(hp)
estimator = trainer_lib.create_estimator(
    FLAGS.model,
    hp,
    run_conf,
    decode_hparams=decode_hp,
    use_tpu=FLAGS.use_tpu)
predictor=tf.contrib.predictor.from_estimator(estimator, input_fn)
```

then got

> InvalidArgumentError: Cannot assign a device for operation transformer/body/parallel_0/body/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/attention: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
> Colocation Debug Info:
> Colocation group had the following types and supported devices: 
> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]
> ImageSummary: CPU 
> 
> Colocation members, user-requested devices, and framework assigned devices, if any:
>   transformer/body/parallel_0/body/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/attention (ImageSummary) /device:GPU:0
> 
> Op: ImageSummary
> Node attrs: max_images=1, T=DT_FLOAT, bad_color=Tensor<type: uint8 shape: [4] values: 255 0 0...>
> Registered kernels:
>   device='CPU'
> 	 [[{{node transformer/body/parallel_0/body/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/attention}}]]
> 
> During handling of the above exception, another exception occurred:

when I print session_config
`print(run_conf.session_config)`

I got 


> gpu_options {
>   per_process_gpu_memory_fraction: 0.95
> }
> **allow_soft_placement: true**
> graph_options {
>   optimizer_options {
>     global_jit_level: OFF
>   }
> }
> isolate_session_state: true


still can't solve this problem"
31572,batch inference is as slow as single image inference in tensorflow c++,"OS:Ubuntu 16.04
version:Tensorflow c++ 2.0-beta1 (compiled with all optimization flag:AVX AVX2 SSE4.1 SSE4.2 FMA XLA)
IDE:eclipse
With CUDA:No (just CPU in my prediction)
I have test the time of single image inference with tensorflow c++ api is 0.02 seconds which is so long that i just can not believe with my own eyes because i have compiled tensorflow c++ shared library with all the optimizations such as AVX/AVX2/FMA/SSE4.1/SSE4.2/FMA. However,i have to find the solution to decrease the cost time in prediction.Someone tells me the time can hugely decrease if i use batch inference instead of single image inference.Unfortunately,the time is 0.7 seconds in batch inference when the batch size is 32.In another word,0.7/32=0.02,it is as slow as single image inference.
This issue is related to tensorflow c++ performance so i ask it here and any help will be much appreciated.I have also put detailed information here [stackoverflow](https://stackoverflow.com/questions/57460782/batch-inference-is-as-slow-as-single-image-inference-in-tensorflow-c) ."
31570, tensorflow estimator doesn't need to depend on tensorflow,"Tensorflow estimator (https://github.com/tensorflow/estimator) installs only a set of python files, and doesn't really require Tensorflow to build. However, its build looks for Tensorflow for some reason.

Please remove the build-time Tensorflow dependency in Tensorflow estimator.

Estimator should actually be either a part of Tensorflow itself, or Tensorflow should depend on it because the Tensorflow's python code imports it.

(Created a bug here because the estimator project doesn't allow issues for some reason.)"
31568,Tensorflow 1.14.0 with C++17 Custom Op Has Binary Incompatibility,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, I have a custom op written in CUDA.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14.0
- Python version: 3.5.6
- Bazel version (if compiling from source): 0.25.1
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version: 10.0
- GPU model and memory: Nvidia GTX 1080

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When I ran ```tf.load_library(""my_ops.so"")```
It will complain about:
```
NotFoundError: my_ops.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeESt17basic_string_viewIcSt11char_traitsIcEE
```

**Describe the expected behavior**
Successfully `load_library`

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Tensorflow 1.14 was built with `bazel build --config=opt --config=cuda  --copt=-march=native --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package`
My custom op is built with Bazel.
"
31567,"TensorFlow for C doesn't compile - cannot open source file ""tensorflow/c/tf_attrtype.h""","My team is requesting that we use the C API for tensorflow.

I followed the instructions outlined under ""Install TensorFlow for C"" (https://www.tensorflow.org/install/lang_c). However, when I attempt to compile it fails with the message:

> cannot open source file ""tensorflow/c/tf_attrtype.h""

I am using Windows 10 with Visual Studio 2017. The steps used for installation/compilation are as follows:

1.  Download Windows CPU zip file
2. Extract to desktop folder tf-demo
   `C:\Users\....\Desktop\tf-demo`
3. Start Visual Studio 2017
4. Create a Console application project
5. Incorporate Example program in Visual Studio's 'main' file/routine.
6. Update Project's Additional Include Directories
   6.1 Right click on project
   6.2 Select Properties
   6.3 Expand Configuration
   6.4 Expand C/C++
   6.5 Select General
   6.6 Select Additional Include Directories
   6.7 Enter the path to the include files
       `C:\Users\....\Desktop\tf-demo\include`
   6.8 [Ok]
   6.9 [Ok]
7. Disable precompiled Headers
   7.1 Right click on project
   7.2 Select Properties
   7.3 Expand Cnfiguration
   7.4 Expand C/C++
   7.5 Select Precompiled Headers
   7.6 Change ""Precompiled Headers"" to ""Not using....""
   7.7 [OK]

After this I compile and it fails with the messages

`
Severity	Code	Description	Project	File	Line	Suppression State
Error (active)	E1696	cannot open source file ""tensorflow/c/tf_attrtype.h""	DemoProg

c:\Users\....\Desktop\tf-demo\include\tensorflow\c\c_api.h	22	
Error	C1083	Cannot open include file: 'tensorflow/c/tf_attrtype.h': No such file or directory	DemoProg	c:\users\....\desktop\tf-demo\include\tensorflow\c\c_api.h	22	
`

I reviewed the contents of the tf-demo directory and there is no tf_attrtype.h anywhere in the install package.
`
C:\Users\....\Desktop\tf-demo>dir /b /s

C:\Users\....\Desktop\tf-demo\include

C:\Users\....\Desktop\tf-demo\lib

C:\Users\....\Desktop\tf-demo\include\tensorflow

C:\Users\....\Desktop\tf-demo\include\tensorflow\c

C:\Users\....\Desktop\tf-demo\include\tensorflow\c\c_api.h

C:\Users\....\Desktop\tf-demo\include\tensorflow\c\eager

C:\Users\....\Desktop\tf-demo\include\tensorflow\c\LICENSE

C:\Users\....\Desktop\tf-demo\include\tensorflow\c\eager\c_api.h

C:\Users\....\Desktop\tf-demo\lib\tensorflow.dll

C:\Users\....\Desktop\tf-demo\lib\tensorflow.lib
`

"
31562,decode_png returns shape of 3 dimensions of question marks,"I am trying to read an image and convert it into tensor using the following code

```
img_path = <image directory>
img_raw = tf.io.read_file(img_path)
img_tensor = tf.image.decode_png(img_raw)
```


and when I try to print its shape 
`print(img_tensor.shape)`

it gives me 3 question marks instead of values:
`(?, ?, ?)
`

why it doesn't work and how to fix this??"
31561,TensorFlow Lite conversion,"**System information**
- TensorFlow running in Google Colab

Text Output from TFLite convert

```
TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, EXPAND_DIMS, FILL, FULLY_CONNECTED, GATHER, LESS, MAXIMUM, MINIMUM, MUL, NOT_EQUAL, PACK, RANGE, RESHAPE, SELECT, SHAPE, SOFTMAX, SPLIT, STRIDED_SLICE, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: LoopCond, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.
```"
31550,"Error: Check failed: dims == sizes.size() (5 vs. 4), when using CPU and MKL instead of Eigen or GPU.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6 High Sierra
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Running my code on cpu with tensorflow 1.14 using mkl from anaconda throws the following error:
2019-08-12 17:42:33.158451: F ./tensorflow/core/util/mkl_util.h:636] Check failed: dims == sizes.size() (5 vs. 4)
Abort trap: 6

The error trace gives me no hint on how to localize the problem (see below). The issue does not occur when installing a tensorflow build using eigen.

**Describe the expected behavior**
The code should work using both MKL or Eigen

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

I was not able to localize the exact issue.
It occurs when running ndnet.py from https://github.com/sziem/deconv_unet_2d3d

**Other info / logs**
Full output:

$ python ndnet.py
WARNING: Logging before flag parsing goes to stderr.
W0812 18:24:24.249456 140736187437952 deprecation.py:323] From /Users/Soenke/anaconda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
testing training
2019-08-12 18:24:26.506695
2019-08-12 18:24:26.621728: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-08-12 18:24:26.691965: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
W0812 18:24:26.750053 140736187437952 deprecation.py:323] From /Users/Soenke/anaconda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0812 18:24:26.794230 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/dataset_handlers/tfdata_dataset_handlers.py:332: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
Cropping to nearest allowed input image size.
Cropping to nearest allowed input image size.
W0812 18:24:27.274060 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/dataset_handlers/tfdata_dataset_handlers.py:139: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
input_shape: (?, 400, 100, 100, 1)
building Unet_v3 for training
net input shape (?, 398, 98, 98, 1)
W0812 18:24:27.390805 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/network_architectures/ops.py:173: conv3d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv3D` instead.
input_block1_2 (?, 396, 96, 96, 2)
W0812 18:24:27.957751 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/network_architectures/ops.py:116: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
W0812 18:24:28.167316 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/network_architectures/ops.py:232: average_pooling3d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.AveragePooling3D instead.
down_block2_4 (?, 196, 46, 46, 4)
bottom_block4_4 (?, 192, 42, 42, 4)
up_block4_2 (?, 380, 80, 80, 2)
output_block2_1 (?, 378, 78, 78, 1)
net output shape (?, 378, 78, 78, 1)
output_shape: (?, 378, 78, 78, 1)
loss is  l2loss
determining number of trainable vars (except batch_norm) for regularization...
done:  2183
saving new ckpt and logs in models/unetv3_small_valid_fp0_pp0_bn00_chlast/poisson_n1000_wl520/seed1_bs1_do0.0_loss=l2loss0_weightreg=0.001l2_loss_datareg=1e-08None_example/run8
Saving 2 logs per epoch by default.
Saving checkpoint every 2 epochs by default.
starting training with start_step 0
epoch 1 / 2
---->saving 0
---->summarizing 0
2019-08-12 18:24:44.145176: F ./tensorflow/core/util/mkl_util.h:636] Check failed: dims == sizes.size() (5 vs. 4)
2019-08-12 18:24:44.145177: F ./tensorflow/core/util/mkl_util.h:636] Check failed: dims == sizes.size() (5 vs. 4)
Abort trap: 6"
31549,tf.Estimator starts with GPU and switches to CPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.4 LTS (Xenial Xerus)
- TensorFlow installed from (source or binary): from pip
- TensorFlow version (use command below): 1.14, but earlier also for 1.11
- Python version: 3.6
- CUDA/cuDNN version: 10.0.130 / 7.6.2
- GPU model and memory: 1080Ti 11GB

Few days ago I've asked a SO question: https://stackoverflow.com/questions/57097880/tensorflow-estimator-gradually-decreasing-gpu-utilization - sadly without a response, so I'm trying here, not sure if this is a bug.

Copied from the question:
I'm training tf.Estimator model (5025056 trainable parameters):

    # simplified code
    model = create_custom_model() #preparing my model here 
    tf.reset_default_graph()
    estimator = tf.estimator.Estimator(model_fn=model.get_model_fn())
    evaluation_hook = tf.contrib.estimator.InMemoryEvaluatorHook(
        estimator=estimator,
        input_fn=lambda: model.eval_input_fn()
    )
    estimator.train(
        input_fn=lambda: model.train_input_fn(),
        hooks=[evaluation_hook]
    )
And dataset is prepared here:

    def train_input_fn():
        dataset = tf.data.TFRecordDataset(""some_filename"")
        dataset = dataset.shuffle(3000)
        dataset = dataset.repeat()
        dataset = dataset.batch(384)
        dataset = dataset.prefetch(1)
        return dataset

My dataset consists of images, 9000 samples, stored in TFRecord (163M). 
GPU is `GeForce GTX 1080 Ti` and CPU is `i5-6600 CPU @ 3.30GHz`.
During the training, at first, everything looks fine - `htop` shows that each core is working approximately the same way (mostly jumping between `0` - `50%` utilization), and gpu stats (shown using `vidia-smi --query-gpu=timestamp,pstate,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 1`) indicate that card is working on `100%` of its power - temperatures around `85C`, `100%` utilization of gpu and memory, also most of the memory is used(10GB).
Up to step `4000` (of `10000`) tensorflow prints `global_step/sec` with values ranging from `1.0` to `1.1`. `15GB` of available `16GB` of RAM is used and no swap.


After that step, that time `global_step/sec` is lower and lower. For step `5000` it is `0.617275`.
Aaround step 2000 it is only `0.0938672` and decreasing (down to `0.0368357` at the end of the training). During this process `nvidia-smi` shows that `utilization.gpu` and `utilization.memory` is `0%` more and more frequently (despite the fact that `memory.used` shows same amount whole time, that is `10GB`), and CPU is working at a `100%` of a single core. RAM is at the same level as when training started, and no swapping occurs. Periodically, mayby once for `20` steps, GPU utilization is slightly higher, but quickly returns to `0`. 


It looks like after some epochs tensorflow trains on CPU instead of GPU? What can be possible cause of that? "
31548,Add adjust_batch or (re_batch) support for tf.data,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): N/A
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

At the moment `tf.data` has both `unbatch()` and `batch(n)` support, where you can either change the output of `tf.data` from `any`-batched ones to `non`-batched ones, or from `non`-batched ones to `n`-batched ones.

What is not available is the transformation from `m`-batched ones to `n`-batched ones directly.

Indirectly it is possible to achieve the above mentioned `m`-batched => `n-batched` through:
```
dataset = dataset.unbatch().batch(n)
```

However, in case each record is small in size (e.g., one record only have one float32) then `.unbatch().batch(n)` could be costly.

It would be great to have a way to shortcut with an `adjust_batch` or `re_batch` that achieve the same thing:
```
dataset = dataset.re_batch(n) # from m-batched to n-batched
```

Note there is already a `RebatchDataset` (which archives different goal) so not sure if `re_batch(n)` could be a good naming.

**Will this change the current api? How?**

No.
**Who will benefit with this feature?**

See description above.

**Any Other info.**
"
31547,tensorflow.keras.layers.BatchNormalization deos not accept float64 input data,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9
- GPU model and memory: 1080 GTX

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
**TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float32**
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow.keras.layers as tks
input_ = tks.Input(shape=[None, None, 3], dtype='float64')
conv = tks.Conv2D(filters=3, kernel_size=3)(input_)
batch = tks.BatchNormalization()(conv)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31546,model.fit_generator() multithreading is broken in tf.keras,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Platform-independent
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): From pip
- TensorFlow version (use command below): 1.14.0 and 2.0
- Python version: 3.6.7

### **Summary**
fit_generator has an option called *workers*, setting this to >1 will use multithreading to queue up batches from a generator.

It raises an exception if the generator is not thread-safe. this is expected. However, **it does not accept thread-safe generators.**

### **Describe the current behavior**
Calling model.fit_generator on a keras model in tf 2.0 or compat.v2 **using a generator object, subclassed from collections.Generator** raises an exception that the given generator object does not have a shape attribute.

This is rooted in the calling of model_iteration which then unsuccessfully attempts to find out wether the generator is in fact a generator by using inspect.isgenerator(), which only recognizes native python generators (constructed by a function containing a yield statement)

however, **native python generators cannot be thread-safe**, thus fit_generator with workers>1 and use_multiprocessing=False is broken in tf.keras

### **Describe the expected behavior**
In keras 2.2.4, fit_generator simply calls the next(gen) function on the generator provided to fit_generator(). this is working as expected.

**Code to reproduce the issue**

```Python
import numpy as np

#switch here to switch between working keras and non-working tf.keras code
do_broken=True 

if do_broken:
    import tensorflow.compat.v2 as tf
    from tensorflow.compat.v2 import keras
    from tensorflow.compat.v2.keras.layers import Dense
    from tensorflow.compat.v2.keras.models import Sequential
    tf.enable_v2_behavior()
else:
    import keras
    from keras.layers import Dense
    from keras.models import Sequential

import threading
from collections import Generator

class mwe_gen(Generator):
    
    def __init__(self,train_data,train_labels,batch_size):
        self.train_data=train_data
        self.train_labels=train_labels
        self.batch_size=batch_size
        self.batch=0
        self.lock=threading.Lock()

    def __iter__(self):
        return self

    def __next__(self):
        return self.next()
    
    def next(self):
        with self.lock:
            batch=self.batch
            batch_size=self.batch_size
            self.batch=self.batch+self.batch_size
            if self.batch>len(self.train_data):
                self.batch=0
        batch_data=self.train_data[batch:batch+batch_size]
        batch_labels=self.train_labels[batch:batch+batch_size]
        return (batch_data,batch_labels)
    
    def send(self,arg):
        return self.next()
    
    def close(self):
        """"""Raise GeneratorExit inside generator.
        """"""
        try:
            self.throw(GeneratorExit)
        except (GeneratorExit, StopIteration):
            pass
        else:
            raise RuntimeError(""generator ignored GeneratorExit"")
    
    def throw(self, type=None, value=None, traceback=None):
        raise StopIteration
        
train_data=np.random.normal(size=(10,1))
train_labels=np.random.normal(size=(10,1))

gen=mwe_gen(train_data,train_labels,5)

model=Sequential()
model.add(Dense(1,input_shape=(1,)))

model.compile(loss=""mse"",optimizer=""sgd"")

model.fit_generator(gen,steps_per_epoch=2)

```

**Other info / logs**
```Python

Traceback (most recent call last):
  File ""multithreaded_gen.py"", line 72, in <module>
    model.fit_generator(gen,steps_per_epoch=2)
  File ""C:\Users\PYRESTONE\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1433, in fit_generator
    steps_name='steps_per_epoch')
  File ""C:\Users\PYRESTONE\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training_generator.py"", line 144, in model_iteration
    shuffle=shuffle)
  File ""C:\Users\PYRESTONE\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training_generator.py"", line 480, in convert_to_generator_like
    num_samples = int(nest.flatten(data)[0].shape[0])
AttributeError: 'mwe_gen' object has no attribute 'shape'

```"
31545,tf.shape() for RaggedTensor is raising an exception,"Let suppose the following context:

```python
ragged_tensor = tf.ragged.constant(
    [['All', 'the', 'world', 'is', 'a', 'stage'],
    ['And', 'all', 'the', 'men', 'and', 'women', 'merely', 'players'],
    ['They', 'have', 'their', 'exits', 'and', 'their', 'entrances']]
)

print(isinstance(ragged_tensor, tf.RaggedTensor))   # True

tf.shape(ragged_tensor)[0]   # Should return ""3""

>>> TypeError: Failed to convert object of type 
<class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. 
Contents: tf.RaggedTensor(
    values=Tensor(""RaggedConstant/values:0"", shape=(21,), dtype=string), 
    row_splits=Tensor(""RaggedConstant/Const:0"", shape=(4,), dtype=int64)
). 
Consider casting elements to a supported type.

# To obtain the information, one shall call the following:
ragged_tensor.bounding_shape()[0]
```

We should merge the two functionalities into TF.Shape and raise a warning/exception in case someone tries to evaluate a ragged dimension."
31544,Severe lag due to GPU having no memory to allocate to,"Running tensorflow on my laptop through anaconda and the following popped up:

2019-08-12 09:32:41.378946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3048 MB memory) -> physical GPU (device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2019-08-12 09:33:14.804467: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.

I am experiencing severe lag that I was originally not.

Any thoughts?

I used the following instructions to install my tensorflow: https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10"
31543,Interrelations of collections and scope counts are not clear from documentation,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/GraphKeys

## Description of issue (what needs changing):

The GraphKeys doc is lacking explanation of ('__variable_store',) and ('__varscope',) keys relation to the others collections e to the variable creation in in a variable scope context.
In my modelling, since iam building a dnn from scratch, its cond sine-qua-non to estimate memory usage like given in this  for VGGNet 
http://cs231n.github.io/convolutional-networks/#case

However to me its not clear which tensors summup the computation memory allocations (even if it initializes in the runtime, you should be able to pre-calculate the estimative from the graph builted, before running).

So iam able to realize the relations of scopes counting, variable creation and using, will be hard to do such kind of memory estimation function.

Today iam using ._collections['variables'], i think it subsums the variables used in any session of training.

### Clear description

Collections are created in the modelling process with the intent of variable management for some functional reason. The developer must have a clear image of which is the intent of each collection and the inter-relation of them (this is somewhat done in GraphKeys, but the mentioned keys are lacking).

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style

### More Info

I have seem the RFC and know that the 2.0beta Variable became abs class and the management and implementation is more flexible, but now i dont have time to migrate the code, so i think this is stuff is good to keep-up updated (if it is possible and desirable by TFlow team), more people may be in the same condition ."
31542,Ploting Gradients to Tensorboard and Console,"**System information**
- Windows 10 Pro Version 1903
- TensorFlow installed from pip in Anaconda:
- TensorFlow version 2.0.0-beta1 (gpu)
- Python version: 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]
- CUDA/cuDNN version: Cuda compilation tools, release 10.0, V10.0.130
- GPU model and memory:  GeForce GTX 980 Ti major: 5 minor: 2 memoryClockRate(GHz): 1.2785

**Describe the current behavior**
Program ends with an unclear error, while trying to retrieve the bias gradients of the two dense layers in the model.

* Writing to tensorboard (*console parameter = False*)
```
Train on 60000 samples
Epoch 1/5
2019-08-12 15:24:48.713962: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.
2019-08-12 15:24:48.718362: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library cupti64_100.dll
   32/60000 [..............................] - ETA: 12:48 - loss: 2.2374 - accuracy: 0.18752019-08-12 15:24:48.840661: I tensorflow/core/platform/default/device_tracer.cc:641] Collecting 81 kernel records, 14 memcpy rec
ords.
59744/60000 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.9123Traceback (most recent call last):
  File ""C:/Users/Harald Schweiger/PycharmProjects/Gradients/gradient_test.py"", line 42, in <module>
    model.fit(x_train, y_train, epochs=5, callbacks=[gradient_cb, tensorboard_cb])
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 643, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 664, in fit
    steps_name='steps_per_epoch')
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 439, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 295, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""C:/Users/Harald Schweiger/PycharmProjects/Gradients/gradient_test.py"", line 32, in on_epoch_end
    tf.summary.histogram(t.name, data=t)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorboard\plugins\histogram\summary_v2.py"", line 77, in histogram
    tensor = _buckets(data, bucket_count=buckets)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorboard\plugins\histogram\summary_v2.py"", line 139, in _buckets
    return tf.cond(is_empty, when_empty, when_nonempty)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 1382, in cond_for_tf_v2
    return cond(pred, true_fn=true_fn, false_fn=false_fn, strict=True, name=name)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 1177, in cond
    result = false_fn()
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorboard\plugins\histogram\summary_v2.py"", line 137, in when_nonempty
    return tf.cond(is_singular, when_singular, when_nonsingular)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 1382, in cond_for_tf_v2
    return cond(pred, true_fn=true_fn, false_fn=false_fn, strict=True, name=name)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 1174, in cond
    if pred:
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 698, in __bool__
    raise TypeError(""Using a `tf.Tensor` as a Python `bool` is not allowed. ""
TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the
 value of a tensor.
```

* Priniting bias gradients to console(*console parameter = True*)
```
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Train on 60000 samples
Epoch 1/5
2019-08-12 15:26:01.265400: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.
2019-08-12 15:26:01.268877: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library cupti64_100.dll
   32/60000 [..............................] - ETA: 12:53 - loss: 2.4094 - accuracy: 0.09382019-08-12 15:26:01.391432: I tensorflow/core/platform/default/device_tracer.cc:641] Collecting 81 kernel records, 14 memcpy rec
ords.
59776/60000 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.9121Tensor: Adam/gradients_1/dense128/BiasAdd_grad/BiasAddGrad:0
Traceback (most recent call last):
  File ""C:/Users/Harald Schweiger/PycharmProjects/Gradients/gradient_test.py"", line 42, in <module>
    model.fit(x_train, y_train, epochs=5, callbacks=[gradient_cb, tensorboard_cb])
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 643, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 664, in fit
    steps_name='steps_per_epoch')
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 439, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 295, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""C:/Users/Harald Schweiger/PycharmProjects/Gradients/gradient_test.py"", line 30, in on_epoch_end
    print('{}\n'.format(K.get_value(t)[:10]))
  File ""C:\Users\Harald Schweiger\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"", line 2981, in get_value
    return x.numpy()
AttributeError: 'Tensor' object has no attribute 'numpy'
```

**Describe the expected behavior**

* Writing to tensorboard (*console parameter = False*)
Tensorboard event file which contains the distribution and histograms of gradients 
derived from the total loss that has been accumulated over the last epoch.

* Priniting to console(*console parameter = True*)
The program should print the first 10 gradient bias values of each of the two dense layer
to the console.

If the exceptions produced here are the expected behavior due to errors in the developers code
a more meaningful error message would be appriciated. 
In that case a correction of the code would be useful for me and other people as well who had to update their code as the *write_grads* parameter has been removed from the tensorboard callback in version 2.0.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.python.keras import backend as K

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu', name='dense128'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax', name='dense10')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])


class GradientCallback(tf.keras.callbacks.Callback):
    console = True

    def on_epoch_end(self, epoch, logs=None):
        weights = [w for w in self.model.trainable_weights if 'dense' in w.name and 'bias' in w.name]
        loss = self.model.total_loss
        optimizer = self.model.optimizer
        gradients = optimizer.get_gradients(loss, weights)
        for t in gradients:
            if self.console:
                print('Tensor: {}'.format(t.name))
                print('{}\n'.format(K.get_value(t)[:10]))
            else:
                tf.summary.histogram(t.name, data=t)


file_writer = tf.summary.create_file_writer(""./metrics"")
file_writer.set_as_default()

# write_grads has been removed
tensorboard_cb = tf.keras.callbacks.TensorBoard(histogram_freq=1, write_grads=True)
gradient_cb = GradientCallback()

model.fit(x_train, y_train, epochs=5, callbacks=[gradient_cb, tensorboard_cb])
```
"
31541,Bug in IOU calculation of NMS for GPU,"**System information**
Ubuntu 18.04, Tensorflow r2.0 from pip

**Describe the current behavior**
https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/core/kernels/non_max_suppression_op.cu.cc#L96

**Describe the expected behavior**
```
const float w = fdimf(xx2, xx1);
const float h = fdimf(yy2, yy1);
const float intersection = w * h;
```

**Code to reproduce the issue**
I believe that the calculation of intersection over union in line 96 of the GPU implementation of non-max suppression is incorrect. 

```
boxes = np.array([[0.2, 0.3, 0.4, 0.5], [0.61, 0.71, 0.81, 0.91], [0.6, 0.7, 0.8, 0.9]], dtype=np.float32)
score = np.array([0.2, 0.7, 0.6], dtype=np.float32)
iou_threshold = 0.5
nms_reference_cpu = tf.image.non_max_suppression(
            boxes=boxes,
            scores=score,
            max_output_size=3,
            iou_threshold=0.5,
        )
= [1, 0]
```

it will not work for GPU, but it will work for CPU. If you remove the +1 (not sure why you would want to do that in the first place), they match.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31540,Tensorflow 2 creating custom dataset ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0.0-beta
- Are you willing to contribute it (Yes/No): yes


**Describe the feature and the current behavior/state.**
IMO there is currently not a really clean way to build an own dataset by using/subclassing the tf.data.Dataset-class. Specially for datasets which have more complex structure as [ICDAR](https://rrc.cvc.uab.es/?ch=4), here the labels are made of text-files with bounding-boxes and transcripts. In these cases even tensorflow_datasets can not be used in a straight way as described [here] (https://www.tensorflow.org/datasets/add_dataset).

So i would like to know what is actually the correct procedure to use the Dataset-Api for creating a custom dataset? 

**Will this change the current api? How?**

**Who will benefit with this feature?**
Most of object detection tasks

**Any Other info.**
"
31539,tf.image.resize_bilinear() outputs garbage pixel values,"I am trying to resize images using bilinear interpolation but unable to get the correct image while saving the image. I am getting the the outputs similar to [#25591](https://github.com/tensorflow/tensorflow/issues/25591). Applying the solution mentioned in that thread is not helping.

Config,
CUDA: 10.0
cudNN 7.4.2
Tensorflow-gpu 1.14
numpy 1.16

Below is the reproducible code. 

```
img_path = 'C:/x.bmp'
output_dir =  'C:/Users/Desktop/output/'
image_res = (536,640)
imgname_string = img_path.split('/')[-1]
print(imgname_string)

def preprocess(img_path):
        img_read = tf.read_file(img_path)
        img_decode = tf.image.decode_bmp(img_read,channels=0)
        img_f32 = tf.image.convert_image_dtype(img_decode, dtype=tf.float32)
        img_4d = tf.expand_dims(img_f32,axis=0)
        img_new = tf.image.resize_bilinear(img_4d, size=image_res)
        img_final = tf.squeeze(img_new,[0])
        
        return img_final

init_op = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init_op)
    process_img = sess.run(preprocess(img_path))
    print(process_img.dtype)
    p = process_img.astype(dtype=np.uint8)
    img = Image.fromarray(np.squeeze(p,axis=-1),'L')
    relative_path = os.path.join(output_dir,imgname_string)
    print(relative_path)
    img.save(relative_path)
```"
31538,Cocoapods cannot install TensorFlowLiteC,"I am getting [!] Error installing TensorFlowLiteC when trying to install pods. I am using 
pod 'TensorFlowLiteSwift' in my podfile. 

Error:
[!] Error installing TensorFlowLiteC
[!] /usr/bin/curl -f -L -o /var/folders/8h/w7cb5w9x4m9dg8l9rr3h1qwas0000gn/T/d20208412-2822-n13n01/file.tgz https://dl.google.com/dl/cpdc/9d0ec5e53f4ff34a/TensorFlowLiteC-0.2.0.tar.gz --create-dirs --netrc-optional --retry 2"
31537,Expose ConvLSTM2DCell,"**System information**
- TensorFlow version (you are using): 2.0.0-beta1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Expose ConvLSTM2DCell via Keras API (as is the case with LSTMCell and GRUCell).

**Will this change the current api? How?**
Yes, ConvLSTM2DCell will be added (to tf.keras.layers).

**Who will benefit with this feature?**
Anyone with an interest in machine learning with video sequences (e.g. https://github.com/tensorflow/models/tree/master/research/video_prediction) who needs a way to integrate convolutional LSTM layers into their model that is more flexible than what the current API provides. 
"
31536,Call to build function of GRUCell object segfaults if Keras precision is set to float16,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: Nvidia RTX 2080 Ti (11 GB)

**Describe the current behavior**
A call to the build() function of an instantiated GRUCell object with recurrent_initializer set to ""orthogonal"" (this is the default) terminates with a segmentation fault if Keras precision is set to float16.

**Describe the expected behavior**
Call to build() should succeed.

**Code to reproduce the issue**
```
import tensorflow as tf
tf.keras.backend.set_floatx(""float16"")

dim = 100
cell = tf.keras.layers.GRUCell(dim, recurrent_initializer=""orthogonal"")
cell.build([dim])
```
Test: set a different recurrent initializer (e.g. `recurrent_initializer=""glorot_uniform""`) or remove `tf.keras.backend.set_floatx(""float16"")`. The function should now execute successfully.

**Other info / logs**
n/a"
31535,Colab TF 2.0 runs out of memory if eager execution is enabled,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (GPU Runtime)
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6 (Google Colab 3.6 Environment)
- CUDA/cuDNN version:  Google Colab (GPU Runtime)
- GPU model and memory:  Google Colab (GPU Runtime)

**Describe the current behavior**
If I use a large U-net with an Inception-ResNetv2 backbone and eager execution is enabled,(default with tf 2.0), the machine runs out of memory. If eager execution is disabled (`tf.compat.v1.disable_eager_execution()`), it works fine.

Maybe this is a bug in Google colab, as the code works fine with eager execution enabled on my machine with 8 GB of RAM and a Geforce 1060.

**Describe the expected behavior**
I would expect it to work in both modes similar.

**Code to reproduce the issue**
https://colab.research.google.com/drive/1-xgkBBUqZw6rS2WhxUMBZuY6j1uxG_e-"
31533,In reference with https://github.com/tensorflow/tensorflow/issues/31406,"
@jdduke 

In reference with the issue
https://github.com/tensorflow/tensorflow/issues/31406

Basically what needs to be identified and what changes needs to be done in accordance with the model.

It can be understood that TensorFlowObjectDetectionAPIModel.java will require the changes. But what are those changes. 

As it is advised to check io shape with graph. Putting its result over here will help in any way possible?

"
31532,.,".
"
31531,Error when using batch renormalisation option under a strategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS: Windows 10
- TensorFlow installed from (source or binary): tensorflow-gpu binary on pip
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When I try to use `renorm=True` with `tf.keras.layers.BatchNormalization` under `tf.distribute.MirroredStrategy` I get the following error: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.


**Describe the expected behavior**
I can use batch renormalisation under mirrored strategy.

**Code to reproduce the issue**
```
import tensorflow as tf

strat = tf.distribute.MirroredStrategy()
with strat.scope():
  inp = tf.keras.Input((28, 28))
  tf.keras.layers.BatchNormalization(renorm=True)(inp)
```

**Other info / logs**

The following is the stack trace:
```
<ipython-input-3-d17cda041220> in <module>()
      2 with strat.scope():
      3   inp = tf.keras.Input((28, 28))
----> 4   tf.keras.layers.BatchNormalization(renorm=True)(inp)

16 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    632                     outputs = base_layer_utils.mark_as_return(outputs, acd)
    633                 else:
--> 634                   outputs = call_fn(inputs, *args, **kwargs)
    635 
    636             except TypeError as e:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in call(self, inputs, training)
    734       if self.renorm:
    735         r, d, new_mean, new_variance = self._renorm_correction_and_moments(
--> 736             new_mean, new_variance, training, inputs_size)
    737         # When training, the normalized values (say, x) will be transformed as
    738         # x * gamma + beta without renorm, and (x * r + d) * gamma + beta

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in _renorm_correction_and_moments(self, mean, variance, training, inputs_size)
    598     new_mean = _update_renorm_variable(self.renorm_mean,
    599                                        self.renorm_mean_weight, mean,
--> 600                                        inputs_size)
    601     new_stddev = _update_renorm_variable(self.renorm_stddev,
    602                                          self.renorm_stddev_weight, stddev,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in _update_renorm_variable(var, weight, value, inputs_size)
    593       def _fake_update():
    594         return array_ops.identity(var)
--> 595       return tf_utils.smart_cond(training, _do_update, _fake_update)
    596 
    597     # TODO(yuefengz): colocate the operations

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)
     56         pred, true_fn=true_fn, false_fn=false_fn, name=name)
     57   return smart_module.smart_cond(
---> 58       pred, true_fn=true_fn, false_fn=false_fn, name=name)
     59 
     60 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     57   else:
     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,
---> 59                                  name=name)
     60 
     61 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--> 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in cond(pred, true_fn, false_fn, strict, name, fn1, fn2)
   1975     try:
   1976       context_t.Enter()
-> 1977       orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
   1978       if orig_res_t is None:
   1979         raise ValueError(""true_fn must have a return value."")

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in BuildCondBranch(self, fn)
   1812     """"""Add the subgraph defined by fn() to the graph.""""""
   1813     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
-> 1814     original_result = fn()
   1815     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
   1816     if len(post_summaries) > len(pre_summaries):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in _do_update()
    582           weight_value = array_ops.constant(1., dtype=weight.dtype)
    583         new_var = self._assign_moving_average(var, value, self.renorm_momentum,
--> 584                                               inputs_size)
    585         new_weight = self._assign_moving_average(weight, weight_value,
    586                                                  self.renorm_momentum,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in _assign_moving_average(self, variable, value, momentum, inputs_size)
    447   def _assign_moving_average(self, variable, value, momentum, inputs_size):
    448     with K.name_scope('AssignMovingAvg') as scope:
--> 449       with ops.colocate_with(variable):
    450         decay = ops.convert_to_tensor(1.0 - momentum, name='decay')
    451         if decay.dtype != variable.dtype.base_dtype:

/usr/lib/python3.6/contextlib.py in __enter__(self)
     79     def __enter__(self):
     80         try:
---> 81             return next(self.gen)
     82         except StopIteration:
     83             raise RuntimeError(""generator didn't yield"") from None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)
   4398   def _colocate_with_for_gradient(self, op, gradient_uid,
   4399                                   ignore_existing=False):
-> 4400     with self.colocate_with(op, ignore_existing):
   4401       if gradient_uid is not None and self._control_flow_context is not None:
   4402         self._control_flow_context.EnterGradientColocation(op, gradient_uid)

/usr/lib/python3.6/contextlib.py in __enter__(self)
     79     def __enter__(self):
     80         try:
---> 81             return next(self.gen)
     82         except StopIteration:
     83             raise RuntimeError(""generator didn't yield"") from None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in colocate_with(self, op, ignore_existing)
   4447       raise ValueError(""Trying to reset colocation (op is None) but ""
   4448                        ""ignore_existing is not True"")
-> 4449     op = _op_to_colocate_with(op)
   4450 
   4451     # By default, colocate_with resets the device function stack,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _op_to_colocate_with(v)
   6714   # happen soon, perhaps this hack to work around the circular
   6715   # import dependency is acceptable.
-> 6716   if hasattr(v, ""handle"") and hasattr(v.handle, ""op"") and isinstance(
   6717       v.handle.op, Operation):
   6718     return v.handle.op

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/values.py in handle(self)
    641       device = distribute_lib.get_update_device()
    642       if device is None:
--> 643         raise ValueError(""`handle` is not available outside the replica context""
    644                          "" or a `tf.distribute.Strategy.update()` call."")
    645     return self.get(device=device).handle
```"
31528,"How to identify the input and output array of a model for  "".tflite"" conversion process. ","I am trying to generate quantized .tflite model from .pb file. For the process I require the 'input_arrays' and 'output_arrays' of the model.
I've tried using the below methods to identify the input array and output array. But none of them worked.

Method 1:

```
import tensorflow as tf 
 frozen='/output/freeze/frozen_inference_graph.pb'
 gf = tf.GraphDef() 
 gf.ParseFromString(open(frozen,'rb').read()) 
 [n.name + '=>' +  n.op for n in gf.node if n.op in ('Softmax','Placeholder')]   
 [n.name + '=>' +  n.op for n in gf.node if n.op in ( 'Softmax','Mul')]

```
Method 2:
```
import tensorflow as tf
gf = tf.GraphDef()   
m_file = open('/output/freeze/frozen_inference_graph.pb','rb')
gf.ParseFromString(m_file.read())
for n in gf.node:
    print( n.name )

```
tflife conversion query:
import tensorflow as tf
graph_def_file = ""new/barun/frozen_inference_graph.pb""
input_arrays = ['image_tensor']
output_arrays = ['BoxPredictor_5/ClassPredictor/act_quant/FakeQuantWithMinMaxVars']

converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays,input_shapes={""image_tensor"":[1,300,300,3]})
converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
tflite_model = converter.convert()
open(""frozen_inference_graph_fd2819_2.tflite"", ""wb"").write(tflite_model)`

How to find out the input_array and output_array from a tensorflow model(.pb file) ?``"
31527,Performance Issue in C# Example,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): use code in TF# demo
- OS Platform and Distribution: **Windows 10**
- TensorFlow installed from (source or binary): **source(by pip)**
- TensorFlow version (use command below): **1.13**
- Python version: **3.7**
- CUDA/cuDNN version: run on **CPU**

**Describe the current behavior**
I implemented a Griffin-Lim algorithm(GL, a speech processing method used for reconstructing wave from mel-spectrogram) with TF-python, and dump it into a pb file.
Then I load graph of Griffin-Lim from the pb file, and using runner.Run() to call the computing graph and get the output of the last Node.

The result of output is correct and the voice quality is on par with TF-python generated.
But runner.Run() in TF# runs 5-10 times slower than session.Run() in TF-python.
For example, to generate a 3 seconds wave, runner.Run() in TF# need about 2.7 seconds, but session.Run() in TF-python only need 0.3-0.6 seconds.

here are the generate key codes in C#:

            TFTensor input = new TFTensor(melSpectrum);
            var runner = this.session.GetRunner();
            runner.AddInput(this.input, input);
            runner.Fetch(this.output);
            TFTensor[] results = runner.Run();

I went back and checked the related code I call runner.Run().

1. I have already kept a TFSession instant.
2. When I initialize the session & graph, I also warm up them by faking a melSpectrum and calling upper codes. If sess& graph weren't warmed up, runner.Run() will cost 6.8-7seconds to generate a 3 seconds wav.

**Describe the expected behavior**
I think runner.Run() in TF# should run as fast as session.Run() in TF-python

Any clues?
Thanks
"
31526,TFLite Metal GPU delegate: Add operator does not support broadcasting,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 12.3.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone Xr
- TensorFlow installed from (source or binary): binary, but TFLite compiled from source
- TensorFlow version (use command below): b'v1.13.2-5-g04256c89d8' 1.13.2
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): clang-1001.0.46.4
- CUDA/cuDNN version: n/a
- GPU model and memory: iPhone Xr

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Running the single-operation model given below using the Metal GPU delegate gives a different output than when executing on CPU or with the full Tensorflow interpreter.

**Describe the expected behavior**
The Metal GPU delegate gives the same output as the CPU interpreter.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
[model-broken.tflite.zip](https://github.com/tensorflow/tensorflow/files/3490983/model-broken.tflite.zip)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31525,Tensorflow Estimator InvalidArgumentError,"TF 1.13.1 binary on ubuntu 18.10

the same issue with that on stackoverflow: https://stackoverflow.com/questions/54089982/tensorflow-estimator-invalidargumenterror

The issue occurs when the categorical feature size is very large and reports bug of `Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint.` 

however, when reducing the size of categorical feature size, the issue disappears.

One very important detail is that I build the estimatorSpec myself, which is quite important because using the standard `DNNLinearCombinedClassifier` would not have the above error. At the same time, if I reduce the the size of categorical feature size with the `estimatorSpec` I build myself, the issue also disappears. And the above issue poster on the stackoverflow solves this problem by limiting the feature size to 17k features. Unfortunately with the feature size going larger than 17k, the same issue happens.

So is there any restrict of the categorical feature size on the `estimatorSpec ` ?"
31524, how to replace the wrong node definitions in the frozen graph when importing frozen graph with batchnorm,"there is a confusion when i try to do as the answers in the question #3628, and could anyone give me a proper solution for it?
he says:
''
An ugly way to get it working:
manually replace the wrong node definitions in the frozen graph
RefSwitch --> Switch + add '/read' to the input names
AssignSub --> Sub + remove use_locking attributes
''

but, I printed the nodes in my net,:
''
inputs
labels
keep_prob
MobilenetV2/input
MobilenetV2/Conv/weights/Initializer/truncated_normal/shape
MobilenetV2/Conv/weights/Initializer/truncated_normal/mean
MobilenetV2/Conv/weights/Initializer/truncated_normal/stddev
MobilenetV2/Conv/weights/Initializer/truncated_normal/TruncatedNormal
MobilenetV2/Conv/weights/Initializer/truncated_normal/mul
MobilenetV2/Conv/weights/Initializer/truncated_normal
MobilenetV2/Conv/weights
MobilenetV2/Conv/weights/Assign
MobilenetV2/Conv/weights/read
MobilenetV2/Conv/kernel/Regularizer/l2_regularizer/scale
MobilenetV2/Conv/kernel/Regularizer/l2_regularizer/L2Loss
MobilenetV2/Conv/kernel/Regularizer/l2_regularizer
MobilenetV2/Conv/dilation_rate
MobilenetV2/Conv/Conv2D
MobilenetV2/Conv/BatchNorm/gamma/Initializer/ones
MobilenetV2/Conv/BatchNorm/gamma
MobilenetV2/Conv/BatchNorm/gamma/Assign
MobilenetV2/Conv/BatchNorm/gamma/read
MobilenetV2/Conv/BatchNorm/beta/Initializer/zeros
MobilenetV2/Conv/BatchNorm/beta
MobilenetV2/Conv/BatchNorm/beta/Assign
MobilenetV2/Conv/BatchNorm/beta/read
MobilenetV2/Conv/BatchNorm/moving_mean/Initializer/zeros
MobilenetV2/Conv/BatchNorm/moving_mean
MobilenetV2/Conv/BatchNorm/moving_mean/Assign
MobilenetV2/Conv/BatchNorm/moving_mean/read
MobilenetV2/Conv/BatchNorm/moving_variance/Initializer/ones
MobilenetV2/Conv/BatchNorm/moving_variance
MobilenetV2/Conv/BatchNorm/moving_variance/Assign
MobilenetV2/Conv/BatchNorm/moving_variance/read
MobilenetV2/Conv/BatchNorm/Const
MobilenetV2/Conv/BatchNorm/Const_1
MobilenetV2/Conv/BatchNorm/FusedBatchNorm
MobilenetV2/Conv/BatchNorm/Const_2
MobilenetV2/Conv/BatchNorm/AssignMovingAvg/sub/x
MobilenetV2/Conv/BatchNorm/AssignMovingAvg/sub
MobilenetV2/Conv/BatchNorm/AssignMovingAvg/sub_1
MobilenetV2/Conv/BatchNorm/AssignMovingAvg/mul
MobilenetV2/Conv/BatchNorm/AssignMovingAvg
MobilenetV2/Conv/BatchNorm/AssignMovingAvg_1/sub/x
MobilenetV2/Conv/BatchNorm /AssignMovingAvg_1/sub
MobilenetV2/Conv/BatchNorm/AssignMovingAvg_1/sub_1
MobilenetV2/Conv/BatchNorm/AssignMovingAvg_1/mul
MobilenetV2/Conv/BatchNorm/AssignMovingAvg_1
MobilenetV2/Conv/Relu6
MobilenetV2/expanded_conv/input
''

but! the 'nodes.op' of the 'nodes' above donot exist ""RefSwitch"", please do you any suggestions?"
31523,error while building wheel,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version:  2.0.0-rc.0
- Python version: 3.7.4 x64
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source):  7.4.0
- CUDA/cuDNN version: 10.1/7.6.2
- GPU model and memory: GTX1080Ti GDDR5x 11GB X 6



**Describe the problem**
error while building wheels
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tensorflow_pkgs/
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""setup.py"", line 308, in <module>
    keywords='tensorflow tensor machine learning',
  File ""/home/wmind/anaconda3/lib/python3.7/site-packages/setuptools/__init__.py"", line 145, in setup
    return distutils.core.setup(**attrs)
  File ""/home/wmind/anaconda3/lib/python3.7/distutils/core.py"", line 148, in setup
    dist.run_commands()
  File ""/home/wmind/anaconda3/lib/python3.7/distutils/dist.py"", line 966, in run_commands
    self.run_command(cmd)
  File ""/home/wmind/anaconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/home/wmind/anaconda3/lib/python3.7/site-packages/wheel/bdist_wheel.py"", line 230, in run
    impl_tag, abi_tag, plat_tag = self.get_tag()
  File ""/home/wmind/anaconda3/lib/python3.7/site-packages/wheel/bdist_wheel.py"", line 179, in get_tag
    assert tag == supported_tags[0], ""%s != %s"" % (tag, supported_tags[0])
AssertionError: ('cp37', 'cp@pyvernodots@m', 'linux_x86_64') != ('cp37', 'cp@PYVERNODOTS@m', 'linux_x86_64')
```"
31522,"1.13.2 build failed with ""Not Found"" error for file that exists","**System information**
- OS Platform and Distribution: Ubuntu 18.04 (Windows 10 subsystem)
- TensorFlow installed from: source
- TensorFlow version: 1.13.2
- Python version: 3.6.8
- Bazel version: 0.21.0
- GCC version: 7.4.0

**Describe the problem**
Just trying to get a CPU hardware-specific install; selected ""No"" to all except XLA JIT support during configuration. I don't need CUDA/GPU support.

```
ERROR: /mnt/c/Users/peter/Desktop/projects/tensorflow/tensorflow/python/BUILD:5873:1: Executing genrule //tensorflow/python:nccl_ops_pygenrule failed (Aborted): bash failed: error executing command /bin/bash bazel-out/k8-opt/genfiles/tensorflow/python/nccl_ops_pygenrule.genrule_script.sh
2019-08-11 14:35:54.567507: F tensorflow/python/framework/python_op_gen_main.cc:123] Non-OK-status: api_def_map.LoadFileList(env, api_files) status: Not found: tensorflow/core/api_def/base_api/api_def_Imag.pbtxt; No such file or directory
...
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 11735.779s, Critical Path: 3155.14s
INFO: 7082 processes: 7082 local.
FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
bazel clean --expunge
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**

I verified that `tensorflow/core/api_def/base_api/api_def_Imag.pbtxt` does indeed exist. 
"
31521,Error in Android Example,"Hello,

I want to tell that I am using source code which I got from tensorflow.com/lite website. Now I am frustrated to solve this error. Please see screenshot and let me know. how to fix it and use this. When I start the app then it takes seconds to take to crash my app.

![Screenshot (10)](https://user-images.githubusercontent.com/39325207/62836209-78cb4780-bc7e-11e9-9a8d-2bd625c9dd41.png)
![Screenshot (11)](https://user-images.githubusercontent.com/39325207/62836210-7963de00-bc7e-11e9-94d6-8fa373f1fcb0.png)
![Screenshot (12)](https://user-images.githubusercontent.com/39325207/62836211-7963de00-bc7e-11e9-9502-74f18d87622c.png)


"
31520,yet another windows 10 build fail ( 2.0 rc.0 ),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0-rc.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.26.0 x64
- GCC/Compiler version (if compiling from source):  Visual Studio 2019
- CUDA/cuDNN version: 10.1 / 7.6.2
- GPU model and memory: RTX 2080Ti GDDR6 11GB


**Describe the problem**
build failed 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel build --config=opt --config=v2 --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: D:/repo/tensorflow/tensorflow/core/grappler/optimizers/data/BUILD:712:1: C++ compilation of rule '//tensorflow/core/grappler/optimizers/data:rebatch' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/alan-workstation/_bazel_alan-workstation/ibqopsat/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\ALAN-W~1\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\ALAN-W~1\AppData\Local\Temp
  C:/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-py2-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-py2-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-py2-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-py2-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-py2-opt/bin/external/jpeg /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-py2-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-py2-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-py2-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-py2-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-py2-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-py2-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-py2-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-py2-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_tensorrt /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Iexternal/eigen_archive /Ibazel-out/x64_windows-py2-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-py2-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-py2-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-py2-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-py2-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-py2-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/cublas/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX /Fobazel-out/x64_windows-py2-opt/bin/tensorflow/core/grappler/optimizers/data/_objs/rebatch/rebatch.o /c tensorflow/core/grappler/optimizers/data/rebatch.cc
Execution platform: @bazel_tools//platforms:host_platform
tensorflow/core/grappler/optimizers/data/rebatch.cc(66): fatal error C1001: An internal error has occurred in the compiler.
(compiler file 'msc1.cpp', line 1468)
 To work around this problem, try simplifying or changing the program near the locations listed above.
Please choose the Technical Support command on the Visual C++
 Help menu, or open the Technical Support help file for more information
Internal Compiler Error in C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\amd64\cl.exe.  You will be prompted to send an error report to Microsoft later.
INTERNAL COMPILER ERROR in 'C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\amd64\cl.exe'
    Please choose the Technical Support command on the Visual C++
    Help menu, or open the Technical Support help file for more information
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1520.158s, Critical Path: 200.17s
INFO: 3706 processes: 3706 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
"
31519,TF 2.0  ft.GradientTape() gradient()  second gradient None,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Anaconda 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): tensorflow 2.0b1
- Python version:  3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
second gradient with input  is None



**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
import numpy as np
from matplotlib import pyplot as plt
import time
import math
import tensorflow as tf
import numpy as np
import tensorflow as keras
from tensorflow.keras import layers
from tensorflow import keras
from tensorflow.keras.utils import plot_model
from scipy . stats import multivariate_normal as normal

d = 2
batch_size = 3


def func(x):
    x = x*x
    x = keras.layers.Dense(d)(x)
    return x

Input  = keras.Input(shape = (d,), dtype = tf.float64, name = 'X' )
Output = func(Input)
model = keras.Model(inputs=Input, outputs=Output)



x_train = tf.random.uniform( (batch_size, d), minval=0, maxval=1, dtype=tf.float64, seed=1000)

for epoch in range (1):
    with tf.GradientTape() as t1:
        t1.watch(x_train)
        with tf.GradientTape() as t2:
            t2.watch(x_train)
            predictions = model( x_train )

        dy_dx = t2.gradient(predictions, x_train)
        print(""dy_dx = "", dy_dx)
    dyy_dx = t1.gradient(dy_dx, x_train)

    print(""dyy_dx = "", dyy_dx)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
W0811 23:13:04.473541 11804 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8>>: AssertionError: 
dy_dx =  tf.Tensor(
[[0.03395048 0.86635576]
 [0.52642456 0.80100264]
 [1.05554004 0.05975098]], shape=(3, 2), dtype=float64)
dyy_dx =  None"
31518,saved_model_cli convert hub module to tensorRT model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Colab Notebook
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary): Provided by Colab
- TensorFlow version (use command below): Provided by Colab
- Python version: Provided by Colab
- Bazel version (if compiling from source): Provided by Colab
- GCC/Compiler version (if compiling from source): Provided by Colab
- CUDA/cuDNN version: Provided by Colab
- GPU model and memory: Provided by Colab

**Describe the current behavior**
I am saving the BigGan-Deep-512 module from the TF hub into a savedModel.
And when I try to convert it to a tensorRT module it fails

**Describe the expected behavior**
I expect that the **convert** should be applicable directly to the hub module.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Code to save the hub module to a savedModel:

```
import os
import numpy as np
from datetime import datetime

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.python.summary import summary

print(tf.__version__)

with tf.Graph().as_default():
    module = hub.Module(""https://tfhub.dev/deepmind/biggan-deep-512/1"")
    initial_inputs = {k: tf.compat.v1.placeholder(v.dtype, v.get_shape().as_list(), k)
                      for k, v in module.get_input_info_dict().items()}
    output = module(initial_inputs)
    print(initial_inputs)
    print(output)
    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
    with tf.Session() as session:
        session.run(init_op)
        tf.saved_model.simple_save(
            session,
            ""bg/serving_saved_model"",
            inputs={'z': initial_inputs['z'], 'y': initial_inputs['y'], 'truncation': initial_inputs['truncation']},
            outputs={""output"": output},
            legacy_init_op=tf.tables_initializer()
        )
```

Bash command to convert to tensorRT model:

```
%%bash

saved_model_dir=bg/serving_saved_model
opt_model_dir=bg/opt_saved_model

saved_model_cli convert --dir=${saved_model_dir} --output_dir=${opt_model_dir} --tag_set serve tensorrt --precision_mode FP32 --max_batch_size 1 --is_dynamic_op True
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Here are the logs: 

```
2019-08-11 11:52:49.646188: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-08-11 11:52:49.646774: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5590ff396140 executing computations on platform Host. Devices:
2019-08-11 11:52:49.646832: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-11 11:52:49.651856: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-08-11 11:52:49.799218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:52:49.799757: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5590ff3979c0 executing computations on platform CUDA. Devices:
2019-08-11 11:52:49.799788: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5
2019-08-11 11:52:49.800013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:52:49.800421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59
pciBusID: 0000:00:04.0
2019-08-11 11:52:49.800717: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-11 11:52:49.801856: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-11 11:52:49.803002: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-11 11:52:49.803339: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-11 11:52:49.804771: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-11 11:52:49.805786: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-11 11:52:49.808901: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-11 11:52:49.809023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:52:49.809465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:52:49.809790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-08-11 11:52:49.814498: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-11 11:52:49.815576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-11 11:52:49.815605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-08-11 11:52:49.815613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-08-11 11:52:49.822976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:52:49.823434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:52:49.823790: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2019-08-11 11:52:49.823830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13500 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
WARNING: Logging before flag parsing goes to stderr.
W0811 11:52:49.824687 139988375598976 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py:245: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
W0811 11:53:27.486973 139988375598976 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0811 11:53:48.502595 139988375598976 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py:268: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0811 11:53:48.502814 139988375598976 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
2019-08-11 11:53:54.431183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:53:54.431681: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2019-08-11 11:53:54.431812: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-08-11 11:53:54.432418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:53:54.432767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59
pciBusID: 0000:00:04.0
2019-08-11 11:53:54.432835: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-11 11:53:54.432858: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-11 11:53:54.432869: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-11 11:53:54.432885: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-11 11:53:54.432900: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-11 11:53:54.432910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-11 11:53:54.432920: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-11 11:53:54.432990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:53:54.433378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:53:54.433695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-08-11 11:53:54.433737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-11 11:53:54.433754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-08-11 11:53:54.433762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-08-11 11:53:54.434036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:53:54.434450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-11 11:53:54.434793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13500 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
2019-08-11 11:54:02.561839: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: tf_graph
2019-08-11 11:54:02.561909: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 7430 nodes (-10408), 12752 edges (-11753), time = 4901.51221ms.
2019-08-11 11:54:02.561918: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 7580 nodes (150), 13019 edges (267), time = 683.694ms.
2019-08-11 11:54:02.561924: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 7549 nodes (-31), 12986 edges (-33), time = 1578.22205ms.
Traceback (most recent call last):
  File ""/usr/local/bin/saved_model_cli"", line 10, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 909, in main
    args.func(args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 680, in convert_with_tensorrt
    output_saved_model_dir=args.output_dir)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py"", line 51, in create_inference_graph
    session_config=session_config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 1148, in create_inference_graph
    trt_converter.save(output_saved_model_dir)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 825, in save
    super(TrtGraphConverter, self).save(output_saved_model_dir)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 432, in save
    importer.import_graph_def(self._converted_graph_def, name="""")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 431, in import_graph_def
    raise ValueError(str(e))
ValueError: Input 0 of node module_apply_default/cond/AssignVariableOp/Switch was passed float from module/prev_truncation:0 incompatible with expected resource.

```
"
31517,How to reduce the memory use of image operations,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 1809 x64
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.3 x64
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda_10.0.130_411.31_win10 & cudnn-10.0-windows10-x64-v7.5.1.10
- GPU model and memory: GTX 1070 Max-Q 6376 MB

**Describe the current behavior**

I want to do some operations like:
1. Split the image into 12*12 squares.
2. For each sub square, split it into 6*(2*12) horizontal & 6*(12*2) vertical rectangles.
3. For each rectangle, calculate it's pixels horizontal & vertical gradient's mean and standard deviation.
So, for each sub square, i want 48 values, which is 
6*(hori_gx_mean,hori_gy_mean,hori_gx_std,hori_gy_std)----24
and
6*(vert_gx_mean,vert_gy_mean,vert_gx_std,vert_gy_std)----24

Now I have a 1920 * 1280 image with 3 chanels.
I read image by 
`tf.Session().run(tf.image.decode_image(tf.read_file(imgDir)))/255.0`

and calculate image by 

`
    for y in range(int(srcImg.shape[0]/12)-1):
        for x in range(int(srcImg.shape[1]/12)-1):
            sub12_12 = tf.image.crop_to_bounding_box(_src, y*12, x*12, 12, 12)
            # computes every 2*12 and 12*2 subSet in sub12_12
            for div in range(6):
                # height=2 width=12
                sub12_2 = tf.image.crop_to_bounding_box(
                    sub12_12, div * 2, 0, 2, 12)
                # height=12 width=2
                sub2_12 = tf.image.crop_to_bounding_box(
                    sub12_12, 0, div* 2, 12, 2)
                    
                # computes gradient, return (dy, dx)
                _dy_12_2, _dx_12_2 = tf.image.image_gradients(
                    tf.image.rgb_to_grayscale(sub12_2))
                _dy_2_12, _dx_2_12 = tf.image.image_gradients(sub2_12)

                # computes standard deviation
                _stDy12_2 = tf.math.reduce_std(_dy_12_2)
                _stDx12_2 = tf.math.reduce_std(_dx_12_2)

                _stDy2_12 = tf.math.reduce_std(_dy_2_12)
                _stDx2_12 = tf.math.reduce_std(_dx_2_12)
                # compute gradient's mean
                _uDy12_2 = tf.math.reduce_mean(_dy_12_2)
                _uDx12_2 = tf.math.reduce_mean(_dx_12_2)

                _uDy2_12 = tf.math.reduce_mean(_dy_2_12)
                _uDx2_12 = tf.math.reduce_mean(_dx_2_12)

                # 6 set of horizontal and vertical sub-rectangle feature
                feaBuf.append([_uDy12_2, _uDx12_2, _stDy12_2, _stDx12_2,
                                _uDy2_12, _uDx2_12, _stDy2_12, _stDx2_12])
            _retExp.append(feaBuf)
            feaBuf=[]
`

and cost about 0.5g of memory per line of image, and it ist affordable.

**Describe the expected behavior**

Reduce memory cost to about 500kB per line.
"
31516,tensorflow 2.0 tensorflow.python.eager.function.TfMethodTarget object at could not be transformed and will be executed as-is.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA485A58>> could not be transformed and will be executed as-is. 
Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA485A58>>: AssertionError: W0811 12:49:24.207197  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA4A82B0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA4A82B0>>: AssertionError:

**System information**
Windows 10 , Anaconda, Python 3.7  Tensorflow 2.0b1


You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**

```
import time
import math
import tensorflow as tf
import numpy as np
import tensorflow as keras
from tensorflow.keras import layers
from tensorflow import keras
from tensorflow.keras.utils import plot_model
from scipy . stats import multivariate_normal as normal

d = 10
T = 0.1
n_time = 5
n_sample = 100
batch_size = 100
n_maxstep = 400
h = (T + 0.0) / n_time
t_stamp = np.arange (0, n_time) * h

def f_tf (t, X, Y, Z):
    V =  Y - tf.math.sin (Y)
    return V

def g_tf (t, X):
    V =  tf.math.reduce_sum (X**3, 1,keepdims=True)
    return V

def k_tf ( n_sample ):
    W = np.zeros ([ n_sample, d, n_time  ], dtype = np.float64)
    X_sample  = np.zeros ([ n_sample, d, n_time+1], dtype = np.float64)
    for i in range (n_time):
        W [:, :, i  ] = np.reshape ( normal.rvs ( mean = np.zeros(d,dtype = np.float64),\
                                      cov =1, size = n_sample ), ( n_sample, d))
        X_sample  [:, :, i+1] =  W [:, :, i]
    return W, X_sample

def nn_tf(x):
    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)
    x = keras.layers.Dense(d, batch_size = n_sample)(x)
    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)
    return x

dW = keras.Input(shape = (d, n_time  ), batch_size=n_sample, dtype = tf.float64, name = 'dW')
XX = keras.Input(shape = (d, n_time+1), batch_size=n_sample, dtype = tf.float64, name = 'X' )
X = XX
Y = tf.zeros([n_sample, 1], dtype = tf.float64)
Z = tf.zeros([n_sample, d], dtype = tf.float64)

for it in range(n_time-1):
    with tf.name_scope(str(it+1)):
        Y = Y +  tf.math.reduce_sum( Z * dW[:,:,it],  1, keepdims=True)
        subX = tf.reshape(X[:,:,it], shape = [n_sample, d])
        Z = nn_tf(subX) / d

Y = Y + tf.math.reduce_sum (Z * dW [:, :, n_time-1], 1, keepdims=True)
model = keras.Model(inputs=[XX,dW], outputs=[Y])

optimizer = keras.optimizers.Adam(learning_rate=1e-3)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
dW_train, X_train = k_tf ( n_sample )

for epoch in range (10):
    with tf.GradientTape() as tape:
        predictions = model( [X_train, dW_train] )
        label = g_tf (T, X_train[:, :, n_time])
        loss_value = tf.reduce_sum( tf.keras.losses.MSE (label, predictions ) )
    grads = tape.gradient(loss_value,  model.trainable_variables)
    optimizer.apply_gradients( zip(grads, model.trainable_variables) )
    accuracy = train_accuracy(label, predictions)
    print(""step "", epoch, ""loss = "", loss_value.numpy(), ""accuracy = "", accuracy.numpy())

```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
31515,tensorflow 2.0   tensorflow.python.eager.function.TfMethodTarget object could not be transformed and will be executed as-is,"WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361ABA58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361ABA58>>: AssertionError: 
W0811 11:46:20.769339  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361AB470>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361AB470>>: AssertionError: 
W0811 11:46:20.802253  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1748>>: AssertionError: 
W0811 11:46:20.815217  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1860>>: AssertionError: 
W0811 11:46:20.827151  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361FB1D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361FB1D0>>: AssertionError: 
W0811 11:46:20.838119  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5360F8DD8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5360F8DD8>>: AssertionError: 
W0811 11:46:20.852117  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362195F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362195F8>>: AssertionError: 
W0811 11:46:20.863052  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536110F98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536110F98>>: AssertionError: 
W0811 11:46:20.879015  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362362B0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362362B0>>: AssertionError: 
W0811 11:46:20.888983  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536236320>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536236320>>: AssertionError: 
W0811 11:46:20.905940  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536247B38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536247B38>>: AssertionError: 
W0811 11:46:20.917908  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53626E0B8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53626E0B8>>: AssertionError: 
W0811 11:46:20.938849  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53627C550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53627C550>>: AssertionError: 
W0811 11:46:20.948858  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC668>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC668>>: AssertionError: 
W0811 11:46:20.961826  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362BC588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362BC588>>: AssertionError: 
W0811 11:46:20.970813  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362D5048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362D5048>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361AB470>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361AB470>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1748>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1860>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361FB1D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361FB1D0>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5360F8DD8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5360F8DD8>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362195F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362195F8>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536110F98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536110F98>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362362B0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362362B0>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536236320>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536236320>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536247B38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536247B38>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53626E0B8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53626E0B8>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53627C550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53627C550>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC668>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC668>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362BC588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362BC588>>: AssertionError: 
W0811 11:46:21.001721  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362F85F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362F85F8>>: AssertionError: 
W0811 11:46:21.017639  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC208>>: AssertionError: 
W0811 11:46:21.033596  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536340080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536340080>>: AssertionError: 
W0811 11:46:21.070497  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536350390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536350390>>: AssertionError: 
W0811 11:46:21.083488  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536395C88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536395C88>>: AssertionError: 
W0811 11:46:21.097465  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5363BF6A0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5363BF6A0>>: AssertionError: 
W0811 11:46:21.119367  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5363D2E48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5363D2E48>>: AssertionError: 
W0811 11:46:21.144337  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5364100F0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5364100F0>>: AssertionError: 
W0811 11:46:21.158263  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53642CE10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53642CE10>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362D5048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362D5048>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362F85F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362F85F8>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC208>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536340080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536340080>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536350390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536350390>>: AssertionError: 
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536395C88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536395C88>>: AssertionError: "
31514,Bug in keras.layers.DepthwiseConv2D when using stride and dilation,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (attached)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: Nvidia Quadro P2000 with 4GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When executing the following lines of code, I get an assertion, because the result is not as expected, but actually the values [-29, -19, 1]. I would see how the -19 would be correct if I had a stride of 1 along the y-axis, however, then it would not match up again with the third number being 1 instead of -4. So it is not only that either dilation or stride is ignored, but actually providing for both non-trivial values results in some strange behavior, which I think is actually a bug.
```
import numpy as np
import tensorflow as tf

kernel = np.array([5, 5], dtype=np.float32).reshape([1, 2, 1, 1])
bias = np.array([-4], dtype=np.float32)
img = np.array([-2, -1, -3, -2, 3, -1, -2, -1, 2], dtype=np.float32).reshape([1, 1, 9, 1])

strides = (2, 2)
dilation = (1, 2)

depthconv2d_layer = tf.keras.layers.DepthwiseConv2D(
    depth_multiplier=1,
    kernel_size=(1, 2),
    strides=strides,
    dilation_rate=dilation,
    padding='valid',
    depthwise_initializer=lambda *args, **kwargs: tf.constant(kernel),
    bias_initializer=lambda *args, **kwargs: tf.constant(bias),
)
depthconv2d_output = depthconv2d_layer(tf.constant(img))

with tf.Session() as s:
    s.run(tf.global_variables_initializer())
    result = s.run(depthconv2d_output)
    print(result)
    assert np.all(result == np.array([-29, -4, 1, -4]).reshape(1, 1, 4, 1))
```
**Describe the expected behavior**
No assertion, as I computed the values by hand and expect them to be right.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Nothing else.
"
31513,tf.keras.metrics.TruePositives() returning wrong value in model.fit() when passed as metric to model.compile(),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.7.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When I pass one-hot encoded labels as train and validation data into tensorflow keras' model.fit() function, the metric `tf.keras.metrics.TruePositives()` (and TN, FN, FP) return wrong values.
If
`train_labels` is this: `array([[1, 0], [1, 0], [0, 1]])`

and the resulting `y_pred`'s are `array([[1, 0], [1, 0], [0, 1]])`

then the sum of
```
tf.keras.metrics.TruePositives()
tf.keras.metrics.TrueNegatives() 
tf.keras.metrics.FalsePositives() 
tf.keras.metrics.FalseNegatives()
```
is 6.

**Describe the expected behavior**
The sum should be 3.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
from tensorflow.keras import models, layers
import numpy as np


# Generate Data
train_data = np.random.randint(0, 100, size=(10, 215, 215, 1))
train_labels = np.eye(2)[np.random.randint(0, 2, size=(10, 1)).reshape(-1)]


# Set up metrics
metrics = ['accuracy',
           tf.keras.metrics.TruePositives(),
           tf.keras.metrics.TrueNegatives(),
           tf.keras.metrics.FalseNegatives(),
           tf.keras.metrics.FalsePositives()]

# Set up model type
model = models.Sequential(name='CNN')

# Add layers
model.add(layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), input_shape=train_data[0].shape))
model.add(layers.Conv2D(filters=64, kernel_size=(5, 5), strides=(2, 2)))
model.add(layers.Conv2D(filters=128, kernel_size=(5, 5), strides=(2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(units=128))
model.add(layers.BatchNormalization())
model.add(layers.ReLU())
model.add(layers.Dense(units=2, activation='sigmoid'))
model.compile('sgd', 'binary_crossentropy', metrics)

history = model.fit(train_data, train_labels, batch_size=1, epochs=30)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

This also impacts recall, precision, etc. which are not correctly calculated as a result."
31510,TF 1.14 on AI Platform: MirroredStrategy fails on 2 GPUs with RuntimeError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): AI Platform 1.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14 
- Python version: 3.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: AI Platform
- GPU model and memory: V100 16GB

**Describe the current behavior**

I have written a custom estimator and wanted to train it on 2 GPUs using `tf.distribute.MirroredStrategy`, submitting [this job](https://github.com/sdll/psenet/blob/cc3041da4565da03f5ebe206962ee424c1f36bbb/train.sh#L4) to the AI Platform. Unfortunately, after ~900 steps, the training fails with `RuntimeError: Variable creator scope nesting error: move call to tf.distribute.set_strategy() out of `with` scope.`

**Describe the expected behavior**

The model should train on two GPUs asynchronously.

**Code to reproduce the issue**

The estimator definition starts [here](https://github.com/sdll/psenet/blob/cc3041da4565da03f5ebe206962ee424c1f36bbb/psenet/train.py#L193).

**Other info / logs**

- [the entire AI platform log](https://github.com/tensorflow/tensorflow/files/3489041/log_rc47.txt)
- another curious detail is that the estimator seems to be executed only on a single GPU.

Turning on the placement logging with a similar set-up confirmed that training just used the first GPU before failing.

Here is the usage graph, where you can see a short spike before the failure:

<img width=""975"" alt=""Screen Shot 2019-08-10 at 6 42 35 PM"" src=""https://user-images.githubusercontent.com/17913919/62823852-b0fb5900-bb9e-11e9-8cd2-d4eacd73cfeb.png"">
"
31509,BaseCollectiveExecutor::StartAbort Out of range: warnings when fit model in graph mode (TF 2.0 Nightly),"**System information**
- OS Platform and Distribution : Windows 10
- TensorFlow installed from : Binary
- TensorFlow version : TF 2.0 Nightly GPU Preview
- Python version:3.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: 960M

I have a very simple model that I have made by inheriting from tf.Keras.model, which I feed with a dataset i,e

model = MyModel(...)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, amsgrad=True),
                       loss=loss_fn,
                       run_eagerly=False)

dataset = tf.data.Dataset.from_tensor_slices((x, y))
dataset = dataset.shuffle(buffer_size=10000)
dataset = dataset.batch(batch_size=1000)

model.fit(dataset,
          epochs=100,
          verbose=0,
          callbacks=[LossAndErrorPrintingCallback()])

If I run this using TF 2.0 (beta), works perfectly fine i.e with run_eagerly=False. If I run it using TF nightly preview with run_eagerly=True, again fine. However if I try with run_eagerly=False using nightly preview I get a stream of the following warnings,

 2019-08-10 16:35:40.168418: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}}]]"
31507,Update TensorFlow For Poets,"It would be awesome if someone could update TensorFlow For Poets, with the latest stable TensorFlow and Inception v4.

Currently, there are lots of deprecation warnings logged when TensorFlow For Poets is run on TensorFlow 1.7.

In addition, I am unable to get TensorFlow For Poets to run using Inception v4.  The code currently downloads ``inception-2015-12-05.tgz``, which contains the training file ``classify_image_graph_def.pb``.  But when I tried to substitute ``inception_v4_2016_09_09.tar.gz``, I found that it contained a ``inception_v4.ckpt`` file instead.  I understand that ``.ckpt`` implies a checkpoint file, but I do not know how to convert a ``.ckpt`` file to the ``.pb`` file that TensorFlow For Poets is trying to load.

I am sure all of this is documented somewhere, e.g. how to swap out the code to initialize from a .ckpt instead, but given that the example is supposed to be an introduction, doing so is a bit above my head at present.  Likely so for other newcomers.

Thanks for the awesome work.  I have gotten TforP running with the old inception model and my own image classifications, but my results, while good, I think could be improved somewhat by the newer Inception v4 pre-trained model.
"
31505,Tensorflow 2.0 does not use GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): yes
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.7
- CUDA/cuDNN version: 10.1
- GPU model and memory: Nvidia MX250 4Gb

**Describe the current behavior**
I'm playing around with tf2.0 while mostly still working on tf1.14. I have created two conda environments - one for tf1.14 and the other one for tf2.0 - so all hardware and drivers are absolutely the same. Tf1.14 is working good and uses gpu as it supposed to, but tf2.0 runs on cpu.

**Code to reproduce the issue**
if i run:
```python
from tensorflow.python.client import device_lib
device_lib.list_local_devices()

i get an output with tf2.0:

[name: ""/device:CPU:0""
 device_type: ""CPU""
 memory_limit: 268435456
 locality {
 }
 incarnation: 9820328803404595310, name: ""/device:XLA_CPU:0""
 device_type: ""XLA_CPU""
 memory_limit: 17179869184
 locality {
 }
 incarnation: 7262628900557013132
 physical_device_desc: ""device: XLA_CPU device"", name: ""/device:XLA_GPU:0""
 device_type: ""XLA_GPU""
 memory_limit: 17179869184
 locality {
 }
 incarnation: 4554198733675012648
 physical_device_desc: ""device: XLA_GPU device""]

and 

import tensorflow as tf
tf.test.is_gpu_available()
 
gives False. 
Same code in tf1.14 gives the following:

[name: ""/device:CPU:0""
 device_type: ""CPU""
 memory_limit: 268435456
 locality {
 }
 incarnation: 11125515865111208996, name: ""/device:XLA_CPU:0""
 device_type: ""XLA_CPU""
 memory_limit: 17179869184
 locality {
 }
 incarnation: 7071753861411904965
 physical_device_desc: ""device: XLA_CPU device"", name: ""/device:GPU:0""
 device_type: ""GPU""
 memory_limit: 3333029888
 locality {
   bus_id: 1
   links {
   }
 }
 incarnation: 17329402577157719353
 physical_device_desc: ""device: 0, name: GeForce MX250, pci bus id: 0000:02:00.0, compute capability: 6.1"", name: ""/device:XLA_GPU:0""
 device_type: ""XLA_GPU""
 memory_limit: 17179869184
 locality {
 }
 incarnation: 17993919851972383985
 physical_device_desc: ""device: XLA_GPU device""]

And:

import tensorflow as tf
tf.test.is_gpu_available()

Gives True
```"
31504,"NotImplementedError: tf.GradientTape.gradients() does not support graph control flow operations like tf.cond or tf.while at this time. Use tf.gradients() instead. If you need this feature, please file a feature request at https://github.com/tensorflow/tensorflow/issues/new","loss_object = tf.keras.losses.CategoricalCrossentropy()

def create_adversarial_pattern(input_image, input_label):
  with tf.GradientTape() as tape:
    tape.watch(input_image)
    prediction = model(input_image)
    print(prediction.shape)
    loss = loss_object(input_label, prediction)
    print('Loss:',loss)

  # Get the gradients of the loss w.r.t to the input image.
  
  gradient = tape.gradient(loss, input_image)
  print('................')
  print('Gradient tensor:',gradient)
  # Get the sign of the gradients to create the perturbation
  signed_grad = tf.sign(gradient)

  return signed_grad


perturbations = create_adversarial_pattern(new_img, y_pred1)
print(perturbations)



Hello, can anyone help me to solve the error???  Why am I not getting a tensor shape of gradient
(1,299,299,3). I followed https://www.tensorflow.org/beta/tutorials/generative/adversarial_fgsm
lnk to do my code. TIA "
31502, module 'tensorflow._api.v2.train' has no attribute 'AdamOptimizer',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31501,Can't translate saved model to MLIR,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary for training, Source for translation
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.3
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.0/7.4.1
- GPU model and memory: P100


**Describe the current behavior**

I trained the official resnet imagenet model (r1) until it exported a saved_model.pb and associated variables folder

I then copied both of those into a separate directory and verified that the model was saved correctly by loading it and inferencing a few images, which returned the correct results.

I cloned tensorflow and called the following command in the tensorflow directory to translate the saved model to MLIR

`bazel run //tensorflow/compiler/mlir:tf-mlir-translate -- --graphdef-to-mlir --tf-input-arrays=input_tensor --tf-input-shapes=32,244,244,3 --tf-input-data-types=DT_FLOAT --tf-output-arrays=ArgMax ~/MLIR/saved_model.pb -o ~/MLIR/saved_model.mlir`

However, I got the following error

> INFO: Analysed target //tensorflow/compiler/mlir:tf-mlir-translate (0 packages loaded, 0 targets configured).
> INFO: Found 1 target...
> Target //tensorflow/compiler/mlir:tf-mlir-translate up-to-date:
>   bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate
> INFO: Elapsed time: 0.571s, Critical Path: 0.00s
> INFO: 0 processes.
> INFO: Build completed successfully, 1 total action
> INFO: Running command line: bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --graphdef-to-mlir '--tf-input-arrays=input_tensor' '--tf-input-shapes=32,244,244,3' '--tf-input-data-types=DT_FLOAT' '--tf-output-arrays=ArgMax' /home/alber

_It got cut off by the edge of the terminal above_

> INFO: Build completed successfully, 1 total action
> 2019-08-09 16:59:17.399053: E tensorflow/compiler/mlir/tensorflow/utils/import_utils.cc:66] Error parsing Protobuf: /home/albert/MLIR/saved_model.pbtxt
> 2019-08-09 16:59:17.399429: E tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate.cc:81] Graph import failed: Invalid argument: Could not parse input file

I tried converting the saved_model.pb to a .pbtxt and running the same command, but I got the same error

I've uploaded the contents of the .pbtxt file here: [http://m.uploadedit.com/bbtc/1565397330160.txt](http://m.uploadedit.com/bbtc/1565397330160.txt)
"
31500,CUDA dll check not reporting correct version in comments,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Line 76 in tensorflow/python/platform/self_check.py
        except OSError:
          raise ImportError(
              ""Could not find %r. TensorFlow requires that this DLL be ""
              ""installed in a directory that is named in your %%PATH%% ""
              ""environment variable. Download and install CUDA %s from ""
              ""this URL: https://developer.nvidia.com/cuda-90-download-archive""
              % (build_info.cudart_dll_name, build_info.cuda_version_number))

The URL directs you to version 9 but the current dll checks for version 10. 


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31499,MultiWorkerMirroredStrategy stuck in all_reduce,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave
- TensorFlow installed from (source or binary): PyPi
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.5
- CUDA/cuDNN version: none. (MacOS)
- GPU model and memory: none. (MacOS)

**Describe the current behavior**
I am setting up a multi-worker cluster with two workers and I want to reduce the value of a tensor from each worker. Tensorflow is simply blocking on the collective all reduce operation (tensorflow.python.ops.collective_ops.all_reduce) and never synchronizes between workers.

**Describe the expected behavior**
I expect the collective op to return the sum of the tensor across each worker. More specifically, I expect the code below to print the value `[[2.,3.]]`

**Code to reproduce the issue**
```python
import json
import os
import sys
from multiprocessing import Process
import tensorflow as tf
from tensorflow.distribute.cluster_resolver import TFConfigClusterResolver

tf.logging.set_verbosity('DEBUG')

def test_dist(task_id):
    resolver = TFConfigClusterResolver()
    cluster = resolver.cluster_spec()
    server = tf.distribute.Server(
        cluster, job_name=""worker"", task_index=task_id)

    dist = tf.distribute.experimental.MultiWorkerMirroredStrategy(
        tf.distribute.experimental.CollectiveCommunication.RING)

    print('num replicas', dist.num_replicas_in_sync)

    with tf.device('/job:worker/task:{0}/device:CPU:0'.format(task_id)):
        t = tf.Variable([1.0,3.0*task_id], dtype=tf.float32, name='myvar')

    def sum_deltas_fn(v):
        return tf.identity(v)

    with dist.scope():
        all_ts = dist.experimental_run_v2(sum_deltas_fn, args=[t])
        delta_sums_results = dist.reduce(tf.distribute.ReduceOp.SUM, all_ts)

        sess = tf.compat.v1.Session(server.target)
        sess.run(tf.compat.v1.global_variables_initializer())

        print('tensor', delta_sums_results)
        print('tensor value', sess.run(delta_sums_results))

test_dist(int(sys.argv[1]))
```

I run two workers in one terminal each:
```
# Terminal 1
export TF_CONFIG='{ ""cluster"": { ""worker"": [""localhost:8027"", ""localhost:8028""] }, ""task"": {""type"": ""worker"", ""index"": 0} }'
python collective_reduce.old.py 0
```

```
# Terminal 2
export TF_CONFIG='{ ""cluster"": { ""worker"": [""localhost:8027"", ""localhost:8028""] }, ""task"": {""type"": ""worker"", ""index"": 1} }'
python collective_reduce.old.py 1
```

**Other info / logs**
The output is:

worker 1
```
WARNING: Logging before flag parsing goes to stderr.
W0809 16:15:59.427072 4541060544 deprecation_wrapper.py:119] From collective_reduce.old.py:8: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

2019-08-09 16:15:59.428197: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-09 16:15:59.431492: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8027, 1 -> localhost:8028}
2019-08-09 16:15:59.432571: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:8027
I0809 16:15:59.470151 4541060544 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0
W0809 16:15:59.471471 4541060544 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
I0809 16:15:59.472136 4541060544 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['localhost:8027', 'localhost:8028']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.RING
num replicas 2
I0809 16:15:59.488581 4541060544 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0809 16:15:59.489030 4541060544 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
tensor Tensor(""allreduce/CollectiveReduce:0"", shape=(2,), dtype=float32, device=/job:worker/replica:0/task:0/device:CPU:0)
```

worker 2
```
WARNING: Logging before flag parsing goes to stderr.
W0809 16:16:07.349786 4473202112 deprecation_wrapper.py:119] From collective_reduce.old.py:8: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

2019-08-09 16:16:07.350931: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-09 16:16:07.354157: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8027, 1 -> localhost:8028}
2019-08-09 16:16:07.355424: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:8028
I0809 16:16:07.392420 4473202112 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0
W0809 16:16:07.393295 4473202112 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
I0809 16:16:07.393734 4473202112 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['localhost:8027', 'localhost:8028']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.RING
num replicas 2
I0809 16:16:07.408463 4473202112 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0809 16:16:07.408880 4473202112 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
tensor Tensor(""allreduce/CollectiveReduce:0"", shape=(2,), dtype=float32, device=/job:worker/replica:0/task:1/device:CPU:0)
```

Whether this is a bug or me doing something wrong, it is very unclear in the docs how to do anything with `MultiWorkerMirroredStrategy`.
Also, the example with setting the env variable directly with os.environ here https://www.tensorflow.org/guide/distribute_strategy#setting_up_tf_config_environment_variable
does not register, and the ClusterSpec will be empty"
31498,TF2.0beta1 - Not JSON Serializable when using tf.keras.experimental.export_saved_model,"Most code is from one of the tensorflow 2.0 beta guides: [Writing layers and models with TensorFlow Keras](https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example)

**Describe the current behavior**

TypeError: ('Not JSON Serializable:', b'\n\x06Square\x12\x06Square\x1a\x0fz_mean/Identity*\x07\n\x01T\x12\x020\x01')

**Describe the expected behavior**

Save model correctly.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras import layers

# Get training data.
(x_train, _), _ = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype('float32') / 255

original_dim = 784
intermediate_dim = 64
latent_dim = 32

def sampling(inputs):
    z_mean, z_log_var = inputs
    batch = tf.shape(z_mean)[0]
    dim = tf.shape(z_mean)[1]
    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon    

# Define encoder model.
original_inputs = tf.keras.Input(shape=(original_dim,), name='encoder_input')
x = layers.Dense(intermediate_dim, activation='relu')(original_inputs)
z_mean = layers.Dense(latent_dim, name='z_mean')(x)
z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)
z = tf.keras.layers.Lambda(sampling)((z_mean, z_log_var))
encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')

# Define decoder model.
latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')
x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)
outputs = layers.Dense(original_dim, activation='sigmoid')(x)
decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')

# Define VAE model.
outputs = decoder(z)
vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name='vae')

# Add KL divergence regularization loss.
kl_loss = - 0.5 * tf.reduce_mean(
    z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)
vae.add_loss(kl_loss)

# Train.
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())
vae.fit(x_train, x_train, epochs=3, batch_size=64)

# Save model.
tf.keras.experimental.export_saved_model(vae, 'vae_functional_saved_model')
```
**Other info / logs**
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-4eb0216166eb> in <module>
----> 1 tf.keras.experimental.export_saved_model(vae, 'vae_functional_saved_model')

~/.miniconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py in export_saved_model(model, saved_model_path, custom_objects, as_text, input_signature, serving_only)
    167 
    168   try:
--> 169     _export_model_json(model, saved_model_path)
    170   except NotImplementedError:
    171     logging.warning('Skipped saving model JSON, subclassed model does not have '

~/.miniconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py in _export_model_json(model, saved_model_path)
    175 def _export_model_json(model, saved_model_path):
    176   """"""Saves model configuration as a json string under assets folder.""""""
--> 177   model_json = model.to_json()
    178   model_json_filepath = os.path.join(
    179       saved_model_utils.get_or_create_assets_dir(saved_model_path),

~/.miniconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in to_json(self, **kwargs)
   1447     model_config = self._updated_config()
   1448     return json.dumps(
-> 1449         model_config, default=serialization.get_json_type, **kwargs)
   1450 
   1451   def to_yaml(self, **kwargs):

~/.miniconda/envs/tf2/lib/python3.7/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)
    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    237         separators=separators, default=default, sort_keys=sort_keys,
--> 238         **kw).encode(obj)
    239 
    240 

~/.miniconda/envs/tf2/lib/python3.7/json/encoder.py in encode(self, o)
    197         # exceptions aren't as detailed.  The list call should be roughly
    198         # equivalent to the PySequence_Fast that ''.join() would do.
--> 199         chunks = self.iterencode(o, _one_shot=True)
    200         if not isinstance(chunks, (list, tuple)):
    201             chunks = list(chunks)

~/.miniconda/envs/tf2/lib/python3.7/json/encoder.py in iterencode(self, o, _one_shot)
    255                 self.key_separator, self.item_separator, self.sort_keys,
    256                 self.skipkeys, _one_shot)
--> 257         return _iterencode(o, 0)
    258 
    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,

~/.miniconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/util/serialization.py in get_json_type(obj)
     67     return dict(obj)
     68 
---> 69   raise TypeError('Not JSON Serializable:', obj)

TypeError: ('Not JSON Serializable:', b'\n\x06Square\x12\x06Square\x1a\x0fz_mean/Identity*\x07\n\x01T\x12\x020\x01')
```"
31494,export_lib.get_temp_export_dir returns incorrect value with mixed bytes and str,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MacBook Pro
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

The return value of export_lib.get_temp_export_dir is mixed with string and bytes, where the bytes portion is in literal form of temp-b'1234567890', including the letter b and the quotes, these will then become part of the directory name created.

**Describe the expected behavior**
The return value should be temp-1234567890.

**Code to reproduce the issue**
```
from tensorflow_estimator.python.estimator.export import export_lib
from tensorflow.python.lib.io import file_io
import time

base1 = ""/tmp/test/export_base""
temp1 = export_lib.get_temp_export_dir(export_lib.get_timestamped_export_dir(base1))
print(""temp1: "" + temp1.decode(""utf-8""))
file_io.recursive_create_dir(temp1)
arr = os.listdir(base1)
print(arr)
os.rmdir(temp1)

```
**Other info / logs**

Output of above code:

```
temp1: /tmp/test/export_base/temp-b'1565380472'
[""temp-b'1565380472'""]

```

As you can see the b'' became literal.
"
31493,Error making prediction using `Models with multiple inputs and outputs` example code,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab. 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: whatever Colab has. 
- GPU model and memory: NA (GPU not enabled)

**Describe the current behavior**

I'm running the example code here: 
https://www.tensorflow.org/beta/guide/keras/functional#models_with_multiple_inputs_and_outputs
This runs successfully but if I try to make a prediction with the model by running
```
model(title_data, body_data, tags_data)
```
I get the following error: 
```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-37-6db901800232> in <module>()
----> 1 model.call(tf.constant(title_data), tf.constant(body_data), tf.constant(tags_data))

1 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)
    903     output_shapes = []
    904     for x in self.outputs:
--> 905       assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)
    906       tensor = tensor_dict[str(id(x))]
    907       output_shapes.append(x.shape)

AssertionError: Could not compute output Tensor(""priority/Identity:0"", shape=(None, 1), dtype=float32)
```

**Describe the expected behavior**

No error! 

**Code to reproduce the issue**

I can reproduce what I think is the same error using: 
```
import tensorflow as tf
import numpy as np

input_a = tf.keras.layers.Input((4,))
input_b = tf.keras.layers.Input((4,))

output = tf.keras.layers.Concatenate()([input_a, input_b])

model = tf.keras.Model(inputs = (input_a, input_b), outputs = output)

a = np.random.rand(3, 4).astype(np.float32)
b = np.random.rand(3, 4).astype(np.float32)

pred = model(a, b)
```
"
31486,Shape of RaggedTensor is unknown when converting to tensor,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): From Pypa
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.6.4

**Describe the current behavior**

I am using the estimator API. I have variable-lenght input (sentences), so I want to use RaggedTensors in my `tf.data.Datasets`. However, it seems that when converting the RaggedTensor back to tensor, the shape is not evaluated correctly.

**Describe the expected behavior**

When using `x.to_tensor(...)` on the elements of the dataset, the shape should be evaluated to the shape of the tensor.

**Code to reproduce the issue**

```
import tensorflow as tf

shapes = [None]
types = tf.string
defaults = ""<pad>""


def generator_fn_ragged():
    yield (
        [""The"", ""brown"", ""fox"", ""jumps""],
        [""The"", ""brown"", ""fox"", ""jumps"", ""over"", ""the"", ""lazy"", ""dog""],
    )


def input_fn():
    dataset = tf.data.Dataset.from_generator(
        generator_fn_ragged,
        output_shapes=shapes,
        output_types=tf.string,
    )
    dataset = dataset.padded_batch(2, shapes, defaults)
    
    def _ragged(*features):
        return [tf.RaggedTensor.from_tensor(x, padding=""<pad>"") for x in features]

    dataset = dataset.map(_ragged)

    def _unragged(*features):
        return [x.to_tensor(default_value=""<pad>"") for x in features]

    dataset = dataset.map(_unragged)
    
    return dataset

dataset = input_fn()
for el in dataset:
    print(el)
    break
```

This outputs:
```
(<tf.Tensor 'IteratorGetNext_7608:0' shape=<unknown> dtype=string>,)
```

Whereas this:
```
import tensorflow as tf

shapes = [None]
types = tf.string
defaults = ""<pad>""


def generator_fn_ragged():
    yield (
        [""The"", ""brown"", ""fox"", ""jumps""],
        [""The"", ""brown"", ""fox"", ""jumps"", ""over"", ""the"", ""lazy"", ""dog""],
    )


def input_fn():
    dataset = tf.data.Dataset.from_generator(
        generator_fn_ragged,
        output_shapes=shapes,
        output_types=types,
    )
    dataset = dataset.padded_batch(2, shapes, defaults)
    return dataset

dataset = input_fn()
for el in dataset:
    print(el)
    break
```

Outputs:
```
Tensor(""IteratorGetNext_4:0"", shape=(?, ?), dtype=string)
```"
31479,undeclared inclusion in rule inplace_ops_gpu,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0-rc0
- Python version: 3.7.3 x64
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.26.0 x64
- GCC/Compiler version (if compiling from source):  Visual Studio 2019
- CUDA/cuDNN version:  10.1/7.6.2
- GPU model and memory:
RTX2080Ti GDDR6 11GB


**Describe the problem**

windows build error in MSYS2 MinGW x64


**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt --config=v2 --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package



**Any other info / logs**
```
ERROR: D:/repo/tensorflow/tensorflow/core/kernels/BUILD:1252:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:inplace_ops_gpu':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/inplace_ops_functor_gpu.cu.cc':
  'C:/users/alan-workstation/appdata/local/temp/nvcc_inter_files_tmp_dir/inplace_ops_functor_gpu.cu.cudafe1.stub.c'
  'C:/users/alan-workstation/appdata/local/temp/nvcc_inter_files_tmp_dir/inplace_ops_functor_gpu.cu.fatbin.c'
```"
31478,"[C++] The first SessionOptions controls the ""inter intra number"" of all following Sessions?","
**Describe the current behavior**
    I trained a model. It runs near 25 milliseconds each inference when the `inter_op_parallelism_threads=1` and `intra_op_parallelism_threads=8` and near 90 milliseconds each inference when the `inter_op_parallelism_threads=1` and `intra_op_parallelism_threads=1`.
  I found that the first `SessionOptions` will determine all the following Sessions' Options.
  For example, I created a Session with `inter=1, intra=1` and close it. Then I create another Session with `inter=1, intra=8` and run my model in the later Session. The inference speed of the model is 90ms which corresponding to `inter=1, intra=1` but not `inter=1, intra=8`. It seems the first SessionOptions still control the later Session.
 
**Code to reproduce the issue**
    
    void DoSessionAction(boost::filesystem::path model_path, unsigned inter, unsigned intra){
      std::unique_ptr<tensorflow::Session> sess_;
      tensorflow::SessionOptions sess_options_;
      sess_options_.config.set_use_per_session_threads(false);
      sess_options_.config.set_inter_op_parallelism_threads(inter);
      sess_options_.config.set_intra_op_parallelism_threads(intra);
      sess_.reset(tensorflow::NewSession(sess_options_));
      tensorflow::GraphDef graph_def;
      auto default_env = tensorflow::Env::Default();
      tensorflow::ReadBinaryProto(default_env, model_path.string(), &graph_def);
      sess_->Create(graph_def);
      sess_->Close();
    }

    // This situation the inference time is 90ms.
    int main(int argc, char **argv) {
        DoSessionAction(model_path, 1, 1);
        DoSessionAction(model_path, 1, 8);
        RunModel(); 
    }

    // This situation the inference time is 25ms.
    int main(int argc, char **argv) {
        DoSessionAction(model_path, 1, 8);
        RunModel(); 
    }
**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from source
- TensorFlow version: 1.14.1
- Python version: 3.5.2
- Bazel version: 0.24
- GCC/Compiler version: 5.4.0

Is there anything wrong with my usage of Session or SessionOptions?
Thanks!

"
31477,Error building Custom Op with TF2.0 Nightly,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): TF 2.0 Nightly
- Python version: 3.6
- GCC/Compiler version (if compiling from source): VS2017

I have a number of custom ops which I am able to build / operate fine using TF v1.14. I have just tried to recompile them using 2.0 Nightly version of tensorflow and run into the following set of build errors relating to op_kernehl.h

1>d:\data\documents\github\tensorflow\tensorflow\core\framework\op_kernel.h(690): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int
1>d:\data\documents\github\tensorflow\tensorflow\core\framework\op_kernel.h(690): error C2143: syntax error: missing ';' before '*'
1>d:\data\documents\github\tensorflow\tensorflow\core\framework\op_kernel.h(690): error C2238: unexpected token(s) preceding ';'
1>d:\data\documents\github\tensorflow\tensorflow\core\framework\op_kernel.h(1134): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int
1>d:\data\documents\github\tensorflow\tensorflow\core\framework\op_kernel.h(1134): error C2143: syntax error: missing ';' before '*'
1>d:\data\documents\github\tensorflow\tensorflow\core\framework\op_kernel.h(1134): error C2334: unexpected token(s) preceding '{'; skipping apparent function body"
31476,Layer test does not set weights when testing with Sequential API,"**System information**
- Have I written custom code: No
- OS Platform and Distribution: macOS 10.15
- TensorFlow installed from: pip installed
- TensorFlow version: 1.14.0 (v1.14.0-rc1-22-gaf24dc91b5)
- Python version: 3.6

**Current Behaviour**
When using the `layer_test` util function in `keras.testing_utils` when testing the layer in the context of the Sequential API the weights are not set, and as a result the output of the layer will be different than expected. If expected_output is provided this will likely differ to the output from the layer and hence the test will fail.

I think that the weights on the layer should be set before calling the sequential model which can be solved with one extra line. This is done for all of the other tests already.

```python
model = keras.models.Sequential()
model.add(layer)
# add `layer.set_weights(weights)` here to fix problem
actual_output = model.predict(input_data)
...
if expected_output is not None:
  np.testing.assert_allclose(actual_output, expected_output, rtol=1e-3)
```
"
31475,provide custom library path,"I have installed TensorFlow using ""pip install tensorflow==1.13.1""
I am trying to run the TensorFlow with specific version 1.13.1
but it failed due to incompatible due to ""ImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found"". 

I have already installed the glibc on path /opt/glibc_2.16/

If, I gave custom path using ""export LD_LIBRARY_PATH=/opt/glibc_2.16/lib:$LD_LIBRARY_PATH""
Then ""Segmentation fault (core dumped)"" is shown.

How can I provide this path to tensorflow?
Please help.

(base) [rakesh@localhost rasax]$ ls /opt/glibc_2.16/
bin  etc  include  lib  libexec  sbin  share  var

For error reporting during run is given below:-

(base) [rakesh@localhost rasax]$ python -m tensorflow
Traceback (most recent call last):
  File ""/home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/rakesh/anaconda3/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/rakesh/anaconda3/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/rakesh/anaconda3/lib/python3.7/runpy.py"", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""/home/rakesh/anaconda3/lib/python3.7/runpy.py"", line 142, in _get_module_details
    return _get_module_details(pkg_main_name, error)
  File ""/home/rakesh/anaconda3/lib/python3.7/runpy.py"", line 109, in _get_module_details
    __import__(pkg_name)
  File ""/home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/rakesh/anaconda3/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/rakesh/anaconda3/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /home/rakesh/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

"
31474,"tflite unsupported ops:  Log1p, SparseReorder, SegmentSum","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
tflite unsupported ops:  Log1p, SparseReorder, SegmentSum
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, EXPAND_DIMS, FILL, FULLY_CONNECTED, GATHER, LESS, LOGISTIC, MUL, RESHAPE, SELECT, SHAPE, SOFTMAX, STRIDED_SLICE, SUM, TILE, UNIQUE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Log1p, SparseReorder.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31473,TF 2.0 Beta MobileNetV2 model.fit() and model.evaluate() works unexpectedly ,"Dear Expert,
I am using TF 2.0-beta to have some tests based on the tf.keras API, and I think the result of MobileNetV2 model.fit(evaluate) is not reasonable. I doubt it is a bug, and I currently only found it in MobileNetV2 model. Could you please help to have a look? The details follows below.

Thanks in advance.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04

- TensorFlow installed from (source or binary): pip

- TensorFlow version (use command below): 2.0.0-beta

- Python version: 3.5.2

- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla P4

**Describe the current behavior**
Training a MobileNetV2 model , I see the loss is descending and becomes very small after a few epochs while the val_loss is not.  During my checking progress, I intentionally use the training data to evaluate the model, and i can see the evaluation result is quite different from the training progress shows.

The program output is attached as 1.log.
[1.log](https://github.com/tensorflow/tensorflow/files/3486313/1.log)



**Describe the expected behavior**
I do not believe such a huge gap comes from BN/Dropout or the batch difference, I doubt there is a bug in computing the loss in keras.model.fit()? 
Besides, I print the prediction result array. The value changes from different inputs are very small, it seems to me this is under-fitting? However, from the loss curve I cannot deduce it..(More like a over-fitting case to me..). Could you please help to figure it out?

**Code to reproduce the issue**

    # !!! I loaded the model as below:
    model = keras.applications.mobilenet_v2.MobileNetV2(input_shape=(299, 299, 3), include_top=True, weights=None, classes=CATEGORY_SIZE)
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    model.summary()

    print('###Trainin begin...###')
    # !!! I loaded the model as below:
    model = keras.applications.mobilenet_v2.MobileNetV2(input_shape=(299, 299, 3), include_top=True, weights=None, classes=CATEGORY_SIZE)
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    model.summary()
    model.fit(train_generator,
              epochs=200,
              callbacks=[],
              shuffle=True,
              validation_data = test_generator)

    # !!! I intentionally use the training data to evaluate the model, 
    # !!! expect that the loss and val should be almost equal to the value in fit() progess?
    print('###Evaluate with training data###')
    model.evaluate(train_generator)
    print('###Evaluate with testing data###')
    model.evaluate(test_generator)

    # !!! Use an array instead of a generator and print the prediction array see what happens
    # !!! All the prediction arrays gets the similar value from different inputs. Is this under-fitting?
    print('###Predict images###')
    i = 0
    test_images = np.empty((0, 299, 299, 3))
    test_labels = np.empty(0)
    for x, y in test_generator:
        i = i + 1
        if i > len(test_generator):
            break
        test_images = np.concatenate((test_images, x), axis=0)
        test_labels = np.concatenate((test_labels, y), axis=0)

    error_count = 0
    predictions = model.predict(test_images)
    for i in range(test_images.shape[0]):
        predicted_label = np.argmax(predictions[i])
        if not predicted_label == test_labels[i]:
            error_count = error_count + 1
            print(""Label for image %d: %d"" % (i, test_labels[i]))
            print(""Predict label for image %d: %d"" % (i, predicted_label))
            print(np.around(predictions[i], decimals=3))
        print(""Total error count is %d, total predict count is %d"" % (error_count, test_images.shape[0]))
"
31472,ERROR: missing input file '//tensorflow/tools/pip_package:build_pip_package.sh',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0

**Describe the problem**
I am trying to build tensorflow from source in an intel python environment, with mkl support.

./configure

You have bazel 0.26.1 installed.
Please specify the location of python. [Default is /home/vallari/anaconda3/bin/python]: /home/vallari/anaconda3/envs/inteldp/bin/python


Found possible Python library paths:
  /home/vallari/anaconda3/envs/inteldp/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/vallari/anaconda3/envs/inteldp/lib/python3.6/site-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: y
Clang will be downloaded and used to compile tensorflow.

Do you wish to build TensorFlow with MPI support? [y/N]: y
MPI support will be enabled for TensorFlow.

Please specify the MPI toolkit folder. [Default is /usr/local]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

COMMAND:-
bazel build --config=mkl -c opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --verbose_failures //tensorflow/tools/pip_package:build_pip_package
 I get this log

While running this command

Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=127
INFO: Reading rc options for 'build' from /home/vallari/tensorflow-master/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include
INFO: Reading rc options for 'build' from /home/vallari/tensorflow-master/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/vallari/anaconda3/envs/inteldp/bin/python --action_env PYTHON_LIB_PATH=/home/vallari/anaconda3/envs/inteldp/lib/python3.6/site-packages --python_path=/home/vallari/anaconda3/envs/inteldp/bin/python --config=xla --config=download_clang --config=None --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:xla in file /home/vallari/tensorflow-master/.tf_configure.bazelrc: --define with_xla_support=true
INFO: Found applicable config definition build:download_clang in file /home/vallari/tensorflow-master/.bazelrc: --crosstool_top=@local_config_download_clang//:toolchain --define=using_clang=true --action_env TF_DOWNLOAD_CLANG=1
INFO: Found applicable config definition build:None in file /home/vallari/tensorflow-master/.tf_configure.bazelrc: --define with_mpi_support=true
INFO: Found applicable config definition build:mkl in file /home/vallari/tensorflow-master/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c opt
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:657:12: in srcs attribute of cc_library rule //tensorflow/core:lib_proto_parsing: please do not import '//tensorflow/core/platform:protobuf.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:1127:12: in srcs attribute of cc_library rule //tensorflow/core:framework_lite: please do not import '//tensorflow/core/platform:default/integral_types.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:1127:12: in srcs attribute of cc_library rule //tensorflow/core:framework_lite: please do not import '//tensorflow/core/platform:default/mutex.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:1127:12: in srcs attribute of cc_library rule //tensorflow/core:framework_lite: please do not import '//tensorflow/core/platform:default/mutex_data.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:1125:1: in linkstatic attribute of cc_library rule //tensorflow/core:framework_lite: setting 'linkstatic=1' is recommended if there are no object files
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:abi.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:annotation.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:byte_order.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:context.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:cord.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:cpu_feature_guard.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:cpu_info.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:cuda_libdevice_path.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:demangle.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:denormal.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:dynamic_annotations.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:env.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:env_time.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:458:12: in srcs attribute of cc_library rule //tensorflow/core:human_readable_json: please do not import '//tensorflow/core/platform:default/human_readable_json.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:error.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:file_statistics.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:file_system.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:file_system_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:fingerprint.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:grpc_services.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:host_info.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:human_readable_json.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:init_main.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:470:12: in srcs attribute of cc_library rule //tensorflow/core:logger: please do not import '//tensorflow/core/platform:logger.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:load_library.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:logger.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:logging.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:macros.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:mem.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:monitoring.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:mutex.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:net.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:notification.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:null_file_system.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:numa.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:platform.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:platform_strings.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:platform_strings_computed.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:prefetch.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:profile_utils/android_armv7a_cpu_utils_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:profile_utils/clock_cycle_profiler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:profile_utils/cpu_utils.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:profile_utils/i_cpu_utils_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:protobuf.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:protobuf_compiler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:protobuf_internal.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:regexp.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:setround.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:snappy.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:stacktrace.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:stacktrace_handler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:stream_executor_no_cuda.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:strong_hash.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:subprocess.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:tensor_coding.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:test.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:test_benchmark.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:thread_annotations.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:tracing.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:tstring.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:types.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2440:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:unbounded_work_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2438:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2974:1: in srcs attribute of cc_library rule //tensorflow/core:stream_executor: please do not import '//tensorflow/core/platform:stream_executor.h' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation in /home/vallari/tensorflow-master/tensorflow/core/BUILD:2974:1
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2991:12: in srcs attribute of cc_library rule //tensorflow/core:stream_executor_no_cuda: please do not import '//tensorflow/core/platform:stream_executor.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2534:12: in srcs attribute of cc_library rule //tensorflow/core:jpeg_internal: please do not import '//tensorflow/core/platform:jpeg.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2514:12: in srcs attribute of cc_library rule //tensorflow/core:gif_internal: please do not import '//tensorflow/core/platform:gif.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:559:12: in srcs attribute of cc_library rule //tensorflow/core:platform_strings: please do not import '//tensorflow/core/platform:platform_strings.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:559:12: in srcs attribute of cc_library rule //tensorflow/core:platform_strings: please do not import '//tensorflow/core/platform:platform_strings_computed.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:abi.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:annotation.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:byte_order.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:context.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:cord.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:cpu_feature_guard.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:cpu_info.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:cuda_libdevice_path.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:demangle.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:denormal.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:dynamic_annotations.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:env.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:env_time.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:error.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:file_statistics.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:file_system.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:file_system_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:fingerprint.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:grpc_services.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:host_info.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:human_readable_json.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:init_main.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:load_library.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:logger.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:logging.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:macros.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:mem.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:monitoring.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:mutex.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:net.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:notification.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:null_file_system.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:numa.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:platform.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:platform_strings.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:platform_strings_computed.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:prefetch.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/android_armv7a_cpu_utils_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/clock_cycle_profiler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/cpu_utils.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/i_cpu_utils_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:protobuf.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:protobuf_compiler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:protobuf_internal.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:regexp.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:setround.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:snappy.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:stacktrace.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:stacktrace_handler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:stream_executor_no_cuda.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:strong_hash.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:subprocess.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:tensor_coding.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:test.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:test_benchmark.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:thread_annotations.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:tracing.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:tstring.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:types.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:unbounded_work_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:default/monitoring.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:default/mutex.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:default/tracing.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:default/unbounded_work_queue.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:posix/env.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:posix/error.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:posix/load_library.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:posix/net.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:posix/port.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:posix/posix_file_system.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:posix/subprocess.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:cpu_feature_guard.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:denormal.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:env.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:file_system.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:file_system_helper.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/android_armv7a_cpu_utils_helper.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/clock_cycle_profiler.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/cpu_utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:protobuf_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:setround.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:stacktrace_handler.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:tensor_coding.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/core/BUILD:2464:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:tracing.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/vallari/tensorflow-master/tensorflow/python/BUILD:3692:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/vallari/tensorflow-master/tensorflow/contrib/metrics/BUILD:17:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/vallari/tensorflow-master/tensorflow/python/BUILD:86:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/vallari/tensorflow-master/tensorflow/contrib/learn/BUILD:16:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/vallari/tensorflow-master/tensorflow/contrib/learn/BUILD:16:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/vallari/tensorflow-master/tensorflow/contrib/bayesflow/BUILD:18:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/vallari/tensorflow-master/tensorflow/contrib/BUILD:12:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (421 packages loaded, 29682 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base /home/-/.cache/bazel/_bazel_vallari/23d93ff557e4e0ca9cc172196f68414b/sandbox
ERROR: missing input file '//tensorflow/tools/pip_package:build_pip_package.sh'
ERROR: /home/vallari/ensorflow-master/tensorflow/tools/pip_package/BUILD:265:1: //tensorflow/tools/pip_package:build_pip_package: missing input file 'LabelCause{label=//tensorflow/tools/pip_package:build_pip_package.sh, msg=missing input file '//tensorflow/tools/pip_package:build_pip_package.sh'}'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/vallari/tensorflow-master/tensorflow/tools/pip_package/BUILD:265:1 1 input file(s) do not exist
INFO: Elapsed time: 8.128s, Critical Path: 0.05s
INFO: 0 processes.
FAILED: Build did NOT complete successfully






The file exists there.
I have used bazel clean, same error comes.
Refers to https://github.com/tensorflow/tensorflow/issues/23031 , but a file is missing.

Suggest!"
31471,Why did system restart improve model behaviour?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13.1
- Python version: 3.7
- CUDA/cuDNN version: 10
- GPU model and memory: 105Ti(mobile) 4GB

**Describe the current behavior**
I was trying to train custom models and pretrained models on a custom dataset but no models were learning anything. The accuracy after many epochs was always stuck at 55-60% accuracy. 
After many days of frustration, I shut down the system. The next day the system booted and the models were learning and converging near 100%. How and why did this behavior occur? 
"
31470,java.lang.RuntimeException: Failed to find input Node 'image_tensor',"Hi,

I have downloaded the sample and tried run the **Android** Example.
It is working fine.

Now I have my own .tflite model and .txt label file I have put them in assets and replaced
**TF_OD_API_LABELS_FILE** ,**TF_OD_API_MODEL_FILE** to my file path 


I have ended up with the error
`java.lang.RuntimeException: Failed to find input Node 'image_tensor'`

I am running this sample on my **samsung A50.**

I am not much aware about any detail but the simple steps as above and the error. Can you guys please take deep dive into this and get me out of it?


"
31468,TFLite Metal GPU delegate: crash in Mul operation,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 12.3.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone Xr
- TensorFlow installed from (source or binary): binary, but TFLite compiled from source
- TensorFlow version (use command below): b'v1.13.2-5-g04256c89d8' 1.13.2
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): clang-1001.0.46.4
- CUDA/cuDNN version: n/a
- GPU model and memory: iPhone Xr

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Running the single-operation model given below using the Metal GPU delegate causes the following error:
> Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (IOAF code 3)

**Describe the expected behavior**
Running the model completes successfully.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
[model-broken.tflite.zip](https://github.com/tensorflow/tensorflow/files/3484839/model-broken.tflite.zip)

This basic model fails to run on GPU (with the error specified above), but will run fine on CPU.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

TFLite interpreter is being run using provided sample code and the crash only occurs on GPUthe model runs correctly on CPU."
31467,tfdv.validate_tfexamples_in_tfrecord can't be found in TensorFlow Data Validation,"The tutorial say using tfdv.validate_tfexamples_in_tfrecord to check for errors on a per-example basis. But I can't import it, and also can't find source in code.

Please check this function .
## URL(s) with the issue:https://www.tensorflow.org/tfx/data_validation/get_started

"
31464,Can't get MnasNet mean & stdev for input normalization: get correct classes but incorrect scores.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** Stock example `label_image.py`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary):** Tensorflow Lite installed from source.
- **TensorFlow version (use command below):** v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- **Python version:** 3.6.7
- **Bazel version (if compiling from source):** using Make, not bazel
- **GCC/Compiler version (if compiling from source):** gcc
- **CUDA/cuDNN version:** no CUDA
- **GPU model and memory:** no GPU


### Describe the current behavior
The docs are quite unclear about which normalization technique and constants are used for inputs. But generally speaking, popular models like `mobilenet` use an input range of `[0,1]`.
This is the case for most models hosted [here](https://www.tensorflow.org/lite/guide/hosted_models), except `MnasNet`. For some reasons, I cannot properly use inference with the `MnasNet_1.0_224` `.tflite` model using input ranges of `[0,1]`. For instance, after normalizing my input from 0 to 1, I do get the correct output class but get an incorrect output score. In fact, **normalizing RGB channels from `[0,255]` to `[0,1]` produces scores above 100% (sometimes 900%...) using TFLite's `MnasNet_1.0_224`**.

Before writing this issue, I have randomly tested other input ranges like `[-1,1]` and `[-127,128]` but with no luck either. In PyTorch docs there are some references of using a normalization per channels like `[0.485*255, 0.456*255, 0.456*255]` for the mean and `[0.229*255, 0.224*255, 0.225*255]` for the standard deviation (for each R,G and B channel respectively), but once again to no avail: **correct class with unscaled scores**.

### Describe the expected behavior

More than an expected behavior I would expect some documentation around models. Recently I could see a new convention to ship models with a text file informing input and output channels names which I think is great. However, IMHO it should also ship with `mean` and `stdev` used for input normalization. Moreover, and here I am deviating from the original issue briefly, I would love to see these information **coded** in some constant variables like `input.get_normalization_range()` or something like that (just my 2cts).

Now regarding that specific issue, is anyone able to tell if that is an MnasNet bug not respecting classic ranges, or a deliberate training-related optimization ?
And, of course, I'd love to see a code or pseudo-code to get my input right using MnasNet :smile: 


### Code to reproduce the issue

Just download the `MnasNet_1.0_224.tflite` model here: [mnasnet_1.0_224_09_07_2018.tgz](https://storage.cloud.google.com/download.tensorflow.org/models/tflite/mnasnet_1.0_224_09_07_2018.tgz)
Extract it.
Then, copy the stock python script `label_image.py` here: [label_image.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/python/label_image.py)
Then get a random image to run inference.

Finally just run inference but providing mnasnet model instead of default one:
```
python3 label_image.py --model_file mnasnet_1.0_224/mnasnet_1.0_224.tflite
```
"
31458,tf.metrics reset_state() not called when executing in graph mode,"`tf.keras.metrics.*` are reset using the method `.reset_states()`, however when executed in graph mode the method isn't called and the metric isn't reset.

Example to reproduce:

```
import tensorflow as tf

print(tf.__version__)

mean = tf.metrics.Mean()

def eager_working():
    
    mean(1.)
    mean.reset_states()
    mean(2.)
    
    tf.print(mean.result())

@tf.function
def graph_not_working():
    
    mean(1.)
    mean.reset_states()
    mean(2.)
    
    tf.print(mean.result())

@tf.function
def graph_working():
    mean(1.)
    mean.total.assign(0)
    mean.count.assign(0)
    mean(2.)
    
    tf.print(mean.result())


eager_working()
mean.reset_states()
graph_not_working()
mean.reset_states()
graph_working()
```

Output:
```
2.0.0-beta1
2
1.5
2
```"
31455,python for loops in eager model yield expected results for  keras model.predict but not for saved model with multiple outputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): NA
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): na
- GCC/Compiler version (if compiling from source): na
- CUDA/cuDNN version: na
- GPU model and memory: na

**Describe the current behavior**
tf.keras Model's predict method prediction differs from tf.saved_model's prediction
**Describe the expected behavior**
`tf.saved_model.save()` should either fail to serialize, or should yield correct predictions on reload.

**Code to reproduce the issue**
```python
import tensorflow as tf
tf.enable_eager_execution()
print(tf.__version__)
from tensorflow.keras.layers import Input, Dense, Lambda
from tensorflow.keras.models import Model
import numpy as np

in0 = Input(shape=(1,), dtype=""float32"", name='my_input_0')
in1 = Input(shape=(1,), dtype=""float32"", name='my_input_1')
concatted = Lambda(lambda inputs: tf.concat(inputs, axis=-1))([in0,in1])

outputs = Dense(3)(concatted)

#------------- way 1 (does not work)-----------------

# outs = [Lambda(lambda outputs: outputs[...,i], name=f'output_{i}')(outputs) for i in range(3)]

#------------- way 2 (does not work)-----------------

outs = []
for i in range(3):
  outs.append(Lambda(lambda outputs: outputs[...,i], name=f'output_{i}')(outputs))

#------------- way 3 (does work)-----------------

# out0 = Lambda(lambda outputs: outputs[...,0], name='my_output_0')(post_process)
# out1 = Lambda(lambda outputs: outputs[...,1], name='my_output_1')(post_process)
# out2 = Lambda(lambda outputs: outputs[...,2], name='my_output_2')(post_process)
# outs=[out0, out1, out2]

#----------------------------

my_model = Model(inputs=[in0,in1], outputs=outs)
tf.keras.backend.learning_phase = 0

my_model.predict([np.array([[.5],[.3]]), np.array([[-.1],[.2]])])
```

yields
```
1.14.0
[array([0.13002533, 0.03461001], dtype=float32),
 array([-0.45988005, -0.3892737 ], dtype=float32),
 array([-0.28218567, -0.14224575], dtype=float32)]
```
On the other hand,
```python
tf.saved_model.save(my_model, './mymodel')
reloaded = tf.saved_model.load_v2('./mymodel')
sig = reloaded.signatures['serving_default']
sig(my_input_0=tf.constant(np.array([[.5],[.3]]), dtype=tf.float32), my_input_1=tf.constant(np.array([[-.1],[.2]]), dtype=tf.float32))
```
yields
```
{'output_0': <tf.Tensor: id=316, shape=(2,), dtype=float32, numpy=array([-0.28218567, -0.14224575], dtype=float32)>,
 'output_1': <tf.Tensor: id=317, shape=(2,), dtype=float32, numpy=array([-0.28218567, -0.14224575], dtype=float32)>,
 'output_2': <tf.Tensor: id=318, shape=(2,), dtype=float32, numpy=array([-0.28218567, -0.14224575], dtype=float32)>}
```
try it for yourself here:
https://colab.research.google.com/drive/1x1eDTl-nMuhZTQYWSi_I9F_SguFZRP2i


Provide a reproducible test case that is the bare minimum necessary to generate the problem.




**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31451,"Error when selecting 1 GPU + TensorBoard: ""(...) XLA_GPU_JIT device number 0""","On TensorFlow 1.14 (OS Ubuntu 16.04), when I call fit() of a tf.Keras model using TensorBoard as one of the tf.Keras.callbacks, and selecting one GPU to use prior training, it fails with:

```
(...)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py 
in ensure_initialized(self)
    410         if self._execution_mode == ASYNC:
    411           pywrap_tensorflow.TFE_ContextOptionsSetAsync(opts, True)
--> 412         self._context_handle = pywrap_tensorflow.TFE_NewContext(opts)
    413       finally:
    414         pywrap_tensorflow.TFE_DeleteContextOptions(opts)

InvalidArgumentError: Invalid device ordinal value (1). Valid range is [0, 0].
	while setting up XLA_GPU_JIT device number 1
```

If I suppress the callbacks argument in fit(), the training works.  The code to reproduce follows:

```
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import TensorBoard

gpu_id = 0
sess_config = tf.compat.v1.ConfigProto()
sess_config.gpu_options.allow_growth = True
sess_config.gpu_options.visible_device_list = '{}'.format(gpu_id)
sess = tf.compat.v1.Session(config=sess_config)
tf.compat.v1.keras.backend.set_session(sess)

(Xtr, Ytr), (Xva, Yva) = tf.keras.datasets.cifar10.load_data()
Xtr, Ytr, Xva, Yva, nc = Xtr[:1000], Ytr[:1000], Xva[:100], Yva[:100], 10
Xtr, Xva = Xtr.astype('float32') / 255, Xva.astype('float32') / 255
Ytr, Yva, ins = to_categorical(Ytr, nc), to_categorical(Yva, nc), Xtr.shape[1:]

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(8, (3, 3), input_shape=ins, activation='relu'))
model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))
model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(nc, activation='softmax'))
opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

l_cb = [TensorBoard(log_dir='./tb_logs/cur', batch_size=32, write_graph=False)]

model.fit(x=Xtr, y=Ytr, batch_size=32, epochs=100, callbacks=l_cb,
          validation_data=(Xva, Yva), shuffle='batch')
```"
31449,GPU race conditions from `tf.map_fn` ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): All the code that caused this issue uses Tensorflow/Keras operations.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0/7.6.2
- GPU model and memory: RTX 6000x2, 48 GB

**Describe the current behavior**

I've created a custom layer called `ROI` in Keras that uses `tf.map_fn`, [precisely because it has unknown parameter that it needs to take as tensor object](https://github.com/keras-team/keras/issues/12139). 

This layer works perfectly on CPU inference and training, it also works perfectly on GPU during inference. But during training with a powerful GPU, exception about GPU colocation of `ROI` layer occurs:

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation ROI/map/while/Identity_1: Could not satisfy explicit device specification '' because the node node ROI/map/while/Identity_1 (defined at /path/to/custom/layer/custom.py:70) placed on device No device assignments were active during op 'ROI/map/while/Identity_1' creation.
```

```
[[node ROI/map/while/Identity_1 (defined at /path/to/custom/layer/custom.py:70) Additional information about colocations: No node-device colocations were active during                                                                                                                                                              op 'ROI/map/while/Identity_1' creation.
No device assignments were active during op 'ROI/map/while/Identity_1' creation.`
```

Manual colocation of `ROI` layer to CPU device with `tf.device` worked, but I want `ROI` to support GPU as well.

**My hypothesis**

`ROI` layer works on CPU because only single core at a time should handle the layer - even if multiprocessing is activated, there are few cores slowly balancing the task. 

But whenever GPU is utilized, thousands of cores are working together in parallel and they are not waiting for each other to finish their tasks. Thus one of the processes tries to gather data from `TensorArray` that is still in while loop (using `tf.map_fn`), which causes the error.

**Describe the expected behavior**

Tensorflow should be able to handle these race conditions by waiting for its own `tf.map_fn` to be finished instead of raising exceptions. 

**Code to reproduce the issue**

[This is the code
](https://gist.github.com/ShellRox/a1c11565ed53f219bc1033164947aa72) that instantly causes the mentioned issue on my local machine.

**Other info / logs**

Full Log:

```
Traceback (most recent call last):
  File ""/path/to/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/path/to/site-packages/tensorflow/python/client/session.py"", line 1339, in _run_fn
    self._extend_graph()
  File ""/path/to/site-packages/tensorflow/python/client/session.py"", line 1374, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation ROI/map/while/Identity_1: Could not satisfy explicit device specification '' because the node {{colocation_node ROI/map/while/Identity_1}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:GPU:0].
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
StridedSliceGrad: GPU CPU XLA_CPU XLA_GPU
NextIteration: GPU CPU XLA_CPU XLA_GPU
Mul: GPU CPU XLA_CPU XLA_GPU
Equal: GPU CPU XLA_CPU XLA_GPU
DynamicStitch: GPU CPU XLA_CPU XLA_GPU
Fill: GPU CPU XLA_CPU XLA_GPU
FloorMod: GPU CPU XLA_CPU XLA_GPU
Shape: GPU CPU XLA_CPU XLA_GPU
Reshape: GPU CPU XLA_CPU XLA_GPU
TensorArrayReadV3: GPU CPU XLA_CPU XLA_GPU
TensorArrayScatterV3: GPU CPU XLA_CPU XLA_GPU
TensorArraySizeV3: GPU CPU XLA_CPU XLA_GPU
Const: GPU CPU XLA_CPU XLA_GPU
TensorArrayWriteV3: GPU CPU XLA_CPU XLA_GPU
Identity: GPU CPU XLA_CPU XLA_GPU
GreaterEqual: GPU CPU XLA_CPU XLA_GPU
Exit: GPU CPU XLA_CPU XLA_GPU
Cast: GPU CPU XLA_CPU XLA_GPU
ControlTrigger: GPU CPU XLA_CPU XLA_GPU
TensorArrayGradV3: GPU CPU XLA_CPU XLA_GPU
Pack: GPU CPU XLA_CPU XLA_GPU
Enter: GPU CPU XLA_CPU XLA_GPU
TensorArrayV3: GPU CPU XLA_CPU XLA_GPU
Merge: GPU CPU XLA_CPU XLA_GPU
StackV2: GPU CPU XLA_CPU XLA_GPU
Range: GPU CPU XLA_CPU XLA_GPU
TensorArrayGatherV3: GPU CPU XLA_CPU XLA_GPU
StackPushV2: GPU CPU XLA_CPU XLA_GPU
Switch: GPU CPU XLA_CPU XLA_GPU
RealDiv: GPU CPU XLA_CPU XLA_GPU
Add: GPU CPU XLA_CPU XLA_GPU
StridedSlice: GPU CPU XLA_CPU XLA_GPU
Max: GPU CPU XLA_CPU XLA_GPU
LoopCond: GPU CPU XLA_CPU XLA_GPU
Sum: GPU CPU XLA_CPU XLA_GPU
StackPopV2: GPU CPU XLA_CPU XLA_GPU
Sub: GPU CPU XLA_CPU XLA_GPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  ROI/map/TensorArray_2 (TensorArrayV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  ROI/map/while/Identity_1 (Identity)
  ROI/map/while/map/TensorArray_1 (TensorArrayV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  ROI/map/while/map/while/Identity_1 (Identity)
  ROI/map/while/map/while/strided_slice_4/stack (Pack)
  ROI/map/while/map/while/strided_slice_4/stack_1 (Pack)
  ROI/map/while/map/while/strided_slice_4 (StridedSlice)
  ROI/map/while/map/while/Max (Max)
  ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3/Enter (Enter)
  ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3 (TensorArrayWriteV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  ROI/map/while/map/while/Exit_2 (Exit)
  ROI/map/while/map/TensorArrayStack/TensorArraySizeV3 (TensorArraySizeV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  ROI/map/while/map/TensorArrayStack/range/start (Const)
  ROI/map/while/map/TensorArrayStack/range/delta (Const)
  ROI/map/while/map/TensorArrayStack/range (Range)
  ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3 (TensorArrayGatherV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  ROI/map/while/TensorArrayWrite/TensorArrayWriteV3/Enter (Enter)
  ROI/map/while/TensorArrayWrite/TensorArrayWriteV3 (TensorArrayWriteV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  ROI/map/TensorArrayStack/TensorArraySizeV3 (TensorArraySizeV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  ROI/map/TensorArrayStack/range/start (Const)
  ROI/map/TensorArrayStack/range/delta (Const)
  ROI/map/TensorArrayStack/range (Range)
  ROI/map/TensorArrayStack/TensorArrayGatherV3 (TensorArrayGatherV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  training/MultiplierWrapper/gradients/f_count_3 (Const)
  training/MultiplierWrapper/gradients/f_count_4 (Enter)
  training/MultiplierWrapper/gradients/Merge_2 (Merge)
  training/MultiplierWrapper/gradients/Switch_2 (Switch)
  training/MultiplierWrapper/gradients/Add_1/y (Const)
  training/MultiplierWrapper/gradients/Add_1 (Add)
  training/MultiplierWrapper/gradients/f_count_5 (Exit)
  training/MultiplierWrapper/gradients/Const (Const)
  training/MultiplierWrapper/gradients/f_acc (StackV2)
  training/MultiplierWrapper/gradients/Enter (Enter)
  training/MultiplierWrapper/gradients/StackPushV2 (StackPushV2)
  training/MultiplierWrapper/gradients/StackPopV2/Enter (Enter)
  training/MultiplierWrapper/gradients/StackPopV2 (StackPopV2)
  training/MultiplierWrapper/gradients/b_count_4 (Const)
  training/MultiplierWrapper/gradients/b_count_5 (Enter)
  training/MultiplierWrapper/gradients/Merge_3 (Merge)
  training/MultiplierWrapper/gradients/GreaterEqual_1/Enter (Enter)
  training/MultiplierWrapper/gradients/GreaterEqual_1 (GreaterEqual)
  training/MultiplierWrapper/gradients/b_count_6 (LoopCond)
  training/MultiplierWrapper/gradients/Switch_3 (Switch)
  training/MultiplierWrapper/gradients/Sub_1 (Sub)
  training/MultiplierWrapper/gradients/b_count_7 (Exit)
  training/MultiplierWrapper/gradients/ROI/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 (TensorArrayGradV3)
  training/MultiplierWrapper/gradients/ROI/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/gradient_flow (Identity)
  training/MultiplierWrapper/gradients/ROI/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3 (TensorArrayScatterV3)
  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3 (TensorArrayGradV3)
  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/gradient_flow (Identity)
  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Const (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPushV2 (StackPushV2)
  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2 (StackPopV2)
  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3 (TensorArrayReadV3)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/Const (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/f_acc (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPushV2 (StackPushV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2 (StackPopV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/Const_1 (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/f_acc_1 (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPushV2_1 (StackPushV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2_1/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2_1 (StackPopV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 (TensorArrayGradV3)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/gradient_flow (Identity)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/Const (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/f_acc (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/StackPushV2 (StackPushV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/StackPopV2/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/StackPopV2 (StackPopV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3 (TensorArrayScatterV3)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Exit_2_grad/b_exit (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/Const (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/f_acc (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPushV2 (StackPushV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2 (StackPopV2)
  training/MultiplierWrapper/gradients/b_sync (ControlTrigger)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3 (TensorArrayGradV3)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/gradient_flow (Identity)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Const (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPushV2 (StackPushV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2 (StackPopV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3 (TensorArrayReadV3)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Shape (Shape)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Size (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/add/Const (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/add (Add)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/mod (FloorMod)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Shape_1 (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/range/start (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/range/delta (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/range (Range)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Fill/value (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Fill (Fill)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/Const (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/f_acc (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/StackPushV2 (StackPushV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/StackPopV2/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/StackPopV2/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/StackPopV2 (StackPopV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch (DynamicStitch)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/Const (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/f_acc (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/StackPushV2 (StackPushV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/StackPopV2/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/StackPopV2/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/StackPopV2 (StackPopV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape (Reshape)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape_1 (Reshape)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/Const (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/f_acc (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/StackPushV2 (StackPushV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/StackPopV2/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/StackPopV2/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/StackPopV2 (StackPopV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal (Equal)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Cast (Cast)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Sum (Sum)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape_2 (Reshape)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/truediv (RealDiv)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/mul (Mul)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/Shape (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Const (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/f_acc (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPushV2 (StackPushV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2 (StackPopV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Const_1 (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/f_acc_1 (StackV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Enter_2 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Enter_3 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPushV2_1 (StackPushV2)
  training/MultiplierWrapper/gradients/NextIteration_2 (NextIteration)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2_1/Enter (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2_1/Enter_1 (Enter)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2_1 (StackPopV2)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/b_sync (ControlTrigger)
  training/MultiplierWrapper/gradients/NextIteration_3 (NextIteration)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Const_2 (Const)
  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad (StridedSliceGrad)

         [[{{node ROI/map/while/Identity_1}}]]
```"
31447,Adding speech output for tensorflowlite apps,"Hello, I am currently building a image recognizer using tensorflow lite. I also wanted to add speech output with the prediction of the model. For example, if the model predicts that a given image is a daisy flower, it will say out loud ""daisy"".

**System information**
- TensorFlow version (you are using): Tensorflow lite 2.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?** The api would be modified such that text to speech is involved.

**Who will benefit with this feature?** Adding speech would greatly help people with visual impairments, and benefit other app users in general.

**Any Other info.**
"
31446,Init operation is not added automatically to collections with tf.GraphKeys.INIT_OP,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda/pip
- TensorFlow version (use command below): 1.14
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
In:
```python
graph = tf.Graph()
with graph.as_default():

  with tf.variable_scope('signal_in'):
    signal_in = tf.placeholder(tf.float32, shape=(10,40,2,1))

  with tf.variable_scope('dascope1'):
    conv_linear = tf.keras.layers.Conv2D( 8, (8,2), padding='valid', name='conv_linear', use_bias=True, kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137) )(signal_in)
  
  with tf.variable_scope('softmax'):
    logits = tf.contrib.layers.fully_connected(conv_linear, 2, activation_fn=None, normalizer_fn=None, normalizer_params=None, weights_initializer=tf.initializers.lecun_normal(seed=731), weights_regularizer=None, biases_initializer=tf.initializers.lecun_normal(seed=777), biases_regularizer=None, reuse=None, variables_collections=None, outputs_collections=None, trainable=True, scope='logit')
    softmax = tf.nn.softmax(logits,axis=0)            
    
  with tf.variable_scope('loss'):
    l_vec = tf.placeholder(tf.float32, shape=(10,2))
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0)(l_vec, softmax)         
    minimize_op = tf.train.AdamOptimizer(learning_rate=0.05).minimize(loss)
    tf.global_variables_initializer()

print(graph.get_collection_ref(tf.GraphKeys.INIT_OP))
``` 
returns [] to stdout
**Describe the expected behavior**
it must return the value of the collection keyed by tf.GraphKeys.INIT_OP, in this case it should be something like loss/init  type NoOp

**Code to reproduce the issue**
Given above.

**Other info / logs**
this must be a easy to circumvent bug, but for coherence, i think must be corrected.
"
31445,bazel fully recompiles the codebase even though no files were changed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n/a
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
1.14
- Python version:
3.5
- Installed using virtualenv? pip? conda?:
virtualenv
- Bazel version (if compiling from source):
Build label: 0.26.1
- GCC/Compiler version (if compiling from source):
gcc version 5.4.0 20160609
- CUDA/cuDNN version:
n/a
- GPU model and memory:
n/a



**Describe the problem**
The problem is that bazel rebuilds the whole codebase during the sequential bazel runs even though literally no files changed. I am building two targets:
`bazel run //tensorflow/lite/python:tflite_convert`
and
`bazel build //tensorflow/tools/pip_package:build_pip_package --host_force_python=PY3 PYTHON_BIN_PATH=/home/kk/tmp/tfclean/bin/python`

The thing is that if say I take the first command and run it 3 times, then the full compilation will only be performed for the first time, two remaining runs will result in no compilation steps. The same for the second command. 

However, if I run the first, then the second command, the first again - the full recompilation will happen again, even though no files were changed. 

Why is it so? No files were changed, while does bazel recompile the whole tensorflow codebase for the second time? 

**Provide the exact sequence of commands/steps that you executed before running into the problem**

These three commands will results in the full tensorflow codebase recompilation, even though no source files were changed. 

`bazel build //tensorflow/tools/pip_package:build_pip_package --host_force_python=PY3 PYTHON_BIN_PATH=/home/kk/tmp/tfclean/bin/python`
`bazel run //tensorflow/lite/python:tflite_convert`
`bazel build //tensorflow/tools/pip_package:build_pip_package --host_force_python=PY3 PYTHON_BIN_PATH=/home/kk/tmp/tfclean/bin/python`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31444,tensorflow c++ header missing ubuntu,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 
- Python version:3.6
- Installed using virtualenv? pip? conda?:PIP
- Bazel version (if compiling from source):0.26.

**Describe the problem**
I am using tensor flow for c++ project, but i get some problem with install, No such file or directory #include ""tensorflow/core/lib/framework/graph.pb.h"".
**Provide the exact sequence of commands / steps that you executed before running into the problem**
 g++ some_file.cpp   -ltensorflow -o hello_tf

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
fatal error: tensorflow/core/lib/framework/graph.pb.h: No such file or directory #include ""tensorflow/core/lib/framework/graph.pb.h"""
31442,python API debugging best practices,"Hi guys,

I work for ARM Ltd and we are looking into adding int16 quantization support for the TFLite format. For that, I would need to change the source code of some files, including the command-line interface implemented in /tensorflow/lite/python/tflite_convert.py. I was just wondering what is the most efficient way to debug my code and iterate quickly?

I am currently using CLIon+Google Bazel Plugin. This combination allows quick and convenient code development process. Thanks to the tools, I can place breakpoints, see variables, etc(only for CPP though). What is the most efficient way to debug python APIs?

I get, for example, I could rebuild the tflite_convert tool every time with the following command:
bazel build //tensorflow/lite/python:tflite_convert.

This is fine, but I cant debug this executable, which is inconvenient and does not allow quick iterations.
May I ask you to share your best practices for tensorflow python APIs development?
Many thanks,
Konstantin"
31441,Zero copy between CPU and GPU,"<em>I made sure that this is a feature request. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): __tensorflow-gpu 1.13.1+nv19.5__
- Are you willing to contribute it (Yes/No): __No__, not involved in project, could try



**Describe the feature and the current behavior/state.**

I use some network which requires **a lot of CPU <-> GPU transfers**, namely NMSv2 on a hardware platform where CPU and GPU have **unified (common) memory**. Such a transfers cause a lot of copying between the same memory, while **zero-copy** technique could be used.

NVIDIA team [says](https://devtalk.nvidia.com/default/topic/1059017/jetson-agx-xavier/zero-copy-with-tensorflow/post/5369698/#5369698) that this request should be filed here.


**Will this change the current api? How?**

**Yes.** Probably requires adding some new option in config, which handles whether or not to copy between CPU & GPU. like
```python
config.gpu_options.zero_copy = True
```

Maybe some further investigation is needed.

**Who will benefit with this feature?**

- **Users** who use embedded devices with unified memory will get **performance increase** in cases with many CPU <-> GPU transfers.
- **Tensorflow itself** could use this feature to persuade developers to choose it for embedded systems.

**Any Other info.**

- A [question](https://devtalk.nvidia.com/default/topic/1059017/jetson-agx-xavier/zero-copy-with-tensorflow/) about zero copy on NVIDIA DevTalk forum
- A [post](http://arrayfire.com/zero-copy-on-tegra-k1/) about sources modification need to make zero-copy work"
31440,Better documentation for Dataset.from_tensors/from_tensor_slices,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices

## Description of issue (what needs changing):

While following Google's ML crash course, I found it very difficult to understand the difference between `Dataset.from_tensors/from_tensor_slices` and when to use each. One thing that confused me was that `from_tensors` only creates a single tensor, despite the name including the plural form ""tensors"".

Beginners get introduced to these APIs very early, but the current documentation consists of one terse sentence about behaviour (plus a multi-line warning about memory usage):

```
FTS:
Creates a Dataset whose elements are slices of the given tensors.

FT:
Creates a Dataset with a single element, comprising the given tensors.
```

I think this would benefit from some elaboration and a clearer description of how the two are related. Given that users enocunter this API very early, the behaviour should ideally be obvious. A small example would help communicate this, e.g:

```
my_data = { ""my_feature"" : [ [1, 2, 3], [4, 5, 6] ] }
tf.data.Dataset.from_tensors(my_data) # Models a single, 2x3 tensor.
```

```
my_data = { ""my_feature"" : [ [1, 2, 3], [4, 5, 6] ] }
tf.data.Dataset.from_tensor_slices(my_data) # Splits on rows. Models two, 1x3 tensors.
```

### Clear description

### Correct links

Fine AFAIK

### Parameters defined

Fine AFAIK

### Returns defined

Fine AFAIK

### Raises listed and defined

Fine AFAIK

### Usage example

**There is currently no usage example, and I think the documentation would greatly benefit from one.**

### Request visuals, if applicable

There are currently no visuals. They might possibly help, but I think a usage example is probably sufficient. 

### Submit a pull request?

I'm not sure if I'll submit a PR to improve this. I'd like to, but I'm still quite new to TF and wouldn't like to introduce any inaccuracies.
"
31439,TensorFlow 1.14 eager execution not working with tf.function,"**System information**
- OS Platform and Distribution: macOS Mojave
- TensorFlow installed from: binary
- TensorFlow version: v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: Python 3.7.3
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

I use `tf.math.argmax(predictions).numpy()` to get the prediction in a function. But with the `tf.function` declared before the function, there will raise an AttributeError: 'Tensor' object has no attribute 'numpy'.

**Code to reproduce the issue**
Similar to [this tutorial](https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention), but with `tf.math.argmax(predictions).numpy()` inside the `train_step()` to print the prediction.
"
31438,Failed to load the native TensorFlow runtime.,"
C:\Users\safalabolo\Desktop\7-inversion>c:\Python36\python.exe apply_algo.py
WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`
c:\Python36\lib\site-packages\theano\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory
  warnings.warn(""DeprecationWarning: there is no c++ compiler.""
WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
WARNING (theano.tensor.blas): Failed to import scipy.linalg.blas, and Theano flag blas.ldflags is empty. Falling back on slower implementations for dot(matrix, vector), dot(vector, matrix) and dot(vector, vector) (cannot import name 'NUMPY_MKL')
Using TensorFlow backend.
Traceback (most recent call last):
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 35, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 30, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Impossibile trovare il modulo specificato.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""apply_algo.py"", line 11, in <module>
    loaded_model = pickle.load(open(modelname, 'rb'))
  File ""c:\Python36\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""c:\Python36\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""c:\Python36\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""c:\Python36\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""c:\Python36\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""c:\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""c:\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 35, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 30, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Impossibile trovare il modulo specificato.

"
31436,[TF 2.0 API Docs] tf.keras.backend.relu,"## URL(s) with the issue:

https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/backend/relu

## Description of the issue (what needs changing):

The documentation GitHub symbol link on the official `API_Docs` redirects to another symbol than the expected symbol.

### Correct links

No

### Parameters defined

No

### Returns defined

No

### Raises listed and defined
No

### Usage example

No

### Request visuals, if applicable

Yes

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
31434,tensorflow 1.14 build failed ,"
**System information**
- OS Platform and Distribution (Linux Ubuntu 18.04):
- TensorFlow installed from (source):
- TensorFlow version: r1.14
- Python version: v3.6.8
- Bazel version (tried v0.24.1, v0.26, v0.28):
- GCC/Compiler version (gcc version 7.4):
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: GTX1050



**Describe the problem**
I tried to use below command to build concat_op_test application.
`bazel test -c opt  --strip=never  -s //tensorflow/core/kernels:concat_op_test --verbose_failures` 
The compiler I select is clang and build with CUDA support.
I got below errors when bazel version is 0.24.1 and 0.26
```
ERROR: Skipping '//tensorflow/core/kernels:concat_op_test': error loading package 'tensorflow/core/kernels': Encountered error while reading extension file 'cu
da/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1266
                _create_local_cuda_repository(repository_ctx)
        File ""/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1199, in _create_local_cuda_repository
                _tpl(repository_ctx, ""crosstool:BUILD"", c...)
        File ""/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 745, in _tpl
                repository_ctx.template(out, Label((""//third_party/gpus/%s...)), ...)
class com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
ERROR: error loading package 'tensorflow/core/kernels': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cu
da//cuda': Traceback (most recent call last):
        File ""/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1266
                _create_local_cuda_repository(repository_ctx)
        File ""/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1199, in _create_local_cuda_repository
                _tpl(repository_ctx, ""crosstool:BUILD"", c...)
        File ""/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 745, in _tpl
                repository_ctx.template(out, Label((""//third_party/gpus/%s...)), ...)
class com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
``` 
I got below errors when bazel version is 0.28.0 (I changed the bazel version check in configure.py   `current_bazel_version = check_bazel_version('0.24.1', '0.28.2')` )
```
Starting local Bazel server and connecting to it...
Internal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIREC
TORY:@local_config_cuda' (requested by nodes 'REPOSITORY:@local_config_cuda')
        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:528)
        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassCastException: class com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.createFileFromTemplate(SkylarkRepositoryContext.java:295)
        at jdk.internal.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.base/java.lang.reflect.Method.invoke(Unknown Source)
        at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:135)
        at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:930)
        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:898)
        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)
        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)
        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)
        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)
        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:231)
        at com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:144)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:214)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:167)
        at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.fetchRepository(RepositoryDelegatorFunction.java:288)
        at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:215)
        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:451)
        ... 4 more

INFO: Elapsed time: 2.858s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/core/kernels
    Fetching @local_config_cuda; fetching
Internal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@local_config_cuda' (requested by nodes 'REPOSITORY:@local_config_cuda')
        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:528)
        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassCastException: class com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.createFileFromTemplate(SkylarkRepositoryContext.java:295)
        at jdk.internal.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.base/java.lang.reflect.Method.invoke(Unknown Source)
        at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:135)
        at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:930)
        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:898)
        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)
        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)
        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)
        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)
        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:231)
        at com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:144)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:214)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:167)
        at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.fetchRepository(RepositoryDelegatorFunction.java:288)
        at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:215)
        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:451)
        ... 4 more
java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@local_config_cuda' (requested by nodes 'REPOSITORY:@local_config_cuda')
        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:528)
        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassCastException: class com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.createFileFromTemplate(SkylarkRepositoryContext.java:295)
        at jdk.internal.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.base/java.lang.reflect.Method.invoke(Unknown Source)
        at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:135)
        at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:930)
        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:898)
        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)
        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)
        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)
        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)
        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:231)
        at com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:144)
        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:214)
        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)
        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:167)
        at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.fetchRepository(RepositoryDelegatorFunction.java:288)
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/core/kernels
    Fetching @local_config_cuda; fetching
```

"
31431,Python 3.7 on Windows.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
Tried many.

- Are you willing to contribute it (Yes/No):
Yes


**Describe the feature and the current behavior/state.**
Make Tensorflow work for python 3.7 on Windows.

**Will this change the current api? How?**
I don't know.

**Who will benefit with this feature?**
Users and developers.

**Any Other info.**
It's a huge issue, dlib has same problem."
31429,SIGABRT on `tf.image.encode_png` with empty tensor,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): tf-nightly-2.0-preview==2.0.0.dev20190807
- TensorFlow version (use command below): v1.12.1-8193-ge7d48dc 2.0.0-dev20190807
- Python version: 3.6.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Calling `tf.image.encode_png` kills the process with SIGABRT if you pass
a tensor that has no elements.

**Describe the expected behavior**

It should never SIGABRT.

**Code to reproduce the issue**

```python
import tensorflow as tf

image = tf.cast(tf.tile([[[0, 0, 0]]], [0, 0, 1]), tf.uint8)
# Or: = tf.cast(tf.reshape([], [0, 0, 3]))
try:
  tf.print(image)
  tf.print(tf.shape(image))
  tf.image.encode_png(image)
finally:
  print(""We never get here!"")
```

```
$ TF_CPP_MIN_LOG_LEVEL=1 python test.py
[]
[0 0 3]
2019-08-07 16:32:12.864303: F tensorflow/core/lib/png/png_io.cc:347] 'image' Must be non NULL
Aborted
$ echo $?
134
```

**Other info / logs**

This also affects `tf.summary.image`.
"
31426,Optimizer other than GradientDescent throwing errors on first run,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colag
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.14
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the current behavior**
On running the train method, it is throwing error despite initializing all variables before the optimize step. 

**Describe the expected behavior**
It should not throw errors and initialize all variables.
**Code to reproduce the issue**
Colab [gist](https://colab.research.google.com/gist/dsgupta/bb90037cb95573f610ff2d974883286d/test.ipynb)

"
31421,[TF 2.0] Issues Serializing SavedModel Serving Default Signature,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX + Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): `pipenv install --pre tensorflow==2.0.0-beta1 --python=3.6.8`
- TensorFlow version (use command below): `2.0.0-beta1`
- Python version: `3.6.8`
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

I want to use a `SavedModel` to run batch inference in `Spark`. A few libraries implement this functionality, such as [Databrick's Spark Deep Learning](https://github.com/databricks/spark-deep-learning), however, they are implemented using `TF 1.13.1` or lower. I am using `TF 2` to train and save my models, so I would like to use the same version for deploying my model. 

To use an object in PySpark it must be serializable, but I am getting a pickle issue when trying to use a `SavedModel`'s `signatures['serving_default]` concrete function. 

Previously, this call was serializable
```python
# TensorFlow 1.X
outputs = session.run(f(placeholder), feed_dict={placeholder: input})
``` 

Now, this is giving issues
```python
# TensorFlow 2.0
outputs = f(input)
```

This behavior can be reproduced without using `Spark`. You only need to try `pickle.dump` the `signatures['serving_default']` concrete function.

**Describe the expected behavior**

I expect both the `TF 1.X` and `TF 2.0` ways of predicting from a `SavedModel` to be serializable

```python
# TensorFlow 1.X
outputs = session.run(f(placeholder), feed_dict={placeholder: input})
# TensorFlow 2.0
outputs = f(input)
```

Let me know if I am approaching this problem the wrong way! Any help is appreciated. 

**Code to reproduce the issue**

```python

# Example
import os
import pickle

from pathlib import Path

import tensorflow as tf
from tensorflow.keras import layers


# Create a simple model
inputs = tf.keras.Input(shape=(784,), name='img')

x = layers.Dense(64, activation='relu')(inputs)
x = layers.Dense(64, activation='relu')(x)
outputs = layers.Dense(10, activation='softmax')(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mnist_model')

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype('float32') / 255

# Compile
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=tf.keras.optimizers.RMSprop(),
              metrics=['accuracy'])

# Train
history = model.fit(x_train, y_train,
                    batch_size=64,
                    epochs=5,
                    validation_split=0.2)

# Save to SavedModel 
output_dir = ""/tmp/workdir/v1/""
output_directory = Path(output_dir)
output_directory.mkdir(parents=True, exist_ok=True)

model_save_path = os.path.join(output_dir,'model')

model.save(tf_model_save_path, save_format=""tf"")


# Load the SavedModel

saved_model = tf.saved_model.load(model_save_path, tags=['serve'])

# Get the 'predict' concrete function
infer = saved_model.signatures['serving_default']

pickle_file = os.path.join(output_dir, 'serialized')

# Try serialize the function 
with open(pickle_file, 'wb') as f:
    pickle.dump(infer, f)

# Expect error 
# _pickle.PickleError: can't pickle repeated message fields, convert to list first
```

**Other info / logs**
````
_pickle.PickleError: can't pickle repeated message fields, convert to list first
````"
31420,Cannot install TF wheel file for Python3 due to functools32 dependency,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 (Linux v5.1.0)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: b28755fbd2f33baaff2ce703f456513f087f8e76 (`master`)
- Python version: 3.5.2
- Installed using virtualenv? pip? conda?: pip3 19.2.1
- Bazel version (if compiling from source): 0.28.1
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
Since commit 82c46d74d9ece4f6b5832f27a7ae580d95e1a312, `pip3` refuses to install the tensorflow wheel file due to the missing `functools32` dependency. Since I'm running Python3, I cannot install `functools32` since it is a backport for Python2.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
# Inside working copy of tensorflow
$ bazel build //tensorflow/tools/pip_package:build_pip_package
...
$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /data/Packages/
INFO: Elapsed time: 61.754s, Critical Path: 33.82s
INFO: 533 processes: 533 local.
INFO: Build completed successfully, 753 total actions
Wed Aug 7 12:47:45 CDT 2019 : === Preparing sources in dir: /tmp/tmp.jkwGwPI847
...
Wed Aug 7 12:49:09 CDT 2019 : === Building wheel
warning: no files found matching '*.pyd' under directory '*'
warning: no files found matching '*.pd' under directory '*'
warning: no files found matching '*.dylib' under directory '*'
warning: no files found matching '*.dll' under directory '*'
warning: no files found matching '*.lib' under directory '*'
warning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'
warning: no files found matching '*' under directory 'tensorflow/include/third_party'
Wed Aug 7 12:49:40 CDT 2019 : === Output wheel file is in: /data/Packages/

$ pip3 install --upgrade /data/Packages/tensorflow-1.14.0-cp27-cp27mu-linux_x86_64.whl
Processing /data/Packages/tensorflow-1.14.0-cp27-cp27mu-linux_x86_64.whl
Collecting protobuf>=3.6.1 (from tensorflow==1.14.0)
  Using cached https://files.pythonhosted.org/packages/22/cb/8ca68af4233c09f2dd9833f3b9c6d8e706da2d33988dfb3732a777e15e4b/protobuf-3.9.1-cp35-cp35m-manylinux1_x86_64.whl
Collecting gast>=0.2.0 (from tensorflow==1.14.0)
Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow==1.14.0)
  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl
Collecting keras-applications>=1.0.8 (from tensorflow==1.14.0)
  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl
Collecting termcolor>=1.1.0 (from tensorflow==1.14.0)
Collecting opt-einsum>=2.3.2 (from tensorflow==1.14.0)
Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow==1.14.0)
  Using cached https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl
Collecting functools32>=3.2.3 (from tensorflow==1.14.0)
  Using cached https://files.pythonhosted.org/packages/c5/60/6ac26ad05857c601308d8fb9e87fa36d0ebf889423f47c3502ef034365db/functools32-3.2.3-2.tar.gz
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-hmfnihxv/functools32/setup.py'""'""'; __file__='""'""'/tmp/pip-install-hmfnihxv/functools32/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base pip-egg-info
         cwd: /tmp/pip-install-hmfnihxv/functools32/
    Complete output (1 lines):
    This backport is for Python 2.7 only.
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31419,Memory leakage when converting to tensor,"`
import numpy as np
import tensorflow as tf

for i in range(5000):
    print(i)
    array = np.random.random((1024, 1024))
    tf.convert_to_tensor(array, dtype=tf.float32)
`

Tensorflow version is 1.14.0, Numpy version is 1.17.0, python version is 3.6.8
The process is killed when i ~= 2400 on my machine
The command ""watch -d free -m"" shows that memory decreases over time until it gets close to zero, then crashes

I did not find a way to free the memory from the unreferenced tensors

Best,
Benot"
31417,libnvinfer-dev : Depends: libnvinfer5 (= 5.0.2-1+cuda10.0) but 5.1.5-1+cuda10.1 is to be installed,"can someone tell me how to change the libvnfer to the right one ?
libnvinfer-dev : Depends: libnvinfer5 (= 5.0.2-1+cuda10.0) but 5.1.5-1+cuda10.1 is to be installed
E: Unable to correct problems, you have held broken packages."
31415,tools.graph_transforms.TransformGraph has no docs or example of usage,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

the provided wrapper function has no docs

## Description of issue (what needs changing):

provide a docs such
examples of usage:
TransformGraph( graph.as_graph_def(), [], [], ['remove_nodes(op=loss/init)']) ...

### Clear description

I wanna to use this method has a way to do specifics editions and graph redefinitions while building the model, from inside python, withou having to go to command line.

### Parameters defined

I think how to use the parameters are exactly the problem, the README.md from the repository gives bazel example, but it dosnt work as it should in the wrapper
### Usage example

this is my first try, i could not get the desirable result (strip init op from graph):

```python
graph = tf.Graph()
with graph.as_default():

  with tf.variable_scope('signal_in'):
    signal_in = tf.placeholder(tf.float32, shape=(10,40,2,1))

  with tf.variable_scope('dascope1'):
    conv_linear = tf.keras.layers.Conv2D( 8, (8,2), padding='valid', name='conv_linear', use_bias=True, kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137) )(signal_in)
  
  with tf.variable_scope('softmax'):
    logits = tf.contrib.layers.fully_connected(conv_linear, 2, activation_fn=None, normalizer_fn=None, normalizer_params=None, weights_initializer=tf.initializers.lecun_normal(seed=731), weights_regularizer=None, biases_initializer=tf.initializers.lecun_normal(seed=777), biases_regularizer=None, reuse=None, variables_collections=None, outputs_collections=None, trainable=True, scope='logit')
    softmax = tf.nn.softmax(logits,axis=0)            
    
  with tf.variable_scope('loss'):
    l_vec = tf.placeholder(tf.float32, shape=(10,2))
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0)(l_vec, softmax)         
    minimize_op = tf.train.AdamOptimizer(learning_rate=0.05).minimize(loss)
    tf.global_variables_initializer()
```
then:

```python
graphdef = tf.tools.graph_transforms.TransformGraph( graph.as_graph_def(), [], [], ['remove_nodes(op=loss/init)'])

with tf.Graph().as_default() as g:  
  tf.import_graph_def(graphdef,name = '')
  for op in g.get_operations():
    if op.name.split('/')[-1] == 'init':
      print('True')
``` 

returns True
So how to use this wrapper ? note that init op dosnt have any input output but only dependency arrows as input.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Waiting for instructions of the community about the use of this function.

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
31414,ImportError: DLL load failed: %1 is not a valid Win32 application.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Windows 10):
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 1.13.1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: conda 
- CUDA/cuDNN version:  10.0.130/ 7.6.0
- GPU model and memory:  Running on laptop to demo a conda environment build so not using GPU, 16 GB memory 

**Describe the problem**

After building my conda environment using the environment.yml from a known working system, I encounter a ""ImportError: DLL load failed: %1 is not a valid Win32 application."" issue which occurs when the script attempts to import pywrap_tensorflow (see traceback below). This is strange because both the origin computer (from which the environment was copied) and the source computer are 64-bit and tensorflow is certainly not a 32-bit application. The error seems to be coming from the imp module. Has anyone else encountered a similar issue? 


**Provide the exact sequence of commands / steps that you executed before running into the problem**

conda env create -n <envname> -f  <Environment.yml>
python <main.py>


**Any other info / logs**

Traceback (most recent call last):
  File ""hyperparam_grid_search.py"", line 10, in <module>
    import tensorflow as tf
  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg).pywrap_tensorflow_internal import *
ImportError: Traceback (most recent call last):ib\site-packages\tens  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *s\tens  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()l', fp, pathna  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper_module
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\mmusil\.conda\envs\AutoTune5\lib\imp.py"", line 343,  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\imp.py"", line 243, in load_moduled(spec)
    return load_dynamic(name, filename, file)id Win32 application.
  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)ive TensorFlow runtime.
ImportError: DLL load failed: %1 is not a valid Win32 application.

"
31413,//tensorflow/contrib/metrics:metric_ops_test fails with Assertion error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04 s390x
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.15
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
python tensorflow/contrib/metrics/python/ops/metric_ops_test.py
```
======================================================================
FAIL: testWithMultipleUpdates (__main__.AucWithConfidenceIntervalsTest)
testWithMultipleUpdates (__main__.AucWithConfidenceIntervalsTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor
    yield
  File ""/usr/local/lib/python2.7/dist-packages/absl/third_party/unittest3_backport/case.py"", line 162, in run
    testMethod()
  File ""tensorflow/contrib/metrics/python/ops/metric_ops_test.py"", line 2313, in testWithMultipleUpdates
    batch_size = 50
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 1073, in decorated
    return f(*args, **kwds)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 2303, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 2272, in _assertAllCloseRecursive
    (path_str, path_str, msg)))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 2207, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=""\n"".join(msgs), equal_nan=True)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1501, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 827, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=1e-06, atol=1e-06
Mismatched value: a is different from b.
not close lhs = 0.753345469611
not close rhs = 0.753343403339
not close dif = 2.0662712803e-06
not close tol = 1.75334340334e-06
dtype = float64, shape = ()
Mismatch: 100%
Max absolute difference: 2.06627128e-06
Max relative difference: 2.74280132e-06
 x: array(0.753345)
 y: array(0.753343, dtype=float32)
```



**Describe the expected behavior**
The test should pass on s390x.
"
31412,DeprecationWarning: the imp module is deprecated,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: tensorflow-gpu==2.0.0b1
- **Python version**: 3.6.8
- **CUDA/cuDNN version**: 10.0.130/7.6.2
- **Exact command to reproduce**:

Create `test.py` file with the following content:
`import tensorflow as tf`

Run pytest (5.0.1) on it:
`pytest test.py`

Result:
`============================================================================================= warnings summary =============================================================================================
venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15
  venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
    import imp
`

### Describe the problem
TensorFlow is using ""imp"" which is deprecated since Python 3.4. Are there plans to update it?
Thanks!"
31410,MirrorStrategy() fills up memory of GPU that is not selected for training in TF 2.0.,"I am using `tf.distribute.MirroredStrategy()` in order to train a `tf.keras` model. I have 2 T4 GPUs from Google available. I want  to train my model consisting LSTM layer on only one GPU i.e. `/gpu:1`. So, in order to select it, I define `devices = [""/gpu:1""]` inside `MirrorStrategy()` as suggested [here](https://www.tensorflow.org/guide/distribute_strategy#mirroredstrategy), now when I run the file for training, the memory of first GPU(/gpu:0) is also filling up completely however, during training only GPU (/gpu:1) is being utilized completely. My question is why is this happening since I only want to utilize GPU (/gpu:1)? In this case, first GPU is useless.
However, if I select the first GPU(/gpu:0) for training then it uses only 112MB of memory of the second GPU(/gpu:1).
```
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |
| N/A   60C    P0    28W /  70W |  14449MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |
| N/A   75C    P0    74W /  70W |  14517MiB / 15079MiB |     95%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     20576      C   python                                     14439MiB |
|    1     20576      C   python                                     14507MiB |
+-----------------------------------------------------------------------------+"
31408,Value of sqrt(2) is calculated incorrectly and inconsistently,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): not known
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): not known
- CUDA/cuDNN version: 10.0/7.6.0
- GPU model and memory: Tesla K-80 12gb

**Describe the current behavior**
I just computed sqrt(2) 100 times in google colab. The values are computed inconsistently, and a bit incorrectly, in a single tensor.

**Describe the expected behavior**
sqrt(2) should be computed the same way every time.

**Code to reproduce the issue**
```
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf

tf.enable_eager_execution()

sqrt2s = tf.sqrt(tf.constant([2.0]*100, dtype=tf.float32))
print(sqrt2s)
print(sqrt2s[0].numpy())
print(sqrt2s[-1].numpy())
print(sqrt2s[0].numpy() == sqrt2s[-1].numpy())
```
The results:
```
tf.Tensor(
[1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134
 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142135 1.4142135
 1.4142135 1.4142135], shape=(100,), dtype=float32)
1.4142134
1.4142135
False
```
**Other info / logs**
The notebook is here:
https://colab.research.google.com/drive/1PQp2KtFUoaUEHETrIUne-J1Cu2blg2cG"
31407,"What is the correct  way to compile a 'so' file without ""undefined reference to..."" error?","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  5.2.5-arch1-1-ARCH
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- Bazel version (if compiling from source): 0.27.0- (@non-git)
- GCC/Compiler version (if compiling from source): 9.1.0
- CUDA/cuDNN version: cuda: 10.1 ,cudnn: 7.6
- GPU model and memory: 16G

I'm using the **tfcompile** to convert my trained model into object files. **Everything works fine if I compile it as** `cc_binary` with **Bazel**.
**As I must wrap all the tf-relative codes into a single part**, I compiled the converted .o files with `cc_library` and got a so file indeed.But when it was linked as a third lib in non-bazel env it issued like this:
```
/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `xla::ExecutableRunOptions::set_intra_op_thread_pool(Eigen::ThreadPoolDevice const*)'
/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `__xla_cpu_runtime_EigenConvF32'
/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `tensorflow::XlaCompiledCpuFunction::~XlaCompiledCpuFunction()'
/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `__xla_cpu_runtime_EigenMatMulF32'
/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `typeinfo for tensorflow::XlaCompiledCpuFunction'
/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `tensorflow::XlaCompiledCpuFunction::Run()'
```
I also tried to **add these missing objs into a static lib and passed it to the linker hand by hand.
It works but really horrible because of the linkage chain**.
Is there a novel way to this case?
here is my **BUID** file:
```
cc_library(
    name = ""captchaLib"",
    srcs = [
    	 ""captcha_run.cc"",
	 ""captcha.h"",
	 ""captcha_model.o"",
	 ""captcha_helper.o"",
	 ],
    copts=[""-fPIC"",""-std=c++11"",""-O3""],
    deps = [
       ""//tensorflow/compiler/xla:shape_util"",
         ""//tensorflow/compiler/tf2xla:xla_compiled_cpu_function"",
         ""//tensorflow/core:framework_lite"",
        ""//third_party/eigen3"",
       ""@com_google_absl//absl/strings"",
      ] + [
            ""//tensorflow/compiler/xla/service/cpu:runtime_conv2d"",
            ""//tensorflow/compiler/xla/service/cpu:runtime_key_value_sort"",
            ""//tensorflow/compiler/xla/service/cpu:runtime_matmul"",
            ""//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d"",
            ""//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul"",
	    ],
    linkstatic=False,
)
```"
31406,Using another model despite of provided in android sample,"@ry @jmhodges 

I have replaced the tensorflow lite (Detect.tflite) model in the android sample.

I ended up with several errors on building the project.

One of them is 

` java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.`

After some try and error I ended up with another error mentioned below

 `java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 1917, 4] and a Java object with shape [1, 3].`

Please look it into asap so can move forward with the sample.



"
31404,Does the forward propagation of tensorflow code really use GPUs?,https://stackoverflow.com/questions/57372076/does-the-forward-propagation-of-tensorflow-code-really-use-gpus
31403,Upgrade python version in official tensorflow docker image,"Using docker image from tensorflow official website
[https://www.tensorflow.org/install#run-a-tensorflow-container](url) 
Installs python2.7 default, which is planned to be deprecated near in future.

> DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support

This issue is  request to provide updated python version with docker image."
31402,Support for Unix/Linux netpbm (.pgm/.ppm) image formats,"As we are dealing with very large problems with millions of images, cropping small images
from hundreds of thousands of large images and then transforming the ROIs to .jpg or .png for tensorflow only, is a real resource problem. See .pdf of article on: arXiv:1904.08421
This is especially true because this is a dynamic environment with new large images coming in all the time such that any form of prepackaging data sets becomes a stumbling block in resource space. It would make a tremendous difference if TF were able to handle raw Unix/Linux NetPBM images directly (binary: .pbm, grey: .pgm, color: .ppm). See: https://sourceforge.net/projects/netpbm/  (600 package downloads per month).
On the web, proposals are made to handle NetPBM conversion in Python, but that does not appear to be the most efficient path. With our problem scale, the communication, I/O and conversion overhead is much more noticeable than in the well-known closed-data set TF examples. "
31401,How can I use the tensorflow.python.profiler.model_analyzer under the estimator framework,"I use the estiamtor framework and I want to run profiler within it.
for the feature of estimator, I just write the profiler code like following:
```
with tf.contrib.tfprof.ProfileContext(<my config>) as pctx:
    pctx.add_auto_profiling(<my profiler>)
    estimator.train(<my params>)
```
Luckly, It's work! I can get the profiler and show it on the profiler ui [(profiler ui)](https://github.com/tensorflow/profiler-ui)
Then, I want to use the model_analyzer just like following(just add after the train ):
```
from tensorflow.python.profiler import model_analyzer
model_analyzer(options=model_analyzer.ALL_ADVICE)
```
here is a tip that: No RunMeta.
I know in the normal pipline the runmeta will produce by the profiler, but under estimator, do not need the runmeta.
I would be very grateful for any reply.
"
31400,Tensorflow Speech Command model showing many false positive,"I am using tensorflow speech commands project to make a model that works on 4 commands : up, down, left, right. While the true positive rate is very good, the false positive rate is also very high. How to tackle this problem?"
31399,Bug in tflite_convert: when converting tf.gather operation (r1.14),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): built from r1.14 (https://github.com/tensorflow/tensorflow/tree/r1.14)
- TensorFlow version (or github SHA if from source): 1.14



I might found a bug in tflite_convert, and I want to report this.
I am not sure whether you aware of this or not, but I will explain it for the safety.

The following converting code gives an error in making tflite model.
It complains about checking dims() during the conversion.

```
import tensorflow as tf
import numpy as np

export_dir = 'test_saved_model'
builder = tf.saved_model.builder.SavedModelBuilder(export_dir)

weight = np.random.rand(32,512)

x = tf.placeholder(tf.float32, shape=[None,512])
W = tf.constant(weight,dtype=tf.float32, name='kernel')
z = x*W

gdim = tf.constant(3,dtype=tf.int32)
gaxis = tf.constant(0,dtype=tf.int32)
Wg = tf.get_variable(""kernel"", [32,512])
y = tf.gather(Wg,gdim,gaxis)
res = y





with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        builder.add_meta_graph_and_variables(sess, [""test""])
        builder.save(as_text=True)

        converter = tf.lite.TFLiteConverter.from_session(sess, [x], [res])

        # options for converter
        converter.dump_graphviz_dir=""graph_dir"" # outputs graph visualization file (.dot)
        converter.dump_graphviz_video=True

        tflite_model = converter.convert()

```

I found that it is caused by the resolve_constant_gather.cc
https://github.com/tensorflow/tensorflow/blob/a497fb2da9258cbfaa4e443f446c5ab039ef4a32/tensorflow/lite/toco/graph_transformations/resolve_constant_gather.cc#L42

It seems that the Gather function assumes that 'coords_array' is always 1-dim array.
However, tf.gather may accept it as 0-dim array, a scalar value, so that the checking fail if 'coords_array' became a scalar.


My solution is following modification for Gather function.
It worked for me when I personally built and installed from the modified source.

I hope someone to check about this and fix the issue if necessary.

```
// Gathers data from axis 0.
template <ArrayDataType Type>
inline void Gather(const Array& input_array, const Array& coords_array,
                   Array* output_array) {
  const Shape& input_shape = input_array.shape();
  const std::vector<DataType<Type>>& input_data =
      input_array.GetBuffer<Type>().data;
  const Shape& coords_shape = coords_array.shape();
  const std::vector<int32>& coords_data =
      coords_array.GetBuffer<ArrayDataType::kInt32>().data;

  const Shape& output_shape = output_array->shape();
  std::vector<DataType<Type>>& output_data =
      output_array->GetMutableBuffer<Type>().data;
  output_data.resize(RequiredBufferSizeForShape(output_shape));

  int stride = 1;
  for (int i = 1; i < input_shape.dimensions_count(); ++i) {
    stride *= input_shape.dims(i);
  }

  if (coords_shape.dimensions_count()==0){
        // when coords_array is 0-dim (a constant)
        CHECK_EQ(stride * 1, output_data.size());

        DCHECK_GE(coords_data[0], 0);
        DCHECK_LT(coords_data[0], input_shape.dims(0));
        DataType<Type>* out = output_data.data();
        const DataType<Type>* in = input_data.data() + coords_data[0] * stride;
        memcpy(out, in, sizeof(DataType<Type>) * stride);
  }
  else {
        // when coords_array is 1-dim array
        CHECK_EQ(coords_shape.dims(0), output_array->shape().dims(0));

        // Let's make sure we have enough space for all element in the memcpy()
        // below, which writes 'stride' elements starting at 'i * stride'.
        CHECK_EQ(stride * coords_shape.dims(0), output_data.size());

        for (int i = 0; i < coords_shape.dims(0); ++i) {
            DCHECK_GE(coords_data[i], 0);
            DCHECK_LT(coords_data[i], input_shape.dims(0));
            DataType<Type>* out = output_data.data() + i * stride;
            const DataType<Type>* in = input_data.data() + coords_data[i] * stride;
        }
  }
}

```"
31398,The precision difference between tensorflow and numpy for matrix multiplication,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.0
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Tensorflow matrix multiplication on CPU does not reproduce same result as numpy matrix multiplication for float32. Furthermore, (X @ Y) and (Y.T @ X.T).T produces different results.
Is this a normal behavior, if it is, then which version of the matrix product should be taken as correct output?

**Describe the expected behavior**
Theoretically (X @ Y) and (Y.T @ X.T).T should evaluate to same value and their value should be same as numpy version.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import numpy as np
import tensorflow as tf

x = np.arange(150, dtype=np.float32).reshape(-1, 5)/10
y = np.arange(200, dtype=np.float32).reshape(-1, 5)/10
X_tf = tf.Variable(x)
XT_tf = tf.Variable(x.T)
Y_tf = tf.Variable(y)
YT_tf = tf.Variable(y.T)
YXT1_tf = Y_tf @ XT_tf
YXT2_tf = tf.transpose(X_tf @ YT_tf)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    YXT1, YXT2 = sess.run([YXT1_tf, YXT2_tf])
yxt1 = y @ x.T
yxt2 = (x @ y.T).T

print(np.max(np.abs(YXT1-YXT2)), np.max(np.abs(yxt1-yxt2)))
print(np.max(np.abs(yxt1-YXT1)), np.max(np.abs(yxt1-YXT2)))
```

which outputs
0.00012207031 0.0 
0.00012207031 0.00012207031

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31397,how to transfer ckpt to pb or saved model?,"as the title shows, how we do this?

tensorlfow has lots of saved format, and version issues.

there is no such document,about how we can do this.

i search the similar issues, you all said these question maybe suitalbe for stackoverflow.

but none of you have answered the question.

maybe this is an document issue or not.

thank you very much if you can give me the instructions"
31395,how to remove pruned weights from graph?,"i have prune some weights as 0, to improve the  inference speed, i want to remove these weights from my original graph, how can i realize this?"
31392,"CUDA 10.1, failure to copy cublas_v2.h to build directory (again)","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 29
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 19.2
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version: 10.1 / 7.5.0.56
- GPU model and memory: GeForce GT 750M with 2 GB


**Describe the problem**

As with the *second* issue in [#28936](https://github.com/tensorflow/tensorflow/issues/28936), I am having problems when compiling tensorflow with CUDA 10.1. It seems that libcublas_v2.h is not being copied to the build directory. That ticket was closed with a comment indicating that it was due to a bug that had been fixed -- but the bug seems to have returned.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Installed CUDA 10.1 and cuDNN 7.5.
2. Created logical links from /usr/lib64 to /usr/local/cuda-10.1/targets/x86_64-linux/lib/ for libcublas.so.10.1 (This removed the *first* issue reported in [#28936](https://github.com/tensorflow/tensorflow/issues/28936).)
3. Ran ./configure, accepting defaults, except 'Y' for CUDA support, 3.0 for Cuda capability (actually, this is the default, as my GPU is correctly identified)
4. bazel clean --expunge
5. TMP=/tmp time bazel build --config=opt --local_resources=6144,6,1.0 //tensorflow/tools/pip_package:build_pip_package

This ran for more than an hour before yielding the error:

```
ERROR: /usr/local/src/git/tensorflow/tensorflow/core/kernels/BUILD:2809:1: C++ compilation of rule '//tensorflow/core/kernels:cholesky_op' failed (Exit 1)
In file included from tensorflow/core/kernels/cholesky_op.cc:35:0:
./tensorflow/core/kernels/cuda_solvers.h:29:10: fatal error: cuda/include/cublas_v2.h: No such file or directory
 #include ""cuda/include/cublas_v2.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~
```

Note that unlike the report for [#28936](https://github.com/tensorflow/tensorflow/issues/28936), the file is missing from cuda/include/, rather than third_party/gpus/cuda/include/.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31377,Quantized SSD TFLite runs much slower in GPU (GTX 1080) when compared to its original model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): NO
- TensorFlow version (use command below): 1.14
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10/7.5
- GPU model and memory: GTX 1080 TI

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I converted the SSD into the TFLite model with quantization and ran a speed comparison between the quantized SSD and the original SSD. The time used for each image: 
quantized SSD - 110 ms 
original SSD - 12 ms
the quantized SSD is almost ten times slower than the original one in GPU. 
Some additional info:
SSD is based on the mobilenet_v1, no other costumed operations. 
Codes used to do the conversion:
tflite_convert \
  --output_file=./ssd.tflite \
  --graph_def_file=./tflite_graph.pb \
  --inference_type=FLOAT \
  --input_arrays=normalized_input_image_tensor \
  --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
  --std_dev_values=128 --mean_values=128 \
  --default_ranges_min=-6 --default_ranges_max=6 \
  --input_shapes=1,300,300,3 \
  --allow_custom_ops

The two models are in the link:
https://drive.google.com/drive/folders/1Q5aOZRlxhDsMdiSYYSuWlUyQP82FJTdC?usp=sharing

Anyone can give me some suggestion?  Thank you!

"
31373,tf.data.Dataset.map + tf.numpy_function = lost shape sadness,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I slightly modified a tf documentation code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 LTS / macOS 14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): 1.14
- TensorFlow version (use command below): conda
- Python version: 3.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: colab
- GPU model and memory: colab

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`tf.numpy_function` works are wrapping numpy / python functions with tf 1.14 in eager and normal mode, but shape is lost


```python
# slight modification from ""Load Images with tf.data"" https://www.tensorflow.org/tutorials/load_data/images


def preprocess_image(image, resize=[192, 192]):
  image = tf.image.decode_jpeg(image, channels=3)
  image = tf.image.resize(image, [192, 192])
  image /= 255.0  # normalize to [0,1] range

  return image


def load_and_preprocess_image(path, resize=[192, 192]):
  image = tf.read_file(path)
  return preprocess_image(image, resize)

# dataset only contains paths, so wrap whatever value for `resize` in lambda
_load_and_preprocess_image = lambda path: load_and_preprocess_image(path, [192,192])

# we ""have"" numpy functionality for handling images so wrap in `tf.numpy_function`
tf_load_and_preprocess_image = lambda path: tf.numpy_function(_load_and_preprocess_image, [path], tf.float32)

# map
image_ds2_error_boogaloo = path_ds.map(tf_load_and_preprocess_image, num_parallel_calls=AUTOTUNE)

# no shape
image_ds2_error_boogaloo
# `<DatasetV1Adapter shapes: , types: tf.float32>`

```


**Describe the expected behavior**

shape is not lost

supposedly fixed here #16052 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

https://colab.research.google.com/drive/1DeGMPxb8cyHm5QLpqJ9-cB2sbF8PC3vA#scrollTo=qGiGN5rl4s2f&line=22&uniqifier=1


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31370,tf.data.experimental.make_csv_dataset header flag not working as described,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
`Custom code`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
```os: Linux
os kernel version: #22-Ubuntu SMP Tue Jul 2 13:27:33 UTC 2019
os release version: 5.0.0-21-generic
os platform: Linux-5.0.0-21-generic-x86_64-with-debian-buster-sid
linux distribution: ('debian', 'buster/sid', '')
linux os distribution: ('debian', 'buster/sid', '')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='dev-XPS-13-9343', release='5.0.0-21-generic', version='#22-Ubuntu SMP Tue Jul 2 13:27:33 UTC 2019', machine='x86_64', processor='x86_64')
architecture: ('64bit', '')
machine: x86_64
GNU/Linux
```
- TensorFlow installed from (source or binary):
`binary using pip`
- TensorFlow version (use command below):

```Name: tensorflow
Version: 2.0.0b1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/dev/miniconda3/envs/pythonapu/lib/python3.6/site-packages
Required-by: 
```

- Python version:
```
Python 3.6.8 :: Anaconda, Inc.
python version: 3.6.8
python branch: 
python build version: ('default', 'Dec 30 2018 01:22:34')
python compiler version: GCC 7.3.0
python implementation: CPython
```
**Describe the current behavior**

`tf.data.experimental.make_csv_dataset(header=True)` includes the header data in the dataset

**Describe the expected behavior**

According to the docs:
```
header: A bool that indicates whether the first rows of provided CSV files
      correspond to header lines with column names, and should not be included
      in the data.
```
The data should not be included in the data.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

I have a dataset in csv `/tmp/foo.csv`:
```
A,B,C
1,NA,1
1,NA,1
1,NA,1
1,NA,1
```
I can run something like with `header=True`
```
 dataset_file = tf.keras.utils.get_file(""foo2"" + str(uuid.uuid4()) + "".csv"", ""file:///tmp/foo.csv"")
dataset = tf.data.experimental.make_csv_dataset(
        dataset_file, batch_size=4, header=False,
        label_name=""A"", na_value='NA', column_names=[""A"", ""B"", ""C""],
        field_delim=',')
    for feature_batch, label_batch in dataset.take(1):
        print(label_batch)
        print(""features:"")
        for key, value in feature_batch.items():
            print(key + ' ' + value)
```



which gives:

```
Traceback (most recent call last):
  File ""/home/xyz/workspace/pythonapi/main/services/dataloader.py"", line 149, in <module>
    load()
  File ""/home/xyz/workspace/pythonapi/main/services/dataloader.py"", line 143, in load
    print(key + ' ' + value)
  File ""/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 909, in r_binary_op_wrapper
    x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name=""x"")
  File ""/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1100, in convert_to_tensor
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
  File ""/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1158, in convert_to_tensor_v2
    as_ref=False)
  File ""/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1237, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 305, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 246, in constant
    allow_broadcast=True)
  File ""/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 254, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 115, in convert_to_eager_tensor
    return ops.EagerTensor(value, handle, device, dtype)
TypeError: Cannot convert provided value to EagerTensor. Provided value: C  Requested dtype: int32
```
When C should not be in the dataset.

**Other info / logs**

With `header=False`

works fine but not as described in the documentation as this suggests the csv does not contain the header and prints 
```
tf.Tensor([b'1' b'1' b'1' b'A'], shape=(4,), dtype=string)
features:
tf.Tensor([b'B ' b'B ' b'B ' b'B B'], shape=(4,), dtype=string)
tf.Tensor([b'C 1' b'C 1' b'C 1' b'C C'], shape=(4,), dtype=string)
```"
31369,TensorflowLite Runtime Installation Doesn't provide Interpreter Package,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: 
[Platform 1]
NAME=""Ubuntu""
VERSION=""16.04.5 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.5 LTS""
VERSION_ID=""16.04""

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:

[Platform 2]
PRETTY_NAME=""Mendel GNU/Linux 3 (Chef)""
NAME=""Mendel GNU/Linux""
VERSION_ID=""3""
VERSION=""3 (chef)""
ID=mendel
ID_LIKE=debian

- TensorFlow installed from (source or binary): {Didn't install trying to use tflite_runtime} [TFLITE RUNTIME](https://www.tensorflow.org/lite/guide/python#run_an_inference_using_tflite_runtime)
- TensorFlow version: N/A
- Python version: Mendel 3.5.3 / Ubuntu 16.04 3.6.9
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):  Mendel (gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516), Ubuntu 16.04 (gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

I tried to install the TFLITE Runtime using 

https://www.tensorflow.org/lite/guide/python#run_an_inference_using_tflite_runtime

When I tried to import as follows.

`from tflite_runtime import Interpreter`

I get the following error in both devices (Ubuntu 16.04 and Mendel)

`>>> from tflite_runtime import Interpreter
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
`

But when I just do 

`import tflite_runtime`

Doesn't give me an error in either platform. 
I tried using IntelliSense and it shows me no Interpreter API. 

Am I doing something wrong? How to fix this issue? 




**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31368,Internal compiler error,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): Docker image latest-gpu-py3
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.1
- GPU model and memory: RTX 2080 Ti / 12 GB

I have created a fully-quantized tf lite model from a saved model. But trying to compile it with the edgetpu_compiler, I get an error:

```
user@ubuntu:~/tf/tensorflow1_14$ edgetpu_compiler saved_converted_linearmodel_tpu_1.14.0.tflite 
Edge TPU Compiler version 2.0.258810407
INFO: Initialized TensorFlow Lite runtime.

Internal compiler error. Aborting!
```
Error message is unfortunately not very helpful. The non-compiled version is loadable and produces the correct results.

I have attached the model that I try to compile, as well as its visualization (via visualize.py).

[litemodel.tar.gz](https://github.com/tensorflow/tensorflow/files/3472515/litemodel.tar.gz)
"
31367,Operation 'ExtractImagePatches' has no attr named '_XlaCompile',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):b'unknown' 1.12.0
- Python version:Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0 7.0.5
- GPU model and memory: GTX 1080 8G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Can't take gradients, fails with InvalidArgumentError: Operation 'ExtractImagePatches' has no attr named '_XlaCompile'
**Describe the expected behavior**
Getting gradients without error
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
inputs = np.array([[[x + y * 10 for x in [1,2]] for y in range(5)]])
inputs = inputs.reshape((1,1,inputs.shape[1],inputs.shape[2]))
print(""inputs:"", inputs)

import tensorflow as tf
tf.reset_default_graph()
with tf.Graph().as_default() as g:
    x = tf.placeholder(""float"", [1, 1, None, 2])
    y = tf.extract_image_patches(images=x, ksizes=[1, 1, 3, 1], strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding='VALID')
    gradient = tf.gradients(y, x)
    init = tf.global_variables_initializer()
    
with tf.Session(graph=g) as sess:
    sess.run(init)
    print(sess.run([y], feed_dict = {x:inputs}))

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31365,tf.summary.histogram broken in tf-nightly-gpu-2.0-preview,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-dev20190806
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Calling `tf.summary.histogram` on a non eager tensor results in an exception: ```OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.```

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
inputs = tf.keras.layers.Input(shape=(300,))
tf.summary.histogram(""in"", inputs)
```


"
31364,Tons of warnings just by importing tf (2.0.0-beta1),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-beta1
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14

**Describe the current behavior**

I just created a virtual environnement to test tensorflow 2.0.0-beta1 with no other package installed. Just calling `import tensorflow` launched many warning messages. It is strange that I already use 2.0.0-beta1 in other virtual environnements and never saw these warnings before.

**Code to reproduce the issue**

In bash:
```
python3 -m venv testenv
source testenv/bin/activate
pip install tensorflow==2.0.0-beta1
python
```
Then in python:
```
import tensorflow as tf
# print(tf.version.GIT_VERSION, tf.version.VERSION)
 ```

**Warning messages**
```
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
```
"
31363,tf.function changes behavior of += operator on tf.Tensors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install (wheel?)
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.7.3
- GPU model and memory: CPU


**Describe the current behavior**
Behaviour of += operator on a tensor is different when called via tf.function and when called directly

**Describe the expected behavior**
tf.function does not change the behavior of any operations on tensors.

**Code to reproduce the issue**
```
import tensorflow as tf
print(tf.__version__)

class First:
    def __init__(self, initial: int):
        self.value = tf.constant(initial)

    def increment(self):
        self.value += 1

    def __str__(self):
        return f'Object with tensor = {self.value}'

@tf.function
def increment(obj: First):
    obj.increment()

c1 = First(100)

c1.increment()
print(c1)

increment(c1)
print(c1)
```

**Other info / logs**
output:
```
Object with tensor = 101
Object with tensor = Tensor(""add:0"", shape=(), dtype=int32)
```

Expected output:

```
Object with tensor = 101
Object with tensor = 102
```
"
31362,Non-tensorflow code gets executed only on the first run of tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install (wheel?)
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.7.3
- GPU model and memory: CPU

**Describe the current behavior**
Non-tensorflow code gets executed only on the first run of tf.function

**Describe the expected behavior**
Either exception is thrown or the code is executed every time. As it is, it just welcomes bugs.

**Code to reproduce the issue**
```
import tensorflow as tf

print(tf.__version__)

class MyObj:
    def __init__(self):
        self.value = 0

obj = MyObj()

@tf.function
def with_py_side_effect(tensorflow_stuff, o):

    # do my complex tf stuff
    ...

    o.value += 1
    return tensorflow_stuff

for i in range(5):
    print(i, obj.value)
    a = with_py_side_effect(None, obj)
```

**Other info / logs**
My output:
```
/Users/ikkamens/.pyenv/versions/3.7.3/bin/python /Users/ikkamens/Library/Preferences/PyCharmCE2019.2/scratches/tf_foo_python.py
2019-08-06 11:53:11.888584: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2.0.0-beta1
0 0
1 1
2 1
3 1
4 1

Process finished with exit code 0
```"
31361,UniqueV2 reports incorrect output shape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04.6 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.5.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0.130
- GPU model and memory: Tesla K40C, 11441MiB

**Describe the current behavior**
When not using eager execution, UniqueV2 always reports its first output to have rank 1.

**Describe the expected behavior**
UniqueV2 should report its first output to have the same rank as its input.

**Code to reproduce the issue**
The bug can be exposed by forcing non-eager execution through `tf.function` or `tf.compat.v1.disable_eager_execution()`. The former is demonstrated below:
```py
import tensorflow as tf
from tensorflow.python.ops import gen_array_ops

def unique_rank(x, axis):
    unique = gen_array_ops.unique_v2(x, axis)
    return tf.rank(unique[0])

# 2D input should produce a 2D output
x = tf.ones([2, 2])
print(""UniqueV2 output 0 rank:"", tf.function(unique_rank)(x, [0]))
print(""UniqueV2 output 0 rank executing eagerly:"", unique_rank(x, [0]))
```
This outputs
```
UniqueV2 output 0 rank: tf.Tensor(1, shape=(), dtype=int32)
UniqueV2 output 0 rank executing eagerly: tf.Tensor(2, shape=(), dtype=int32)
```
but should output
```
UniqueV2 output 0 rank: tf.Tensor(2, shape=(), dtype=int32)
UniqueV2 output 0 rank executing eagerly: tf.Tensor(2, shape=(), dtype=int32)
```
"
31360,Using tf.feature_columns in exported estimators fails when using tf.feature_columns.indicator_column,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

### System information
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: binary
- TensorFlow version: 1.14.0
- Python version: 3.6.8
- CUDA/cuDNN version: CPU

Tensorflow Serving information:
Using tensorflow serving docker image, tag latest, as of 05/08/2019 (August 5th):
- TensorFlow ModelServer: 1.14.0-rc0
- TensorFlow Library: 1.14.0

### Describe the current behavior
_[I originally opened an issue at tensorflow/serving](https://github.com/tensorflow/serving/issues/1409), but I got redirected here as it is a tensorflow issue._

Tensorflow serving doesn't handle a 'feature_columns input layer' in the Estimator model_fn.
When using tf.feature_columns.input_layer or tf.keras.layer.DenseFeatures to process feature_columns in the model_fn: If you have a feature_column that is a categorical_column wrapped by an indicator_column, Tensorflow serving fails.

Tensorflow serving doesn't seem to properly handle the indicator_column. It responds with:
```python
{
    ""error"": ""Input to reshape is a tensor with <n> values, but the requested shape has <n squared>\n\t [[{{node input_layer/<feature name>_indicator/Reshape}}]]""
}
```
[The reshape op where the error is thrown](https://github.com/tensorflow/tensorflow/blob/29ecfbf1e7ab2f073e69770753174667079d10b5/tensorflow/core/kernels/reshape_op.h#L92)

[I asked around on stackoverflow](https://stackoverflow.com/questions/57327655/is-there-a-way-to-export-custom-tensorflow-r1-14-estimators-that-are-able-to-p), if there were workarounds, no response so far.

The main advantage of tf.feature_columns happens to be the indicator_column (which allows for easy one-hot encoding of features in the model code). It is also pushed in multiple Tensorflow guides as something that's used. **I think this bug blocks practical use of the tf.feature_columns module in production.**

_When not using the indicator_column as a feature_column, all seems well_

### Describe the expected behavior
Tensorflow serving should just serve inference requests, (as it does when not using indicator_column)

### Code to reproduce the issue
Script to export saved_models from estimators that use feature_columns:
```
""""""Code for testing tensorflow serving reshape bug""""""

import tensorflow as tf

feature_columns = [
    # Feature columns that use indicator column
    tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity('test', 2))
]

estimator_params = {
    'feature_columns': feature_columns
}


def model_fn(features, labels=None, mode=None, params=None):
    is_training = (mode == tf.estimator.ModeKeys.TRAIN)

    inputs = tf.feature_column.input_layer(features, params['feature_columns'])

    if not is_training:
        return tf.estimator.EstimatorSpec(
            mode,
            predictions=inputs
        )

    a = tf.Variable(1, dtype=tf.float32, trainable=True)

    # Doesn't need to train, but the model needs to be trainable for exporting to work
    loss = tf.reduce_mean(a * inputs)

    optimizer = params.get('optimizer', None) or tf.train.AdamOptimizer(learning_rate=0.001)
    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())

    return tf.estimator.EstimatorSpec(
        mode=mode,
        loss=loss,
        train_op=train_op
    )


def input_fn():
    return {'test': tf.constant([1, 0], dtype=tf.int64)}, tf.constant([3, 2], dtype=tf.float32)


def serving_input_fn():
    receiver_tensors = {
        'test': tf.placeholder(tf.int64, shape=[None, 1], name='test')
    }

    return tf.estimator.export.ServingInputReceiver(receiver_tensors, receiver_tensors)


# Custom estimator
estimator = tf.estimator.Estimator(model_fn=model_fn, params=estimator_params)
# Canned estimator
# estimator = tf.estimator.DNNRegressor([2, 2, 1], feature_columns=feature_columns)

estimator.train(input_fn=input_fn, steps=5)

estimator.export_saved_model('./', serving_input_fn)
```
Serve the generated saved_model with Tensorflow serving.
Now make requests to it.

Example body for custom estimators:
```
{
	""inputs"": {
		""test"": [0, 1]
	}
}
```
Example body for the canned estimator:
```
{
	""signature_name"": ""predict"",
	""inputs"": {
		""test"": [0, 1]
	}
}
```

### Source code / logs
All I get as response from tensorflow serving:
```
{
    ""error"": ""Input to reshape is a tensor with 2 values, but the requested shape has 4\n\t [[{{node input_layer/test_indicator/Reshape}}]]""
}
```
It seems to always be: ""... with _n_ values"", ""... requested shape has _n squared_"". Always in the reshape op.

I think it may be due to improper (de)serialization to or from Saved Model when using tf.keras.DenseFeatures or tf.feature_column.input_layer.
It may also be some bugged tensorflow op (just guessing)

"
31359,tflite output different result with pbfile when using only one convolutional layer ?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): In a Ubuntu18.04 docker container.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow==1.14
- TensorFlow version (use command below): tf-cpu==1.14.0
- Python version: python3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No.
- GPU model and memory: No.

**Describe the current behavior**
tflite output different result with pbfile when using only one convolutional layer ?

**Describe the expected behavior**
tflite should output the same result with pbfile when using only one convolutional layer.

**Code to reproduce the issue**
```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import warnings
warnings.filterwarnings(action='ignore', category=FutureWarning)

######################################################################

import numpy as np
import tensorflow as tf
from tensorflow.python.tools.freeze_graph import freeze_graph

CONFIG = tf.compat.v1.ConfigProto()
CONFIG.gpu_options.allow_growth = True

######################################################################

inputs_debug = [""inpT""]
outputs_debug = [""outT""]
shape_debug = (1, 32, 240, 1)
dtype_debug = np.float32
pbPath_debug = './debug/fuck.pb'
litePath_debug = './debug/fuck.lite'
checkpoint_debug = './debug/fuck.ckpt'

######################################################################

def convert(pbPath, litePath, inputs, outputs):
    from_frozen_graph = tf.lite.TFLiteConverter.from_frozen_graph
    converter = from_frozen_graph(pbPath, inputs, outputs)

    tflite_model = converter.convert()
    open(litePath, ""wb"").write(tflite_model)

######################################################################

def generate_data(shape, dtype=np.float32):
    data = np.array(np.random.randint(0, 255, shape), dtype=dtype)
    if dtype == np.float32: data = (data - 127.0) / 128.0

    return [data]

######################################################################

def get_tf_engine(pbPath, inpNs, outNs):
    inpNs = [inpN + "":0"" for inpN in inpNs]
    outNs = [outN + "":0"" for outN in outNs]

    graph = tf.compat.v1.Graph()
    with graph.as_default():
        f = tf.io.gfile.GFile(pbPath, ""rb"")
        graphDef = tf.compat.v1.GraphDef()
        graphDef.ParseFromString(f.read())

        Ts = tf.import_graph_def(
            graphDef, name='',
            return_elements=inpNs + outNs,
        )
        inpT, outT = Ts[:len(inpNs)], Ts[len(inpNs):]
        session = tf.compat.v1.Session(config=CONFIG)

    def get_tf_target(data):
        return session.run(
            outT,
            {iT: d for (iT, d) in zip(inpT, data)}
        )

    return get_tf_target

######################################################################

def get_lite_engine(litePath):
    interpreter = tf.lite.Interpreter(litePath)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    outputs_details = interpreter.get_output_details()

    def get_lite_output(data):
        for (input_detail, d) in zip(input_details, data):
            interpreter.set_tensor(input_detail['index'], d)
        interpreter.invoke()

        return [
            interpreter.get_tensor(outputs_detail['index'])
            for outputs_detail in outputs_details
        ]

    return get_lite_output

######################################################################

if __name__ == '__main__':
    if not os.path.exists(checkpoint_debug+"".meta""):
        with tf.compat.v1.Graph().as_default():
            inpT = tf.compat.v1.placeholder(
                dtype_debug, shape_debug, inputs_debug[0])

            x = inpT
            x = tf.layers.conv2d(x, 64, (3,3), (1,1), 'same', dilation_rate=(1,1))

            outT = tf.identity(x, outputs_debug[0])

            saver = tf.compat.v1.train.Saver()
            with tf.compat.v1.Session(config=CONFIG) as session:
                session.run(tf.compat.v1.global_variables_initializer())

                saver.save(session, checkpoint_debug)

    if not os.path.exists(pbPath_debug):
        freeze_graph(
            input_graph=None,
            input_saver=None,
            input_binary=True,
            input_checkpoint=checkpoint_debug,
            output_node_names=','.join(outputs_debug),
            restore_op_name=None,
            filename_tensor_name=None,
            output_graph=pbPath_debug,
            clear_devices=True,
            initializer_nodes=None,
            variable_names_whitelist="""",
            variable_names_blacklist="""",
            input_meta_graph=checkpoint_debug+"".meta"",
            input_saved_model_dir=None,
        )

    if not os.path.exists(litePath_debug):
        convert(pbPath_debug, litePath_debug, inputs_debug, outputs_debug)

    get_tf_target = get_tf_engine(
        pbPath_debug, inputs_debug, outputs_debug)
    get_lite_output = get_lite_engine(litePath_debug)

    for i in range(10):
        data = generate_data(shape_debug, dtype_debug)

        targets = get_tf_target(data)
        outputs = get_lite_output(data)

        for (target, output) in zip(targets, outputs):
            print(target.shape, output.shape, end='\t')
            print(np.allclose(target, output, 1e-5, 1e-8), end='\n')

```

**I use the code above to generate a pb file with only one convolutional layer, and convert it to a tflite file. And the output of the pb and tflite files are different as following. I wonder why ?**

```
INFO: Initialized TensorFlow Lite runtime.
(1, 32, 240, 64) (1, 32, 240, 64)       False
(1, 32, 240, 64) (1, 32, 240, 64)       False
(1, 32, 240, 64) (1, 32, 240, 64)       False
(1, 32, 240, 64) (1, 32, 240, 64)       False
(1, 32, 240, 64) (1, 32, 240, 64)       False
(1, 32, 240, 64) (1, 32, 240, 64)       False
(1, 32, 240, 64) (1, 32, 240, 64)       False
(1, 32, 240, 64) (1, 32, 240, 64)       False
(1, 32, 240, 64) (1, 32, 240, 64)       False
(1, 32, 240, 64) (1, 32, 240, 64)       False

```
"
31358,[freeze_graph]Node def expected inputs do not match with tf.keras layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Anaconda binary (```tensorflow-gpu```)
- TensorFlow version (use command below): ```b'unknown' 1.13.1```
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A (Because of conda binary)
- GPU model and memory: GTX 1080Ti, 11Gb

**Describe the current behavior**
When I try to freeze my TF graph that uses ```tf.keras``` layers I get the following exception.

```
Traceback (most recent call last):
  File ""/home/stefan/miniconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 426, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node model/batch_normalization_v1/cond/Const_2}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/stefan/Documents/work/asr/reproduce_bug/bug.py"", line 100, in <module>
    export()
  File ""/home/stefan/Documents/work/asr/reproduce_bug/bug.py"", line 95, in export
    filename_tensor_name='')
  File ""/home/stefan/miniconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py"", line 146, in freeze_graph_with_def_protos
    _ = importer.import_graph_def(input_graph_def, name="""")
  File ""/home/stefan/miniconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/stefan/miniconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 430, in import_graph_def
    raise ValueError(str(e))
ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node model/batch_normalization_v1/cond/Const_2}}
```

It seems to be related to the ```tf.keras.layers.BatchNormalization```. When I pass the ```training=True``` or ```training=False``` to it the error goes away.

**Describe the expected behavior**
The graph should freeze no matter the ```training``` parameter.

**Code to reproduce the issue**
Standalone script required to reproduce.
```
import os

import tensorflow as tf
from tensorflow.python.keras.datasets import mnist
from tensorflow.python.tools import freeze_graph


class Model:
    def __init__(self):
        self._conv = tf.keras.layers.Conv2D(filters=64, kernel_size=5, activation=tf.nn.relu)
        self._batch_norm = tf.keras.layers.BatchNormalization()
        self._flatten = tf.keras.layers.Flatten()
        self._logits = tf.keras.layers.Dense(units=10)

    def forward(self, inputs):
        with tf.name_scope('model'):
            conv_out = self._conv(inputs)
            norm_out = self._batch_norm(conv_out)
            flat_out = self._flatten(norm_out)
            logits = self._logits(flat_out)

        return tf.identity(logits, name='logits')

    @staticmethod
    def loss(logits, labels):
        with tf.name_scope('loss'):
            loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

        return loss


def train():
    def map_fn(x, y):
        return tf.expand_dims(tf.math.divide(tf.cast(x, tf.float32), 255), axis=2), tf.cast(y, tf.int32)

    (x_train, y_train), (_, _) = mnist.load_data()

    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    dataset = dataset.map(map_fn).batch(128).repeat(10)

    iterator = dataset.make_one_shot_iterator()
    inputs, labels = iterator.get_next()

    model = Model()
    logits = model.forward(inputs)
    loss = model.loss(logits, labels)

    with tf.name_scope('optimizer'):
        optimizer = tf.train.AdamOptimizer(learning_rate=1e4).minimize(loss,
                                                                       global_step=tf.train.get_or_create_global_step())

    with tf.train.MonitoredTrainingSession(checkpoint_dir='./tmp') as sess:
        while not sess.should_stop():
            print(sess.run([optimizer, loss])[1])


def export():
    tf.reset_default_graph()

    export_dir = './tmp/frozen'
    model = Model()

    inputs = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1], name='input_placeholder')
    _ = model.forward(inputs)

    with open('./tmp/checkpoint') as f:
        line = f.readline()
        checkpoint = line[line.find('""') + 1:line.rfind('""')]
        print(checkpoint)

    with tf.Session(graph=tf.get_default_graph()) as sess:
        saver = tf.train.Saver()

        sess.run(tf.global_variables_initializer())
        print(os.path.join('./tmp', checkpoint))
        saver.restore(sess, os.path.join('./tmp', checkpoint))

        builder = tf.saved_model.Builder('./tmp/frozen')
        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING], strip_default_attrs=True)
        builder.save()

        graph = tf.graph_util.remove_training_nodes(sess.graph.as_graph_def(), protected_nodes=['logits'])

        freeze_graph.freeze_graph_with_def_protos(
            input_graph_def=graph,
            input_saver_def=None,
            input_saved_model_dir=export_dir,
            saved_model_tags=[tf.saved_model.tag_constants.SERVING],
            input_checkpoint=os.path.join('./tmp', checkpoint),
            output_node_names='logits',
            output_graph=os.path.join('./tmp', 'frozen.pb'),
            clear_devices=True,
            initializer_nodes='',
            restore_op_name='',
            filename_tensor_name='')


if __name__ == '__main__':
    train()
    export()

```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31357,Android Studio using build.gradle appears Failed to find 'android-25',"**System information**
- OS Platform and Distribution :Windows 10
- Mobile device : Xiao Mi Mix2
- TensorFlow version: 1.10.0
- Python version: 3.6

**Describe the problem**
Android Studio:

6:47:20 Gradle sync started
6:47:32 Gradle sync failed: Failed to find target with hash string 'android-25' in: D:\software\SDK
        Consult IDE log for more details (Help | Show Log)

When I use the 'build.gradle', it delivers this problem
So I change buildToolVersion to 21, it delivers as the follows:

* What went wrong:
A problem occurred evaluating root project 'cmake'.
> Plugin with id 'com.android.library' not found.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

In the past , we success build the apk(not me), but not anymore. 

relative source:
https://github.com/LGXNOTLGX/androidAudioRecg

help me please"
31355,Custom OPS with registering gradient functions error in eager backprop,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): PIP
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: V10.0.130 / V7.0.5
- GPU model and memory: NVIDIA TITAN Xp

**Describe the current behavior**
If I run the code with directly
```
2019-08-06 11:32:33.687311: E tensorflow/stream_executor/cuda/cuda_driver.cc:1003] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-08-06 11:32:33.687356: E tensorflow/stream_executor/gpu/gpu_timer.cc:78] Invalid argument: error recording CUDA event on stream 0x7fce8a062590: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-08-06 11:32:33.687558: F ./tensorflow/core/kernels/reduction_gpu_kernels.cu.h:644] Non-OK-status: CudaLaunchKernel( ColumnReduceKernel<IN_T, T*, Op>, grid_dim, block_dim, 0, cu_stream, in, (T*)temp_storage.flat<int8_t>().data(), extent_x, extent_y, op, init) status: Internal: an illegal memory access was encountered
Aborted (core dumped)
```

If I run the code with `cuda-memcheck --binary-patching no python train_simple.py`
```
Traceback (most recent call last):
  File ""train_simple.py"", line 71, in <module>
    grads = tape.gradient(loss, model.trainable_variables)
  File ""/media/disk1/fordata/web_server/kangfu/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 1002, in gradient
    unconnected_gradients=unconnected_gradients)
  File ""/media/disk1/fordata/web_server/kangfu/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py"", line 76, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File ""/media/disk1/fordata/web_server/kangfu/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 137, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/media/disk4/fordata/web_server/kangfu/HDRNet-TF/hdrnet_ops.py"", line 24, in _bilateral_slice_grad
    grid_tensor, guide_tensor, input_tensor, grad, has_offset=has_offset)
  File ""<string>"", line 355, in bilateral_slice_apply_grad
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Failed launch BilateralSliceApplyGradKernel. [Op:BilateralSliceApplyGrad]
```
**Describe the expected behavior**
The code should calculate gradient properly without error. The custom ops run correctly in inferring mode, however, it throws bug during `model.fit` or below:
```
with tf.GradientTape() as tape:
    output = model(_full_res)
    loss = tf.keras.losses.mse(output, _full_res)
grads = tape.gradient(loss, model.trainable_variables)
```

**Code to reproduce the issue**
I define the custom ops like below:

```
REGISTER_KERNEL_BUILDER(Name(""BilateralSlice"").Device(DEVICE_GPU), BilateralSliceOp);
REGISTER_KERNEL_BUILDER(Name(""BilateralSliceGrad"").Device(DEVICE_GPU), BilateralSliceGradOp);
REGISTER_KERNEL_BUILDER(Name(""BilateralSliceApply"").Device(DEVICE_GPU), BilateralSliceApplyOp);
REGISTER_KERNEL_BUILDER(Name(""BilateralSliceApplyGrad"").Device(DEVICE_GPU), BilateralSliceApplyGradOp);
```
It seems like the custom ops not work OK in eager mode
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31354,Tensorflow library query API,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
Mac / Windows / Linux - x86 + Aarch64 

- TensorFlow version (you are using):
1.11.0 and upwards

- Are you willing to contribute it (Yes/No):
If I can help I will gladly

**Describe the feature and the current behavior/state.**
Presently tensorflow_framework and it's friends (e.g. the _jni version etc) use cpu_feature_guard / cpu_info to check if your CPU supports the instruction set the build was performed with and complains if you're missing features / extra features.

There should be a way to programmatically query the library and CPU (+ GPU???) to discover what feature set the library was compiled with etc.

Presently there are a couple of un-exposed methods that do some of the desired functionality...

InfoAboutUnusedCPUFeatures and TestCPUFeature are examples of useful features that could be exposed via a c_api extension

What I propose is that the library expose the features used to compile it and the features the host is capable of supporting with various overlapping calls allowing the caller to identify the following cases...

1) The library matches the host perfectly (very rare)
2) The library includes features not supported by the host
3) The library works on the host but does not use all available features (a-la feature-guard)
 
(2) + (3) should return a set of unsupported / extra features

**Will this change the current api? How?**
No, it can be implemented as an extra C API

**Who will benefit with this feature?**
Potentially any user who experiences a crash from unsupported features or could reasonably expect better performance from a rebuild

My personal experience showed a 100% improvement in speed on my Mac and Linux machines using CPU-only builds. On GPU builds the change is, of course, trivial

As Tensorflow is increasingly becoming a hobbyist interest - ""Build a brain out of a Raspberry Pi"" fluff pieces it seems advisable to cater for the non academic / commercial user

**Any Other info.**
Potentially this feature would allow external API developers to check for host issues before starting rather than, at worst, dump the core (see this happen several times)"
31352,(Windows) ptxas_utils.cc(54): 'tensorflow::SubProcess' unimplemented class?,"tags: 2.0.0-beta0 subtype:windows type:build/install

**System information**
- OS Platform and Distribution: Windows 10
- Mobile device: -
- TensorFlow installed from: Source
- TensorFlow version: r2.0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: NO, pip is installed, NO
- Bazel version: 26.0
- GCC/Compiler version: -
- CUDA/cuDNN version: 10.1 / 7.6.2
- GPU model and memory: GTX 1060 3GB
- CPU: i7-860 (AVX unsupported) (64-bit)

**Describe the problem**
I know CUDA 10.1/VS2019 is unsupported for 2.0 currently, but the error may have nothing to do with that? Tensorflow's SubProcess class is not implemented for Windows? Is this a necessity, how is Tensorflow-gpu else compiled for Windows?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`python ./configure`
```
bazel build --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings
```


**Any other info / logs**
```
C:/users/admin/documents/git-tf/tensorflow/tensorflow/stream_executor/cuda/BUILD:97:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:ptxas_utils' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/admin/_bazel_admin/52rkf7yz/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\Admin\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\Admin\AppData\Local\Temp
  C:/Program Files/Python36/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-py2-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-py2-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-py2-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-py2-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-py2-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-py2-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-py2-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-py2-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-py2-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-py2-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-py2-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-py2-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-py2-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Iexternal/nsync/public /Ibazel-out/x64_windows-py2-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-py2-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-py2-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-py2-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-py2-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-py2-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/cuda/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI -nvcc_options=disable-warnings /Fobazel-out/x64_windows-py2-opt/bin/tensorflow/stream_executor/cuda/_objs/ptxas_utils/ptxas_utils.o /c tensorflow/stream_executor/cuda/ptxas_utils.cc
Execution platform: @bazel_tools//platforms:host_platform
tensorflow/stream_executor/cuda/ptxas_utils.cc(54): error C2039: 'SetProgram': is not a member of 'tensorflow::SubProcess'
.\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'
tensorflow/stream_executor/cuda/ptxas_utils.cc(55): error C2039: 'SetChannelAction': is not a member of 'tensorflow::SubProcess'
.\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'
tensorflow/stream_executor/cuda/ptxas_utils.cc(56): error C2039: 'Start': is not a member of 'tensorflow::SubProcess'
.\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'
tensorflow/stream_executor/cuda/ptxas_utils.cc(62): error C2039: 'Communicate': is not a member of 'tensorflow::SubProcess'
.\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'
tensorflow/stream_executor/cuda/ptxas_utils.cc(192): error C2039: 'SetProgram': is not a member of 'tensorflow::SubProcess'
.\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'
tensorflow/stream_executor/cuda/ptxas_utils.cc(193): error C2039: 'SetChannelAction': is not a member of 'tensorflow::SubProcess'
.\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'
tensorflow/stream_executor/cuda/ptxas_utils.cc(195): error C2039: 'Start': is not a member of 'tensorflow::SubProcess'
.\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'
tensorflow/stream_executor/cuda/ptxas_utils.cc(199): error C2039: 'Communicate': is not a member of 'tensorflow::SubProcess'
.\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1863.507s, Critical Path: 326.27s
INFO: 2580 processes: 2580 local.
FAILED: Build did NOT complete successfully
```"
31351,"tf.gradients with ""ValueError: Cannot create a tensor proto whose content is larger than 2GB.""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary(pip tensorflow-gpu)
- TensorFlow version (use command below): 1.14.0 (v1.14.0-rc1-22-gaf24dc91b5)
- Python version: 3.6.7
- Bazel version (if compiling from source): Noe
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: 10.1/7.6.1
- GPU model and memory: 1080ti (12GB)

**Describe the current behavior**
Hi, 
I tried to use tf.gradeints to calculate `integrated-gradients` which is one of the Explainability Methods. To do that, I have to treat ~30,000 x ~30,000 matrix(pred) like the following code. My task is Knowledge-Graph.

```python
pred = sess.run(model, feed_dict=feed_dict)
tf_grads = tf.gradients(pred, placeholders)
```

However, I encountered `ValueError: Cannot create a tensor proto whose content is larger than 2GB.`. Is this bug? or Are there some methods to avoid this issue?

Thank you in advance,

ref) https://github.com/ankurtaly/Integrated-Gradients

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
```
File ""/home/ono/ws/GraphCNN_kg/gcn_modules/visualization.py"", line 302, in cal_feature_IG_for_kg                                                                                                                                                                                                                                                                        
    visualizer.cal_integrated_gradients(sess, placeholders, out_prediction, divide_number)                                                                                                                                                                                                                                                                                
  File ""/home/ono/ws/GraphCNN_kg/gcn_modules/visualization.py"", line 264, in cal_integrated_gradients                                                                                                                                                                                                                                                                     
    out = tf.reduce_sum(prediction)                                                                                                                                                                                                                                                                                                                                       
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func                                                                                                                                                                                                                                            
    return func(*args, **kwargs)                                                                                                                                                                                                                                                                                                                                          
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 1410, in reduce_sum_v1                                                                                                                                                                                                                                          
    return reduce_sum(input_tensor, axis, keepdims, name)                                                                                                                                                                                                                                                                                                                 
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py"", line 180, in wrapper                                                                                                                                                                                                                                                
    return target(*args, **kwargs)                                                                                                                                                                                                                                                                                                                                        
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 1457, in reduce_sum                                                                                                                                                                                                                                             
    input_tensor, _ReductionDims(input_tensor, axis), keepdims,                                                                                                                                                                                                                                                                                                           
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 1345, in _ReductionDims                                                                                                                                                                                                                                         
    return range(0, array_ops.rank(x))                                                                                                                                                                                                                                                                                                                                    
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py"", line 180, in wrapper                                                                                                                                                                                                                                                
    return target(*args, **kwargs)                                                                                                                                                                                                                                                                                                                                        
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 486, in rank                                                                                                                                                                                                                                                   
    return rank_internal(input, name, optimize=True)                                                                                                                                                                                                                                                                                                                      
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 506, in rank_internal                                                                                                                                                                                                                                          
    input_tensor = ops.convert_to_tensor(input)                                                                                                                                                                                                                                                                                                                           
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1087, in convert_to_tensor                                                                                                                                                                                                                                     
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)                                                                                                                                                                                                                                                                                                      
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1145, in convert_to_tensor_v2                                                                                                                                                                                                                                  
    as_ref=False)                                                                                                                                                                                                                                                                                                                                                         
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1224, in internal_convert_to_tensor                                                                                                                                                                                                                            
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)                                                                                                                                                                                                                                                                                                   
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 305, in _constant_tensor_conversion_function                                                                                                                                                                                                           
    return constant(v, dtype=dtype, name=name)                                                                                                                                                                                                                                                                                                                            
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 246, in constant                                                                                                                                                                                                                                       
    allow_broadcast=True)                                                                                                                                                                                                                                                                                                                                                 
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 284, in _constant_impl                                                                                                                                                                                                                                 
    allow_broadcast=allow_broadcast))                                                                                                                                                                                                                                                                                                                                     
  File ""/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 537, in make_tensor_proto                                                                                                                                                                                                                              
    ""Cannot create a tensor proto whose content is larger than 2GB."")                                                                                                                                                                                                                                                                                                     
ValueError: Cannot create a tensor proto whose content is larger than 2GB. 
```
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31350, ModuleNotFoundError: No module named 'tensorflow.contrib',"Have spent a couple days troubleshooting.
Am getting the following error

> ModuleNotFoundError: No module named 'tensorflow.contrib'

The offending line is

> import tensorflow.contrib.tensorrt as trt

Here are my setup specs

> Windows 10
> 
> Python 3.6.8
> 
> CUDA 10.0
> 
> cuDNN v 7.6.2
> 
> Tensorflow (gpu) 1.14.0
> 
> GeForce GTX 960M
> 
> Driver version 431.60
> 
> Intel Core i7-6700HQ 2.6 GHz*
> 

Any feedback or troubleshooting steps appreciated!
"
31349,Adding custom op in 1.14,"I noticed there was a same topic #30632. The starter of the issue switches to tf 1.12 and solves the problem. But the problem is still not resolved. When I follow the instruction and try to compile the cuda kernel, I change the cuda_kernel_helper.h to gpu_kernel_helper.h and run into the same problem.

python2.7/site-packages/tensorflow/include/tensorflow/core/util/gpu_kernel_helper.h:22:10: fatal error: third_party/gpus/cuda/include/cuda_fp16.h: No such file or directory
 #include ""third_party/gpus/cuda/include/cuda_fp16.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Can someone figure out a reason behind this error? I pip installed tensorflow. Should I build from source?

"
31343,Behavior of tf.data.Dataset when `steps_per_epoch` is set,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
[tf.data.dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)
[model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)

## Description of issue (what needs changing):
My `tf.data.Dataset` does not have `repeat` set which means it should go forever. At the end of `steps_per_epoch`, does the `tf.data.Dataset` shuffle itself? Or does it pick up from where it left off? Or does it reset? 

I couldn't find a clear explanation online from the googling I did. My dataset is about 14 million examples, and the loss seems to be decreasing between epochs (with `steps_per_epoch` set). I'm just worried that it's fitting on the same X samples again and again

It's not entirely clear to me what is happening in the background with `fit`"
31335,A custom operator get Segmentation Fault in tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version:
3.6.8
- GCC/Compiler version (if compiling from source):
g++ 7.4.0

**Describe the current behavior**
When calling a custom op from a python function with tf.function, I got a segmentation fault.
The op run normally without tf.function.

**Code to reproduce the issue**
I implemented a custom op with [zero_out_op_kernel_1.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/adding_an_op/zero_out_op_kernel_1.cc) in tensorflow repository.

I called the operator from the following python code.

```python
import tensorflow as tf

_zero_out_module = tf.load_op_library('custom_ops.so')
zero_out = _zero_out_module.zero_out

@tf.function
def make_zero(x):
    return zero_out(x)

c = tf.constant([4,2,8,9])
res  = make_zero(c)
```

**Other info / logs**
I confirmed that InferenceContext is NULL.
This bug is similar to #30494


"
31334,Can't install a version of TF with filter_for_shard,"Hey, 

I'm using a program which calls for filter_for_shard (tf.data.experimetnal.filter_for_shard). The API claims it should exist in TF 1.13, but I have not been able to get this to work (or find the file) with any installation of TF 1.13. Any ideas on what to install to get this working?

Thanks in advance!"
31332,,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31331,Error freezing saved model if it contains a tf.keras.layer.BatchNormalisation layer,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 1903
- TensorFlow installed from: pip
- TensorFlow version: 1.14.0
- Python version: 3.7.3
- GPU model and memory: RTX 2080 Ti 

**Problem**

To make a frozen graph, I first create a saved model using `tf.saved_model.simple_save` and then freeze it using `tensorflow.python.tools.freeze_graph.freeze_graph`.

If a model contains some `tf.keras.layers.BatchNormalisation` layers, freezing will fail in TF 1.14.0 with:

`ValueError: Tensor name 'batch_normalization/cond/ReadVariableOp/Switch:1' is invalid.`

TF 1.13.1 does not give an error

**Code to reproduce the issue**

```
import tensorflow as tf
import tensorflow.keras.backend as K
import os
import datetime
from tensorflow.python.tools import freeze_graph
from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input, Activation, BatchNormalization
from tensorflow.keras.models import Model

inputs = Input(shape=(128, 128, 1))
x = Conv2D(4, (3, 3))(inputs)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Flatten()(x)
x = Dense(5, activation='softmax')(x)
model = Model(inputs, x, name='test')
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

K.set_learning_phase(0)
save_dir = ""./tmp_{:%Y-%m-%d_%H%M%S}"".format(datetime.datetime.now())
tf.saved_model.simple_save(K.get_session(),
                           save_dir,
                           inputs={""input"": model.inputs[0]},
                           outputs={""output"": model.outputs[0]})

freeze_graph.freeze_graph(None,
                          None,
                          None,
                          None,
                          model.outputs[0].op.name,
                          None,
                          None,
                          os.path.join(save_dir, ""frozen_model.pb""),
                          False,
                          """",
                          input_saved_model_dir=save_dir)
```

**Update:**

Seems to be a problem in `graph_util_impl.py`, in particular https://github.com/tensorflow/tensorflow/commit/0f486fc67070ba888204741c404a55a5f1a41fbc#diff-2d2827fd48cee6884e3587c901ad6952

@gargn 

If I change this file back to its 1.13 version there is no more error.

**Update 2:**

I put `K.set_learning_phase(0)` before creating the model, then it works. I don't know what effect this has though? Does it just turn off batch normalisation altogether?

**Update 3**

Final remarks:
- Putting `K.set_learning_phase(0)` before creating the model will let it save, however the Batch Normalisation layer doesn't seem to do anything (updating turned off?) so it is not a solution.
- Changing `graph_util_impl.py` to its 1.13.1 version will let it save without error, however there will be an error `ValueError: Input 0 of node batch_normalization/cond/ReadVariableOp/Switch was passed float from batch_normalization/gamma:0 incompatible with expected resource.` when loading the frozen graph from the protobuf.
- The workaround is to save the model weights, clear the session (so that tensor names are not different because of having two graphs), set learning phase to 0, recreate the model, load the weights, and then freeze (example code in my comment [below](https://github.com/tensorflow/tensorflow/issues/31331#issuecomment-518655879))

"
31330,tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name:  An op that loads optimization parameters into HBM for embedding.,"When I run the code I report the following problem

`from tensorflow.examples.tutorials.mnist import input_data`
`dataSet = input_data.read_data_sets(""MNIST_data/"", one_hot=True)`

Traceback (most recent call last):
  File ""/home/steven/idea/project/tensorflow/study01/testMinst.py"", line 6, in <module>
    from tensorflow.examples.tutorials.mnist import input_data
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/__init__.py"", line 21, in <module>
    from tensorflow.examples.tutorials.mnist import input_data
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py"", line 30, in <module>
    from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py"", line 48, in <module>
    from tensorflow.contrib import distribute
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/__init__.py"", line 34, in <module>
    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/tpu_strategy.py"", line 27, in <module>
    from tensorflow.contrib.tpu.python.ops import tpu_ops
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/__init__.py"", line 69, in <module>
    from tensorflow.contrib.tpu.python.ops.tpu_ops import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py"", line 39, in <module>
    resource_loader.get_path_to_datafile(""_tpu_ops.so""))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/load_library.py"", line 60, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: 
An op that loads optimization parameters into HBM for embedding. Must be
preceded by a ConfigureTPUEmbeddingHost op that sets up the correct
embedding table configuration. For example, this op is used to install
parameters that are loaded from a checkpoint before a training loop is
executed."
31329,auto-deeplab: nas_network runs into ValueError with personal data,"Hello,

I've been trying to use auto deeplab on my own dataset. The only part I touched is the stem creation. Instead of feeding a image set of dimension [?, y, x, 3] I feed data of dimension [?, y, x, 13]. The first 10 features of the last dimension correspond to character embeddings, and the last 3 are unrelated floats in the [0, 1] range. There are no NANs in this dataset.

To do this, I replace the beginning of the _nas_net function in nas_network.py. I get an array with shape [?, y, x, 4] as input, the first feature is a char ID to be replaced by a 10 dimensional embedding, the 3 other features stay as they are. This leads to a [?, y, x, 13] tensor which is then used by the rest of the code.

As my input seems sensible enough, I believe this may be a bug with the implementation. Could someone investigate this further, please?

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tried on both ubuntu 18.04 and windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.3
- CUDA/cuDNN version: 10.1/ 7.6
- GPU model and memory: Nvidia Geforce GTX 1080TI 11GB on ubuntu, CPU on windows

**Describe the current behavior**
program crashes with error pasted at the bottom of this issue

**Describe the expected behavior**
auto deeplab keeps going as expected

**Code to reproduce the issue**
replace the content of /tensorflow/models/research/deeplab/core/nas_network.py by [this code](https://pastebin.com/jFu8EUxT) and run nas_network.py directly. The only modification I brought to this file, besides the new __main__ part at the bottom to run a simple fail case, can be found in the _nas_stem function.


**Other info / logs**
```
Traceback (most recent call last):
  File ""C:/Users/lrizzello/source/repos/Company/Company.Python/deeplab/core/nas_network.py"", line 308, in <module>
    hnasnet(all_features, pipeline_parameters, n_classes)
  File ""C:/Users/lrizzello/source/repos/Company/Company.Python/deeplab/core/nas_network.py"", line 294, in hnasnet
    final_endpoint=final_endpoint)
  File ""C:/Users/lrizzello/source/repos/Company/Company.Python/deeplab/core/nas_network.py"", line 183, in _build_nas_base
    cell_num=cell_num)
  File ""C:\Users\lrizzello\source\repos\Company\Company.Python\deeplab\core\nas_cell.py"", line 82, in __call__
    h = h1 + h2
  File ""C:\Users\lrizzello\AppData\Local\Continuum\anaconda3\envs\py37\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 812, in binary_op_wrapper
    return func(x, y, name=name)
  File ""C:\Users\lrizzello\AppData\Local\Continuum\anaconda3\envs\py37\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 374, in add
    ""Add"", x=x, y=y, name=name)
  File ""C:\Users\lrizzello\AppData\Local\Continuum\anaconda3\envs\py37\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\lrizzello\AppData\Local\Continuum\anaconda3\envs\py37\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\lrizzello\AppData\Local\Continuum\anaconda3\envs\py37\lib\site-packages\tensorflow\python\framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""C:\Users\lrizzello\AppData\Local\Continuum\anaconda3\envs\py37\lib\site-packages\tensorflow\python\framework\ops.py"", line 1823, in __init__
    control_input_ops)
  File ""C:\Users\lrizzello\AppData\Local\Continuum\anaconda3\envs\py37\lib\site-packages\tensorflow\python\framework\ops.py"", line 1662, in _create_c_op
    raise ValueError(str(e))
ValueError: Dimensions must be equal, but are 14 and 13 for 'hnasnet/cell_5/comb_iter_0/combine/add' (op: 'Add') with input shapes: [?,14,25,40], [?,13,25,40].
```
"
31328,I wonder if the tflite supports GCN(Graph Convolution Network)?,
31327,libtensorflow_framework.so No such file or directory,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.14.0
- Python version: Py36
- Bazel version (if compiling from source): bazel release 0.26.1
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version:
- GPU model and memory:

I tried to include the op in the code as follows
```
    idtable_op_module = tf.load_op_library(
    resource_loader.get_path_to_datafile('libid_table_op.so'))
```

only to get the traceback infomation:
```
Traceback (most recent call last):
  File ""wide_deep.py"", line 12, in <module>
    import jarvis.tensorflow as jtf
  File ""/opt/ml/job/python/jarvis/tensorflow/__init__.py"", line 4, in <module>
    from jarvis.tensorflow.feature_transform import *
  File ""/opt/ml/job/python/jarvis/tensorflow/feature_transform.py"", line 10, in <module>
    from jarvis.tensorflow import id_column
  File ""/opt/ml/job/python/jarvis/tensorflow/id_column.py"", line 44, in <module>
    from jarvis.tensorflow.id_table_ops import IdHashTable
  File ""/opt/ml/job/python/jarvis/tensorflow/id_table_ops.py"", line 29, in <module>
    resource_loader.get_path_to_datafile('libid_table_op.so'))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/load_library.py"", line 61, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so: cannot open shared object file: No such file or directory
```

The trackback shows that there's no `libtensorflow_framework.so`, I tried to find this file in tf 1.14.0 binary version(not compiled version) and only found `site-packages/tensorflow_core/libtensorflow_framework.so.1`, but still no `libtensorflow_framework.so`. So what's the correct way to include `so` file, is it a bug in tf 1.14?"
31325,reduce_max on empty int32 array returns -2147483648,"**System information**
- Have I written custom code: **no**
- OS Platform and Distribution: **Linux**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version: **1.14.0**
- Python version: **2.7**

**Describe the current behavior**
```python
# Call reduce_max on an empty Tensor.
x = tf.reduce_max(tf.zeros([0], tf.int32))
with tf.Session() as sess:
  print x.eval()
# prints -2147483648
```
**Describe the expected behavior**
Would be better if `reduce_max` threw an error when fed an empty Tensor."
31324,Passing a Variable as learning_rate to Adam optimizer does not work as expected,"tag:bug_template

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary through pip3
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: sys.version_info(major=3, minor=5, micro=6, releaselevel='final', serial=0)
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.0.130_410.48 / 10.0-linux-x64-v7.4.2.24
- GPU model and memory: GeForce GTX 1080 with 7598 MB memory

**Describe the current behavior**

If a tf.Variable is passed as learning_rate to the Adam optimizer, and the variable is later changed that does not seem to affect the optimizer. Instead, the optimizer seems to ""cache"" the value of the variable at the time when the optimizer was constructed.

**Describe the expected behavior**

My expectation was that if I pass a tf.Variable as the learning_rate argument to tf.keras.optimizers.Adam(), and later assign a new value to the variable, that would affect the optimization.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION)

import sys
print(sys.version_info)

tf_a = tf.Variable(1.0)
print('Variable tf_a initialized to {}.'.format(tf_a.numpy()))

tf_lr = tf.Variable(0.1, trainable=False)

tf_opt = tf.keras.optimizers.Adam(learning_rate=tf_lr)

@tf.function
def train_step():
    with tf.GradientTape() as tf_tape:
        tf_loss = tf_a**2
        
    tf_gradients = tf_tape.gradient(tf_loss, [tf_a])

    tf_opt.apply_gradients(zip(tf_gradients, [tf_a]))

print('After one step with learning rate {}... '.format(tf_lr.numpy()), end='')
train_step()
print('Variable tf_a is {}.'.format(tf_a.numpy()))

tf_lr.assign(0.0)

for _ in range(10):
    print('After another step, now with learning rate {}... '.format(tf_lr.numpy()), end='')
    train_step()
    print('Variable tf_a is {}.'.format(tf_a.numpy()))
```

The code above produces the following output on my system:

```
v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
sys.version_info(major=3, minor=5, micro=6, releaselevel='final', serial=0)
Variable tf_a initialized to 1.0.
After one step with learning rate 0.10000000149011612... Variable tf_a is 0.8999971747398376.
After another step, now with learning rate 0.0... Variable tf_a is 0.8004083633422852.
After another step, now with learning rate 0.0... Variable tf_a is 0.7015821933746338.
After another step, now with learning rate 0.0... Variable tf_a is 0.6039347052574158.
After another step, now with learning rate 0.0... Variable tf_a is 0.5079591274261475.
After another step, now with learning rate 0.0... Variable tf_a is 0.41423195600509644.
After another step, now with learning rate 0.0... Variable tf_a is 0.3234161138534546.
After another step, now with learning rate 0.0... Variable tf_a is 0.23625943064689636.
After another step, now with learning rate 0.0... Variable tf_a is 0.1535806804895401.
After another step, now with learning rate 0.0... Variable tf_a is 0.07624538242816925.
After another step, now with learning rate 0.0... Variable tf_a is 0.005127914249897003.
```
As you can see tf_a keeps changing at a fast pace. My expectation was that after setting the learning rate variable to 0.0 updates would no-longer change tf_a.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31323,Passing a callable learning_rate to Adam optimizer does not work as documented,"tag:bug_template

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary through pip3
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: sys.version_info(major=3, minor=5, micro=6, releaselevel='final', serial=0)
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.0.130_410.48 / 10.0-linux-x64-v7.4.2.24
- GPU model and memory: GeForce GTX 1080 with 7598 MB memory

**Describe the current behavior**

The Adam optimizer does not seem to keep calling the supplied learning_rate callable. It seems like it's being called once or a few times, but then a ""cached"" value is repeatedly used in updates.

**Describe the expected behavior**

According to the [documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/Adam) it should be possible to pass a ""callable that takes no arguments and returns the actual value to use"" as learning_rate to tf.keras.optimizers.Adam(), and this ""can be useful for changing these values across different invocations of optimizer functions"".

My expectation was that the Adam optimizer would keep calling the supplied callable at each update (i.e. from within apply_gradients()).

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION)

import sys
print(sys.version_info)

tf_a = tf.Variable(1.0)
print('Variable tf_a initialized to {}.'.format(tf_a.numpy()))

lr = 0.1
def get_lr():
    global lr
    return lr

tf_opt = tf.keras.optimizers.Adam(learning_rate=get_lr)

@tf.function
def train_step():
    with tf.GradientTape() as tf_tape:
        tf_loss = tf_a**2
        
    tf_gradients = tf_tape.gradient(tf_loss, [tf_a])

    tf_opt.apply_gradients(zip(tf_gradients, [tf_a]))

print('After one step with learning rate {}... '.format(get_lr()), end='')
train_step()
print('Variable tf_a is {}.'.format(tf_a.numpy()))

lr = 0.0

for _ in range(10):
    print('After another step, now with learning rate {}... '.format(get_lr()), end='')
    train_step()
    print('Variable tf_a is {}.'.format(tf_a.numpy()))
```

The code above produces the following output on my system:

```
v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
sys.version_info(major=3, minor=5, micro=6, releaselevel='final', serial=0)
Variable tf_a initialized to 1.0.
After one step with learning rate 0.1... Variable tf_a is 0.8999971747398376.
After another step, now with learning rate 0.0... Variable tf_a is 0.8004083633422852.
After another step, now with learning rate 0.0... Variable tf_a is 0.7015821933746338.
After another step, now with learning rate 0.0... Variable tf_a is 0.6039347052574158.
After another step, now with learning rate 0.0... Variable tf_a is 0.5079591274261475.
After another step, now with learning rate 0.0... Variable tf_a is 0.41423195600509644.
After another step, now with learning rate 0.0... Variable tf_a is 0.3234161138534546.
After another step, now with learning rate 0.0... Variable tf_a is 0.23625943064689636.
After another step, now with learning rate 0.0... Variable tf_a is 0.1535806804895401.
After another step, now with learning rate 0.0... Variable tf_a is 0.07624538242816925.
After another step, now with learning rate 0.0... Variable tf_a is 0.005127914249897003.
```
As you can see tf_a keeps changing at a fast pace. My expectation was that after setting the learning rate to 0.0 updates would no-longer change tf_a.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31321,Build C API with visual compiler 2010,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.13
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): Visual Studio compiler 2010
- CUDA/cuDNN version: 10
- GPU model and memory: N/A



**Describe the problem**
Hello,

I would like to build tensorflow C api dll for cpu using compiler 2010.

Is this possible? As i have tried but there are errors produced using bazel or CMake
Bazel errors is as following 
**Provide the exact sequence of commands / steps that you executed before running into the problem**

1- Run C:\ProgramData\Anaconda3\python.exe configure.py with the default configuration
2- Run bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=monolithic //tensorflow:libtensorflow.so


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

WARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
INFO: Invocation ID: 35bf2202-8201-4eeb-8b4a-696872ec1a86
INFO: Build option --conlyopt has changed, discarding analysis cache.
INFO: Analysed target //tensorflow:libtensorflow.so (0 packages loaded, 7854 targets configured).
INFO: Found 1 target...
ERROR: C:/users/l-madham/_bazel_l-madham/enzg4re4/external/protobuf_archive/BUILD:259:1: C++ compilation of rule '@protobuf_archive//:protoc_lib' failed (Exit 2): cl.exe failed: error executing command
  cd C:/users/l-madham/_bazel_l-madham/enzg4re4/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Microsoft SDKs\Windows\v7.0A\include;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Windows\Microsoft.NET\Framework64\v3.5;C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 10.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 10.0\Common7\Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v7.0A\bin\NETFX 4.0 Tools\x64;C:\Program Files (x86)\Microsoft SDKs\Windows\v7.0A\bin\x64;C:\Program Files (x86)\Microsoft SDKs\Windows\v7.0A\bin;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages
    SET TEMP=C:\Users\l-madham\AppData\Local\Temp
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\l-madham\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 10.0/VC/bin/amd64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw -w -mavx -mavx2 -mfma -mfpmath=both -msse4.2 /DHAVE_PTHREAD /wd4018 /wd4514 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/protoc_lib/plugin.obj /c external/protobuf_archive/src/google/protobuf/compiler/plugin.cc
Execution platform: @bazel_tools//platforms:host_platform
cl : Command line warning D9002 : ignoring unknown option '/Gw'
cl : Command line warning D9002 : ignoring unknown option '-mavx'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mfma'
cl : Command line warning D9002 : ignoring unknown option '-mfpmath=both'
cl : Command line warning D9002 : ignoring unknown option '-msse4.2'
external/protobuf_archive/src\google/protobuf/stubs/mutex.h(33) : fatal error C1083: Cannot open include file: 'mutex': No such file or directory
Target //tensorflow:libtensorflow.so failed to build
INFO: Elapsed time: 14.823s, Critical Path: 1.04s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
"
31320,Build error TensorFlow Lite for ARM64,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Jetson Nano
- TensorFlow installed from (source or binary): - 
- TensorFlow version: master
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: -
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**
Two build errors occurred.
1.neon_tensor_utils.cc
```
tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc: In function int32x4_t tflite::tensor_utils::RoundToNearest(float32x4_t):
tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc:883:79: error: invalid static_cast from type uint32x4_t {aka __vector(4) unsigned int} to type int32x4_t {aka __vector(4) int}
   const int32x4_t mask = static_cast<int32x4_t>(vcltq_f32(input, zero_val_dup));
```
2.quant_lstm_sup.cc
```
make: *** No rule to make target '/root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/nnapi/quant_lstm_sup.o', needed by '/root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a'.  Stop.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed [https://www.tensorflow.org/lite/guide/build_arm64](https://www.tensorflow.org/lite/guide/build_arm64) .
[Cross-compile for arm64](https://www.tensorflow.org/lite/guide/build_arm64#cross-compile_for_arm64) and [Compile natively on arm64](https://www.tensorflow.org/lite/guide/build_arm64#compile_natively_on_arm64) are the same result.

Cross-compile for ARM64
```
sudo apt-get update
sudo apt-get install crossbuild-essential-arm64
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
./tensorflow/lite/tools/make/download_dependencies.sh
./tensorflow/lite/tools/make/build_aarch64_lib.sh
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

1.neon_tensor_utils.cc
The place where a build error occurs seems to be as follows.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc#L883-L884
vcltq_f32 can not be cast to int32x4_t, so I think it should be changed to vcvtq_f32_u32.
```
  const uint32x4_t mask = vcltq_f32(input, zero_val_dup);
  const float32x4_t casted_mask = vcvtq_f32_u32(mask);
```

2.quant_lstm_sup.cc
quant_lstm_sup.cc has moved. see this commit (1ffdcbe).
However, the Makefile has not been changed.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/make/Makefile#L170"
31319,CUDA driver version is insufficient for CUDA runtime version,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
kernel version 4.9.0
LInux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version:
Tensorflow 1.14 
Tensorflow-gpu 1.14
- Python version:
Python 3.7.3
- Installed using virtualenv? pip? conda?:
conda
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
Not from source but gcc is 6.3.0
- CUDA/cuDNN version:
cuDNN : 7.6.0
- GPU model and memory:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   44C    P0    70W / 149W |      0MiB / 11441MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+


**Describe the problem**

So I am trying to run a tensorflow dependent code, after fixing compatibility issues with my current tensorflow version, I run into this issue raised but from Nividia website, it looks to me the cuda meets the requirement. So I am wondering if this is from tensorflow. Is there any version pf tensorflow that's compatible with my system requirements and cuda?
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Not applicable. I didn't bother to install any driver so as not to break my system. I only try to run my code and got to this issue.

**Any other info / logs**
Traceback (most recent call last):
  File ""TF.py"", line 48, in <module>
    sess = tf.compat.v1.Session(config=sess_config)
  File ""/home/jfadugba/miniconda3/envs/peexoo/lib/python3.7/site-packages/tensorf
low/python/client/session.py"", line 1570, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/jfadugba/miniconda3/envs/peexoo/lib/python3.7/site-packages/tensorf
low/python/client/session.py"", line 693, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. St
atus: CUDA driver version is insufficient for CUDA runtime version
"
31318,InvalidArgumentError: Cannot assign a device for operation embedding_1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I have written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS version: **Fedora 29.5.1.18 (also tested Ubuntu 18.10)**
- TensorFlow installed from (source or binary): **tensorflow/tensorflow:latest-gpu-py3-jupyter**
- TensorFlow version (use command below): **1.14.0**
- Python version: **3.6.8**
- CUDA/cuDNN version: **10.0.130**
- GPU model and memory: **GeForce RTX 2080 ti, 11 Gb**

**Describe the current behavior**
I'm using keras. I try to to fit model that contains **Embedding** layer. When I call `model.fit_generator(...)` I get an error: 

_InvalidArgumentError: Cannot assign a device for operation embedding/embeddings/Initializer/random_uniform/sub: Could not satisfy explicit device specification '' because the node {{colocation_node embedding/embeddings/Initializer/random_uniform/sub}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU 
Const: GPU CPU XLA_CPU XLA_GPU 
ResourceSparseApplyRMSProp: CPU 
RandomUniform: GPU CPU XLA_CPU XLA_GPU 
ReadVariableOp: GPU CPU XLA_CPU XLA_GPU 
Sub: GPU CPU XLA_CPU XLA_GPU 
Add: GPU CPU XLA_CPU XLA_GPU 
Mul: GPU CPU XLA_CPU XLA_GPU 
VarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU 
VarHandleOp: GPU CPU XLA_CPU XLA_GPU 
AssignVariableOp: GPU CPU XLA_CPU XLA_GPU 
ResourceGather: GPU CPU XLA_CPU XLA_GPU_ 

ResourceSparseApplyRMSProp looks strange for me.

After getting this error I cannot fit new (simplified) model, because I get this error again. I get this error even I run `tensorflow.keras.backend.get_value(model.optimizer.lr)`

**Describe the expected behavior**
Model fits without any problems, like the same model without Embedding layer (no _dest_input_).
**Code to reproduce the issue**

```
lstm_input = Input(shape=(30, 5,))
steady_input = Input(shape=(3,),
                    name='steady_float') # None  shape  ?
dest_input = Input(shape=(1,), name='steady_dest')
ns_input = Input(shape=(1,))

x1 = layers.Bidirectional(layers.LSTM(512, activation='relu', return_sequences=True))(lstm_input)
x1 = layers.Bidirectional(layers.LSTM(256, activation='relu'))(x1)
x1 = Model(inputs=lstm_input, outputs=x1)

x2 = layers.Dense(512, activation=""relu"")(steady_input)
x2 = Model(inputs=steady_input, outputs=x2)

x3 = layers.Embedding(12, 3)(dest_input)
x3 = layers.Flatten()(x3)
x3 = layers.Dense(512, activation=""relu"")(x3)
x3 = Model(inputs=dest_input, outputs=x3)

x = layers.concatenate([x1.output, x2.output, x3.output])
x = layers.Dense(128, activation='relu')(x)

y1_output_tensor = layers.Dense(5, name='y1')(x)
y2_output_tensor = layers.Dense(5, name='y2')(x)
model = Model(inputs=[x1.input, x2.input, x3.input],
                         outputs=[y1_output_tensor, y2_output_tensor])

ep_n = 200
learning_rate = 0.001
decay_rate = learning_rate / ep_n
momentum = 0.7
model.compile(optimizer=RMSprop(lr=learning_rate, momentum=momentum, decay=decay_rate), loss=['mae', 'mae'])

#train_gen and test_get - simple generatora with shuffle
batch_size = 128
history = model.fit_generator(train_gen,
                              steps_per_epoch=1000,
                              epochs=ep_n,
                              validation_data=test_gen,
                              validation_steps=X_test.shape[0]//batch_size)
```

**Other info / logs**
Some times I get this error on simplified model:

```
# define two sets of inputs
inputA = Input(shape=(1,))
inputB = Input(shape=(128,))
 
# the first branch operates on the first input
x = Embedding(1000, 3)(inputA)
x = layers.Flatten()(x)
x = layers.Dense(4096, activation=""relu"")(x)
x = layers.Dense(2048, activation=""relu"")(x)
x = layers.Dense(1024, activation=""relu"")(x)
x = layers.Dense(512, activation=""relu"")(x)
x = layers.Dense(256, activation=""relu"")(x)
x = Dense(128, activation=""relu"")(x)
x = Dense(64, activation=""relu"")(x)
x = Model(inputs=inputA, outputs=x)
 
# the second branch opreates on the second input
y = Dense(1024, activation=""relu"")(inputB)
y = Dense(512, activation=""relu"")(y)
y = Dense(256, activation=""relu"")(y)
y = Dense(128, activation=""relu"")(y)
y = Dense(64, activation=""relu"")(y)
y = Model(inputs=inputB, outputs=y)
 
# combine the output of the two branches
combined = layers.concatenate([x.output, y.output])
 
# apply a FC layer and then a regression prediction on the
# combined outputs
z = Dense(2, activation=""relu"")(combined)
z = Dense(1, activation=""linear"")(z)
 
# our model will accept the inputs of the two branches and
# then output a single value
model = Model(inputs=[x.input, y.input], outputs=z)

ep_n = 10
learning_rate = 0.001
decay_rate = learning_rate / ep_n
momentum = 0.7
model.compile(optimizer=RMSprop(lr=learning_rate, momentum=momentum, decay=decay_rate

input_array_a = np.random.randint(1000, size=(500000, 1))
input_array_b = np.random.randint(32, size=(500000, 128))
output_array = np.random.randint(9, size=(500000, 1))

def generator_shuffle(x_a, x_b, y, batch_size=1024):
    max_index = len(x_a) - 1
    while 1:
        rows = np.random.randint(0, max_index, batch_size)
        yield [x_a[rows], x_b[rows]], y[rows]

tr_gen = generator_shuffle(input_array_a,
                           input_array_b,
                           output_array)

history = model.fit_generator(tr_gen, steps_per_epoch=3, epochs=10)
```
Probably, the issue occuring depends on CPU usage.

I'm attaching full log.

[gpu_error.txt](https://github.com/tensorflow/tensorflow/files/3464780/gpu_error.txt)
"
31317,[Sum/Average Loss Reduction Strategy] Why going with Sum as default ?,"TF 1.14.0 introduced the following:

> Set default loss reduction as AUTO for improving reliability of loss scaling with distribution strategy and custom training loops. AUTO indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE. When used in distribution strategy scope, outside of built-in training loops such as tf.keras compile and fit, we expect reduction value to be 'None' or 'SUM'. Using other values will raise an error. 

Source: https://github.com/tensorflow/tensorflow/releases/tag/v1.14.0

----------

Why `SUM_OVER_BATCH_SIZE` was made the default when the followings are standard practices:
- averaging the loss over one batch 
- averaging gradients over multiple ranks

Could anyone give examples of use-cases where sum would give better results than average ? And why would this made default when averaging seems far more used in the state of the art ?"
31315,AttributeError: module 'tensorflow' has no attribute 'gfile' - Version 2.0.0-alpha0,"Tensorflow Version - **2.0.0-alpha0**

Error occurred while running a notebook from the Tensorflow site  - [**Build a linear model with Estimators**](https://www.tensorflow.org/tutorials/estimators/linear )
<br>
Download the dataset:
```
from official.wide_deep import census_dataset
from official.wide_deep import census_main

census_dataset.download(""/tmp/census_data/"")
```

> ---------------------------------------------------------------------------
> AttributeError                            Traceback (most recent call last)
> <ipython-input-9-fa1d43ace000> in <module>()
>       2 from official.wide_deep import census_main
>       3 
> ----> 4 census_dataset.download(""/tmp/census_data/"")
> 
> /content/models/official/wide_deep/census_dataset.py in download(data_dir)
>      76 def download(data_dir):
>      77   """"""Download census data if it is not already present.""""""
> ---> 78   **tf.gfile.MakeDirs(data_dir)**
>      79 
>      80   training_file_path = os.path.join(data_dir, TRAINING_FILE)
> 
> **AttributeError: module 'tensorflow' has no attribute 'gfile'**
<hr>

## The document suggests the following changes in the file.
### _Should I go about making the following changes manually in the file census_dataset.py ?_
WARNING: Logging before flag parsing goes to stderr.
W0625 16:04:36.412110 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:78: The name tf.gfile.MakeDirs is deprecated. Please use **tf.io.gfile.makedirs** instead.

W0625 16:04:36.413802 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:81: The name tf.gfile.Exists is deprecated. Please use **tf.io.gfile.exists** instead.

W0625 16:04:38.253764 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:62: The name tf.gfile.Open is deprecated. Please use **tf.io.gfile.GFile** instead.

W0625 16:04:38.488776 139807458662144 deprecation_wrapper.py:119] From /tmpfs/src/temp/site/en/tutorials/estimators/models/official/wide_deep/census_dataset.py:73: The name tf.gfile.Remove is deprecated. Please use **tf.io.gfile.remove** instead. 
#28632 https://github.com/tensorflow/tensorflow/issues/28632
#https://github.com/google/gin-config/issues/9
#https://github.com/tobegit3hub/simple_tensorflow_serving/issues/45"
31312,RAM usage during fit() on a tf.keras model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: conda repository
- **TensorFlow version (use command below)**: 2.0.1
- **Python version**: 3.7.3
- **CUDA/cuDNN version**: CUDA 10, cuDNN 7.6
- **GPU model and memory**: GeForce GTX 1050 Ti, 4 GB memory

### Describe the problem
Situation: running the `fit` command on a `tf.keras` model.
Problem: When running on CPU, the `fit` command takes ~25MB. When running on GPU, it takes **~970MB**.


### Source code / logs
Use the following code, in a script called `test.py`:
```
from memory_profiler import profile
from tensorflow import keras
import tensorflow as tf
import numpy as np


@profile
def test():
    optimizer = keras.optimizers.Adam(lr=1e-3)
    loss = keras.losses.BinaryCrossentropy()

    model = keras.models.Sequential()
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(1))

    model.compile(loss=loss, optimizer=optimizer, metrics=['acc'])

    num_samples = 1280
    batch_size = 64

    inputs = np.random.uniform(size=(num_samples, 100, 100, 3)).astype(np.float32)
    targets = np.random.randint(0, 2, size=(num_samples, 1)).astype(np.int32)

    history = model.fit(inputs, targets, epochs=3, validation_data=(inputs, targets))

test()
```

and use the following commands, in an environment with tensorflow-gpu and **memory_profiler**:
`export CUDA_VISIBLE_DEVICES=0; python -m memory_profiler test.py`
`export CUDA_VISIBLE_DEVICES=""-1""; python -m memory_profiler test.py`

The outputs I get:
```
Line #    Mem usage    Increment   Line Contents
================================================
     9    249.9 MiB    249.9 MiB   @profile
    10                             def test():
    11    249.9 MiB      0.0 MiB       optimizer = keras.optimizers.Adam(lr=1e-3)
    12    249.9 MiB      0.0 MiB       loss = keras.losses.BinaryCrossentropy()
    13                             
    14    252.6 MiB      2.7 MiB       model = keras.models.Sequential()
    15    252.6 MiB      0.0 MiB       model.add(keras.layers.Flatten())
    16    252.6 MiB      0.0 MiB       model.add(keras.layers.Dense(1))
    17                             
    18    252.9 MiB      0.3 MiB       model.compile(loss=loss, optimizer=optimizer, metrics=['acc'])
    19                             
    20    252.9 MiB      0.0 MiB       num_samples = 1280
    21    252.9 MiB      0.0 MiB       batch_size = 64
    22                             
    23    399.6 MiB    146.6 MiB       inputs = np.random.uniform(size=(num_samples, 100, 100, 3)).astype(np.float32)
    24    399.6 MiB      0.0 MiB       targets = np.random.randint(0, 2, size=(num_samples, 1)).astype(np.int32)
    25                             
    26   1383.0 MiB    983.5 MiB       history = model.fit(inputs, targets, epochs=3, validation_data=(inputs, targets))
```
```
Line #    Mem usage    Increment   Line Contents
================================================
     9    249.8 MiB    249.8 MiB   @profile
    10                             def test():
    11    249.8 MiB      0.0 MiB       optimizer = keras.optimizers.Adam(lr=1e-3)
    12    249.8 MiB      0.0 MiB       loss = keras.losses.BinaryCrossentropy()
    13                             
    14    252.6 MiB      2.7 MiB       model = keras.models.Sequential()
    15    252.6 MiB      0.0 MiB       model.add(keras.layers.Flatten())
    16    252.9 MiB      0.3 MiB       model.add(keras.layers.Dense(1))
    17                             
    18    252.9 MiB      0.0 MiB       model.compile(loss=loss, optimizer=optimizer, metrics=['acc'])
    19                             
    20    252.9 MiB      0.0 MiB       num_samples = 1280
    21    252.9 MiB      0.0 MiB       batch_size = 64
    22                             
    23    399.5 MiB    146.6 MiB       inputs = np.random.uniform(size=(num_samples, 100, 100, 3)).astype(np.float32)
    24    399.5 MiB      0.0 MiB       targets = np.random.randint(0, 2, size=(num_samples, 1)).astype(np.int32)
    25                             
    26    429.3 MiB     29.8 MiB       history = model.fit(inputs, targets, epochs=3, validation_data=(inputs, targets))
```

**Moreover**, when I try to use the InceptionV3 model, I get the following results:
RAM used when training on GPU, using the `fit` method: 13.4 GB
RAM used when training on GPU, using a custom training loop: 943 MB
RAM used when training on CPU, using the `fit` method: 13 GB
RAM used when training on CPU, using a custom training loop: 582 MB"
31309,Custom loss function fails with sample_weight and batch_size > 1,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Debian 9.9
- TensorFlow installed from: conda (-c anaconda)
- TensorFlow version: 1.14.0
- Python version: 3.7.3
- GPU model and memory: n/a - tested in CPU mode

**Describe the current behavior**

An error occurs when training an LSTM with a custom loss function, using `sample_weight` and `batch_size > 1`. The error does not occur if `batch_size = 1`, or if `sample_weight = None`.

**Describe the expected behavior**

I would expect custom loss functions to work irrespective of batch size and sample weights.

**Code to reproduce the issue**

Heres a minimal example:
```python
import numpy as np
import tensorflow as tf

batch_size = 32  # no problem if this is 1
sequence_len = 1
embedding_size = 100

x_train = np.random.randn(batch_size, sequence_len, embedding_size)
y_train = np.random.randn(batch_size, embedding_size)
sample_weight = np.random.randn(batch_size)  # no problem if this is None

train_input = tf.keras.Input(shape=(sequence_len, embedding_size),
                             batch_size=batch_size)

lstm_layer = tf.keras.layers.LSTM(200,
                                  return_sequences=False,
                                  )(train_input)

dense_layer = tf.keras.layers.Dense(embedding_size,
                                    )(lstm_layer)

model = tf.keras.models.Model(inputs=train_input, outputs=dense_layer)

model.summary()

# Custom loss function. This function could of course be replaced with
# tf.keras.losses.mean_squared_error, but I have a use case where I need a
# custom loss function.
class customLoss(tf.keras.losses.Loss):
    def call(self, y_true, y_pred):
        return tf.reduce_mean(tf.math.squared_difference(y_true, y_pred))

model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),
              loss=customLoss())

loss = model.train_on_batch(x_train,
                            y=y_train,
                            sample_weight=sample_weight)

```

**Other info / logs**

In #29026, @pavithrasv has pointed out that loss functions from `tf.losses` do not work with keras, and suggested to use loss functions from `tf.keras.losses` instead (thanks again!). Consequently, I thought that defining a custom loss function using the `tf.keras.losses.Loss` base class should be possible. (Please note that in my actual use case I have a more complex custom loss function for which I need some math operations from `tf.math`.)

Traceback:
```bash
Traceback (most recent call last):
  File ""/home/john/PhD/GitLab/literary_lstm/bug_minimal_example_03.py"", line 38, in <module>
    sample_weight=sample_weight)
  File ""/home/john/miniconda3/envs/py_tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1175, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File ""/home/john/miniconda3/envs/py_tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py"", line 3292, in __call__
    run_metadata=self.run_metadata)
  File ""/home/john/miniconda3/envs/py_tf/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1458, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got 32
	 [[{{node loss_1/dense_loss/weighted_loss/Squeeze}}]]
```
"
31308,TF 2.0 nigtly 190803 generates errors W0803 during prediction,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): !pip install tf-nightly-gpu-2.0-preview
- TensorFlow version: 2.0.0-dev20190803
- Python version: 
- Installed using virtualenv? pip? conda?: !pip install tf-nightly-gpu-2.0-preview
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
When running predictions I receive a great number of errors with the same information thus impeding performance. No such problem for TF 2.0.0-beta1
Sample of error text:
W0803 19:06:51.695915 140613664049024 training_utils.py:1211] When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
forecast = []
results = []
for time in range(len(series) - window_size):
  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))

forecast = forecast[split_time-window_size:]
results = np.array(forecast)[:, 0, 0]


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31307,Performance decline after updating from 1.13.1 to 1.14.0,"I have observed an obvious performance decline after I update TF from 1.13.1 to 1.14.0, even if the code has never changed. 
I am using CUDA 10.0 and tf.contrib.opt.AdamWOptimizer"
31306,A correct way to use tf.contrib.opt.AdamWOptimizer,"tf.contrib.opt.AdamWOptimizer requires two arguments: weight_decay and learning_rate. Since learning_rate is usually decayed along with the training, should weight_decay also be decayed with the same schedule? Would you please provide an example? "
31305,Parallel sessions for Comp Graph editing and optimization,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No):
yes


**Describe the feature and the current behavior/state.**
It dosnt seen to be possible to do this in Tensorflow. Ive read that pyTorch has this ability although never tested it since iam developing in Tensorflow for a couple of months now (and Tensorflow has a handful of libs and other langs api what make me not want to go test the other frame)

**Will this change the current api? How?**
Maybe, i dont know really, i wanna you opinion if it will change the API and if it is desirable.

**Who will benefit with this feature?**
Although the infra host machine has to be very powerful it's really a benefit to have a ability to change your comp graph on the fly while the other graph trains, it speeds up the researching process.

**Any Other info.**
Iam not talking about editing the same graph that is optimizing, but editing one instance diverse of graph then that one that is optimizing"
31304,Constructor for diagonal arrays (like numpy.diag),"**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): No

At the moment the diagonal matrix constructor `tf.linalg.tensor_diag` does not support a second argument to specify which diagonal to fill. Something like the second argument in `numpy.diag` would be great.

This allows to simplify the process of generating powers of super- or sub-diagonal matrices by allowing a user to compute the off-diagonal elements directly and then filling the appropriate diagonal of a new array. Useful also to generate matrix exponentials of super- and sub-diagonal arrays.
"
31303,Change `kernel_initializer` in some `tf.keras` layers for improved performance,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: P100

**Describe the current behavior**
The current `Conv2D` and `dense` layers in the `tf.keras` package have `glorot_uniform` as the default kernel initializer which doesn't play well with advanced activations like `relu`, `prelu`, `selu`, etc.  Given the fact that `relu` is a default choice for modern architectures, the kernel should be initialized from a different distribution.

**Describe the expected behavior**
The `kernel initializer` should be changed to `he_uniform` in `Conv`, `SeparableConv`, `DepthwiseConv` and `Dense` layers from this

```
__init__(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
```

to this
```
__init__(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer='he_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
```

**Other info / logs**
Here are the resources that you can look into:
1) The original paper for `Kaiming init`: https://arxiv.org/pdf/1502.01852.pdf
2) Blog post: https://towardsdatascience.com/why-default-cnn-are-broken-in-keras-and-how-to-fix-them-ce295e5e5f2
3) Blog post: https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79"
31302,bazel build benchmark_model for android_arm failed In MacOS,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
macOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
1.14.0
- Python version:
3.5
- Installed using virtualenv? pip? conda?:
anconda
- Bazel version (if compiling from source):
0.24.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
clang-1001.0.46.4
- GPU model and memory:



**Describe the problem**
bazel build benchmark_model for android_arm failed In MacOS
**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build -c opt --config=android_arm   tensorflow/lite/tools/benchmark:benchmark_model --verbose_failures

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

it seems cannot find /bin/false,  but I found the file /usr/bin/false 
so I tried:  ln -s /bin/false /usr/bin/false ,  but It's not permitted
Then I replace  /bin/false  to /usr/bin/false in total workspace and bazel clean &&  rebuild ,
It' s still not work.
 it's only worked when I remove the  --config=android_arm

==============================================
bazel build -c opt --config=android_arm   tensorflow/lite/tools/benchmark:benchmark_model --verbose_failures
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=125
INFO: Reading rc options for 'build' from /Users/80256276/workspace/github/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include
INFO: Reading rc options for 'build' from /Users/80256276/workspace/github/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/80256276/workspace/anaconda3/envs/tensorflow-debug/bin/python --action_env PYTHON_LIB_PATH=/Users/80256276/workspace/anaconda3/envs/tensorflow-debug/lib/python3.5/site-packages --python_path=/Users/80256276/workspace/anaconda3/envs/tensorflow-debug/bin/python --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:android_arm in file /Users/80256276/workspace/github/tensorflow/.bazelrc: --config=android --cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a
INFO: Found applicable config definition build:android in file /Users/80256276/workspace/github/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
INFO: Build options --cpu, --crosstool_top, and --host_crosstool_top have changed, discarding analysis cache.
INFO: Analysed target //tensorflow/lite/tools/benchmark:benchmark_model (0 packages loaded, 1610 targets configured).
INFO: Found 1 target...
ERROR: /Users/80256276/workspace/github/tensorflow/tensorflow/lite/experimental/ruy/BUILD:136:1: C++ compilation of rule '//tensorflow/lite/experimental/ruy:blocking_counter' failed (Exit 1): false failed: error executing command 
  (cd /private/var/tmp/_bazel_80256276/20fdfdd4dfe4794de5bc0e54eeed4611/execroot/org_tensorflow && \
  exec env - \
    PATH=/Users/80256276/workspace/anaconda3/condabin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/go/bin:/Users/80256276/Library/Android/sdk/platform-tools:/Users/80256276/workspace/anaconda3/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/Users/80256276/workspace/anaconda3/envs/tensorflow-debug/bin/python \
    PYTHON_LIB_PATH=/Users/80256276/workspace/anaconda3/envs/tensorflow-debug/lib/python3.5/site-packages \
    TF_CONFIGURE_IOS=0 \
  /bin/false -MD -MF bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/_objs/blocking_counter/blocking_counter.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/_objs/blocking_counter/blocking_counter.o' -iquote . -iquote bazel-out/armeabi-v7a-opt/genfiles -iquote bazel-out/armeabi-v7a-opt/bin -c tensorflow/lite/experimental/ruy/blocking_counter.cc -o bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/experimental/ruy/_objs/blocking_counter/blocking_counter.o)
Execution platform: @bazel_tools//platforms:host_platform
src/main/tools/process-wrapper-legacy.cc:58: ""execvp(/bin/false, ...)"": No such file or directory
Target //tensorflow/lite/tools/benchmark:benchmark_model failed to build
INFO: Elapsed time: 0.471s, Critical Path: 0.05s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
"
31301,nsync ~per_thread() issue causing SIGSEGV in glibc __run_exit_handlers exit.c,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have linked against libtensorflow_cc.so but have used static linking
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 6.10 build / CentOS 7.4 runtime
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): v1.12.0
- TensorFlow version (use command below):
- Python version: NA
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): gcc 4.8.5
- CUDA/cuDNN version: 10.0.130,7.4.2.24
- GPU model and memory:GTX 1060


**Describe the current behavior**
Segfault at exit when unloading the Tensorflow Plugin in Autodesk Flame 2020.0

Error message
```
Program received signal SIGSEGV, Segmentation fault.
0x00007fa78f98adc0 in ?? ()
```

Stacktrace
```
(gdb) bt
#0  0x00007fa78f98adc0 in  ()
#1  0x00007faba74ceb19 in  () at /lib64/libstdc++.so.6
#2  0x00007faba6bc5b69 in __run_exit_handlers (status=0, listp=0x7faba6f526c8 <__exit_funcs>, run_list_atexit=run_list_atexit@entry=true) at exit.c:77
#3  0x00007faba6bc5bb7 in __GI_exit (status=<optimized out>) at exit.c:99
#4  0x000000000218b5e9 in  ()
#5  0x0000000000703ce9 in  ()
#6  0x000000000218a5ee in  ()
#7  0x000000000218a6f1 in  ()
#8  0x0000000000704358 in  ()
#9  0x00000000004d9eb5 in  ()
#10 0x00007faba6bae3d5 in __libc_start_main (main=
    0x4d72c0, argc=1, argv=0x7ffcb9bb5dd8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7ffcb9bb5dc8)
    at ../csu/libc-start.c:266
#11 0x00000000005c6ee1 in  ()
```
Symbol at 0x00007fa78f98adc0  <_ZN5nsync12_GLOBAL__N_110per_threadD2Ev>
c++filt _ZN5nsync12_GLOBAL__N_110per_threadD2Ev

 nsync::(anonymous namespace)::per_thread::~per_thread()

see:
https://github.com/google/nsync/blob/5e8b19a81e5729922629dd505daa651f6ffdf107/platform/c%2B%2B11/src/per_thread_waiter.cc#L31

**Describe the expected behavior**
Close down cleanly.

**Code to reproduce the issue**
Not possible, as Autodesk Flame framework is required

**Other info / logs**
Looking at https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/tools/benchmark/benchmark_model.cc 

There doesn't seem to be any special destructors used.

Closes down cleanly in other runtime environments.

The Plugin can be in any state when exiting, but other destructors can be called earlier to clean up the session.

What destructors would have to be used to make sure that the nsync::(anonymous_namespace)::per_thread::~per_thread() desctructor cannot result in the segfault with the atexit handlers from glibc?
"
31300,Problem when saving/loading keras model with '.tf' extension and stateful ConvLSTM2D as a layer,"**System information**
- WSL Win10 Ubuntu 18.04 (it also happens in a real ubuntu 18)
- tf-nightly-gpu-2.0-preview==2.0.0.dev20190802 (happens in cpu and gpu)
- python3.7

**Describe the current behavior**

I have a simple test to serialized and deserialize a model which has a stateful LSTM. The 'tf' version returns an error, the 'h5' version works ok.

**Describe the expected behavior**

Correct serialization and deserialization of the code in both cases.

**Code  to reproduce the issue**

```python
import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential()
model.add(layers.ConvLSTM2D(32, (3, 3), activation='relu', batch_size=32, input_shape=(1, 28, 28, 1), padding='same', strides=2,
                            stateful=True))
model.add(layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same'))
model.add(layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))
model.compile(optimizer='adam', loss=['mse'])
model.save('teste.tf')
model = tf.keras.models.load_model('teste.tf')
print(model)
```

```python
2019-08-03 04:32:09.606349: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-08-03 04:32:09.606533: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-08-03 04:32:09.606718: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-GUERINJR): /proc/driver/nvidia/version does not exist
2019-08-03 04:32:09.607091: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-03 04:32:09.616262: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-08-03 04:32:09.617989: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ffff2cf88d0 executing computations on platform Host. Devices:
2019-08-03 04:32:09.618144: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
WARNING: Logging before flag parsing goes to stderr.
W0803 04:32:10.136887 140094640752448 save.py:130] Skipping full serialization of object <tensorflow.python.keras.layers.convolutional_recurrent.ConvLSTM2D object at 0x7f69f49960f0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
2019-08-03 04:32:10.714903: W tensorflow/python/util/util.cc:288] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
W0803 04:32:10.904490 140094640752448 deprecation.py:506] From /home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1784: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Traceback (most recent call last):
  File ""/home/nguerinjr/Documents/deep_coding_project/img_common/teste.py"", line 21, in <module>
    model.save('teste.tf')
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1164, in save
    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 107, in save_model
    saved_model_save.save(model, filepath, overwrite, include_optimizer)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py"", line 86, in save
    save_lib.save(model, filepath)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 855, in save
    meta_graph_def, saveable_view, signatures)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 585, in _fill_meta_graph_def
    signatures = _generate_signatures(signature_functions, resource_map)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 459, in _generate_signatures
    function, mapped_inputs, resource_map)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 411, in _call_function_with_mapped_captures
    function.graph.captures, resource_map)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 333, in _map_captures_to_created_tensors
    .format(interior))
AssertionError: Tried to export a function which references untracked object Tensor(""StatefulPartitionedCall/args_4:0"", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.

Process finished with exit code 1
```

**Working code  for comparisons**

When not stateful, the code works ok

```python
import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential()
model.add(layers.ConvLSTM2D(32, (3, 3), activation='relu', batch_size=32, input_shape=(1, 28, 28, 1), padding='same', strides=2))
model.add(layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same'))
model.add(layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))
model.compile(optimizer='adam', loss=['mse'])
model.save('teste.tf')
model = tf.keras.models.load_model('teste.tf')
print(model)
```

```python
2019-08-03 04:36:08.915912: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-08-03 04:36:08.916094: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-08-03 04:36:08.916278: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-GUERINJR): /proc/driver/nvidia/version does not exist
2019-08-03 04:36:08.916617: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-03 04:36:08.925594: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-08-03 04:36:08.927217: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fffe038cb10 executing computations on platform Host. Devices:
2019-08-03 04:36:08.927361: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-08-03 04:36:10.081493: W tensorflow/python/util/util.cc:288] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING: Logging before flag parsing goes to stderr.
W0803 04:36:10.614284 140013092800320 deprecation.py:506] From /home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1784: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
<tensorflow.python.keras.saving.saved_model.load.Sequential object at 0x7f56a80c2cf8>

Process finished with exit code 0
```

** Working code 2 for comparisons**

When 'h5' works ok in both cases

```python
import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential()
model.add(layers.ConvLSTM2D(32, (3, 3), activation='relu', batch_size=32, input_shape=(1, 28, 28, 1), padding='same', strides=2))
model.add(layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same'))
model.add(layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))
model.compile(optimizer='adam', loss=['mse'])
model.save('teste.h5')
model = tf.keras.models.load_model('teste.h5')
print(model)
```

```python
2019-08-03 04:37:27.733190: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-08-03 04:37:27.733380: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-08-03 04:37:27.733531: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-GUERINJR): /proc/driver/nvidia/version does not exist
2019-08-03 04:37:27.733891: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-03 04:37:27.743100: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-08-03 04:37:27.745015: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fffd380cc50 executing computations on platform Host. Devices:
2019-08-03 04:37:27.745198: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f32b028f550>

Process finished with exit code 0
```

```python
import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential()
model.add(layers.ConvLSTM2D(32, (3, 3), activation='relu', batch_size=32, input_shape=(1, 28, 28, 1), padding='same', strides=2,
                            stateful=True))
model.add(layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same'))
model.add(layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))
model.compile(optimizer='adam', loss=['mse'])
model.save('teste.h5')
model = tf.keras.models.load_model('teste.h5')
print(model)
```

```python
2019-08-03 04:37:48.546310: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-08-03 04:37:48.546554: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-08-03 04:37:48.546700: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-GUERINJR): /proc/driver/nvidia/version does not exist
2019-08-03 04:37:48.547055: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-03 04:37:48.556123: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-08-03 04:37:48.557773: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fffcde1a9a0 executing computations on platform Host. Devices:
2019-08-03 04:37:48.557917: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f091c0ac860>

Process finished with exit code 0
```

"
31299,Lower performance when use multi gpu,"**System information**
-Windows 10 1903 workstation x64
-ASRock X570 Taichi
-AMD Ryzen 3700x
-Corsair 8GB LPX 3000MHz RAM x4 (32GB Tot.)
-GIGABYTE RTX 2080ti Turbo 11G x2 (with out nvlink)
-SAMSUNG PM981 256GB NVMe SSD For System and DataSet
-Cuda 10.0.130, cuDNN 7.6.0
-python 3.7.4 x64


**Describe the current behavior**
When I use three examples from doc about train keras model with multi gpus, it used more time than single gpu.
Should I buy a NVLink or change the code.
On example please if need change the code.

**Code**
```
import os
import tensorflow as tf
from DataSets import Imagenet
from Models import efficientnet

tf_config = tf.ConfigProto()
tf_config.gpu_options.allow_growth = True
with tf.Session(config=tf_config) as sess:
    model = efficientnet.efficientnet_b7(1000)
    model.summary()
    model.load_weights('/Models/efficientnet_b7')
    model = tf.keras.utils.multi_gpu_model(model,2, cpu_merge=False)
    model.compile('sgd', 'sparse_categorical_crossentropy', ['accuracy'])
    train_ds, test_ds = Imagenet.classification_dataset(6,100,True,[224,224,3])
    while True:
        model.fit(train_ds,
                  epochs=int(Imagenet.train_images/50000),
                  steps_per_epoch=10000,
                  callbacks=[tf.keras.callbacks.ModelCheckpoint('/Models/efficientnet_b7','loss',save_best_only=True,save_weights_only=True,mode='min')])
        model.evaluate(test_ds,steps=int(Imagenet.test_images/5))
```

**Other info / logs**

> 2019-08-03 12:25:38.658805: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
> 2019-08-03 12:25:38.665648: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
> 2019-08-03 12:25:39.153806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
> name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
> pciBusID: 0000:0e:00.0
> 2019-08-03 12:25:39.158542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
> name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
> pciBusID: 0000:0f:00.0
> 2019-08-03 12:25:39.171135: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
> 2019-08-03 12:25:39.174610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
> 2019-08-03 12:25:40.496910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2019-08-03 12:25:40.500248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1
> 2019-08-03 12:25:40.501954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N N
> 2019-08-03 12:25:40.503661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N N
> 2019-08-03 12:25:40.506310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8694 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:0e:00.0, compute capability: 7.5)
> 2019-08-03 12:25:40.522921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8695 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:0f:00.0, compute capability: 7.5)"
31298,Serialization problems in keras when using add_loss or '.tf' extension for saving,"**System information**
- WSL Win10 Ubuntu 18.04 (it also happens in a real ubuntu 18)
- tf-nightly-gpu-2.0-preview==2.0.0.dev20190802 (happens in cpu and gpu)
- python3.7

**Describe the current behavior**

I have a simple test to serialized and deserialize a model which has no compiled loss, just one added with add_loss.

**Describe the expected behavior**

Correct serialization and deserialization of the code

**Code 1 to reproduce the issue**

Occurs with both the pure loss function (which on earlier version would return json errors, but now seems ok), but also with the Lambda wrapper.

``` python
import tensorflow as tf
import tensorflow.keras as tf_k

model = tf_k.models.Sequential()
model.add(tf_k.layers.Conv2D(32, (3, 3), activation='relu', batch_size=32, input_shape=(28, 28, 1), padding='same', strides=2))
model.add(tf_k.layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same'))
model.add(tf_k.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))
#loss = tf_k.layers.Lambda(lambda i: tf_k.losses.mse(*i))([model.inputs[0], model.outputs[0]])
loss = tf_k.losses.mean_squared_error(model.inputs[0], tf.zeros_like(model.inputs[0]))
model.add_loss(loss)
model.compile(optimizer='adam')
model.save('teste.h5')
model = tf.keras.models.load_model('teste.h5')
```

```python
2019-08-03 03:38:05.624310: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-08-03 03:38:05.624506: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-08-03 03:38:05.624652: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-GUERINJR): /proc/driver/nvidia/version does not exist
2019-08-03 03:38:05.624991: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-03 03:38:05.634458: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-08-03 03:38:05.636116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fffe06c0120 executing computations on platform Host. Devices:
2019-08-03 03:38:05.636259: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
WARNING: Logging before flag parsing goes to stderr.
W0803 03:38:05.683956 139979377280832 training_utils.py:1320] Output conv2d_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to conv2d_1.
Traceback (most recent call last):
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1554, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 1 inputs specified of 2 inputs in Op while building NodeDef 'tf_op_layer_SquaredDifference/SquaredDifference' using Op<name=SquaredDifference; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/nguerinjr/Documents/deep_coding_project/img_common/teste.py"", line 14, in <module>
    model = tf.keras.models.load_model('teste.h5')
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 138, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 162, in load_model_from_hdf5
    custom_objects=custom_objects)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/model_config.py"", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/serialization.py"", line 98, in deserialize
    printable_module_name='layer')
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 191, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 369, in from_config
    model.add(layer)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 195, in add
    output_tensor = layer(self.outputs[0])
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 799, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2507, in call
    return self._make_op(inputs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2530, in _make_op
    c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1557, in _create_c_op
    raise ValueError(str(e))
ValueError: 1 inputs specified of 2 inputs in Op while building NodeDef 'tf_op_layer_SquaredDifference/SquaredDifference' using Op<name=SquaredDifference; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true>

Process finished with exit code 1
```

**Code 2 to reproduce the issue**

Putting the '.tf' suffix, only changes the error message. Here's an example

``` python
import tensorflow as tf
import tensorflow.keras as tf_k

model = tf_k.models.Sequential()
model.add(tf_k.layers.Conv2D(32, (3, 3), activation='relu', batch_size=32, input_shape=(28, 28, 1), padding='same', strides=2))
model.add(tf_k.layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same'))
model.add(tf_k.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))

#loss = tf_k.layers.Lambda(lambda i: tf_k.losses.mse(*i))([model.inputs[0], model.outputs[0]])
loss = tf_k.losses.mean_squared_error(model.inputs[0], tf.zeros_like(model.inputs[0]))
model.add_loss(loss)
model.compile(optimizer='adam')
model.save('teste.tf')
model = tf.keras.models.load_model('teste.tf')
```

```python
2019-08-03 03:38:42.774544: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-08-03 03:38:42.774737: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-08-03 03:38:42.774932: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-GUERINJR): /proc/driver/nvidia/version does not exist
2019-08-03 03:38:42.775273: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-03 03:38:42.784443: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-08-03 03:38:42.786142: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fffc04e4040 executing computations on platform Host. Devices:
2019-08-03 03:38:42.786289: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
WARNING: Logging before flag parsing goes to stderr.
W0803 03:38:42.835703 140157751789376 training_utils.py:1320] Output conv2d_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to conv2d_1.
2019-08-03 03:38:43.281262: W tensorflow/python/util/util.cc:288] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
W0803 03:38:43.369712 140157751789376 deprecation.py:506] From /home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1784: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Traceback (most recent call last):
  File ""/home/nguerinjr/Documents/deep_coding_project/img_common/teste.py"", line 14, in <module>
    model = tf.keras.models.load_model('teste.tf')
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 142, in load_model
    return saved_model_load.load(filepath, compile)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py"", line 86, in load
    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py"", line 541, in load_internal
    export_dir)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py"", line 103, in __init__
    self._finalize()
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py"", line 127, in _finalize
    node.add(layer)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 195, in add
    output_tensor = layer(self.outputs[0])
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 799, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py"", line 57, in return_outputs_and_add_losses
    outputs, losses = fn(inputs, *args, **kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 382, in _initialize
    *args, **kwds))
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1806, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2106, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1997, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 884, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 325, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py"", line 262, in restored_function_body
    ""\n\n"".join(signature_descriptions)))
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (1 total):
    * Tensor(""inputs:0"", shape=(32, 28, 28, 1), dtype=float32)
  Keyword arguments: {}

Expected these arguments to match one of the following 1 option(s):

Option 1:
  Positional arguments (1 total):
    * [TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs/0')]
  Keyword arguments: {}

Process finished with exit code 1
```

> Is there something wrong with this simple example? Something I'm missing. Or it's just that the add_loss still have some problems wrt serialization / deserialization?

I perceived a bunch of errors have been fixed since my last report about this custom uses of keras (https://github.com/tensorflow/tensorflow/issues/30378). For example, I know a workaround for this, which seems ok in this nightly version, is to use a custom compiled loss, which was not working at that time, as another issue I had reported  (https://github.com/tensorflow/tensorflow/issues/30384)

```python
import tensorflow as tf
import tensorflow.keras as tf_k

model = tf_k.models.Sequential()
model.add(tf_k.layers.Conv2D(32, (3, 3), activation='relu', batch_size=32, input_shape=(28, 28, 1), padding='same', strides=2))
model.add(tf_k.layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same'))
model.add(tf_k.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))

def my_loss(y_true, y_pred):
    return y_true

def my_loss2(y_true, y_pred):
    return y_pred

model.compile(optimizer='adam', loss=[my_loss])
model.save('teste.h5')

model = tf.keras.models.load_model('teste.h5', custom_objects={'my_loss': my_loss, 'my_loss2': my_loss2})
print(model)
```

```python
/home/nguerinjr/Documents/deep_coding_project/venv/bin/python /home/nguerinjr/Documents/deep_coding_project/img_common/teste.py
2019-08-03 03:46:57.744222: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-08-03 03:46:57.744406: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-08-03 03:46:57.744557: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-GUERINJR): /proc/driver/nvidia/version does not exist
2019-08-03 03:46:57.744891: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-03 03:46:57.755141: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-08-03 03:46:57.756825: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fffea867eb0 executing computations on platform Host. Devices:
2019-08-03 03:46:57.757004: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f4e203ee940>

Process finished with exit code 0
```

But, as an extension to these tests, I've noticed the '.tf' version does not work correctly, which seems another related bug:

```python
import tensorflow as tf
import tensorflow.keras as tf_k

model = tf_k.models.Sequential()
model.add(tf_k.layers.Conv2D(32, (3, 3), activation='relu', batch_size=32, input_shape=(28, 28, 1), padding='same', strides=2))
model.add(tf_k.layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same'))
model.add(tf_k.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))

def my_loss(y_true, y_pred):
    return y_true

def my_loss2(y_true, y_pred):
    return y_pred

model.compile(optimizer='adam', loss=[my_loss])
model.save('teste.tf')

model = tf.keras.models.load_model('teste.tf', custom_objects={'my_loss': my_loss, 'my_loss2': my_loss2})
print(model)
```

```python
/home/nguerinjr/Documents/deep_coding_project/venv/bin/python /home/nguerinjr/Documents/deep_coding_project/img_common/teste.py
2019-08-03 03:47:16.464514: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-08-03 03:47:16.464702: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-08-03 03:47:16.464859: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-GUERINJR): /proc/driver/nvidia/version does not exist
2019-08-03 03:47:16.465200: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-03 03:47:16.474429: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-08-03 03:47:16.476021: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fffbb87a7b0 executing computations on platform Host. Devices:
2019-08-03 03:47:16.476185: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-08-03 03:47:16.873280: W tensorflow/python/util/util.cc:288] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING: Logging before flag parsing goes to stderr.
W0803 03:47:16.921350 140320731367232 deprecation.py:506] From /home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1784: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Traceback (most recent call last):
  File ""/home/nguerinjr/Documents/deep_coding_project/img_common/teste.py"", line 25, in <module>
    model = tf.keras.models.load_model('teste.tf', custom_objects={'my_loss': my_loss, 'my_loss2': my_loss2})
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 142, in load_model
    return saved_model_load.load(filepath, compile)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py"", line 93, in load
    model._training_config))  # pylint: disable=protected-access
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 340, in compile
    self.loss, self.output_names)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py"", line 1329, in prepare_loss_functions
    loss_functions = nest.map_structure(get_loss_function, loss)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py"", line 523, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py"", line 523, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py"", line 1086, in get_loss_function
    loss_fn = losses.get(loss)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py"", line 1166, in get
    return deserialize(identifier)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py"", line 1157, in deserialize
    printable_module_name='loss function')
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 210, in deserialize_keras_object
    raise ValueError('Unknown ' + printable_module_name + ':' + object_name)
ValueError: Unknown loss function:my_loss

Process finished with exit code 1
```

The real point about my interest in the add_loss it that's the only way, at least I think, to use losses in a flexible way. I still need very flexible losses. add_loss seems an interesting way to do this."
31297,"tf.gradients with unconnected_gradients=""zero"" returns wrong shape for unconnected resource variables","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 5.2.1-arch1-1-ARCH x86_64
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5
- Python version: 3.6.8
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Calling `tf.gradients` with `unconnected_gradients=""zero""` returns scalars for unconnected resource variables.

**Describe the expected behavior**
Calling `tf.gradients` with `unconnected_gradients=""zero""` returns appropriately shaped zero tensors for unconnected resource variables.

**Code to reproduce the issue**
```python
a = tf.Variable(initial_value=[2., 3.])
b = tf.Variable(initial_value=[3., 4.], use_resource=True)
c = tf.constant(0.)
print(tf.gradients(c, [a, b], unconnected_gradients=""zero""))
# => [<tf.Tensor 'zeros_like:0' shape=(2,) dtype=float32>,
#     <tf.Tensor 'zeros_like_1:0' shape=() dtype=float32>]
```"
31291,tf.keras.optimizers.SGD with momentum does not fit when model metrics are provided,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.4 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14
- Python version: 3.6
- CUDA/cuDNN version: CUDA Version: 10.1
- GPU model and memory: 1080TI 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`momentum` in  `tensorflow.keras.optimizers.SGD` and `metrics` in model compilation cannot be used together when using `fit_generator`. However, each works independently. The problem does not exist when using `fit`

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from tensorflow.keras.layers import Embedding, Input, Dense, Lambda
from tensorflow.keras import backend as K
from tensorflow.keras import Model
import numpy as np
from tensorflow import keras
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""


def get_more():
    while(True):
        yield ({'a_input': np.random.randint(0, 10, (32, 1200))},
               np.random.rand(32, 1))


def build():

    input = Input(shape=(1200,), name='a_input', dtype='int32')
    x = Embedding(input_dim=10,
                  output_dim=4,
                  input_length=1200,
                  trainable=True, name='embedding')(input)
    x = Dense(1, activation='linear')(x)
    x = Lambda(lambda x: K.squeeze(x, axis=-1))(x)
    x = Dense(1, name='output')(x)
    this_model = Model(input, x)

    return this_model


####### FAIL
# Situation 1 (+METRICS +MOMENTUM)
#######
K.clear_session()
this_model = build()
optimizer = keras.optimizers.SGD(lr=0.05, momentum=0.9)
this_model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])
this_model.fit_generator(iter(get_more()), steps_per_epoch=10)

####### PASS
# Situation 2 (+METRICS -MOMENTUM)
#######
K.clear_session()
this_model = build()
optimizer = keras.optimizers.SGD(lr=0.05)
this_model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])
this_model.fit_generator(iter(get_more()), steps_per_epoch=10)

####### PASS
# Situation 3 (-METRICS +MOMENTUM)
#######
K.clear_session()
this_model = build()
optimizer = keras.optimizers.SGD(lr=0.05, momentum=0.9)
this_model.compile(loss='mse', optimizer=optimizer)
this_model.fit_generator(iter(get_more()), steps_per_epoch=10)

####### PASS
# Situation 4 (+METRICS +MOMENTUM)  (fit instead of fit_generator)
#######
K.clear_session()
this_model = build()
optimizer = keras.optimizers.SGD(lr=0.05, momentum=0.9)
this_model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])
x_in = {'a_input': np.random.randint(0,10,(500,1200))}
y_out = np.random.rand(500,1)
this_model.fit(x_in, y_out)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1355     try:
-> 1356       return fn(*args)
   1357     except errors.OpError as e:

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1338       # Ensure any changes to the graph are reflected in the runtime.
-> 1339       self._extend_graph()
   1340       return self._call_tf_sessionrun(

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1373     with self._graph._session_run_lock():  # pylint: disable=protected-access
-> 1374       tf_session.ExtendSession(self._session)
   1375 

InvalidArgumentError: Cannot assign a device for operation embedding/embeddings/Initializer/random_uniform/sub: Could not satisfy explicit device specification '' because the node {{colocation_node embedding/embeddings/Initializer/random_uniform/sub}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
ResourceSparseApplyKerasMomentum: CPU 
Identity: GPU CPU XLA_CPU XLA_GPU 
ResourceGather: GPU CPU XLA_CPU XLA_GPU 
AssignVariableOp: GPU CPU XLA_CPU XLA_GPU 
RandomUniform: GPU CPU XLA_CPU XLA_GPU 
VarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU 
Const: GPU CPU XLA_CPU XLA_GPU 
Mul: GPU CPU XLA_CPU XLA_GPU 
ReadVariableOp: GPU CPU XLA_CPU XLA_GPU 
Sub: GPU CPU XLA_CPU XLA_GPU 
VarHandleOp: GPU CPU XLA_CPU XLA_GPU 
Add: GPU CPU XLA_CPU XLA_GPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  embedding/embeddings/Initializer/random_uniform/shape (Const) 
  embedding/embeddings/Initializer/random_uniform/min (Const) 
  embedding/embeddings/Initializer/random_uniform/max (Const) 
  embedding/embeddings/Initializer/random_uniform/RandomUniform (RandomUniform)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embeddings/Initializer/random_uniform/sub (Sub) 
  embedding/embeddings/Initializer/random_uniform/mul (Mul) 
  embedding/embeddings/Initializer/random_uniform (Add) 
  embedding/embeddings (VarHandleOp)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embeddings/IsInitialized/VarIsInitializedOp (VarIsInitializedOp)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embeddings/Assign (AssignVariableOp)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embeddings/Read/ReadVariableOp (ReadVariableOp)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embedding_lookup (ResourceGather)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embedding_lookup/Identity (Identity) 
  VarIsInitializedOp_4 (VarIsInitializedOp)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  SGD/embedding/embeddings/momentum/Initializer/zeros (Const) 
  SGD/embedding/embeddings/momentum (VarHandleOp) 
  SGD/embedding/embeddings/momentum/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) 
  SGD/embedding/embeddings/momentum/Assign (AssignVariableOp) 
  SGD/embedding/embeddings/momentum/Read/ReadVariableOp (ReadVariableOp) 
  SGD/SGD/update_embedding/embeddings/ResourceSparseApplyKerasMomentum (ResourceSparseApplyKerasMomentum) 
  VarIsInitializedOp_7 (VarIsInitializedOp) 

	 [[{{node embedding/embeddings/Initializer/random_uniform/sub}}]]

```"
31287,Optimizer (Adam) does not propagate hyperparameter update on first update,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 64b
- TensorFlow version (use command below): 2.0.0b1
- Python version: 3.6

**Describe the current behavior**
Using tf.keras.optimizers.Adam and updating hyperparameters results in hyperparameters not updating on the first update.

**Describe the expected behavior**
Hyperparameters DO update on the first run, ie they are initialized to tf.Variables at constructor of the optimizer, not on first call.

**Code to reproduce the issue**
`adam = tf.keras.optimizers.Adam()`
`adam.learning_rate = 0.00001   # this does not update learning_rate, but changes python-typed ` `hyperparameters in ""_hyper"" dictionary to tf.Variables`
`adam.learning_rate = 0.00001  # this finally updates the tf.Variable `


Calling adam._hyper before update yields:

`{'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999}`

Calling adam._hyper after first update yields:

`{'learning_rate': <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>, 'decay': <tf.Variable 'decay:0' shape=() dtype=float32, numpy=0.0>, 'beta_1': <tf.Variable 'beta_1:0' shape=() dtype=float32, numpy=0.9>, 'beta_2': <tf.Variable 'beta_2:0' shape=() dtype=float32, numpy=0.999>}`







"
31286,ValueError: Cannot add function 'TRTEngineOp_0_native_segment' because a different function with the same name already exists.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubunti 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13.1
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:T4

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Not able to call trt.create_inference_graph more than  once to create TF-TRT nodes for  disjointed sub-graphs.  Throws the above error. 

**Describe the expected behavior**

Should not throw the above error.

**Code to reproduce the issue**

Should not be difficult to create a sample code.  Will try to get one soon. 

**Other info / logs**

```
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 430, in import_graph_def
    raise ValueError(str(e))
ValueError: Cannot add function 'TRTEngineOp_0_native_segment' because a different function with the same name already exists.
```
"
31285,two neural networks running at the same time GPU-Util drops ,"I'm running two neural networks on the same process, one depends  on the other. the first one is an image classifier (YOLO) the other one is a Lstm that depends on the deteccions of the yolo, when I run more than one YOLO network everythings goin right, the util of the gpu is hight, same when I run more than one Lstm, the problem is when the two networks runs together, the gpu-uils drops and the all process is slow. I tried in python and c++ tensorflow api same results any help ? "
31284,tf.2 VAE example iteration time same despite increasing number of GPUs,"- Have I written custom code: 
No. I am using the copy of the code provided in the VAE tf2 MNIST example (https://www.tensorflow.org/beta/tutorials/generative/cvae). The only change I made: I commented out lines related to IPython module which is not used as I am using it without Colab, directly running on gcloud.

- OS Platform and Distribution: 
Linux Ubuntu 16.04 installed on Google Cloud compute engine VM

- TensorFlow installed from (source or binary):
pip install tensorflow-gpu==2.0.0-beta1

- TensorFlow version (use command below):
2.0-beta1

- Python version:
Python3.5

- GCC/Compiler version (if compiling from source):
gcc-5.4 (auto-installed with nvidia-cuda)

- CUDA/cuDNN version:
cuda-10.0 / libcudnn7_7.6.1.34

- GPU model and memory:
I am comparing 3 configurations:
CONFIG_GPU_8: GCE, 8xV100 128 GB total memory with 24x vCPUs 128GB total memory
vs.
CONFIG_GPU_4: GCE, 4xP100 128 GB total memory with 16x vCPU 64 GB total memory
vs.
CONFIG_GPU_0: local machine WITHOUT GPU, only 2 CPUs with 3.5 GB total memory (in this case tf version is no-gpu version: tensorflow==2.0.0-beta1)


**Current behavior**
When running the MNIST training example, the reported time spent on each training epoch is the following (on average):
CONFIG_GPU_8:  ~17.5 seconds
CONFIG_GPU_4:  ~17.6 seconds
CONFIG_GPU_0:  ~98 seconds

**Describe the expected behavior**
The training speed should improve much more when going from CONFIG_GPU_0 to CONFIG_GPU_4 and it should improve by more than a factor of 2 when going from CONFIG_GPU_4 to CONFIG_GPU_8 (notice that in addition to using twice as many GPUs, each GPU in CONFIG_GPU_8 is V100 which is in itself should be much more powerful than P100 in CONFIG_GPU_4).


**Code to reproduce the issue**

```
from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf
import os
import time
import numpy as np
import glob
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
import PIL
import imageio
# from IPython import display

(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()

train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')

# Normalizing the images to the range of [0., 1.]
train_images /= 255.
test_images /= 255.

# Binarization
train_images[train_images >= .5] = 1.
train_images[train_images < .5] = 0.
test_images[test_images >= .5] = 1.
test_images[test_images < .5] = 0.

TRAIN_BUF = 60000
BATCH_SIZE = 100

TEST_BUF = 10000

train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(TRAIN_BUF).batch(BATCH_SIZE)
test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(TEST_BUF).batch(BATCH_SIZE)


# Network architecture

class CVAE(tf.keras.Model):
    def __init__(self, latent_dim):
        super(CVAE, self).__init__()
        self.latent_dim = latent_dim
        self.inference_net = tf.keras.Sequential(
            [
                tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),
                tf.keras.layers.Conv2D(
                    filters=32, kernel_size=3, strides=(2, 2), activation='relu'),
                tf.keras.layers.Conv2D(
                    filters=64, kernel_size=3, strides=(2, 2), activation='relu'),
                tf.keras.layers.Flatten(),
                # No activation
                tf.keras.layers.Dense(latent_dim + latent_dim),
            ]
        )

        self.generative_net = tf.keras.Sequential(
            [
              tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
              tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),
              tf.keras.layers.Reshape(target_shape=(7, 7, 32)),
              tf.keras.layers.Conv2DTranspose(
                  filters=64,
                  kernel_size=3,
                  strides=(2, 2),
                  padding=""SAME"",
                  activation='relu'),
              tf.keras.layers.Conv2DTranspose(
                  filters=32,
                  kernel_size=3,
                  strides=(2, 2),
                  padding=""SAME"",
                  activation='relu'),
              # No activation
              tf.keras.layers.Conv2DTranspose(
                  filters=1, kernel_size=3, strides=(1, 1), padding=""SAME""),
            ]
        )

    def sample(self, eps=None):
        if eps is None:
            eps = tf.random.normal(shape=(100, self.latent_dim))
        return self.decode(eps, apply_sigmoid=True)

    def encode(self, x):
        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)
        return mean, logvar

    def reparameterize(self, mean, logvar):
        eps = tf.random.normal(shape=mean.shape)
        return eps * tf.exp(logvar * .5) + mean

    def decode(self, z, apply_sigmoid=False):
        logits = self.generative_net(z)
        if apply_sigmoid:
            probs = tf.sigmoid(logits)
            return probs

        return logits



# Define loss and optimizer:

optimizer = tf.keras.optimizers.Adam(1e-4)

def log_normal_pdf(sample, mean, logvar, raxis=1):
  log2pi = tf.math.log(2. * np.pi)
  return tf.reduce_sum(
      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),
      axis=raxis)

def compute_loss(model, x):
  mean, logvar = model.encode(x)
  z = model.reparameterize(mean, logvar)
  x_logit = model.decode(z)

  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)
  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])
  logpz = log_normal_pdf(z, 0., 0.)
  logqz_x = log_normal_pdf(z, mean, logvar)
  return -tf.reduce_mean(logpx_z + logpz - logqz_x)

def compute_gradients(model, x):
  with tf.GradientTape() as tape:
      loss = compute_loss(model, x)
  return tape.gradient(loss, model.trainable_variables), loss

def apply_gradients(optimizer, gradients, variables):
  optimizer.apply_gradients(zip(gradients, variables))



# Training and generating images:

epochs = 100
latent_dim = 50
num_examples_to_generate = 16

# keeping the random vector constant for generation (prediction) so
# it will be easier to see the improvement.
random_vector_for_generation = tf.random.normal(
    shape=[num_examples_to_generate, latent_dim])
model = CVAE(latent_dim)

def generate_and_save_images(model, epoch, test_input):
  predictions = model.sample(test_input)
  fig = plt.figure(figsize=(4,4))

  for i in range(predictions.shape[0]):
      plt.subplot(4, 4, i+1)
      plt.imshow(predictions[i, :, :, 0], cmap='gray')
      plt.axis('off')

  # tight_layout minimizes the overlap between 2 sub-plots
  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
  # plt.show()


generate_and_save_images(model, 0, random_vector_for_generation)

for epoch in range(1, epochs + 1):
  start_time = time.time()
  for train_x in train_dataset:
    gradients, loss = compute_gradients(model, train_x)
    apply_gradients(optimizer, gradients, model.trainable_variables)
  end_time = time.time()

  if epoch % 1 == 0:
    loss = tf.keras.metrics.Mean()
    for test_x in test_dataset:
      loss(compute_loss(model, test_x))
    elbo = -loss.result()
    # display.clear_output(wait=False)
    print('Epoch: {}, Test set ELBO: {}, '
          'time elapse for current epoch {}'.format(epoch,
                                                    elbo,
                                                    end_time - start_time))
    generate_and_save_images(
        model, epoch, random_vector_for_generation)



# display image using the epoch number

def display_image(epoch_no):
  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))


plt.imshow(display_image(epochs))
plt.axis('off')# Display images

# generate gif of all the saved images

anim_file = 'cvae.gif'

with imageio.get_writer(anim_file, mode='I') as writer:
  filenames = glob.glob('image*.png')
  filenames = sorted(filenames)
  last = -1
  for i,filename in enumerate(filenames):
    frame = 2*(i**0.5)
    if round(frame) > round(last):
      last = frame
    else:
      continue
    image = imageio.imread(filename)
    writer.append_data(image)
  image = imageio.imread(filename)
  writer.append_data(image)

print(""done"")
# import IPython
# if IPython.version_info >= (6,2,0,''):
#   display.Image(filename=anim_file)

```


**Other info / logs**
See below the log for the first 8 training epochs for each of the 3 configuration, one after the other. Please notice that the GPUs (4 or 8) are properly detected and recognized and there is no error and everything seems to work fine apart from the lack of performance improvement despite adding extra GPUs and memory.


CONFIG_GPU_8:

> sudo+ssh://kristofgiber@34.90.27.163:22/usr/bin/python3.5 -u /home/kristofgiber/VAE/MNIST/vae-tf2-mnist.py
> Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
> 11493376/11490434 [==============================] - 0s 0us/step
> 2019-08-02 16:09:34.912011: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
> 2019-08-02 16:09:35.661220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:35.662802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:04.0
> 2019-08-02 16:09:35.662873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:35.664279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:05.0
> 2019-08-02 16:09:35.664331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:35.665726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:06.0
> 2019-08-02 16:09:35.665773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:35.667167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:07.0
> 2019-08-02 16:09:35.667215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:35.668597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:08.0
> 2019-08-02 16:09:35.668641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:35.670037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:09.0
> 2019-08-02 16:09:35.670083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:35.671471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:0a.0
> 2019-08-02 16:09:35.671515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:35.672967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:0b.0
> 2019-08-02 16:09:35.841569: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
> 2019-08-02 16:09:36.289413: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
> 2019-08-02 16:09:36.633804: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
> 2019-08-02 16:09:36.782869: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
> 2019-08-02 16:09:37.188381: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
> 2019-08-02 16:09:37.447535: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
> 2019-08-02 16:09:38.620709: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
> 2019-08-02 16:09:38.620897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.622663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.624230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.625717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.627194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.628617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.630123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.631639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.633059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.634502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.635955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.637491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.639068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.640578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.642202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.643758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:38.645293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
> 2019-08-02 16:09:38.732034: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
> 2019-08-02 16:09:39.951150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:39.962194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:39.994318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.003139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.031364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.065523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.095806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.136847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.154130: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x554e540 executing computations on platform CUDA. Devices:
> 2019-08-02 16:09:40.154183: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
> 2019-08-02 16:09:40.154192: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla V100-SXM2-16GB, Compute Capability 7.0
> 2019-08-02 16:09:40.154198: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Tesla V100-SXM2-16GB, Compute Capability 7.0
> 2019-08-02 16:09:40.154203: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Tesla V100-SXM2-16GB, Compute Capability 7.0
> 2019-08-02 16:09:40.154209: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): Tesla V100-SXM2-16GB, Compute Capability 7.0
> 2019-08-02 16:09:40.154215: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): Tesla V100-SXM2-16GB, Compute Capability 7.0
> 2019-08-02 16:09:40.154221: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): Tesla V100-SXM2-16GB, Compute Capability 7.0
> 2019-08-02 16:09:40.154227: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (7): Tesla V100-SXM2-16GB, Compute Capability 7.0
> 2019-08-02 16:09:40.429307: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000129999 Hz
> 2019-08-02 16:09:40.431667: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e65bb0 executing computations on platform Host. Devices:
> 2019-08-02 16:09:40.431701: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
> 2019-08-02 16:09:40.444469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.445999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:04.0
> 2019-08-02 16:09:40.446087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.447539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:05.0
> 2019-08-02 16:09:40.447587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.449025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:06.0
> 2019-08-02 16:09:40.449072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.450563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:07.0
> 2019-08-02 16:09:40.450613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.452069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:08.0
> 2019-08-02 16:09:40.452116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.453551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:09.0
> 2019-08-02 16:09:40.453595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.455093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:0a.0
> 2019-08-02 16:09:40.455146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.456592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
> name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
> pciBusID: 0000:00:0b.0
> 2019-08-02 16:09:40.456638: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
> 2019-08-02 16:09:40.456649: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
> 2019-08-02 16:09:40.456658: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
> 2019-08-02 16:09:40.456666: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
> 2019-08-02 16:09:40.456675: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
> 2019-08-02 16:09:40.456683: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
> 2019-08-02 16:09:40.456692: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
> 2019-08-02 16:09:40.456726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.458227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.459791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.461375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.462851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.464313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.465793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.467269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.468718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.470246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.471724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.473176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.474585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.476026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.477528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.479125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.480704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
> 2019-08-02 16:09:40.521657: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
> 2019-08-02 16:09:40.544337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2019-08-02 16:09:40.544370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
> 2019-08-02 16:09:40.544379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N Y N N 
> 2019-08-02 16:09:40.544384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N 
> 2019-08-02 16:09:40.544390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N Y 
> 2019-08-02 16:09:40.544395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N 
> 2019-08-02 16:09:40.544401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y 
> 2019-08-02 16:09:40.544406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   Y N N N Y N Y Y 
> 2019-08-02 16:09:40.544412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y 
> 2019-08-02 16:09:40.544417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N Y N Y Y Y N 
> 2019-08-02 16:09:40.546196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.547686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.549121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.550673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.552148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.553626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.555043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.556558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.558033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.560367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14927 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
> 2019-08-02 16:09:40.561099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.562619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15021 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0)
> 2019-08-02 16:09:40.563084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.564620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15021 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:06.0, compute capability: 7.0)
> 2019-08-02 16:09:40.565064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.566603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15021 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:07.0, compute capability: 7.0)
> 2019-08-02 16:09:40.567064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.568599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 15021 MB memory) -> physical GPU (device: 4, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:08.0, compute capability: 7.0)
> 2019-08-02 16:09:40.569266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.570857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 15021 MB memory) -> physical GPU (device: 5, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:09.0, compute capability: 7.0)
> 2019-08-02 16:09:40.571494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.573081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 15021 MB memory) -> physical GPU (device: 6, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:0a.0, compute capability: 7.0)
> 2019-08-02 16:09:40.573449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-02 16:09:40.574937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 15021 MB memory) -> physical GPU (device: 7, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:0b.0, compute capability: 7.0)
> 2019-08-02 16:10:01.170625: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
> 2019-08-02 16:10:04.602996: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
> WARNING: Logging before flag parsing goes to stderr.
> W0802 16:10:18.277444 139908641928960 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_impl.py:182: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> Use tf.where in 2.0, which has the same broadcast rule as np.where
> Epoch: 1, Test set ELBO: -186.47406005859375, time elapse for current epoch 19.488028049468994
> Epoch: 2, Test set ELBO: -148.20664978027344, time elapse for current epoch 17.490494966506958
> Epoch: 3, Test set ELBO: -121.98389434814453, time elapse for current epoch 17.440164804458618
> Epoch: 4, Test set ELBO: -111.5521011352539, time elapse for current epoch 17.79745864868164
> Epoch: 5, Test set ELBO: -105.51742553710938, time elapse for current epoch 18.477241277694702
> Epoch: 6, Test set ELBO: -101.38650512695312, time elapse for current epoch 17.363281726837158
> Epoch: 7, Test set ELBO: -98.83760070800781, time elapse for current epoch 17.08821129798889
> Epoch: 8, Test set ELBO: -96.58488464355469, time elapse for current epoch 16.95319962501526




CONFIG_GPU_4:

> sudo+ssh://kristofgiber@35.233.7.176:22/usr/bin/python3.5 -u /home/kristofgiber/pycharm_project/VAE/MNIST/vae-tf2-mnist.py
> Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
> 11493376/11490434 [==============================] - 0s 0us/step
> 2019-08-01 22:24:39.613833: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
> 2019-08-01 22:24:40.055413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:40.056313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
> name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
> pciBusID: 0000:00:04.0
> 2019-08-01 22:24:40.056387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:40.057185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
> name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
> pciBusID: 0000:00:05.0
> 2019-08-01 22:24:40.057242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:40.058027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
> name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
> pciBusID: 0000:00:06.0
> 2019-08-01 22:24:40.058080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:40.058874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
> name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
> pciBusID: 0000:00:07.0
> 2019-08-01 22:24:40.190754: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
> 2019-08-01 22:24:40.667941: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
> 2019-08-01 22:24:40.888660: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
> 2019-08-01 22:24:40.975729: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
> 2019-08-01 22:24:41.345616: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
> 2019-08-01 22:24:41.705419: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
> 2019-08-01 22:24:43.004212: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
> 2019-08-01 22:24:43.004423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.005380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.006229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.007045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.007955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.009103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.009919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.010734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.011540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3
> 2019-08-01 22:24:43.116342: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
> 2019-08-01 22:24:43.899581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.899630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.899708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.899709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:43.975951: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ff38a0 executing computations on platform CUDA. Devices:
> 2019-08-01 22:24:43.976033: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
> 2019-08-01 22:24:43.976044: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla P100-PCIE-16GB, Compute Capability 6.0
> 2019-08-01 22:24:43.976068: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Tesla P100-PCIE-16GB, Compute Capability 6.0
> 2019-08-01 22:24:43.976076: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Tesla P100-PCIE-16GB, Compute Capability 6.0
> 2019-08-01 22:24:44.240728: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
> 2019-08-01 22:24:44.241991: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x581cf30 executing computations on platform Host. Devices:
> 2019-08-01 22:24:44.242029: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
> 2019-08-01 22:24:44.245715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.246563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
> name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
> pciBusID: 0000:00:04.0
> 2019-08-01 22:24:44.246633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.247439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
> name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
> pciBusID: 0000:00:05.0
> 2019-08-01 22:24:44.247499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.248289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
> name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
> pciBusID: 0000:00:06.0
> 2019-08-01 22:24:44.248340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.249122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
> name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
> pciBusID: 0000:00:07.0
> 2019-08-01 22:24:44.249193: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
> 2019-08-01 22:24:44.249219: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
> 2019-08-01 22:24:44.249247: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
> 2019-08-01 22:24:44.249273: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
> 2019-08-01 22:24:44.249297: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
> 2019-08-01 22:24:44.249312: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
> 2019-08-01 22:24:44.249344: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
> 2019-08-01 22:24:44.249386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.250193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.251001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.251828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.252663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.253475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.254302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.255141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.255953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3
> 2019-08-01 22:24:44.256368: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
> 2019-08-01 22:24:44.260879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2019-08-01 22:24:44.260918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 
> 2019-08-01 22:24:44.260929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y N N 
> 2019-08-01 22:24:44.260936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N N N 
> 2019-08-01 22:24:44.260943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   N N N Y 
> 2019-08-01 22:24:44.260950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   N N Y N 
> 2019-08-01 22:24:44.262165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.263013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.263904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.264737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.265689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.323846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15121 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
> 2019-08-01 22:24:44.324561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.325385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15216 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:05.0, compute capability: 6.0)
> 2019-08-01 22:24:44.326219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.327027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15216 MB memory) -> physical GPU (device: 2, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:06.0, compute capability: 6.0)
> 2019-08-01 22:24:44.327741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2019-08-01 22:24:44.328630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15216 MB memory) -> physical GPU (device: 3, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:07.0, compute capability: 6.0)
> 2019-08-01 22:25:04.473450: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
> 2019-08-01 22:25:07.950765: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
> WARNING: Logging before flag parsing goes to stderr.
> W0801 22:25:20.674070 140465772844800 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_impl.py:182: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> Use tf.where in 2.0, which has the same broadcast rule as np.where
> Epoch: 1, Test set ELBO: -188.11668395996094, time elapse for current epoch 18.63205885887146
> Epoch: 2, Test set ELBO: -145.51783752441406, time elapse for current epoch 17.303343534469604
> Epoch: 3, Test set ELBO: -122.49842071533203, time elapse for current epoch 17.674144983291626
> Epoch: 4, Test set ELBO: -112.31964111328125, time elapse for current epoch 17.33092761039734
> Epoch: 5, Test set ELBO: -106.07232666015625, time elapse for current epoch 17.66222333908081
> Epoch: 6, Test set ELBO: -101.88775634765625, time elapse for current epoch 17.44078493118286
> Epoch: 7, Test set ELBO: -98.75122833251953, time elapse for current epoch 17.688755989074707
> Epoch: 8, Test set ELBO: -96.532958984375, time elapse for current epoch 17.955048084259033





CONFIG_GPU_0:

> 8f8678b6dd38:python3 -u /opt/project/VAE/MNIST/vae-tf2-mnist.py
> Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
> 11493376/11490434 [==============================] - 13s 1us/step
> 2019-08-01 22:42:21.195909: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
> 2019-08-01 22:42:21.226199: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299735000 Hz
> 2019-08-01 22:42:21.227229: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3736960 executing computations on platform Host. Devices:
> 2019-08-01 22:42:21.227333: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
> 2019-08-01 22:42:21.228834: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.
> 2019-08-01 22:42:22.128135: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.
> WARNING: Logging before flag parsing goes to stderr.
> W0801 22:42:23.402617 140101723539200 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_impl.py:182: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> Use tf.where in 2.0, which has the same broadcast rule as np.where
> Epoch: 1, Test set ELBO: -188.50021362304688, time elapse for current epoch 92.1476879119873
> 2019-08-01 22:43:59.586590: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.
> Epoch: 2, Test set ELBO: -143.65415954589844, time elapse for current epoch 97.12298655509949
> 2019-08-01 22:45:41.824794: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.
> Epoch: 3, Test set ELBO: -121.26383972167969, time elapse for current epoch 96.25376057624817
> 2019-08-01 22:47:28.103122: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.
> Epoch: 4, Test set ELBO: -111.30313110351562, time elapse for current epoch 98.79311537742615
> Epoch: 5, Test set ELBO: -105.29031372070312, time elapse for current epoch 110.05837559700012
> Epoch: 6, Test set ELBO: -101.54240417480469, time elapse for current epoch 102.19214820861816
> Epoch: 7, Test set ELBO: -98.74303436279297, time elapse for current epoch 97.86401653289795
> Epoch: 8, Test set ELBO: -96.51799774169922, time elapse for current epoch 96.82956838607788




"
31283,Tensorflow graph execution does not give expected result.,"I am using `Tesla V100-PCIE-16GB` GPU on my system and using tensorflow version 1.13.1 .
I have two python example script. `example-1.py` runs fine. `example-2.py` contains same statements as `example-1.py` but multiple times. `example-2.py` fails with GPU OOM error. Because it transfer all the matrix data to the GPU initially.

### example-1.py
```
import tensorflow as tf
import time

N=20000
m=N
n=N
k=N

tf.random.set_random_seed(1234)
with tf.device('/cpu:0'):
  a1=tf.random.normal([m, k])
  b1=tf.random.normal([k, n])

  a2=tf.random.normal([m, k])
  b2=tf.random.normal([k, n])

with tf.device('/gpu:0'):
  c1=tf.matmul(a1, b1)
  c2=tf.matmul(a2, b2)

with tf.device('/cpu:0'):
  RESULT=tf.concat([c1, c2], 0)

sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
start=time.time()
a_out=sess.run(RESULT)
print(a_out.shape)
print(""Time Taken: %s"" % str(time.time() - start))
```

### example-2.py
```
import tensorflow as tf
import time

N=20000
m=N
n=N
k=N

tf.random.set_random_seed(1234)
with tf.device('/cpu:0'):
  a1=tf.random.normal([m, k])
  b1=tf.random.normal([k, n])

  a2=tf.random.normal([m, k])
  b2=tf.random.normal([k, n])

with tf.device('/gpu:0'):
  c1=tf.matmul(a1, b1)
  c2=tf.matmul(a2, b2)

with tf.device('/cpu:0'):
  RESULT=tf.concat([c1, c2], 0)

with tf.device('/cpu:0'):
  a3=tf.random.normal([m, k])
  b3=tf.random.normal([k, n])

  a4=tf.random.normal([m, k])
  b4=tf.random.normal([k, n])

with tf.device('/gpu:0'):
  c3=tf.matmul(a3, b3)
  c4=tf.matmul(a4, b4)

with tf.device('/cpu:0'):
  tmp=tf.concat([c3, c4], 0)
  RESULT=tf.concat([RESULT, tmp], 0)

with tf.device('/cpu:0'):
  a5=tf.random.normal([m, k])
  b5=tf.random.normal([k, n])

  a6=tf.random.normal([m, k])
  b6=tf.random.normal([k, n])

with tf.device('/gpu:0'):
  c5=tf.matmul(a5, b5)
  c6=tf.matmul(a6, b6)

with tf.device('/cpu:0'):
  tmp=tf.concat([c5, c6], 0)
  RESULT=tf.concat([RESULT, tmp], 0)

sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
start=time.time()
a_out=sess.run(RESULT)
print(a_out.shape)
print(""Time Taken: %s"" % str(time.time() - start))
```

### Other Details:
```
$ python --version
Python 3.6.8 :: Anaconda, Inc.
$ python -c 'import tensorflow as tf; print(tf.__version__)'
1.13.1
$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION=""Ubuntu 18.04.2 LTS""
```

Thanks.
"
31282,"""TypeError: expected str, bytes or os.PathLike object, not _io.BytesIO"" when trying to restore a model from a byte stream in tensorflow 2.0.0-beta0","in tensorflow version 2.0.0-alpha0 I was able to serialize and deserialize a model in memory by using the following code:

```
    def _serializeModel(self, model):
        import h5py

        with h5py.File('does not matter', driver='core', backing_store=False) as h5file:
            model.save(h5file)
            h5file.flush()
            return h5file.id.get_file_image().hex()

    def _restoreModel(self, serialized):
        from tensorflow.keras.models import load_model
        from io import BytesIO

        return load_model(BytesIO(bytes.fromhex(serialized)))
```

from tensorflow 2.0.0-beta0, instead, the `restoreModel` function throws the error:

```
  File ""/usr/local/lib/python3.6/site-packages/statwolfml/models/base_model.py"", line 142, in train
    self._model = self._restoreModel(self._model)
  File ""/usr/local/lib/python3.6/site-packages/statwolfml/models/base_model.py"", line 165, in _restoreModel
    return load_model(BytesIO(bytes.fromhex(serialized)))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py"", line 136, in load_model
    isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
  File ""/usr/local/lib/python3.6/site-packages/h5py/_hl/base.py"", line 41, in is_hdf5
    fname = os.path.abspath(fspath(fname))
TypeError: expected str, bytes or os.PathLike object, not _io.BytesIO
```

Did you remove the support to byte stream or is this a bug?

Thanks


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): all
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: don't know
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): >= 2.0.0-beta0
- Python version: 3.6
"
31281,Running Tensorflow on CPU is faster than running it on GPU,"I have an ASUS n552vw laptop that has a 4GB dedicated Geforce GTX 960M graphic card. I put these lines of code in the beginning of my code to compare training speed using GPU or CPU, and I saw it seems using the CPU wins!

For GPU:

    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    

For CPU:

    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
    

I have installed CUDA, cuDNN, tensorflow-gpu, etc to increase my training speed but seems inverse thing happened! 

When I try the first code, it says(before execution start):

    Train on 2128 samples, validate on 22 samples
    Epoch 1/1
    2019-08-02 18:49:41.828287: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
    2019-08-02 18:49:42.457662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
    name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176
    pciBusID: 0000:01:00.0
    totalMemory: 4.00GiB freeMemory: 3.34GiB
    2019-08-02 18:49:42.458819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
    2019-08-02 18:49:43.776498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
    2019-08-02 18:49:43.777007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
    2019-08-02 18:49:43.777385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
    2019-08-02 18:49:43.777855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3050 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)
    2019-08-02 18:49:51.834610: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally

And it's really slow ` [Finished in 263.2s]`, But when I try the second code it says:

    Train on 2128 samples, validate on 22 samples
    Epoch 1/1
    2019-08-02 18:51:43.021867: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
    2019-08-02 18:51:43.641123: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
    2019-08-02 18:51:43.645072: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: DESKTOP-UQ8B9FK
    2019-08-02 18:51:43.645818: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: DESKTOP-UQ8B9FK

And it's much faster than the first code `[Finished in 104.7s]` ! How is it possible??

This is the part of code that is related to `Tensorflow` :

    model = Sequential()
    model.add((LSTM(un , return_sequences = True)))
    model.add(Dropout(dp)) 
    
    model.add((LSTM(un , return_sequences = True)))
    model.add(Dropout(dp)) 
    
    model.add((LSTM(un , return_sequences = True)))
    model.add(Dropout(dp)) 
    
    model.add((LSTM(un , return_sequences = True)))
    model.add(Dropout(dp)) 
    
    model.add((LSTM(un , return_sequences = False)))
    model.add(Dropout(dp)) 
    
    model.add(RepeatVector(rp))
      
    model.add((LSTM(un , return_sequences= True))) 
    model.add(Dropout(dp))   
    
    model.add((LSTM(un , return_sequences= True))) 
    model.add(Dropout(dp))
    
    model.add((LSTM(un , return_sequences= True))) 
    model.add(Dropout(dp))
    
    model.add((LSTM(un , return_sequences= True))) 
    model.add(Dropout(dp))
    
    model.add((LSTM(un , return_sequences= True))) 
    model.add(Dropout(dp))

    model.add(TimeDistributed(Dense(ds))) "
31279,[TF 2.0 API Docs] tf.tuple,"
## Documentation contributor guide: 
https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/tuple

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):
### Correct links
The GitHub link to `tf.tuple` leads to `tf.tuple_v2`.
### Usage example
There is no usage example.
"
31278,[TF.2.0 API Docs] tf.train.list_variables,"
## URL(s) with the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/train/list_variables

## Description of the issue (what needs changing):
Raises for exceptions to deal with any errors and a few drawings to be added to make it easier to understand.

### Raises listed and defined
No, the errors are not listed and returned.

Are there currently visuals? If not, will it clarify the content?
No, there are not any current visuals.

### Submit a pull request?

No, I am not willing to submit a pull request.
"
31277,[TF 2.0 API Docs] tf.keras.backend.random_uniform,"## URL(s) with the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/backend/random_uniform

## Description of issue (what needs changing):
The function has no usage example and there are no errors raised.

### Raises listed and defined
No raises listed or defined.

### Usage example
No usage example

### Submit a pull request?
No.
"
31276,[TF 2.0 API Docs] tf.nn.log_softmax,"##  URL(s) with the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/log_softmax

## Description of the issue (what needs changing):
- No errors defined
- No visuals

### Are the errors defined?
- There are no errors defined

### Visuals, if applicable
- There are no visuals available. I think that visuals are needed to clarify the content.


"
31275,[TF 2.0 API Docs] tf.io.extract_jpeg,"## URL(s) with the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/encode_jpeg

## Description of issue (what needs changing):
- No link to github source code provided
- No `Raises` provided in the description
- No usage example

### Correct links

No link provided

### Raises listed and defined

Raises are not defined/listed.

### Usage example

There is no usage example

### Request visuals, if applicable

Currently, no visuals, may not be required.


"
31274,Large Batch training throws memory allocation errors in tf.keras and Eager Mode but works fine when using keras imports,"Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Relevant.
TensorFlow installed from (source or binary):
TensorFlow version (use command below): tf-nightly-gpu 1.15.0.dev20190728
Python version: Python 3.7.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: release 10.0, V10.0.130
NVIDIA-SMI 418.43 Driver Version: 418.43 CUDA Version: 10.1
GPU model and memory:
GeForce RTX 2080Ti 11GB

**Describe the current behavior**
tf.keras libraries and eager mode seems to use more GPU memory than keras libraries and limits batch size during training. We train on large batch sizes (ex. 2048) for faster convergence, which works fine when we use keras libraries but when we ported over to tf.keras and use it in eager mode, we see memory allocation errors. We also see this warning just before the memory error. Strangely, the training seem to continue after throwing the error in this reproducible code but our training stops after throwing this error.  See full trace at the end. 

019-08-02 09:35:48.951204: W tensorflow/core/common_runtime/bfc_allocator.cc:305] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.

**Describe the expected behavior**
Not use higher memory than directly using keras. 

**Code to reproduce the issue**
Change mode to tf.keras, which throws memory error but when we change it to direct keras, it works fine.

```
import tensorflow as tf

import numpy as np
from timeit import default_timer as timer

mode = ""tf.keras""
#mode = ""keras""

if(mode == ""tf.keras""):
    print(""Importing tf.keras"")
    tf.enable_eager_execution()
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Activation, CuDNNLSTM, Dense
    from tensorflow.keras.optimizers import Adadelta
else:
    from keras.engine.sequential import Sequential
    from keras.layers import LSTM, Activation, CuDNNLSTM, Dense
    from keras.optimizers import Adadelta

feature_count = 300
batch_size = 2048
look_back = 100
target_groups = 10


def random_data_generator( ):

    x_data_size =(batch_size, look_back, feature_count) # batches, lookback, features
    x_data = np.random.uniform(low=-1.0, high=5, size=x_data_size)
 
    y_data_size = (batch_size, target_groups)
    Y_data = np.random.randint(low=1, high=21, size=y_data_size)
    
    return x_data, Y_data
 
def get_simple_Dataset_generator():        
    while True:
        yield random_data_generator()

def build_model():
    model = Sequential()     
    model.add(CuDNNLSTM(feature_count,
                        batch_input_shape=(batch_size,look_back, feature_count),
                        stateful=False))

    model.add(Dense(target_groups, activation='softmax'))
    optimizer = Adadelta()        

    model.compile(loss='categorical_crossentropy', optimizer=optimizer,
                      metrics = ['accuracy'])
    return model

 
def run_training():
   
    model = build_model()
    train_generator = get_simple_Dataset_generator()
    validation_generator = get_simple_Dataset_generator()
    class_weights = {0:2, 1:8, 2:1, 3:4, 4:8, 5:35, 6:30, 7:4, 8:5, 9:3}
    model.fit_generator(generator = train_generator,
            steps_per_epoch=1,
            epochs=1000,            
            verbose=2,
            validation_data=validation_generator,
            validation_steps=20,
            max_queue_size = 10,
            workers = 0, 
            use_multiprocessing = False,
            class_weight = class_weights
            )

if __name__ == '__main__': 
    run_training()
 
```

**Other info / logs**
 2019-08-02 09:35:48.951204: W tensorflow/core/common_runtime/bfc_allocator.cc:305] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.
2019-08-02 09:35:59.359646: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.06GiB (rounded to 5429285376).  Current allocation summary follows.
2019-08-02 09:35:59.359694: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): 	Total Chunks: 19, Chunks in use: 19. 4.8KiB allocated for chunks. 4.8KiB in use in bin. 177B client-requested in use in bin.
2019-08-02 09:35:59.359706: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359717: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-08-02 09:35:59.359727: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359737: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): 	Total Chunks: 1, Chunks in use: 0. 5.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359752: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): 	Total Chunks: 9, Chunks in use: 8. 90.0KiB allocated for chunks. 82.0KiB in use in bin. 79.3KiB client-requested in use in bin.
2019-08-02 09:35:59.359762: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): 	Total Chunks: 1, Chunks in use: 1. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 16.0KiB client-requested in use in bin.
2019-08-02 09:35:59.359774: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359788: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): 	Total Chunks: 3, Chunks in use: 3. 240.0KiB allocated for chunks. 240.0KiB in use in bin. 240.0KiB client-requested in use in bin.
2019-08-02 09:35:59.359797: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359808: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359818: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): 	Total Chunks: 1, Chunks in use: 0. 666.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359829: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): 	Total Chunks: 4, Chunks in use: 4. 5.49MiB allocated for chunks. 5.49MiB in use in bin. 5.49MiB client-requested in use in bin.
2019-08-02 09:35:59.359838: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): 	Total Chunks: 12, Chunks in use: 12. 31.54MiB allocated for chunks. 31.54MiB in use in bin. 27.01MiB client-requested in use in bin.
2019-08-02 09:35:59.359848: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359857: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359867: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359877: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359887: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:35:59.359902: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): 	Total Chunks: 1, Chunks in use: 1. 234.38MiB allocated for chunks. 234.38MiB in use in bin. 234.38MiB client-requested in use in bin.
2019-08-02 09:35:59.359914: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): 	Total Chunks: 7, Chunks in use: 5. 9.32GiB allocated for chunks. 6.97GiB in use in bin. 6.89GiB client-requested in use in bin.
2019-08-02 09:35:59.359924: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 5.06GiB was 256.00MiB, Chunk State: 
2019-08-02 09:35:59.359937: I tensorflow/core/common_runtime/bfc_allocator.cc:891]   Size: 1016.97MiB | Requested Size: 996.52MiB | in_use: 0 | bin_num: 20, prev:   Size: 2.34MiB | Requested Size: 2.34MiB | in_use: 1 | bin_num: -1
2019-08-02 09:35:59.359949: I tensorflow/core/common_runtime/bfc_allocator.cc:891]   Size: 1.35GiB | Requested Size: 351.6KiB | in_use: 0 | bin_num: 20, prev:   Size: 1.14GiB | Requested Size: 1.14GiB | in_use: 1 | bin_num: -1
2019-08-02 09:35:59.359958: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8108529920
2019-08-02 09:35:59.359971: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd418000000 next 48 of size 5429285376
2019-08-02 09:35:59.359980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd55b9c5200 next 52 of size 1228800000
2019-08-02 09:35:59.359989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd5a4da5200 next 18446744073709551615 of size 1450444544
2019-08-02 09:35:59.359997: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 1073741824
2019-08-02 09:35:59.360005: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc000000 next 50 of size 2457600
2019-08-02 09:35:59.360014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc258000 next 47 of size 2457600
2019-08-02 09:35:59.360022: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc4b0000 next 54 of size 2457600
2019-08-02 09:35:59.360030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd5fc708000 next 18446744073709551615 of size 1066369024
2019-08-02 09:35:59.360041: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 536870912
2019-08-02 09:35:59.360048: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd63c000000 next 57 of size 245760000
2019-08-02 09:35:59.360057: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd64aa60000 next 18446744073709551615 of size 291110912
2019-08-02 09:35:59.360064: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 268435456
2019-08-02 09:35:59.360074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd6b0000000 next 18446744073709551615 of size 268435456
2019-08-02 09:35:59.360081: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 268435456
2019-08-02 09:35:59.360090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd6c0000000 next 18446744073709551615 of size 268435456
2019-08-02 09:35:59.360097: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8388608
2019-08-02 09:35:59.360106: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e000000 next 18 of size 1440000
2019-08-02 09:35:59.360114: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e15f900 next 21 of size 1440000
2019-08-02 09:35:59.360122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e2bf200 next 22 of size 1440000
2019-08-02 09:35:59.360130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e41eb00 next 18446744073709551615 of size 4068608
2019-08-02 09:35:59.360139: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 16777216
2019-08-02 09:35:59.360147: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71ee00000 next 37 of size 2457600
2019-08-02 09:35:59.360155: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f058000 next 55 of size 2521344
2019-08-02 09:35:59.360163: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f2bf900 next 60 of size 2889728
2019-08-02 09:35:59.360171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f581100 next 58 of size 2457600
2019-08-02 09:35:59.360179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f7d9100 next 56 of size 2457600
2019-08-02 09:35:59.360188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71fa31100 next 18446744073709551615 of size 3993344
2019-08-02 09:35:59.360196: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 4194304
2019-08-02 09:35:59.360204: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd721600000 next 6 of size 1440000
2019-08-02 09:35:59.360213: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd72175f900 next 18446744073709551615 of size 2754304
2019-08-02 09:35:59.360223: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 1048576
2019-08-02 09:35:59.360231: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00000 next 1 of size 1280
2019-08-02 09:35:59.360241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00500 next 4 of size 256
2019-08-02 09:35:59.360249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00600 next 7 of size 256
2019-08-02 09:35:59.360258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00700 next 12 of size 256
2019-08-02 09:35:59.360265: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00800 next 13 of size 256
2019-08-02 09:35:59.360274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00900 next 14 of size 256
2019-08-02 09:35:59.360281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00a00 next 15 of size 256
2019-08-02 09:35:59.360290: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00b00 next 16 of size 256
2019-08-02 09:35:59.360298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00c00 next 17 of size 256
2019-08-02 09:35:59.360307: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00d00 next 20 of size 256
2019-08-02 09:35:59.360314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00e00 next 25 of size 256
2019-08-02 09:35:59.360322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00f00 next 26 of size 256
2019-08-02 09:35:59.360330: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01000 next 27 of size 256
2019-08-02 09:35:59.360338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01100 next 28 of size 256
2019-08-02 09:35:59.360346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01200 next 29 of size 256
2019-08-02 09:35:59.360354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01300 next 32 of size 256
2019-08-02 09:35:59.360361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01400 next 33 of size 256
2019-08-02 09:35:59.360370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01500 next 34 of size 256
2019-08-02 09:35:59.360378: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01600 next 35 of size 256
2019-08-02 09:35:59.360387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01700 next 38 of size 256
2019-08-02 09:35:59.360394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae01800 next 9 of size 5376
2019-08-02 09:35:59.360403: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae02d00 next 10 of size 9728
2019-08-02 09:35:59.360411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae05300 next 19 of size 9728
2019-08-02 09:35:59.360420: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae07900 next 8 of size 14336
2019-08-02 09:35:59.360428: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae0b100 next 11 of size 12032
2019-08-02 09:35:59.360437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae0e000 next 23 of size 9728
2019-08-02 09:35:59.360445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae10600 next 24 of size 12032
2019-08-02 09:35:59.360455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae13500 next 30 of size 8192
2019-08-02 09:35:59.360462: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae15500 next 31 of size 81920
2019-08-02 09:35:59.360471: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae29500 next 40 of size 81920
2019-08-02 09:35:59.360478: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae3d500 next 41 of size 8192
2019-08-02 09:35:59.360486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae3f500 next 42 of size 16384
2019-08-02 09:35:59.360494: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae43500 next 44 of size 8192
2019-08-02 09:35:59.360502: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae45500 next 45 of size 81920
2019-08-02 09:35:59.360510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae59500 next 18446744073709551615 of size 682752
2019-08-02 09:35:59.360518: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 2097152
2019-08-02 09:35:59.360525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79b000000 next 18446744073709551615 of size 2097152
2019-08-02 09:35:59.360534: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: 
2019-08-02 09:35:59.360544: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 19 Chunks of size 256 totalling 4.8KiB
2019-08-02 09:35:59.360553: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB
2019-08-02 09:35:59.360561: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 8192 totalling 16.0KiB
2019-08-02 09:35:59.360569: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 9728 totalling 28.5KiB
2019-08-02 09:35:59.360578: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 12032 totalling 23.5KiB
2019-08-02 09:35:59.360586: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 14336 totalling 14.0KiB
2019-08-02 09:35:59.360596: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 16384 totalling 16.0KiB
2019-08-02 09:35:59.360604: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 81920 totalling 240.0KiB
2019-08-02 09:35:59.360612: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 4 Chunks of size 1440000 totalling 5.49MiB
2019-08-02 09:35:59.360621: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2097152 totalling 2.00MiB
2019-08-02 09:35:59.360629: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 6 Chunks of size 2457600 totalling 14.06MiB
2019-08-02 09:35:59.360637: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2521344 totalling 2.40MiB
2019-08-02 09:35:59.360645: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2754304 totalling 2.63MiB
2019-08-02 09:35:59.360653: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2889728 totalling 2.76MiB
2019-08-02 09:35:59.360662: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 3993344 totalling 3.81MiB
2019-08-02 09:35:59.360670: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4068608 totalling 3.88MiB
2019-08-02 09:35:59.360680: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 245760000 totalling 234.38MiB
2019-08-02 09:35:59.360688: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 268435456 totalling 512.00MiB
2019-08-02 09:35:59.360696: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 291110912 totalling 277.62MiB
2019-08-02 09:35:59.360704: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1228800000 totalling 1.14GiB
2019-08-02 09:35:59.360715: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 5429285376 totalling 5.06GiB
2019-08-02 09:35:59.360723: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 7.24GiB
2019-08-02 09:35:59.360731: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 10288519424 memory_limit_: 10288519578 available bytes: 154 curr_region_allocation_bytes_: 17179869184
2019-08-02 09:35:59.360743: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                 10288519578
InUse:                  7771009536
MaxInUse:               7771009536
NumAllocs:                     155
MaxAllocSize:           5429285376

2019-08-02 09:35:59.360758: W tensorflow/core/common_runtime/bfc_allocator.cc:424] *****************************************************************_____________*__________***********
2019-08-02 09:36:09.532360: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.06GiB (rounded to 5429285376).  Current allocation summary follows.
2019-08-02 09:36:09.532426: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): 	Total Chunks: 19, Chunks in use: 19. 4.8KiB allocated for chunks. 4.8KiB in use in bin. 177B client-requested in use in bin.
2019-08-02 09:36:09.532438: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532446: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-08-02 09:36:09.532453: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532461: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): 	Total Chunks: 1, Chunks in use: 0. 5.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532470: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): 	Total Chunks: 9, Chunks in use: 8. 90.0KiB allocated for chunks. 82.0KiB in use in bin. 79.3KiB client-requested in use in bin.
2019-08-02 09:36:09.532478: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): 	Total Chunks: 1, Chunks in use: 1. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 16.0KiB client-requested in use in bin.
2019-08-02 09:36:09.532486: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532495: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): 	Total Chunks: 3, Chunks in use: 3. 240.0KiB allocated for chunks. 240.0KiB in use in bin. 240.0KiB client-requested in use in bin.
2019-08-02 09:36:09.532501: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532510: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532520: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): 	Total Chunks: 1, Chunks in use: 0. 666.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532528: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): 	Total Chunks: 4, Chunks in use: 4. 5.49MiB allocated for chunks. 5.49MiB in use in bin. 5.49MiB client-requested in use in bin.
2019-08-02 09:36:09.532538: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): 	Total Chunks: 12, Chunks in use: 12. 31.54MiB allocated for chunks. 31.54MiB in use in bin. 27.01MiB client-requested in use in bin.
2019-08-02 09:36:09.532545: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532552: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532558: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532567: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532575: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-08-02 09:36:09.532589: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): 	Total Chunks: 1, Chunks in use: 1. 234.38MiB allocated for chunks. 234.38MiB in use in bin. 234.38MiB client-requested in use in bin.
2019-08-02 09:36:09.532600: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): 	Total Chunks: 7, Chunks in use: 5. 9.32GiB allocated for chunks. 6.97GiB in use in bin. 6.89GiB client-requested in use in bin.
2019-08-02 09:36:09.532609: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 5.06GiB was 256.00MiB, Chunk State: 
2019-08-02 09:36:09.532624: I tensorflow/core/common_runtime/bfc_allocator.cc:891]   Size: 1016.97MiB | Requested Size: 996.52MiB | in_use: 0 | bin_num: 20, prev:   Size: 2.34MiB | Requested Size: 2.34MiB | in_use: 1 | bin_num: -1
2019-08-02 09:36:09.532635: I tensorflow/core/common_runtime/bfc_allocator.cc:891]   Size: 1.35GiB | Requested Size: 351.6KiB | in_use: 0 | bin_num: 20, prev:   Size: 1.14GiB | Requested Size: 1.14GiB | in_use: 1 | bin_num: -1
2019-08-02 09:36:09.532642: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8108529920
2019-08-02 09:36:09.532657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd418000000 next 48 of size 5429285376
2019-08-02 09:36:09.532664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd55b9c5200 next 52 of size 1228800000
2019-08-02 09:36:09.532673: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd5a4da5200 next 18446744073709551615 of size 1450444544
2019-08-02 09:36:09.532679: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 1073741824
2019-08-02 09:36:09.532687: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc000000 next 50 of size 2457600
2019-08-02 09:36:09.532693: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc258000 next 47 of size 2457600
2019-08-02 09:36:09.532700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc4b0000 next 54 of size 2457600
2019-08-02 09:36:09.532706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd5fc708000 next 18446744073709551615 of size 1066369024
2019-08-02 09:36:09.532713: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 536870912
2019-08-02 09:36:09.532719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd63c000000 next 57 of size 245760000
2019-08-02 09:36:09.532726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd64aa60000 next 18446744073709551615 of size 291110912
2019-08-02 09:36:09.532731: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 268435456
2019-08-02 09:36:09.532738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd6b0000000 next 18446744073709551615 of size 268435456
2019-08-02 09:36:09.532747: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 268435456
2019-08-02 09:36:09.532753: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd6c0000000 next 18446744073709551615 of size 268435456
2019-08-02 09:36:09.532761: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8388608
2019-08-02 09:36:09.532767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e000000 next 18 of size 1440000
2019-08-02 09:36:09.532773: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e15f900 next 21 of size 1440000
2019-08-02 09:36:09.532780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e2bf200 next 22 of size 1440000
2019-08-02 09:36:09.532787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e41eb00 next 18446744073709551615 of size 4068608
2019-08-02 09:36:09.532795: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 16777216
2019-08-02 09:36:09.532803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71ee00000 next 37 of size 2457600
2019-08-02 09:36:09.532810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f058000 next 55 of size 2521344
2019-08-02 09:36:09.532818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f2bf900 next 60 of size 2889728
2019-08-02 09:36:09.532826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f581100 next 58 of size 2457600
2019-08-02 09:36:09.532833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f7d9100 next 56 of size 2457600
2019-08-02 09:36:09.532841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71fa31100 next 18446744073709551615 of size 3993344
2019-08-02 09:36:09.532847: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 4194304
2019-08-02 09:36:09.532853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd721600000 next 6 of size 1440000
2019-08-02 09:36:09.532861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd72175f900 next 18446744073709551615 of size 2754304
2019-08-02 09:36:09.532866: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 1048576
2019-08-02 09:36:09.532876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00000 next 1 of size 1280
2019-08-02 09:36:09.532884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00500 next 4 of size 256
2019-08-02 09:36:09.532891: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00600 next 7 of size 256
2019-08-02 09:36:09.532898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00700 next 12 of size 256
2019-08-02 09:36:09.532904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00800 next 13 of size 256
2019-08-02 09:36:09.532912: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00900 next 14 of size 256
2019-08-02 09:36:09.532917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00a00 next 15 of size 256
2019-08-02 09:36:09.532926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00b00 next 16 of size 256
2019-08-02 09:36:09.532933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00c00 next 17 of size 256
2019-08-02 09:36:09.532939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00d00 next 20 of size 256
2019-08-02 09:36:09.532946: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00e00 next 25 of size 256
2019-08-02 09:36:09.532953: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00f00 next 26 of size 256
2019-08-02 09:36:09.532959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01000 next 27 of size 256
2019-08-02 09:36:09.532966: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01100 next 28 of size 256
2019-08-02 09:36:09.532973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01200 next 29 of size 256
2019-08-02 09:36:09.532979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01300 next 32 of size 256
2019-08-02 09:36:09.532985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01400 next 33 of size 256
2019-08-02 09:36:09.532991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01500 next 34 of size 256
2019-08-02 09:36:09.532997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01600 next 35 of size 256
2019-08-02 09:36:09.533005: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01700 next 38 of size 256
2019-08-02 09:36:09.533010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae01800 next 9 of size 5376
2019-08-02 09:36:09.533018: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae02d00 next 10 of size 9728
2019-08-02 09:36:09.533025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae05300 next 19 of size 9728
2019-08-02 09:36:09.533031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae07900 next 8 of size 14336
2019-08-02 09:36:09.533038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae0b100 next 11 of size 12032
2019-08-02 09:36:09.533045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae0e000 next 23 of size 9728
2019-08-02 09:36:09.533050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae10600 next 24 of size 12032
2019-08-02 09:36:09.533061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae13500 next 30 of size 8192
2019-08-02 09:36:09.533068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae15500 next 31 of size 81920
2019-08-02 09:36:09.533074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae29500 next 40 of size 81920
2019-08-02 09:36:09.533081: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae3d500 next 41 of size 8192
2019-08-02 09:36:09.533087: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae3f500 next 42 of size 16384
2019-08-02 09:36:09.533094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae43500 next 44 of size 8192
2019-08-02 09:36:09.533102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae45500 next 45 of size 81920
2019-08-02 09:36:09.533109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae59500 next 18446744073709551615 of size 682752
2019-08-02 09:36:09.533115: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 2097152
2019-08-02 09:36:09.533123: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79b000000 next 18446744073709551615 of size 2097152
2019-08-02 09:36:09.533128: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: 
2019-08-02 09:36:09.533137: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 19 Chunks of size 256 totalling 4.8KiB
2019-08-02 09:36:09.533143: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB
2019-08-02 09:36:09.533150: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 8192 totalling 16.0KiB
2019-08-02 09:36:09.533157: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 9728 totalling 28.5KiB
2019-08-02 09:36:09.533163: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 12032 totalling 23.5KiB
2019-08-02 09:36:09.533172: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 14336 totalling 14.0KiB
2019-08-02 09:36:09.533180: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 16384 totalling 16.0KiB
2019-08-02 09:36:09.533188: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 81920 totalling 240.0KiB
2019-08-02 09:36:09.533195: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 4 Chunks of size 1440000 totalling 5.49MiB
2019-08-02 09:36:09.533203: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2097152 totalling 2.00MiB
2019-08-02 09:36:09.533210: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 6 Chunks of size 2457600 totalling 14.06MiB
2019-08-02 09:36:09.533218: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2521344 totalling 2.40MiB
2019-08-02 09:36:09.533226: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2754304 totalling 2.63MiB
2019-08-02 09:36:09.533234: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2889728 totalling 2.76MiB
2019-08-02 09:36:09.533242: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 3993344 totalling 3.81MiB
2019-08-02 09:36:09.533248: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4068608 totalling 3.88MiB
2019-08-02 09:36:09.533255: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 245760000 totalling 234.38MiB
2019-08-02 09:36:09.533262: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 268435456 totalling 512.00MiB
2019-08-02 09:36:09.533268: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 291110912 totalling 277.62MiB
2019-08-02 09:36:09.533275: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1228800000 totalling 1.14GiB
2019-08-02 09:36:09.533283: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 5429285376 totalling 5.06GiB
2019-08-02 09:36:09.533290: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 7.24GiB
2019-08-02 09:36:09.533296: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 10288519424 memory_limit_: 10288519578 available bytes: 154 curr_region_allocation_bytes_: 17179869184
2019-08-02 09:36:09.533307: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                 10288519578
InUse:                  7771009536
MaxInUse:               7771009536
NumAllocs:                     157
MaxAllocSize:           5429285376

2019-08-02 09:36:09.533324: W tensorflow/core/common_runtime/bfc_allocator.cc:424] *****************************************************************_____________*__________***********
"
31273,[TF2.0] Use Adam and GradientTape() to optimize own function wrt to input (no keras model),"**Optimization**
I want to optimize my own loss function with adam optimizer of keras, with respect to the input. Unfortunately this doesn't work with GradientTape(). 

**Executed Code with TF2.0**
```
@tf.function
def train(opt, input):
   with tf.GradientTape() as tape:
        tape.watch(input)
        loss = tf.reduce_mean(input)
    gradients = tape.gradient(loss, input)
    opt.apply_gradients(zip(gradients, input))

opt = tf.keras.optimizers.Adam(learning_rate=1)
input = tf.random.normal((1, 1))

train(opt, input)
```

**Current behavior**
Code gives error if `opt.apply_gradients(zip(gradients, input))` is called.
TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.




"
31272,[TF2] UnicodeDecodeError when using tf.saved_model.save,"**System information**
- Have I written custom code: Yes.
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0.0-dev20190731
- Python version: 3.6.7
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 1080 Ti

**Describe the current behavior**
Code crashes with following exception:
```
Traceback (most recent call last):
  File ""mwe.py"", line 10, in <module>
    tf.saved_model.save(model, '/tmp/model')
  File ""***/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py"", line 869, in save
    saveable_view, asset_info.asset_index)
  File ""***/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py"", line 624, in _serialize_object_graph
    _write_object_proto(obj, obj_proto, asset_file_def_index)
  File ""***/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py"", line 663, in _write_object_proto
    metadata=obj._tracking_metadata)
  File ""***/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2816, in _tracking_metadata
    metadata = json.loads(super(Model, self)._tracking_metadata)
  File ""***/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2198, in _tracking_metadata
    metadata['config'] = self.get_config()
  File ""***/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 878, in get_config
    layer_config = layer.get_config()
  File ""***/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2378, in get_config
    'node_def': self.node_def.SerializeToString().decode('utf-8'),
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 68: invalid start byte
```

This seems to be a regression; I was running code I had working ago some weeks/months with earlier TF2 nightlies, and this appears to me as unexpected behavior.

Might be introduced by https://github.com/tensorflow/tensorflow/commit/7cc180f107f142432358ac33787466de90afd776

To me, `<protobuf object>` `.SerializeToString().decode('utf-8')` seems incorrect, since `SerializeToString()` can return arbitrary binary data (https://developers.google.com/protocol-buffers/docs/pythontutorial#parsing-and-serialization ""Note that the bytes are binary, not text; we only use the str type as a convenient container.""), decodability into UTF-8 cannot be guaranteed. This bug might have gone unnoticed, as many other serializations just happen to be be UTF-8 decodable 

**Describe the expected behavior**

A properly saved model.

**Code to reproduce the issue**
```python
import tensorflow as tf
import tensorflow.keras as keras

input_ = keras.layers.Input((None, None, None), dtype='float32')

output = tf.image.extract_patches(input_, sizes=(1,128,128,1), strides=(1,128,128,1), rates=(1,1,1,1), padding='VALID')

model = keras.Model(inputs=[input_], outputs=[output])

tf.saved_model.save(model, '/tmp/model')
```"
31271,"[tflite][micro] [patch] Incorrect arguments to ComputePaddingHeightWidth, causing wrong offsets in maxpool implementation","With a 1x4 kernel on a one dimensional tensor the max pool result was always minimum (0 in my case), because no input data was selected.

See padding.h: TfLitePaddingValues ComputePaddingHeightWidth(int stride_height, int stride_width, int dilation_rate_height, int **dilation_rate_width**, int in_height, int in_width, int filter_height, int filter_width, TfLitePadding **padding**, int* out_height, int* out_width)

```
---
 tensorflow/lite/experimental/micro/kernels/pooling.cc | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/tensorflow/lite/experimental/micro/kernels/pooling.cc b/tensorflow/lite/experimental/micro/kernels/pooling.cc
index 385143050f..4acda7ef68 100644
--- a/tensorflow/lite/experimental/micro/kernels/pooling.cc
+++ b/tensorflow/lite/experimental/micro/kernels/pooling.cc
@@ -44,9 +44,9 @@ TfLiteStatus CalculateOpData(const TfLiteContext* context,
   int out_height, out_width;
 
   data->padding = ComputePaddingHeightWidth(
-      params->stride_height, params->stride_width, /* dilation_rate= */ 1,
+      params->stride_height, params->stride_width, /* dilation_rate= */ 1,1,
       height, width, params->filter_height, params->filter_width,
-      params->padding, params->padding, &out_height, &out_width);
+      params->padding, &out_height, &out_width);
 
   return kTfLiteOk;
 }
-- 
```"
31270,"tf.data.Iterator complain ""Function tf_data_structured_function_wrapper_xxxx is not defined.""","
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS release 6.3 (Final)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.10.0
- Python version: 2.7.14
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: - (CPU)
- GPU model and memory: - (CPU)

('v1.10.0-0-g656e7a2b34', '1.10.0')

**Describe the current behavior**

As the title said, it complains 

```
NotFoundError: Function tf_data_structured_function_wrapper_qCoCAdpVJm4 is not defined.
	 [[Node: IteratorGetNext_8 = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator_9)]]
```

I follow the sample code of [api doc](https://www.tensorflow.org/versions/r1.10/api_docs/python/tf/data/Iterator#from_structure) , **Except that I change the `tf.data.Dataset` creating place.**

In original sample code, it creates 2 datasets before run `sess.run`, while I create 1 dataset  after the other dataset has run out, and it complains.

If it is hard to understand, I'll try to describe it after the following code section.

**Describe the expected behavior**

All is ok.

**Code to reproduce the issue**

```Python
from tensorflow.data import Dataset 
from tensorflow.data import Iterator
import tensorflow as tf

iterator = Iterator.from_structure(tf.int64, tf.TensorShape([]))

def gen():
    for i in range(30):
        yield i

def model_fn(ite):
  return (ite + 1, ite + 23)

prediction, loss = model_fn(iterator.get_next())

with tf.Session() as sess:
  dataset_range = Dataset.from_generator(gen, tf.int64)
  range_initializer = iterator.make_initializer(dataset_range)

  sess.run(range_initializer)
  while True:
    try:
      pred, loss_val = sess.run([prediction, loss])
      print pred
    except tf.errors.OutOfRangeError:
      break

  dataset_evens = Dataset.from_generator(gen, tf.int64)
  evens_initializer = iterator.make_initializer(dataset_evens)

  sess.run(evens_initializer)
  while True:
    try:
      ## <------ HERE!! OH, it complaint!
      pred, loss_val = sess.run([prediction, loss])
      print pred
    except tf.errors.OutOfRangeError:
      break
```

if I change the `dataset_evens = Dataset.from_generator(gen, tf.int64)` to the place after `dataset_range = Dataset.from_generator(gen, tf.int64)`, it also will be OK.

**Other info / logs**

Explain Why I need create after `sess.run`:
because our data is generated from a generator, and I want to shuffle it at every epoch end.
The easiest way and most clear way is change the generator (let it generates data in shuffle order) at every epoch end.

Why not `repeat()`?  => Oh, it can't report the `epoch end` signal.

Has other way? => YES, I can keep only 1 generator and change the generator's inner state to shuffle, BUT it may be tricky and cause confusion?


"
31269,no such file or directory -Vs Build tools 2017 no longer available,"**System information**
- OS Platform and Distribution: Windows Server 2016
- TensorFlow installed from: trying to install from source
- TensorFlow version: Current/Master
- Python version: 3.6
- Bazel version (if compiling from source): 0.26.1
- CUDA/cuDNN version: CPU only
- GPU model and memory: CPU only

**Describe the problem**
When trying to build tensorflow with bazel, the Visual Studio 2017 Build Tools cannot be located:
    C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC (No such file or directory)  

Unfortunately the Build tools for VS 2017 could no longer be located on the Microsoft Website...
VS Build tools for VS 2019 have been installed but are not supported.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Rather a deprecation than a bug. Is there any source for 2017 Build tools available? or possibility to build with 2019 build tools?
"
31267,How to configure Tensorflow to use a specific GPU?,"These are the activated devices that I have:

    [name: ""/device:CPU:0""
    device_type: ""CPU""
    memory_limit: 268435456
    locality {
    }
    incarnation: 5415837867258701517
    , name: ""/device:GPU:0""
    device_type: ""GPU""
    memory_limit: 3198956339
    locality {
      bus_id: 1
      links {
      }
    }
    incarnation: 12462133041849407996
    physical_device_desc: ""device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0""
    ]

What I want to do is to configure my program to use `GeForce GTX 960M` and also make this configuration permanent for all my previous/future programs if is it possible?"
31266,undeclared inclusion(s) in rule '//tensorflow/lite/toco/python:toco_python_api',"
**System information**
- OS Platform and Distribution: 19.04
- TensorFlow installed from (source or binary): 2.0beta1
- TensorFlow version: 2.0beta1
- Python version: 3.7.3
- Bazel version: 0.26.0
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1/7.6.2
- GPU architecture: 5.2 

**Describe the problem**
```console
ERROR: ....../tensorflow/tensorflow/lite/toco/python/BUILD:22:1: undeclared inclusion(s) in rule '//tensorflow/lite/toco/python:toco_python_api':
```

Any further suggestions?"
31265,No gradients provided for any variable ,"tensorflow : 2.0.0-beta1
python : 3.5.2

raise error 
```
ValueError: No gradients provided for any variable: ['dense/kernel:0', 
```
my code blew 

```
from tensorflow import keras
import tensorflow as tf
import os


class Generator(keras.Model):

    def __init__(self, n_f=512, n_k=4):
        super(Generator, self).__init__()
        self.model = keras.models.Sequential([
            keras.layers.Input((100)),
            keras.layers.Dense(3 * 3 * n_f, activation=tf.nn.leaky_relu),
            keras.layers.Reshape((3, 3, n_f)),
            keras.layers.Conv2DTranspose(filters=n_f // 2, kernel_size=3, strides=2, padding='valid'),
            keras.layers.BatchNormalization(),
            keras.layers.LeakyReLU(),
            keras.layers.Conv2DTranspose(filters=n_f // 4, kernel_size=n_k, strides=2, padding='same'),
            keras.layers.BatchNormalization(),
            keras.layers.LeakyReLU(),
            keras.layers.Conv2DTranspose(filters=1, kernel_size=n_k, strides=2, padding='same'),

        ])

    def call(self, inputs, training=None, mask=None):
        return self.model(inputs, training=training, mask=mask)


class Discriminator(keras.Model):

    def __init__(self, n_f=512, n_k=4):
        super(Discriminator, self).__init__()

        self.model = keras.models.Sequential([
            keras.layers.Input((28, 28, 1)),
            keras.layers.Conv2D(filters=n_f, kernel_size=n_k, strides=2, padding='same', activation=tf.nn.leaky_relu),
            keras.layers.Conv2D(filters=n_f * 2, kernel_size=n_k, strides=2, padding='same'),
            keras.layers.BatchNormalization(),
            keras.layers.Conv2D(filters=n_f * 4, kernel_size=n_k, strides=2, padding='same'),
            keras.layers.BatchNormalization(),
            keras.layers.Flatten(),
            keras.layers.Dense(1),
        ])

    def call(self, inputs, training=None, mask=None):
        return self.model(inputs, training=training, mask=mask)


class GAN():

    def __init__(self, generator, discriminator, lr=1e-3):
        self.generator = generator
        self.discriminator = discriminator

        self.d_optimizer = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5)
        self.g_optimizer = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5)

    def _get_loss(self, logits, label):
        return tf.losses.mean_squared_error(logits, label)

    def _gan_loss_fn(self, generator, discriminator, input_noise, real_image):
        fake_image = generator(input_noise)
        real_logits = discriminator(fake_image)
        fake_logits = discriminator(real_image)

        g_loss = self._get_loss(fake_logits, tf.ones_like(fake_logits, dtype=tf.int32))
        d_loss = self._get_loss(real_logits, tf.ones_like(real_logits, dtype=tf.int32)) + \
                 self._get_loss(fake_logits, tf.zeros_like(fake_logits, dtype=tf.int32))

        return g_loss, d_loss

    def train(self, real_image, input_noise, verbose=False):
        g_loss, d_loss = self._gan_loss_fn(self.generator, self.discriminator, input_noise, real_image)
        with tf.GradientTape() as tape:
            grads = tape.gradient(g_loss, self.generator.trainable_variables)
            self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))

        with tf.GradientTape() as tape:
            grads = tape.gradient(d_loss, self.discriminator.trainable_variables)
            self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_variables))

        if verbose:
            print(""d_loss : %.5f , g_loss : %.5f"" % (d_loss, g_loss))

    def save_weights(self, path):
        os.makedirs(path, exist_ok=True)
        self.generator.save_weights(os.path.join(path, 'generator'))
        self.discriminator.save_weights(os.path.join(path, 'discriminator'))

    def restore_weights(self, path):
        self.generator.load_weights(os.path.join(path, 'generator'))
        self.discriminator.load_weights(os.path.join(path, 'discriminator'))


if __name__ == '__main__':
    import numpy as np
    import sys
    print(sys.version)
    print(tf.__version__)

    gan = GAN(Generator(), Discriminator())
    batch_size = 64
    gan.train(np.random.rand(batch_size, 28, 28, 1).astype(np.float32),
              np.random.rand(batch_size, 100).astype(np.float32))

```"
31264,GradientTape() unable to compute the gradient wrt Keras model inputs,"**System information**
- TensorFlow version (you are using): 2.0.0beta
- Are you willing to contribute it (Yes/No): yes



**Describe the feature and the current behavior/state.**
It is easy to use keras.backend.gradients to compute the gradients of loss wrt model input in a Keras model. However this function has been banned in TF 2.0 and GradientTape now cannot provide this usage, as model input cannot be watched before model.fit() and will be useless to be watched after that! Here is an example:

```
def train_step(images, labels):
  with tf.GradientTape() as tape:
    predictions = model(images)
    tape.watch(model.inputs)
    loss = loss_object(labels, predictions)
  print(tape.gradient(loss, model.inputs))
```
It will result in [None] no matter where I put the tape.watch sentense.

**Will this change the current api? How?**
I think no.

**Who will benefit with this feature?**
Everyone who need to calculate the gradients for further usage.

"
31262,"I have installed ""tensorflos-gpu"" but it doesn't work(Asus N552VW with 2 graphic cards)","I just installed `tensorflow-gpu` successfully. I also installed `tensorflow` after that successfully. I tried the following code on `spyder3`:

    from tensorflow.python.client import device_lib
    print(device_lib.list_local_devices())

But the result is:

    [name: ""/device:CPU:0""
    device_type: ""CPU""
    memory_limit: 268435456
    locality {
    }
    incarnation: 18318063967424845606
    ]

It seems the GPU version of tensorflow doesn't work! How can I fix it?

**System information**
- OS Platform and Distribution (Windows 10):
- TensorFlow installed from (pip install tensorflow/tensorflow-gpu):
- TensorFlow version: 1.14
- Python version: 3.7.3
- Installed using virtualenv? pip
- CUDA/cuDNN version: 10.1 /  7.6.2
- GPU model and memory: Geforce GTX 960M

I also must mention that my laptop has 2 graphic cards that the second card that I installed CUDA/cuDNN for that is `Geforce GTX 960M`.

I also tried the following code:

    import tensorflow as tf
    
    # Creates a graph.
    with tf.device('/device:GPU:2'):
      a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
      b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
      c = tf.matmul(a, b)
    # Creates a session with allow_soft_placement and log_device_placement set
    # to True.
    sess = tf.Session(config=tf.ConfigProto(
          allow_soft_placement=True, log_device_placement=True))
    # Runs the op.
    print(sess.run(c))

And got this result:

    [[22. 28.]
     [49. 64.]]"
31261,Build TFLite Model Benchmark Tool with GPU delegate for iOS?,"**System information**
- TensorFlow version (you are using):master repogitory
- Are you willing to contribute it (Yes/No):Yes

**Describe the feature and the current behavior/state.**
I'm trying to build TFLite Model Benchmark Tool with GPU delegate on iOS devices.
@freedomtan showed us the great patches for Android and some samples on #27402. I deeply appreciate his commit.
However it is still difficult for me to build the latest TFLite model benchmark tool with the latest GPU delegate. Unfortunately, building TFLite core library `libtensorflow-lite.a` and benchmark module `benchmark-lib.a` depends on the shell script `/tensorflow/lite/tools/make/build_ios_universal_lib.sh` using CMake. On the other hand, building GPU delegate module `libmetal_delegate.a` depends on bazel `//tensorflow/lite/delegates/gpu:metal_delegate`. Hence I'm puzzled. 
Would you like to provide bazel script to build TFLite Model Benchmark Tool with GPU delegate for iOS?

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Anyone using the TFLite.

**Any Other info.**
"
31258,tensorboard error:'tensorflow.python.estimator.api.estimator' has no attribute 'SessionRunHook',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win 10
- TensorFlow installed from (source or binary):binary
- TensorFlow version:1.14.0
- Python version:3.6.4
- Installed using virtualenv? pip? conda?:pip




**Describe the problem**
  I input 'tensorboard' in cmd,and get an error:'tensorflow.python.estimator.api.estimator' has no attribute 'SessionRunHook'.
  My tensorflow,tensorboard,tensorflow-estimator all share the same version,1.14.0.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
tensorboard

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31256,can contrib.model_pruning speed up old model? ,"i wonder if when i prune my old model, can it speed up the old large dense model?"
31255,how to do prediction after quantizing model,"while i transformer ckpt to pb model by freeze and quantize model to compression_pb_model,  and then serve the compression_pb_model occuring the errors like this:

ValueError: NodeDef mentions attr 'batch_dims' not in Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>; NodeDef: {{node bert/embeddings/GatherV2}} = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, batch_dims=0](bert/embeddings/word_embeddings/read, bert/embeddings/Reshape, zeros/Const). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).

however, i run the code in the same meachine,,,thx,,,"
31253,Using class_weights in fit_generator causes continuous increase in CPU memory usage until depletion. OOM,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Relevant.
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): tf-nightly-gpu           1.15.0.dev20190728
- Python version: Python 3.7.4 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: release 10.0, V10.0.130
NVIDIA-SMI 418.43       Driver Version: 418.43       CUDA Version: 10.1  
- GPU model and memory:
GeForce RTX 2080Ti 11GB

**Describe the current behavior**
When using class_weights in fit_generator causes the training process to continuously consume more and more CPU RAM until depletion. There is a stepped increased in memory usage after each epoch. See below for the reproducible example.  To keep the reproducible example small, I decreased the size of the dataset and batch size, which shows the trend of increasing memory. While training with my actual data, it depletes the full 128GB RAM by 70 EPOCS.  

In the code below, if you comment out the class weights, the program trains without depleting memory. 

**Describe the expected behavior**
Not leak memory. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
tf.enable_eager_execution()
import numpy as np
 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import CuDNNLSTM, Dense
from tensorflow.keras.optimizers import Adadelta


feature_count = 25
batch_size = 16
look_back = 5
target_groups = 10

def random_data_generator( ):
    x_data_size =(batch_size, look_back, feature_count) # batches, lookback, features
    x_data = np.random.uniform(low=-1.0, high=5, size=x_data_size)
 
    y_data_size = (batch_size, target_groups)
    Y_data = np.random.randint(low=1, high=21, size=y_data_size)
    
    return x_data, Y_data
 
def get_simple_Dataset_generator():        
    while True:
        yield random_data_generator()

def build_model():
    model = Sequential()
    model.add(CuDNNLSTM(feature_count,
                    batch_input_shape=(batch_size,look_back, feature_count),
                    stateful=False))  
    model.add(Dense(target_groups, activation='softmax'))
    optimizer = Adadelta(learning_rate=1.0, epsilon=None) 
    model.compile(loss='categorical_crossentropy', optimizer=optimizer) 
    return model


def run_training():
   
    model = build_model()
    train_generator = get_simple_Dataset_generator()
    validation_generator = get_simple_Dataset_generator()
    class_weights = {0:2, 1:8, 2:1, 3:4, 4:8, 5:35, 6:30, 7:4, 8:5, 9:3}

    model.fit_generator(generator = train_generator,
            steps_per_epoch=1,
            epochs=1000,            
            verbose=2,
            validation_data=validation_generator,
            validation_steps=20,
            max_queue_size = 10,
            workers = 0, 
            use_multiprocessing = False,
            class_weight = class_weights
            )

if __name__ == '__main__': 
    run_training()
 
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
# Memory Usage When class_weights are used
![Mem_with_class_weights](https://user-images.githubusercontent.com/46456896/62335750-aea84900-b492-11e9-9b23-72b29c6f2499.png)

# Memory Without class_weights
![Mem_wo_class_weights](https://user-images.githubusercontent.com/46456896/62335751-aea84900-b492-11e9-9f2a-ef9c296850ee.png)
"
31252,Model can't be checkpointed with Keras+MultiworkerMirroredStrategy,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin-18.0.0-x86_64-i386-64bit
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-beta1
Python version:3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
You can collect some of this information using our environment capture
script
You can also obtain the TensorFlow version with: 1. TF 1.0: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" 2. TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""

Describe the current behavior
This is a followup for https://github.com/tensorflow/tensorflow/issues/31070

I tried following 2 solutions
1.I applied 6345ad5
to my tensorflow installed code
2. I install latest nightly dev build

both gave me following error, seems though previous commit change data type to int64, somewhere else still expects int32

```
2019-08-01 22:41:51.971726: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at collective_ops.cc:354 : Internal: RecvBufResponse returned 8 bytes where to_tensor expected 4
Traceback (most recent call last):
  File ""example_tf2.py"", line 124, in <module>
    steps_per_epoch = parallel_steps)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py"", line 643, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py"", line 776, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 360, in _run_single_worker
    return worker_fn(strategy)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py"", line 771, in _worker_fn
    return fn(instance, model, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py"", line 681, in fit
    steps_name='steps_per_epoch')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py"", line 294, in model_iteration
    batch_outs = f(actual_inputs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py"", line 813, in execution_function
    return [out.numpy() for out in distributed_function(input_fn)]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py"", line 416, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py"", line 359, in _initialize
    *args, **kwds))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/function.py"", line 1360, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/function.py"", line 1648, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/function.py"", line 1541, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 716, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py"", line 309, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 706, in wrapper
    raise e.ag_error_metadata.to_exception(type(e))
tensorflow.python.autograph.impl.api.StagingError: in converted code:

    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py:804 distributed_function  *
        outputs = strategy.experimental_run_v2(
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:708 experimental_run_v2
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1710 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:708 _call_for_each_replica
        fn, args, kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:195 _call_for_each_replica
        coord.join(threads)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py:389 join
        six.reraise(*self._exc_info_to_raise)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception
        yield
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:926 run
        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py:908 train_on_batch
        output_loss_metrics=self._output_loss_metrics)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_eager.py:307 train_on_batch
        output_loss_metrics=output_loss_metrics))
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_eager.py:260 _process_single_batch
        model.trainable_weights))
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:434 apply_gradients
        self._create_hypers()
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:608 _create_hypers
        aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:770 add_weight
        aggregation=aggregation)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/base.py:713 _add_variable_with_custom_getter
        **kwargs_for_getter)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:154 make_variable
        shape=variable_shape if variable_shape else None)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:260 __call__
        return cls._variable_v1_call(*args, **kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call
        shape=shape)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:60 getter
        return captured_getter(captured_previous, **kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/shared_variable_creator.py:69 create_new_variable
        v = next_creator(*args, **kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:60 getter
        return captured_getter(captured_previous, **kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1250 creator_with_resource_vars
        return self._create_variable(*args, **kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:368 _create_variable
        _real_mirrored_creator, *args, **kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:251 _create_mirrored_variable
        value_list = real_mirrored_creator(devices, *args, **kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:355 _real_mirrored_creator
        v = next_creator(*args, **kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:60 getter
        return captured_getter(captured_previous, **kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py:347 variable_capturing_scope
        lifted_initializer_graph=lifted_initializer_graph, **kwds)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:264 __call__
        return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py:139 __init__
        initial_value() if init_from_fn else initial_value,
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:330 _overridden_initial_value_fn
        group_key, collective_instance_key)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/collective_ops.py:161 broadcast_recv
        instance_key=instance_key)
    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_collective_ops.py:66 collective_bcast_recv
        _six.raise_from(_core._status_to_exception(e.code, message), None)
    /root/.local/lib/python2.7/site-packages/six.py:737 raise_from
        raise value

    InternalError: RecvBufResponse returned 8 bytes where to_tensor expected 4 [Op:CollectiveBcastRecv]


```

Describe the expected behavior
Keras model could be checkpoint-ed under multi worker training

Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from __future__ import absolute_import, division, print_function, unicode_literals
import datetime
import json
import os
import tensorflow_datasets as tfds
import tensorflow as tf
import subprocess
import shlex
import sys

tfds.disable_progress_bar()

BUFFER_SIZE = 60000
BATCH_SIZE = 64

NUM_WORKERS = 2
GLOBAL_BATCH_SIZE = NUM_WORKERS * BATCH_SIZE

if __name__ == ""__main__"":
  worker_addrs = ['localhost:9999', 'localhost:9998']
  os.environ['TF_CONFIG'] = json.dumps({
      'cluster': {
          'worker': worker_addrs,
      },
      'task': {'type': 'worker', 'index': int(sys.argv[1])}
  })

  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
        metrics=['accuracy'])
    return model

  datasets, info = tfds.load(name='mnist',
                             with_info=True,
                             as_supervised=True)

  train_datasets_unbatched = datasets['train'].map(scale).shuffle(BUFFER_SIZE)

  train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE)

  with strategy.scope():
    multi_worker_model = build_and_compile_cnn_model()

   
  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath='/tmp/chk.hdf5',
        monitor='val_loss',
        save_best_only=True,
        load_weights_on_restart=True)

  multi_worker_model.fit(x=train_datasets, epochs=100, callbacks = [checkpoint_callback])

```
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
31250,google.protobuf.json_format.MessageToDict incorrectly decodes bytes_list,"**System information**

> - Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

YES, custom code.
> - OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

CentOS 7-1810 in VirtualBox VM
```
uname -a
Linux centos7-1810 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
> - TensorFlow installed from (source or binary):

Binary.  Unchanged from tf-nightly.
> - TensorFlow version (use command below):

Note that a warning is printed if I use tf.GIT_VERSION.
```
print(tf.version.GIT_VERSION, tf.VERSION) 
v1.12.1-7396-g12481e7e74 1.15.0-dev20190730
```
> - Python version:

Python 3.6.8 :: Anaconda, Inc.

**Describe the current behavior**
google.protobuf.json_format.MessageToDict, I believe ""decodes"" bytes_list incorrectly.  What I get is:
`['VGF4aSBBZmZpbGlhdGlvbiBTZXJ2aWNlcw==']`

**Describe the expected behavior**
google.protobuf.json_format.MessageToDict should decode my byte_list to:
`[b'Taxi Affiliation Services']`
, which is what I get when looking directly at the tf.train.Example.

Data is from the chicago_taxi_pipeline demo at:
https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline

""data_tfrecord-00000-of-00001"" is the ""eval"" output of:
tfx.components.example_gen.csv_example_gen.component.CsvExampleGen

**Code to reproduce the issue**
```
import tensorflow as tf
import google.protobuf.json_format as jfmt

print(tf.__version__)
example = next(tf.python_io.tf_record_iterator(""data_tfrecord-00000-of-00001""))
result = tf.train.Example.FromString(example)

company_name_direct = result.features.feature['company'].bytes_list.value
print(company_name_direct)

result_as_dict = jfmt.MessageToDict(result)
company_name_indirect = result_as_dict['features']['feature']['company']['bytesList']['value'] 
print(company_name_indirect)
```
```
# output:
1.15.0-dev20190730
[b'Taxi Affiliation Services']
['VGF4aSBBZmZpbGlhdGlvbiBTZXJ2aWNlcw==']
```

**Other info / logs**
I have not tested this on any other build except 1.15 nightly.
"
31249,tensorflow + numpy compatibility?,"
```console
  ~ pip show tensorflow
pipName: tensorflow
Version: 1.14.0
  ~ pip show numpy
Name: numpy
Version: 1.17.0
```


```console
>>> import tensorflow as tf
~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])

```"
31248,WALSMatrixFactorization: Input matrix is not invertible. node cond/MatrixSolve,"**System information**
- Custom code: Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device: Noe
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.13.0-rc2-0-gc865ec5621 / 1.13.0-rc2
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Factorizing a small 4x4 (dense) matrix with WALSMatrixFactorizaion results in the following error:

> InvalidArgumentError (see above for traceback): Input matrix is not invertible.
> 	 [[node cond/MatrixSolve (defined at /home/milad/PycharmProjects/wals_rec/temp.py:55) ]] 

**Describe the expected behavior**
Factorize the matrix and evaluate without the error.

**Code to reproduce the issue**

	import numpy as np
	import tensorflow as tf
	from tensorflow.contrib.factorization import WALSMatrixFactorization
	import shutil


	def make_input_fn(args):

		def to_sparse(values):
			indices = np.array([[0], [1], [2], [3]], dtype=np.int64)
			sp_tensor = tf.SparseTensor(indices=indices, values=values, dense_shape=values.shape)
			return sp_tensor

		def _input_fn():
			rows_dataset = tf.data.Dataset.from_tensor_slices(args['matrix_values']). \
				map(map_func=to_sparse). \
				repeat(count=None). \
				batch(batch_size=args['batch_size'])

			cols_dataset = tf.data.Dataset.from_tensor_slices(np.transpose(args['matrix_values'])). \
				map(map_func=to_sparse). \
				repeat(count=None). \
				batch(batch_size=4)

			features = {
				WALSMatrixFactorization.INPUT_ROWS: rows_dataset.make_one_shot_iterator().get_next(),
				WALSMatrixFactorization.INPUT_COLS: cols_dataset.make_one_shot_iterator().get_next(),
				WALSMatrixFactorization.PROJECT_ROW: tf.constant(True, name=""PROJECT_ROW"")
			}
			return features, None

		return _input_fn


	def train_and_evaluate(args):

		def experiment_fn(output_dir):
			return tf.contrib.learn.Experiment(
				tf.contrib.factorization.WALSMatrixFactorization(
					num_rows=args[""nrows""],
					num_cols=args[""ncols""],
					row_weights=None,
					col_weights=None,
					regularization_coeff=0,
					embedding_dimension=args[""n_embeds""],
					model_dir=output_dir),
				train_input_fn=make_input_fn(args),
				eval_input_fn=make_input_fn(args),
				train_steps=10,
				eval_steps=1,
				# Error doesn't happen if min_eval_frequency > train_steps
				min_eval_frequency=5)

		from tensorflow.contrib.learn.python.learn import learn_runner
		learn_runner.run(experiment_fn=experiment_fn, output_dir=args[""output_dir""])


	if __name__ == '__main__':

		K = 4
		args = {
			'ncols': K,
			'nrows': K,
			'output_dir': 'wals_temp',
			'n_embeds': 2,
			'n_steps': 5,
			'batch_size': K,
			'matrix_values': np.array([[1, 2, 3, 4],
									   [2, 4, 8, 12],
									   [3, 6, 7, 13],
									   [3, 5, 7, 9]], dtype=np.float32)

		}

		shutil.rmtree(args['output_dir'], ignore_errors=True)
		train_and_evaluate(args)

**Other info / logs**
The error happens on [this line](https://github.com/tensorflow/tensorflow/blob/27d5e277dd6b9a6c3a331ae0dfd51361607d6b9b/tensorflow/contrib/factorization/python/ops/factorization_ops.py#L919). The cause seems to be that the (column) Gramian suddenly becomes zero during evaluation and therefore the lhs of the normal equation is not invertible (training steps run fine). I confirmed this by running it step by step and using tfdbg.
"
31247,Branch r2.0 Failed Source Build on macOS Mojave,"**System information**
- OS Platform and Distribution: macOS Mojave, 10.14.5 (18F203), (15-inch 2018)
- TensorFlow installed from (source or binary): Source
- TensorFlow version: r2.0
- Python version: Here's the result for `python` in terminal
```
  Python 3.7.3 (default, Mar 27 2019, 16:54:48) 
  [Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
  Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```
- Bazel version (if compiling from source): 0.26.0
- GCC/Compiler version (if compiling from source): The result for `gcc -v`
```
Configured with: --prefix=/Applications/Xcode-beta.app/Contents/Developer/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/4.2.1
Apple clang version 11.0.0 (clang-1100.0.31.5)
Target: x86_64-apple-darwin18.6.0
Thread model: posix
InstalledDir: /Applications/Xcode-beta.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
```
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**
Build failed with `bazel run //tensorflow/python/autograph/operators:py_builtins_test` command. I have tried both answering yes and no for `Do you wish to download a fresh release of clang?` question on `./configure`, everything else is the default.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Clone tensorflow from official github repo
2. Checkout to branch r2.0
3. run `./configure` and press enter (all default) except for `Do you wish to download a fresh release of clang?` where I tried both yes & no
4. `bazel run //tensorflow/python/autograph/operators:py_builtins_test`


**Any other info / logs**
Here's one of the `./configure` log: https://pastebin.com/yqsPgKeS
Here's the terminal log for `yes` on bazel run command: https://pastebin.com/cvsdbcHY
Here's the terminal log for `no` on bazel run command: https://pastebin.com/VrEVn8ig
"
31246,Trying to quantize resnet50 tf model,"I am trying to quantize [resnet50 tf model](https://github.com/tensorflow/models/tree/master/official/resnet) (NHWC) using imagenet images. 

I am using [this](https://www.tensorflow.org/lite/performance/post_training_quantization) tutorial for reference.

The code is like that - 
![quantize](https://user-images.githubusercontent.com/39498824/62318443-ba731b80-b450-11e9-9dc4-32ec55ac3c0a.png)


I am confused about how to take image inputs as a numpy array inside representative_dataset_gen(). I have tried like this -

![dataset_gen](https://user-images.githubusercontent.com/39498824/62318466-c3fc8380-b450-11e9-9c85-691395f86e38.png)

It is not working.

"
31245,gRPC: terminate called after throwing an instance of 'std::bad_alloc',"System information
     OS Platform and Distribution: CentOS Linux release 7.4.1708 (Core)
     TensorFlow version : Tensorflow-1.12.0 built from source

Describe the problem:
Running a model parallel implementation results in the following error:
``` terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
  Aborted
```
The code runs fine on a single node i.e. with a single worker, but distributing across 2 nodes/2 workers results in the above error. Suspect it is related to grpc.

Source code / logs
On the chief worker:
```
import tensorflow as tf
from time import time

def conv_op(inputs, kernel_, name):
    with tf.variable_scope(name) as scope:
        conv = tf.nn.conv3d(inputs, kernel_, [1, 1, 1, 1, 1], dilations=[1, 1, 1, 1, 1], padding='SAME')
    return conv

def inference_withconv(inputs, kernel_):
    with tf.device('/job:worker/task:{}'.format(0)):
        conv1_woc = conv_op(inputs, kernel_, 'conv1_woc')
    with tf.device('/job:worker/task:{}'.format(1)):
        conv2_woc = conv_op(conv1_woc, kernel_, 'conv2_woc')
    with tf.device('/job:worker/task:{}'.format(0)):
        convadd_results = tf.math.add(conv1_woc, conv2_woc)
        return convadd_results

def run_benchmark():
    image_shape = (1,1024,1024,1024,1)
    kernel_1_shape = (5,5,5,1,1)
    bias_shape = (1)

    dummy_image = tf.truncated_normal(
       image_shape,
       dtype=tf.float32,
       mean=0,
       stddev=1,
       name='3Dconv_image_1')

    dummy_kernel_1 = tf.truncated_normal(
       kernel_1_shape,
       dtype=tf.float32,
       mean=0,
       stddev=1,
       name='3Dconv_kernel_1')

    image_shape = (1, 1024, 1024, 1024, 1)
    image_init = tf.placeholder(tf.float32, shape=image_shape, name='input_1')

    res_ = inference_withconv(image_init, dummy_kernel_1)

    # Define the cluster spec
    cluster_spec = tf.train.ClusterSpec({'worker' : [('<ip_address_1>' + "":"" + '2222'), ('<ip_address_2>' + "":"" + '2222')]})

    task_id=0 # Chief worker
    server_config = tf.ConfigProto(inter_op_parallelism_threads=2, intra_op_parallelism_threads=20)
    server = tf.train.Server(cluster_spec, job_name='worker', task_index=task_id, config=server_config)

    session_config = tf.ConfigProto(
      inter_op_parallelism_threads=2,
      intra_op_parallelism_threads=20)

    with tf.Session(server.target, config=session_config) as sess:
        sess.run(tf.initialize_all_variables())
        image_, kernel_1 = sess.run([dummy_image, dummy_kernel_1])
        infer_results_ = sess.run(res_, feed_dict={'input_1:0': image_, '3Dconv_kernel_1:0': kernel_1})

if __name__ == '__main__':
    run_benchmark()
```

On the non-chief worker (on another node with a different IP address):

```
import tensorflow as tf

# Define the cluster spec
cluster_spec = tf.train.ClusterSpec({'worker' : [('<ip_address_1>' + "":"" + '2222'), ('<ip_address_2>' + "":"" + '2222')]})

task_id=1 # Non-chief worker
server_config = tf.ConfigProto(inter_op_parallelism_threads=2, intra_op_parallelism_threads=20)
server = tf.train.Server(cluster_spec, job_name='worker', task_index=task_id, config=server_config)

server.join()


```

"
31244,Make TF-TRT  input and output  semantic same as  TF ,"**System information**
- TensorFlow version (you are using):1.13.1
- Are you willing to contribute it (Yes/No):Do not think have the capability yet. 



**Describe the feature and the current behavior/state.**

In TF-TRT the outputs  have to be specified in create_inference_graph unlike in TF, where outputs can be selected at sess.run ().     Also, in TF not all placeholders need to be assigned at sess.run () if  in the network they have not effect in the output.  That is not the case with TF-TRT. These 2 semantic differences make it very hard for complex TF network to be converted to TF-TRT. 

**Will this change the current api? How?**

**Who will benefit with this feature?**
Any TF-TRT user. 

**Any Other info.**
"
31243,Model loaded via tf.keras.load_model is very slow,"OS: Ubuntu 18.04
Python 3.6.8
Tensorflow version 2.0.0b1 (GPU)
GPU: Titan RTX
CUDA version 10.0

Issue: current script is working fast
```
data=np.random.rand(30,16000)
data = np.expand_dims(data, axis=2)
#model = tf.keras.models.load_model('newmodel.h5')
model = keras.Sequential()
model.add(keras.layers.LSTM(15, input_shape=(16000, 1), return_sequences=True))
for i in range(8):
    model.add(keras.layers.LSTM(15, return_sequences=True))
model.add(keras.layers.Dense(1))
model.compile(loss='mae', optimizer='adam')
est=model.predict(data)
model.save(""newmodel.h5"")
```

After finishing this script I launch next one, which work very slow (30-40 sec).

```
data=np.random.rand(30,16000)
data = np.expand_dims(data, axis=2)
model = tf.keras.models.load_model('newmodel.h5')
est=model.predict(data)
```
the last two lines of first script output
```
2019-08-01 20:09:24.245721: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-01 20:09:24.631869: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
```
Second script for some reason doesn't output the loading of libcudnn.so.7
"
31242,events.out.tfevents file is getting too large as training continues,"
Tensorboard event file is getting too large (> 10Gig) as the training continues. I use custom estimator and input_fn reads data from a generator. Everything works fine except this huge tensorboard file that I think it also reduces the speed. When I increase save_summary_steps parameter, this problem doesn't go away. I have seen other blogs complaining about it and none of them provided a viable solution for it. 





**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): tensorflow-gpu==1.12.0 
tensorflow==1.13.1
tensorflow-estimator==1.13.0
- Python version: Python 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 9
- GPU model and memory: Tesla M40 24GB or Nvidia Geforece GTX 1080Ti 


"
31241,Error when changing transformer hyperparameters,"## URL(s) with the issue:
https://www.tensorflow.org/beta/tutorials/text/transformer#set_hyperparameters

## Description of issue (what needs changing):
I've been working with the 'Transformer model for language understanding' notebook on my own dataset. I got it to work with the default hyperparameters. The tutorial explains that I can create a Transformer XL by adjusting the hyperparameters to those that are used in the paper. I changed them to the suggested values, and I am now getting a `ValueError` when I try to train.
```
ValueError: Tensor's shape (8220, 128) is not compatible with supplied shape (8220, 512)
```
I think that this means that some object is not configured properly (not using the hyper parameter variables), but I can't figure out where it's happening. I tried restarting the runtime and running everything again, but it didn't help.

## System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have made small adjustments to the provided code in the transformer notebook to accommodate my own data. I also changed the values of the hyper parameters of the model.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  I am running the notebook in a Colab GPU runtime; Linux Ubuntu 18.04.2
- TensorFlow installed from (source or binary): I'm not exactly sure, I install it using this command, provided with the notebook `pip install -q tensorflow-gpu==2.0.0-beta1`
- TensorFlow version (use command below): `tensorflow-gpu==2.0.0-beta1`
- Python version: 3.6.8
- CUDA version: CUDA: 10.0.130
- GPU model and memory: I'm not sure how to get this info, but it's a Colab GPU runtime.

## Code snippets
I changed the output encoder from a `SubwordTextEncoder` to a `TokenTextEncoder`:
```
tokenizer_out = tfds.features.text.TokenTextEncoder(
    unique_concepts
)
```
I changed the `tf_encode` function to use a single argument instead of two:
```
def tf_encode(element):
    return tf.py_function(encode, [element[0], element[1]], [tf.int64, tf.int64])
```
And I changed the hyperparameter values:
```
num_layers = 6
d_model = 512
dff = 2048
num_heads = 8
```"
31240,SequenceFeatures gets overly complex when used with a sequence_numeric_column,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- TensorFlow installed from (source or binary): from pip install
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14

**Describe the current behavior**

The behaviour of `SequenceFeatures` layer is really different between when it's used with a feature column produced with `sequence_numeric_column` and when it's used with a feature column produced with `sequence_categorical_column_with_identity`and `embedding_column`.

I've [already reported an issue](https://github.com/tensorflow/tensorflow/issues/29879) where the `call` method of `SequenceFeatures` used with `sequence_numeric_column` unnecessarily requires its input to be a `SparseTensor` instead of a regular `Tensor`.

Today I was experimenting with Keras Funcional API by building a model using two `SequenceFeatures` layers along with both `sequence_numeric_column` and `embedding_column`. I got graphic depictions of the model using `tf.keras.Model.summary` and `tf.keras.utils.plot_model` and discovered that `SequenceFeatures`  with `sequence_numeric_column` gets decomposed in a million of sub-layers of type `TensorFlowOpLayer`, as opposite to the clean and to the point depiction of `SequenceFeatures` with `embedding_column`. This has the only effect of overloading the results of `summary`and `plot_model`with not so relevant information instead of having a concise insight of the graph.

Here is the obtained plot:
![Large and complex plot of the model](https://github.com/durandg12/nn_pictures/blob/master/total_mess.png)

The ![upper part](https://github.com/durandg12/nn_pictures/blob/master/zoom.png) of the plot is due to the conversion of the input into sparse tensor, so this is related to [the previous issue](https://github.com/tensorflow/tensorflow/issues/29879). The focus of the issue I'm opening today is ![this part](https://github.com/durandg12/nn_pictures/blob/master/zoom2.png) which should not exist in my opinion.

**Describe the expected behavior**

Assuming my previous issue is solved, I think that the result of `plot_model` should simply ![look like this](https://github.com/durandg12/nn_pictures/blob/master/expected_model.png).

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
from tensorflow.feature_column import embedding_column, sequence_categorical_column_with_identity, \
    sequence_numeric_column
from tensorflow.keras import Input, Model
from tensorflow.keras.experimental import SequenceFeatures
from tensorflow.keras.layers import Input, Dense


#on my computer I have used a
#custom plot_model instead of tf.keras.utils.plot_model 
#because of a bug, see nota bene below
#from deep.modeltodot import plot_model
from tensorflow.keras.utils import plot_model

print(tf.version.GIT_VERSION, tf.version.VERSION)

#Model preparation
seq_fc_dense = sequence_numeric_column('denseFeat')
seq_layer_dense = SequenceFeatures(seq_fc_dense, name='denseFeatLayer')

nb_cat = 5
seq_fc_cat = sequence_categorical_column_with_identity('catFeat', nb_cat)
seq_fc_cat = embedding_column(seq_fc_cat, 2)
seq_layer_cat = SequenceFeatures(seq_fc_cat, name='catFeatLayer')

input_dense = Input(shape=(None,), name='denseFeat')
input_cat = Input(shape=(None,), name='catFeat', dtype=tf.int32)
# we need to convert input_dense to a sparse tensor, see https://github.com/tensorflow/tensorflow/issues/29879
zero = tf.constant(0, dtype=tf.float32)
indices = tf.where(tf.not_equal(input_dense, zero))
values = tf.gather_nd(input_dense, indices)
sparse = tf.SparseTensor(indices, values, tf.cast(tf.shape(input_dense), dtype=tf.int64))

x_dense = seq_layer_dense({'denseFeat': sparse})[0]
x_cat = seq_layer_cat({'catFeat': input_cat})[0]
x = tf.concat([x_dense, x_cat], -1)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs={'denseFeat': input_dense, 'catFeat': input_cat}, outputs=output)

#model.summary()
plot_model(model)
```

**Nota bene**
The custom `plot_model` that I used to make my plots is the same as `tf.keras.utils.plot_model` except that I replaced the following import in keras/utils/vis_utils.py:
```
try:
  # pydot-ng is a fork of pydot that is better maintained.
  import pydot_ng as pydot
except ImportError:
  # pydotplus is an improved version of pydot
  try:
    import pydotplus as pydot
  except ImportError:
    # Fall back on pydot if necessary.
    try:
      import pydot
    except ImportError:
      pydot = None
```
by `import pydot_ng as pydot`and I removed the call to `_check_pydot` in `model_to_dot` because it was raising an exception.
"
31239,Tflite model show a completely different result than frozen inference model,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, I use a Github repo
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung A3 2016
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):1.13.1
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0/7.4
- GPU model and memory: Nvidia GeForce 840m



**Describe the current behavior**
Hello, 
I have trained my custom dataset using the Github project [link](https://github.com/yinguobing/cnn-facial-landmark) to create a model that detects eye region with landmarks.
I have converted the ckpt files model to pb and from pb to tflite model.
Everything is OK here when testing with ckpt files and pb frozen graph in prediction it gives 75 %, 80 % predictions :
That a result on ibug dataset
![resulttaa](https://user-images.githubusercontent.com/19480228/62306855-e7d8bd00-b482-11e9-92e1-05756165c8c3.PNG)

I have used ML Kit firebase SDK to integrate tflite model and I have followed instruction for how to use custom model.
For the first time, I tried to print just values (x,y) couples of points ( landmarks ) that can be drawn on the eye region contours

**Describe the expected behavior**
I got values but are not the same with the same image tested with ckpt files.
I have 

**Code to reproduce the issue**
I have add a new issue on firebase Ml kit , you can find the code and logs [here](https://github.com/firebase/quickstart-android/issues/923)


Update:
This is the difference showing between tflite model prediction and frozen inference model prediction:

+ Tflite model prediction:

![2019-08-07](https://user-images.githubusercontent.com/19480228/62698776-426f9d00-b9de-11e9-9e7a-0927c467ff78.png) 

+ frozen inference graph model prediction:

![image](https://user-images.githubusercontent.com/19480228/62699230-55cf3800-b9df-11e9-98e9-8a2fb5f9563a.png)



How can I solve this?
I really need help to fix this problem.
Thanks
"
31237,Incomplete description of LSTM.__call__() params,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/48af54a586790cc29150be3d8665d7e8a1770257/tensorflow/python/keras/layers/recurrent.py#L2461

## Description of issue (what needs changing):

The current documentation reads:

```
  Call arguments:
    inputs: A 3D tensor.
    mask: Binary tensor of shape `(samples, timesteps)` indicating whether
      a given timestep should be masked.
    training: Python boolean indicating whether the layer should behave in
      training mode or in inference mode. This argument is passed to the cell
      when calling it. This is only relevant if `dropout` or
      `recurrent_dropout` is used.
    initial_state: List of initial state tensors to be passed to the first
      call of the cell.
```

It would be worth mentioning that

- mask, training, and initial_state are optional
- if initial_state is not provided, default zeros are imputed (by calling `cell.get_initial_state()` or `cell.zero_state()`), see tensorflow_core/python/ops/rnn.py lines 677 or 1382.

Due to iheritance of RNN layers (incl. mixins) and missing documentation at some places (haven't found any comments on `get_initial_state`), it's quite hard to figure it out.

### Submit a pull request?

I'd like to keep it to someone who really knows the Keras internals; to me it's still a bit cryptic (i.e., where exactly `get_initial_state_fn` comes from, etc.)"
31236,PyCharm debugger automatically evaluates Tensor._shape and spams console,"**System information**
- OS Platform and Distribution: macOS 10.14.6
- TensorFlow version: 1.14.0
- Python version: 3.6.8

**Describe the current behavior**
When analyzing a tensor in eager-execution mode in the ""Watches"" of the PyCharm Debugger, the deprecated object Tensor._shape is automatically evaluated. This results in following warning in the console (two times for one evaluation). When evaluating many tensor, this substantially spams the console.

```console
W0801 15:29:42.581780 4505736640 ops.py:465] Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.
```

**Describe the expected behavior**
Significant reduce in the amount of warnings, ideally to zero.

**Code to reproduce the issue**
```python
import tensorflow as tf

tf.compat.v1.enable_eager_execution()

import numpy as np

tensor_1 = tf.convert_to_tensor(np.ones((20, 20)))

print()  # set breakpoint here
```"
31235,tf.numpy_function throws error when encounter np.transpose,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): kinda
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04LTS Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a
- TensorFlow installed from (source or binary): conda forge
- TensorFlow version (use command below): 1.14.0
- Python version: 2.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 9
- GPU model and memory: Quadrao 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am trying to move some of Keras's image [transformation functions](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/affine_transformations.py) into my dataset constructor function, e.g. 

```python
# inside my function that makes a dataset
filenames = [...]

preprocess_fn = lambda p: preprocess_image(p, some, default, arguments)

# workaround as shown by @mrry in https://github.com/tensorflow/tensorflow/issues/12396
tf_preproc_fn = lambda p: tf.numpy_function(preprocess_fn, [p], tf.float32)

path_ds = tf.data.Dataset.from_tensor_slices(fullpaths)
imgs_ds = path_ds.map(tf_preproc_fn, num_parallel_calls=AUTOTUNE)


```
where  at somepoint in `preprocess_fn` `np.rollaxis` is called, which in turns calls `np.transpose`. this throws:

```

  File ""/home/user/anaconda3/envs/tf/lib/python3.6/site-packages/numpy/core/numeric.py"", line 1554, in rollaxis
    return a.transpose(axes)

AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'transpose'


	 [[{{node PyFunc}}]] [Op:IteratorGetNextSync]
```


**Describe the expected behavior**

That `tf.numpy_function` just ""works"". Already had to use `tf.cast(tensor.shape[0], tf.float32)` to prevent some errors.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.



```python
# from keras
def apply_channel_shift(x:list, intensity:float, channel_axis:int=0)->list:
    '''
    Performs a channel shift.

    Arguments
        x (np.ndarray): Input tensor. Must be 3D.
        intensity (float): Transformation intensity.
        channel_axis (int): Index of axis for channels in the input tensor.

    Returns
        Numpy image tensor.
    '''
    x = np.rollaxis(x, channel_axis, 0)
    min_x, max_x = np.min(x), np.max(x)
    channel_images = [
        np.clip(x_channel + intensity,
                min_x,
                max_x)
        for x_channel in x]
    x = np.stack(channel_images, axis=0)
    x = np.rollaxis(x, 0, channel_axis + 1)
    return x

_fn = lambda img_tnsr : apply_channel_shift(img_tnsr, 1, 3)
tf_fn = tf.numpy_function(lambda i: _fn(i), [i], [tf.float32])

path_ds.map(tf_fn)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31234,model pruning,"in tensorflow/tensorflow/contrib/model_pruning/, can it speed up the inference of the pruned model?"
31233,tensorflow r1.10  gpu slow when createDevices,"when i run tensorflowr1.10 in gpu , i found createDevice is slow . and Pause 4 minutes .

problem:
```
2019-07-31 23:15:38.412176: I server.cpp:152] Starting the server
2019-07-31 23:19:23.888051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966  Device interconnect StreamExecutor with stiength 1 edge matrix
```

code
```
bool CreateSessionFromGraph(
        const string& graph,
        std::unique_ptr<tensorflow::Session>& session) {
      tensorflow::GraphDef graph_def;
      Status status = ReadBinaryProto(tensorflow::Env::Default(),
          graph, &graph_def);
      LOG(INFO) << ""load "" << graph;
      if (!status.ok()) {
        LOG(FATAL) << ""Failed to load: "" << graph;
        return false;
      }
      tensorflow::SessionOptions options;
      options.config.set_allow_soft_placement(true);
      options.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.5);
      options.config.mutable_gpu_options()->set_allow_growth(true);
      options.config.set_intra_op_parallelism_threads(0);
      options.config.set_inter_op_parallelism_threads(0);
      session.reset(tensorflow::NewSession(options));
      status = session->Create(graph_def);
      if (!status.ok()) {
        LOG(FATAL) << ""Failed to create session."";
        return false;
      }
      return true;
    }

    TFModel::TFModel(
        const Device& device,
        const string& graph): device_(device), graph_(graph){
      CreateSessionFromGraph(graph_, session_);
    }

    TFModel::~TFModel() {
      session_->Close();
    }
```

env:
```

$ nvidia-smi 
Thu Aug  1 19:03:57 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:0D:00.0 Off |                    0 |
| N/A   34C    P0    39W / 250W |   5136MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux
Thread model: posix
gcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) 

$ g++ -v
Using built-in specs.
COLLECT_GCC=g++
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux
Thread model: posix
gcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) 

$ cat /etc/redhat-release 
CentOS Linux release 7.2.1511 (Core) 
```"
31232,Transfer learning with frozen model ( Resnet ),"Hello,
I want to use transfer learning to detect 80 eye region landmarks instead of 68 face landmarks.
I have a pre-trained frozen graph model pb .
I want to know what are the steps that I can follow to perform the transfer learning to my model?
I see this script, but I don't know how can I use it.
https://github.com/tensorflow/models/blob/master/official/resnet/resnet_run_loop.py"
31231,DistributionStrategy is not supported by tf.keras.models.Model.fit_generator ,"Hi! Recently I've encountered a `NonImplemetedError` while trying to apply a fit_generator method of a `tf.keras.models.Model` with a `MultiWorkerDistributionStrategy`. It is almost a year since this handlers were added to the code ( https://github.com/tensorflow/tensorflow/commit/9541ce3475ea70fd8eb9552f60de462127f15440#diff-de9b96ac2d81503324cbbbe21732031f ) and I'm wondering whether to expect an implementation to be added any time soon? (with the release of TF2.0 for example)

Making efforts to find a workaround I've tried to transform a generator to TF Dataset by `tf.data.Dataset.from_generator` to replace the `fit_generator` by `fit` but encountered similar problem. The obtained object has type `DatasetV1Adapter`which is also incompatible with distribution strategies

I dare to assume that for a wide society of TF users and for me in particular this functionality would be of a great interest. Dealing with large, domain specific data sets that doesn't fit into memory, one  often has no choice other than writing a custom data generator. When big data is involved the distributed training might be crucial. 

I would highly appreciate any information on the current state of the problem or possible workarounds from the TensorFlow developers team. Thanks in advance!


**System information**
- TensorFlow version (you are using): 2.0.0.dev20190729
- Are you willing to contribute it (Yes/No): No

"
31230,Porting micro_vision example to other other boards.,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.


Is it possible to run this [example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_vision) on an Arduino UNO following the the same workflow as in the example, or at least with some minor hacks? TIA"
31229,Incorrect predictions of Mobilenet_V2,"Hi,
I built the tensorflow lite on iMX8(4xA53) platform. I can get correct results when using models of mobilenet_v1_1.0_224, mobilenet_v1_1.0_224_quant and mobilenet_v2_1.0_224 as input model for  [label_image](https://github.com/tensorflow/tensorflow/tree/v2.0.0-alpha0/tensorflow/lite/examples/label_image). But I got unreasonable predictons when using [mobilenet_v2_1.0_quant](https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgzl).

Here are the results:
_./label_image -i dog.bmp -m **mobilenet_v1_1.0_224.tflite** -l labels.txt                          
Loaded model mobilenet_v1_1.0_224.tflite
resolved reporter
invoked 
average time: 200.31 ms 
0.987781: 209 Labrador retriever
0.00432148: 208 golden retriever
0.00298653: 163 beagle
0.00134322: 160 Rhodesian ridgeback
./label_image -i dog.bmp -m **mobilenet_v1_1.0_224_quant.tflite** -l labels.txt 
Loaded model mobilenet_v1_1.0_224_quant.tflite
resolved reporter
invoked 
average time: 85.934 ms 
0.984314: 209 Labrador retriever
0.00392157: 435 bath towel
0.00392157: 208 golden retriever
0.00392157: 169 redbone
0.00392157: 163 beagle
./label_image -i dog.bmp -m **mobilenet_v2_1.0_224.tflite** -l labels.txt 
Loaded model mobilenet_v2_1.0_224.tflite
resolved reporter
invoked 
average time: 180.721 ms 
0.946971: 209 Labrador retriever
0.00624463: 160 Rhodesian ridgeback
0.0035156: 208 golden retriever
0.00168908: 244 bull mastiff
0.0015452: 163 beagle
./label_image -i dog.bmp -m **mobilenet_v2_1.0_224_quant.tflite** -l labels.txt 
Loaded model mobilenet_v2_1.0_224_quant.tflite
resolved reporter
invoked 
average time: 87.001 ms 
**0.780392: 209 Labrador retriever
0.529412: 853 tennis ball
0.529412: 160 Rhodesian ridgeback
0.509804: 208 golden retriever
0.498039: 244 bull mastiff**_

My questionn is that is this a bug about the Mobilenet_V2 quantized models in tensorflow lite 2.0 alpha?  Would you please help? Thanks in advance."
31228,contrib.receptive_field: ValueError: Weight layer's name input to conv layer does not end with '/read',"I want to compute the receptive field of some convolutional model defined using tf.keras.
I get the graph directly from the session in which the model is built and run `compute_receptive_field_from_graph_def`:

```
from tensorflow.contrib import receptive_field
from tensorflow.keras.layers import Input, Conv2D, AvgPool2D, Dense, Flatten
from tensorflow.keras.models import Model                                                                                                                                           
from tensorflow.keras.backend import get_session

def build_model(input_shape):
    inp = Input(shape=input_shape, name='my_input')
    x = Conv2D(32, (3, 3))(inp)
    x = AvgPool2D((2, 2))(x)
    x = Conv2D(32, (3, 3))(x)
    x = AvgPool2D((2, 2))(x)
    x = Conv2D(32, (3, 3), name='my_output')(x)
    x = Flatten()(x)
    x = Dense(100)(x)
    x = Dense(10, activation='softmax')(x)
    return x

model = build_model([64,64,3])
g = get_session().graph
receptive_field.compute_receptive_field_from_graph_def(g, 'my_input', 'my_output/BiasAdd')
```

I set the name of the output as the last name of the operation in my_output layer, obtained by checking `g.get_operations()`, i.e. 'my_output/BiasAdd'

```
...
 <tf.Operation 'my_output/kernel/Initializer/random_uniform/shape' type=Const>,
 <tf.Operation 'my_output/kernel/Initializer/random_uniform/min' type=Const>,
 <tf.Operation 'my_output/kernel/Initializer/random_uniform/max' type=Const>,
 <tf.Operation 'my_output/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>,
 <tf.Operation 'my_output/kernel/Initializer/random_uniform/sub' type=Sub>,
 <tf.Operation 'my_output/kernel/Initializer/random_uniform/mul' type=Mul>,
 <tf.Operation 'my_output/kernel/Initializer/random_uniform' type=Add>,
 <tf.Operation 'my_output/kernel' type=VarHandleOp>,
 <tf.Operation 'my_output/kernel/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>,
 <tf.Operation 'my_output/kernel/Assign' type=AssignVariableOp>,
 <tf.Operation 'my_output/kernel/Read/ReadVariableOp' type=ReadVariableOp>,
 <tf.Operation 'my_output/bias/Initializer/zeros' type=Const>,
 <tf.Operation 'my_output/bias' type=VarHandleOp>,
 <tf.Operation 'my_output/bias/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>,
 <tf.Operation 'my_output/bias/Assign' type=AssignVariableOp>,
 <tf.Operation 'my_output/bias/Read/ReadVariableOp' type=ReadVariableOp>,
 <tf.Operation 'my_output/dilation_rate' type=Const>,
 <tf.Operation 'my_output/Conv2D/ReadVariableOp' type=ReadVariableOp>,
 <tf.Operation 'my_output/Conv2D' type=Conv2D>,
 <tf.Operation 'my_output/BiasAdd/ReadVariableOp' type=ReadVariableOp>,
 <tf.Operation 'my_output/BiasAdd' type=BiasAdd>,
...
```
but I get

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-13-aad80edbd6db> in <module>
----> 1 receptive_field.compute_receptive_field_from_graph_def(g, 'my_input', 'my_output/BiasAdd')

~/.miniconda3/envs/phaunos_ml/lib/python3.6/site-packages/tensorflow/contrib/receptive_field/python/util/receptive_field.py in compute_receptive_field_from_graph_def(graph_def, input_node, output_node, stop_propagation, input_resolution)
    272       (kernel_size_x, kernel_size_y, stride_x, stride_y, padding_x, padding_y,
    273        _, _) = parse_layer_parameters.get_layer_params(
--> 274            node, name_to_node, node_info[node.name].input_size)
    275       logging.vlog(
    276           3, ""kernel_size_x = %s, kernel_size_y = %s, ""

~/.miniconda3/envs/phaunos_ml/lib/python3.6/site-packages/tensorflow/contrib/receptive_field/python/util/parse_layer_parameters.py in get_layer_params(node, name_to_node, input_resolution, force)
    275   if node.op == ""Conv2D"" or node.op == ""DepthwiseConv2dNative"":
    276     stride_x, stride_y = _stride_size(node, name_to_node)
--> 277     kernel_size_x, kernel_size_y = _conv_kernel_size(node, name_to_node)
    278     # Compute the padding for this node separately for each direction.
    279     total_padding_x, padding_x = _padding_size_conv_pool(

~/.miniconda3/envs/phaunos_ml/lib/python3.6/site-packages/tensorflow/contrib/receptive_field/python/util/parse_layer_parameters.py in _conv_kernel_size(node, name_to_node)
     86   if not weights_layer_read_name.endswith(""/read""):
     87     raise ValueError(
---> 88         ""Weight layer's name input to conv layer does not end with '/read'"")
     89   weights_layer_param_name = weights_layer_read_name[:-5]
     90   weights_node = name_to_node[weights_layer_param_name]

ValueError: Weight layer's name input to conv layer does not end with '/read'
```

I could not find any layer ending with '/read', as suggested. I also tried just 'my_output', but got `ValueError: Output node was not found`. I also tried to set `g.as_graph_def()` instead of `g` as the function's first argument and failed, so here I am.





"
31227,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, TANH. Here is a list of operators for which you will need custom implementations: DEPTH_TO_SPACE.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31226,bad juju in docker vm url,"goal as user: 
would like to click the url produced referring to localhost tokens. using docker as recceomended by tensorflow project  is the fastest and most repeatable way to get gpu+tensorflow+ jupyter
  
console output:

```
________                               _______________                
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / 
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/


WARNING: You are running this container as root, which can cause new files in
mounted volumes to be created as the root user on your host machine.

To avoid this, run the container by specifying your user's userid:

$ docker run -u $(id -u):$(id -g) args...

[I 07:28:24.895 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret
jupyter_http_over_ws extension initialized. Listening on /http_over_websocket
[I 07:28:25.795 NotebookApp] Serving notebooks from local directory: /tf
[I 07:28:25.795 NotebookApp] The Jupyter Notebook is running at:
[I 07:28:25.795 NotebookApp] http://(2950c268081b or 127.0.0.1):8888/?token=9c954171d97b7cf33d46c63611fe4c4f04558871aaf5eb06
[I 07:28:25.795 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 07:28:25.798 NotebookApp] 
    
    To access the notebook, open this file in a browser:
        file:///root/.local/share/jupyter/runtime/nbserver-1-open.html
    Or copy and paste one of these URLs:
        http://(2950c268081b or 127.0.0.1):8888/?token=9c954171d97b7cf33d46c63611fe4c4f04558871aaf5eb06
```  

problem:

` http://(2950c268081b or 127.0.0.1):8888 ` is not a legal url in most browsers.  in even fewer browsers, this will result in useful copy and paste 

"
31225,tensotflow website,"
## URL(s) with the issue:

https://www.tensorflow.org/tutorials/guide/keras

## Description of issue (what needs changing):

can't open URL





"
31224,Can not load tflite model,"# TF information
```
tf-nightly           1.15.0.dev20190730
```
# Model Information

I use the bert's `run_classifier.py` to export the classifier model
```
1141         name_to_features = {
1142             ""input_ids"": tf.VarLenFeature(tf.int64),
1143             ""input_mask"": tf.VarLenFeature(tf.int64),
1144             ""segment_ids"": tf.VarLenFeature(tf.int64),
1145             ""label_ids"": tf.FixedLenFeature([], tf.int64)
1146         }
1147         serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(name_to_features)
1148         estimator.export_savedmodel(FLAGS.output_dir, serving_input_receiver_fn)
```
Then the estimator to export model.

# Code to generate tflite model
```
import tensorflow as tf
import sys
saved_model_dir = sys.argv[1]
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
tflite_quant_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_quant_model)
```

# Code to load tflite model
```import numpy as np
import tensorflow as tf

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""converted_model.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
print(input_details)
output_details = interpreter.get_output_details()
print(output_details)

# Test model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```
When load tflite load model path I get error message
```
$python test_quant.py
WARNING: Logging before flag parsing goes to stderr.
W0801 15:01:28.343195 140634983794496 __init__.py:328] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
INFO: Initialized TensorFlow Lite runtime.
Traceback (most recent call last):
  File ""test_quant.py"", line 5, in <module>
    interpreter = tf.lite.Interpreter(model_path=""converted_model.tflite"")
  File ""/home/guohuawu.wgh/miniconda3/envs/quant/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter.py"", line 206, in __init__
    model_path))
ValueError: Input array not provided for operation 'reshape'.
```

"
31222,Specifying shapehandle's shape when creating new op in tensorflow (rank problem of the output tensor),"System: Ubuntu 18.04
Tensorflow: 1.13.1
CuDNN: 7.4.2
CUDA: 10.2
NVIDIA: 430.26
Python: 3.6
GPU: 2080TI

I have successfully compiled the op registration file and tested when only using this file. But during training process, I tried to call the function defined in the op, these errors were encountered, which vary every time:

`Segmentation fault (core dumped)`

or

```
double free or corruption (!prev)
Aborted (core dumped)
```

or


```
Traceback (most recent call last):
  File ""/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1659, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 99648624 for '...myop' (op: 'myop') with input shapes: [50,2048,3].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 327, in <module>
    network.build_graph(training=True)
  File ""main.py"", line 36, in build_graph
    reuse=None, bn_decay=self.bn_decay, up_ratio=opt.u)
  File ""/home/.../graph.py"", line 87, in graph
    p_out = myfunc(x)
  File ""/home/.../myop.py"", line 19, in myfunc
    return myop_module.myfunc(x)
  File ""<string>"", line 68, in myfunc
  File ""/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1823, in __init__
    control_input_ops)
  File ""/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1662, in _create_c_op
    raise ValueError(str(e))

ValueError: Shape must be rank 1 but is rank 99648624 for '...myop' (op: 'myop') with input shapes: [50, 1000, 3].
```

And please note the number `99648624` above is uncertain, sometimes it could be 0 or any weird number.

Below is the code for registering the op in tensorflow, where I specify the output's dimension as `(b,200,200,1)`:
 
````
.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
        ::tensorflow::shape_inference::ShapeHandle input
        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 3, &input));   
        ::tensorflow::shape_inference::ShapeHandle dim2;
        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(200, &dim2));    
        ::tensorflow::shape_inference::ShapeHandle dim3;
        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(200, &dim3));    
        ::tensorflow::shape_inference::ShapeHandle dim4;
        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &dim4));    
        ::tensorflow::shape_inference::ShapeHandle output = c->MakeShape({c->Dim(input, 0), c->Dim(dim2, 0), c->Dim(dim3, 0), c->Dim(dim4, 0)});
        c->set_output(0, output);
        return Status::OK();
    });
````

I believe that in my code of op registration, the output shape has been already successfully determined. There is no similar question that I can find online. Please help!
"
31220,"""Build from source"" fails during ""bazel build""","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.14
- Python version: 3.5
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: CUDA Version 10.0.130 / cuDNN 7.4.2
- GPU model and memory: GeForce GTX 1080 Ti (11178MiB)


**Describe the problem**
I am trying to install TensorFlow (TF) from source, since I found that I have to revise some codes in order to use the TensorFlow's MFCC function with TensorFlow Lite (https://github.com/tensorflow/tensorflow/issues/26174).

Currently, my TF 1.14 installed with pip works fine in the virtual environment, but when I tried to install it from source, some errors occur during the ""bazel build"", and I could not find how to fix it.

When I run 
bazel build --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --verbose_failures //tensorflow/tools/pip_package:build_pip_package, 
it runs into the problem.

When I repeatedly run the same command (without touching anything), some different error messages come out (maybe they look different to me since I'm very ignorant to what's happening during the build process).

The error messages (from the terminal) are copied and pasted below, and the full logs are attached as files.



**Provide the exact sequence of commands / steps that you executed before running into the problem**

I followed the manual provided by https://www.tensorflow.org/install/source

(1) Install Bazel
Downloaded the binary installer ""bazel-0.26.1-installer-linux-x86_64.sh"" (from https://github.com/bazelbuild/bazel/releases).
./bazel-0.26.1-installer-linux-x86_64.sh --user

(2) Cloned tensorflow
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow

(3) Manually revised mfcc.cc
Revised ""tensorflow/lite/kernels/mfcc.cc"" according to ""https://github.com/tensorflow/tensorflow/issues/26174""

(4) Configure the build
dreadbird@dreadbird:~/tensorflow$ ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.26.1 installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.5


Found possible Python library paths:
  /usr/local/lib/python3.5/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.0 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include
Found cuDNN 7 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1,6.1]: 6.1


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=noignite    	# Disable Apache Ignite support.
	--config=nokafka     	# Disable Apache Kafka support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished

(5) Build the pip package (Bazel build)
bazel build --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --verbose_failures //tensorflow/tools/pip_package:build_pip_package



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Below are the error messages that I got by running the same command of ""bazel build"" three times (The entire log messages are attached as files).
bazel build --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --verbose_failures //tensorflow/tools/pip_package:build_pip_package

[error_log1.txt](https://github.com/tensorflow/tensorflow/files/3454957/error_log1.txt)
[error_log2.txt](https://github.com/tensorflow/tensorflow/files/3454958/error_log2.txt)
[error_log3.txt](https://github.com/tensorflow/tensorflow/files/3454959/error_log3.txt)

(1)
ERROR: missing input file '@nccl_archive//:src/collectives.h'
ERROR: /home/dreadbird/tensorflow/tensorflow/cc/BUILD:506:1: Creating runfiles tree bazel-out/host/bin/tensorflow/cc/ops/control_flow_ops_gen_cc.runfiles [for host] failed: build-runfiles failed: error executing command 
  (cd /home/dreadbird/.cache/bazel/_bazel_dreadbird/275612784e938fcd472711fffeaee420/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64: \
    PATH=/usr/local/cuda-10.0/bin:/home/dreadbird/bin:/home/dreadbird/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/dreadbird/bin:/home/dreadbird/bin \
  /home/dreadbird/.cache/bazel/_bazel_dreadbird/install/8212a63544dc9a497979f8d7c698aee5/_embedded_binaries/build-runfiles bazel-out/host/bin/tensorflow/cc/ops/control_flow_ops_gen_cc.runfiles_manifest bazel-out/host/bin/tensorflow/cc/ops/control_flow_ops_gen_cc.runfiles): Process terminated by signal 15: Process terminated by signal 15
ERROR: /home/dreadbird/tensorflow/tensorflow/python/BUILD:4790:1: //tensorflow/python:pywrap_tensorflow_internal_py_wrap: missing input file '@nccl_archive//:src/collectives.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/dreadbird/tensorflow/tensorflow/python/BUILD:4790:1 1 input file(s) do not exist
INFO: Elapsed time: 163.877s, Critical Path: 40.98s
INFO: 1714 processes: 1714 local.
FAILED: Build did NOT complete successfully

(2)
ERROR: missing input file '@nccl_archive//:src/nccl.h'
ERROR: /home/dreadbird/tensorflow/tensorflow/cc/BUILD:610:1: Creating runfiles tree bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc.runfiles [for host] failed: build-runfiles failed: error executing command 
  (cd /home/dreadbird/.cache/bazel/_bazel_dreadbird/275612784e938fcd472711fffeaee420/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64: \
    PATH=/usr/local/cuda-10.0/bin:/home/dreadbird/bin:/home/dreadbird/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/dreadbird/bin:/home/dreadbird/bin \
  /home/dreadbird/.cache/bazel/_bazel_dreadbird/install/8212a63544dc9a497979f8d7c698aee5/_embedded_binaries/build-runfiles bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc.runfiles_manifest bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc.runfiles): Process terminated by signal 15: Process terminated by signal 15
ERROR: /home/dreadbird/tensorflow/tensorflow/python/BUILD:4790:1: //tensorflow/python:pywrap_tensorflow_internal_py_wrap: missing input file '@nccl_archive//:src/nccl.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/dreadbird/tensorflow/tensorflow/python/BUILD:4790:1 1 input file(s) do not exist
INFO: Elapsed time: 0.861s, Critical Path: 0.19s
INFO: 10 processes: 10 local.
FAILED: Build did NOT complete successfully

(3)
ERROR: /home/dreadbird/tensorflow/tensorflow/BUILD:537:1: Linking of rule '//tensorflow:libtensorflow_framework.so.1.14.0' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/dreadbird/.cache/bazel/_bazel_dreadbird/275612784e938fcd472711fffeaee420/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64: \
    PATH=/usr/local/cuda-10.0/bin:/home/dreadbird/bin:/home/dreadbird/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/dreadbird/bin:/home/dreadbird/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/host/bin/tensorflow/libtensorflow_framework.so.1.14.0 -Wl,--version-script,tensorflow/tf_framework_version_script.lds '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow_framework.so.1 -pthread -pthread -pthread -pthread -Wl,-S -Wl,-no-as-needed -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/host/bin/tensorflow/libtensorflow_framework.so.1.14.0-2.params)

"
31219,'tensorflow/lite/c/c_api_internal.h' file not found,"pod create the library MyLibray.framework    s.dependency 'TensorFlowLite'.

build error in tensorlow_lite.framework.  #include ""tensorflow/lite/c/c_api_internal.h""

prompt:  'tensorflow/lite/c/c_api_internal.h' file not found

In my framework. just #include <tensorflow_lite/tensorflow/lite/kernels/register.h>

why?

i need to set up what? 
"
31218,ImportError: cannot import name 'monitoring' from 'tensorflow.python.eager',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 7.4.0
- GPU model and memory: 1060 p100 6G

**Describe the current behavior**

> (base) anlly@anlly-Vostro-3668:~/tensorflow/tensorflow$ bazel-bin/tensorflow/python/tools/freeze_graph \
--input_graph=/home/anlly/mobilenet/models-master/research/slim/flower_mobilenet_v1.pb \
--input_checkpoint=/home/anlly/mobilenet/models-master/research/slim/train_log/model.ckpt-100000 \
--input_binary=true \
--output_graph=/home/anlly/mobilenet/models-master/research/slim/flower_frozen_mobilenet_v1.pb \
--output_node_names=MobilenetV1/Predictions/Reshape_1
Traceback (most recent call last):
  File ""/home/anlly/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 57, in <module>
    from tensorflow.python.tools import saved_model_utils
  File ""/home/anlly/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/saved_model_utils.py"", line 21, in <module>
    from tensorflow.contrib.saved_model.python.saved_model import reader
  File ""/home/anlly/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/contrib/saved_model/__init__.py"", line 27, in <module>
    from tensorflow.contrib.saved_model.python.saved_model.keras_saved_model import *
  File ""/home/anlly/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/contrib/saved_model/python/__init__.py"", line 27, in <module>
    from tensorflow.contrib.saved_model.python.saved_model import *
  File ""/home/anlly/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/contrib/saved_model/python/saved_model/__init__.py"", line 27, in <module>
    from tensorflow.contrib.saved_model.python.saved_model import keras_saved_model
  File ""/home/anlly/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/contrib/saved_model/python/saved_model/keras_saved_model.py"", line 44, in <module>
    from tensorflow_estimator.python.estimator import keras as estimator_keras_util
  File ""/home/anlly/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py"", line 25, in <module>
    import tensorflow_estimator.python.estimator.estimator_lib
  File ""/home/anlly/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py"", line 22, in <module>
    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier
  File ""/home/anlly/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/baseline.py"", line 67, in <module>
    from tensorflow_estimator.python.estimator import estimator
  File ""/home/anlly/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 37, in <module>
    from tensorflow.python.eager import monitoring
ImportError: cannot import name 'monitoring' from 'tensorflow.python.eager' (/home/anlly/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/eager/__init__.py)



**Describe the expected behavior**
Combine into pb file.

**Code to reproduce the issue**
download https://github.com/tensorflow/tensorflow/archive/v1.13.1.zip
/home/anlly/mobilenet/models-master/research/slim
After training.


>  BUILD
 data
 datasets
 deployment
 download_and_convert_data.py
 eval_image_classifier.py
 export_inference_graph.py
 export_inference_graph_test.py
 flower_mobilenet_v1.pb
 flowers
 __init__.py
 nets
 preprocessing
 README.md
 scripts
 setup.py
 slim_walkthrough.ipynb
 train_image_classifier.py
 train_log
 WORKSPACE

> $ cd /home/anlly/tensorflow/tensorflow
$ bazel build tensorflow/python/tools:freeze_graph
$ tree -L 1
 ACKNOWLEDGMENTS
 ADOPTERS.md
 arm_compiler.BUILD
 AUTHORS
 bazel-bin -> /home/anlly/.cache/bazel/_bazel_anlly/4719d97187e55f4eec6999680f3515c6/execroot/org_tensorflow/bazel-out/k8-opt/bin
 bazel-genfiles -> /home/anlly/.cache/bazel/_bazel_anlly/4719d97187e55f4eec6999680f3515c6/execroot/org_tensorflow/bazel-out/k8-opt/genfiles
 bazel-out -> /home/anlly/.cache/bazel/_bazel_anlly/4719d97187e55f4eec6999680f3515c6/execroot/org_tensorflow/bazel-out
 bazel-tensorflow -> /home/anlly/.cache/bazel/_bazel_anlly/4719d97187e55f4eec6999680f3515c6/execroot/org_tensorflow
 bazel-testlogs -> /home/anlly/.cache/bazel/_bazel_anlly/4719d97187e55f4eec6999680f3515c6/execroot/org_tensorflow/bazel-out/k8-opt/testlogs

>  BUILD
 CODE_OF_CONDUCT.md
 CODEOWNERS
 configure
 configure.py
 CONTRIBUTING.md
 ISSUES.md
 ISSUE_TEMPLATE.md
 LICENSE
 models.BUILD
 README.md
 RELEASE.md
 SECURITY.md
 tensorflow
 third_party
 tools
 WORKSPACE



> $ bazel-bin/tensorflow/python/tools/freeze_graph \
--input_graph=/home/anlly/mobilenet/models-master/research/slim/flower_mobilenet_v1.pb \
--input_checkpoint=/home/anlly/mobilenet/models-master/research/slim/train_log/model.ckpt-100000 \
--input_binary=true \
--output_graph=/home/anlly/mobilenet/models-master/research/slim/flower_frozen_mobilenet_v1.pb \
--output_node_names=MobilenetV1/Predictions/Reshape_1

Then throw an error.
`
ImportError: cannot import name 'monitoring' from 'tensorflow.python.eager' 
`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31217,"TF Lite quantize with representative data, memory keep growing","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.6.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
The memory keep growing with given representative data set.
The problem doesn't come from how I load data, since no problem if I test for loop over my data.
Probably something in quantization process is holding the data.

**Describe the expected behavior**
No memory leak.

**Code to reproduce the issue**
```
def representative_data_loader(FLAGS, tgt_h, tgt_w):
    # Get the image name list
    input_dir = FLAGS.representative_datset  # a path to some image dataset
    if (input_dir == 'None'):
        raise ValueError('Input directory is not provided')

    if not os.path.exists(input_dir):
        raise ValueError('Input directory not found')

    image_list_LR_temp = os.listdir(input_dir)
    image_list_LR = [os.path.join(input_dir, _) for _ in image_list_LR_temp if _.split('.')[-1] == 'png' or _.split('.')[-1] == '.jpeg' or _.split('.')[-1] == 'jpg']

    image_list_ds = tf.data.Dataset.from_tensor_slices(image_list_LR)
    image_list_ds = image_list_ds.shuffle(len(image_list_LR))
    image_list_ds = image_list_ds.repeat()

    # Read in and preprocess the images
    def preprocess_image(image):
        image = tf.image.decode_image(image, channels=3)
        image = tf.image.convert_image_dtype(image, tf.float32)
        image = tf.image.random_crop(image, [tgt_h, tgt_w, 3])

        return image

    def load_and_preprocess_image(path):
        image = tf.read_file(path)
        return preprocess_image(image)
    image_ds = image_list_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    image_ds = image_ds.batch(1)
    # Push path and image into a list
    Data = collections.namedtuple('Data', 'paths_LR, inputs')

    return Data(
        paths_LR=image_list_ds,
        inputs=image_ds
    )

def memory(): # from somewhere online
    import os
    import psutil
    pid = os.getpid()
    py = psutil.Process(pid)
    memoryUse = py.memory_info()[0]/2.**30  # memory use in GB...I think
    print('memory use:', memoryUse)

def main():
    in_ch = 3
    up_factor = 2
    #h, w = (32, 32)
    h, w = (536, 536)
    inputs_raw = tf.placeholder(tf.float32, shape=[1, h, w, in_ch], name='inputs_raw')
    outputs = tf.split(value = inputs_raw, num_or_size_splits = tf.constant([int(h/2), int(h/2)]), axis = 1)[0]

    # to test dataset
    data = representative_data_loader(FLAGS, h, w)
    memory()
            
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        # to test dataset
        image_iter = data.inputs.make_initializable_iterator()
        sess.run(image_iter.initializer)
        image_el = image_iter.get_next()

        converter = lite.TFLiteConverter.from_session(sess, [inputs_raw], [outputs])

        converter.optimizations = [lite.Optimize.DEFAULT]
        def representative_data_gen():
            for i in range(1000):
                print(i, end = ' ')
                memory()
                yield [sess.run(image_el)]
        converter.representative_dataset = representative_data_gen
        tflite_model = converter.convert()
```
**Other info / logs**
memory just keep growing, until running out"
31216,training code utilize more cpu memory than gpu memory,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):5.4.0
- CUDA/cuDNN version:10
- GPU model and memory:RTX2060 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am using tensorflow 1.12.0 with GPU RTX2060 with 6GB memory. I am using reinforcement learning code to train the robot in a gazebo platform using ROS. The issue is that it is utilizing more CPU memory rather than GPU memory. It takes a lot of time to train a robot. I attached a screenshot of the code and the system memory status along with GPU status. From the status, it is shown that it utilized whole 16gb system memory while it utilized 80% GPU memory but the temp and GPU utilization status is low. Its means GPU is not taking the load.
![Screenshot 2019-07-31 16:51:16](https://user-images.githubusercontent.com/45818401/62258506-5135dd00-b43d-11e9-8568-d8575a833655.png)
![Screenshot 2019-08-01 09:03:16](https://user-images.githubusercontent.com/45818401/62258507-5135dd00-b43d-11e9-8bb2-0a4857d20daf.png)
![Screenshot 2019-08-01 09:03:34](https://user-images.githubusercontent.com/45818401/62258509-5135dd00-b43d-11e9-87c3-8cd830c1abf3.png)
![Screenshot 2019-08-01 09:03:43](https://user-images.githubusercontent.com/45818401/62258510-51ce7380-b43d-11e9-9444-e2a23933652c.png)




**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31214,"INFO:tensorflow:Graph was finalized, Job hangs indefinitely when running with istio-sidecars","Hi guys, i am rather new to tensorflow- and trying to run a distributed tensorflow job (from here https://github.com/learnk8s/distributed-tensorflow-on-k8s). Its a standard character recognition job using MNIST dataset. I can successfully run the job on locally deployed Kubernetes cluster without any issue. But when i try to deploy the same job in an istio-injection=enabled namespace, the job keeps on running forever. Upon looking at the logs of individual pods ... its both chief-pod & worker-pods are stuck at ""INFO:tensorflow:Graph was finalized."" step. Which is repeated indefinitely.
`error`
```
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
```  

**System information**
- OS Platform and Distribution (Linux Ubuntu 18.04):
- TensorFlow installed from (pip install):
- TensorFlow version:1.8

chief-pod logs
```
WARNING:tensorflow:From /app/main.py:95: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'tfjob1-ps-0.istio-test.svc:2222'], u'chief': [u'tfjob1-chief-0.istio-test.svc:2222'], u'worker': [u'tfjob1-worker-0.istio-test.svc:2222', u'tfjob1-worker-1.istio-test.svc:2222']}, u'task': {u'index': 0, u'type': u'chief'}}
INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'chief', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe6a9ab9190>, '_model_dir': './out/vars', '_num_worker_replicas': 3, '_task_id': 0, '_log_step_count_steps': 100, '_master': u'grpc://tfjob1-chief-0.istio-test.svc:2222', '_save_checkpoints_steps': 20, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_global_id_in_cluster': 0, '_save_summary_steps': 20, '_num_ps_replicas': 1}
INFO:tensorflow:Start Tensorflow server.
2019-07-31 23:54:35.170532: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-31 23:54:35.175567: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job chief -> {0 -> localhost:2222}
2019-07-31 23:54:35.175633: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> tfjob1-ps-0.istio-test.svc:2222}
2019-07-31 23:54:35.175660: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> tfjob1-worker-0.istio-test.svc:2222, 1 -> tfjob1-worker-1.istio-test.svc:2222}
2019-07-31 23:54:35.179473: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:2222
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
```

I am not an expert in tensorflow, so please provide some insights what might be causing this error. I get the same logs from the worker-pods as well (something regarding socket closed). This only happens when i try to run the job with istio-sidecars, otherwise it runs fine. 

thanks!"
31212,New name disambiguation for scope argment in tf.contrib.copy_graph.copy_op_to_graph,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No):
yes
just add the following code in the place of the if else for new_name 
Code:

```python 
  istag = scope.find('_')
  first = orgname.find('/')  
  appendname = scope.find('//')
  if scope == '' or (appendname != -1 and scope.split('//')[0] == orgname.split('/')[0]):  
    new_name = orgname
  elif istag != -1:    
    if appendname != -1:
      names = orgname.split('/')
      if names[0] == scope[:istag]:
        new_name = names[0]+scope[istag:appendname]+orgname[first:]
      elif scope[:istag] != """":
        new_name = scope[:istag]+orgname[first:] 
      else:
        new_name = names[0]+scope+orgname[first:] 
    else:
      names = orgname.split('/')
      if names[0] == scope[:istag]:
        new_name = names[0]+scope[istag:]+orgname[first:]
      elif scope[:istag] != """":
        new_name = scope[:istag]+orgname[first:] 
      else:
        new_name = names[0]+scope+orgname[first:]         
  elif appendname != -1 and scope.split('//')[0] != orgname.split('/')[0]:
    new_name = scope[:appendname]+'/'+ orgname
  else:
    new_name = scope + orgname[orgname.find('/'):]   

  return new_name    
```

**Any Other info.**
 
Hi @anush-o , i just added an option to append the new name in the exemplary code, instead of only doing the substitution. 
This is done by  doing scope=name+'//' . This will allow to not override names without intention, but you could always add '//' if you need to define a new name block.

I think this could be implemented with a look-up table (dict) for readability.

Hope this helps. Meanwhile, i will be reading the governance guidelines for contribution to follow the right path.
"
31210,[TF 2.0] tf.function causes dataset iteration to crash in multi-GPU mode,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the current behavior**
When using `tf.function` decorator for a function iterating over a dataset in Multi-GPU, the Colab notebook crashes and gives the following logs:

```
tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Node 'while/body/_21/replica_1/StatefulPartitionedCall': Control dependencies must come after regular dependencies
terminate called after throwing an instance of 'std::out_of_range'
what(): vector::_M_range_check
KernelRestarter: restarting kernel (1/5), keep random ports
```
It works perfectly without `tf.function` decorator

**Describe the expected behavior**
Dataset should iterate as it does in eager mode without `tf.function` decorator.

**Code to reproduce the issue**

```
with strategy.scope():
    def _compute_loss(labels, predictions):
            per_example_loss = loss_object(labels, predictions)
            return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)


    def _train_step(cat_cols, num_cols, labels):

        with tf.GradientTape() as tape:
            inputs = [tf.unstack(cat_cols, axis=-1),num_cols]
            predictions = model(inputs=inputs, training=True)
            loss = _compute_loss(labels, predictions)

        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        predictions = tf.round(predictions)
        train_accuracy.update_state(labels, predictions)
        train_auc.update_state(labels, predictions)
        return loss

    @tf.function
    def _distributed_train_step(dist_train):
        total_loss = 0.0
        num_batches = 0.0
        for cat, num, label in dist_train:
            per_replica_losses = strategy.experimental_run_v2(_train_step,
                                       args=(cat, num, label,))
            total_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

            num_batches += 1
        return total_loss/num_batches




    def train(dist_train):

        for epoch in range(num_epochs):
            print(f""Training epoch {epoch+1}."", end="""")
            train_loss = _distributed_train_step(dist_train)
            template = 'Epoch {}, Loss: {:.3f}, Accuracy: {:.3f}, AUC: {:.3f}'
            print (template.format(epoch + 1, train_loss,
                                 train_accuracy.result() * 100, train_auc.result()))


            train_accuracy.reset_states()
            train_auc.reset_states()

train(dist_train)
```

**Edit**: Did some more digging, and I was able to narrow the issue down to the line `gradients = tape.gradient(loss, model.trainable_variables)`"
31206,TFLite Invalid tensors 'MatMul' were found.,"TFLite does not convert a containing a MatMul node.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `No`
- OS Platform and Distribution: `Ubuntu 19.04 64bit`
- TensorFlow installed from (source or binary): `source`
- TensorFlow version (use command below): `v1.14.0-20-g456fbc0e49 1.14.1`
- Python version: `3.7.3`
- Bazel version (if compiling from source): `0.25.2`
- GCC/Compiler version (if compiling from source): `gcc 8.3.0`
- CUDA/cuDNN version: `None`
- GPU model and memory: `not compiled with CUDA support`

**Describe the current behavior**
When trying to convert [this model](https://drive.google.com/file/d/1oqWFKYXGwars93QmTgr0hd_dsWec77gJ/view?usp=sharing) the TFLiteConverter fails with `
ValueError: Invalid tensors 'MatMul' were found.`. The matmul operation in general does work, whith the example from: https://github.com/tensorflow/tensorflow/issues/27640

```
Traceback (most recent call last):
  File ""tensorflow_matmul_issue.py"", line 7, in <module>
    converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes={""0"": [1, 128, 128, 1]})
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 641, in from_frozen_graph
    sess.graph, output_arrays)
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/lite/python/util.py"", line 116, in get_tensors_from_tensor_names
    "","".join(invalid_tensors)))
ValueError: Invalid tensors 'MatMul' were found.
```
**Describe the expected behavior**
No error

**Code to reproduce the issue**
```python
import tensorflow as tf
graph_def_file = ""model_lcnn_29v2.pb""

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, [""0""], [""MatMul""], input_shapes={""0"": [1, 128, 128, 1]})
tflite_model = converter.convert()
open(""model_lcnn_29v2.tflite"", ""wb"").write(tflite_model)
```
- [model](https://drive.google.com/file/d/1oqWFKYXGwars93QmTgr0hd_dsWec77gJ/view?usp=sharing)
"
31205,TFLite: Changing weights,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.12.1-7529-g3e0ad8a004 2.0.0-dev20190731
- Python version: 3.6.8

I'm attempting to implement Soft Conditional Computation (https://arxiv.org/pdf/1904.04971.pdf).  In this approach, the convolution weights are calculated dynamically for each input.  With a concrete function this works, however tflite seems to freeze the weights from the first input.  I was able to resolve this by calling resize_tensor_input() and allocate_tensors() for each image, at the cost of a small performance hit:

```
    # Note: need to fake resize the input & reallocate tensors
    interpreter.resize_tensor_input(0, (1, in_size, in_size, nb_channels,))
    interpreter.allocate_tensors()
```

My first thought was that tflite hardcodes the weights as an optimisation, however upon looking through the source code this doesn't seem to be the case, leading me to think this is a bug?

Example to reproduce the issue:
```
import tensorflow as tf
import numpy as np

reallocate_tensors = False  # Turn this on to reallocate tensors every run, fixing tflite accuracy
nb_channels = 5
wt_size = 100
in_size = 30

class DynamicWeightTest(tf.keras.layers.Layer):
    def __init__(self,
                 channels,
                 select_size,
                 **kwargs
                 ):
        super().__init__()

        self.channels = channels
        self.select_size = select_size


    def call(self, inputs):
        x = inputs[0]
        wt = inputs[1]

        x = tf.nn.conv2d(x, wt, (1, 1), 'VALID')

        return x


input_wt = tf.keras.layers.Input(shape=(1, nb_channels, wt_size), dtype=tf.float32)
input_data = tf.keras.layers.Input(shape=(in_size, in_size, nb_channels,), dtype=tf.float32)
x = DynamicWeightTest(channels=wt_size, select_size=nb_channels)([input_data, input_wt])

model = tf.keras.Model(inputs=[input_data, input_wt], outputs=[x])

# Get the concrete function from the Keras model.
model_fn = tf.function(lambda x: model(x))
model_fn_concrete = model_fn.get_concrete_function([input_data, input_wt])


# tflite
converter = tf.lite.TFLiteConverter.from_concrete_functions([model_fn_concrete])
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

for _ in range(10):
    data = np.array(np.random.random_sample((1, in_size, in_size, nb_channels,)), dtype=np.float32)
    wt = np.array(np.random.random_sample((1, 1, nb_channels, wt_size)), dtype=np.float32)
    out_ref = model([data, wt])

    # concrete
    out_fn = model_fn_concrete(tf.constant(data), tf.constant(wt))

    # Note: need to fake resize the input & reallocate tensors
    if reallocate_tensors:
        interpreter.resize_tensor_input(0, (1, in_size, in_size, nb_channels,))
        interpreter.allocate_tensors()

    # Call tflite
    interpreter.set_tensor(input_details[0]['index'], data)
    interpreter.set_tensor(input_details[1]['index'], wt)
    interpreter.invoke()
    out_test = interpreter.get_tensor(output_details[0]['index'])

    out_ref = out_ref.numpy()
    out_fn = out_fn.numpy()
    diff = 100. * np.sum(np.abs(out_test - out_ref)) / np.sum(np.abs(out_ref))
    diff_fn = 100. * np.sum(np.abs(out_fn - out_ref)) / np.sum(np.abs(out_ref))
    test_sum = np.sum(out_test)

    print('Diff concrete function is: %f, diff tflite is: %f' % (diff_fn, diff))
```

Example output (without resize & allocate fix):
Diff concrete function is: 0.000000, diff tflite is: 0.000002
Diff concrete function is: 0.000000, diff tflite is: 33.312505
Diff concrete function is: 0.000000, diff tflite is: 34.966878
Diff concrete function is: 0.000000, diff tflite is: 31.750450
Diff concrete function is: 0.000000, diff tflite is: 33.716212
Diff concrete function is: 0.000000, diff tflite is: 34.539647
Diff concrete function is: 0.000000, diff tflite is: 35.657966
Diff concrete function is: 0.000000, diff tflite is: 35.823278
Diff concrete function is: 0.000000, diff tflite is: 36.413545
Diff concrete function is: 0.000000, diff tflite is: 33.483852

Edit: Changed above example to include a variable that turns on/off fix"
31204,"BatchMatMul, Merge, Switch.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
   Windows 10
- TensorFlow installed from (source or binary):
   Github
- TensorFlow version (or github SHA if from source):
   1.14.0

**Provide the text output from tflite_convert**
   
```
TOCO failed. See console for info.
2019-07-30 13:09:37.522804: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 67 operators, 113 arrays (0 quantized)
2019-07-30 13:09:37.523543: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 65 operators, 110 arrays (0 quantized)
2019-07-30 13:09:37.524229: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 65 operators, 110 arrays (0 quantized)
2019-07-30 13:09:37.525344: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 49 operators, 87 arrays (0 quantized)
2019-07-30 13:09:37.526111: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 49 operators, 87 arrays (0 quantized)
2019-07-30 13:09:37.526697: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 49 operators, 87 arrays (0 quantized)
2019-07-30 13:09:37.527380: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 256 bytes, theoretical optimal value: 128 bytes.
2019-07-30 13:09:37.527969: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, FULLY_CONNECTED, GREATER, LESS_EQUAL, LOGISTIC, MUL, PACK, REDUCE_MAX, REDUCE_MIN, SELECT, SPLIT, TANH, TRANSPOSE, UNPACK. Here is a list of operators for which you will need custom implementations: BatchMatMul, Merge, Switch.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**
I am trying to convert a static LSTM model with an initial_state as input. If the initial_state =None, it will convert to tflite model correctly. 



Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
ConverterError                            Traceback (most recent call last)
<ipython-input-6-647e6fe58eae> in <module>
      2 converter=tf.lite.TFLiteConverter.from_frozen_graph(output_graph,input_arrays={'input_tensor'},#'input_state'},
      3                                                     output_arrays={'output','output_state'})
----> 4 tflite_model = converter.convert()
      5 open(""./TF_LSTM_V2.tflite"", ""wb"").write(tflite_model)

~\Anaconda3\envs\tf_1_14\lib\site-packages\tensorflow\lite\python\lite.py in convert(self)
    896           input_tensors=self._input_tensors,
    897           output_tensors=self._output_tensors,
--> 898           **converter_kwargs)
    899     else:
    900       result = _toco_convert_graph_def(

~\Anaconda3\envs\tf_1_14\lib\site-packages\tensorflow\lite\python\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)
    402   data = toco_convert_protos(model_flags.SerializeToString(),
    403                              toco_flags.SerializeToString(),
--> 404                              input_data.SerializeToString())
    405   return data
    406 

~\Anaconda3\envs\tf_1_14\lib\site-packages\tensorflow\lite\python\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    170       stderr = _try_convert_to_unicode(stderr)
    171       raise ConverterError(
--> 172           ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    173   finally:
    174     # Must manually cleanup files."
31203,"import tensorflow fail ,error code 3221225501","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win7 SP1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pycharm Interpreter version 
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda_10.0.130_411.31
- GPU model and memory:
 import tensorflow fail. 
You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
![](https://user-images.githubusercontent.com/15312660/62227908-54a27780-b3ef-11e9-888c-d09d41f3ad53.png)

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


Traceback (most recent call last):
  File ""C:\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
ine 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_inte
al.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_inte
al.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descri
ion)
  File ""C:\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code 3221225501

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python37\lib\site-packages\tensorflow\__init__.py"", line 28, in <mo
le>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-
port
  File ""C:\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49,
n <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
ine 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"",
ine 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_inte
al.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_inte
al.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descri
ion)
  File ""C:\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code 3221225501


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors"
31202,Documentation is missing explanation of how to write custom loss functions for the keras api,"## URL(s) with the issue:

https://www.tensorflow.org/beta/guide/keras/training_and_evaluation#specifying_a_loss_metrics_and_an_optimizer

## Description of issue (what needs changing):

Documentation is missing how to write custom loss functions and how to use the keras.losses.Loss class to write a loss function that takes more inputs than just y_true and y_pred despite the portion saying it will show how to write custom losses and metrics. Currently it shows how to write custom metrics and in layer loss only.

### Clear description

People can use this method to implement loss function not implemented in tensorflow or their own variations of loss functions. It also allows ports of loss functions without a keras.losses exposure such as tf.nn.weighted_cross_entropy_with_logits to be implemented in a model.compile use case without the need to write a custom training loop.

### Correct links

N/A

### Parameters defined

N/A

### Returns defined

N/A

### Raises listed and defined

N/A

### Usage example

There is no documentation, but a good example would be porting a loss function from tensorflow.nn to be used in a class extending keras.losses.Loss and showing how to use a user written function without the keras.losses.Loss class.

### Request visuals, if applicable

No visuals are there, but a portion of code similar to the custom metric code shown would be a good visual. 

A custom loss function I wrote in python follows:
```
class WeightedBinaryCrossEntropy(keras.losses.Loss):
    """"""
    pos_weight: Scalars the effec on loss by the positive class by whatever is passed into it.
    weight: Scalars all the loss. Can be used to increase scalar of negative weight only by passing 1/weight to pos_weight. 
            To affect pos_weight even more after this multiply in the other scalar you had in mind for it
    """"""
    def __init__(self, pos_weight, weight, from_logits=False, reduction=keras.losses.Reduction.AUTO, name='weighted_binary_crossentropy'):
        super(WeightedBinaryCrossEntropy, self).__init__(reduction=reduction, name=name)
        self.pos_weight = pos_weight
        self.weight = weight
        self.from_logits = from_logits

    def call(self, y_true, y_pred):
        if not self.from_logits:
            with tf.name_scope('Weighted_Cross_Entropy'):
                # Manually calculated the weighted cross entropy. Formula is qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) where z are labels, x is logits, and q is the weight.
                # Since the values passed are from sigmoid (assumably in this case) sigmoid(x) will be replaces with y_pred
                x_1 = y_true * self.pos_weight * -tf.math.log(y_pred + 1e-6) # qz * -log(sigmoid(x)) 1e-6 is added as an epsilon to stop passing a zero into the log
                x_2 = (1 - y_true) * -tf.math.log(1 - y_pred + 1e-6) # (1 - z) * -log(1 - sigmoid(x)). Epsilon is added to prevent passing a zero into the log
                return tf.add(x_1, x_2) * self.weight # Must be negative as it is maximized when passed to optimizers
        # Use built in function
        return tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, self.pos_weight) * self.weight
```

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
31201,ERROR: Failed to prepare for TPU. generic::failed_precondition: Custom op already assigned to a different TPU.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): Docker image latest-gpu-py3
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: 10.1
- GPU model and memory: RTX 2080 Ti

I'm trying to compile tflite models with the edgetpu_compiler for usage with the Coral TPU stick. I have two versions of the tflite file. For the first one, I do not want to use the TPU acceleration (to compare the performances). I invoked the TFLiteConverter with the following configuration:
```
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
converter.inference_input_type = tf.float32
converter.inference_output_type = tf.float32
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
open(<tflite_file_name>, 'wb').write(tflite_model)
```
For the TPU version, I chose the following options:
```
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
converter.representative_dataset = representative_dataset_gen
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
open(<tflite_file_name>, 'wb').write(tflite_model)
```

To check the output of these, I used the visualize.py script. The second graph has quantize nodes in it. To my understanding, I have to convert the second tflite file with the edgetpu_compiler (the first one is expectedly NOT convertible, as it is not quantized). This seems to work, as the log file outputs:

```
Edge TPU Compiler version 2.0.258810407
Input: linearmodel_1.14.0_uint8.tflite
Output: linearmodel_1.14.0_uint8_edgetpu.tflite

Operator                       Count      Status

FULLY_CONNECTED                1          Mapped to Edge TPU
QUANTIZE                       2          Mapped to Edge TPU
```

However, if I use it with the C++ API, I run into an error when allocating tensors:

```
ERROR: Failed to prepare for TPU. generic::failed_precondition: Custom op already assigned to a different TPU.
ERROR: Node number 0 (edgetpu-custom-op) failed to prepare.
```

I have not found a solution to this problem. I can however, load the non-compiled tflite model, although I suspect this does not provide any time savings.

I have uploaded all tflite models, their html visualizations and the edgetpu log with verbosity 10 to a github repo of mine: https://github.com/DocDriven/experimental

Is this issue already known to you?"
31200,illegal instruction,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31199,Adding AVX2 and FMA makes tensorflow slower on my CPUs,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: none 
- GPU model and memory: none

I have built several different tensorflow versions from source to compare the performance during inference. Two of my many versions are:
- prebuilt TF 1.14 C API libpackage (with AVX)
- self-built TF 1.14 CAPI libpackage  (with AVX,AVX2,FMA)

Strangely, I experience that the prebuilt version reaches better latency and throughput for every single comparison. I am a bit confused about that as adding additional instruction set architectures during the build should rather result in improved performance...? I am not sure where the difference might be. The self-built version is without XLA JIT.

My question: is there an obvious reason why this might occur? The hardware i use is:
Intel Xeon Gold 6130 GPU 2.10 GHz (on a cluster, starting with 24 cores up to 480 cores)"
31197,contrib.graph_editor maintenance after Contrib module sunset,"Hi all, i would like to know if there is a pro-vision to where these functionalities, of graph_editor such as subgraph_view, will be maintained. Or it won't? 

They are very handy to architecture/comp-graph engineering. It is of particular interest for me to do research in many directions.

Thank you in advance."
31196,Multiple builds of zlib library,"During compilation of TF, `tensorflow_framework` target (`libtensorflow_framework.so`) will end up with multiple definitions of zlib. The reason is that `protobuf` will bring `zlib`

 whereas the rest of the system is using `zlib_archive`. There is a patch `//third_party/protobuf/protobuf.patch` to force the `protobuf` to use `zlib_archive` but it is not working because the logic introduced in `thrid_party/repo.bzl` to use `git apply` command instead of `patch -p1` on Unix machine is not working.

**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04.2 LTS
- TensorFlow: Branch # master, Commit # 7ab1171547f9c52b7f467604a5c13c98947dac1c 
- Python version: python3.6
- Bazel version: 0.24.1
- GCC/Compiler version: 7.3.1 

**Diff view**
```
diff --git a/third_party/repo.bzl b/third_party/repo.bzl
index a7506b4efe..083ebf9d6a 100644
--- a/third_party/repo.bzl
+++ b/third_party/repo.bzl
@@ -62,13 +62,15 @@ def _repos_are_siblings():
     return Label(""@foo//bar"").workspace_root.startswith(""../"")
 
 # Apply a patch_file to the repository root directory
-# Runs 'git apply' on Unix, 'patch -p1' on Windows.
+# Runs 'patch -p1'
 def _apply_patch(ctx, patch_file):
-    if _is_windows(ctx):
-        patch_command = [""patch"", ""-p1"", ""-d"", ctx.path("".""), ""-i"", ctx.path(patch_file)]
-    else:
-        patch_command = [""git"", ""apply"", ""-v"", ctx.path(patch_file)]
-    cmd = _wrap_bash_cmd(ctx, patch_command)
+    # Don't check patch on Windows, because patch is only available under bash.
+    if not _is_windows(ctx) and not ctx.which(""patch""):
+        fail(""patch command is not found, please install it"")
+    cmd = _wrap_bash_cmd(
+        ctx,
+        [""patch"", ""-p1"", ""-d"", ctx.path("".""), ""-i"", ctx.path(patch_file)],
+    )
     _execute_and_check_ret_code(ctx, cmd)
 
 def _apply_delete(ctx, paths):
```

"
31195,Can not build //tensorflow/core/kernels:cwise_ops_test,"**System information**
- OS Platform and Distribution ( Linux Ubuntu 18.04):
- TensorFlow installed from (source ):
- TensorFlow version: r2.0
- Python version: Python 3.6.8 (default, Jan 14 2019, 11:02:34) 
- Installed using virtualenv? pip? conda?: build test binary with bazel
- Bazel version (if compiling from source):Build label: 0.24.1
- GCC/Compiler version (if compiling from source): gcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) 
- CUDA/cuDNN version: cuda 10.0/ cudnn 7.6
- GPU model and memory: GeForce GTX 1050 / 2000M



**Describe the problem**
I am trying to build cwise_ops_test with bazel test -c opt --strip=never --config=cuda_clang -s //tensorflow/core/kernels:cwise_ops_test command , but failed with below error message. 
How to add --expt-relaxed-constexpr for clang? I saw this compiler option added with nvcc, so I guess clang also need a similar option, where I can add this option?

```
WARNING: The following configs were expanded more than once: [cuda_clang, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'test' from /home/chengleiwang/code/tensorflow/.bazelrc:
  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include
INFO: Reading rc options for 'test' from /home/chengleiwang/code/tensorflow/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/bin/clang --config=cuda_clang --action_env TF_CONFIGURE_IOS=0
INFO: Reading rc options for 'test' from /home/chengleiwang/code/tensorflow/.tf_configure.bazelrc:
  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium --test_tag_filters=-benchmark-test,-no_oss,-oss_serial --build_tag_filters=-benchmark-test,-no_oss --test_tag_filters=-gpu --build_tag_filters=-gpu
INFO: Found applicable config definition build:cuda_clang in file /home/chengleiwang/code/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_clang=true --define=using_clang=true
INFO: Found applicable config definition build:using_cuda in file /home/chengleiwang/code/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:cuda_clang in file /home/chengleiwang/code/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_clang=true --define=using_clang=true
INFO: Found applicable config definition build:using_cuda in file /home/chengleiwang/code/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:cuda_clang in file /home/chengleiwang/code/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_clang=true --define=using_clang=true
INFO: Found applicable config definition build:using_cuda in file /home/chengleiwang/code/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
Loading: 
Loading: 0 packages loaded
Analyzing: target //tensorflow/core/kernels:cwise_ops_test (0 packages loaded, 0 targets configured)
INFO: Analysed target //tensorflow/core/kernels:cwise_ops_test (2 packages loaded, 143 targets configured).
INFO: Found 1 test target...
[0 / 4] [-----] BazelWorkspaceStatusAction stable-status.txt
SUBCOMMAND: # //tensorflow/stream_executor:plugin_registry [action 'Linking tensorflow/stream_executor/libplugin_registry.so']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/k8-opt/bin/tensorflow/stream_executor/libplugin_registry.so -Wl,-no-as-needed -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/stream_executor/libplugin_registry.so-2.params)
SUBCOMMAND: # //tensorflow/stream_executor:dnn [action 'Linking tensorflow/stream_executor/libdnn.so']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/k8-opt/bin/tensorflow/stream_executor/libdnn.so -Wl,-no-as-needed -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/stream_executor/libdnn.so-2.params)
SUBCOMMAND: # //tensorflow/stream_executor:blas [action 'Linking tensorflow/stream_executor/libblas.so']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/k8-opt/bin/tensorflow/stream_executor/libblas.so -Wl,-no-as-needed -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/stream_executor/libblas.so-2.params)
SUBCOMMAND: # //tensorflow/core/grappler/optimizers:loop_optimizer [action 'Linking tensorflow/core/grappler/optimizers/libloop_optimizer.so']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/libloop_optimizer.so -Wl,-no-as-needed -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/libloop_optimizer.so-2.params)
SUBCOMMAND: # //tensorflow/core/kernels:depthwise_conv_op_gpu [action 'Compiling tensorflow/core/kernels/depthwise_conv_op_gpu_float.cu.cc']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_float.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_float.cu.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/genfiles/external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/curl -iquote bazel-out/k8-opt/genfiles/external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/k8-opt/genfiles/external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/genfiles/external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/genfiles/external/aws -iquote bazel-out/k8-opt/bin/external/aws -iquote external/cub_archive -iquote bazel-out/k8-opt/genfiles/external/cub_archive -iquote bazel-out/k8-opt/bin/external/cub_archive -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/k8-opt/bin/external/cub_archive/_virtual_includes/cub -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/k8-opt/genfiles/external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/k8-opt/genfiles/external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/genfiles/external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread -x cuda '-DGOOGLE_CUDA=1' -fcuda-flush-denormals-to-zero -c tensorflow/core/kernels/depthwise_conv_op_gpu_float.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_float.cu.pic.o)
SUBCOMMAND: # //tensorflow/core/kernels:depthwise_conv_op_gpu [action 'Compiling tensorflow/core/kernels/depthwise_conv_op_gpu_half.cu.cc']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_half.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_half.cu.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/genfiles/external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/curl -iquote bazel-out/k8-opt/genfiles/external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/k8-opt/genfiles/external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/genfiles/external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/genfiles/external/aws -iquote bazel-out/k8-opt/bin/external/aws -iquote external/cub_archive -iquote bazel-out/k8-opt/genfiles/external/cub_archive -iquote bazel-out/k8-opt/bin/external/cub_archive -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/k8-opt/bin/external/cub_archive/_virtual_includes/cub -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/k8-opt/genfiles/external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/k8-opt/genfiles/external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/genfiles/external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread -x cuda '-DGOOGLE_CUDA=1' -fcuda-flush-denormals-to-zero -c tensorflow/core/kernels/depthwise_conv_op_gpu_half.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_half.cu.pic.o)
SUBCOMMAND: # //tensorflow/core/kernels:depthwise_conv_op_gpu [action 'Compiling tensorflow/core/kernels/depthwise_conv_op_gpu_double.cu.cc']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/genfiles/external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/curl -iquote bazel-out/k8-opt/genfiles/external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/k8-opt/genfiles/external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/genfiles/external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/genfiles/external/aws -iquote bazel-out/k8-opt/bin/external/aws -iquote external/cub_archive -iquote bazel-out/k8-opt/genfiles/external/cub_archive -iquote bazel-out/k8-opt/bin/external/cub_archive -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/k8-opt/bin/external/cub_archive/_virtual_includes/cub -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/k8-opt/genfiles/external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/k8-opt/genfiles/external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/genfiles/external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread -x cuda '-DGOOGLE_CUDA=1' -fcuda-flush-denormals-to-zero -c tensorflow/core/kernels/depthwise_conv_op_gpu_double.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/rnn/cell_common.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_common.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_common.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/rnn/cell_common.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_common.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/rnn/cell_gru_lbr.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_gru_lbr.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_gru_lbr.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/rnn/cell_gru_lbr.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_gru_lbr.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/ref_shuffle.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_shuffle.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_shuffle.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/ref_shuffle.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_shuffle.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/rnn/cell_rnn.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_rnn.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_rnn.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/rnn/cell_rnn.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_rnn.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/rnn/cell_lstm.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_lstm.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_lstm.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/rnn/cell_lstm.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/cell_lstm.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/rnn/ref_rnn.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_rnn.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_rnn.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/rnn/ref_rnn.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_rnn.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/ref_softmax.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_softmax.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_softmax.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/ref_softmax.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_softmax.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/rnn/rnn_utils.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/rnn_utils.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/rnn_utils.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/rnn/rnn_utils.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/rnn_utils.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/simple_concat.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/simple_concat.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/simple_concat.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/simple_concat.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/simple_concat.pic.o)
[25 / 42] Compiling tensorflow/core/kernels/depthwise_conv_op_gpu_float.cu.cc; 1s local ... (11 actions running)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/simple_sum.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/simple_sum.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/simple_sum.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/simple_sum.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/simple_sum.pic.o)
INFO: From Compiling external/mkl_dnn/src/cpu/rnn/cell_gru_lbr.cpp:
external/mkl_dnn/src/cpu/rnn/cell_gru_lbr.cpp:45:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
         PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/rnn/cell_gru_lbr.cpp:104:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
         PRAGMA_OMP_SIMD()
 
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/ref_lrn.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_lrn.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_lrn.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/ref_lrn.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_lrn.pic.o)
INFO: From Compiling external/mkl_dnn/src/cpu/rnn/cell_lstm.cpp:
external/mkl_dnn/src/cpu/rnn/cell_lstm.cpp:44:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
         PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/rnn/cell_lstm.cpp:85:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
         PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/rnn/cell_lstm.cpp:113:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
         PRAGMA_OMP_SIMD()
 
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/ref_eltwise.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_eltwise.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_eltwise.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/ref_eltwise.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_eltwise.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/ref_inner_product.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_inner_product.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_inner_product.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/ref_inner_product.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_inner_product.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/jit_uni_lrn_kernel_f32.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_lrn_kernel_f32.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_lrn_kernel_f32.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/jit_uni_lrn_kernel_f32.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_lrn_kernel_f32.pic.o)
INFO: From Compiling external/mkl_dnn/src/cpu/ref_softmax.cpp:
external/mkl_dnn/src/cpu/ref_softmax.cpp:146:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
         PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/ref_softmax.cpp:151:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
     PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/ref_softmax.cpp:181:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
     PRAGMA_OMP_SIMD(reduction(+ : tsum))
 
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/ref_convolution.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_convolution.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_convolution.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/ref_convolution.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_convolution.pic.o)
INFO: From Compiling external/mkl_dnn/src/cpu/ref_shuffle.cpp:
external/mkl_dnn/src/cpu/ref_shuffle.cpp:96:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
             PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/ref_shuffle.cpp:104:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
             PRAGMA_OMP_SIMD()
 
[31 / 48] Compiling tensorflow/core/kernels/depthwise_conv_op_gpu_float.cu.cc; 2s local ... (11 actions running)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/ref_batch_normalization.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_batch_normalization.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_batch_normalization.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/ref_batch_normalization.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/ref_batch_normalization.pic.o)
INFO: From Compiling external/mkl_dnn/src/cpu/simple_sum.cpp:
external/mkl_dnn/src/cpu/simple_sum.cpp:53:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
             PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/simple_sum.cpp:58:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
                 PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/simple_sum.cpp:69:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
             PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/simple_sum.cpp:74:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
                 PRAGMA_OMP_SIMD()
 
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/common/inner_product.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/inner_product.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/inner_product.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/common/inner_product.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/inner_product.pic.o)
INFO: From Compiling external/mkl_dnn/src/cpu/ref_lrn.cpp:
external/mkl_dnn/src/cpu/ref_lrn.cpp:124:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
             PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/ref_lrn.cpp:215:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
             PRAGMA_OMP_SIMD()
 
INFO: From Compiling external/mkl_dnn/src/cpu/simple_concat.cpp:
external/mkl_dnn/src/cpu/simple_concat.cpp:98:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
             PRAGMA_OMP_SIMD()
 
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/gemm_x8s8s32x_convolution.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/gemm_x8s8s32x_convolution.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/gemm_x8s8s32x_convolution.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/gemm_x8s8s32x_convolution.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/gemm_x8s8s32x_convolution.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/nhwc_pooling.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/nhwc_pooling.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/nhwc_pooling.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/nhwc_pooling.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/nhwc_pooling.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/jit_uni_reorder_utils.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_reorder_utils.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_reorder_utils.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/jit_uni_reorder_utils.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_reorder_utils.pic.o)
[35 / 52] Compiling tensorflow/core/kernels/depthwise_conv_op_gpu_float.cu.cc; 3s local ... (12 actions running)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/nchw_pooling.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/nchw_pooling.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/nchw_pooling.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/nchw_pooling.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/nchw_pooling.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/jit_uni_pooling.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_pooling.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_pooling.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/jit_uni_pooling.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_pooling.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/common/batch_normalization.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/batch_normalization.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/batch_normalization.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/common/batch_normalization.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/batch_normalization.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/common/primitive_attr.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/common/primitive_attr.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o)
[41 / 58] Compiling tensorflow/core/kernels/depthwise_conv_op_gpu_float.cu.cc; 4s local ... (11 actions running)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/common/memory_desc_wrapper.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory_desc_wrapper.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory_desc_wrapper.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/common/memory_desc_wrapper.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory_desc_wrapper.pic.o)
INFO: From Compiling external/mkl_dnn/src/cpu/nhwc_pooling.cpp:
In file included from external/mkl_dnn/src/cpu/nhwc_pooling.cpp:26:0:
external/mkl_dnn/src/cpu/nhwc_pooling.hpp:115:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
         PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/nhwc_pooling.cpp:336:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
                 PRAGMA_OMP_SIMD()
 
external/mkl_dnn/src/cpu/nhwc_pooling.cpp:372:0: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
                 PRAGMA_OMP_SIMD()
 
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/jit_uni_eltwise.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_eltwise.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_eltwise.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/jit_uni_eltwise.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_eltwise.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/jit_transpose_src_utils.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_transpose_src_utils.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_transpose_src_utils.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/jit_transpose_src_utils.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_transpose_src_utils.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/jit_uni_i8i8_pooling.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_i8i8_pooling.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_i8i8_pooling.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/jit_uni_i8i8_pooling.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_i8i8_pooling.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_convolution.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_convolution.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_convolution.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_convolution.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_convolution.pic.o)
[46 / 63] Compiling tensorflow/core/kernels/depthwise_conv_op_gpu_float.cu.cc; 6s local ... (11 actions running)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/jit_uni_dw_conv_kernel_f32.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_dw_conv_kernel_f32.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_dw_conv_kernel_f32.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/jit_uni_dw_conv_kernel_f32.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_dw_conv_kernel_f32.pic.o)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/cpu/jit_uni_batch_normalization.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_batch_normalization.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_batch_normalization.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/jit_uni_batch_normalization.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_uni_batch_normalization.pic.o)
[47 / 64] Compiling tensorflow/core/kernels/depthwise_conv_op_gpu_float.cu.cc; 7s local ... (12 actions running)
SUBCOMMAND: # @mkl_dnn//:mkldnn_single_threaded [action 'Compiling external/mkl_dnn/src/common/deconvolution.cpp']
(cd /home/chengleiwang/.cache/bazel/_bazel_chengleiwang/2f444a5606241cb2da19b329364d6093/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    PATH=/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/home/chengleiwang/.local/bin:/home/chengleiwang/bin:/home/chengleiwang/.vscode-server/bin/2213894ea0415ee8c85c5eea0d0ff81ecc191529/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o' -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/common/deconvolution.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o)
INFO: From Compiling tensorflow/core/kernels/depthwise_conv_op_gpu_half.cu.cc:
./tensorflow/core/lib/bfloat16/bfloat16.h(74): warning: calling a constexpr __host__ function(""real"") from a __host__ __device__ function(""bfloat16"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/lib/bfloat16/bfloat16.h(74): warning: calling a constexpr __host__ function from a __host__ __device__ function is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/lib/bfloat16/bfloat16.h(77): warning: calling a constexpr __host__ function(""real"") from a __host__ __device__ function(""bfloat16"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/lib/bfloat16/bfloat16.h(77): warning: calling a constexpr __host__ function from a __host__ __device__ function is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/lib/bfloat16/bfloat16.h(166): warning: calling a constexpr __host__ function from a __host__ __device__ function is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/lib/bfloat16/bfloat16.h(170): warning: calling a constexpr __host__ function from a __host__ __device__ function is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

external/com_google_absl/absl/strings/string_view.h(495): warning: expression has no effect

./tensorflow/core/util/tensor_format.h(127): warning: missing return statement at end of non-void function ""tensorflow::GetTensorSpatialDims""

./tensorflow/core/util/tensor_format.h(151): warning: missing return statement at end of non-void function ""tensorflow::GetTensorDimsFromSpatialDims""

./tensorflow/core/util/gpu_device_functions.h(480): warning: calling a constexpr __host__ function from a __host__ __device__ function is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(490): warning: calling a constexpr __host__ function from a __host__ __device__ function is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(648): error: calling a constexpr __host__ function(""real"") from a __device__ function(""GpuAtomicAdd"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(649): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""GpuAtomicAdd"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(655): error: calling a constexpr __host__ function(""real"") from a __device__ function(""GpuAtomicAdd"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(656): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""GpuAtomicAdd"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(826): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator+"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(826): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator+"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(827): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator+"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(827): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator+"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(833): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator-"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(833): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator-"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(834): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator-"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(834): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator-"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(840): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator*"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(840): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator*"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(841): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator*"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(841): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator*"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(847): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator/"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(847): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator/"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(848): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator/"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(848): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator/"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(854): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator+"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(854): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator+"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(855): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator+"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(855): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator+"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(861): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator-"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(861): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator-"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(862): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator-"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(862): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator-"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(868): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator*"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(868): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator*"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(869): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator*"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(869): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator*"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(875): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator/"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(875): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator/"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(876): error: calling a constexpr __host__ function(""real"") from a __device__ function(""operator/"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

./tensorflow/core/util/gpu_device_functions.h(876): error: calling a constexpr __host__ function(""imag"") from a __device__ function(""operator/"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.

external/com_google_absl/absl/types/optional.h(425): warning: expression has no effect
          detected during instantiation of ""const T &absl::optional<T>::operator*() const & [with T=stream_executor::dnn::AlgorithmDesc]"" 
./tensorflow/stream_executor/dnn.h(802): here

36 errors detected in the compilation of ""/tmp/tmpxft_00001be0_00000000-6_depthwise_conv_op_gpu_half.cu.cpp1.ii"".
ERROR: /home/chengleiwang/code/tensorflow/tensorflow/core/kernels/BUILD:4094:1: output 'tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_half.cu.pic.o' was not created
ERROR: /home/chengleiwang/code/tensorflow/tensorflow/core/kernels/BUILD:4094:1: not all outputs were created or valid
Target //tensorflow/core/kernels:cwise_ops_test failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.246s, Critical Path: 8.82s
INFO: 28 processes: 28 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```"
31193,Add multi-class classification on tflite,"As of now, I believe tensorflowlite can only return predictions belonging to one class. But,it would be great if there was  away to add 2 or more labels in the models. For example, if I want to classify whether a given image is a vegetable or fruit, and also what type of vegetable or fruit, that would be multi label classification.


**System information**
- TensorFlow lite:
- Are you willing to contribute it (No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**
Problems requiring more than 1 label can be solved with higher accuracy and ease.

**Who will benefit with this feature?**
Users of TensorflowLite

"
31192,conv2d_transpose param tensor shapes differ from conv2d shapes,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Ubuntu 16.04
- Mobile device: Not tested
- TensorFlow installed from: binary
- TensorFlow version: 1.13 and 1.12
- Python version: 3.7.3 and 3.6.8
- Bazel version:
- GCC/Compiler version:
- CUDA/cuDNN version: 10.0/7.3.1
- GPU model and memory: Nvidia, driver 418.56, 11178MiB

**Describe the current behavior**
The kernel tensor for conv2d and conv2d_transpose layers (in contrib.layers) behave differently. The number of outputs determines the number of featuremaps/filters and is at the last position (X,X,X,here) while for conv2d_transpose layers it is (X,X,here,X) as the following code example will show. Is this behaviour wanted? Because when building in model where the input_dimensions and out_put_dimensions differ I need to manually transpose the last 2 dimensions in Order for the model to be trainable.

**Describe the expected behavior**
The expected Behaviour would be that the output number for conv2d_layers is at the same position as for usual conv2d_layers.

This problem is independent of the data_format and saving/loading procedures.

**Code to reproduce the issue**
This is a small code snippet for reproduction. Please set the breakpoint at the last `print(...)` and evaluate the shapes-dictionary. You will see that `conv2d` layer 'conv1' has the kernel shape 
`(3, 3, 1, 32)`, indicating that 1 is the depth/number of channels for the input. 
32 is the number of filters/outputs/feature maps. 

If you look at the `conv2d_transposed` layer 'up1' the number of outputs is set to 16 but the kernel shape is `(3,3,16,32)` indicating that the input dimensions are 16 even though they are 32 from the `conv2d` layer 'conv3' preceding it. I believe that this shape should instead be `(3,3,32,16)` because the number of outputs for the `conv2d_transposed` layer is set to 16. 


```
import tensorflow as tf
import tensorflow.contrib.layers as layers


from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)
train_X = mnist.train.images
train_Y = mnist.train.labels
test_X = mnist.test.images


def shapes_of_built_model(layer_names):
    layer_names_and_shapes = {}
    for name in layer_names:
        layer_names_and_shapes[name] = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)[0].shape
    return layer_names_and_shapes


def model(inputs_):
    # Encoder
    conv1 = layers.conv2d(inputs_, num_outputs=32, kernel_size=(3, 3), scope='conv1')
    conv_str2 = layers.conv2d(conv1, num_outputs=32, kernel_size=(3, 3), stride=2, scope='conv_str2')
    conv2 = layers.conv2d(conv_str2, num_outputs=32, kernel_size=(3, 3), scope='conv2')
    encoded = layers.conv2d(conv2, num_outputs=32, kernel_size=(3, 3), stride=2, scope='encoding')

    conv3 = layers.conv2d(encoded, num_outputs=32, kernel_size=(3, 3), scope='conv3')
    upsample1 = layers.conv2d_transpose(conv3, num_outputs=16, kernel_size=3, stride=2, scope='up1')
    upsample2 = layers.conv2d_transpose(upsample1, num_outputs=32, kernel_size=3,  stride=2, scope='up2')
    logits = layers.conv2d(upsample2, num_outputs=1, kernel_size=(3, 3),  scope='logits', padding='SAME')
    decoded = tf.sigmoid(logits, name='reconstruct')
    return decoded


with tf.name_scope('Input'):
    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='X')
    y = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='Y')

output_logits = model(x)

with tf.variable_scope('Train'):
    with tf.variable_scope('Loss'):
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')
    tf.summary.scalar('loss', loss)
    with tf.variable_scope('Optimizer'):
        optimizer = tf.train.AdamOptimizer(learning_rate=0.05, name='Adam-op')
        optimizer = optimizer.minimize(loss)
    with tf.variable_scope('Accuracy'):
        correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')
    tf.summary.scalar('accuracy', accuracy)
    with tf.variable_scope('Prediction'):
        cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')


init = tf.global_variables_initializer()
merged = tf.summary.merge_all()
sess = tf.InteractiveSession()
sess.run(init)

names = ['conv1', 'conv_str2', 'conv2', 'encoding', 'conv3', 'up1', 'up2']
shapes = shapes_of_built_model(names)
print('set breakpoint here')
```"
31190,external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot open Foundation: No such file or directory,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):mac 10.12.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:V1.14.0
- Python version:3.7
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):2.5.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 9.0.0 (clang-900.0.39.2)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
- GPU model and memory:



**Describe the problem**

I downloaded the tag 1.14.0 from git and found this problem after executing the following command


**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \
   --crosstool_top=//external:android/crosstool \
   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
   --cxxopt=-std=c++11 \
   --cpu=armeabi-v7a 


**Any other info / logs**
ERROR: /Users/qbq_wzk/zhangshexin/tensorflow-1.14.0/tensorflow/contrib/android/BUILD:60:1: Linking of rule '//tensorflow/contrib/android:libtensorflow_inference.so' failed (Exit 1)
external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot open Foundation: No such file or directory
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1318.638s, Critical Path: 169.36s
INFO: 886 processes: 886 local.
FAILED: Build did NOT complete successfully
"
31189,Win 10 download mnist get SSL CERTIFICATE_VERIFY_FAILED error,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.5
- Bazel version (if compiling from source): na
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10.1
- GPU model and memory:
**Code to reproduce the issue**
Here is the error I get:
`>>> from tensorflow.examples.tutorials.mnist import input_data`
`C:\Anaconda\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
  from ._conv import register_converters as _register_converters`
`>>> mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)`
`WARNING: Logging before flag parsing goes to stderr.
W0731 06:36:14.795812 11136 deprecation.py:323] From <stdin>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.`
`Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
W0731 06:36:14.795812 11136 deprecation.py:323] From C:\Users\demouser\AppData\Roaming\Python\Python36\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.`
`Instructions for updating:
Please write your own downloading logic.`
`W0731 06:36:14.795812 11136 deprecation.py:323] From C:\Users\demouser\AppData\Roaming\Python\Python36\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.`
`Instructions for updating:
Please use urllib or similar directly.
Traceback (most recent call last):`
  `File ""C:\Anaconda\lib\urllib\request.py"", line 1318, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File ""C:\Anaconda\lib\http\client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""C:\Anaconda\lib\http\client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""C:\Anaconda\lib\http\client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""C:\Anaconda\lib\http\client.py"", line 1026, in _send_output
    self.send(msg)
  File ""C:\Anaconda\lib\http\client.py"", line 964, in send
    self.connect()
  File ""C:\Anaconda\lib\http\client.py"", line 1400, in connect
    server_hostname=server_hostname)
  File ""C:\Anaconda\lib\ssl.py"", line 407, in wrap_socket
    _context=self, _session=session)
  File ""C:\Anaconda\lib\ssl.py"", line 814, in __init__
    self.do_handshake()
  File ""C:\Anaconda\lib\ssl.py"", line 1068, in do_handshake
    self._sslobj.do_handshake()
  File ""C:\Anaconda\lib\ssl.py"", line 689, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)`

`During handling of the above exception, another exception occurred:`

`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\demouser\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\demouser\AppData\Roaming\Python\Python36\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py"", line 260, in read_data_sets
    source_url + TRAIN_IMAGES)
  File ""C:\Users\demouser\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\demouser\AppData\Roaming\Python\Python36\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py"", line 252, in maybe_download
    temp_file_name, _ = urlretrieve_with_retry(source_url)
  File ""C:\Users\demouser\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\demouser\AppData\Roaming\Python\Python36\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py"", line 205, in wrapped_fn
    return fn(*args, **kwargs)
  File ""C:\Users\demouser\AppData\Roaming\Python\Python36\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py"", line 233, in urlretrieve_with_retry
    return urllib.request.urlretrieve(url, filename)
  File ""C:\Anaconda\lib\urllib\request.py"", line 248, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
  File ""C:\Anaconda\lib\urllib\request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Anaconda\lib\urllib\request.py"", line 526, in open
    response = self._open(req, data)
  File ""C:\Anaconda\lib\urllib\request.py"", line 544, in _open
    '_open', req)
  File ""C:\Anaconda\lib\urllib\request.py"", line 504, in _call_chain
    result = func(*args)
  File ""C:\Anaconda\lib\urllib\request.py"", line 1361, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File ""C:\Anaconda\lib\urllib\request.py"", line 1320, in do_open
    raise URLError(err)`
`urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)>`

**Describe the current behavior**
I reinstall the certifi, openssl, cryptography, ca-certificates. But it did not work.
I changed the mnist url from https to http. It worked. But I don't think this is a good solution. it just bypass ssl.
I use openssl s_client command checked the connect to the mnist url. I got the certificate information. So I guess the certification is installed and added to ssl. But why do I still get  SSL CERTIFICATE_VERIFY_FAILED error?
I also checked: 'conda config --show ssl_verify'  It showed the accert.pem file path



"
31187,Failed to compile 'tensorflow/lite/experimental/ruy/pack_avx512.cc',"**System information**
- Have I written custom code - YES, but not in the failing part
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master branch
- Python version: 3.6
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory:N/A

```
tensorflow/lite/experimental/ruy/pack_avx512.cc: In function 'void ruy::{anonymous}::HalfPackFloatAvx512(const float*, const float*, int, int, int, float*, float*)':
tensorflow/lite/experimental/ruy/pack_avx512.cc:343:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t0 = LoaduTwo(src_ptr0, src_ptr4);
                                         ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:344:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t1 = LoaduTwo(src_ptr1, src_ptr5);
                                         ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:345:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t2 = LoaduTwo(src_ptr2, src_ptr6);
                                         ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:346:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t3 = LoaduTwo(src_ptr3, src_ptr7);
                                         ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:363:9: error: '_mm256_storeu_epi32' was not declared in this scope
         _mm256_storeu_epi32(packed_ptr + 0 * 16, _mm512_castsi512_si256(r0));
         ^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:363:9: note: suggested alternative: '_mm256_store_epi64'
         _mm256_storeu_epi32(packed_ptr + 0 * 16, _mm512_castsi512_si256(r0));
         ^~~~~~~~~~~~~~~~~~~
         _mm256_store_epi64
tensorflow/lite/experimental/ruy/pack_avx512.cc:382:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t0 = MaskLoaduTwo(row_mask, src_ptr0, src_ptr4);
                                                       ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:383:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t1 = MaskLoaduTwo(row_mask, src_ptr1, src_ptr5);
                                                       ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:384:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t2 = MaskLoaduTwo(row_mask, src_ptr2, src_ptr6);
                                                       ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:385:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t3 = MaskLoaduTwo(row_mask, src_ptr3, src_ptr7);
                                                       ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:402:9: error: '_mm256_storeu_epi32' was not declared in this scope
         _mm256_storeu_epi32(trailing_buf + 0 * 16, _mm512_castsi512_si256(r0));
         ^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:402:9: note: suggested alternative: '_mm256_store_epi64'
         _mm256_storeu_epi32(trailing_buf + 0 * 16, _mm512_castsi512_si256(r0));
         ^~~~~~~~~~~~~~~~~~~
         _mm256_store_epi64
tensorflow/lite/experimental/ruy/pack_avx512.cc: In function 'void ruy::Pack8bitAvx512(const int8_t*, int8_t, const int8_t*, int, int, int, int8_t*, int32_t*)':
tensorflow/lite/experimental/ruy/pack_avx512.cc:465:3: error: 'memset' was not declared in this scope
   memset(trailing_buf, 0, kTrailingBufSize * sizeof(std::int8_t));
   ^~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:465:3: note: suggested alternative: 'Offset'
   memset(trailing_buf, 0, kTrailingBufSize * sizeof(std::int8_t));
   ^~~~~~
   Offset
tensorflow/lite/experimental/ruy/pack_avx512.cc:500:5: error: 'memcpy' was not declared in this scope
     memcpy(packed_ptr + Layout::kCols * non_trailing_rows, trailing_buf,
     ^~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:500:5: note: suggested alternative: '_m_empty'
     memcpy(packed_ptr + Layout::kCols * non_trailing_rows, trailing_buf,
     ^~~~~~
     _m_empty
tensorflow/lite/experimental/ruy/pack_avx512.cc: In function 'void ruy::PackFloatAvx512(const float*, const float*, int, int, int, float*)':
tensorflow/lite/experimental/ruy/pack_avx512.cc:516:5: error: 'memset' was not declared in this scope
     memset(trailing_buf, 0, sizeof(trailing_buf));
     ^~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:516:5: note: suggested alternative: 'Offset'
     memset(trailing_buf, 0, sizeof(trailing_buf));
     ^~~~~~
     Offset
tensorflow/lite/experimental/ruy/pack_avx512.cc:524:5: error: 'memcpy' was not declared in this scope
     memcpy(packed_ptr + 16 * non_trailing_rows, trailing_buf,
     ^~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:524:5: note: suggested alternative: '_m_empty'
     memcpy(packed_ptr + 16 * non_trailing_rows, trailing_buf,
```
"
31185,Add FP16 precision support to TFLite external C API,"**System information**
- TensorFlow version (you are using):master repository
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
Add FP16 precision support to TFLite external C API.


**Will this change the current api? How?**
This change has no influence.

**Who will benefit with this feature?**
TFLite extenal C API users

**Any Other info.**
Here is [my patch](https://github.com/stakemura/tensorflow/commit/d36a995ccb053145162bb993d351086b3eaa10c0)"
31184,Transfer learning trained by custom TF 2.0 training loop performs worse than keras fit ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 7.6.0
- GPU model and memory: GTX1660Ti, 6 GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I performed transfer learning on pretrained model with TF custom training loop and keras fit.
Both of the settings are the same but TF custom training loop performs worse than the keras fit . I have no idea what's the problem.

I have asked the questions on StackOverflow but not got the answer what I want
https://stackoverflow.com/questions/57268705/transfer-learning-with-pretrained-model-by-tf-gradienttape-cant-converge

**Describe the expected behavior**
The loss and accuracy of model trained by tf.GradientTape should be similar to the one trained by keras fit with the same settings

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

physical_devices = tf.config.experimental.list_physical_devices('GPU')
try:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
    pass

cifar10 = tf.keras.datasets.cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

def process_data(img, lbl):
    img = tf.image.resize(img, (96, 96))
    img = (img-128) / 128
    return img, lbl
train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(128)
test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(128)
train_data = train_data.map(process_data)
test_data = test_data.map(process_data)
train_data, test_data

# load the pretrained model
base_model = keras.applications.MobileNetV2(input_shape=(96, 96, 3), include_top=False, pooling='avg')
x = base_model.outputs[0]
outputs = layers.Dense(10, activation=tf.nn.softmax)(x)
model = keras.Model(inputs=base_model.inputs, outputs=outputs)

# Trained with keras fit
model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])
history = model.fit(train_data, epochs=1)

# The results are: loss: 0.4345 - accuracy: 0.8585

# Trained with tf.GradientTape
optimizer = keras.optimizers.Adam()
train_loss = keras.metrics.Mean()
train_acc = keras.metrics.SparseCategoricalAccuracy()
def train_step(data, labels):    
    with tf.GradientTape() as gt:
        pred = model(data)
        loss = keras.losses.SparseCategoricalCrossentropy()(labels, pred)

    grads = gt.gradient(loss, model.trainable_variables)

    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    train_loss(loss)
    train_acc(labels, pred)

model = keras.Model(inputs=base_model.inputs, outputs=outputs)
for xs, ys in train_data:
    train_step(xs, ys)

print('train_loss = {:.3f}, train_acc = {:.3f}'.format(train_loss.result(), train_acc.result()))

# The results are:  train_loss = 12.832, train_acc = 0.099
```

**Other info / logs**
If the model trained by tf.GradientTape with smaller learning rate 0.0001 (the default is 0.001), it works well, train_loss = 0.275, train_acc = 0.915 , but that's not the real solution what I expected, it's just a workaround.
"
31182,"TF2-gpu: _SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, ","System information

OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from binary:
TensorFlow version (use command below):tf-nightly-gpu-2.0-preview 2.0.0.dev20190729
Python version: 3.6.4
CUDA/cuDNN version: CUDA-10.0/ Cudnn7.6.1
GPU model and memory: 3 Titan XP 

**Describe the current behavior**
RNN Masking of length cause crash
**Code to reproduce the issue**
```
    mask = tf.sequence_mask(length, dtype=tf.bool)
    mask = K.expand_dims(mask)
    lstm = layers.LSTM(num_hidden[i], return_sequences=True,
                          return_state=True)
    inputs, _, _ = lstm(inputs, mask=mask)
```

**Other info / logs**
```

W0731 09:54:25.986333 139687813953280 network.py:896] Layer signature_rnn_0 was passed non-serializable keyword arguments: {'mask': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=bool>}. They will not be included in the serialized model (and thus will be missing at deserialization time).
W0731 09:54:25.987287 139687813953280 network.py:896] Layer signature_rnn_1 was passed non-serializable keyword arguments: {'mask': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=bool>}. They will not be included in the serialized model (and thus will be missing at deserialization time).
W0731 09:54:25.987648 139687813953280 network.py:896] Layer signature_rnn_2 was passed non-serializable keyword arguments: {'mask': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=bool>}. They will not be included in the serialized model (and thus will be missing at deserialization time).
W0731 09:54:35.721126 139687813953280 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:468: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Apply a constraint manually following the optimizer update step.
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py"", line 61, in quick_execute
    num_outputs)
TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: ExpandDims:0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""run.py"", line 110, in <module>
    main()
  File ""run.py"", line 77, in main
    train_model.fit(train_dataset, callbacks=callbacks)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 724, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py"", line 681, in fit
    steps_name='steps_per_epoch')
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 298, in model_iteration
    batch_outs = f(actual_inputs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py"", line 836, in execution_function
    return [out.numpy() for out in distributed_function(input_fn)]
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 451, in __call__
    return self._stateless_fn(*args, **kwds)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1765, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1089, in _filtered_call
    self.captured_inputs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1167, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 471, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py"", line 75, in quick_execute
    ""tensors, but found {}"".format(keras_symbolic_tensors))
tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=bool>]

```
"
31181,C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS + docker
- All other messages: using the following docker image to build tensorflow from source:
```
tensorflow/tensorflow
devel-py3
34334eb7f043
14 hours ago
1.82GB
```

**Describe the problem**
```
ERROR: /tensorflow_src/tensorflow/core/BUILD:2954:1: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 4)
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1810.589s, Critical Path: 798.42s
INFO: 5095 processes: 5095 local.
FAILED: Build did NOT complete successfully
```
And I couldn't find the file ```///usr/share/doc/gcc-7/README.Bugs``` ...

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```
"
31180,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, FILL, GATHER, GREATER_EQUAL, LEAKY_RELU, LOGISTIC, MAX_POOL_2D, MUL, PACK, REDUCE_MIN, REDUCE_PROD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SQUEEZE, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: NonMaxSuppressionV3, Round, Where","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31175,TF2.0 - memory leak caused by autograph retracing due to bound method argument,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 30
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.5.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

Factoring out a train step function into our custom model lead to a memory leak due to continuously retracing the full autograph epoch dataset loop.
The root cause was that bound instance methods are not identical `model.step is not model.step`.

**Describe the expected behavior**

*According to [Issue 36175: Identity of bound methods - Python tracker](https://bugs.python.org/issue36175) this is supposed to be expected behavior in Python.
I merely want to clarify the autograph behaviour and document this for other tensorflow users.*

I guess there might be reasons to use identity (`is`) comparison on arguments when deciding whether an autograph needs retracing, but if it were possible to use equality comparison (`==` ) for arguments that are methods this surprising behaviour could be avoided.

**Code to reproduce the issue**

```py
import tensorflow as tf

@tf.function
def run_epoch(dataset, step):
    print('retrace')
    for X in dataset:
        step(X)

class Model:
    def step(self, X):
        return X * 2

dataset = tf.data.Dataset.from_tensor_slices(list(range(128)))
model = Model()

for i in range(20):
    # leads to retrace of run_epoch due to non-identity model.step
    # (i.e. `model.step is not model.step`)
    run_epoch(dataset, model.step)
```

**Other info / logs**

Possible workarounds:
- store the bound method in a variable and thus keep using the same instance
  `step_fn = model.step`
- declare the method as `@staticmethod` and pass the model instance explicitly as first parameter
"
31174,TF 1.14 Keras model throws exception when inputs are not the deepest nodes,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.9
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.1
- GPU model and memory: Titan V

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

TensorFlow 1.14 introduced a bug in Keras models.  The specific change is the introduction of these lines:

https://github.com/tensorflow/tensorflow/blob/a5120db4d6917f943176ef3c5bb938064604c761/tensorflow/python/keras/engine/network.py#L849-L850

They make the assumption that a model's inputs will be deeper than any other node.  When that is not true, any nodes whose depth is equal to or greater than the inputs does not get evaluated, producing an exception.

The problem is demonstrated by the following script.  It creates a model where an input and a variable have the same depth.  In TensorFlow 1.13 this runs correctly, but in 1.14 it throws an exception:

```
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    print(model(model.inputs))
  File ""/home/peastman/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 634, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/home/peastman/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 751, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)
  File ""/home/peastman/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 903, in _run_internal_graph
    assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)
AssertionError: Could not compute output Tensor(""add/add:0"", shape=(?, 1), dtype=float32)
```

**Describe the expected behavior**

Calling the model should return a Tensor, not throw an exception.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf
import tensorflow.keras.layers as layers

class Variable(layers.Layer):

  def __init__(self, initial_value, **kwargs):
    super(Variable, self).__init__(**kwargs)
    self.var = tf.Variable(initial_value)

  def call(self, inputs):
    return self.var

var = Variable([1.0])([])
input = layers.Input(shape=(1,))
output = layers.Add()([input, var])
model = tf.keras.Model(inputs=[input], outputs=[output])
print(model(model.inputs))
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31173,un-deprecate `write_grads` for `fit`,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0-beta
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

```
tb_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir,
                                                 write_grads=True)
```

- ignores the `write_grads` 

One might argue that I can write a custom `train` function then accumulate the gradients myself but some features I'm using cause the custom train function to fail (`tf.feature_columns`) [BUG: [TF 2.0] categorical_column_with_vocabulary_list not usable in custom training loop](https://github.com/tensorflow/tensorflow/issues/30453)

**Will this change the current api? How?**

writes the grads to Tensorboard

**Who will benefit with this feature?**

People who want to visualize the gradients of their model

**Any Other info.**

Why is it getting deprecated?
"
31172,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): no idea, likely binary
- TensorFlow version: 2.0.0b1
- Python version: Python 3.7.4
- Installed using pip
- CUDA/cuDNN version: using CPU
- GPU model and memory: using CPU



Running the testing in the object detection API sample models (@ https://github.com/tensorflow/models) gives me an error stating `ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime.`

After creating an virtual environment with `venv`, I ran:

`git clone https://github.com/tensorflow/models.git`

`cd models/research`

`protoc object_detection/protos/*.proto --python_out=.`

`python object_detection/builders/model_builder_test.py`, which resulted in the following error:


```
Traceback (most recent call last):
  File ""C:\Users\nick desktop\Desktop\Projects\MachineLearning\ObjectDetection\tensorflow-env-obj\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\nick desktop\Desktop\Projects\MachineLearning\ObjectDetection\tensorflow-env-obj\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\nick desktop\Desktop\Projects\MachineLearning\ObjectDetection\tensorflow-env-obj\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\nick desktop\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\nick desktop\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 20, in <module>
    import tensorflow as tf
  File ""C:\Users\nick desktop\Desktop\Projects\MachineLearning\ObjectDetection\tensorflow-env-obj\lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\nick desktop\Desktop\Projects\MachineLearning\ObjectDetection\tensorflow-env-obj\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\nick desktop\Desktop\Projects\MachineLearning\ObjectDetection\tensorflow-env-obj\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\nick desktop\Desktop\Projects\MachineLearning\ObjectDetection\tensorflow-env-obj\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\nick desktop\Desktop\Projects\MachineLearning\ObjectDetection\tensorflow-env-obj\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\nick desktop\Desktop\Projects\MachineLearning\ObjectDetection\tensorflow-env-obj\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\nick desktop\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\nick desktop\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

Any ideas on how to fix this error? Thanks for the help ahead of time!"
31171,404 Error during oxford-pets tutorial dataset loading,"**System information**
- OS Platform and Distribution (Windows 10):
- TensorFlow installed from (binary):
- TensorFlow version: GPU 2.0 beta
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: nvidia GTX 1050 4gb



**Describe the problem**
problematic tutorial: https://www.tensorflow.org/beta/tutorials/images/segmentation
In the segmentation tutorial, with the copied code from the tutorial, when loading oxford pet dataset from tensorflow_datasets I get:
tensorflow_datasets.core.download.downloader.DownloadError: Failed to get url http://www.robots.ox.ac.uk/~vgg/data/pets/data\images.tar.gz. HTTP code: 404.

I can  go to the site with the web browser with no problems (and even download the dataset).
**Provide the exact sequence of commands / steps that you executed before running into the problem**
from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf
from tensorflow_examples.models.pix2pix import pix2pix
import tensorflow_datasets as tfds
tfds.disable_progress_bar()
from IPython.display import clear_output
import matplotlib.pyplot as plt
#problematic line:
dataset, info = tfds.load('oxford_iiit_pet:3.0.0', with_info=True)


**Any other info / logs**
tensorflow_datasets.core.download.downloader.DownloadError: Failed to get url http://www.robots.ox.ac.uk/~vgg/data/pets/data\images.tar.gz. HTTP code: 404.

Am i doing something wrong, or the site structure changed?"
31169,Tensorflow Keras load model,"I have a keras model which I'm able to load with Keras (with Tensorflow Backend). Tensorflow Keras however is not able to load that model, which makes it impossible to use `TFLiteConverter.from_keras_model_file`

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04 64-bit
- TensorFlow installed from (source or binary): binary (CPU)
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc9 1.14.0
- Keras Version: 2.2.4
- Python version: 3.7.3

**Describe the behavior**
The model works fine with Keras
```python
from keras.models import load_model

inp = np.random.standard_normal([2, 1, 128, 128]).astype(np.float32)
m29v2 = load_model(""model_lcnn_29v2.h5"")
print(m29v2.predict(inp)[0].shape)
```
but it does not work with Tensorflow Keras:
```python
from tensorflow.keras.models import load_model

inp = np.random.standard_normal([2, 1, 128, 128]).astype(np.float32)
m29v2 = load_model(""model_lcnn_29v2.h5"")
print(m29v2.predict(inp)[0].shape)
```
I will get following exception:
```
    m29v2 = load_model(""demo_data/model_lcnn_29v2.h5"")
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 146, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 212, in load_model_from_hdf5
    custom_objects=custom_objects)
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/saving/model_config.py"", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py"", line 89, in deserialize
    printable_module_name='layer')
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 192, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1131, in from_config
    process_node(layer, node_data)
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1087, in process_node
    layer(flat_input_tensors[0], **kwargs)
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 591, in __call__
    self._maybe_build(inputs)
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1881, in _maybe_build
    self.build(input_shapes)
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py"", line 1005, in build
    raise ValueError('The last dimension of the inputs to `Dense` '
ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.
```

Btw other loading other keras models work with Tensorflow Keras but that one for some reason not.

[Link to problematic model](https://drive.google.com/open?id=1dDEvk0dvjv4DveaI7IWT91_BSYa0fJ8l)"
31167,[TF2.0] [Support] TPU training code Breaks after 2 steps of Training,"I'm training a model on TPU, however the code execution breaks after exactly after 2 steps. I'm using tf.distribute.experimental.TPUStrategy for the training.
After running exactly two steps *inside* experimental_run_v2(), it breaks.
The execution doesn't even come out of experimental_run_v2() (I'm logging every microstep (forward passing, loss calculation, applying gradients, etc) to keep a note of the steps).

the trainer code is located here: [https://github.com/captain-pool/GSOC/blob/90980a1bbfd9753fbba5e189a1329f84ba86b448/E3_Distill_ESRGAN/libs/train.py#L200-L245](https://github.com/captain-pool/GSOC/blob/90980a1bbfd9753fbba5e189a1329f84ba86b448/E3_Distill_ESRGAN/libs/train.py#L200-L245)

Stack Overflow Question: [https://stackoverflow.com/questions/57274370/tpu-training-code-breaks-after-2-steps-of-training](https://stackoverflow.com/questions/57274370/tpu-training-code-breaks-after-2-steps-of-training)

Here's the error log:

```
I0730 14:25:03.909630 139951185348352 train.py:247] Starting Adversarial Training
I0730 14:25:04.162598 139951185348352 train.py:259] Start Train
I0730 14:25:09.262624 139951185348352 api.py:512] Student Fake
I0730 14:25:34.609512 139951185348352 api.py:512] Teacher fake
W0730 14:25:43.462079 139951185348352 deprecation.py:323] From /home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
I0730 14:25:43.486022 139951185348352 api.py:512] student_ra
I0730 14:25:44.305178 139951185348352 api.py:512] teacher_ra
I0730 14:25:44.322464 139951185348352 api.py:512] disc_loss
I0730 14:25:44.415445 139951185348352 api.py:512] gen_loss
I0730 14:25:45.656436 139951185348352 api.py:512] gen gradient
I0730 14:25:46.245677 139951185348352 api.py:512] disc gradient
W0730 14:25:51.213224 139951185348352 deprecation.py:323] From /home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/distribute/values.py:878: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Apply a constraint manually following the optimizer update step.
I0730 14:25:52.171965 139951185348352 api.py:512] gen apply
I0730 14:26:03.028684 139951185348352 api.py:512] disc apply
I0730 14:26:10.171932 139951185348352 api.py:512] Student Fake
I0730 14:26:10.637515 139951185348352 api.py:512] Teacher fake
I0730 14:26:11.779316 139951185348352 api.py:512] student_ra
I0730 14:26:12.462838 139951185348352 api.py:512] teacher_ra
I0730 14:26:12.480808 139951185348352 api.py:512] disc_loss
I0730 14:26:12.508683 139951185348352 api.py:512] gen_loss
I0730 14:26:13.662986 139951185348352 api.py:512] gen gradient
I0730 14:26:14.201171 139951185348352 api.py:512] disc gradient
I0730 14:26:15.363607 139951185348352 api.py:512] gen apply
I0730 14:26:16.708091 139951185348352 api.py:512] disc apply
Traceback (most recent call last):
  File ""main.py"", line 129, in <module>
    train_and_export(**vars(FLAGS))
  File ""main.py"", line 89, in train_and_export
    trainer.train_adversarial(student_generator)
  File ""/home/rick/GSOC/E3_Distill_ESRGAN/libs/train.py"", line 260, in train_adversarial
    train_step(image_lr, image_hr)
  File ""/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py"", line 445, in __call__
    return self._stateless_fn(*args, **kwds)
  File ""/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py"", line 1730, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py"", line 665, in _filtered_call
    self.captured_inputs)
  File ""/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py"", line 778, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py"", line 471, in call
    ctx=ctx)
  File ""/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Assigned device '/job:worker/replica:0/task:0/device:TPU:0' does not have registered OpKernel support for _Arg
	 [[{{node rrdb_student_conv2d_conv2d_rrdb_student_conv2d_kernel_139950763791080_handle_inputs_0}}]] [Op:__inference_train_step_44255]
```

CC: @srjoglekar246 , @vbardiovskyg "
31166,Possible tf.matmul bug (wrong results) on tensorflow-gpu,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.6.7
- GCC/Compiler version (if compiling from source): 8.2.0
- CUDA/cuDNN version: cuda-10.0
- GPU model and memory:


**Describe the current behavior**
tf.matmul on tensorflow-gpu gave wrong results. Here is a simplified version of the code.

```
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

r = [[ 1.0, 0.0],    [0.0, 1.0]]

x, y = np.meshgrid(list(range(400)), list(range(400)))
coords = np.stack([x,y],-1).reshape((400,400,2,1))
coords = tf.convert_to_tensor(coords,dtype=tf.float32)

r1 = tf.constant(r)

newCoords = tf.matmul(r1, coords)

sess = tf.Session()
ret = sess.run(newCoords,feed_dict={r1:r})

plt.matshow(ret[:,:,0,0])
plt.show()
```
When I ran it on my tensorflow-gpu, here is the result:
![bug](https://gist.githubusercontent.com/sWizad/3a25d6559ea3308e5cc2731519635c32/raw/2479341ed43fb8e69c94eda3a1362d5cbde7d2d7/Figure_1.png)
Looks like it stops computing halfway through and gave the rest 0 as a result.

**Describe the expected behavior**
Here is the result with CPU:
![CPU](https://user-images.githubusercontent.com/45821224/62137176-08631480-b2d5-11e9-8b8e-6348e8b206e7.png)

**********************************************************
Below is my old post. Initially, I thought the problem was related to TFRecord, but seems like this problem occurs without even using tfrecord too. For completeness, I keep the old example code with tfrecord.

```
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

def parser(serialized_example):
      fs = tf.io.parse_single_example(
          serialized_example,
          features={ ""r"": tf.FixedLenFeature([4], tf.float32) })
      fs[""r""] = tf.reshape(fs[""r""], [2, 2])
      return fs

r = [[ 1.0, 0.0],[0.0, 1.0]]

with tf.io.TFRecordWriter(""cc.test"") as tfrecord_writer:
    feature = {""r"": tf.train.Feature(float_list=tf.train.FloatList(value=np.array(r).flatten() ))}
    example = tf.train.Example(features=tf.train.Features(feature=feature))
    tfrecord_writer.write(example.SerializeToString())
dataset = tf.data.TFRecordDataset([""cc.test""])
dataset = dataset.map(parser).repeat().make_one_shot_iterator()
features = dataset.get_next()

x, y = tf.meshgrid(list(range(400)), list(range(400)))
coords = tf.stack([x, y], -1)     #(h,w,2)
coords = tf.expand_dims(tf.cast(coords,tf.float32),-1) #(h,w,2,1)

r1 = features[""r""]
r2 = tf.constant(r)

newCoords = tf.matmul(r1, coords)

sess = tf.Session()
ret = sess.run(newCoords[:,:,0,0])
plt.matshow(ret)
plt.show()
```
The code will create ""cc.test"" file to save the variable ``r`` and load it as ``r1``. Then matmul ``r1`` with some big varibles.



"
31165,[tf 2.0] use summary apis without occupying gpu memory,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0.0-dev20190725

**Describe the feature and the current behavior/state.**
With current tf 2.0 summary in a gpu-equipped machine, the process will occupy gpu memory as follows:
```
In [1]: import tensorflow as tf

In [2]: tf.__version__
Out[2]: '2.0.0-dev20190725'

In [3]: !nvidia-smi
Tue Jul 30 21:47:26 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0    26W / 250W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |
| N/A   28C    P0    25W / 250W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

In [4]: writer = tf.summary.create_file_writer(""./testdir"")
...
info be omitted to avoid clutter
...

In [5]: with writer.as_default():
   ...:     tf.summary.scalar(""test"", 123, step=1)
   ...:

In [6]: !nvidia-smi
Tue Jul 30 21:47:44 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0    41W / 250W |  15460MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |
| N/A   29C    P0    36W / 250W |    418MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     63963      C   ...2.0/envs/tf-nightly-2.0-0725/bin/python 15449MiB |
|    1     63963      C   ...2.0/envs/tf-nightly-2.0-0725/bin/python   407MiB |
+-------------------------------------
```

We expect some apis which we can use to log metrics without gpu usage, such as `tf.Summary` and `tf.Summary.Value` in tf 1.x.

**Will this change the current api? How?**
Not sure.

**Who will benefit with this feature?**
Who wants to use tensorflow apis to log results in a separate process, which is expected to occupy no GPU memory.
"
31163,[MLIR] couldn't convert MLIR to GraphDef via tf-mlir-translate,"`tf-mlir-translate` fails while converting MLIR to GraphDef.

`$ uname -a`
```
Linux pc-ubuntu 4.15.0-54-generic #58-Ubuntu SMP Mon Jun 24 10:55:24 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```

`$ cd tensorflow/compiler/mlir`
`$ cat add.pbtxt `
```
node {
  name: ""Add""
  op: ""Add""
  input: ""input0""
  input: ""input1""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
}
node {
  name: ""input0""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
}
node {
  name: ""input1""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 27
}
```

`$ bazel run tf-mlir-translate -- --graphdef-to-mlir --tf-input-arrays=input0,input1 --tf-input-data-types=DT_INT32,DT_INT32 --tf-input-shapes=10:10 --tf-output-arrays=Add ${PWD}/add.pbtxt -o ${PWD}/add.mlir`

`$ cat add.mlir`
```


module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 27 : i32}} {
  func @main(%arg0: tensor<10xi32>, %arg1: tensor<10xi32>) -> tensor<10xi32>
  attributes  {tf.entry_function = {inputs = ""input0, input1"", outputs = ""Add""}} {
    %0 = tf_executor.graph {
      %1:2 = tf_executor.island {
        %4 = ""tf.Placeholder.input""(%arg0) {device = """", dtype = ""tfdtype$DT_INT32"", name = ""input0"", shape = ""tfshape$dim { size: 10 }""} : (tensor<10xi32>) -> tensor<10xi32>
        tf_executor.yield %4 : tensor<10xi32>
      } {device = """", dtype = ""tfdtype$DT_INT32"", name = ""input0"", shape = ""tfshape$dim { size: 10 }""}
      %2:2 = tf_executor.island {
        %4 = ""tf.Placeholder.input""(%arg1) {device = """", dtype = ""tfdtype$DT_INT32"", name = ""input1"", shape = ""tfshape$dim { size: 10 }""} : (tensor<10xi32>) -> tensor<10xi32>
        tf_executor.yield %4 : tensor<10xi32>
      } {device = """", dtype = ""tfdtype$DT_INT32"", name = ""input1"", shape = ""tfshape$dim { size: 10 }""}
      %3:2 = tf_executor.island {
        %4 = ""tf.Add""(%1#0, %2#0) {T = ""tfdtype$DT_INT32"", device = """", name = ""Add""} : (tensor<10xi32>, tensor<10xi32>) -> tensor<10xi32>
        tf_executor.yield %4 : tensor<10xi32>
      } {T = ""tfdtype$DT_INT32"", device = """", name = ""Add""}
      tf_executor.fetch %3#0 : tensor<10xi32>
    }
    return %0 : tensor<10xi32>
  }
}
```
`$ bazel run tf-mlir-translate -- --mlir-to-graphdef ${PWD}/add.mlir -o ${PWD}/converted_add.pbtxt`

**output**
```
...
INFO: Build completed successfully, 1 total action
INFO: Build completed successfully, 1 total action
2019-07-30 15:38:00.617166: E tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate_registration.cc:82] Graph export failed: Failed precondition: op node 'tf_executor.graph' was not a TF op!
```
"
31162,Can not use large dimension in Embedding layer on GPU(s).,"I am using TF2.0 latest nightly build and I am trying to train LSTM model for text classification on very large dataset of 16455928 sentences. For embedding layer in the model, I have a vocab size of 366856 and I used 1000 as embedding dimension value in it, on which the 2 GPUs(Tesla T4 from Google) ran out of memory.
Since I can not lower the size of vocabulary (maybe there is a way), so I used lower value for embedding dimension (100) on which the model starts training. Now my question is if there is a way I can use higher value of embedding dimension?. Maybe by putting set of layers of my model on different GPUs, if so then what is the way in TF2.0? Also, will using more number GPUs help? Thank you!"
31161,Cannot convert a Tensorflow GraphDef model to tflite format,"<h1> Description </h1>

I have been trying to convert a custom fastai based model to tflite and tf-js for deployment purpose. I was able to convert my fastai based model to onnx format and then to tensorflow format. I was able to perform inference on tensorflow model. However, when i try to convert the same model to tflte and tf-js i get error using the tflite converter API.
I get multiple errors in sequence.
1. the converter always searches for saved_model.pb | saved_model.pbtxt file only
![image](https://user-images.githubusercontent.com/16400126/62123641-fe95dd00-b2e5-11e9-9d12-dacc246024bc.png)
1. This error is solved by saving the pb file by name of saved_model.pb However, i face the next error after this which is something related to not finding right tags in metagraph.
![image](https://user-images.githubusercontent.com/16400126/62123741-37ce4d00-b2e6-11e9-82ed-2e1744dfe88d.png)

I am not able to resolve this issue, I have tried to create a session and add variables an tags to my graph but to no success. 
![image](https://user-images.githubusercontent.com/16400126/62123900-90054f00-b2e6-11e9-9e56-ed4dd69f8070.png)

Please help with this.
"
31160,same one line code but get different type.,"```python
import tensorflow as tf
import numpy as np

# 2.0.0-beta1
print(tf.__version__)

# [P1]Out result is: tensorflow.python.framework.ops.EagerTensor
print(type(tf.random.uniform([], maxval=1.0)))

def preprocess(n):
    # [P2]Out result is: <class 'tensorflow.python.framework.ops.Tensor'>
    print(type(tf.random.uniform([], maxval=1.0)))
    return n

ds = tf.data.Dataset.from_tensor_slices(tf.cast([1], tf.float32))\
.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\
.repeat()

```
There is one line same code '**type(tf.random.uniform([], maxval=1.0))**' in two positions [P1],[P2], but we got two different type, one is EagerTensor and the other is Tensor. 

Some people know why? Thanks for your reading.
"
31155,quantized for SplitV,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- TensorFlow installed from (source or binary):pip install tensorflow==1.14
- TensorFlow version (or github SHA if from source):1.14.0


**Provide the text output from tflite_convert**

```
tflite_convert  --output_file=quantized.tflite  --graph_def_file=mixnet.pb   --inference_type=QUANTIZED_UINT8     --input_arrays=truediv  --output_arrays=Softmax  --mean_values=128   --std_dev_values=127  --default_ranges_min=0  --default_ranges_max=6
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**
W tensorflow/lite/toco/graph_transformations/quantize.cc:132] Constant array mixnet-s/mixnet_model/blocks_0/conv2d/kernel lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2019-07-30 01:54:53.127377: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type SplitV for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Fatal Python error: Aborted


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31154,Workers are out-of-sync with MultiWorkerMirroredStrategy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin-18.0.0-x86_64-i386-64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190729
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I follow the guide in https://www.tensorflow.org/beta/tutorials/distribute/multi_worker_with_keras to try MultiWorkerMirroredStrategy with Keras, as my understanding, training would be synced across workers. 
But after I start training as following 

python example_tf2_local.py 0 chief
python example_tf2_local.py 0 worker

I found worker/chief are training in different pace, e.g. 

if I start chief at first, chief would not wait for worker, it just starts its own training. 
If I start worker/chief at the same time, still I saw one would be behind another one a few epochs sometime.

But as document stated "" MultiWorkerMirroredStrategy implements synchronous distributed training across multiple workers""

And it's same if I simply starts two workers without chief.

**Describe the expected behavior**
According to document ""MultiWorkerMirroredStrategy implements synchronous distributed training across multiple workers"", workers need to be synced during training

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
from __future__ import absolute_import, division, print_function, unicode_literals
import datetime
import json
import os
import tensorflow_datasets as tfds
import tensorflow as tf
import subprocess
import shlex
import sys

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

tfds.disable_progress_bar()

BUFFER_SIZE = 60000
BATCH_SIZE = 64

NUM_WORKERS = 2
GLOBAL_BATCH_SIZE = NUM_WORKERS * BATCH_SIZE

if __name__ == ""__main__"":
  worker_addrs = ['localhost:9999']
  chief_addr = ['localhost:9998']
  os.environ['TF_CONFIG'] = json.dumps({
      'cluster': {
          'worker': worker_addrs,
          'chief' : chief_addr
      },
      'task': {'type': sys.argv[2], 'index': int(sys.argv[1])}
  })

  print('TF_CONFIG:' + os.environ['TF_CONFIG'])

  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
        metrics=['accuracy'])
    return model

  datasets, info = tfds.load(name='mnist',
                             with_info=True,
                             as_supervised=True)

  train_datasets_unbatched = datasets['train'].map(scale).shuffle(BUFFER_SIZE)

  train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE)

  with strategy.scope():
    multi_worker_model = build_and_compile_cnn_model()
  multi_worker_model.fit(x=train_datasets, epochs=100)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31153,Quantization aware training does not add weights quantization and fake quantization nodes to Conv2D layers without bias and followed by non-addition ops,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.1/7
- GPU model and memory: GTX 1080ti/12G

**Describe the current behavior**
Calling `tf.contrib.quantize.create_training_graph()` or `tf.contrib.quantize.create_eval_graph()` will not add weight quantization and activation quantization nodes to Conv2D layers with `use_bias=False, activation=None` and without following batch normalization layer.

**Describe the expected behavior**
A weights quantization node and an activation quantization node should be added to this layer, even it does not have biases, activation function, and batch normalization.

The issue seems to be related to [THIS](https://github.com/tensorflow/tensorflow/blob/dfe0d543aa6777b780f42f12fd503aebc71839d7/tensorflow/contrib/quantize/python/quantize.py#L529-L561) part

**Code to reproduce the issue**
```python
import tensorflow as tf
slim = tf.contrib.slim

if __name__ == '__main__':
    with tf.Session() as sess:
        input = tf.ones((2, 5, 5, 10))

        conv_bias = tf.keras.layers.Conv2D(
            15,
            kernel_size=3,
            dilation_rate=(1, 1),
            activation=None,
            use_bias=True,
            kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
            kernel_regularizer=slim.l2_regularizer(0.00005),
            kernel_constraint=tf.keras.constraints.UnitNorm(axis=2))

        conv_no_bias = tf.keras.layers.Conv2D(
            15,
            kernel_size=3,
            dilation_rate=(1, 1),
            activation=None,
            use_bias=False,
            kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
            kernel_regularizer=slim.l2_regularizer(0.00005),
            kernel_constraint=tf.keras.constraints.UnitNorm(axis=2))

        x_2 = conv_bias(input)
        x_1 = conv_no_bias(input)

        output = tf.concat((x_1, x_2), axis=3)
        tf.contrib.quantize.create_training_graph(quant_delay=0)
        writer = tf.summary.FileWriter('./bug_report', sess.graph)

```
**Other info / logs**
Graph from tensorboard:

![image](https://user-images.githubusercontent.com/18194209/62093263-810e9600-b22d-11e9-87d5-17703fa7542e.png)
"
31149,Feature Request: Capability to fix the seed for all keras kernel initializers,"**System information**
- TensorFlow version (you are using): 2.0-beta
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
If you create a Keras model, the only way to make your model reproducible is to repeat setting the seed for the `kernel_initializer` in every layer. This makes the code bulky and more prone to error. For example, a reproducible code looks like this:

```
np.random.seed(1)
tf.set_random_seed(1)
# FEATURE REQUEST for another parameter like the ones above that we can set
# here and avoid repeating initializer seed in each and every layer below

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1), input_shape=[1]))
model.add(tf.keras.layers.Dense(8, activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1)))
model.add(tf.keras.layers.Dense(1, activation='linear', kernel_initializer=keras.initializers.glorot_uniform(seed=1)))
```
I wonder if you can add a default seed parameter that we can set and it affects all the initializers. 

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Anyone who likes clean code and would like to create reproducible models.
"
31148,FailedPreconditionError when training BERT on Colab,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: N/A
- TensorFlow installed from (source or binary): Not sure
- TensorFlow version (use command below): 1.14.0
- Python version: Python 3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
We are trying to train a BERT model on Google Colab in keras with tensorflow hub.  The codes are largely copied from https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b with slight modification.  
The model after compilation failed to generate the prediction. I continue to get the error message 'FailedPreconditionError' despite 'with tf.Session() as sess:
     sess.run(tf.global_variables_initializer())'

**Describe the expected behavior**
We expect the model to run and build predictions. 


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```def build_model(max_seq_length): 
    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=""input_ids"")
    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=""input_masks"")
    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=""segment_ids"")
    
    bert_inputs = [in_id, in_mask, in_segment]
    
    bert_output = BertLayer(n_fine_tune_layers=3, pooling=""sequence_output"")(bert_inputs)
    
    print(bert_output)
    
    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)
    
    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)
    
    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)
    
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    model.summary()
    
    return model

def initialize_vars(sess):
    sess.run(tf.local_variables_initializer())
    sess.run(tf.global_variables_initializer())
    sess.run(tf.tables_initializer())
    K.set_session(sess)

sess = tf.Session()

model = build_model(max_seq_length)

with tf.Session() as sess:
     sess.run(tf.global_variables_initializer())

model.fit(
    [train_input_ids, train_input_masks, train_segment_ids], 
    train_labels,
    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),
    epochs=1,
    batch_size=32
)```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```Tensor(""bert_layer_12/bert_layer_12_module_apply_tokens/bert/encoder/Reshape_13:0"", shape=(?, ?, 768), dtype=float32)
Model: ""model_12""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 32)]         0                                            
__________________________________________________________________________________________________
input_masks (InputLayer)        [(None, 32)]         0                                            
__________________________________________________________________________________________________
segment_ids (InputLayer)        [(None, 32)]         0                                            
__________________________________________________________________________________________________
bert_layer_12 (BertLayer)       (None, None, 768)    108931396   input_ids[0][0]                  
                                                                 input_masks[0][0]                
                                                                 segment_ids[0][0]                
__________________________________________________________________________________________________
dense_24 (Dense)                (None, None, 256)    196864      bert_layer_12[0][0]              
__________________________________________________________________________________________________
dense_25 (Dense)                (None, None, 1)      257         dense_24[0][0]                   
==================================================================================================
Total params: 109,128,517
Trainable params: 21,460,737
Non-trainable params: 87,667,780
__________________________________________________________________________________________________
Train on 65 samples, validate on 124 samples
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-43-622dfa03de82> in <module>()
     11     validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),
     12     epochs=1,
---> 13     batch_size=32
     14 )

3 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
   1456         ret = tf_session.TF_SessionRunCallable(self._session._session,
   1457                                                self._handle, args,
-> 1458                                                run_metadata_ptr)
   1459         if run_metadata:
   1460           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

FailedPreconditionError: Error while reading resource variable bert_layer_12_module/bert/encoder/layer_10/attention/output/LayerNorm/gamma from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/bert_layer_12_module/bert/encoder/layer_10/attention/output/LayerNorm/gamma/N10tensorflow3VarE does not exist.
	 [[{{node bert_layer_12/bert_layer_12_module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul/ReadVariableOp}}]]```
"
31147,Tensorflow 1.14 Keras functional API mixed with ops using placeholders throws InvalidArgumentError 'You must feed a value for placeholder tensor',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Keras layers that have an input dependent on a Tensorflow placeholder will throw an InvalidArgumentError on the op creation step asking to feed a value for the placeholder. Specifically, this happens during the `base_layer_utils.create_keras_history(inputs)` step in the `__call__` function on the layer, where the inputs to the Keras layer are passed through a `GraphExecutionFunction` object made during `backend.function([], op_input)([])`. This exception is new in Tensorflow 1.14.0.

**Describe the expected behavior**
As in previous versions of Tensorflow, I would not expect the InvalidArgumentError to be thrown when I am building the graph mixing Keras with Tensorflow.

**Code to reproduce the issue**
```python
import tensorflow as tf

def compile_errors():
    tf_graph = tf.Graph()
    with tf_graph.as_default():
        image = tf.keras.Input(shape=[224, 224, 3], dtype=tf.float32, name=""image"")
        scale = tf.placeholder(dtype=tf.float32, shape=[], name=""scale"")
        scaled_image = image * scale
        conv = tf.keras.layers.Conv2D(filters=32, kernel_size=3, name=""conv2d"")(scaled_image)
        # conv errors on __call__ here due to create_keras_history making a GraphExecutionFunction:
        # tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'scale' with dtype float
        # 	 [[{{node scale}}]]

def compile_succeeds():
    tf_graph = tf.Graph()
    with tf_graph.as_default():
        image = tf.keras.Input(shape=[224, 224, 3], dtype=tf.float32, name=""image"")
        scale = tf.placeholder(dtype=tf.float32, shape=[], name=""scale"")
        scaled_image = tf.keras.layers.Lambda(function=lambda tensors: tensors[0] * tensors[1])([image, scale])
        conv = tf.keras.layers.Conv2D(filters=32, kernel_size=3, name=""conv2d"")(scaled_image)
        # this succeeds

if __name__ == ""__main__"":
    try:
        compile_errors()
        print(""Functional API compilation succeeded."")
    except Exception as e:
        print(""Functional API compilation errored!"", e)
    print()
    compile_succeeds()
    print(""Explicit Keras Lambda Layer compilation succeeded."")
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31144,Having issue with the input and output arrays. Here is my jupyter notebook. ,"**System information**
Model Name:	MacBook Pro
Model Identifier:	MacBookPro11,3
Processor Name:	Intel Core i7
Processor Speed:	2.5 GHz
Number of Processors:	1
Total Number of Cores:	4
L2 Cache (per Core):	256 KB
L3 Cache:	6 MB
Hyper-Threading Technology:	Enabled
Memory:	16 GB
Boot ROM Version:	153.0.0.0.0
SMC Version (system):	2.19f12
Serial Number (system):	C02NF1XAG3QD
Hardware UUID:	411A542B-6D05-553C-A6D5-7C2EBA5068C6

tensorflow version 2.0.0b1
python version . 3.7.3

**Provide the text output from tflite_convert**

```
import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, Embedding,SpatialDropout1D
from keras.callbacks import EarlyStopping
from keras.utils import to_categorical
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from keras.layers.recurrent import LSTM
from keras.preprocessing.sequence import pad_sequences
Using TensorFlow backend.
In [2]:
data = {'email_subject': ['Good Morning','Blackberry','Really Nice Weather'], 
        'secondary_folder_name': ['Sync Issues','Execution Reports','First']}
df = pd.DataFrame(data)
In [3]:
import re
from nltk.corpus import stopwords
In [4]:
df = df.reset_index(drop=True)
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """"""
        text: a string
        
        return: modified initial string
    """"""
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.
    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. 
    text = text.replace('x', '')
#    text = re.sub(r'\W+', '', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text
In [5]:
# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 400
# Max number of words in each
MAX_SEQUENCE_LENGTH =40
# This is fixed.
EMBEDDING_DIM = 100
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!""#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(df['email_subject'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
Found 6 unique tokens.
In [6]:
X = tokenizer.texts_to_sequences(df['email_subject'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)
Shape of data tensor: (3, 40)
In [7]:
Y = pd.get_dummies(df['secondary_folder_name']).values
print('Shape of label tensor:', Y.shape)
Shape of label tensor: (3, 3)
In [8]:
import sklearn 
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 101)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)
(2, 40) (2, 3)
(1, 40) (1, 3)
In [9]:
model = keras.Sequential()
model.add(keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(keras.layers.SpatialDropout1D(0.2))
model.add(keras.layers.SimpleRNN(100, dropout=0.2, recurrent_dropout=0.2))
model.add(keras.layers.Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
epochs = 40
batch_size = 64
history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])
WARNING: Logging before flag parsing goes to stderr.
W0729 14:14:34.209316 4525888960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0729 14:14:34.222216 4525888960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1628: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 40, 100)           40000     
_________________________________________________________________
spatial_dropout1d (SpatialDr (None, 40, 100)           0         
_________________________________________________________________
simple_rnn (SimpleRNN)       (None, 100)               20100     
_________________________________________________________________
dense (Dense)                (None, 3)                 303       
=================================================================
Total params: 60,403
Trainable params: 60,403
Non-trainable params: 0
_________________________________________________________________
W0729 14:14:34.670812 4525888960 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:454: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Apply a constraint manually following the optimizer update step.
Train on 1 samples, validate on 1 samples
Epoch 1/40
1/1 [==============================] - 0s 339ms/sample - loss: 1.4005 - acc: 0.0000e+00 - val_loss: 1.1219 - val_acc: 0.0000e+00
Epoch 2/40
1/1 [==============================] - 0s 16ms/sample - loss: 1.1713 - acc: 0.0000e+00 - val_loss: 1.1644 - val_acc: 0.0000e+00
Epoch 3/40
1/1 [==============================] - 0s 17ms/sample - loss: 1.4085 - acc: 0.0000e+00 - val_loss: 1.2018 - val_acc: 0.0000e+00
Epoch 4/40
1/1 [==============================] - 0s 18ms/sample - loss: 0.7954 - acc: 1.0000 - val_loss: 1.2390 - val_acc: 0.0000e+00
In [10]:
from keras.models import load_model
keras_file =""move.h5""
keras.models.save_model(model,keras_file)
from tensorflow import lite
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
W0729 14:14:35.465211 4525888960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0729 14:14:35.466186 4525888960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0729 14:14:35.466797 4525888960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0729 14:14:36.299986 4525888960 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0729 14:14:36.300852 4525888960 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0729 14:14:36.908327 4525888960 module_wrapper.py:136] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-10-d6b22a2e1927> in <module>
      3 keras.models.save_model(model,keras_file)
      4 from tensorflow import lite
----> 5 converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)

/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py in from_keras_model_file(cls, model_file, input_arrays, input_shapes, output_arrays, custom_objects)
    833     _set_tensor_shapes(input_tensors, input_shapes)
    834 
--> 835     graph_def = _freeze_graph(sess, input_tensors, output_tensors)
    836     return cls(
    837         graph_def,

/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/util.py in freeze_graph(sess, input_tensors, output_tensors)
    247     output_arrays = [get_tensor_name(tensor) for tensor in output_tensors]
    248     return tf_graph_util.convert_variables_to_constants(sess, graph_def,
--> 249                                                         output_arrays)
    250   else:
    251     return sess.graph_def

/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py in new_func(*args, **kwargs)
    322               'in a future version' if date is None else ('after %s' % date),
    323               instructions)
--> 324       return func(*args, **kwargs)
    325     return tf_decorator.make_decorator(
    326         func, new_func, 'deprecated',

/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py in convert_variables_to_constants(sess, input_graph_def, output_node_names, variable_names_whitelist, variable_names_blacklist)
    300         source_op_name = get_input_name(map_name_to_node[source_op_name])
    301       if map_name_to_node[source_op_name].op != ""VarHandleOp"":
--> 302         raise ValueError(""Cannot find the variable that is an input ""
    303                          ""to the ReadVariableOp."")
    304 

ValueError: Cannot find the variable that is an input to the ReadVariableOp.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31143,CUDA Download link needs update,"**Describe the current behavior**

When CUDA is not installed, I get the following error

```
...\tensorflow\python\platform\self_check.py in preload_check()

ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive
```

The link is for CUDA 9.0 rather than 10.0. 

**Describe the expected behavior**

Use one of these links (whichever is appropriate): 

```
https://developer.nvidia.com/cuda-10.0-download-archive
https://developer.nvidia.com/cuda-10.1-download-archive
```"
31142,Tensor flow not found - spyder,"Hello,
I am new of python environment.
I want to use spyder and tensor flow in windows PC, but I get the error: 

`no module named 'tensorflow' spyder`

I simply downloaded anaconda from https://www.anaconda.com/distribution/#download-section.
What's wrong? I already tried to look for similar problems on internet but I am quite confused to the procedure. Can you provide me a step-by-step guide on how to install the module?

Thank you so much,
A"
31141,Tensorflow script that runs perfectly well on Windows 10 laptop gives error on ubuntu 18 cloud vps. How can I solve this problem? ,"Here is the error message thrown:

(base) ps1@ubuntu-s-1vcpu-2gb-nyc1-01:~/xbot_hyperopt$ ls
hyperopt_v2.py  xbot.py
(base) ps1@ubuntu-s-1vcpu-2gb-nyc1-01:~/xbot_hyperopt$ screen -r
er
    output_shape = fn(instance, input_shape)
  File ""/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 2143, in build
    constraint=self.kernel_constraint)
  File ""/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 384, in add_weight
    aggregation=aggregation)
  File ""/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 663, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 142, in make_variable
    variable_shape = tensor_shape.TensorShape(shape)
  File ""/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 774, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 774, in <listcomp>
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 716, in as_dimension
    return Dimension(value)
  File ""/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 185, in __init__
    self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not '_ListWrapper'
(tf_cpu) ps1@ubuntu-s-1vcpu-2gb-nyc1-01:~/xbot_hyperopt$

"
31140,[go] Include MetaGraphDef in SavedModel for go library,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Currently, the MetaGraphDef is not included in the SavedModel struct (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/saved_model.go). The underlying cause is that the protobufs are not pregenerated for go.


**Will this change the current api? How?**
Yes, but It should be a backwards compatible change - the new exposed MetaGraphDef field will be added to SavedModel struct.

**Who will benefit with this feature?**
Everyone who runs inference using go and needs to dynamically load the model's meta graph. Currently, the meta graph /  signature needs to be hardcoded in the code, and it may be fragile if input or output tensor names are change in the python code that generated the saved model. 

**Any Other info.**
Please see the similar issue for java here: https://github.com/tensorflow/tensorflow/issues/19441"
31139,ps ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
31138,"[TF 2.0] gRPC error, in TPUStrategy experimental_distribute_dataset","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): TF 2.0 Beta 1
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Raises this error.
```
InternalError: Failed copying input tensor from /job:worker/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:1/device:CPU:0 in order to run ExperimentalAutoShardDataset: Unable to parse tensor proto
Additional GRPC error information:
{""created"":""@1564422681.083878500"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Unable to parse tensor proto"",""grpc_status"":3} [Op:ExperimentalAutoShardDataset]
```
**Describe the expected behavior**
Work without any error
**Code to reproduce the issue**
Running the following on Colab produces the error.
`!pip3 install tensorflow==2.0.0b1 &> /dev/null`
```python3
import tensorflow as tf
import os
cluster = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=""grpc://%s"" % os.environ[""COLAB_TPU_ADDR""])
tf.config.experimental_connect_to_host(cluster.get_master())
tf.tpu.experimental.initialize_tpu_system(cluster)
strategy = tf.distribute.experimental.TPUStrategy(cluster)
dataset = tf.data.Dataset.range(100).batch(16)
distributed_dataset = strategy.experimental_distribute_dataset(dataset)
```
CC:
@srjoglekar246 
@vbardiovskyg 

"
31137,tf.concat throws error after another call of tf.concat if values is a single Tensor or a list of length 1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- TensorFlow installed from (source or binary): from pip install
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14

**Describe the current behavior**

Assume that I have already made a previous call of `tf.concat`. When calling again `tf.concat` with a `values` argument which is either a list of `Tensor` objects of length 1 or a single `Tensor` object, I get the following error:

```Duplicate node name in graph: 'concat'```

It seems to be a naming conflict.

**Describe the expected behavior**

From [tf.concat documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/concat?hl=en):
> `values`: A list of `Tensor` objects or a single `Tensor`.

`tf.concat` should not throw an error when `values` is a list of length 1 or a single `Tensor` object and no naming conflict should rise.

**Code to reproduce the issue**

```
import tensorflow as tf
from tensorflow.keras import Input


print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))

i = Input(shape=3)
j = Input(shape=4)
try:
    print(tf.concat([i, j], axis=-1))
except Exception as e:
    print(type(e))
    print(e)
try:
    print(tf.concat([i, j], axis=-1))
except Exception as e:
    print(type(e))
    print(e)
try:
    print(tf.concat([i], axis=-1))
except Exception as e:
    print(type(e))
    print(e)
try:
    print(tf.concat(i, axis=-1))
except Exception as e:
    print(type(e))
    print(e)
```

which outputs:
```
Using Tensorflow version 2.0.0-beta1 (git version v2.0.0-beta0-16-g1d91213fe7)
Tensor(""concat:0"", shape=(None, 7), dtype=float32)
Tensor(""concat_1:0"", shape=(None, 7), dtype=float32)
<class 'ValueError'>
Duplicate node name in graph: 'concat'
<class 'ValueError'>
Duplicate node name in graph: 'concat'
```

If I comment both `print(tf.concat([i, j], axis=-1))` lines then `print(tf.concat([i], axis=-1))` does not fail, but `print(tf.concat(i, axis=-1))` do. If I also comment `print(tf.concat([i], axis=-1))` then `print(tf.concat(i, axis=-1))` resolves without error.

**Other info / logs**

Full error log:
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1550   try:
-> 1551     c_op = c_api.TF_FinishOperation(op_desc)
   1552   except errors.InvalidArgumentError as e:

InvalidArgumentError: Duplicate node name in graph: 'concat'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-2-bd15a2b20363> in <module>
----> 1 print(tf.concat(i, axis=-1))

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in concat(values, axis, name)
   1282           dtype=dtypes.int32).get_shape().assert_is_compatible_with(
   1283               tensor_shape.scalar())
-> 1284       return identity(values[0], name=scope)
   1285   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
   1286 

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in identity(input, name)
     84       return copied
     85   else:
---> 86     ret = gen_array_ops.identity(input, name=name)
     87     # Propagate handle data for happier shape inference for resource variables.
     88     if hasattr(input, ""_handle_data""):

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in identity(input, name)
   4251   # Add nodes to the TensorFlow graph.
   4252   _, _, _op = _op_def_lib._apply_op_helper(
-> 4253         ""Identity"", input=input, name=name)
   4254   _result = _op.outputs[:]
   4255   _inputs_flat = _op.inputs

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    786         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,
    787                          input_types=input_types, attrs=attr_protos,
--> 788                          op_def=op_def)
    789       return output_structure, op_def.is_stateful, op
    790 

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in create_op(***failed resolving arguments***)
    463     return super(FuncGraph, self).create_op(
    464         op_type, inputs, dtypes, input_types, name, attrs, op_def,
--> 465         compute_device=compute_device)
    466 
    467   def capture(self, tensor, name=None):

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--> 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(***failed resolving arguments***)
   3294           input_types=input_types,
   3295           original_op=self._default_original_op,
-> 3296           op_def=op_def)
   3297       self._create_op_helper(ret, compute_device=compute_device)
   3298     return ret

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1712           op_def, inputs, node_def.attr)
   1713       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
-> 1714                                 control_input_ops)
   1715 
   1716     # Initialize self._outputs.

~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1552   except errors.InvalidArgumentError as e:
   1553     # Convert to ValueError for backwards compatibility.
-> 1554     raise ValueError(str(e))
   1555 
   1556   return c_op

ValueError: Duplicate node name in graph: 'concat'
```
"
31136,Inconsistent gradient,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.4.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Colab GPU

**Describe the current behavior**
I have a very simple Keras model and I want to compute the gradient of the different layers using TensorFlow. I start by creating the computational graph in the first cell of a Jupyter notebook. Here is the code of the computational graph:

    import tensorflow as tf
    import tensorflow.keras as keras
    import tensorflow.keras.backend as K
    import numpy as np
    from tensorflow.keras.layers import Dense, Input, Layer
    from tensorflow.keras.models import Model
    input_tensor = Input(shape=(20,), name=""input"")
    print(input_tensor.name)
    hidden = Dense(100, activation='relu')(input_tensor)
    out1 = Dense(10, activation='relu', name=""out1"")(hidden)
    model = Model(inputs=input_tensor, outputs=[out1])
    grad = []
    for i in range(4):
       grad.append(tf.gradients(out1, model.trainable_weights[i]))
    model.compile(loss={""out1"": ""mse""}, 
    optimizer=tf.train.AdamOptimizer(learning_rate=0.001))

    np.random.seed(0)
    X = np.random.random((3, 20)).astype(np.float32)
    Y = np.random.random((3, 10)).astype(np.float32)
    model.fit(x={'input' : X}, y={'out1' : Y}, batch_size=1, epochs=10)

Then each time I run the tf.gradients operator, I get a different gradient vector (the gradient changes). This result is not reasonable. Where is the problem in my code?

And here is the code for the created Session:

    with tf.Session() as sess:
       sess.run(tf.global_variables_initializer())
       out_grad = sess.run(grad, feed_dict={'input:0':X})
       print(out_grad)

**Describe the expected behavior**
The same gradient each time I run the `out_grad = sess.run(grad, feed_dict={'input:0':X})`

"
31135,[TF 2.0] How to globally force CPU?,"In TF 1.x it was possible to force CPU only by using:

```
config = tf.ConfigProto(device_count = {'GPU': 0})
```
However, `ConfigProto` doesn't exist in TF 2.0 and changing a OS environment variable seems very clunky.

What's the TF 2.0 way of doing this?

"
31134,Can not run  ./build_ios_universal_lib.sh,"According to https://www.tensorflow.org/lite/guide/build_ios, I am running tensorflow/lite/tools/make/build_ios_universal_lib.sh. However, when I run it, it gives me the error: 

Undefined symbols for architecture x86_64:
  ""tflite::ResourceVariable::~ResourceVariable()"", referenced from:
      tflite::Interpreter::~Interpreter() in benchmark-lib.a(interpreter.o)
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [/Users/broccoli/Downloads/tensorflow-master/tensorflow/lite/tools/make/gen/ios_x86_64/bin/benchmark_model] Error 1


Is there any way to fix this error?"
31131,Unable to save model when using Mirrored Strategy with multiple GPUs,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9 (stretch) (Linux 4.9.0-9-amd64)
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.12.1-7260-gd4bc7d98c6 2.0.0-dev20190729
- Python version: 3.7.2
- GPU model and memory: 2 X Tesla V100 (24 gig RAM)

**Describe the current behavior**
An error is thrown when saving a keras model that has been compiled in using a Mirrored Strategy using more than one GPU.

**Describe the expected behavior**
Should be able to save and reload models that have been saved under Mirror Strategy.

**Code to reproduce the issue**

```import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import InceptionV3

mirrored_strategy = tf.distribute.MirroredStrategy()


def get_model():
    with mirrored_strategy.scope():
        model = InceptionV3(input_shape=(299, 299, 3),
                                 include_top=True,
                                 weights='imagenet',
                                 pooling='avg')

        model.compile(loss='sparse_categorical_crossentropy',
                      optimizer=tf.keras.optimizers.Adam(),
                      metrics=['accuracy'])

    return model


model = get_model()
model.save(""my_model"")
```

**Other info / logs**
See attached.
[logs.txt](https://github.com/tensorflow/tensorflow/files/3442574/logs.txt)

"
31128,What is the right way to use intra_op_parallelism_threads and inter_op_parallelism_threads?,"Hi,
  I create a `Session` with `tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)`. 
When I run the `Session`, I use the `top` command to observe the situations. But I found the program still use 1700% CPU. Why did this happen? What's the right way to control the number of cores/threads used by tensorflow?
thx!

"
31126, fatal error C1083: Cannot open include file: 'ab sl/strings/string_view.h': No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (win10):
- TensorFlow installed from (source):
- TensorFlow version:1.14.
- Python version:3.6
- Bazel version (if compiling from source):0.22
- GCC/Compiler version (if compiling from source):cmake3.13.5+MSbuild



**Describe the problem**
I want to build .dll and .lib of TF_C++ API on WIN10.    an error :cannot find absl. And I have no idea to fix it.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
cmake .. -Thost=x64 -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/swigwin-4.0.0/swig.exe -DPYTHON_EXECUTABLE=D:/Anaconda3/envs/tfcc36/python.exe -DPYTHON_LIBRARIES=D:/Anaconda3/envs/tfcc36/libs/python36.lib -Dtensorflow_BUILD_ALL_KERNELS=ON -Dtensorflow_BUILD_CC_EXAMPLE=ON -Dtensorflow_BUILD_CC_TESTS=OFF -Dtensorflow_BUILD_CONTRIB_KERNELS=ON -Dtensorflow_BUILD_MORE_PYTHON_TESTS=OFF -Dtensorflow_BUILD_PYTHON_BINDINGS=ON -Dtensorflow_BUILD_PYTHON_TESTS=OFF -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_DISABLE_EIGEN_FORCEINLINE=OFF -Dtensorflow_ENABLE_GPU=OFF -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF -Dtensorflow_ENABLE_HDFS_SUPPORT=OFF -Dtensorflow_ENABLE_MKLDNN_SUPPORT=OFF -Dtensorflow_ENABLE_MKL_SUPPORT=OFF -Dtensorflow_ENABLE_POSITION_INDEPENDENT_CODE=ON -Dtensorflow_ENABLE_SNAPPY_SUPPORT=ON -Dtensorflow_ENABLE_SSL_SUPPORT=OFF -Dtensorflow_OPTIMIZE_FOR_NATIVE_ARCH=ON -Dtensorflow_VERBOSE=OFF
MSBuild /p:Configuration=Release ALL_BUILD.vcxproj
**Any other info / logs**

D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\ALL_BUILD.vcxproj() (1) ->
D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\_beam_search_ops.vcxproj() (2) ->
D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj() (3) ->
D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal_static.vcxproj() (4) ->
D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_c.vcxproj() (5) ->
D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_cc_framework.vcxproj() (6) ->
D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj() (7) ->
D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\proto_text.vcxproj() (8) ->
D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj() (9) ->
(ClCompile ) ->
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\core\coding.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\gif\gif_io.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\core\status.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\core\threadpool.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\hash\crc32c.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\db\sqlite.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/gtl/array_slice.h(19): fatal error C1083: Cannot open include file: 'abs
l/types/span.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\lib\histogra
m\histogram.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\hash\hash.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\block_builder.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\path.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\inputbuffer.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\block.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\buffered_inputstream.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\format.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\iterator.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\inputstream_interface.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\record_writer.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\random_inputstream.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\record_reader.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\snappy\snappy_inputbuffer.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\snappy\snappy_outputbuffer.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\table.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\table_builder.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\two_level_iterator.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\zlib_inputstream.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\jpeg\jpeg_mem.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/gtl/array_slice.h(19): fatal error C1083: Cannot open include file: 'abs
l/types/span.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\lib\random\d
istribution_sampler.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\io\zlib_outputbuffer.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow\core\lib\png\png_io.cc(27): fatal error C1083: Cannot open include file: 'absl/ba
se/casts.h': No such file or directory [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/gtl/array_slice.h(19): fatal error C1083: Cannot open include file: 'abs
l/types/span.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\lib\random\r
andom_distributions.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\strings\numbers.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\strings\ordered_code.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/strings/proto_text_util.h(19): fatal error C1083: Cannot open include fi
le: 'absl/strings/str_cat.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core
\lib\strings\proto_text_util.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\monitoring\collection_registry.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\monitoring\sampler.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\strings\scanner.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/strings/str_util.h(22): fatal error C1083: Cannot open include file: 'ab
sl/base/macros.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\lib\string
s\str_util.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\strings\base64.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\li
b\strings\strcat.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow\core\lib\wav\wav_io.cc(22): fatal error C1083: Cannot open include file: 'absl/ba
se/casts.h': No such file or directory [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\fr
amework\resource_handle.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow\core\lib\strings\proto_serialization.cc(19): fatal error C1083: Cannot open inclu
de file: 'absl/memory/memory.h': No such file or directory [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_
core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\pl
atform\default\human_readable_json.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\pl
atform\file_system.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\pl
atform\file_system_helper.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\pl
atform\platform_strings.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\pl
atform\tensor_coding.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]
  D:\tfVS\tensorflow-r1.14\tensorflow/core/lib/core/stringpiece.h(29): fatal error C1083: Cannot open include file: 'ab
sl/strings/string_view.h': No such file or directory (compiling source file D:\tfVS\tensorflow-r1.14\tensorflow\core\pl
atform\windows\windows_file_system.cc) [D:\tfVS\tensorflow-r1.14\tensorflow\contrib\cmake\build\tf_core_lib.vcxproj]

    105 
    48 
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31125,tf_core_lib,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31124,Error 'int8x8_t' does not name a type when building with bazel,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ARMv8 aarch64 CentOS Linux release 7.6.1810
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.14
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc4.8.5
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

Hello,
I encountered the following error while trying to build tensorflow on an ARM machine:

```
/<install folder>/tensorflow/lite/kernels/BUILD:374:1: C++ compilation of rule '//tensorflow/lite/kernels:builtin_op_kernels' failed (Exit 1)
cc1plus: warning: command line option '-std=gnu99' is valid for C/ObjC but not for C++ [enabled by default]
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:22:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:22,
                 from tensorflow/lite/kernels/depthwise_conv.cc:29:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::WorkspacePrefetchWrite<(tflite::DepthwiseConvImplementation)3>::Run(int8, int, int8*)':
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5775:11: error: 'int8x8_t' does not name a type
     const int8x8_t fill_data_vec_int8 = vdup_n_s8(fill_data);
           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5776:11: error: 'uint32x2_t' does not name a type
     const uint32x2_t fill_data_vec = vreinterpret_u32_s8(fill_data_vec_int8);
           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5780:55: error: 'fill_data_vec' was not declared in this scope
       vst1_lane_u32(reinterpret_cast<uint32_t*>(ptr), fill_data_vec, 0);
                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5780:71: error: 'vst1_lane_u32' was not declared in this scope
       vst1_lane_u32(reinterpret_cast<uint32_t*>(ptr), fill_data_vec, 0);
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5783:19: error: 'fill_data_vec' was not declared in this scope
                   fill_data_vec, 0);
                   ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5783:35: error: 'vst1_lane_u32' was not declared in this scope
                   fill_data_vec, 0);
                                   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 26.504s, Critical Path: 10.13s
INFO: 23 processes: 23 local.
FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Add the following code to the top of `<install folder>/tensorflow/WORKSPACE`, as suggested [here](https://github.com/tensorflow/tensorflow/issues/28824#issuecomment-493675905):

```
http_archive(
    name = ""build_bazel_rules_nodejs"",
    # Replace with a real SHA256 checksum
    sha256 = ""{SHA256}""
    # Replace with a real commit SHA
    strip_prefix = ""rules_nodejs-{HEAD}"",
    urls = [""https://github.com/bazelbuild/rules_nodejs/archive/{HEAD}.tar.gz""],
)
```
2. Run `./configure` with all option set to n
3. Build tensorflow `bazel build --host_copt=""-std=gnu99"" --copt=""-std=gnu99"" --conlyopt=""-std=gnu99"" //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
See above


Any help would be appreciated.
"
31123,how can I feed multi inputs with tf2.0,"how can i feed value to actions in loss functions  error  like this
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,4]
```

my code here
```
from tensorflow import keras
import tensorflow as tf
import numpy as np


class DQN:
    def __init__(self, actions, state_shape):
        self.actions = actions
        self.state_shape = state_shape
        self.create_model()

    def loss_func(self, actions):
        def nested_func(label, logits):
            print(label, logits)
            return tf.losses.mean_squared_error(tf.reduce_sum(tf.multiply(actions, logits), axis=-1), label)

        return nested_func

    def create_model(self):
        def loss_func(actions):
            def nested_func(label, logits):
                return tf.losses.mean_squared_error(tf.reduce_sum(tf.multiply(actions, logits), axis=-1),
                                                    tf.reduce_sum(label, axis=-1))

            return nested_func

        actions = keras.Input((self.actions))
        self.model = keras.models.Sequential([
            keras.layers.Conv2D(filters=32, kernel_size=8, strides=4, padding='same', activation='relu'),
            keras.layers.MaxPool2D(pool_size=(2, 2), strides=1, padding='same'),
            keras.layers.Conv2D(filters=64, kernel_size=4, strides=2, padding='same', activation='relu'),
            keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'),
            keras.layers.Flatten(),
            keras.layers.Dense(512, activation='relu'),
            keras.layers.Dense(self.actions, activation=None),
        ])
        self.model.compile(optimizer=tf.train.AdamOptimizer(1e-3), loss=loss_func(actions))



if __name__ == '__main__':
    model = DQN(4, (80, 80, 4))
    states = np.random.rand(100, 80, 80, 4)
    actions = np.random.randint(0, 4, (100, 4))
    label = np.random.randint(0, 4, (100, 4))
    model.model.fit(states, label)
```

"
31121,How to use Tensorboard logging in tf2.0 step loop of the @tf.function,"Hello,

I have been trying to figure out how to log to tensorboard from the gradienttape loop function

thanks,
tejavoo"
31120,The aip of instance normaalization,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):tf2.0
- Are you willing to contribute it (Yes/No):yes



**Describe the feature and the current behavior/state.**

tensorflow/python/keras/layers/normalization.py
**Will this change the current api? How?**
tensorflow/python/keras/layers/normalization.py add a instance normalizatiom layer class
**Who will benefit with this feature?**

**Any Other info.**
"
31119,How to create a tensorflow lite plugin for Unity,"I'm trying to use tensorflow lite in unity.
I need plugins to run my project.
Especially a **plugin for IOS** [tensorflowlite_c]

Thank you!"
31118,Tensorflowlite 1.14: bazel windows build is broken,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template

System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10

Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n/a

TensorFlow installed from (source or binary):
Build from source

TensorFlow version:
r1.14, 456fbc0e498e3d10604973de9f46ca48d62267cc

Python version:
2.7.15

Installed using virtualenv? pip? conda?:
n/a

Bazel version (if compiling from source):
0.21.0

GCC/Compiler version (if compiling from source):
MSVC 2017

CUDA/cuDNN version:
n/a

GPU model and memory:
n/a

Describe the problem
Windows build of tensorflow lite fails.

Provide the exact sequence of commands / steps that you executed before running into the problem

```
$ bazel --output_user_root=D:\test\tensorflow_1.14.windows build  --jobs 1 -c opt --cxxopt=--std=c++11 //tensorflow/lite:libtensorflowlite.so

INFO: Repository 'flatbuffers' used the following cache hits instead of downloading the corresponding file.              
* Hash '3f4a286642094f45b1b77228656fbd7ea123964f19502f9ecfd29933fd23a50b' for http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz                                                                                
If the definition of 'flatbuffers' was updated, verify that the hashes were also updated.                               
ERROR: An error occurred during the fetch of repository 'flatbuffers':                                                     
java.io.IOException: Could not create symlink from 
D:/test/tensorflow/third_party/flatbuffers/build_defs.bzl to 
D:/test/tensorflow_1.14.windows/s5ozolul/external/flatbuffers/build_defs.bzl: 
D:/test/tensorflow_1.14.windows/s5ozolul/external/flatbuffers/build_defs.bzl (File exists)                                                                             
ERROR: D:/test/tensorflow/tensorflow/lite/BUILD:149:1: //tensorflow/lite:framework depends on 
//tensorflow/lite/schema:schema_fbs in repository @ which failed to fetch. no such package 
'@flatbuffers//': java.io.IOException: Could not create symlink from 
D:/test/tensorflow/third_party/flatbuffers/build_defs.bzl to 
D:/test/tensorflow_1.14.windows/s5ozolul/external/flatbuffers/build_defs.bzl: 
D:/test/tensorflow_1.14.windows/s5ozolul/external/flatbuffers/build_defs.bzl (File exists)                                                                                                                     
ERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: no such package 
'@flatbuffers//': java.io.IOException: Could not create symlink from 
D:/test/tensorflow/third_party/flatbuffers/build_defs.bzl to 
D:/test/tensorflow_1.14.windows/s5ozolul/external/flatbuffers/build_defs.bzl: 
D:/test/tensorflow_1.14.windows/s5ozolul/external/flatbuffers/build_defs.bzl (File exists)                                                                            
INFO: Elapsed time: 96.770s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (22 packages loaded, 98 targets configured)                                    
currently loading: tensorflow/lite/schema
```"
31117,Model does not converge during training with distribute strategy and tf.py_function in dataset.map,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution:Windows 10 Pro
- TensorFlow installed from:binary(pip)
- TensorFlow version:2.0.0-beta1
- Python version:3.6.8
- CUDA/cuDNN version:10.0
- GPU model and memory:Tesla K80

**Describe the current behavior**
Model does not converge and training is extremely slow with distribute strategy, if tf.py_function called in dataset.map. 
Works fine without distribute strategy or implementing map function without tf.py_function.

**Describe the expected behavior**
Succesful training with distribute strategy when tf.py_function called in dataset.map.

**Code to reproduce the issue**
Reproducible in Colaboratory with TF 1.14.0
https://colab.research.google.com/drive/1J98yNqa4gslsTe1SiCrhomhNB6UREkLC
```
import tensorflow as tf
from tensorflow import keras


def get_keys():
    return list(range(-1000, 1000))

# generate positive feature and label
def generate_positive():
    x = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)
    y = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)
    label = tf.constant(1)
    return x, y, label


# generate negative feature and label
def generate_negative():
    x = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)
    y = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)
    label = tf.constant(0)
    return x, y, label


# generate pos/neg feature and label
def generate_pf(key):
    if key > -1:
        x = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)
        y = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)
        label = tf.constant(1)
    else:
        x = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)
        y = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)
        label = tf.constant(0)

    x = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(x)
    y = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(y)
    feature = tf.stack([x, y])
    return feature, label


# dataset map function: works with or without distribute strategy
def map_tf(key):
    x, y, label = tf.cond(tf.greater(key, -1), generate_positive, generate_negative)
    x = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(x)
    y = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(y)
    feature = tf.stack([x, y])
    return feature, label


# dataset map function: works without distribute strategy
def map_py_func(key):
    feature, label = tf.py_function(func=generate_pf,
                                    inp=[key],
                                    Tout=[tf.float32, tf.int32])
    # TODO: skip shape setting if training without distribute strategy
    feature.set_shape([2])
    label.set_shape([1])
    return feature, label


def get_dataset():
    keys = get_keys()
    dataset = tf.data.Dataset.from_tensor_slices(keys)
    dataset = dataset.repeat()
    dataset = dataset.shuffle(buffer_size=len(keys))
    # replace 'map_py_func' with 'map_tf' to have successful training
    dataset = dataset.map(lambda key: map_py_func(key), 
                          num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.batch(100)
    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    return dataset


def get_model():
    x = inputs = keras.Input([2])
    x = keras.layers.Dense(units=4, activation='relu')(inputs=x)
    x = keras.layers.Dense(units=4, activation='relu')(inputs=x)
    x = keras.layers.Dense(units=1, activation='sigmoid')(inputs=x)
    return keras.Model(inputs=inputs, outputs=x)


# server = tf.distribute.Server.create_local_server()
train_dataset = get_dataset()
val_dataset = get_dataset()

strategy = tf.distribute.OneDeviceStrategy('/gpu:0')
# strategy = tf.distribute.MirroredStrategy(devices=[""/gpu:0"", ""/gpu:1""],
#                                           cross_device_ops=tf.distribute.ReductionToOneDevice())
with strategy.scope():
    model = get_model()
    optimizer = keras.optimizers.Adam(learning_rate=1e-2)
    model.compile(optimizer=optimizer,
                  loss=keras.losses.BinaryCrossentropy(name='loss'),
                  metrics=[keras.metrics.BinaryAccuracy(name='accuracy')],
                  run_eagerly=False)

model.fit(train_dataset,
          initial_epoch=0,
          epochs=20,
          steps_per_epoch=50,
          validation_data=val_dataset,
          validation_steps=1,
          validation_freq=1)
```

**Other info / logs**
```
  1/100 [..............................] - ETA: 11:27 - loss: 15.6796 - accuracy: 0.5120
  2/100 [..............................] - ETA: 6:00 - loss: 18.0561 - accuracy: 0.4938 
  3/100 [..............................] - ETA: 4:10 - loss: 19.7856 - accuracy: 0.4705
  4/100 [>.............................] - ETA: 3:14 - loss: 18.6577 - accuracy: 0.4707
  5/100 [>.............................] - ETA: 2:42 - loss: 17.9387 - accuracy: 0.4689
  6/100 [>.............................] - ETA: 2:19 - loss: 17.1595 - accuracy: 0.4741
  7/100 [=>............................] - ETA: 2:04 - loss: 16.9366 - accuracy: 0.4744
  8/100 [=>............................] - ETA: 1:53 - loss: 16.6326 - accuracy: 0.4814
  9/100 [=>............................] - ETA: 1:44 - loss: 16.1086 - accuracy: 0.4825
 10/100 [==>...........................] - ETA: 1:36 - loss: 15.7002 - accuracy: 0.4848
```"
