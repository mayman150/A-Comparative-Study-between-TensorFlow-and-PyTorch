Issue Number,Issue Title,Issue Body
47812,Does anymore know how to install for tensorflow version 1.01 in google colabs ,
47809,silently crash,"Folks,

I have a strange issue that the latest night build never starts training nor I see any stack traces.

```
Build
2.5.0-dev20210314
```

it sees GPU

```
from tensorflow.python.client import device_lib
2021-03-15 08:32:21.311250: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
>>> print(device_lib.list_local_devices())
2021-03-15 08:32:23.915414: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the f
ollowing CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-15 08:32:23.918372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-03-15 08:32:23.942584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1779] Found device 0 with properties:
pciBusID: 0000:02:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2021-03-15 08:32:23.942998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-15 08:32:23.971817: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-15 08:32:23.972051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-15 08:32:23.989838: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-15 08:32:23.995070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-15 08:32:24.000006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-15 08:32:24.024262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-15 08:32:24.025408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-15 08:32:24.025666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1917] Adding visible gpu devices: 0
2021-03-15 08:32:24.527083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-15 08:32:24.527332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1310]      0
2021-03-15 08:32:24.527503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1323] 0:   N
2021-03-15 08:32:24.527805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Created TensorFlow device (/device:GPU:0 with 6629 MB memory) -> physical GPU (device: 0, nam
e: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 17497860804910539641
, name: ""/device:GPU:0""
device_type: ""GPU""
memory_limit: 6951272448
locality {
  bus_id: 1
  links {
  }
}
incarnation: 16814837068753917860
physical_device_desc: ""device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1""
]
```

* System window 10 all update.

* Visual Studio Code 2019.

* All Cuda test 11.22 passed.

```
test.py
2021-03-15 08:41:55.464473: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-15 08:41:57.975692: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-03-15 08:41:58.002676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1779] Found device 0 with properties:
pciBusID: 0000:02:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2021-03-15 08:41:58.003092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-15 08:41:58.032297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-15 08:41:58.032535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-15 08:41:58.050106: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-15 08:41:58.055219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-15 08:41:58.059974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-15 08:41:58.079919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-15 08:41:58.080858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-15 08:41:58.081073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1917] Adding visible gpu devices: 0
Num GPUs Available:  1
2021-03-15 08:41:58.567984: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the f
ollowing CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-15 08:41:58.569479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1779] Found device 0 with properties:
pciBusID: 0000:02:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2021-03-15 08:41:58.569775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1917] Adding visible gpu devices: 0
2021-03-15 08:41:59.048119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-15 08:41:59.048383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1310]      0
2021-03-15 08:41:59.048591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1323] 0:   N
2021-03-15 08:41:59.049151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6629 MB memory)
-> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
2021-03-15 08:41:59.053009: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-15 08:41:59.332823: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-15 08:41:59.333765: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-15 08:41:59.334101: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-15 08:41:59.334533: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-15 08:41:59.334936: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-15 08:41:59.341584: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-15 08:41:59.342176: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
```

Code I run

```
import tensorflow as tf
import numpy as np

print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
tf.debugging.set_log_device_placement(True)

tf.autograph.set_verbosity(5)
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0


model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(10)
            ])

predictions = model(x_train[:1]).numpy()
print(predictions)

tf.nn.softmax(predictions).numpy()
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
loss_fn(y_train[:1], predictions).numpy()

model.compile()
model.summary()

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test,  y_test, verbose=2)

probability_model = tf.keras.Sequential([
      model, tf.keras.layers.Softmax()
        ])
probability_model(x_test[:5])
```
"
47807,The GPU memory allocate logic bug in tensorflow 1.15,"<em>
Hello, I tried to upgrade my code from tensorflow 1.12 to tensorflow 1.15. Everything is ok in tf1.12, but I found out some gpu memory allocate logic bug in tf1.15. 

Here are some details: 

1.I fail to use allow_growth=False, tensorflow report ""Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR"", but in server of my lab, everything is ok, so I guess that maybe it relates to the gpu memory my local computer occupy, e.g. compiz, Xorg. 

2.When i set the allow_growth=True, the code works in tf 1.15, but the gpu memory-usae is so weired, I think gpu memory-usage is linear function of batch_size, but not directly proportional in tensorflow 1.15 which is different from tensorflow 1.12. In tf1.15, the gpu memory-usage is 10G when batchsize=1, 11G when batchsize=2, 19G when batchsize=3, and 17G wen batch size =4. 

In conclusion, I guess that maybe some gpu memory allocate bugs exist in tensorflow 1.15. 

</em>

**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04)
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):tensorflow 1.15.
- Python version:3.6.13
- Bazel version (if compiling from source):1.0.0
- GCC/Compiler version (if compiling from source):7.5.0
- CUDA/cuDNN version:cuda:10 cudnn 7.6.0
- GPU model and memory: 32G tesla V100 (server)and 12G RTX 2080 (local)

"
47806,TFLM Magic Wand example perform late ,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Himax WE-1

Hi, 

I would like to ask for comments from developers who have implemented the magic wand example.

I ran the magic wand example with a Himax WE-1 board. However, there was a problem with slow reaction.
1. After executing the program, it is not recognized even if one of the wing ring slopes is executed continuously. For example, running the wing several times didn't bring up anything in the prompt window.

2. As it still did not recognize, I restarted by pressing the reset button. So, the previously entered action was continuously displayed. In other words, when the program was executed, it could not be launched and was pushed and operated. For example, after entering the wing multiple times in the previous operation, the prompt window did not appear, so as soon as the reset button was pressed, the wing was launched twice in succession.

When running other examples supported by the Himax board, it does not appear to be a problem in the prompt window as there was no problem.

I would like to hear from developers who are having the same problem as me or who have no problem when running magic wand.

Thanks



"
47805,Hello World Example not supported in Himax WE-1 board. ,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Himax WE-1

Hi,

I'm curious why TFLM doesn't support the Hello World example of the Himax WE-1 board. Until January of this year, we have confirmed that TFLM supports the Hello World example of the Himax WE-1 board. I am curious about the reason for not applying at this time. (The Arduino IDE still supports the Hello World example.)

Thanks 

"
47804,Quadratic slowdown on LSTM models in tflite,"**System information**
This reproduces on both colab and my local machine. I assume colab is more standard and thus more interesting, but my local info for completeness:
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, a bit!
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1 (also reproduces on nightly)
- Python version: 3.8.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
LSTM models experience quadratic slowdown when converted to tflite.

The attached notebook shows the behaviour clearly: when using a standard keras model for predictions, the prediction latency is linear in the length of the sequence provided, as expected. When the same model is converted to tflite, the performance is initially good but scales up quadratically with the input size, and becomes effectively unusable for large sequences.

**Describe the expected behavior**
I would expect performance to scale linearly with sequence length in tflite, as it does in keras, and as you'd expect from the underlying maths.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1APyFWSL3ln5gZFEotw_5wwUDpaSJrzD-

**Other info / logs**
The notebook shows the context pretty clearly, but I'm happy to provide more info if needed!
"
47802,module 'gast' has no attribute 'Constant',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code based on https://www.tensorflow.org/tutorials/structured_data/time_series
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windoes 10 pro build 19041.867
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (python pip)
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: v11.2 with cudnn64_8.dll
- GPU model and memory: GeForce GTX 1080 Ti Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9525 MB memory)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Console output:
2021-03-12 11:01:01.858750: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll

	I'm currently running Tensorflow version 2.4.1

2021-03-12 11:01:09.201806: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-12 11:01:09.202893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-03-12 11:01:09.222367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2021-03-12 11:01:09.222840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-12 11:01:09.231298: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-12 11:01:09.231559: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-12 11:01:09.236389: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-12 11:01:09.238217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-12 11:01:09.243569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-12 11:01:09.247807: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-12 11:01:09.248947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-12 11:01:09.249278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-12 11:01:09.249799: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-12 11:01:09.251831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2021-03-12 11:01:09.252388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-12 11:01:09.252646: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-12 11:01:09.252901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-12 11:01:09.253149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-12 11:01:09.253374: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-12 11:01:09.253613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-12 11:01:09.253851: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-12 11:01:09.254088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-12 11:01:09.254365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-12 11:01:10.247733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-12 11:01:10.247986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-12 11:01:10.248140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-12 11:01:10.248903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9525 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
2021-03-12 11:01:10.250531: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set

WARNING:tensorflow:AutoGraph could not transform <bound method WindowGenerator.split_window of Total window size: 48
Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
Label indices: [47]
Label column name(s): ['T (degC)']> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert"
47800,Tensorflow (Using TensorFlow backend.),"while running the INVASE code I got this error,

Using TensorFlow backend.
Traceback (most recent call last):
  File ""C:\Users\Mahsa\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Users\Mahsa\AppData\Local\Programs\Python\Python37\lib\ctypes\__init__.py"", line 364, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found
 How could I fix it? 
I have installed many Tersorflow libraries and check if one works or not, but noone has works!
"
47799,Process finished with exit code 132 (interrupted by signal 4: SIGILL),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.2.1 Apple M1
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8

**Describe the current behavior**
After running my script I receive the following error: ""Process finished with exit code 132 (interrupted by signal 4: SIGILL)""

**Describe the expected behavior**
I expect the script to run entirely

**Standalone code to reproduce the issue**

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

training_dataset = pd.read_csv('/Users/--/Downloads/Archive/AAPL.csv')
print(training_dataset)
training_set = training_dataset.iloc[:,1:-1].values
print(training_set)
from sklearn.preprocessing import MinMaxScaler

sc = MinMaxScaler(feature_range = (0, 1))
training_set_scaled = sc.fit_transform(training_set)
print(training_set_scaled)

X_train = []
Y_train = []

for i in range(252,len(training_set)):
    X_train.append(training_set_scaled[i - 252:i,0])
    Y_train.append(training_set_scaled[i,0])

X_train, Y_train = np.array(X_train), np.array(Y_train)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

model = Sequential()

model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))

model.add(Dropout(0.2))

model.add(model.add(LSTM(units = 50, return_sequences = True)))

model.add(Dropout(0.2))

model.add(model.add(LSTM(units = 50, return_sequences = True)))

model.add(Dropout(0.2))

model.add(model.add(LSTM(units = 50, return_sequences = True)))

model.add(Dropout(0.2))

model.add(Dense(units = 1))

model.compile(optimizer = 'adam', loss = 'mean_squared_error')

model.fit(X_train, Y_train, epochs = 100, batch_size = 32)

"
47798,Cuda Error for CNN,"
**CUDA Error with CNN algorithm, 
CUDA Toolkit 11.0 installed 
TensorFlow shows GPU and runs GPU very well in other algorithms but not in CNN.**

**_https://keras.io/examples/audio/speaker_recognition_using_cnn/_** I am trying to run this code in my RTX 3070



 'python' 'c:\Users\krish\.vscode\extensions\ms-python.python-2021.2.633441544\pythonFiles\lib\python\debugpy\launcher' '50523' '--' 'c:\Users\krish\Desktop\All_DL\keras_site\cnn_test.py'
2021-03-14 21:26:45.065591: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
Found 6 files belonging to 2 directories
2021-03-14 21:26:47.470858: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set   
2021-03-14 21:26:47.474642: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-03-14 21:26:47.502462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-03-14 21:26:47.509399: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-14 21:26:47.516038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-14 21:26:47.522950: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-14 21:26:47.530936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-14 21:26:47.542472: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-14 21:26:47.549435: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-14 21:26:47.558565: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-14 21:26:47.563129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-14 21:26:47.572197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-14 21:26:47.575148: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-14 21:26:47.593140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-03-14 21:26:47.608325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-14 21:26:47.611339: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-14 21:26:47.623072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-14 21:26:47.626485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-14 21:26:47.629093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-14 21:26:47.639563: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-14 21:26:47.645983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-14 21:26:47.655138: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-14 21:26:47.658821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-14 21:26:48.096628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-14 21:26:48.099998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-03-14 21:26:48.102166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-03-14 21:26:48.104363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6441 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)
2021-03-14 21:26:48.132896: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
6 noise files were split into 354 noise samples where each is 1 sec. long
Our class names: ['Benjamin_Netanyau', 'Jens_Stoltenberg', 'Julia_Gillard', 'Magaret_Tarcher', 'Nelson_Mandela']
Processing speaker Benjamin_Netanyau
Processing speaker Jens_Stoltenberg
Processing speaker Julia_Gillard
Processing speaker Magaret_Tarcher
Processing speaker Nelson_Mandela
Found 7501 files belonging to 5 classes.
Using 6751 files for training.
Using 750 files for validation.
Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input (InputLayer)              [(None, 8000, 1)]    0
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 8000, 16)     64          input[0][0]
__________________________________________________________________________________________________
activation (Activation)         (None, 8000, 16)     0           conv1d_1[0][0]
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 8000, 16)     784         activation[0][0]
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 8000, 16)     32          input[0][0]
__________________________________________________________________________________________________
add (Add)                       (None, 8000, 16)     0           conv1d_2[0][0]
                                                                 conv1d[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 8000, 16)     0           add[0][0]
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 4000, 16)     0           activation_1[0][0]
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 4000, 32)     1568        max_pooling1d[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 4000, 32)     0           conv1d_4[0][0]
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 4000, 32)     3104        activation_2[0][0]
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 4000, 32)     544         max_pooling1d[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 4000, 32)     0           conv1d_5[0][0]
                                                                 conv1d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 4000, 32)     0           add_1[0][0]
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, 2000, 32)     0           activation_3[0][0]
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 2000, 64)     6208        max_pooling1d_1[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 2000, 64)     0           conv1d_7[0][0]
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 2000, 64)     12352       activation_4[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 2000, 64)     0           conv1d_8[0][0]
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 2000, 64)     12352       activation_5[0][0]
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 2000, 64)     2112        max_pooling1d_1[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 2000, 64)     0           conv1d_9[0][0]
                                                                 conv1d_6[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 2000, 64)     0           add_2[0][0]
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (None, 1000, 64)     0           activation_6[0][0]
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, 1000, 128)    24704       max_pooling1d_2[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 1000, 128)    0           conv1d_11[0][0]
__________________________________________________________________________________________________
conv1d_12 (Conv1D)              (None, 1000, 128)    49280       activation_7[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 1000, 128)    0           conv1d_12[0][0]
__________________________________________________________________________________________________
conv1d_13 (Conv1D)              (None, 1000, 128)    49280       activation_8[0][0]
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 1000, 128)    8320        max_pooling1d_2[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 1000, 128)    0           conv1d_13[0][0]
                                                                 conv1d_10[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 1000, 128)    0           add_3[0][0]
__________________________________________________________________________________________________
max_pooling1d_3 (MaxPooling1D)  (None, 500, 128)     0           activation_9[0][0]
__________________________________________________________________________________________________
conv1d_15 (Conv1D)              (None, 500, 128)     49280       max_pooling1d_3[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 500, 128)     0           conv1d_15[0][0]
__________________________________________________________________________________________________
conv1d_16 (Conv1D)              (None, 500, 128)     49280       activation_10[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 500, 128)     0           conv1d_16[0][0]
__________________________________________________________________________________________________
conv1d_17 (Conv1D)              (None, 500, 128)     49280       activation_11[0][0]
__________________________________________________________________________________________________
conv1d_14 (Conv1D)              (None, 500, 128)     16512       max_pooling1d_3[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, 500, 128)     0           conv1d_17[0][0]
                                                                 conv1d_14[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 500, 128)     0           add_4[0][0]
__________________________________________________________________________________________________
max_pooling1d_4 (MaxPooling1D)  (None, 250, 128)     0           activation_12[0][0]
__________________________________________________________________________________________________
average_pooling1d (AveragePooli (None, 83, 128)      0           max_pooling1d_4[0][0]
__________________________________________________________________________________________________
flatten (Flatten)               (None, 10624)        0           average_pooling1d[0][0]
__________________________________________________________________________________________________
dense (Dense)                   (None, 256)          2720000     flatten[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 128)          32896       dense[0][0]
__________________________________________________________________________________________________
output (Dense)                  (None, 5)            645         dense_1[0][0]
==================================================================================================
Total params: 3,088,597
Trainable params: 3,088,597
Non-trainable params: 0
__________________________________________________________________________________________________
2021-03-14 21:26:50.279589: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
Epoch 1/100
2021-03-14 21:26:51.735210: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-14 21:26:52.441015: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2021-03-14 21:26:52.443810: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2021-03-14 21:26:52.446376: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2021-03-14 21:26:52.450450: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2021-03-14 21:26:52.453265: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2021-03-14 21:26:52.460886: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2021-03-14 21:26:52.469255: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2021-03-14 21:26:52.478998: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2021-03-14 21:26:52.482605: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2021-03-14 21:26:53.456269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-14 21:26:54.448825: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
2021-03-14 21:26:54.452490: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
2021-03-14 21:26:54.458915: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
2021-03-14 21:26:54.462869: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
Traceback (most recent call last):
  File ""c:\Users\krish\Desktop\All_DL\keras_site\cnn_test.py"", line 279, in <module>
    history = model.fit(
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 888, in _call
    return self._stateless_fn(*args, **kwds)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 2942, in __call__
    return graph_function._call_flat(
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 555, in call
    outputs = execute.execute(
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[node model/conv1d/conv1d (defined at c:\Users\krish\Desktop\All_DL\keras_site\cnn_test.py:279) ]] [Op:__inference_train_function_3382]

Function call stack:
train_function
"
47797,Length for attr 'output_shapes' of 0 must be at least minimum 1,"I am trying to convert a list of lists of strings into a tf.data.Dataset object, but it throws this error:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Length for attr 'output_shapes' of 0 must be at least minimum 1
	; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=""default""; attr=preserve_cardinality:bool,default=false> [Op:ParallelMapDatasetV2]
```
As you can see, I specified output_shapes and output_types, I even tried converting the list into a ragged tensor first but that also changed nothing. 
Here is my code:
```
    tf_train_ds = tf.data.Dataset.from_generator(
        lambda: train_list,
        output_types=(tf.string),
        output_shapes=(178728,)
        #output_shapes=(tf.TensorSpec(shape=(2,), dtype=tf.string), tf.TensorSpec(shape=(2,), dtype=tf.string))
        # output_signature=(
        #     tf.TensorSpec(shape=(2,), dtype=tf.string),
        #     tf.TensorSpec(shape=(2,), dtype=tf.string)
        # )
    )
```

I am stuck with this error for so so long..."
47795,distribution strategy in tensorflow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
47794,undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb,"This is an issue reported earlier and it remains for the following versions of tensorflow and keras:

Successfully installed tensorflow-2.4.1

Successfully installed keras-2.4.3

When trying to execute previously well-functioning keras/tensorflow-pythoncode output is: 

tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb

Are the above versions of tensorflow and keras incompatible? Is a downgrade of tensorflow requried, and if so to which version ?
"
47793,  programs that use this library are impossible to understand,"My cpu has a frequency of 3GHz, that is, electric current runs through it's circuitry 3 thousand million times per second, in something more than 2 seconds it cycles once for every person in the world, this of course is an ordinary modern computer, the small computers in our pockets have the same power. 25 years ago a computer with a thirtiedth of the power beat the strongest chess player in the world taking no more than a couple of minutes per move. With one second of that 1996 cpu I can find the first thousand prime numbers, I can figure out how much sleep I get on average, I can find out the geometry of an ozone atom.  

Now why in the world does a program take 10 hours to build? 3 million million cycles to compile a program? And this is just the beggining, tensorflow models will then be trained, so they will continue to squeeze out a cpu like it's a slave until something useful comes out of the other end. We are spending a small eternity to compile a compiler, which then will spend as many small eternities as money can buy to obtain a result, without care for the way it is accomplished.

Can we still consider this open source? Will someone be able to understand this code, and if they can, will they be able to understand the binaries, and if they manage to do that, will they ever understand the trained models.
The open source movement tries to understand the software that we build and use, but by turning the assembly line up to 11, this approach is effectively creating logic/code faster than we can analyze it. There's definitely applications for such code and human systems will cope, mainly by treating these programs as a black box and studying the systems they interface with instead, but I just wanted to expose that, altough open source, these machine learning libraries are blueprints to build a machine that generates opaque systems, which is kind of a paradox."
47792,Retracing happens when using eager execution for custom code,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab default
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):  Colab default
- TensorFlow version (use command below): 2.4.1
- Python version: Colab default
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: Colab default
- GPU model and memory: Tesla T4


**Describe the current behavior**
I use a `get_param()` function to wrap the `tf.Variable()`  function.  It returns the `Variable` on non-first call or creates it otherwise. There is a `network()` function that gets the network output and a `train_network()` function that runs a training step. Following [this example](https://www.tensorflow.org/guide/function#using_with_multiple_keras_optimizers) I created a Keras Adam optimizer for training  a network.

I found that tracing happens twice for `train_network()` and once for `network()`. I wonder whether it is a bug and whether this will negatively affect the efficiency of the code.

**Describe the expected behavior**
Tracing should happens only once for both `network()`  and `train_network()`

**Standalone code to reproduce the issue**
```
w=get_param('w0', np.random.normal(0,1,[10000,10000]))
@tf.function
def network(): # Define a network 
  print('Tracing network')
  w=get_param('w0')
  return tf.reduce_sum(w**2)

opt = tf.keras.optimizers.Adam(1)

@tf.function
def train_network():
 print('Tracing train_network')
 with tf.GradientTape() as tape:
    loss = network()
    w=get_param('w0')
 grad=tape.gradient(loss, [w])
 grad_and_var=zip(grad,[w])
 opt.apply_gradients(grad_and_var)
 return loss
```
```
for i in range(10):
  t0 = time.time()
  loss = train_network()
  iter_time = time.time() - t0
  print('iter %i:'%i, ' loss=%.2f,' % loss, 'iter_itme=%.4f s' % iter_time) 
```

For the complete program please refer to [this Colab Notebook](https://colab.research.google.com/drive/1RomHVK3fHPCJw8tSst4DERTBSEm9sf0t?usp=sharing)
"
47791,Inconsistent Embedding Layer functionality when initialized under MirroredStrategy(),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:  GeForce GTC 1080 Ti - 12 Gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
 
I'm passing in a tensor to an embedding layer. The tensor contains a value greater than the vocab size specified when initializing the embedding layer. This throws an error, as expected, when the layer is initialized outside the `tf.distribute.MirroredStrategy()`. In other words, if I create a model on a CPU machine or a single GPU machine, passing an invalid tensor throws an error. This is the correct behavior. 

However, if I pass in this tensor under the MirroredStrategy() then the error is no longer thrown and the model continues to train


**Describe the expected behavior**

The expected behavior is that an error should be thrown if an invalid tensor (i.e. a tensor with vocab size greater than the specified value to an embedding layer) is passed to an embedding layer regardless of the strategy used for data parallelism. 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import numpy as np
import tensorflow as tf
import tensorflow.keras.backend as K

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential, load_model, Model 

embed_dim = 64; maxlen = 152; vocab_size = 8
K.clear_session()
X_tk = np.random.randint(1, vocab_size, (10, 152))
X_mask_tk = np.random.randint(1, vocab_size + 1, (10, 152)) #The +1 is for the mask token
l = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
# print(l(X_tk)) #this works
# print(l(X_mask_tk)) #this doesn't work

model = keras.Sequential([layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)])
# print(model(X_tk)) #this works
# print(model(X_mask_tk)) #this doesn't works

model2 = keras.Sequential([layers.Input(shape=(maxlen,)), layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)])
model2.compile(optimizer='Adam', loss='sparse_categorical_crossentropy')
# model2.fit(X_mask_tk, X_tk) #This doesn't work

strategy = tf.distribute.MirroredStrategy()
with strategy.scope(): #im using 2 gpus but I reckon this issue would occur even with 1 gpu

    model3 = keras.Sequential([layers.Input(shape=(maxlen,)), layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)])
    model3.compile(optimizer='Adam', loss='sparse_categorical_crossentropy')
model3.fit(X_mask_tk, X_tk) #This works
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

This should be the expected error when running `model3.fit(...)`:
```
InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  indices[3,21] = 8 is not in [0, 8)
	 [[node sequential_1/embedding_2/embedding_lookup (defined at <ipython-input-8-1c81a1dbdc4a>:25) ]]
	 [[sequential_1/embedding_2/embedding_lookup/_24]]
  (1) Invalid argument:  indices[3,21] = 8 is not in [0, 8)
	 [[node sequential_1/embedding_2/embedding_lookup (defined at <ipython-input-8-1c81a1dbdc4a>:25) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_25411]
```
"
47789,ImageResizerState's public function contains redundant arguments,`ImageResizerState::ValidateAndCreateOutput` needs a tensor as the second argument. Actually it is unnecessary. Because `input` can be retrieved through `context->input(0)`. `ImageResizerGradientState` has a similar issue.
47787,Inference for WeightNormalization,"
WeightNormalization according to
`https://www.tensorflow.org/addons/api_docs/python/tfa/layers/WeightNormalization` 
creates a wrapper around the kernel for normalization. it uses `tf.nn.l2_normalize` for normalization.
The issue is during inference which you don't need to run `tf.nn.l2_normalize` every time.
Is there any tool or a way to replace the kernel with normalized kernel after training?
"
47786,"InvalidArgumentError:  indices[112,8] = -1 is not in [0, 32)","Hi!

Im trying to create an RNN to predict if a stock price will go up or down tomorrow (its just for practice, im not expecting that it works).

But i can not run the model. I let you the code and the error message:

CODE ////////////////////////////////////////////////////////////////////////////////////////////////
`import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM
from tensorflow.keras.layers import SimpleRNN
from tensorflow.keras import initializers

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

inTrain, inTest, outTrain, outTest = train_test_split(pd.read_csv(""CocaColaImput.csv""), pd.read_csv(""CocaColaOutput.csv""), test_size=0.33, random_state=14)
inTrain=inTrain.reset_index(drop=True)
inTest=inTest.reset_index(drop=True)
outTrain=outTrain.reset_index(drop=True)
outTest=outTest.reset_index(drop=True)

model_rnn = Sequential()
model_rnn.add(Embedding(32, 24))
model_rnn.add(Dense(5, activation='relu'))
model_rnn.add(Dense(1, activation='sigmoid'))

model_rnn.compile(loss='mse', optimizer='adam',metrics=['accuracy'])

model_rnn.fit(inTrain, outTrain, batch_size=128, epochs=15, validation_data=(inTest,outTest))`

ERROR MESSAGE ////////////////////////////////////////////////////////////////////////////////////
**The error message:**

_""InvalidArgumentError:  indices[112,8] = -1 is not in [0, 32)
	 [[node sequential_43/embedding_20/embedding_lookup (defined at <ipython-input-208-c4c0034817e8>:1) ]] [Op:__inference_train_function_14324]

Errors may have originated from an input operation.
Input Source operations connected to node sequential_43/embedding_20/embedding_lookup:
 sequential_43/embedding_20/embedding_lookup/14045 (defined at /usr/lib/python3.7/contextlib.py:112)

Function call stack:
train_function""_

/////////////////////////////////////////////////////////////////////////////////////////////////////////

CocaColaImput.csv has a shape of 14868 rows  32 columns; CocaColaOutput.csv has 14868 rows  1 columns.
Im working in the google colaboratory platform, using its standard libraries.


Do you know what i am doing wrong or how can i fix it?"
47785,tape.batch_jacobian gives different results if changing parallel_iterations,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- TensorFlow installed from (source or binary): binary (pip install)
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.5
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**

Using [batch_jacobian](https://www.tensorflow.org/api_docs/python/tf/GradientTape#batch_jacobian) with different values for `parallel_iterations` leads to differences in the output and the computational speed above the expected numerical errors.

**Describe the expected behavior**

Using `tape.batch_jacobian` should result in mostly the same outputs independent of `parallel_iterations`.

In my custom Code (which I did not provide) I encountered differences in calculation speed from 500 us for `parallel_iterations = 3` up to 238 ms for `parallel_iterations=8` without any pattern that could be related to the `parallel_iterations`.

I was able to reproduce this behaviour with a simplified version using `tf.einsum` as a function which provides a jacobian.
The error becomes zero when `parallel_iterations >=16`.


**Standalone code to reproduce the issue**

```py
import tensorflow as tf
data = tf.random.normal((32, 16, 16))

def func(r):
    return tf.einsum(""bij, bij -> bi"", r, r)

@tf.function
def func_grad(r, p_it = 2):
    with tf.GradientTape() as tape:
        tape.watch(r)
        out = func(r)
    return tape.batch_jacobian(out, r, unconnected_gradients=""zero"", parallel_iterations=p_it)

def func_grad_no_parallel(r, p_it = 2):
    with tf.GradientTape(persistent=True) as tape:
        tape.watch(r)
        out = func(r)
    return tape.batch_jacobian(out, r, experimental_use_pfor=False)

def error_analysis(y_true, y_second):
    rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_second)))
    max_err = tf.reduce_max(y_true - y_second)
    print(f""RMSE: {rmse} \t MAX: {max_err}"")

grad_out = func_grad_no_parallel(data)
for x in range(2, 24):
    print(f""Parallel iterations: {x}"")
    jacobian_out = func_grad(data, p_it=x)
    error_analysis(grad_out, jacobian_out)
    %timeit func_grad(data, p_it = x)
    print('-----------------------------------')
```
```
Parallel iterations: 8
RMSE: 0.5038933157920837 	 MAX: 7.24960470199585
1000 loops, best of 5: 667 s per loop
-----------------------------------
Parallel iterations: 9
RMSE: 0.3786795139312744 	 MAX: 7.24960470199585
1000 loops, best of 5: 1.13 ms per loop
-----------------------------------
Parallel iterations: 16
RMSE: 0.5038933157920837 	 MAX: 7.24960470199585
1000 loops, best of 5: 645 s per loop
-----------------------------------
Parallel iterations: 17
RMSE: 0.0 	 MAX: 0.0
1000 loops, best of 5: 1.03 ms per loop
-----------------------------------
```


**Other info / logs**

I was comparing the computation of the jacobian with a persisent GradientTape and looping over the last dimension of my data to get the individual gradients. Without using the `parallel_iterations` the loop over gradients and the jacobian are identical, but computing the jacobian is up to 4x times slower. When using `parallel_iterations` it can get more than twice as fast but comes with large errors/differences to the gradient loop method.

I added some simplified version of this approach to the end of the gist as well. This is not directly related to the bug but I thought, that using the Jacobian would be more efficient than unstacking and looping over the Tensor.

```py

import tensorflow as tf
data = tf.random.normal((32, 16, 3))

def func(r):
    r_mat = r[:, :, tf.newaxis, :] - r[:, tf.newaxis, :, :]
    return tf.einsum(""bijk, bijk -> bi"", r_mat, r_mat)

@tf.function
def func_grad_no_parallel(r):
    with tf.GradientTape(persistent=True) as tape:
        tape.watch(r)
        out = func(r)
    return tape.batch_jacobian(out, r, experimental_use_pfor=False)

@tf.function
def test_grad(r):
    with tf.GradientTape(persistent=True) as tape:
        tape.watch(r)
        out = func(r)
        out = tf.unstack(out, axis=1)
    deriv = []
    for frame in out:
        deriv.append(tape.gradient(frame, r))
    return tf.stack(deriv, axis=1)

tf.reduce_sum(func_grad_no_parallel(data) - test_grad(data))  # they are equal (without parallel_iterations)

%timeit func_grad_no_parallel(data)  # for this simplified version is still slower
%timeit test_grad(data)

```

You can find a [GIST here](https://colab.research.google.com/drive/1h42fHAeWvDn5gRlU5BBWqwHmuBkHQHg5?usp=sharing)
"
47782,M1 arm64 release binaries,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2.3 (20D91)
- TensorFlow installed from (source or binary): tf-nighly
- TensorFlow version: 
- Python version: 3.9.2

**Describe the problem**

`tf-nightly` fails to install on M1 arm64. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
~: where pip
/opt/homebrew/bin/pip
~: where python
/opt/homebrew/bin/python
~: pip --version
pip 21.0.1 from /opt/homebrew/lib/python3.9/site-packages/pip (python 3.9)
~: python --version
Python 3.9.2
~: file /opt/homebrew/bin/python
/opt/homebrew/bin/python: Mach-O 64-bit executable arm64
~: pip install tf-nightly
ERROR: Could not find a version that satisfies the requirement tf-nightly
ERROR: No matching distribution found for tf-nightly
```"
47780,Inconsistency in tf.keras.models.Model.fit() doc,"### URL(s) with the issue
https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit

### Description of the issue (what needs changing):
Related to the `validation_data` parameter, the doc clearly states the following:

> Note that validation_data does not support all the data types that are supported in x, eg, dict, generator or keras.utils.Sequence.

I don't get whether this was just an oversight or it was intended, since this method is working fine even when `validation_data` is a generator. 
"
47779,TensorFlow Lite object detection example code does not work with custom trained model,"### System information

-   **OS Platform and Distribution (macOS Big Sur 11.2)**:
-   **Mobile device (iPhone 7)
-   **TensorFlow Lite installed from (https://www.tensorflow.org/lite/examples/object_detection/overview:View iOS example)**:
-   **TensorFlow (1.14.1)
-   **Xcode version(Version 12.4)
-   **Python (Python 3.7.3)

### Describe the problem
I want to test my custom trained (ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync) model with tensorflow lite, to see if the performance (accuracy and fps) is good enough to be able to start the development of my project for IOS devices or not. But when I execute my .tflite and labelmap.txt files with TensorFlow Lite example code, nothing can be detected.

Here is my flow:

1- I trained my images with ssd_mobilenet_

v1_fpn_shared_box_predictor_640x640_coco14_sync model:
Code:
https://github.com/tensorflow/models
(master branch)
Command:
python3.7 train.py
--logtostderr
--train_dir=/Users/Documents/Temp/tensorflow_last/models/train
--pipeline_config_path=/Users/Documents/Temp/tensorflow_last/models/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
Note:I can also attach my config file, if it is related

2-Converted it to .pb file (tflite_graph.pb):
python3.7 models/research/object_detection/object_detection/export_tflite_ssd_graph.py
--pipeline_config_path=/Users/emre/Documents/Temp/tensorflow_last/models/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
--trained_checkpoint_prefix=/Users/emre/Documents/Temp/tensorflow_last/models/train/model.ckpt-21881
--output_directory=/Users/emre/Documents/Temp/tensorflow_last/models/Lite

3-Converted it to tflite file:
Code:
https://github.com/tensorflow/models
(master branch)
Command:
python3.7 tflite_convert.py --output_file=test.tflite --graph_def_file=/Users/Documents/Temp/tensorflow_last/models/Lite/tflite_graph.pb --input_arrays=image_tensor --output_arrays='detection_boxes','detection_scores','detection_classes','num_detections' --input_shape=1,800,600,3 --allow_custom_ops

4-Downloaded following Tensorflow example code from https://www.tensorflow.org/lite/examples/object_detection/overview
(View iOS example)

The example works fine with the existing pretrained model(which is in tensorflow_lite/examples/lite/examples/object_detection/ios/ObjectDetection/Model/detect.tflite and labelmap.txt) on Xcode version(Version 12.4) and on Mobile device (iPhone 7)).

5-Changed original detect.tflite and labelmap.txt files with my own test.tflite(re-named it as detect.tflite) and labelmap.txt. But when I try to execute the same code to detect my own images with my own .tflite and label map.txt files, nothing can be detected and no error messages pop up.

6-Also tried to execute following pre-trained models with Tensorflow example code, to see if there is a problem with my own custom trained model or not:
https://github.com/asus4/tf-lite-unity-sample/tree/master/Assets/StreamingAssets/mediapipe
But they did not also work properly. Nothing can be detected and no error messages pop up.

I assume that I miss something to modify in example code, since pre-trained models from github.com can also not detect any objects.

Can someone please help me if I need anything else to modify to make my model work? 

Thanks a lot in advance.

### Source code / logs
-No Related log file can I observe in Xcode(Nothing special in debugger output or device logs) 
Note: I have no experience with Xcode. I hope I do not miss any points. I can provide any logs, if you require.
"
47778,python crashes without error performing a tensorflow operation on a new GPU,"EDIT : I don't know if it helps but it looks similar to https://github.com/tensorflow/tensorflow/issues/47770, though on a different hardware
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- TensorFlow installed from (source or binary): see installation procedure
- TensorFlow version: nightly 2.5
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
**Installation procedure :**
1. created a new 3.8 env within anaconda navigator
2. opened the env in terminal and used the two following commands : pip3 install tf-nightly-gpu ; conda install jupyter scikit-learn matplotlib pandas
- CUDA/cuDNN version: cuda_11.2.2_461.33_win10 ; cudnn-11.2-windows-x64-v8.1.1.33 ; DRIVER : 461.72-desktop-win10-64bit-international-dch-whql
- GPU model and memory: NVIDIA RTX 3090

I apologize in advance if I made an obvious mistake, but I am very bad with computers. I basically know the bare minimum for Deep Learning. Thank you very much for your help !

**Describe the problem**
Python crashes without displaying errors. This can be reproduced using the following code :
import tensorflow
a = tensorflow.Variable(1.)
b = tensorflow.Variable(2.)
c = tensorflow.multiply(a, b)
The GPU is correctly detected as far as I can see, and the three first lines work as intended (see full code bellow). The fourth line makes python freeze for a few seconds then python crashes.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I have recently received a new computer with an RTX 3090 and was trying to test it for the first time for some basic Deep Learning. I followed a few guides (e.g. https://www.reddit.com/r/tensorflow/comments/jsalkw/rtx_3090_and_tensorflow_for_windows_10_step_by/ or https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#prerequisites-windows) and installed the following :
- GPU driver (461.72)
- Visual Studio 16.9.1 with C++ desktop development option, including MSVC v142 - VS 2019 C++ build tools ; Kit SDK windows 10 (10.0.19041.0) ; just in time debugger ; profiling C++ tools ; C++ cmake tools for windows ; C++ ATL for BuildTools V142
- CUDA v11.2.2_461.33_win10
- cuDNN cudnn-11.2-windows-x64-v8.1.1.33
- I then copied the file cusolver64_11.dll as indicated by the guide and renamed the copy cusolver64_10.dll

**Any other info / logs**

Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 15:50:08) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2021-03-13 13:54:50.149216: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
>>> a = tf.Variable(1.)
2021-03-13 13:55:12.083695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-03-13 13:55:12.111014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1779] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s
2021-03-13 13:55:12.111148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-13 13:55:12.610340: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-13 13:55:12.610462: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-13 13:55:12.904250: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-13 13:55:12.936850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-13 13:55:13.147562: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-13 13:55:13.337686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-13 13:55:13.341143: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-13 13:55:13.341313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1917] Adding visible gpu devices: 0
2021-03-13 13:55:13.341889: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-13 13:55:13.343072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1779] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s
2021-03-13 13:55:13.343142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1917] Adding visible gpu devices: 0
2021-03-13 13:55:13.722962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-13 13:55:13.723047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1310]      0
2021-03-13 13:55:13.723426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1323] 0:   N
2021-03-13 13:55:13.724062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21692 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
>>> b = tf.Variable(2.)
>>> c = tf.multiply(a,b)

(tf_nightly_2_5_test2) C:\Users\>"
47777,enh: allow stateless numpy function,"
**System information**
- TensorFlow version (you are using): 2.4
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
`tf.numpy_function` is assumed to be stateful. However, many pure Python functions (as the example with `np.*` in the docs) are actually stateless. `numpy_function` calls `py_func_common` that actually has a 'stateful' argument. Can't we just pass this through?

**Will this change the current api? How?**
Add an argument `stateful` to `numpy_function`. Defaults to `True`, keeps backwards compatibility

**Who will benefit with this feature?**

This allows optimizations in the graph

**Any Other info.**
Is there any reason why this is not passed through currently?"
47776,flow_from_directory doesn't create enough batches,"Hello I have an issue with flow_from_directory to yield enough data.


**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version (use command below): TF 2.5
- Python version: 3.7

- CUDA/cuDNN version: CUDA 11.2
- GPU model and memory: GTX 1660 TI


**Describe the current behavior**
I wanted to yield data with flow_from_directory method. But it didn't work well.

**Describe the expected behavior**
I expected to have enough batches for my training.

**Standalone code to reproduce the issue**
` from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator


import matplotlib.pyplot as plt

import os

new_DS = r'C:\Users\lucas\Projets Code\ConvNet Dogs And Cats\cats_and_dogs_small'
train_folder = os.path.join(new_DS, 'train')
test_folder = os.path.join(new_DS, 'test')


train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40,width_shift_range=0.2,height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
validation_folder = os.path.join(new_DS, 'validation')
test_datagen = ImageDataGenerator(rescale=1./255)


train_generator = train_datagen.flow_from_directory(train_folder,target_size=(150,150),batch_size=32, class_mode='binary')

validation_generator = test_datagen.flow_from_directory(validation_folder, target_size=(150,150),batch_size=32, class_mode='binary')



model = models.Sequential()
model.add(layers.Conv2D(32, (3,3), activation='relu',input_shape=(150,150,3)))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3,3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(128, (3,3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(128, (3,3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Flatten())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(512, activation='relu')) #provides learning features from all the combinations
                                                #of the features of the previous layer
model.add(layers.Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])



history = model.fit(train_generator, steps_per_epoch=100, epochs=100, validation_data=validation_generator,validation_steps=50)
model.save('cats_and_dogs_2.h5')
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1,len(acc)+1)

plt.plot(epochs,acc,'bo', label='Training acc')
plt.plot(epochs, val_acc,'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
 `

**Other info / logs** Include any logs or source code that would be helpful to
Here is what I got in my terminal : 

> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-13 06:51:31.781669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-13 06:51:31.792390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1310]
2021-03-13 06:51:32.680059: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/100
 63/100 [=================>............] - ETA: 22s - loss: 0.7003 - acc: 0.4875WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.
100/100 [==============================] - 44s 418ms/step - loss: 0.6964 - acc: 0.4993 - val_loss: 0.7465 - val_acc: 0.5000
"
47771,tf.keras.layers.Input with ragged=True is not setting uniform outer dimensions,"**System information**
- Have I written custom code: Yes, but this is expected functionality of tf.keras.layers.Input for ragged input
- OS Platform and Distribution: Linux Ubuntu 20 LTS
- TensorFlow installed from: binary
- TensorFlow version: 2.2.0
- Python version: 3.8.5
- Bazel version: N/A
- GCC/Compiler version: N/A
- CUDA/cuDNN version: 11.0
- GPU model and memory: V100 / 32G

**Describe the current behavior**
```
input_tiles = tf.keras.layers.Input(shape=(2, None, 128, 128, 3), dtype=tf.float32, ragged=True)
input_tiles.shape
Out[97]: TensorShape([None, None, None, 128, 128, 3])
```

**Describe the expected behavior**
```
input_tiles = tf.keras.layers.Input(shape=(2, None, 128, 128, 3), dtype=tf.float32, ragged=True)
input_tiles.shape
Out[97]: TensorShape([None, 2, None, 128, 128, 3])
```

More specifically, there should be only 1 ragged axis since there was only 1 dim listed as None in the shape param of tf.keras.layers.Input with ragged=True.

[Tensorflow Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Input)
""A boolean specifying whether the placeholder to be created is ragged. Only one of 'ragged' and 'sparse' can be True. In this case, values of 'None' in the 'shape' argument represent ragged dimensions. For more information about RaggedTensors, see this guide.""

**Standalone code to reproduce the issue**
See above.

"
47770,TF2.5 quietly fails after 'Created TensorFlow device' step,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, build 18363
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): no
- TensorFlow version (use command below): v1.12.1-52710-g601acc3950f 2.5.0-dev20210312
- Python version: 3.9.2
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 11.0.2 / cuDNN v8.0.1 RC2 (June 26th, 2020), for CUDA 11.0
- GPU model and memory: Quadro T1000 (Notebook version) with 4GB vram

**Describe the current behavior**
Importing tensorflow works fine, but using tf 2.5 with my GPU results in the program being terminated with no errors thrown.
Sample output:
```
2021-03-13 00:00:28.101487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-13 00:00:30.597453: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-03-13 00:00:30.644337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1779] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: Quadro T1000 computeCapability: 7.5
coreClock: 1.455GHz coreCount: 14 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s
2021-03-13 00:00:30.654714: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-13 00:00:30.662719: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-13 00:00:30.667912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-13 00:00:30.676216: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-13 00:00:30.683153: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-13 00:00:30.728592: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-13 00:00:30.745117: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-13 00:00:30.767488: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-13 00:00:30.775182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1917] Adding visible gpu devices: 0
2021-03-13 00:00:30.778549: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-13 00:00:30.789455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1779] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: Quadro T1000 computeCapability: 7.5
coreClock: 1.455GHz coreCount: 14 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s
2021-03-13 00:00:30.795660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1917] Adding visible gpu devices: 0
2021-03-13 00:00:31.250426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-13 00:00:31.254948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1310]      0 
2021-03-13 00:00:31.257903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1323] 0:   N 
2021-03-13 00:00:31.262166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2171 MB memory) -> physical GPU (device: 0, name: Quadro T1000, pci bus id: 0000:01:00.0, compute capability: 7.5)
```
**Describe the expected behavior**
Exceution doesn't stop after the ""Created TensorFlow device"" step
**Standalone code to reproduce the issue**
I'm using [this sample](https://www.tensorflow.org/tutorials/quickstart/beginner) that I copy-paste in a .py file
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
See above"
47769,tensorflow.keras.utils.Sequence does not work with TPU,"Paste the self-contained mwe below into a Google Colab instance. It runs fine if hardware acceleration is CPU or GPU but if it is TPU then the following traceback will be produced:
```
   /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1540 convert_to_tensor
        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function
        return constant(v, dtype=dtype, name=name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:265 constant
        allow_broadcast=True)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:283 _constant_impl
        allow_broadcast=allow_broadcast))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py:445 make_tensor_proto
        raise ValueError(""None values not supported."")

    ValueError: None values not supported.
```
I suspect the error has something to do with the `element_spec` attribute which is present on `tensorflow.data.Dataset` objects but not on `tensorflow.keras.utils.Sequence` objects. The logic I have implemented in the `RandomBatches` class is, afaik, not reimplementable using the tensorflow.data API.
```
from os import environ
from tensorflow.config import *
from tensorflow.data import *
from tensorflow.distribute import *
from tensorflow.distribute.cluster_resolver import *
from tensorflow.keras import *
from tensorflow.keras.layers import *
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import *
from tensorflow.keras.utils import Sequence
from tensorflow.tpu.experimental import *
import numpy as np
import tensorflow as tf

def select_strategy():
    gpus = list_physical_devices('GPU')
    tpu_addr = environ.get('COLAB_TPU_ADDR')
    if not tpu_addr:
        dev = '/GPU:0' if gpus else '/CPU:0'
        return OneDeviceStrategy(device = dev)
    resolver = TPUClusterResolver('grpc://' + tpu_addr)
    experimental_connect_to_cluster(resolver)
    initialize_tpu_system(resolver)
    tpus = list_logical_devices('TPU')
    return TPUStrategy(resolver)
def make_dataset(seq, sl, bs):
    def split_input_target(chunk):
        return chunk[:-1], chunk[1:]
    def flatten_window(win):
        return win.batch(sl + 1, drop_remainder = True)
    source = tf.constant(seq, dtype = tf.int32)
    return Dataset.from_tensor_slices(seq) \
                  .window(sl + 1, sl, drop_remainder = True) \
                  .flat_map(flatten_window) \
                  .map(split_input_target) \
                  .batch(bs, drop_remainder = True)
class RandomBatches(Sequence):
    def __init__(self, d, sl, bs):
        self.d = d
        self.sl = sl
        self.bs = bs
        self.batches = self.random_batches()
    def __len__(self):
        return len(self.d) // self.sl // self.bs
    def on_epoch_end(self):
        self.batches = self.random_batches()
    def random_batches(self):
        hi = len(self.d) - self.sl - 1
        return np.random.randint(hi, size = (len(self), self.bs))
    def __getitem__(self, i):
        d, bs, sl = self.d, self.bs, self.sl
        batch = self.batches[i]
        return (np.array([d[s:s + sl] for s in batch]),
                np.array([d[s + 1:s + sl + 1] for s in batch]))

BS, SL = 128, 256
seq = np.random.randint(100, size = 10_000_000)
with select_strategy().scope():
    inp = Input(shape = (None,), batch_size = BS, dtype = tf.int32)
    embedding = Embedding(input_dim = 100, output_dim = 100)
    lstm = LSTM(512, stateful = False, return_sequences = True)
    time_dist = TimeDistributed(Dense(100))
    out = time_dist(lstm(embedding(inp)))
    model = Model(inputs = [inp], outputs = [out])
    loss = SparseCategoricalCrossentropy(from_logits = True)
    opt = RMSprop(learning_rate = 0.004)
    model.compile(optimizer = opt, loss = loss)
    model.summary()
    model.fit(x = RandomBatches(seq, SL, BS), epochs = 100)
    # model.fit(x = make_dataset(seq, SL, BS), epochs = 100)
```"
47765,Converting models with mutable resource variable with v2 TFLiteConveter,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1

### 2. Code

This is how the original model is built:

```python
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import *

def get_student_model():
    base_model = MobileNetV2(weights=None, include_top=False,
                input_shape=(224, 224, 3))
    base_model.trainable = True
    inputs = Input(shape=(224, 224, 3)) 
    x = experimental.preprocessing.Rescaling(1./127.5, offset=-1)(inputs)
    y = base_model(x, training=True)
    y = GlobalAveragePooling2D()(y)
    y = Dense(512, activation=""relu"")(y)
    y = Dropout(0.5)(y)
    outputs = Dense(30, activation='softmax')(y)
    model = tf.keras.Model(inputs, outputs)
    return model
```

This is how the TensorFlow Lite model is generated:

```python
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model(""mobilenetv2_student_no_true_labels_e_125_t_2"")
converter.experimental_new_converter = False
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()
open(""student_mobilenetv2.tflite"", 'wb').write(tflite_model)
print('Model size is %f MBs.' % (len(tflite_model) / 1024 / 1024.0))
```

### 3. Failure after conversion

When trying to convert with `converter.experimental_new_converter = True` the conversion runs into - 

```
ConverterError: <unknown>:0: error: loc(""Conv_1_bn/moving_mean""): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable
```

With tf-nightly (2.5.0-dev20210312) this issue seems to be persisting as well. 

As per https://github.com/tensorflow/tensorflow/issues/43833#issuecomment-744087884, I understand with the MLIR converter this is probably still unsupported. 

_What is more surprising are the outputs of the TFLite models converted with the TOCO converter. I have tried with just the float model i.e. when the conversion is done without any optimization flag set. The results remain fixed._ 

Here's the Colab Notebook that reproduces this issue: https://colab.research.google.com/gist/sayakpaul/2326ccd94e3d33e1c1ced7dfd7f09519/inference_tflite.ipynb. 

Cc: @MeghnaNatraj @abattery "
47764,validate_indices is deprecated and will be removed in a future version. Instructions for updating: The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.?,"I was trying to train a classification model, this is my model

```
text_inputs = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessor = hub.load(""https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"")
tokenize = hub.KerasLayer(preprocessor.tokenize)
tokenized_inputs = [tokenize(text_inputs)]
seq_length = 512
bert_pack_inputs = hub.KerasLayer(
    preprocessor.bert_pack_inputs,
    arguments=dict(seq_length=seq_length))
encoder_inputs = bert_pack_inputs(tokenized_inputs)
encoder = hub.KerasLayer(""https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1"", trainable=True)
encoder_outputs = encoder(encoder_inputs)['pooled_output']
output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(encoder_outputs)
```


```
model = tf.keras.Model(inputs=[text_inputs], outputs=[output_layer])
model.summary()


# class weights
target_labels = y_train.tolist()
class_weights = compute_class_weight(
    ""balanced"", classes=np.unique(target_labels), y=target_labels
)
class_weights = dict(zip(np.unique(target_labels), class_weights))

model.compile(tf.keras.optimizers.Adam(3e-05, epsilon=1e-08, clipnorm=1.0), loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[""acc""])

model.fit(x_train, y_train, batch_size=64, validation_data=(x_valid, y_valid), epochs=10, callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)], class_weight=class_weights)
```


```
WARNING:tensorflow:From /home/intellectfaces/Desktop/Work/apps/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.
Instructions for updating:
The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.
WARNING:tensorflow:From /home/intellectfaces/Desktop/Work/apps/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.
Instructions for updating:
The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.
Epoch 1/10
 740/7930 [=>............................] - ETA: 32:56 - loss: 0.6752 - acc: 0.5807

```

What does these warnings mean? It only appears when I use class_weights option in model.fit, if i remove that argument no warnings. Does this warning cause any issue with what my model is learning, am I not suppose to use class_weights anymore?

Please provide some assistance.


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5 nightly gpu
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.1
- GPU model and memory: 24GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47761,Critical loss of accuracy when converting from MobileNet V2 model to tflite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 1.13.1 (and 2.0.0)




### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:
- Model produces wrong results and/or has lesser accuracy.

I have retrained a Mobilenet_V3_1.0 model using Deeplabcut. When I convert the model to tflite, the accuracy drops to <0.01 whereas the accuracy was previously >0.9. If I do the same, but retrain a resnet_50 model, the tflite conversion works correctly, giving pretty much the same accuracy as the original.

I also found that the accuracy of the tflite model is inversely proportional to the accuracy (or training iteration) of the original retrained model. So the more training iterations and the better the accuracy of the model, the worse the accuracy of the tflite model.

Here's the code I used to convert the graph (which works fine on a model retrained from a resnet base)

video =<path to video>
model_file<path to .pb model file>

cap = cv2.VideoCapture(video)
ret, frame = cap.read()
with tf.io.gfile.GFile(model_file, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

graph = tf.Graph()
with graph.as_default():
	tf.import_graph_def(graph_def, name=""DLC"")

graph.finalize()
op_names = [str(op.name) for op in graph.get_operations()]

output_nodes = [op_names[-1], op_names[-2]]
output_nodes = [on.replace(""DLC/"", """") for on in output_nodes]

converter = tf.lite.TFLiteConverter.from_frozen_graph(
                model_file,
                [""Placeholder""],
                output_nodes,
                input_shapes={""Placeholder"": [1, frame.shape[0], frame.shape[1], 3]},
            )
tflite_model = converter.convert()
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)

And then for calling the tflite model: 
tflite_interpreter = tf.lite.Interpreter(model_content=tflite_model)
tflite_interpreter.allocate_tensors()
inputs = tflite_interpreter.get_input_details()
outputs = tflite_interpreter.get_output_details()

ret, frame = cap.read()
im = np.expand_dims(frame, axis=0).astype(np.float32)
tflite_interpreter.set_tensor(
		        inputs[0][""index""],
		        im
		    )
tflite_interpreter.invoke()

outputs_np = [
		           tflite_interpreter.get_tensor(outputs[0][""index""]),
		           tflite_interpreter.get_tensor(outputs[1][""index""]),
		        ]

In short - I'm not sure why the conversion would work fine for a retrained resnet model and not a mobilenet model? Any ideas?
Thanks!"
47760,Slower and bigger model after int8 quantization,"@tensorflow/micro

**Describe the problem**
I'm running the person_detection example in tensorflow/lite/micro.

When comparing the old uint8 and the new int8 models, the int8 model requires more space as it's using both SOFTMAX and RESHAPE ops. It also seems to be slower than the old uint8 model.

What is the advantage of using the new int8 model?"
47759,Cuda version should be 11 instead of 10 in Install CUDA with apt(docs),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu16.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Just a minor change in documentation in [Install CUDA with apt](https://www.tensorflow.org/install/gpu#install_cuda_with_apt) where CUDA 10.0 needs to be updated to CUDA 11.0.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
47758,estimator guide page  doesn't work!,"![image](https://user-images.githubusercontent.com/45354219/110937207-62fb9800-8375-11eb-8495-01b475df1007.png)
https://www.tensorflow.org/guide/estimators

"
47757,Getting issues while evaluting the object detection model on tensorflow 2.O,"Greetings,

I am trying TF-OD API for object detection. I am getting issues while evaluating the model by given methods in the documentation. It always showing out of memory. Even I tried on the colab. How should I evaluate the model.

Tensorflow Version- 2.4.1
CUDA:- True with 2 GB graphics
Python Version- 3.8 Ananconda
Platform- Windows 10

## URL(s) with the issue:
https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#evaluating-the-model-optional


Thanks for helping out."
47756,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, MUL, PACK, RESHAPE, SHAPE, STRIDED_SLICE, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: Conv3D, Conv3DBackpropInputV2, MaxPool3D.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, MUL, PACK, RESHAPE, SHAPE, STRIDED_SLICE, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: Conv3D, Conv3DBackpropInputV2, MaxPool3D.
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
47755,Cached augmentation in segmentation tutorial - this does not increase dataset size,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/images/segmentation

## Description of issue (what needs changing):

Augmentation, to increase the size of the dataset, has to convert one source datapoint into many augmented datapoints. But in this tutorial, augmentation is applied once to each datapoint - effectively keeping the dataset size the same. The root cause is that augmentation is applied before a `Dataset.cache()`.

These are the relevant lines of the tutorial:

```python
@tf.function
def load_image_train(datapoint):
  input_image = tf.image.resize(datapoint['image'], (128, 128))
  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))

  # This bit is random augmentation - mixed into the load function
  if tf.random.uniform(()) > 0.5:
    input_image = tf.image.flip_left_right(input_image)
    input_mask = tf.image.flip_left_right(input_mask)

  input_image, input_mask = normalize(input_image, input_mask)

  return input_image, input_mask

# Here we load the dataset, including augmentation
train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)

# Then we cache that single round of augmentation, and repeat that single round forever
train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
```

I believe this should look more like (untested):

```python

@tf.function
def load_image_train(datapoint):
  input_image = tf.image.resize(datapoint['image'], (128, 128))
  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))

  input_image, input_mask = normalize(input_image, input_mask)

  return input_image, input_mask

# We can cache here, because the cached dataset is deterministic
train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE).cache()

@tf.function
def random_augment(input_image, input_mask):
  if tf.random.uniform(()) > 0.5:
    input_image = tf.image.flip_left_right(input_image)
    input_mask = tf.image.flip_left_right(input_mask)
  return input_image, input_mask

# Now apply augmentation after caching, getting different results each time
train_dataset = train.map(random_augment).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
```"
47753,I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll,"I run my code on pycharm
include :  import tensorflow as tf

In the run window show me the red line: 
I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll

tf version: tensorflow-2.4.0rc4-cp37-cp37m-win_amd64.whl

How can I fix it
"
47752,Tensorflow Random segmentation faults,"I am trying to run the demo code from official tensorflow [website][1]
I am attaching the full code (copied and arranged) here for ease
```
import tensorflow as tf

# print(""1"")
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import time
import os

# print(""2"")
os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""


# @tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)
    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y, logits)
    return loss_value


# @tf.function
def test_step(x, y):
    val_logits = model(x, training=False)
    val_acc_metric.update_state(y, val_logits)


inputs = keras.Input(shape=(784,), name=""digits"")
x1 = layers.Dense(64, activation=""relu"")(inputs)
x2 = layers.Dense(64, activation=""relu"")(x1)
outputs = layers.Dense(10, name=""predictions"")(x2)
model = keras.Model(inputs=inputs, outputs=outputs)

# Instantiate an optimizer.
optimizer = keras.optimizers.SGD(learning_rate=1e-3)
# Instantiate a loss function.
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()
# Prepare the training dataset.
batch_size = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

# Reserve 10,000 samples for validation.
x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

# Prepare the training dataset.
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

# Prepare the validation dataset.
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.batch(batch_size)

epochs = 2
for epoch in range(epochs):
    print(""\nStart of epoch %d"" % (epoch,))
    start_time = time.time()

    # Iterate over the batches of the dataset.
    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        loss_value = train_step(x_batch_train, y_batch_train)

        # Log every 200 batches.
        if step % 200 == 0:
            print(
                ""Training loss (for one batch) at step %d: %.4f""
                % (step, float(loss_value))
            )
            print(""Seen so far: %d samples"" % ((step + 1) * 64))

    # Display metrics at the end of each epoch.
    train_acc = train_acc_metric.result()
    print(""Training acc over epoch: %.4f"" % (float(train_acc),))

    # Reset training metrics at the end of each epoch
    train_acc_metric.reset_states()

    # Run a validation loop at the end of each epoch.
    for x_batch_val, y_batch_val in val_dataset:
        test_step(x_batch_val, y_batch_val)

    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print(""Validation acc: %.4f"" % (float(val_acc),))
    print(""Time taken: %.2fs"" % (time.time() - start_time))
    print(""end"")
```
Without any reason, this code enters Segmentation Fault in Tensorflow 2.3.1 right at the beginning
```
>python dummy.py 
2021-03-11 17:45:52.231509: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Segmentation fault (core dumped)
```
Interestingly if I put some random print statements at the very start(those ```print(""1"")``` etc statements, the code will execute till the end and suffer segmentation fault at the end(redundant output not shown)
```
Start of epoch 1
Training loss (for one batch) at step 0: 1.0215
Seen so far: 64 samples
Training loss (for one batch) at step 200: 0.9116
Seen so far: 12864 samples
Training loss (for one batch) at step 400: 0.4894
Seen so far: 25664 samples
Training loss (for one batch) at step 600: 0.5636
Seen so far: 38464 samples
Training acc over epoch: 0.8416
Validation acc: 0.8296
Time taken: 3.16s
end
Segmentation fault (core dumped)
```
Another observation is, if I uncomment the `@tf.function` on top of my `trainStep` and `testStep` functions, the code enters into segfault again but after it prints
```Start of epoch 0```

System Specs - 
OS - Ubuntu 14.04
CPU - Intel Core I7
RAM - 64GB
CUDA version - 10.1
CUDNN - 7.6
TF version - 2.3.1(installed via ```pip```)
VRAM - 12GB
GPU - NVIDIA GTX1080TI

Using conda environment with python3.6/python3.7(tried both separately)
Even tried python virtualenv and still got the issue

I thought it might be some issue with my TF2.3 version. Hence I tried everything from TF2.0.0 till TF2.3.2 (CUDA 11 is not supported on Ubuntu 14 and hence can't work with TF2.4.1). I either enter ```Segmentation Fault``` at some place or the other. If not that I get this error.

```*** Error in `python': double free or corruption (!prev): 0x000000000167c080 ***```
I even got libtcmalloc-minimal4 which was suggested for double free error on another thread.

### The code only worked for TF2.0.x without any issue. But I can't use this, as 2.0.x does not support RaggedTensor backprop as mentioned in [this](https://github.com/tensorflow/tensorflow/issues/35802) thread.
`
Can someone explain what is going wrong with my Tensorflow package?




  [1]: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"
47751,Any way to train a TF1 graph in TF2 without using tf.compat.v1.Session?,"Hello, hope yall are staying safe and healthy! Currently my group is migrating from TF 1.8 to TF 2.4.1. Some of our work is dependent on `tensorflow_gan` back when it used to be under `models/research`. We would take the initial graph generated and then train it with our own framework. The files are:

```
checkpoint
model.ckpt.data-00000-of-00001
model.ckpt.meta
model.ckpt.index
```

And below was our TF 1.8 code that would train the GAN. I apologize that due to policy, I cannot copy and paste, but this is just to have an idea:

```
tf.train.import_meta_graph(graph_path + .meta)
graph = tf.get_default_graph()
gen_input = graph.get_tensor_by_name(inputs/gen_input:0)
gen_loss = graph.get_tensor_by_name(losses/truediv:0)
gen_train_op =  graph.get_tensor_by_name(train_ops/generator_train/train_op/control_dependency:0)
gen_output = graph.get_tensor_by_name(Generator/output/add:0)
dis_input = graph.get_tensor_by_name(inputs/x_placeholder:0)

with tf.Session() as sess:
	with tf.variable_scope(tf.get_variable_scope(), reuse=False):
		assert tf.get_variable_scope().reuse == False
	tf.train.Saver().restore(sess, restore_path)

	for iteration in range(max_epochs):
		real_data, fake_data = dataset.get_next_batch()
		_, loss = sess.run([gen_train_op, gen_loss], feed_dict={gen_input:fake_data, dis_input:real_data})
```

We would like to be as close to TF2 and use Keras as much as possible. Is there any way to wrap the tensors from `graph.get_tensor_by_name` or the whole graph into a `tf.Keras.Model`, or must we use `tf.compat.v1.Session`? Wed ideally like to convert this into a `saved_model.pb` that can be imported and used like a `tf.Keras.Model` for additional training, not just inference.

If not, is it possible to combine a loaded graph with a `tf.Keras.Model` for training? For instance, having a model be a backbone to another, and then train on the whole model.

Thanks!


"
47750,Dataset Iterator fails with exception: InvalidArgumentError: Requires delta != 0: 0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8
- CUDA/cuDNN version: 11.0
- GPU model and memory: GTX 980M

**Describe the current behavior**
Dataset iterator fails intermittently for an unknown reason, with error InvalidArgumentError: Requires delta != 0: 0. Sometimes the error will happen after thousands of iterations, other times the error will happen immediately.

**Describe the expected behavior**
The Dataset Iterator should succeed, or at least provide a useful error message describing what the issue is. I have no idea what delta is or why it is 0.

**Standalone code to reproduce the issue**
I can't provide my full pipeline for IP reasons, though this is generally it. I've basically got a bunch of files that I transform, then sample evenly from.

```
def get_balanced_class_repeating_dataset(folder_list, parser):
    num_files_to_shuffle = 20
    # TODO: fix num_samples to use partition multiple once partitions get big enough
    num_samples_to_shuffle = 200  # some multiple of the size of a partition to improve shuffling
    dataset_array = []
    for folder in folder_list:
        file_count = len(folder_list[folder])
        filename_list = tf.data.Dataset.from_tensor_slices(folder_list[folder]).shuffle(file_count).repeat(-1)
        ds = filename_list.interleave(lambda filename: tf.data.TFRecordDataset(filename), cycle_length=num_files_to_shuffle)  # shuffle the files then interleave 10
        ds = ds.shuffle(num_samples_to_shuffle)
        ds = ds.repeat(-1)  # repeat the shuffled dataset, not the other way around
        dataset_array.append(ds)
    dataset = tf.data.experimental.sample_from_datasets(dataset_array, weights=None, seed=None)
    dataset = dataset.map(lambda record: parser(record))
    return dataset
```

    def get_training_dataset(self):
        dataset = get_balanced_class_repeating_dataset(self.train_files, self._parse_tfrecord_train)
        dataset = dataset.batch(self.batch_size)
        return dataset

**Other info / logs** 
Stack Trace
```
Traceback (most recent call last):
  File ""C:\Users\lukeb\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\eager\context.py"", line 2113, in execution_mode
    yield
  File ""C:\Users\lukeb\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 730, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
  File ""C:\Users\lukeb\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py"", line 2578, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""C:\Users\lukeb\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Requires delta != 0: 0
	 [[{{node range}}]] [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/health-AI/python/cardiac/v03/train_model.py"", line 31, in <module>
    model_trainer.train()
  File ""C:\health-AI\python\cardiac\v03\trainers\v1\classifier_trainer_v1.py"", line 32, in train
    for train_step, (data, labels) in enumerate(train_ds):
  File ""C:\Users\lukeb\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 747, in __next__
    return self._next_internal()
  File ""C:\Users\lukeb\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 739, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""C:\Users\lukeb\Anaconda3\envs\tf2\lib\contextlib.py"", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File ""C:\Users\lukeb\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\eager\context.py"", line 2116, in execution_mode
    executor_new.wait()
  File ""C:\Users\lukeb\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\eager\executor.py"", line 69, in wait
    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Requires delta != 0: 0
	 [[{{node range}}]]

Process finished with exit code 1
```
"
47748,C API gradient tape support,"**System information**
- TensorFlow version: 2.4.1
- Are you willing to contribute it: Probably not


**Describe the feature and the current behavior/state.**

Gradients are being ported to C++, with the tape API in [c/eager/gradients.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/gradients.h).

However, there is no equivalent C API, and as per https://github.com/tensorflow/community/pull/335 it does not seem there are current plans to add one.

I assume this is on the radar somewhere, but I don't see any tracking issues, so I'm making this as tracking (and a request).

**Will this change the current api? How?**

It will expose the existing gradient tape api to the C API.  Gradient registration should be exposed as well to allow for registering custom gradients (it seems to be tape specific?).

**Who will benefit with this feature?**

It will enable eager mode to support gradients for projects using the C API bindings, including the [official JVM bindings](https://github.com/tensorflow/java) (which is what prompted this issue) and those that depend on it."
47746,Failed to convert QAT model to tflite when I use tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,"1. System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
TensorFlow installation (pip package or built from source):pip package
TensorFlow library (version, if pip package or github SHA, if built from source):2.3.1
2. Code

Provide code to help us reproduce your issues using one of the following options:
When I use tf.lite.TFLiteConverter to convert keras ACTIVATIONS_INT16_WEIGHTS_INT8 QAT model to tflite fileThere are some bugs and the conversion is failed.
This is my convert code and Bugs.
quant_converter1 = tf.lite.TFLiteConverter.from_keras_model(model1_quant)
quant_converter1.optimizations = [tf.lite.Optimize.DEFAULT]
quant_converter1.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
quant_tflite_model1 = quant_converter1.convert()
with open(model1_tflite_quant, 'wb') as f:
f.write(quant_tflite_model1)
as
**I updated the TensorFlow to the latest stable version v2.4.1 and also face the same error** @amahendrakar
![image](https://user-images.githubusercontent.com/12438262/110884962-e10c6000-82dd-11eb-9659-af72df2f8edd.png)

"
47744,undeclared includion error when building libtensorflowlite.so,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
macOS Big Sur Version 11.2.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
master branch af4b0dfbb251cae9f6407c851cfd5bc11a172b3b
- Python version:
3.8
- Installed using virtualenv? pip? conda?:
conda
- Bazel version (if compiling from source):
3.7.2
- GCC/Compiler version (if compiling from source):
Apple clang version 12.0.0 (clang-1200.0.32.29) 
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
When compiling libtensorflowlite.so with //tensorflow/lite/delegates/flex::delegate as dependency, build fail with undeclared includions in error.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build \
//tensorflow/lite:libtensorflowlite.so \
--config=monolithic \
--cxxopt='--std=c++14' \
-c opt \

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=112
INFO: Reading rc options for 'build' from /Users/richardyao/Code/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/richardyao/Code/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=@org_tensorflow//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=@org_tensorflow//third_party/toolchains/java:tf_java_toolchain --//tensorflow/core/kernels/mlir_generated:enable_gpu=false --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /Users/richardyao/Code/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/richardyao/opt/miniconda3/bin/python3 --action_env PYTHON_LIB_PATH=/Users/richardyao/opt/miniconda3/lib/python3.8/site-packages --python_path=/Users/richardyao/opt/miniconda3/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /Users/richardyao/Code/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/richardyao/Code/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /Users/richardyao/Code/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:monolithic in file /Users/richardyao/Code/tensorflow/.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:macos in file /Users/richardyao/Code/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14
INFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (3 packages loaded, 158 targets configured).
INFO: Found 1 target...
ERROR: /Users/richardyao/Code/tensorflow/tensorflow/compiler/mlir/xla/BUILD:262:11: undeclared inclusion(s) in rule '//tensorflow/compiler/mlir/xla:hlo_utils':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/mlir/xla/hlo_utils.cc':
  'bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'
Target //tensorflow/lite:libtensorflowlite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1429.033s, Critical Path: 828.11s
INFO: 1071 processes: 13 internal, 1058 local.
FAILED: Build did NOT complete successfully


"
47742,"Error while using tensorflow==2.4.1  #tensorflow-probability==0.12.1, for the code that works with TF1.15 and TFP 0.8.0","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, referring to https://github.com/zhulingchen/tfp-tutorial
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- TensorFlow version (use command below):v1.15.4-39-g3db52be 1.15.5 (code works fine with TF15)
- Python version:3.6
- CUDA/cuDNN version:11
- GPU model and memory:12GB

**Describe the current behavior**
Learning TFP. the Code works fine with TF15 and TFP0.8, but error while using with TF2.4 and TFP0.9

Note: same code works in TF2.4 by adding below lines
`import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()`
But would not like to use this, since I would like to use the improvements made in TF2.4, including tf.distribute.MirroredStrategy

**Describe the expected behavior**
The TFP code should work with TF2.4, please guide me to make changes 

**Standalone code to reproduce the issue**
https://github.com/rrklearn2020/learn_tfp.git

**Other info / logs** Include any logs or source code that would be helpful to
-```
-------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-24-3cf063f5b2a9> in <module>
    103                               validation_data=(x_test, y_test),
    104                               shuffle=True,
--> 105                               callbacks=callbacks)
    106                 else:
    107                     print('Using real-time data augmentation.')

............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1097                 batch_size=batch_size,
   1098                 _r=1):
-> 1099               callbacks.on_train_batch_begin(step)
   1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:

............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_begin(self, batch, logs)
    442     """"""
    443     if self._should_call_train_batch_hooks:
--> 444       self._call_batch_hook(ModeKeys.TRAIN, 'begin', batch, logs=logs)
    445 
    446   def on_train_batch_end(self, batch, logs=None):

............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)
    292 
    293     if hook == 'begin':
--> 294       self._call_batch_begin_hook(mode, batch, logs)
    295     elif hook == 'end':
    296       self._call_batch_end_hook(mode, batch, logs)

............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_begin_hook(self, mode, batch, logs)
    301     """"""Helper function for `on_*_batch_begin` methods.""""""
    302     hook_name = 'on_{mode}_batch_begin'.format(mode=mode)
--> 303     self._call_batch_hook_helper(hook_name, batch, logs)
    304 
    305     if self._check_timing:

............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs)
    358         if numpy_logs is None:  # Only convert once.
    359           numpy_logs = tf_utils.to_numpy_or_python_type(logs)
--> 360         hook(batch, numpy_logs)
    361 
    362     if self._check_timing:

............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_begin(self, batch, logs)
    690     """"""
    691     # For backwards compatibility.
--> 692     self.on_batch_begin(batch, logs=logs)
    693 
    694   @doc_controls.for_subclass_implementers

<ipython-input-8-f80d1df575fd> in on_batch_begin(self, batch, logs)
      8     def on_batch_begin(self, batch, logs=None):
      9         if self.update_per_batch:
---> 10             n_batch_per_epoch = int(np.ceil(self.params['samples'] / self.params['batch_size']))
     11             idx_total_batch = (self.epoch - self.n_silent_epoch) * n_batch_per_epoch + batch + 1
     12             kl_weight = (idx_total_batch / n_batch_per_epoch) / self.n_annealing_epoch

KeyError: 'samples'




```"
47741,What is the supported sequence length of bert-tiny and bert-small?,"Here are few bert models I came across recently but couldn't figure out the maximum supported sequence length, Is it 512 tokens per sentence?

Does these models have the same 512 token limit as the original bert-base model


```
L=2 | 2/128 (BERT-Tiny) | 2/256 | 2/512 | 2/768
L=4 | 4/128 | 4/256 (BERT-Mini) | 4/512 (BERT-Small) | 4/768

```
"
47739,The code does not work,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
47738,Change a1825c95 breaks TFLite for Raspberry Pi,"When compiling the TensorFlow Lite Python wheel for Raspberry Pi (as described on https://www.tensorflow.org/lite/guide/build_cmake_pip), the result throws an exception when I try to use it:

~~~~
Traceback (most recent call last):
  File "".../venv/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 45, in <module>
    from tensorflow.lite.python import metrics_portable as metrics
ModuleNotFoundError: No module named 'tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  ...
    import tflite_runtime.interpreter as tflite
  File "".../venv/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 47, in <module>
    from tensorflow.lite.python import metrics_nonportable as metrics
ModuleNotFoundError: No module named 'tensorflow'
~~~~

The offending lines were added in change a1825c95, in the file `tensorflow/lite/python/interpreter.py`:

~~~~
diff --git a/tensorflow/lite/python/interpreter.py b/tensorflow/lite/python/interpreter.py
index f7ef3b34ba6..5c5898b6d4d 100644
--- a/tensorflow/lite/python/interpreter.py
+++ b/tensorflow/lite/python/interpreter.py
@@ -40,6 +40,13 @@ else:
     return lambda x: x
 
 
+try:
+  from tensorflow.lite.python import metrics_portable as metrics
+except ImportError:
+  from tensorflow.lite.python import metrics_nonportable as metrics
+# pylint: enable=g-import-not-at-top
+
+
 class Delegate(object):
   """"""Python wrapper class to manage TfLiteDelegate objects.
 
@@ -321,6 +328,9 @@ class Interpreter(object):
             delegate._get_native_delegate_pointer())  # pylint: disable=protected-access
     self._signature_defs = self.get_signature_list()
 
+    self._metrics = metrics.TFLiteMetrics()
+    self._metrics.increase_counter_interpreter_creation()
+
   def __del__(self):
     # Must make sure the interpreter is destroyed before things that
     # are used by it like the delegates. NOTE this only works on CPython
~~~~

I removed these lines from the copy of `interpreter.py` after installing the wheel and the rest of the code works fine.

It appears that these metrics are needed for unit testing, but something needs to be changed so they are not used when the TFLite package is run a system where TensorFlow itself is not present (e.g. a small platform like Raspberry Pi)."
47737,Building TFLite Python wheel consumes excessive memory,"I am building the TFLite runtime using the instructions described at https://www.tensorflow.org/lite/guide/build_cmake_pip

When I run this (in a Linux VM with 8GB RAM and 8GB swap), the build process consumes all memory and crashes.  There are hundreds of instances of the C compiler running at once, which is causing the problem.

It appears that there are two places where a raw `-j` (without any number of jobs) is passed to Make, causing it to spawn parallel builds without limit.

I made the following changes locally to fix it:

~~~~
$ git diff
diff --git a/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh b/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh
index 832106ec7a8..75adb911078 100755
--- a/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh
+++ b/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh
@@ -113,7 +113,7 @@ case ""${TENSORFLOW_TARGET}"" in
     ;;
 esac
 
-cmake --build . --verbose -j -t _pywrap_tensorflow_interpreter_wrapper
+cmake --build . --verbose -t _pywrap_tensorflow_interpreter_wrapper
 cd ""${BUILD_DIR}""
 
 case ""${TENSORFLOW_TARGET}"" in
diff --git a/tensorflow/lite/tools/pip_package/setup.py b/tensorflow/lite/tools/pip_package/setup.py
index a85053b1602..e0ab72cd82a 100644
--- a/tensorflow/lite/tools/pip_package/setup.py
+++ b/tensorflow/lite/tools/pip_package/setup.py
@@ -82,7 +82,7 @@ def make_args(target='', quiet=True):
   args = ([
       'make', 'SHELL=/bin/bash', 'BUILD_WITH_NNAPI=false', '-C', TENSORFLOW_DIR
   ] + MAKE_CROSS_OPTIONS +
-          ['-f', RELATIVE_MAKEFILE_PATH, '-j',
+          ['-f', RELATIVE_MAKEFILE_PATH, # '-j',
            str(get_build_cpus())])
   if quiet:
     args.append('--quiet')
diff --git a/tensorflow/tools/ci_build/devtoolset/build_devtoolset.sh b/tensorflow/tools/ci_build/devtoolset/build_devtoolset.sh
index f56f834743c..b79076f52a9 100755
--- a/tensorflow/tools/ci_build/devtoolset/build_devtoolset.sh
+++ b/tensorflow/tools/ci_build/devtoolset/build_devtoolset.sh
@@ -114,7 +114,7 @@ cd ""${TARGET}-build""
       --with-linker-hash-style=""gnu"" \
       --with-tune=""generic"" \
       && \
-    make -j 42 && \
+    make -j 16 && \
     make install
 
 # Create the devtoolset libstdc++ linkerscript that links dynamically against
diff --git a/tensorflow/tools/ci_build/linux/cmake/run.sh b/tensorflow/tools/ci_build/linux/cmake/run.sh
index d9bf4f01b5d..52c6bd5daa6 100755
--- a/tensorflow/tools/ci_build/linux/cmake/run.sh
+++ b/tensorflow/tools/ci_build/linux/cmake/run.sh
@@ -29,7 +29,7 @@ pushd build
 cmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake
 # When building do not use all CPUs due to jobs running out of memory.
 # TODO(gunan): Figure out why we run out of memory in large GCE instances.
-make --jobs 20 tf_python_build_pip_package
+make tf_python_build_pip_package
 
 virtualenv cmake_test --system-site-packages
 source cmake_test/bin/activate
@@ -44,7 +44,7 @@ mv $WHEEL_FILE_PATH $FIXED_WHEEL_PATH
 pip install --upgrade $FIXED_WHEEL_PATH
 
 # Run all tests.
-ctest -C Release --output-on-failure -j
+ctest -C Release --output-on-failure
 
 # Finalize and go back to the initial directory.
 deactivate
~~~~

Please consider incorporating these changes.  Especially the changes to `build_pip_package_with_cmake.sh` and `pip_package/setup.py`, which I believe are the files directly responsible for the problem.  With these changes, the Python wheel compiles fine and the system's memory usage remains within normal sizes."
47736,I cannot predict with f.data.experimental.bucket_by_sequence_length,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA v11.0, cuDNN 8
- GPU model and memory: Nvidia Quadro P2000 64 GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I cannot predict with f.data.experimental.bucket_by_sequence_length.



**Describe the expected behavior**

I don't know if there is a bug in my code or there is a bug in the implementation. How come training with dataset using tf.data.experimental.bucket_by_sequence_length is ok, but predicting is not ok? My error has to do with incorrect dimension but isn't that what tf.data.experimental.bucket_by_sequence_length  and assigning shaped of (None, X) are used for?

**Standalone code to reproduce the issue**

```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

def gen_train():    
         for x, y in zip(X_train, y_train):        
                yield (x, y)
def gen_val():    
         for x, y in zip(X_test, y_test):        
         yield (x, y)

X_train_to_dataset = tf.data.Dataset.from_generator(lambda: X_train, output_types=(tf.float64), output_shapes=(tf.TensorShape([None, len(X_train[0][0])])))

train_data = tf.data.Dataset.from_generator(gen_train, output_types=(tf.float64,
tf.float64),output_shapes(tf.TensorShape([None, len(X_train[0][0])]),  tf.TensorShape([None, len(y_train[0][0])])))

train_data = tf.data.Dataset.zip((train_data.map(lambda c, d: c), train_data.map(lambda a, b: tf.expand_dims(b[:, 1], axis=1))))

val_data = tf.data.Dataset.from_generator(gen_val, output_types=(tf.float64, tf.float64),  output_shapes=(tf.TensorShape([None, len(X_train[0][0])]), tf.TensorShape([None, len(y_train[0][0])])))

val_data = tf.data.Dataset.zip((val_data.map(lambda c, d: c), val_data.map(lambda a, b: tf.expand_dims(b[:, 1], axis=1))))

def element_length_fn(x, y):    
         return tf.shape(x)[0]

max_train = len(max(X_train, key=len))
min_train = len(min(X_train, key=len))
max_val = len(max(X_test, key=len))
min_val = len(min(X_test, key=len))
batch_size = 64
bucket_boundaries_train = list(np.arange(min_train, max_train, 10)) 
bucket_batch_sizes_train = [batch_size] * (len(bucket_boundaries_train) + 1)
bucket_boundaries_val = list(np.arange(min_val, max_val, 10)) 
bucket_batch_sizes_val = [batch_size] * (len(bucket_boundaries_val) + 1)

train_data = train_data.apply(tf.data.experimental.bucket_by_sequence_length(
              element_length_func=element_length_fn,
              bucket_batch_sizes=bucket_batch_sizes_train,
              bucket_boundaries=bucket_boundaries_train))

val_data = val_data.apply(tf.data.experimental.bucket_by_sequence_length(
              element_length_func=element_length_fn,
              bucket_batch_sizes=bucket_batch_sizes_val,
              bucket_boundaries=bucket_boundaries_val,)

def create_model():    
     i = Input(shape=(None, len(X_train[0][0])))
     x = Bidirectional(LSTM(128, return_sequences=True))(i)    
     x = TimeDistributed(Dense(1))(x)    
     return tf.keras.Model(inputs=i, outputs=x)

model = create_model()    
model.compile(loss='mse',optimizer='adam',metrics=[metrics.RootMeanSquaredError()])
model.fit(train_data,
         epochs=5,
         validation_data=val_data)

NormLayer = preprocessing.Normalization()
NormLayer.adapt(X_train_to_dataset)
model = create_model_bdlstm128(NormLayer)

model.compile(loss='mse',optimizer='adam',metrics=[metrics.RootMeanSquaredError()])

model.fit(train_data, epochs=5000, validation_data=val_data)

y_pred = model.predict(val_data.map(lambda a, b: a)) 


```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The below is the error for ```y_pred = model.predict(val_data.map(lambda a, b: a)) ```:

```
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""C:\Users\ipancorbo\Anaconda3\envs\tf_recsys\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1646, in predict
    all_outputs = nest.map_structure_up_to(batch_outputs, concat, outputs)
  File ""C:\Users\ipancorbo\Anaconda3\envs\tf_recsys\lib\site-packages\tensorflow\python\util\nest.py"", line 1159, in map_structure_up_to
    return map_structure_with_tuple_paths_up_to(
  File ""C:\Users\ipancorbo\Anaconda3\envs\tf_recsys\lib\site-packages\tensorflow\python\util\nest.py"", line 1257, in map_structure_with_tuple_paths_up_to
    results = [
  File ""C:\Users\ipancorbo\Anaconda3\envs\tf_recsys\lib\site-packages\tensorflow\python\util\nest.py"", line 1258, in <listcomp>
    func(*args, **kwargs) for args in zip(flat_path_gen, *flat_value_gen)
  File ""C:\Users\ipancorbo\Anaconda3\envs\tf_recsys\lib\site-packages\tensorflow\python\util\nest.py"", line 1161, in <lambda>
    lambda _, *values: func(*values),  # Discards the path arg.
  File ""C:\Users\ipancorbo\Anaconda3\envs\tf_recsys\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 2716, in concat
    return array_ops.concat(tensors, axis=axis)
  File ""C:\Users\ipancorbo\Anaconda3\envs\tf_recsys\lib\site-packages\tensorflow\python\util\dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""C:\Users\ipancorbo\Anaconda3\envs\tf_recsys\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1677, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File ""C:\Users\ipancorbo\Anaconda3\envs\tf_recsys\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 1192, in concat_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""C:\Users\ipancorbo\Anaconda3\envs\tf_recsys\lib\site-packages\tensorflow\python\framework\ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [64,29,1] vs. shape[3] = [64,30,1] [Op:ConcatV2] name: concat

```"
47729,keras.io List of topics on the right cannot be navigated,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue: 
https://keras.io/guides/training_with_built_in_methods/
It is an example. 
This issue applies to the whole site

## Description of issue (what needs changing): 
List of topics on the right cannot be navigated at keras.io

### Clear description
When navigating docs at keras.io when the list of topics on the right (LOTOR) is longer than the browser height you cannot touch topics in the bottom. There is no scroll on the page, so the topics are un reachable.

### Suggested solution
LOTOR is fixed in position now 
LOTOR should have a scroll bar or move with all the page 

Many thanks"
47728,fit_generator is calling __getitem__ with id=0 twice in the first epoch,"**System information**
- Have I written custom code : Yes
- OS Platform and Distribution: MacOS 11.2.2
- TensorFlow installed from (source or binary): Installed with pip
- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.7.4

**Describe the current behavior**
When using a generator and `fit_generator` (or `fit` for the case) method, the first time `__getitem__` is called, repeats the idx = 0, which produces that the generator either sends the same batch twice or have errors since `on_epoch_end` is not called on time.

**Describe the expected behavior**
__getitem__ should always be called with incremental indexes as argument. 

**Standalone code to reproduce the issue**

```
from tensorflow.keras.utils import Sequence
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
import numpy as np

class DataGenerator(Sequence):
    def __init__(self, batch_size = 512, shape = (10,)):
        self.shape = shape
        self.batch_size = batch_size

    def on_epoch_end(self):
        # do nothing
        pass

    def __getitem__(self, idx):
        print(""[+] Idx: %d"" % idx)
        y = np.ones((10,2))
        y[:,0] = 0
        return np.random.random((10,10)), y

    def __len__(self):
        return 7

inp = Input(shape = (10,), dtype = np.float)
x = Dense(10, activation = 'relu')(inp)
out = Dense(2, activation='softmax')(x)
model = Model(inputs=inp, outputs= out)

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

generator = DataGenerator()

model.fit_generator(generator, epochs = 2, shuffle = False, workers = 0)
```
The output of the above code is: 
```
[+] Idx: 0
Epoch 1/10
[+] Idx: 0
1/7 [===>..........................] - ETA: 1s - loss: 0.7483 - accuracy: 0.3000[+] Idx: 1
[+] Idx: 2
[+] Idx: 3
[+] Idx: 4
[+] Idx: 5
[+] Idx: 6
7/7 [==============================] - 0s 1ms/step - loss: 0.7374 - accuracy: 0.4012
Epoch 2/10
[+] Idx: 0
1/7 [===>..........................] - ETA: 0s - loss: 0.6861 - accuracy: 0.6000[+] Idx: 1
[+] Idx: 2
[+] Idx: 3
[+] Idx: 4
[+] Idx: 5
[+] Idx: 6
```
When it shouldn't  call` __getitem__` twice with `Idx: 0`
"
47725,Keras tuner - Incorrect parameters for model training,"URL with the issue: 
https://www.tensorflow.org/tutorials/keras/keras_tuner#instantiate_the_tuner_and_perform_hypertuning

Description of issue (what needs changing):

On the second to last code cell where the hypermodel  is re-instantiated and trained with the optimal number of epochs, the test set is incorrectly used for the fit process. The train set should be used for this process, since we then proceed to evaluate the model on the test set in the final code block.

Is the link to the source code correct?

Yes

Are all parameters defined and formatted correctly?

Yes

Are return values defined?

Yes

Are you planning to also submit a pull request to fix the issue?

Yes, here: https://github.com/tensorflow/docs/pull/1844"
47724,GPU and CPU gradients diverge in TF 2.4 for approximate gelu activation,"Hi,

Upon upgrading from TF 2.2 to TF 2.4, my team noticed a problem with an approximate version of `gelu` shown below as `gelu_approximate`, which exists also in TF v2.4 as `tf.nn.gelu(approximate=True)`. The gradients for `gelu_approximate` calculated on the GPU diverge from the gradients on the CPU as the input gets farther from 0. Since the implementation of `gelu_approximate` consists of primitives `tanh`, `pow`, etc, we should make sure something more fundamental isn't broken in TF. Please see the following reproduce code, which prints `""GPU and CPU gradients are not close!""` for v2.4 but not for v2.2.

```python
# gelu_problem.py
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
 

def gelu_approximate(x):
    # Copied and inlined from tf.nn.gelu(approximate=True) which exists in TF v2.4 but not TF v2.2
    return 0.5 * x * (1.0 + tf.tanh(0.7978845608028654 * (x + 0.044715 * tf.pow(x, 3))))


def gelu_gradient(x, device):
    with tf.GradientTape() as tape:
        tape.watch(x)
        with tf.device(device):
            y = gelu_approximate(x)
    return tape.gradient(y, x)
 
 
def main():
    print(f""TF version is {tf.__version__}"")
    x = tf.linspace(-500.0, 500.0, 500)
    cpu = gelu_gradient(x, ""/CPU:0"")
    gpu = gelu_gradient(x, ""/GPU:0"")
    abs_error = np.abs(cpu - gpu)
    df = pd.DataFrame(dict(cpu=cpu, gpu=gpu, abs_error=abs_error), index=x)
    try:
        np.testing.assert_allclose(cpu, gpu, atol=1e-3)
    except (AssertionError,) as e:
        print(""GPU and CPU gradients are not close!"")
        print(e)
    df.plot(title=""CPU vs. GPU gradients for tf.nn.gelu(approximate=True)"")
    plt.show()
 
 
if __name__ == ""__main__"":
    main()
```

Output on our system:

```
$ nvidia-smi
Wed Mar 10 15:49:02 2021      
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:04:00.0 Off |                    0 |
| N/A   39C    P0    42W / 250W |  29877MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:06:00.0 Off |                    0 |
| N/A   40C    P0    38W / 250W |  31045MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:07:00.0 Off |                    0 |
| N/A   40C    P0    37W / 250W |  31045MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:08:00.0 Off |                    0 |
| N/A   41C    P0    38W / 250W |  31045MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                                
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A   4126251      C   .../3/7/x/dist/bin/python3.7    29871MiB |
|    1   N/A  N/A   4126251      C   .../3/7/x/dist/bin/python3.7    31039MiB |
|    2   N/A  N/A   4126251      C   .../3/7/x/dist/bin/python3.7    31039MiB |
|    3   N/A  N/A   4126251      C   .../3/7/x/dist/bin/python3.7    31039MiB |
+-----------------------------------------------------------------------------+
 
$ python -um gelu_problem
2021-03-10 15:48:44.908579: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
TF version is 2.4.0
2021-03-10 15:48:50.149409: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-10 15:48:50.150776: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-10 15:48:50.220817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:04:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2021-03-10 15:48:50.220842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-03-10 15:48:50.224004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-03-10 15:48:50.224038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-03-10 15:48:50.226493: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-10 15:48:50.227454: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-10 15:48:50.229775: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-10 15:48:50.231379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-03-10 15:48:50.235484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-03-10 15:48:50.237107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-10 15:48:50.237501: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-10 15:48:50.239017: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-10 15:48:50.239769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:04:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2021-03-10 15:48:50.239789: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-03-10 15:48:50.239805: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-03-10 15:48:50.239819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-03-10 15:48:50.239832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-10 15:48:50.239845: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-10 15:48:50.239858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-10 15:48:50.239872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-03-10 15:48:50.239885: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-03-10 15:48:50.241280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-10 15:48:50.241307: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-03-10 15:48:51.004798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-10 15:48:51.004834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-03-10 15:48:51.004839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-03-10 15:48:51.010315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1908 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:04:00.0, compute capability: 7.0)
GPU and CPU gradients are not close!
 
Not equal to tolerance rtol=1e-07, atol=0.001
 
Mismatched elements: 466 / 500 (93.2%)
Max absolute difference: 3.1899037
Max relative difference: 1.
 x: array([ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,
        0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,
        0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,...
 y: array([-3.189903e+00, -3.151703e+00, -3.113807e+00, -3.076217e+00,
       -3.038931e+00, -3.001947e+00, -2.965265e+00, -2.928882e+00,
       -2.892799e+00, -2.857012e+00, -2.821522e+00, -2.786328e+00,...
```

![gelu_problem](https://user-images.githubusercontent.com/18267168/110800398-21220300-824a-11eb-85a9-4dfee7af8334.png)

This particular system is running Debian Linux, version 9.13. Python is version 3.7.9, TF is version 2.4.0. CUDNN is 7.6.5 and CUDA is 10.1.

Thank you,

Ben and team"
47723,Still unable to install in python 3.9(64 BIT) in windows 10 ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47722,keras.MultiHeadAttention fails to save / load weights correctly,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes,
```
import tensorflow.keras as tk
import numpy as np

def main(**kwargs):
    inputs = tk.Input(shape=(1,2))
    outputs = tk.layers.MultiHeadAttention(1, 2)(inputs, inputs)
    model = tk.Model(inputs, outputs)
    model.compile(tk.optimizers.Adam(),'MSE')

    #Adding .fit call doesn't change the result
    #data = np.ones((1,1,2))
    #model.fit(x=data, y=data)

    model.save(""bug0"")
    m0 = tk.models.load_model(""bug0"")
    m1 = tk.models.load_model(""bug0"")

    print()
    print(m0.weights[0])
    print(m1.weights[0])


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(e)
```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

```
$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 20.04.2 LTS
Release:        20.04
Codename:       focal

```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):

```
pip install tensorflow
```

- TensorFlow version (use command below):
```
$ python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v2.4.0-49-g85c8b2a817f 2.4.1
```
- Python version:
```
$ python --version
Python 3.8.5
```
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When loading a saved model containing a MultiHeadAttention layer, the layer wights are different every time the model is loaded.

**Describe the expected behavior**

The layer weights should be the same every time the model is loaded.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow.keras as tk
import numpy as np

def main(**kwargs):
    inputs = tk.Input(shape=(1,2))
    outputs = tk.layers.MultiHeadAttention(1, 2)(inputs, inputs)
    model = tk.Model(inputs, outputs)
    model.compile(tk.optimizers.Adam(),'MSE')

    #Adding .fit call doesn't change the result
    #data = np.ones((1,1,2))
    #model.fit(x=data, y=data)

    model.save(""bug0"")
    m0 = tk.models.load_model(""bug0"")
    m1 = tk.models.load_model(""bug0"")

    print()
    print(m0.weights[0])
    print(m1.weights[0])


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(e)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
2021-03-11 07:17:30.470279: W tensorflow/python/util/util.cc:348] Sets are not currently considered\
 sequences, but this may change in the future, so consider avoiding using them.
WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_cond\
itional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_f\
n while saving (showing 5 of 30). These functions will not be directly callable after loading.
WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_cond\
itional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_f\
n while saving (showing 5 of 30). These functions will not be directly callable after loading.

<tf.Variable 'multi_head_attention/query/kernel:0' shape=(2, 1, 2) dtype=float32, numpy=
array([[[ 0.36944628, -0.00638223]],

       [[-0.96410084, -0.4008801 ]]], dtype=float32)>
<tf.Variable 'multi_head_attention/query/kernel:0' shape=(2, 1, 2) dtype=float32, numpy=
array([[[ 0.94459724,  0.45157957]],

       [[ 0.88224053, -0.97082996]]], dtype=float32)>
```
"
47721,TF C-API GPU not found via TF_SessionListDevices,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.4
- CUDA/cuDNN version: 11.0 / 8.0.4
- GPU model and memory: GeForce MX250 2048MB GDDR5



I'm currently working on a CNN deployment via the Tensorflow C-API. When checking for the available devices via **TF_SessionListDevices** only the CPU is listed. I already checked my CUDA installation and verified it on the python side which detects the GPU. Trying to set the device via hex string doesn't fix it.

**Provide the exact sequence of commands / steps that you executed before running into the problem**


    TF_Graph* Graph = TF_NewGraph();
    TF_Status* Status = TF_NewStatus();

    uint8_t config[13] = { 0xa, 0x7, 0xa, 0x3, 0x47, 0x50, 0x55, 0x10, 0x0, 0x10, 0x1, 0x28, 0x1};

    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();
    TF_Buffer* RunOpts = NULL;

    TF_SetConfig(SessionOpts, (void*)config, 13, Status);

    if (TF_GetCode(Status) == TF_OK)
    {
        printf(""TF_SetConfig OK\n"");
    }
    else
    {
        printf(""%s"", TF_Message(Status));
    }

    const char* saved_model_dir = ""\\model"";
    const char* tags = ""serve""; // default model serving tag; can change in future
    int ntags = 1;

    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);
    if (TF_GetCode(Status) == TF_OK)
    {
        printf(""TF_LoadSessionFromSavedModel OK\n"");
    }
    else
    {
        printf(""%s"", TF_Message(Status));
    }


    //Check devices
    TF_DeviceList* dl = TF_SessionListDevices(Session, Status);
    if (TF_GetCode(Status) == TF_OK)
    {
        printf(""TF_SessionListDevices OK\n"");
        int count = TF_DeviceListCount(dl);
        const char* list;
        const char* tpe;
        printf(""DeviceList:\n"");
        for (int i = 0; i < count; i++)
        {
            list = TF_DeviceListName(dl, i, Status);
            tpe = TF_DeviceListType(dl, i, Status);
            printf(list);
            printf(""\n"");
        }


        TF_DeleteDeviceList(dl);
    }
    else
    {
        printf(""%s"", TF_Message(Status));
    }

Expected output should be two list members with type CPU and GPU each, but only the CPU device is listed.

Any help on this issue would be appreciated.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47720,mportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.4
- Python version:3.8.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: no
- GPU model and memory:no



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
G:\anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     63   try:
---> 64     from tensorflow.python._pywrap_tensorflow_internal import *
     65   # This try catch logic is because there is no bazel equivalent for py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-5-d6579f534729> in <module>
----> 1 import tensorflow

G:\anaconda\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

G:\anaconda\lib\site-packages\tensorflow\python\__init__.py in <module>
     37 # go/tf-wildcard-import
     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
---> 39 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     40 
     41 from tensorflow.python.eager import context

G:\anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     81 for some common reasons and solutions.  Include the entire stack trace
     82 above this error message when asking for help."""""" % traceback.format_exc()
---> 83   raise ImportError(msg)
     84 
     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""G:\anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47719,ValueError: Input tensors to a Functional must come from `tf.keras.Input`. Received: 0 (missing previous layer metadata).,"I use tensorflow 2.4 on google colab with a script like the following
```
n = 10
plt.plot(y[:(n * 24)])
#n = len(test_X)
for i in range(0, n):
    avg_pred= predict_with_uncertainty(model, test_X[i].reshape(1, 24, test_X[i].shape[1]), 100)
    
    yHat = scaler.inverse_transform(avg_pred).reshape(avg_pred.shape[0] * avg_pred.shape[1])
    YHat_SD = scaler.inverse_transform(ts_std).reshape(ts_std.shape[0] * ts_std.shape[1])
    y = scaler.inverse_transform(test_y).reshape(prediction.shape[0] * prediction.shape[1])
    plt.plot(range(i * 24, (i + 1) * 24), yHat, color='red')
    # plots the uncertainty to the degree of half a standard deviation
    plt.fill_between(range(i * 24, (i + 1) * 24),
                     yHat + 1.96 * YHat_SD, 
                     yHat - 1.96 * YHat_SD, 
                     facecolor='red', alpha=0.25)

    # plots the uncertainty to the degree of a full standard deviation
    plt.fill_between(range(i * 24, (i + 1) * 24),
                     yHat + 3 * YHat_SD, 
                     yHat - 3 * YHat_SD, 
                     facecolor='red', alpha=0.25)
    plt.legend(['Truth', 'Prediction'], loc='upper right')
plt.show()```

error:
ValueError                                Traceback (most recent call last)
<ipython-input-20-74b3d2442e4c> in <module>()
      3 #n = len(test_X)
      4 for i in range(0, n):
----> 5     avg_pred= predict_with_uncertainty(model, test_X[i].reshape(1, 24, test_X[i].shape[1]), 100)
      6 
      7     yHat = scaler.inverse_transform(avg_pred).reshape(avg_pred.shape[0] * avg_pred.shape[1])

6 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in _validate_graph_inputs_and_outputs(self)
    689                          'must come from `tf.keras.Input`. '
    690                          'Received: ' + str(x) +
--> 691                          ' (missing previous layer metadata).')
    692       # Check that x is an input tensor.
    693       # pylint: disable=protected-access

ValueError: Input tensors to a Functional must come from `tf.keras.Input`. Received: 0 (missing previous layer metadata).

if using this script on tensorflow 1.15.0 it works

the solution that you give me means a lot to me
"
47718,Suppressed warnings in cortex_m_generic_makefile and cortex_m_corstone_300_makefile,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source): 8cb7f3a462db810d70521fab26eac8e2fed9e2b4
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
The files cortex_m_generic_makefile.inc and cortex_m_corstone_300_makefile.inc both need to suppress additional warnings compared to default the Makefile.

**Please provide the exact sequence of commands/steps when you ran into the problem**
For example remove OMIT_ERRORS from the mentioned files and try to build.
make -j -f tensorflow/lite/micro/tools/make/Makefile  TARGET=cortex_m_corstone_300 TARGET_ARCH=cortex-m55 test_kernel_conv_test 


"
47717,Feeding a batch of different image shapes for a model with dynamic input shape,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 18
- TensorFlow installed from binary
- TensorFlow version: 1.15
- Python version: 3.7
- Running on CPU.

**Describe the current behavior**
I am working on a model with a flexible input shape (None, None,3). That is, the images can be of any size but the number of channels should be 3. The model is compatible with handling dynamic shapes. As you can see in the following:
```Python

arr1 = np.random.random((100, 100, 3))
arr2 = np.random.random((104, 100, 3))
arr3 = np.random.random((200, 240, 3))

out = model.predict(np.expand_dims(arr1, 0))
print(out.shape)

out = model.predict(np.expand_dims(arr2, 0))
print(out.shape)

out = model.predict(np.expand_dims(arr3, 0))
print(out.shape)
```
And the output of this section is:
```Python
(1, 100, 100, 3)
(1, 104, 100, 3)
(1, 200, 240, 3)
```
However, this is a single image inference. When we have batches (e.g. when training with batch_size > 1), the batch will have a shape of (1, ) since NumPy cannot handle an array of arrays with different shapes. And as a result, I cannot feed the model batches of images. Shouldn't there be a way to feed a batch of images with different shapes to model?

This is my result on feeding a batch:
```Python

batch = [arr1, arr1, arr1]
out = model.predict(batch)
print(out.shape)

batch = [arr2, arr2, arr2]
out = model.predict(batch)
print(out.shape)


batch = [arr1, arr2] #this one generates the error
out = model.predict(batch)
print(out.shape)
```
And the result is:
```Python
(3, 100, 100, 3)
(3, 104, 100, 3)
Traceback (most recent call last):
  File ""flexnet.py"", line 57, in <module>
    out = model.predict(batch)
  File ""/home/amin/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 908, in predict
    use_multiprocessing=use_multiprocessing)
  File ""/home/amin/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 716, in predict
    x, check_steps=True, steps_name='steps', steps=steps)
  File ""/home/amin/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2471, in _standardize_user_data
    exception_prefix='input')
  File ""/home/amin/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py"", line 563, in standardize_input_data
    'with shape ' + str(data_shape))
ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (2, 1)
```

"
47716,KeyError: 'custom_node:0' when loading saved model contains a custom node that already implement via register_ops.,"HI tensorflower.

I want to load a saved model with a custom op that's not support by tensorflow, so I write a dummy op register to tensorflow with the guide of https://www.tensorflow.org/guide/create_op, and the registration is succeed but I still get keyerror while parsing saved_model, here are some code snippets:
```
import tensorflow as tf
print(tf.__version__)

lsa_module = tf.load_op_library('./LinearSumAssignmentOp.so')

path = ""/path/to/saved/model""
imported = tf.saved_model.load(path)
```
I get the following outputs
```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-2-45f6f99eff8a> in <module>
      2 
      3 path = ""./to_nv_1/saved_models""
----> 4 imported = tf.saved_model.load(path)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py in load(export_dir, tags, options)
    869     ValueError: If `tags` don't match a MetaGraph in the SavedModel.
    870   """"""
--> 871   return load_internal(export_dir, tags, options)[""root""]
    872 
    873 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, options, loader_cls, filters)
    900       try:
    901         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,
--> 902                             ckpt_options, filters)
    903       except errors.NotFoundError as err:
    904         raise FileNotFoundError(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py in __init__(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, filters)
    130     self._concrete_functions = (
    131         function_deserialization.load_function_def_library(
--> 132             meta_graph.graph_def.library))
    133     self._checkpoint_options = ckpt_options
    134 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py in load_function_def_library(library, load_shared_name_suffix)
    356     # import).
    357     with graph.as_default():
--> 358       func_graph = function_def_lib.function_def_to_graph(copy)
    359     _restore_gradient_functions(func_graph, renamed_functions)
    360 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function_def_to_graph.py in function_def_to_graph(fdef, input_shapes)
     62       input_shapes = input_shapes_attr.list.shape
     63   graph_def, nested_to_flat_tensor_name = function_def_to_graph_def(
---> 64       fdef, input_shapes)
     65 
     66   with func_graph.as_default():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function_def_to_graph.py in function_def_to_graph_def(fdef, input_shapes)
    258   for node_def in graph_def.node:
    259     for i in range(len(node_def.input)):
--> 260       node_def.input[i] = nested_to_flat_tensor_name[node_def.input[i]]
    261 
    262   return graph_def, nested_to_flat_tensor_name

KeyError: 'lane_local_mapper/lane_associator/cond/LinearSumAssignmentOp:matches:0'
```

Any idea about this error? I don't know the input and output shape of this node, so the REGISTER_OP .SetShapeFn likely deduce an error output shape, does it cause the error?

Thank && Best Regard."
47715,TF Lite GPU Delegate produces weird output,"## System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.5 LTS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Samsung Galaxy S10(Exynos)**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below):**tags/v2.4.1**
- Python version: **3.6.9 / 2.7.17 both**
- Bazel version (if compiling from source): **3.1.0**
- GCC/Compiler version (if compiling from source): **gcc version 9.3.0 (Ubuntu 9.3.0-11ubuntu0~18.04.1)**
- GPU model and memory: **Exynos**

## Model Info

### test_model_512.tflite

![testmodel512](https://user-images.githubusercontent.com/39717352/110747329-91984680-8281-11eb-8d42-9529fb9eda2a.png)

### test_model_4096.tflite

![testmodel4096](https://user-images.githubusercontent.com/39717352/110747341-952bcd80-8281-11eb-8820-a45acd77fe95.png)

### proof_model_512.tflite

![proofmodel512](https://user-images.githubusercontent.com/39717352/110747356-9a891800-8281-11eb-9340-e45fe6b17b4f.png)

### proof_model_4096.tflite

![proofmodel4096](https://user-images.githubusercontent.com/39717352/110747369-9e1c9f00-8281-11eb-8b14-27582b89581d.png)

## Symptom

Hello Tensorflow.
I'm trying to use Tensorflow Lite GPU Delegate to accelerate my model on GPU.
But it produces weird output, refer below test result.

I think that GPU Delegate frees memory address of `output_1` tensor after `Add`(B=-0.125).
Because the `proof_model`s that are added `Add`(B=0) operation between `Tanh` and `output_1` does not produce weird outputs.

Would you please check this?

## Test results

### test_model_4096.tflite

```
$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m test_model_4096.tflite 

INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
[output_1]
Output #0: element #1: CPU value - 0.686587, GPU value - 0, abs diff - 0.686587
Output #0: element #2: CPU value - 0.720795, GPU value - 0, abs diff - 0.720795
Output #0: element #3: CPU value - 0.140191, GPU value - 0, abs diff - 0.140191
Output #0: element #4: CPU value - -0.63919, GPU value - 0, abs diff - 0.63919
Output #0: element #5: CPU value - -0.743797, GPU value - 0, abs diff - 0.743797
Output #0: element #6: CPU value - -0.272364, GPU value - 0, abs diff - 0.272364
Output #0: element #7: CPU value - 0.576355, GPU value - 0, abs diff - 0.576355
Output #0: element #8: CPU value - 0.757089, GPU value - 0, abs diff - 0.757089
Output #0: element #9: CPU value - 0.39027, GPU value - 0, abs diff - 0.39027
Output #0: element #10: CPU value - -0.496026, GPU value - 0, abs diff - 0.496026
Printed 10 different elements, threshhold - 0.0001, next different elements skipped
Total 4092 different elements, for output #0, threshhold - 0.0001
[output_2]
Output #1: element #5: CPU value - -0.868797, GPU value - -0.868652, abs diff - 0.000144362
Output #1: element #6: CPU value - -0.397364, GPU value - -0.397217, abs diff - 0.000147194
Output #1: element #7: CPU value - 0.451355, GPU value - 0.45166, abs diff - 0.000305593
Output #1: element #8: CPU value - 0.632089, GPU value - 0.631836, abs diff - 0.000252664
Output #1: element #9: CPU value - 0.26527, GPU value - 0.265381, abs diff - 0.000110865
Output #1: element #11: CPU value - -0.88659, GPU value - -0.886719, abs diff - 0.000128627
Output #1: element #12: CPU value - -0.615389, GPU value - -0.615234, abs diff - 0.000155032
Output #1: element #14: CPU value - 0.632621, GPU value - 0.632812, abs diff - 0.000191331
Output #1: element #16: CPU value - -0.405204, GPU value - -0.405029, abs diff - 0.000174552
Output #1: element #17: CPU value - -0.8699, GPU value - -0.870117, abs diff - 0.000217617
Printed 10 different elements, threshhold - 0.0001, next different elements skipped
Total 1977 different elements, for output #1, threshhold - 0.0001
CPU time - 0.014039ms
GPU time(CPU->GPU->CPU) - 3.01996ms
```

### test_model_512.tflite

```
$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m test_model_512.tflite

INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
[output_1]
Output #0: element #1: CPU value - 0.686587, GPU value - 1.7236e-43, abs diff - 0.686587
Output #0: element #2: CPU value - 0.720795, GPU value - -7529.91, abs diff - 7530.63
Output #0: element #3: CPU value - 0.140191, GPU value - 1.7236e-43, abs diff - 0.140191
Output #0: element #4: CPU value - -0.63919, GPU value - -1.08755e-05, abs diff - 0.639179
Output #0: element #5: CPU value - -0.743797, GPU value - 1.7236e-43, abs diff - 0.743797
Output #0: element #6: CPU value - -0.272364, GPU value - -9.21461e-06, abs diff - 0.272355
Output #0: element #7: CPU value - 0.576355, GPU value - 1.7236e-43, abs diff - 0.576355
Output #0: element #8: CPU value - 0.757089, GPU value - -8.09113e-16, abs diff - 0.757089
Output #0: element #9: CPU value - 0.39027, GPU value - 2.4975e-39, abs diff - 0.39027
Output #0: element #10: CPU value - -0.496026, GPU value - -174.68, abs diff - 174.184
Printed 10 different elements, threshhold - 0.0001, next different elements skipped
Total 407 different elements, for output #0, threshhold - 0.0001
[output_2]
Output #1: element #5: CPU value - -0.868797, GPU value - -0.868652, abs diff - 0.000144362
Output #1: element #6: CPU value - -0.397364, GPU value - -0.397217, abs diff - 0.000147194
Output #1: element #7: CPU value - 0.451355, GPU value - 0.45166, abs diff - 0.000305593
Output #1: element #8: CPU value - 0.632089, GPU value - 0.631836, abs diff - 0.000252664
Output #1: element #9: CPU value - 0.26527, GPU value - 0.265381, abs diff - 0.000110865
Output #1: element #11: CPU value - -0.88659, GPU value - -0.886719, abs diff - 0.000128627
Output #1: element #12: CPU value - -0.615389, GPU value - -0.615234, abs diff - 0.000155032
Output #1: element #14: CPU value - 0.632621, GPU value - 0.632812, abs diff - 0.000191331
Output #1: element #16: CPU value - -0.405204, GPU value - -0.405029, abs diff - 0.000174552
Output #1: element #17: CPU value - -0.8699, GPU value - -0.870117, abs diff - 0.000217617
Printed 10 different elements, threshhold - 0.0001, next different elements skipped
Total 239 different elements, for output #1, threshhold - 0.0001
CPU time - 0.006577ms
GPU time(CPU->GPU->CPU) - 1.96819ms
```

### proof_model_4096.tflite

```
$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m proof_model_4096.tflite

INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
[output_1]
Output #0: element #5: CPU value - -0.743797, GPU value - -0.743652, abs diff - 0.000144362
Output #0: element #6: CPU value - -0.272364, GPU value - -0.272217, abs diff - 0.000147194
Output #0: element #7: CPU value - 0.576355, GPU value - 0.57666, abs diff - 0.000305593
Output #0: element #8: CPU value - 0.757089, GPU value - 0.756836, abs diff - 0.000252664
Output #0: element #9: CPU value - 0.39027, GPU value - 0.390381, abs diff - 0.000110865
Output #0: element #10: CPU value - -0.496026, GPU value - -0.49585, abs diff - 0.000176162
Output #0: element #11: CPU value - -0.76159, GPU value - -0.761719, abs diff - 0.000128627
Output #0: element #14: CPU value - 0.757621, GPU value - 0.757812, abs diff - 0.000191331
Output #0: element #16: CPU value - -0.280204, GPU value - -0.280029, abs diff - 0.000174552
Output #0: element #17: CPU value - -0.7449, GPU value - -0.745117, abs diff - 0.000217617
Printed 10 different elements, threshhold - 0.0001, next different elements skipped
Total 1852 different elements, for output #0, threshhold - 0.0001
[output_2]
Output #1: element #5: CPU value - -0.868797, GPU value - -0.868652, abs diff - 0.000144362
Output #1: element #6: CPU value - -0.397364, GPU value - -0.397217, abs diff - 0.000147194
Output #1: element #7: CPU value - 0.451355, GPU value - 0.45166, abs diff - 0.000305593
Output #1: element #8: CPU value - 0.632089, GPU value - 0.631836, abs diff - 0.000252664
Output #1: element #9: CPU value - 0.26527, GPU value - 0.265381, abs diff - 0.000110865
Output #1: element #11: CPU value - -0.88659, GPU value - -0.886719, abs diff - 0.000128627
Output #1: element #12: CPU value - -0.615389, GPU value - -0.615234, abs diff - 0.000155032
Output #1: element #14: CPU value - 0.632621, GPU value - 0.632812, abs diff - 0.000191331
Output #1: element #16: CPU value - -0.405204, GPU value - -0.405029, abs diff - 0.000174552
Output #1: element #17: CPU value - -0.8699, GPU value - -0.870117, abs diff - 0.000217617
Printed 10 different elements, threshhold - 0.0001, next different elements skipped
Total 1977 different elements, for output #1, threshhold - 0.0001
CPU time - 0.014ms
GPU time(CPU->GPU->CPU) - 4.58219ms
```

### proof_model_512.tflite

```
$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m proof_model_512.tflite 

INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
[output_1]
Output #0: element #5: CPU value - -0.743797, GPU value - -0.743652, abs diff - 0.000144362
Output #0: element #6: CPU value - -0.272364, GPU value - -0.272217, abs diff - 0.000147194
Output #0: element #7: CPU value - 0.576355, GPU value - 0.57666, abs diff - 0.000305593
Output #0: element #8: CPU value - 0.757089, GPU value - 0.756836, abs diff - 0.000252664
Output #0: element #9: CPU value - 0.39027, GPU value - 0.390381, abs diff - 0.000110865
Output #0: element #10: CPU value - -0.496026, GPU value - -0.49585, abs diff - 0.000176162
Output #0: element #11: CPU value - -0.76159, GPU value - -0.761719, abs diff - 0.000128627
Output #0: element #14: CPU value - 0.757621, GPU value - 0.757812, abs diff - 0.000191331
Output #0: element #16: CPU value - -0.280204, GPU value - -0.280029, abs diff - 0.000174552
Output #0: element #17: CPU value - -0.7449, GPU value - -0.745117, abs diff - 0.000217617
Printed 10 different elements, threshhold - 0.0001, next different elements skipped
Total 228 different elements, for output #0, threshhold - 0.0001
[output_2]
Output #1: element #5: CPU value - -0.868797, GPU value - -0.868652, abs diff - 0.000144362
Output #1: element #6: CPU value - -0.397364, GPU value - -0.397217, abs diff - 0.000147194
Output #1: element #7: CPU value - 0.451355, GPU value - 0.45166, abs diff - 0.000305593
Output #1: element #8: CPU value - 0.632089, GPU value - 0.631836, abs diff - 0.000252664
Output #1: element #9: CPU value - 0.26527, GPU value - 0.265381, abs diff - 0.000110865
Output #1: element #11: CPU value - -0.88659, GPU value - -0.886719, abs diff - 0.000128627
Output #1: element #12: CPU value - -0.615389, GPU value - -0.615234, abs diff - 0.000155032
Output #1: element #14: CPU value - 0.632621, GPU value - 0.632812, abs diff - 0.000191331
Output #1: element #16: CPU value - -0.405204, GPU value - -0.405029, abs diff - 0.000174552
Output #1: element #17: CPU value - -0.8699, GPU value - -0.870117, abs diff - 0.000217617
Printed 10 different elements, threshhold - 0.0001, next different elements skipped
Total 239 different elements, for output #1, threshhold - 0.0001
CPU time - 0.007116ms
GPU time(CPU->GPU->CPU) - 2.74935ms
```
"
47714,I found maybe a bug about model.predict() and model.evaluate(),"My machine is MacBook Pro 2020 13-inch, TensorFlow version is 2.4.
When I use model.predict() and model.evaluate(), no matter which function is used first, the output result of the other function will be seriously different from expected. code show as below
```
import tensorflow as tf
import matplotlib.pyplot as plt
import pylab
import numpy as np

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape((60000, 28, 28, 1))
x_test = x_test.reshape((10000, 28, 28, 1))
x_train = 2 * x_train / 255.0 - 1
x_test = 2 * x_test / 255.0 - 1

plt.imshow(x_train[0, :, :, ])
print()
pylab.show()


def build_model():
    return tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation=tf.nn.relu,
                               input_shape=(28, 28, 1)),
        tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(49, activation=tf.nn.relu),
        tf.keras.layers.Dense(10, activation=tf.nn.softmax)
    ])


def train(x, y):
    model = build_model()
    model.summary()
    model.compile(optimizer=tf.optimizers.Adam(), loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    model.fit(x, y, epochs=2)
    # model.evaluate(x_test, y_test)

    # predict
    print(model.predict(x_test)[0])
    print(y_test[0])
    model.evaluate(x_test,y_test)
    # print(model.predict(x_test)[0])

    return model


model = train(x_train, y_train)
```
"
47713,tf.variable_scope with custom_getter in TF2,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Maybe



**Describe the feature and the current behavior/state.**
I need to achieve similar results in the code below in TF2. By going through enough SO questions and TF docs, It seems impossible, so this feature does not exist in TF2.

    params = tf.trainable_variables(""my_model"")
    ema = tf.train.ExponentialMovingAverage(alpha)
    ema_apply_op = ema.apply(params)


    def custom_getter(getter, *args, **kwargs):
        return ema.average(getter(*args, **kwargs))
    
    with tf.compat.v1.variable_scope(
        ""my_model"", custom_getter=custom_getter, reuse=True
    ):
        avg_model = create_avg_model()

**Will this change the current api? How?**

I don't know.

**Who will benefit with this feature?**

Whoever wants to create a keras model with a custom getter

**Any Other info.**
"
47712,The Risk of posix_memalign in Ruy which TensorFlow Lite used.,"
Hi TF Authors,

We recently got a crash as following when we tflite::Interpreter::Invoke the cartoonized [model](https://github.com/margaretmz/Cartoonizer-with-TFLite/blob/master/android/app/src/main/ml/whitebox_cartoon_gan_int8.tflite):
```
2021-03-09 16:07:22.261 18157-18309 W/libc: memalign(64, 411042816) failed: returning null pointer
2021-03-09 15:55:06.148 551-1048 D/AudioMixer: setDRCHandler, mStreamType: 3, device: 2
2021-03-09 15:55:06.150 17799-17959 E/CRASH: 	#00  pc 001b7b00  /data/app/com.magicpal.magicamiti-VBWn90aZTYIGNTtAk2TwYA==/lib/arm/libtensorflowlite.so
2021-03-09 15:55:06.150 17799-17959 E/CRASH: 	#01  pc 0008cf05  /data/app/com.magicpal.magicamiti-VBWn90aZTYIGNTtAk2TwYA==/lib/arm/libtensorflowlite.so
2021-03-09 15:55:06.150 17799-17959 E/CRASH: 	#02  pc 0008cb99  /data/app/com.magicpal.magicamiti-VBWn90aZTYIGNTtAk2TwYA==/lib/arm/libtensorflowlite.so
2021-03-09 15:55:06.150 17799-17959 E/CRASH: 	#03  pc 001b8e85  /data/app/com.magicpal.magicamiti-VBWn90aZTYIGNTtAk2TwYA==/lib/arm/libtensorflowlite.so
2021-03-09 15:55:06.150 17799-17959 E/CRASH: 	#04  pc 001b8bc1  /data/app/com.magicpal.magicamiti-VBWn90aZTYIGNTtAk2TwYA==/lib/arm/libtensorflowlite.so
2021-03-09 15:55:06.150 17799-17959 E/CRASH: memory near r0:
2021-03-09 15:55:06.150 17799-17959 E/CRASH:     96a4513c 00000000 96a45208 00000000 804ff400  .....R........O.
2021-03-09 15:55:06.150 17799-17959 E/CRASH:     96a4514c 00000000 96a451c0 ade93f09 e57af260  .....Q...?..`.z.
2021-03-09 15:55:06.150 17799-17959 E/CRASH:     96a4515c 4f640380 4f6409a0 00000000 00000000  ..dO..dO........
2021-03-09 15:55:06.150 17799-17959 E/CRASH:     96a4516c 804ff400 00000200 00000010 00000010  ..O.............
2021-03-09 15:55:06.150 17799-17959 E/CRASH:     96a4517c ffffffff ffffffff 00000620 ffffffab  ........ .......
2021-03-09 15:55:06.151 17799-17959 E/CRASH:     96a4522c 00000620 00040000 00000620 00000000   ....... .......

( Its too much to display, check whole logs on  the end of this log snippet,  tensor_crash.txt)

2021-03-09 15:55:06.167 17799-17959 E/CRASH:     4f640a40 abababab abababab abababab abababab  ................
2021-03-09 15:55:06.167 17799-17959 E/CRASH:     4f640a50 abababab abababab abababab abababab  ................
2021-03-09 15:55:06.167 17799-17959 E/CRASH:     4f640a60 abababab abababab abababab abababab  ................
2021-03-09 15:55:06.167 17799-17959 E/CRASH:     4f640a70 abababab abababab abababab abababab  ................
2021-03-09 15:55:06.167 17799-17959 E/CRASH:     4f640a80 abababab abababab abababab abababab  ................
2021-03-09 15:55:06.193 17799-17823 E/CRASH: other thread is trapped; signum = 11
2021-03-09 15:55:06.194 17799-17823 E/MessageQueue: IdleHandler threw exception
    java.lang.Error: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 00000200
    Build fingerprint: 'Redmi/merlin/merlin:10/QP1A.190711.020/V12.0.8.0.QJOCNXM:user/release-keys'
    Revision: '0'
    pid: 17799, tid: 17959, name: UnityMain  >>> com.magicpal.magicamiti <<<
        r0 96a4515c  r1 00000010  r2 00000620  r3 00000620
        r4 00000200  r5 00000010  r6 00000010  r7 96a45150
        r8 ffffffab  r9 96a45228  sl 00000620  fp 96a45208
        ip 4f640390  sp 96a45100  lr 4f6409b0  pc adfbeb00  cpsr 00004627
    
        at libtensorflowlite.001b7b00(Native Method)
        at libtensorflowlite.0008cf05(Native Method)
        at libtensorflowlite.0008cb99(Native Method)
        at libtensorflowlite.001b8e85(Native Method)
        at libtensorflowlite.001b8bc1(Native Method)
2021-03-09 15:55:06.196 551-1048 D/AudioMixer: setDRCHandler, mStreamType: 3, device: 2
2021-03-09 15:55:06.202 551-1048 D/AudioMixer: setDRCHandler, mStreamType: 3, device: 2
    
    --------- beginning of system
```
Whole logs is here, [tensor_crash.txt](https://github.com/tensorflow/tensorflow/files/6120045/tensor_crash.txt).

And we suspect that the [memalign](https://github.com/google/ruy/blob/3c363dc10d06857ad489c034ebb3bbd6d273dfbd/ruy/system_aligned_alloc.cc#L34) called by Ruy cause this crash quietly.
```
2021-03-09 16:07:22.261 18157-18309 W/libc: memalign(64, 411042816) failed: returning null pointer
```

411042816 = 392MB is the exact buffer size of the Tensor 154 in [model](https://github.com/margaretmz/Cartoonizer-with-TFLite/blob/master/android/app/src/main/ml/whitebox_cartoon_gan_int8.tflite) we mentioned. So, we believe that the Interpreter could be crashed when it memalign a big memory and got  a null pointer on Invoke stage. 

However, why we reallocate memory after Interpreter::AllocateTensors? And did we ever considered the cases, which the big temporary buffers would be allocated in the Invoke flow? We are confused that why not assert the return of memalign, and only return a null pointer without any processing? Its seem dangerous and  really unfriendly for developers to debug.

Looking forward to your reply, thx. @renjie-liu @multiverse-tf @wangtz "
47711,std symbols undefined while compiling libtensorflowlite.so for android,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  macOS Big Sur version 11.2.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
  source
- TensorFlow version:
  2.4.0
- Python version:
 3.9
- Installed using virtualenv? pip? conda?:
  trying to build from source
- Bazel version (if compiling from source):
 3.1.0
- GCC/Compiler version (if compiling from source):
 clang-1200.0.32.29
- CUDA/cuDNN version:
- GPU model and memory:
  AMD Radeon Pro 5300M 4 GB



**Describe the problem**
When trying to compile libtensorflowlite.so for android with added ""//tensorflow/lite/delegates/flex:delegate"" dependency (as is stated in the https://www.tensorflow.org/lite/guide/ops_select), compilation would fail due to std::make_unique being undefined.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build \
-s //tensorflow/lite:libtensorflowlite.so \
--config=android_arm64 \
--cxxopt='--std=c++11' \
-c opt \
--config=v2 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

SUBCOMMAND: # //tensorflow/core/kernels:portable_tensorflow_kernels [action 'Compiling tensorflow/core/kernels/control_flow_ops.cc', configuration: 17edd87150895742afa8f50ebcaef1b3cd2bf6b4d6974c05ffc14bdd6966f454, execution platform: @local_execution_config_platform//:platform]
(cd /private/var/tmp/_bazel_richardyao/1258fb4559d515393cc4dfc8787a5562/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=30.0.3 \
    ANDROID_NDK_API_LEVEL=21 \
    ANDROID_NDK_HOME=/usr/local/Caskroom/android-ndk/21/android-ndk-r21 \
    ANDROID_SDK_API_LEVEL=30 \
    ANDROID_SDK_HOME=/Users/richardyao/library/Android/Sdk \
    PATH=/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/local/opt/python@3.9/bin/python3.9 \
    PYTHON_LIB_PATH=/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
  external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android '-D__ANDROID_API__=21' -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig '-Werror=return-type' '-Werror=int-to-pointer-cast' '-Werror=pointer-to-int-cast' '-Werror=implicit-function-declaration' -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/control_flow_ops.pic.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/control_flow_ops.pic.o' -fPIC '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' '-DS_IEXEC=S_IXUSR' -DSUPPORT_SELECTIVE_REGISTRATION -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/arm64-v8a-opt/bin -iquote external/gif -iquote bazel-out/arm64-v8a-opt/bin/external/gif -iquote external/eigen_archive -iquote bazel-out/arm64-v8a-opt/bin/external/eigen_archive -iquote external/com_google_absl -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/arm64-v8a-opt/bin/external/nsync -iquote external/libjpeg_turbo -iquote bazel-out/arm64-v8a-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf -iquote external/double_conversion -iquote bazel-out/arm64-v8a-opt/bin/external/double_conversion -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm64-v8a-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/bin/external/farmhash_archive -iquote external/png -iquote bazel-out/arm64-v8a-opt/bin/external/png -iquote external/zlib -iquote bazel-out/arm64-v8a-opt/bin/external/zlib -iquote external/highwayhash -iquote bazel-out/arm64-v8a-opt/bin/external/highwayhash -iquote external/icu -iquote bazel-out/arm64-v8a-opt/bin/external/icu -iquote external/fft2d -iquote bazel-out/arm64-v8a-opt/bin/external/fft2d -iquote external/gemmlowp -iquote bazel-out/arm64-v8a-opt/bin/external/gemmlowp -isystem external/gif -isystem bazel-out/arm64-v8a-opt/bin/external/gif -isystem external/eigen_archive -isystem bazel-out/arm64-v8a-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/arm64-v8a-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/src -isystem external/double_conversion -isystem bazel-out/arm64-v8a-opt/bin/external/double_conversion -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/farmhash_archive/src -isystem external/png -isystem bazel-out/arm64-v8a-opt/bin/external/png -isystem external/zlib -isystem bazel-out/arm64-v8a-opt/bin/external/zlib -isystem external/icu/icu4c/source/common -isystem bazel-out/arm64-v8a-opt/bin/external/icu/icu4c/source/common -w '-std=c++14' '--std=c++11' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/core/kernels/control_flow_ops.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/control_flow_ops.pic.o)
ERROR: /Users/richardyao/Code/tensorflow/tensorflow/core/kernels/BUILD:6467:11: C++ compilation of rule '//tensorflow/core/kernels:portable_tensorflow_kernels' failed (Exit 1): clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64 -target ... (remaining 154 argument(s) skipped)
tensorflow/core/kernels/data/dataset_utils.cc:883:17: error: no template named 'make_unique' in namespace 'std'; did you mean 'absl::make_unique'?
      counter = std::make_unique<BlockingCounter>(num_batch_elements);
                ^~~~~~~~~~~~~~~~
                absl::make_unique
external/com_google_absl/absl/memory/memory.h:168:55: note: 'absl::make_unique' declared here
typename memory_internal::MakeUniqueResult<T>::scalar make_unique(
                                                      ^
tensorflow/core/kernels/data/dataset_utils.cc:883:17: error: no template named 'make_unique' in namespace 'std'; did you mean 'absl::make_unique'?
      counter = std::make_unique<BlockingCounter>(num_batch_elements);
                ^~~~~~~~~~~~~~~~
                absl::make_unique
external/com_google_absl/absl/memory/memory.h:168:55: note: 'absl::make_unique' declared here
typename memory_internal::MakeUniqueResult<T>::scalar make_unique(
                                                      ^
tensorflow/core/kernels/data/dataset_utils.cc:884:19: error: no template named 'make_unique' in namespace 'std'; did you mean 'absl::make_unique'?
      status_mu = std::make_unique<mutex>();
                  ^~~~~~~~~~~~~~~~
                  absl::make_unique
external/com_google_absl/absl/memory/memory.h:168:55: note: 'absl::make_unique' declared here
typename memory_internal::MakeUniqueResult<T>::scalar make_unique(
                                                      ^
tensorflow/core/kernels/data/dataset_utils.cc:884:19: error: no template named 'make_unique' in namespace 'std'; did you mean 'absl::make_unique'?
      status_mu = std::make_unique<mutex>();
                  ^~~~~~~~~~~~~~~~
                  absl::make_unique
external/com_google_absl/absl/memory/memory.h:168:55: note: 'absl::make_unique' declared here
typename memory_internal::MakeUniqueResult<T>::scalar make_unique(
                                                      ^
4 errors generated.
Target //tensorflow/lite:libtensorflowlite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 690.094s, Critical Path: 483.34s
INFO: 757 processes: 13 internal, 744 local.
FAILED: Build did NOT complete successfully
"
47710,Unable to build tensorflow lite standalone pip package for raspberrypi zero,"**System information**

Raspbian buster lite
Linux raspberrypi 5.4.83+ #1379 Mon Dec 14 13:06:05 GMT 2020 armv6l GNU/Linux
gcc version 8.3.0 (Raspbian 8.3.0-6+rpi1)
python 3.7.3

tensorflow source @ commit a2a5b86c3bd90e03151a25c52c0f6cebbd573228

**Describe the problem**

Unable to build tensorflow lite standalone pip package for raspberrypi zero

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I ran the commands in the aforementioned link on the raspberrypi zero. The only change is I used pip3

sudo apt install swig libjpeg-dev zlib1g-dev python3-dev python3-numpy
pip3 install numpy pybind11
tensorflow/lite/tools/make/download_dependencies.sh
tensorflow/lite/tools/pip_package/build_pip_package.sh

It ran for maybe a day or so before giving up with the following:

g++: fatal error: Killed signal terminated program cc1plus
compilation terminated.
make: *** [tensorflow/lite/tools/make/Makefile:335: /home/pi/tensorflow_src/tensorflow/lite/tools/make/gen/linux_armv6l/obj/tensorflow/lite/kernels/conv.o] Error 1
make: Leaving directory '/home/pi/tensorflow_src'
Traceback (most recent call last):
  File ""setup.py"", line 227, in <module>
    'build_py': CustomBuildPy,
  File ""/usr/lib/python3/dist-packages/setuptools/__init__.py"", line 145, in setup
    return distutils.core.setup(**attrs)
  File ""/usr/lib/python3.7/distutils/core.py"", line 148, in setup
    dist.run_commands()
  File ""/usr/lib/python3.7/distutils/dist.py"", line 966, in run_commands
    self.run_command(cmd)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.7/distutils/command/bdist.py"", line 143, in run
    self.run_command(cmd_name)
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.7/distutils/command/bdist_dumb.py"", line 81, in run
    self.run_command('build')
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.7/distutils/command/build.py"", line 135, in run
    self.run_command(cmd_name)
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""setup.py"", line 133, in run
    self.run_command('build_ext')
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""setup.py"", line 125, in run
    make()
  File ""setup.py"", line 105, in make
    subprocess.check_call(make_args(quiet=False))
  File ""/usr/lib/python3.7/subprocess.py"", line 347, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['make', 'SHELL=/bin/bash', 'BUILD_WITH_NNAPI=false', '-C', '/home/pi/tensorflow_src/tensorflow/lite/tools/pip_package/../../../..', '-f', 'tensorflow/lite/tools/make/Makefile', '-j', '1']' returned non-zero exit status 2."
47709,Pip install issue,"**System information**
- OS Platform and Distribution: Manjaro Linux
- TensorFlow installed from: Pip
- TensorFlow version: 2.4.1
- Python version: 3.9.2
- Installed using virtualenv? pip? conda?: pip 20.3.1
- GPU model and memory: Asus GTX 1070 8GB OC Snow Edition



**Describe the problem**

An issue with Pip is preventing being able to install Tensorflow just by doing `pip install tensorflow` and can't search for it either with Pip because they've shut down the PyPI XMLRPC Search, which has remained shut down since Jan 12 2021 according to their site status page  https://status.python.org

Attempting to install tensorflow produces the following error;

```
$ pip install tensorflow

Defaulting to user installation because normal site-packages is not writeable
ERROR: Could not find a version that satisfies the requirement tensorflow
ERROR: No matching distribution found for tensorflow
```


and attempting to search for the package produces the following error as mentioned with the search being shut down;

```
$ pip search tensorflow
ERROR: Exception:
Traceback (most recent call last):
  File ""/usr/lib/python3.9/site-packages/pip/_internal/cli/base_command.py"", line 224, in _main
    status = self.run(options, args)
  File ""/usr/lib/python3.9/site-packages/pip/_internal/commands/search.py"", line 62, in run
    pypi_hits = self.search(query, options)
  File ""/usr/lib/python3.9/site-packages/pip/_internal/commands/search.py"", line 82, in search
    hits = pypi.search({'name': query, 'summary': query}, 'or')
  File ""/usr/lib/python3.9/xmlrpc/client.py"", line 1116, in __call__
    return self.__send(self.__name, args)
  File ""/usr/lib/python3.9/xmlrpc/client.py"", line 1456, in __request
    response = self.__transport.request(
  File ""/usr/lib/python3.9/site-packages/pip/_internal/network/xmlrpc.py"", line 46, in request
    return self.parse_response(response.raw)
  File ""/usr/lib/python3.9/xmlrpc/client.py"", line 1348, in parse_response
    return u.close()
  File ""/usr/lib/python3.9/xmlrpc/client.py"", line 662, in close
    raise Fault(**self._stack[0])
xmlrpc.client.Fault: <Fault -32500: ""RuntimeError: PyPI's XMLRPC API is currently disabled due to unmanageable load and will be deprecated in the near future. See https://status.python.org/ for more information."">
```

**Any other info / logs**

"
47706,Training stalls after saving checkpoint 0,"Hello,

First of all, I have read and tried all solutions from #32017, unless it isn't the same error exactly.

**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow version: 2.4.1
- Python version: 3.8.2
- CUDA/cuDNN version: 11.2

I have tried to train a model with this command:
melody_rnn_train --config=attention_rnn --run_dir=tmp\\run1 --sequence_example_file=tmp\\sequence_examples\\training_melodies.tfrecord --hparams=""batch_size=64,rnn_layer_sizes=[64,64]"" --num_training_steps=1

The behaviour should be the same than in a normal t2t-trainer output.

There are a lot of warnings about deprecated stuff but, I am pretty sure they are not impoortant so the last output lines are:

```
INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...
I0310 18:09:11.618963  5924 basic_session_run_hooks.py:613] Calling checkpoint listeners before saving checkpoint 0...
INFO:tensorflow:Saving checkpoints for 0 into tmp\\run1\train\model.ckpt.
I0310 18:09:11.622482  5924 basic_session_run_hooks.py:618] Saving checkpoints for 0 into tmp\\run1\train\model.ckpt.
INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...
I0310 18:09:16.462026  5924 basic_session_run_hooks.py:625] Calling checkpoint listeners after saving checkpoint 0...
```

And nothing more happens after that, it stalls and IDK why.
Thanks in advance."
47705,Could not load libcusolver.so.10 with tf-nightly-gpu,"**System information**
- OS Platform and Distribution: Arch Linux
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.5.0 (tf-nightly-gpu)
- Python version: 3.9
- CUDA/cuDNN version: 11.2
- GPU model and memory: 1070 TI 8GB

Problem:
Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/local/lib

I have tried to edit the LD_LIBRARY_PATH like other issues have suggested but that does not work. I do not know which version of CUDA tf-nightly requires. 

CUDA Info:
[taylor@taylor-desktop ~]$ pacman -Qi cuda
Name            : cuda
Version         : 11.2.1-4
Description     : NVIDIA's GPU programming toolkit
Architecture    : x86_64
URL             : https://developer.nvidia.com/cuda-zone
Licenses        : custom:NVIDIA
Groups          : None
Provides        : cuda-toolkit  cuda-sdk  libcudart.so=11.0-64  libcublas.so=11-64  libcublas.so=11-64
                      libcusolver.so=11-64  libcusolver.so=11-64  libcusparse.so=11-64  libcusparse.so=11-64
Depends On      : gcc  gcc-libs  opencl-nvidia  nvidia-utils  python
Optional Deps   : gdb: for cuda-gdb [installed]
                          glu: required for some profiling tools in CUPTI [installed]
Required By     : cudnn
Optional For    : None
Conflicts With  : None
Replaces        : cuda-toolkit  cuda-sdk  cuda-static
Installed Size  : 3.32 GiB
Packager        : Konstantin Gizdov <arch@kge.pw>
Build Date      : Mon Mar 1 05:21:57 2021
Install Date    : Tue Mar 9 16:35:57 2021
Install Reason  : Explicitly installed
Install Script  : Yes
Validated By    : Signature

libcusolver.so=11-64 is installed but tensorflow seems to be looking for libcusolver.so=10. Is that due to tf-nighlty requiring Cuda 11.0? Then it would be the same cuda version as required by tensorflow 2.4. Let me know if you need any more info or I need to format things differently. Thanks."
47704,Numbers get printed out when model.fit() instead of verbosity associated with epochs/iterations -- tf.keras,"This is my code so far, I am using ```tf.data.experimental.bucket_by_sequence_length``` because X_train is a list of 2D lists with differing number of rows -- so when training want to batch with similar sizes. 


```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

train_data = tf.data.Dataset.from_generator(lambda: (X_train, y_train), output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([None, len(X_train[0][0])]),tf.TensorShape([None, len(y_train[0][0])])))

val_data = tf.data.Dataset.from_generator(lambda: (X_test, y_test), output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([None, len(X_train[0][0])]),tf.TensorShape([None, len(y_train[0][0])])))

def element_length_fn(x, y):    
         return tf.shape(x)[0]

max_train = len(max(X_train, key=len))
min_train = len(min(X_train, key=len))
max_val = len(max(X_test, key=len))
min_val = len(min(X_test, key=len))
batch_size = 64
bucket_boundaries_train = list(np.arange(min_train, max_train, 10)) 
bucket_batch_sizes_train = [batch_size] * (len(bucket_boundaries_train) + 1)
bucket_boundaries_val = list(np.arange(min_val, max_val, 10)) 
bucket_batch_sizes_val = [batch_size] * (len(bucket_boundaries_val) + 1)

train_data = train_data.apply(tf.data.experimental.bucket_by_sequence_length(
              element_length_func=element_length_fn,
              bucket_batch_sizes=bucket_batch_sizes_train,
              bucket_boundaries=bucket_boundaries_train))

val_data = val_data.apply(tf.data.experimental.bucket_by_sequence_length(
              element_length_func=element_length_fn,
              bucket_batch_sizes=bucket_batch_sizes_val,
              bucket_boundaries=bucket_boundaries_val,)

def create_model():    
     i = Input(shape=(None, len(X_train[0][0])))
     x = Bidirectional(LSTM(128, return_sequences=True))(i)    
     x = TimeDistributed(Dense(2))(x)    
     return tf.keras.Model(inputs=i, outputs=x)

model = create_model()    
model.compile(loss='mse',optimizer='adam',metrics=[metrics.RootMeanSquaredError()])
model.fit(train_data,
         epochs=5,
         validation_data=val_data)

```



However, when I run this, no error apparently shows. The only bizarre thing is that what looks like lists keeps getting print out, as opposed to the verbosity you would expect when training (see below for a sample of what gets printed out). How can this be fixed?



```
 -6.866918300812502, 8.563543404559187, 21.8377909841752, 6.40732927880351, -6.866918300812502, 8.563543404559187, 262.05, 115.33, -172.22000000000003, -25.5, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1197.0, 0.009000000000000001, 2.3, 0.31, 2075.0, 3.5, 0.0, 0.0, 78516.0, 0.98, 0.0, 0.0, 275947.0, 70900.9, 0.0, 0.0, 5.0, 27149.0, 115.33, 0.0, 18.0, 6.40732927880351, 7.15266763866861, 6.40732927880351, 0.10420424903230041, 0.10420424903230041, 0.0, 0.0, 14.970872683362698, 13.4530996322305, -8.563543404559187, -7.045770353426991, 14.970872683362698, 13.4530996322305, -8.563543404559187, -7.045770353426991, 89.83, 403.59, 25.5, -288.26, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1197.0, 0.009000000000000001, 2.3, 0.31, 2075.0, 3.5, 0.0, 0.0, 78516.0, 0.98, 0.0, 0.0, 275947.0, 70900.9, 0.0, 0.0, 6.0, 27167.0, 403.59, 0.0, 30.0, 13.4530996322305, 14.305335277337198, 13.4530996322305, 0.059574671168793, 0.059574671168793, 0.0, 0.0, 6.40732927880351, 24.934202366029503, 7.045770353426991, -11.481102733799002, 6.40732927880351, 24.934202366029503, 7.045770353426991, -11.481102733799002, 115.33, 1645.66, 288.26, -1242.0700000000002, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1197.0, 0.009000000000000001, 2.3, 0.31, 2075.0, 3.5, 0.0, 0.0, 78516.0, 0.98, 0.0, 0.0, 275947.0, 70900.9, 0.0, 0.0, 7.0, 27197.0, 1645.66, 0.0, 66.0, 24.934202366029503, 26.911911990490605, 24.934202366029503, 0.07348826144942544, 0.07348826144942544, 0.0, 0.0, 13.4530996322305, 25.7129007505185, 11.481102733799002, -0.7786983844889974, 13.4530996322305, 25.7129007505185, 11.481102733799002, -0.7786983844889974, 403.59, 925.66, 1242.0700000000002, 720.0000000000001, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1197.0, 0.009000000000000001, 2.3, 0.31, 2075.0, 3.5, 0.0, 0.0, 78516.0, 0.98, 0.0, 0.0, 275947.0, 70900.9, 0.0, 0.0, 8.0, 27263.0, 925.66, 0.0, 36.0, 25.7129007505185, 26.688391126782303, 25.7129007505185, 0.03655111211574236, 0.03655111211574236, 0.0, 0.0, 24.934202366029503, 26.095100358087002, 0.7786983844889974, -0.38219960756850213, 24.934202366029503, 26.095100358087002, 0.7786983844889974, -0.38219960756850213, 1645.66, 782.85, -720.0000000000001, 142.80999999999995, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1197.0, 0.009000000000000001, 2.3, 0.31, 2075.0, 3.5, 0.0, 0.0, 78516.0, 0.98, 0.0, 0.0, 275947.0, 70900.9, 0.0, 0.0, 9.0, 27299.0, 782.85, 0.0, 30.0, 26.095100358087002, 26.643686954040604, 26.095100358087002, 0.020589740335108075, 0.020589740335108075, 0.0, 0.0, 25.7129007505185, 24.284601969264603, 0.38219960756850213, 1.810498388822399, 25.7129007505185, 24.284601969264603, 0.38219960756850213, 1.810498388822399, 925.66, 1019.95, -142.80999999999995, -237.10000000000002, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1197.0, 0.009000000000000001, 2.3, 0.31, 2075.0, 3.5, 0.0, 0.0, 78516.0, 0.98, 
```
"
47702,Model.fit average gradients by batch_size,"So, it seems `Model.fit` is averaging the gradient over the batch size.

There is a simple [script](https://github.com/NEGU93/cvnn/blob/master/debug/mwe_testing_learning_algo.py) I did to check that assertion and it was discussed in [stackoverlow](https://stackoverflow.com/questions/66566905/debugging-tensorflow-fit-not-making-sense/)

It is highly possible this is by design and not a bug but I wanted to report it just in case it is indeed a bug."
47701,"""Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED"" only when running old TF code","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.1.0
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: 3.5.2
- CUDA/cuDNN version: Driver Version: 418.39       CUDA Version: 10.1
- GPU model and memory: GeForce GTX 1080Ti (11GB)

I've downloaded tensorflow-gpu 2.1.0 because it's compatible with the drivers currently installed. Indeed, TFv2 code works, but  I run into this problem when trying to run old TFv1 code. It seems that this error happens after the first convolution, so I guess there is a problem using convolutions, but it's funny that I have no such problem using TFv2 code. I've searched about it, and I've read that downgrading TF and even the drivers could solve it, but if TFv2 can use convolutions, why old TFv1 code could not? This might be a bug. Any workaround?

TFv2 code that works: https://pastebin.com/D4YMMUPT
Old TFv1 code that does not work: https://pastebin.com/AXAaYbKs

Error log:

> 2021-03-10 17:45:34.210128: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
> 2021-03-10 17:45:34.210184: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
> 2021-03-10 17:45:34.210191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
> WARNING: Logging before flag parsing goes to stderr.
> W0310 17:45:34.787791 139979412670208 deprecation.py:323] From /home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
> Instructions for updating:
> non-resource variables are not supported in the long term
> W0310 17:45:34.792874 139979412670208 deprecation.py:506] From /home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
> Instructions for updating:
> If using Keras pass *_constraint arguments to layers.
> 2021-03-10 17:45:34.911558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
> 2021-03-10 17:45:34.930318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
> pciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
> coreClock: 1.62GHz coreCount: 28 deviceMemorySize: 10.90GiB deviceMemoryBandwidth: 451.17GiB/s
> 2021-03-10 17:45:34.930522: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
> 2021-03-10 17:45:34.931581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
> 2021-03-10 17:45:34.932423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
> 2021-03-10 17:45:34.932628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
> 2021-03-10 17:45:34.933731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
> 2021-03-10 17:45:34.934569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
> 2021-03-10 17:45:34.937042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
> 2021-03-10 17:45:34.938760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
> 2021-03-10 17:45:34.939051: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
> 2021-03-10 17:45:34.961585: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3999980000 Hz
> 2021-03-10 17:45:34.962354: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59b5640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
> 2021-03-10 17:45:34.962397: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
> 2021-03-10 17:45:35.060702: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a3bb90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
> 2021-03-10 17:45:35.060757: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
> 2021-03-10 17:45:35.062292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
> pciBusID: 0000:17:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
> coreClock: 1.62GHz coreCount: 28 deviceMemorySize: 10.90GiB deviceMemoryBandwidth: 451.17GiB/s
> 2021-03-10 17:45:35.062384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
> 2021-03-10 17:45:35.062432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
> 2021-03-10 17:45:35.062480: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
> 2021-03-10 17:45:35.062526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
> 2021-03-10 17:45:35.062573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
> 2021-03-10 17:45:35.062617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
> 2021-03-10 17:45:35.062665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
> 2021-03-10 17:45:35.065186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
> 2021-03-10 17:45:35.065292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
> 2021-03-10 17:45:35.067678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2021-03-10 17:45:35.067707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
> 2021-03-10 17:45:35.067721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
> 2021-03-10 17:45:35.070991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10308 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)
> W0310 17:45:35.072554 139979412670208 deprecation.py:323] From /home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/util/tf_should_use.py:235: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
> Instructions for updating:
> Use `tf.global_variables_initializer` instead.
> 2021-03-10 17:45:35.750980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
> 2021-03-10 17:45:35.904388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
> 2021-03-10 17:45:35.905558: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
> 2021-03-10 17:45:35.905655: E tensorflow/stream_executor/cuda/cuda_dnn.cc:337] Possibly insufficient driver version: 418.39.0
> 2021-03-10 17:45:35.905682: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
> 2021-03-10 17:45:35.905727: E tensorflow/stream_executor/cuda/cuda_dnn.cc:337] Possibly insufficient driver version: 418.39.0
> Traceback (most recent call last):
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/client/session.py"", line 1367, in _do_call
>     return fn(*args)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/client/session.py"", line 1352, in _run_fn
>     target_list, run_metadata)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/client/session.py"", line 1445, in _call_tf_sessionrun
>     run_metadata)
> tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
>   (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[{{node Conv2D}}]]
> 	 [[Mean_1/_7]]
>   (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[{{node Conv2D}}]]
> 0 successful operations.
> 0 derived errors ignored.
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""old_tf_script.py"", line 62, in <module>
>     x:xx, y_: yy, keep_prob: 1.0})
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 790, in eval
>     return _eval_using_default_session(self, feed_dict, self.graph, session)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 5312, in _eval_using_default_session
>     return session.run(tensors, feed_dict)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/client/session.py"", line 960, in run
>     run_metadata_ptr)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/client/session.py"", line 1183, in _run
>     feed_dict_tensor, options, run_metadata)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/client/session.py"", line 1361, in _do_run
>     run_metadata)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/client/session.py"", line 1386, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
>   (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[node Conv2D (defined at old_tf_script.py:18) ]]
> 	 [[Mean_1/_7]]
>   (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[node Conv2D (defined at old_tf_script.py:18) ]]
> 0 successful operations.
> 0 derived errors ignored.
> 
> Errors may have originated from an input operation.
> Input Source operations connected to node Conv2D:
>  Reshape (defined at old_tf_script.py:30)
> 
> Input Source operations connected to node Conv2D:
>  Reshape (defined at old_tf_script.py:30)
> 
> Original stack trace for 'Conv2D':
>   File ""old_tf_script.py"", line 31, in <module>
>     h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
>   File ""old_tf_script.py"", line 18, in conv2d
>     return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/ops/nn_ops.py"", line 1914, in conv2d_v2
>     name=name)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/ops/nn_ops.py"", line 2011, in conv2d
>     name=name)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_nn_ops.py"", line 969, in conv2d
>     data_format=data_format, dilations=dilations, name=name)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 742, in _apply_op_helper
>     attrs=attr_protos, op_def=op_def)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 3322, in _create_op_internal
>     op_def=op_def)
>   File ""/home/user/virtualenv1/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 1756, in __init__
>     self._traceback = tf_stack.extract_stack()
> 

"
47700,"Unable to load tflite file in android, It provides ="" Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/transpose.cc:55 op_context->perm->dims->data[0] != dims (3 != 2) 03-10 16:42:09.948  8010  8010 D check here: Node number 6 (TRANSPOSE) failed to prepare.""","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes** 
 colab link to tensorflow model = [https://colab.research.google.com/drive/1BI0661FaM6L4oqjNX1MYfHpJCKwKQhjt?usp=sharing](url)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Samsung Galaxy M30 s**
- TensorFlow installed from (source or binary):  **pip3 install tensorflow**
- TensorFlow version (use command below): **2.4.1**
- Python version: **3.7**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**

Unable to load tflite model file , It provides Internal error. PLease find Log data that raised for this issue : 
**03-10 16:42:09.948  8010  8010 D check here: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/transpose.cc:55 op_context->perm->dims->data[0] != dims (3 != 2)
03-10 16:42:09.948  8010  8010 D check here: Node number 6 (TRANSPOSE) failed to prepare.
03-10 16:42:09.948  8010  8010 D check here: 
03-10 16:42:09.948  8010  8010 D check here: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
03-10 16:42:09.948  8010  8010 D check here: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:163)
03-10 16:42:09.948  8010  8010 D check here: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:360)
03-10 16:42:09.948  8010  8010 D check here: 	at org.tensorflow.lite.Interpreter.run(Interpreter.java:319)
03-10 16:42:09.948  8010  8010 D check here: 	at com.example.testforlite.MainActivity$1.onClick(MainActivity.java:89)
03-10 16:42:09.948  8010  8010 D check here: 	at android.view.View.performClick(View.java:5794)
03-10 16:42:09.948  8010  8010 D check here: 	at com.google.android.material.button.MaterialButton.performClick(MaterialButton.java:1119)
03-10 16:42:09.948  8010  8010 D check here: 	at android.view.View$PerformClick.run(View.java:22729)
03-10 16:42:09.948  8010  8010 D check here: 	at android.os.Handler.handleCallback(Handler.java:751)
03-10 16:42:09.948  8010  8010 D check here: 	at android.os.Handler.dispatchMessage(Handler.java:95)
03-10 16:42:09.948  8010  8010 D check here: 	at android.os.Looper.loop(Looper.java:154)
03-10 16:42:09.948  8010  8010 D check here: 	at android.app.ActivityThread.main(ActivityThread.java:6138)
03-10 16:42:09.948  8010  8010 D check here: 	at java.lang.reflect.Method.invoke(Native Method)
03-10 16:42:09.948  8010  8010 D check here: 	at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:893)
03-10 16:42:09.948  8010  8010 D check here: 	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:783)**

I have passed input with correct dimension, still There is some error when I run tflite using Interpreter


**Describe the expected behavior**

Expected behaviour should load tflite model and provide output values for the provided input
 
**Standalone code to reproduce the issue**
colab tensorflow model : [https://colab.research.google.com/drive/1BI0661FaM6L4oqjNX1MYfHpJCKwKQhjt?usp=sharing](url) 
android Code link : [https://github.com/Adeesh2411/TensorflowAndroidAPP](url)
tflite file can be found in asset folder


"
47696,model.to_json() TypeError: Not JSON Serializable,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, Linux
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.1

Code to reproduce issue:
```python
import tensorflow as tf
import numpy as np

print(f'TensorFlow version={tf.__version__}')
print(f'Numpy version={np.__version__}')

from tensorflow.keras.models import model_from_json
from tensorflow.keras.layers import Layer
from tensorflow.keras import Input, Model

tf.compat.v1.disable_eager_execution()

inputs = Input(shape=(3,))
output = inputs * 2
loss = tf.keras.backend.mean(inputs)
outputs = [output, loss]
model = Model(inputs, outputs)

loss = tf.reduce_mean(loss)
loss = tf.keras.backend.mean(loss)
model.add_loss(loss)

model.compile(optimizer=""adam"", loss=[None] * len(model.outputs))
model.fit(np.random.random((2, 3)))

model_json = model.to_json()  # TypeError: ('Not JSON Serializable:', b'\n\x03mul\x12\x03Mul\x1a\x07input_1\x1a\x05mul/y*\x07\n\x01T\x12\x020\x01')
with open(""model.json"", ""w"") as json_file:
    json_file.write(model_json)
model.save_weights(""model.h5"")

json_file = open('model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
print(""model_from_json ok"")
```

Causes:
```batch
TypeError                                 Traceback (most recent call last)

<ipython-input-2-da2091051454> in <module>()
     24 model.fit(np.random.random((2, 3)))
     25 
---> 26 model_json = model.to_json()  # TypeError: ('Not JSON Serializable:', b'\n\x03mul\x12\x03Mul\x1a\x07input_1\x1a\x05mul/y*\x07\n\x01T\x12\x020\x01')
     27 with open(""model.json"", ""w"") as json_file:
     28     json_file.write(model_json)

4 frames

/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/serialization.py in get_json_type(obj)
     67     return dict(obj)
     68 
---> 69   raise TypeError('Not JSON Serializable:', obj)

TypeError: ('Not JSON Serializable:', b'\n\x03mul\x12\x03Mul\x1a\x07input_1\x1a\x05mul/y*\x07\n\x01T\x12\x020\x01')
```

Versions above 1.14.0 do not cause this in the function .to_json().

Gist: https://gist.github.com/kiflowb777/8fd25c7cde004b1e885d3c29970bb5c5"
47695,Transpose of a Bi-LSTM CRF working fine on TFLite interpreter of Python but throwing error on Android,"System.Exception: 'Java.Lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/transpose.cc:56 op_context->perm->dims->data[0] != dims (3 != 2)
Node number 6 (TRANSPOSE) failed to prepare.
Please help me, how we can debug this error.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: All
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3
- Python version: 3.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

Layer (type)                 Output Shape              Param #   
=================================================================
sequence_input (InputLayer)  [(None, 75)]              0         
_________________________________________________________________
embedding_37 (Embedding)     (None, 75, 20)            703000    
_________________________________________________________________
bidirectional_37 (Bidirectio (None, 75, 100)           28400     
_________________________________________________________________
time_distributed_37 (TimeDis (None, 75, 50)            5050      
_________________________________________________________________
crf_64 (CRF)                 (None, 75, 21)            1554      
=================================================================
Total params: 738,004
Trainable params: 738,004
Non-trainable params: 0"
47694,_PREEMPTION_ERRORS in one worker cause other workers to get stuck in between-graph async distributed training ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.2
- Python version: Python3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

We are using between-graph async distributed training like this:
```python
# https://github.com/bytedance/fedlearner/blob/master/fedlearner/trainer/estimator.py#L237
device_fn = tf.train.replica_device_setter(
    worker_device=""/job:worker/task:%d"" % self._worker_rank,
    merge_devices=True,
    cluster=self._cluster_spec)
cluster_def = self._cluster_spec.as_cluster_def()
local_address = self._cluster_spec.job_tasks('worker')[
    self._worker_rank]
server = tf.train.Server(tf.train.ClusterSpec(
    {'local': {
        0: local_address
    }}),
    job_name='local',
    task_index=0)
target = 'grpc://' + local_address

config = tf.ConfigProto(cluster_def=cluster_def)
config.inter_op_parallelism_threads = 4
config.intra_op_parallelism_threads = 4
config.experimental.share_session_state_in_clusterspec_propagation \
    = True

```

We have 5 workers and 15 PS. Occasionally one worker would fail in session.run with error like this:
```python
2021-03-10 05:03:22,706 [monitored_session.py:1269] INFO An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: From /job:ps/replica:0/task:14:
```
Monitored session would recreate another session and the worker would resume training.

**The problem is other workers would get stuck in session.run and never return. This doesn't happen to every other worker, but just some of them.**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47692,multi-threading on tensorflow models,"Hello;

I have a TensorFlow model in .h5. I call it from a Flask App via /predict

my project consist on predicting the sentiment of each entering comment from tweeter in streaming. 
I want to load the model one time (at the beginning) , then many threads or processes use this model (share it)

I get a Lock error.

How can I fix this bug, how to allow many threads to share the same model at the same time without locking.?

Thanks"
47691,Numpy v1.20+ compatibility,"**System information**
- TensorFlow version (you are using): tf-nightly-gpu v2.5.0-dev20201214
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**
Tensorflow is incompatible with numpy v1.20+ (recently released). This version introduces new functions and changes behaviours.

**Will this change the current api? How?**
There seems to be an issue with the way tensorflow or numpy  handles symbolic tensors

**Who will benefit with this feature?**
Anyone wishing to use the new features or packages where numpy is a dependency. Virtual environments can circumvent this, but sometimes having numpy v1.20 in the same environment as tf can be neecssary or preferable

**Any Other info.**
"
47689,"layers.LocallyConnected2D throws error when saving a model in tf, saving in .h5 works","Hi, tensorflow community

I'm playing around with LocallyConnected2D and found some weird error when I tried to save a model (model.save).

When I gave the layer implementation=1, the model was saved without any errors.
But, if I set implementation=2, it gave me this error.

Traceback (most recent call last):
  File ""/home/tonglab/Documents/Project/PycharmProjects/LCN/LCN_Keras/LocalConn2d_Keras.py"", line 84, in <module>
    model.save('keras1')
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 2002, in save
    signatures, options, save_traces)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 157, in save_model
    signatures, options, save_traces)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py"", line 89, in save
    save_lib.save(model, filepath, signatures, options)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1033, in save
    obj, signatures, options, meta_graph_def)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1198, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1133, in _build_meta_graph_impl
    checkpoint_graph_view)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_serialization.py"", line 75, in find_function_to_export
    functions = saveable_view.list_functions(saveable_view.root)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 151, in list_functions
    self._serialization_cache)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 2613, in _list_functions_for_serialization
    Model, self)._list_functions_for_serialization(serialization_cache)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 3087, in _list_functions_for_serialization
    .list_functions_for_serialization(serialization_cache))
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py"", line 94, in list_functions_for_serialization
    fns = self.functions_to_serialize(serialization_cache)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py"", line 79, in functions_to_serialize
    serialization_cache).functions_to_serialize)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py"", line 95, in _get_serialized_attributes
    serialization_cache)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py"", line 57, in _get_serialized_attributes_internal
    serialization_cache))
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py"", line 104, in _get_serialized_attributes_internal
    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 155, in wrap_layer_functions
    original_fns = _replace_child_layer_functions(layer, serialization_cache)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 274, in _replace_child_layer_functions
    serialization_cache).functions)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py"", line 95, in _get_serialized_attributes
    serialization_cache)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py"", line 104, in _get_serialized_attributes_internal
    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 193, in wrap_layer_functions
    fn.get_concrete_function()
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 549, in get_concrete_function
    self.call_collection.add_trace(*args, **kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 423, in add_trace
    fn.get_concrete_function(*args, **kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 550, in get_concrete_function
    return super(LayerCall, self).get_concrete_function(*args, **kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 1299, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 1205, in _get_concrete_function_garbage_collected
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 726, in _initialize
    *args, **kwds))
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2969, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3206, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 527, in wrapper
    ret = method(*args, **kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 570, in call_and_return_conditional_losses
    call_output = layer_call(inputs, *args, **kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/layers/local.py"", line 615, in call
    self.compute_output_shape(inputs.shape))
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/layers/local.py"", line 782, in local_conv_matmul
    [K.shape(output_flat)[0],] + output_shape.as_list()[1:])
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/backend.py"", line 3020, in reshape
    return array_ops.reshape(x, shape)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 195, in reshape
    result = gen_array_ops.reshape(tensor, shape, name)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 8378, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 540, in _apply_op_helper
    (input_name, err))
ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.

My tensorflow version is 2.4.1.

Any advice will be appreciated.
Thank you."
47687,"Extremely poor data loader performance, taking too much time to load first batch.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not applicable.
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.10
- CUDA/cuDNN version: 11.0
- GPU model and memory: Tesla T4, 15109MiB.

**Describe the current behaviour**
I'm trying to create a custom data loader for training data, where the data has been downloaded from `tensorflow_datasets`,
and a number of data preprocessing have been applied.

But the data loader is taking too much time to load the first batch of each epoch. 

```python
for lrs, hrs in train_data:
    break
```

The code above is taking well over 5 minutes, which means it is taking 5 minutes just to load the first batch. **Though, once the first batch is loaded, then the loading of next batches are faster, and the rest of the batches are being loaded without any issues, each taking not more than 1 second**

**But again once the loop/epoch is over, then starting through the first batch again is taking 5 minutes.**

I've tested the issue with/without `model.fit` function, facing the same issue in both cases.

**Describe the expected behavior**

The loading of the first batch is actually expected to be slow, but not this slow, cuz even the training of the model each batch is not taking this much time.

**Standalone code to reproduce the issue**
Below is the completed code to reproduce the issue.

```python
HR_SIZE = 128
SCALE = 4
BATCH_SIZE = 32

train_data = tfds.load(f'div2k/bicubic_x4', split = 'train')
# this line will download about 4GB of data....
# so using a colab notebook is recommended to reproduce the issue.

@tf.function
def random_crop(example):
    lr = example['lr']
    hr = example['hr']

    lr_crop_size = HR_SIZE // SCALE
    lr_shape = tf.shape(lr)[:2]

    lr_w = tf.random.uniform(shape = (), maxval = lr_shape[1] - lr_crop_size + 1, dtype = tf.int32)
    lr_h = tf.random.uniform(shape = (), maxval = lr_shape[0] - lr_crop_size + 1, dtype = tf.int32)

    hr_w = lr_w * SCALE
    hr_h = lr_h * SCALE

    lr_cropped = lr[lr_h:lr_h + lr_crop_size, lr_w: lr_w + lr_crop_size]
    hr_cropped = hr[hr_h:hr_h + HR_SIZE, hr_w: hr_w + HR_SIZE]

    return lr_cropped, hr_cropped

@tf.function
def random_horizontal_flip(lr, hr):
    return tf.cond(tf.random.uniform(shape = (), maxval = 1) < 0.5,
                   lambda: (lr, hr),
                   lambda: (tf.image.flip_left_right(lr),
                            tf.image.flip_left_right(hr)))

@tf.function
def random_vertical_flip(lr, hr):
    return tf.cond(tf.random.uniform(shape = (), maxval = 1) < 0.5,
                   lambda: (lr, hr),
                   lambda: (tf.image.flip_up_down(lr),
                            tf.image.flip_up_down(hr)))

@tf.function
def random_gain(lr, hr):
    gain_delta = tf.random.uniform(shape = (), minval = 0.7, maxval = 1.6)
    return tf.cond(tf.random.uniform(shape = (), maxval = 1) < 0.5,
                   lambda: (lr, hr),
                   lambda: (tf.image.adjust_gamma(lr, gamma = 1.0, gain = gain_delta),
                            tf.image.adjust_gamma(hr, gamma = 1.0, gain = gain_delta)))

@tf.function
def random_contrast(lr, hr):
    contrast_delta = tf.random.uniform(shape = (), minval = 0.8, maxval = 1.2)
    return tf.cond(tf.random.uniform(shape = (), maxval = 1) < 0.5,
                   lambda: (lr, hr),
                   lambda: (tf.image.adjust_contrast(lr, contrast_factor = contrast_delta),
                            tf.image.adjust_contrast(hr, contrast_factor = contrast_delta)))

@tf.function
def random_rotate(lr, hr):
    rn = tf.random.uniform(shape = (), maxval = 4, dtype=tf.int32)
    return tf.image.rot90(lr, rn), tf.image.rot90(hr, rn)

train_data = train_data.map(random_crop, num_parallel_calls = tf.data.AUTOTUNE)
train_data = train_data.map(random_horizontal_flip, num_parallel_calls = tf.data.AUTOTUNE)
train_data = train_data.map(random_vertical_flip, num_parallel_calls = tf.data.AUTOTUNE)
train_data = train_data.map(random_gain, num_parallel_calls = tf.data.AUTOTUNE)
train_data = train_data.map(random_contrast, num_parallel_calls = tf.data.AUTOTUNE)
train_data = train_data.map(random_rotate, num_parallel_calls = tf.data.AUTOTUNE)

train_data = train_data.batch(BATCH_SIZE, drop_remainder = True)
train_data = train_data.prefetch(tf.data.AUTOTUNE)
train_data = train_data.shuffle(800)

for lrs, hrs in train_data:
    break
```

And I'm running all of my code in google colab."
47686,SavedModel file does not exist error when running Image classification with TensorFlow Lite Model Maker tutorial,"**System information**
I am just following instructions found on the tutorial page: https://www.tensorflow.org/lite/tutorials/model_maker_image_classification

OS:  windows 10 home

tensorflow lite model maker installed via pip install tflite-model-maker
print(tf.version.GIT_VERSION, tf.version.VERSION):  v2.4.0-49-g85c8b2a817f 2.4.1

Python 3.8.6

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020
Cuda compilation tools, release 11.0, V11.0.194
Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0

**Describe the current behavior**
when executing 

model = image_classifier.create(train_data)

error ending with the following occur:

....

 File ""c:\Users\raywl\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\saved_model\loader_impl.py"", line 111, in parse_saved_model
    raise IOError(""SavedModel file does not exist at: %s/{%s|%s}"" %
OSError: SavedModel file does not exist at: C:\Users\raywl\AppData\Local\Temp\tfhub_modules\87f7b0d2504a48175f521bcaed174acabc93672c/{saved_model.pbtxt|saved_model.pb}

**Describe the expected behavior**

INFO:tensorflow:Retraining the models...
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
hub_keras_layer_v1v2 (HubKer (None, 1280)              3413024   
......

**Standalone code to reproduce the issue**

import os

import numpy as np

import tensorflow as tf
assert tf.__version__.startswith('2')

from tflite_model_maker import configs
from tflite_model_maker import ExportFormat
from tflite_model_maker import image_classifier
from tflite_model_maker import ImageClassifierDataLoader
from tflite_model_maker import model_spec

import matplotlib.pyplot as plt

image_path = tf.keras.utils.get_file(
      'flower_photos.tgz',
      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
      extract=True)
image_path = os.path.join(os.path.dirname(image_path), 'flower_photos')

data = ImageClassifierDataLoader.from_folder(image_path)
train_data, test_data = data.split(0.9)
model = image_classifier.create(train_data)

*error hit*

attached is the full screen dump of the repeatable step until the error. 

I check the path 

C:\Users\raywl\AppData\Local\Temp\tfhub_modules\87f7b0d2504a48175f521bcaed174acabc93672c

and found it to only contain 2 empty folders assets and variables (see screen capture sc_tfhub.png)

I am also able to complete this colab tutorial on chrome without issue (https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb#scrollTo=6cv3K3oaksJv)

Please advise how I can troubleshoot this issue.


<img width=""576"" alt=""sc_tfhub"" src=""https://user-images.githubusercontent.com/10432466/110570140-6f72cb80-8190-11eb-9bcd-a2548c5032f2.PNG"">


[fullScreenDumpMM.txt](https://github.com/tensorflow/tensorflow/files/6112868/fullScreenDumpMM.txt)


Thanks
Raymond
"
47685,Random seed produces different results for different TF versions,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSx 11.2.1
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.7

**Describe the current behavior**
I'm not sure whether this qualifies as a bug but I'm pretty sure this is not the intended behavior. I'm trying to get the same results using tf 1.15 and tf 2.4.1. Please find the 2 examples below which I'm expecting to produce similar results. The first example using `tf.layers.conv2d()` and the second using `tf.keras.layers.Conv2D()`. I tried using `kernel_initializer=some_tf_initializer(seed=seed)` and I also tried without using a kernel initializer, still the same. 

**Describe the expected behavior**

Similar results.

**Standalone code to reproduce the issue** Here's a colab [notebook](https://colab.research.google.com/drive/1PH5ILC1CI1-qSqvvjmwfOP629-QJ-4Hs?usp=sharing) that contains the same code below.

**TF1**

    import tensorflow as tf
    import numpy as np
    
    
    if __name__ == '__main__':
        seed = 1
        np.random.seed(seed)
        tf.set_random_seed(seed)
        session = tf.InteractiveSession()
        initializer = tf.initializers.orthogonal(1.0, seed)
        x = np.random.random((2, 84, 84, 1))
        xp = tf.placeholder(x.dtype, x.shape)
        result = tf.layers.conv2d(xp, 32, 8, 4, kernel_initializer=initializer)
        session.run(tf.global_variables_initializer())
        print(f'Conv2D output:\n{session.run([result], {xp: x})}\n{100 * ""=""}')
        print(f'x:\n{x}')


**Results in**

    Conv2D output:
    [array([[[[-4.92592295e-01,  5.84946930e-01,  6.45957303e-01, ...,
               3.65328020e-01,  4.03869755e-01, -1.06886940e+00],
             [ 5.37549724e-02,  5.47611947e-01,  3.91425858e-01, ...,
               1.20702194e-01,  9.21035825e-02, -1.10599980e+00],
             [-4.68301348e-01,  8.54547125e-01,  2.99738041e-01, ...,
              -7.47656723e-02, -3.29838560e-01, -6.43979360e-01],
             ...,
             [-3.61929553e-01,  5.70854964e-01,  1.49096153e-01, ...,
               4.05001572e-01, -5.70764458e-01, -8.50054931e-01],
             [-5.23792632e-01,  5.79328891e-01,  4.36402398e-01, ...,
               4.66264733e-01, -8.00687780e-02, -1.05017933e+00],
             [-9.59200260e-01,  9.77023018e-01,  7.22006476e-01, ...,
               3.29199395e-01, -6.48041695e-02, -9.65927090e-01]],
    
            [[-3.05192775e-01,  5.62304495e-01,  2.66985564e-01, ...,
               4.01866526e-01, -5.27853938e-01, -1.36294176e+00],
             [-3.33627733e-01,  8.70909716e-01, -1.90685411e-01, ...,
               5.82781510e-01, -9.54447222e-04, -5.93152081e-01],
             [ 1.05312097e-01,  1.34995604e+00,  4.18403345e-01, ...,
               7.29398371e-01, -1.52729852e-01, -1.44087877e+00],
             ...,
             [-2.65877698e-01,  1.30702457e+00,  2.12075863e-01, ...,
               4.02235128e-01, -4.83213829e-02, -9.84811507e-01],
             [-2.33065665e-01,  8.06240360e-01,  2.04250988e-01, ...,
               5.11907848e-01, -2.97176027e-01, -5.89994050e-01],
             [-6.06616196e-01,  7.52128441e-01,  6.05931022e-01, ...,
               8.25940123e-01, -8.11965401e-01, -1.38039418e+00]],
    
            [[-1.14268620e-02,  9.44562211e-01,  7.30843200e-01, ...,
               7.71483717e-01, -3.62299259e-01, -5.19022458e-01],
             [-1.25248650e-01,  8.31288483e-01,  3.01203507e-01, ...,
               6.10087249e-02, -1.37678504e-01, -1.12305468e+00],
             [-2.37495350e-01,  5.80913482e-01,  2.48314393e-01, ...,
               6.06333851e-01, -3.30380816e-01, -1.00515721e+00],
             ...,
             [ 1.95822525e-01,  9.85462620e-01,  9.30753642e-02, ...,
               5.07316387e-01, -2.72563623e-01, -3.44717273e-01],
             [-7.58518148e-01,  7.18211754e-01,  3.56392246e-01, ...,
               5.41345861e-01, -2.84327606e-01, -5.78967512e-01],
             [-9.36528091e-01,  4.51066108e-01,  3.60469945e-01, ...,
               6.06252149e-01, -9.82606865e-02, -1.12673980e+00]],
    
            ...,
    
            [[-3.23906425e-01,  4.89133765e-01,  5.51718357e-01, ...,
              -1.33718077e-01,  1.23015200e-03, -5.54514943e-01],
             [-5.04556877e-01,  7.36618066e-01,  2.02579467e-01, ...,
              -3.74142562e-01,  6.00494771e-03, -1.31111289e+00],
             [-4.16236220e-01,  5.98196128e-01,  6.97848461e-01, ...,
               6.23122747e-01, -1.07252840e-01, -4.83301913e-01],
             ...,
             [ 1.06692401e-01,  1.73388947e+00,  3.22292122e-01, ...,
              -1.75720933e-01, -2.96764992e-01, -1.06044579e+00],
             [-5.62972482e-01,  1.36246077e+00,  9.13179694e-01, ...,
               6.32824517e-01, -4.73498606e-01, -9.47552266e-01],
             [-5.11697389e-01,  6.66774542e-01,  3.82947609e-01, ...,
               5.67110785e-01, -7.11579248e-03, -7.77461939e-01]],
    
            [[-1.11410557e-01,  1.10325702e+00,  5.50558807e-01, ...,
               5.51697720e-01, -3.75442814e-01, -4.68653786e-01],
             [-3.26447172e-01,  1.19972335e+00,  3.82313314e-01, ...,
               2.20346417e-01, -3.39678412e-01, -8.66913575e-01],
             [-3.98474415e-01,  7.14095329e-01,  8.04559140e-02, ...,
               1.19488448e-01, -2.52660129e-01, -9.91879947e-01],
             ...,
             [-5.70385227e-01,  8.52797253e-01,  4.66251675e-01, ...,
              -3.24400644e-01,  3.20423484e-03, -1.39903236e+00],
             [-2.07675682e-01,  6.29120258e-01,  6.79053796e-01, ...,
               1.07336154e-01,  1.87886060e-01, -3.86794041e-01],
             [-2.67700849e-02,  4.49128504e-01,  3.40760390e-01, ...,
               6.36000637e-01, -3.77890278e-01, -3.30207391e-01]],
    
            [[-3.10700139e-01,  8.18823907e-01,  3.67007768e-03, ...,
               4.90558846e-01, -8.36860336e-01, -1.03869365e+00],
             [-4.88181365e-01,  8.00080028e-01,  3.56097390e-01, ...,
               4.76738039e-01, -3.42056012e-01, -5.30593649e-01],
             [-4.60314405e-01,  7.07439805e-01,  4.22496599e-01, ...,
               5.05574451e-01, -1.24705048e-01, -2.87646769e-01],
             ...,
             [-2.57950491e-01,  1.20126622e+00,  5.56477074e-02, ...,
               6.11127028e-01, -1.93377726e-01, -1.07765356e+00],
             [-6.50455755e-01,  1.29336304e+00,  8.44637456e-01, ...,
               1.81637465e-01, -2.88107846e-01, -4.49444781e-01],
             [-4.21331048e-01,  3.20901642e-01,  8.83963968e-01, ...,
               8.65426315e-01, -4.92428131e-01, -8.49800993e-01]]],
    
    
           [[[-5.16387221e-01,  6.99757393e-01,  7.85561831e-01, ...,
               1.03644383e-01, -3.70012470e-01, -5.32289379e-01],
             [-7.05045607e-02,  6.10474017e-01,  3.49680420e-01, ...,
               7.92201453e-01, -5.39308419e-01, -3.42154387e-01],
             [-4.22583634e-01,  8.01291482e-01,  1.48925846e-01, ...,
               7.68131504e-01, -7.38695500e-01, -1.09516989e+00],
             ...,
             [-7.26389962e-01,  6.26188865e-01,  5.42793169e-01, ...,
               3.52803865e-01, -3.46388144e-01, -7.60162696e-01],
             [-9.04680334e-02,  1.75359623e-01,  8.00145598e-01, ...,
               2.51379785e-01, -2.79905131e-01, -8.72223633e-01],
             [-4.20990087e-01,  3.09422635e-01,  4.11481559e-01, ...,
               3.69259084e-02, -1.60866470e-01, -5.05859647e-01]],
    
            [[-4.38932989e-01,  9.72496331e-01,  2.50341307e-01, ...,
               2.83248632e-01, -3.26580435e-01, -1.03773839e+00],
             [-8.19208455e-01,  8.96083502e-01,  4.24397325e-01, ...,
              -5.64658536e-02, -6.09974865e-01, -1.23144756e+00],
             [-9.45132686e-01,  9.13248242e-01,  8.86299832e-01, ...,
               2.15502049e-01, -5.58705913e-01, -2.25369791e-01],
             ...,
             [-8.56722975e-01,  7.94114365e-01,  5.81065297e-01, ...,
               4.96281428e-01, -1.03129758e+00, -1.05092158e+00],
             [-2.82233182e-02,  9.48533077e-01,  6.68768609e-01, ...,
               5.78499983e-01, -6.95651561e-01, -9.76414384e-01],
             [-8.06302266e-01,  1.05147010e+00,  4.22578075e-01, ...,
               2.94475123e-01,  1.74168406e-01, -1.14952206e+00]],
    
            [[-3.06847178e-01,  9.29719098e-01,  2.68849689e-01, ...,
               3.96790007e-01, -3.59629268e-01, -9.25076133e-01],
             [-2.38372207e-01,  5.78896860e-01,  2.17575482e-01, ...,
              -6.97642069e-02, -2.63976905e-01, -9.10845525e-01],
             [-5.40468423e-01,  1.20147694e+00,  4.04620974e-01, ...,
               1.76649501e-01,  1.46967870e-01, -6.74134783e-01],
             ...,
             [-6.27551970e-01,  5.05884731e-01,  2.10479188e-01, ...,
               3.09780840e-01,  1.46682047e-01, -1.07307698e+00],
             [-6.44653887e-01,  8.90779216e-01,  3.82747674e-01, ...,
               1.72155001e-01, -4.48190676e-01, -8.62522695e-01],
             [-1.50221285e-01,  9.35658435e-01,  3.59703911e-01, ...,
              -8.34232530e-02, -1.89649715e-01, -7.73705172e-01]],
    
            ...,
    
            [[-3.47146995e-01,  6.34597952e-02,  4.73544353e-01, ...,
              -8.20012972e-02, -1.78752555e-01, -1.36296041e+00],
             [-8.95298221e-01,  8.66479595e-01,  5.61457267e-01, ...,
               2.22826074e-01, -4.86448503e-01, -8.56547191e-01],
             [-8.62118822e-01,  4.83077034e-01,  3.60908309e-02, ...,
               3.29259093e-01, -4.08073639e-02, -6.45683881e-01],
             ...,
             [-5.01558985e-01,  4.62753228e-01,  4.01966678e-01, ...,
               6.35652593e-01, -7.45519465e-02, -5.39741380e-01],
             [ 1.29982837e-01,  7.31941977e-01, -2.05750614e-01, ...,
               3.16325608e-01, -3.28495177e-01, -1.09927128e+00],
             [-4.90335504e-01,  4.94757973e-01,  9.40801327e-02, ...,
               5.66634378e-02, -7.30613447e-01, -7.25730750e-01]],
    
            [[-2.75066335e-01,  7.35209409e-01,  5.99839688e-01, ...,
               6.71254510e-02,  7.97677772e-02, -7.72461196e-01],
             [-1.58777502e-01,  7.51857910e-01,  3.80584693e-01, ...,
              -1.01868390e-01, -7.12568409e-02, -6.42932271e-01],
             [-1.45951405e-01,  1.03692197e+00,  4.91333873e-01, ...,
               2.98796942e-01, -5.46416283e-01, -1.04881221e+00],
             ...,
             [-9.75652891e-03,  8.28500896e-01,  3.48450207e-01, ...,
               4.07241092e-01, -2.34265134e-01, -5.27081486e-01],
             [-5.65917326e-01,  7.37496827e-01,  1.65005917e-01, ...,
               6.61606291e-01, -2.20420580e-01, -1.11307865e+00],
             [-2.21167732e-01,  4.83734785e-01,  4.59793140e-01, ...,
               4.19304500e-01, -2.96987262e-01, -2.01353157e-01]],
    
            [[-6.33535626e-01,  1.11787318e+00,  6.41380845e-01, ...,
               1.12652086e-01, -1.99377096e-02, -6.69645437e-01],
             [-1.05662073e-01,  3.87718948e-01,  4.30142659e-01, ...,
               3.27637078e-01, -3.59568870e-01, -9.63431155e-01],
             [-1.26793132e-01,  1.29664648e+00,  3.28922837e-01, ...,
               3.21313848e-01, -7.53446525e-01, -7.93674733e-01],
             ...,
             [-5.32937597e-01,  1.09915270e+00,  6.23443352e-01, ...,
               9.96585500e-01, -6.21343220e-01, -1.01232184e+00],
             [ 5.05322966e-02,  1.18874480e+00,  4.57358272e-01, ...,
               4.80935716e-01, -2.04122013e-01, -1.13864994e+00],
             [-1.03084733e-01,  1.14916096e+00,  2.73508528e-01, ...,
               6.76093153e-01, -3.34324702e-01, -1.28436283e+00]]]])]
    ====================================================================================================
    x:
    [[[[4.17022005e-01]
       [7.20324493e-01]
       [1.14374817e-04]
       ...
       [6.23672207e-01]
       [7.50942434e-01]
       [3.48898342e-01]]
    
      [[2.69927892e-01]
       [8.95886218e-01]
       [4.28091190e-01]
       ...
       [1.85762022e-02]
       [7.00221437e-02]
       [4.86345111e-01]]
    
      [[6.06329462e-01]
       [5.68851437e-01]
       [3.17362409e-01]
       ...
       [9.18601778e-01]
       [4.02024891e-04]
       [9.76759149e-01]]
    
      ...
    
      [[5.89549934e-01]
       [3.89137609e-01]
       [5.05975232e-01]
       ...
       [4.35888475e-01]
       [7.89075202e-01]
       [4.66467704e-01]]
    
      [[6.73554921e-01]
       [8.84836452e-01]
       [9.38138449e-01]
       ...
       [7.93970466e-01]
       [2.13784215e-01]
       [6.41105035e-01]]
    
      [[7.31134736e-01]
       [9.50619892e-02]
       [7.00729238e-02]
       ...
       [9.95522026e-01]
       [4.81429517e-01]
       [8.37812754e-01]]]
    
    
     [[[6.03655452e-01]
       [6.64374944e-01]
       [2.72461392e-01]
       ...
       [1.14069927e-01]
       [2.93705095e-01]
       [8.78904978e-03]]
    
      [[2.53263696e-01]
       [8.37712781e-01]
       [8.07756027e-01]
       ...
       [7.37152445e-01]
       [6.00521471e-01]
       [7.37999367e-01]]
    
      [[3.75760828e-01]
       [9.11106703e-01]
       [8.72308594e-01]
       ...
       [9.80268932e-01]
       [1.13198035e-01]
       [4.65678949e-01]]
    
      ...
    
      [[4.94241516e-02]
       [6.34450548e-01]
       [8.93053413e-01]
       ...
       [7.97760317e-01]
       [3.18871974e-01]
       [6.47314782e-01]]
    
      [[4.65136696e-01]
       [5.07669096e-01]
       [4.23295851e-01]
       ...
       [2.98177206e-01]
       [5.32380132e-01]
       [6.12348886e-01]]
    
      [[2.58528146e-01]
       [5.20561003e-02]
       [7.82628170e-01]
       ...
       [8.26242775e-03]
       [7.43071396e-01]
       [3.29652868e-01]]]]

**TF2**

    import tensorflow as tf
    import numpy as np
    from tensorflow.keras.layers import Conv2D
    
    
    if __name__ == '__main__':
        seed = 1
        np.random.seed(seed)
        tf.random.set_seed(seed)
        x = np.random.random((2, 84, 84, 1))
        initializer = tf.initializers.Orthogonal(1.0, seed)
        print(f'Conv2D output:\n{Conv2D(32, 8, 4, kernel_initializer=initializer)(x)}\n{100 * ""=""}')
        print(f'x:\n{x}')

**Results in**

        Conv2D output:
    [[[[-5.41361034e-01  3.66543412e-01 -1.51497812e-03 ... -5.08555949e-01
        -2.81920075e-01  2.03241315e-02]
       [-5.59882522e-01  4.86802310e-01 -6.99946359e-02 ... -2.40252942e-01
        -3.65630835e-01 -3.15424293e-01]
       [-9.42213356e-01  2.15654269e-01 -1.55082747e-01 ... -4.04906332e-01
        -2.01114044e-01  7.04537705e-02]
       ...
       [-3.46490562e-01  3.19557160e-01 -2.92775959e-01 ... -4.28674221e-01
         2.79810458e-01  4.20070797e-01]
       [-3.45766425e-01  1.90711677e-01 -1.57374702e-02 ... -5.71854293e-01
         4.97741327e-02 -1.16687842e-01]
       [-9.33221936e-01 -1.68129373e-02  1.63893417e-01 ... -4.79621142e-01
        -2.14530781e-01  5.61090052e-01]]
    
      [[-7.58794546e-01  5.37562370e-01 -4.74378794e-01 ... -5.63471198e-01
        -2.08135564e-02  3.88803691e-01]
       [-4.21465009e-01  1.65287077e-01 -4.43225324e-01 ... -1.72018170e-01
        -7.92833045e-02  3.92330065e-02]
       [-4.81306076e-01  3.66574019e-01 -3.64440233e-01 ... -5.52142262e-01
        -2.36726597e-01 -3.12441528e-01]
       ...
       [-8.76132190e-01  3.22705477e-01 -1.90438583e-01 ... -6.52492762e-01
        -6.05543517e-02  3.59102860e-02]
       [-7.17206061e-01  1.32312387e-01 -4.24121648e-01 ...  1.82857260e-01
         1.84026435e-01 -1.85313985e-01]
       [-1.20653558e+00  4.54987772e-02 -2.89574206e-01 ... -3.28905612e-01
         3.50747108e-02  1.20641785e-02]]
    
      [[-5.34970939e-01  7.13559866e-01 -4.85820472e-01 ... -4.59470123e-01
         3.37272674e-01  1.63189009e-01]
       [-7.05033600e-01  3.56267929e-01 -4.30038758e-02 ... -5.83572984e-01
        -1.40957370e-01 -2.53099173e-01]
       [-5.99712431e-01  7.10705340e-01 -3.39112192e-01 ... -4.14105803e-01
         1.87255502e-01 -3.33267421e-01]
       ...
       [-5.80564499e-01  4.36445504e-01  3.36502224e-01 ...  1.45576835e-01
         3.74677926e-01 -3.91695678e-01]
       [-7.22398818e-01  3.15642595e-01 -2.88131565e-01 ... -3.45985293e-01
         2.34268203e-01  3.29410911e-01]
       [-7.46637762e-01  2.26648629e-01 -1.73913926e-01 ... -1.78611025e-01
         7.35538006e-02  2.55084813e-01]]
    
      ...
    
      [[-5.49425662e-01 -3.49880159e-02  4.77346703e-02 ... -4.04859513e-01
         3.71269658e-02 -1.85562521e-02]
       [-8.26125443e-01  1.29296139e-01 -2.88902789e-01 ... -8.50088000e-01
        -5.04021049e-01 -4.97736454e-01]
       [-7.37619460e-01  5.00219822e-01  7.76109993e-02 ... -1.41733378e-01
         2.81554639e-01  3.69791657e-01]
       ...
       [-3.06938559e-01  4.47962463e-01 -2.05980629e-01 ... -8.48388433e-01
        -1.59557834e-01  3.17021906e-02]
       [-3.87109697e-01  9.46836114e-01 -2.71658182e-01 ... -6.23931110e-01
        -2.16598317e-01 -1.88935712e-01]
       [-1.31429803e+00  3.86178017e-01 -6.20340466e-01 ... -3.68769616e-02
         1.81194678e-01 -1.06644318e-01]]
    
      [[-7.55118549e-01 -1.59330755e-01 -1.55888349e-01 ... -8.46641809e-02
         5.47275543e-02 -3.14989388e-01]
       [-7.29794323e-01  1.63924411e-01  5.99410906e-02 ...  1.31567806e-01
        -4.67930377e-01  2.95255750e-01]
       [-6.91133916e-01  5.20268269e-02 -7.29867145e-02 ... -1.88851178e-01
         4.07196075e-01  4.01087821e-01]
       ...
       [-5.79473913e-01  7.94014513e-01 -7.01249093e-02 ... -6.76721931e-01
        -1.01779945e-01 -5.97994268e-01]
       [-6.84648752e-01  1.01270474e-01 -4.07643348e-01 ...  3.85033749e-02
         8.26148912e-02  5.69675528e-02]
       [-4.62479711e-01  4.19429392e-01  1.77284703e-04 ... -1.42741054e-01
         3.54655176e-01  2.56564021e-01]]
    
      [[-2.46302426e-01  7.49420345e-01  8.12041853e-03 ... -3.32499892e-01
         1.33527100e-01  6.35570288e-02]
       [-9.58327591e-01  1.98065817e-01  1.93995520e-01 ... -2.94785231e-01
         2.52389193e-01 -1.20501839e-01]
       [-8.17039490e-01  2.07326993e-01 -5.11587203e-01 ... -4.24753636e-01
         2.88008928e-01  5.03057897e-01]
       ...
       [-7.03616858e-01  3.78316432e-01 -5.60314238e-01 ... -4.05634254e-01
        -3.88694972e-01 -1.93394765e-01]
       [-6.37941658e-01  4.12149638e-01  4.30523872e-01 ... -1.07135192e-01
        -4.06805426e-01  2.13325843e-01]
       [-1.10811067e+00  1.87466890e-01 -4.25169766e-01 ... -3.90125453e-01
        -2.32264772e-01 -1.13592006e-01]]]
    
    
     [[[-1.12212503e+00  4.90197897e-01 -1.32902622e-01 ... -2.71363735e-01
         5.97870303e-03 -1.82653069e-01]
       [-5.94440103e-01 -2.69649085e-02 -8.82392377e-02 ... -2.57950276e-01
        -5.64375184e-02  3.79566044e-01]
       [-9.26148295e-01  7.57718146e-01 -1.35923713e-01 ... -3.23437661e-01
         4.14071977e-01  1.71241298e-01]
       ...
       [-7.43670583e-01  5.69033444e-01  8.93335044e-02 ... -4.33211058e-01
         9.38811079e-02 -2.02697456e-01]
       [-7.01980829e-01  8.19573849e-02  1.15745321e-01 ... -3.05091172e-01
         2.67599583e-01  4.00184661e-01]
       [-1.23418115e-01 -7.21083656e-02 -2.83473641e-01 ... -5.09245217e-01
         1.67642578e-01  3.97953391e-02]]
    
      [[-7.82243788e-01  4.62321520e-01 -1.03323981e-01 ... -6.02696836e-01
         1.12790830e-01  6.60246331e-03]
       [-1.18653631e+00  1.10369611e+00  4.03279841e-01 ... -3.35117340e-01
         1.34733140e-01  1.24090314e-01]
       [-7.56276011e-01  4.68323886e-01  2.55332291e-01 ... -5.64108253e-01
        -1.23212464e-01  9.44344103e-02]
       ...
       [-1.07282424e+00  4.46680576e-01  2.62999147e-01 ... -4.93109912e-01
         1.20379120e-01  2.00786784e-01]
       [-5.91401279e-01 -4.76840436e-02 -6.31224453e-01 ... -1.33297965e-02
        -6.85213029e-01  3.06965083e-01]
       [-1.26961339e+00  4.00966257e-01  1.58335969e-01 ... -4.71471280e-01
        -5.21351993e-01 -2.68614203e-01]]
    
      [[-7.59719312e-01 -2.15684757e-01 -5.51491380e-01 ... -4.05447990e-01
         2.04430401e-01 -4.03630167e-01]
       [-7.50933051e-01 -2.70830095e-01 -4.10136461e-01 ... -2.66372442e-01
        -4.25907433e-01  2.79703408e-01]
       [-1.15558457e+00  1.44730762e-01 -1.11366399e-01 ... -3.16568702e-01
        -1.65987372e-01 -2.50013694e-02]
       ...
       [-1.26355612e+00  1.83538213e-01  2.64952302e-01 ... -4.03630674e-01
        -1.93098515e-01  3.03720146e-01]
       [-1.04506040e+00  9.86333251e-01 -4.76556689e-01 ... -5.65628707e-01
         2.07612868e-02  7.63047487e-02]
       [-1.07622218e+00  2.96826065e-01 -5.04598498e-01 ... -4.09753144e-01
        -2.73265153e-01  6.33615702e-02]]
    
      ...
    
      [[-1.13511002e+00  2.11719826e-01 -8.60134482e-01 ... -3.91803771e-01
        -6.59739316e-01 -2.00912848e-01]
       [-1.29758418e+00  2.52690554e-01 -3.54235232e-01 ... -4.23384011e-01
        -5.52129559e-02 -1.58944473e-01]
       [-1.17037618e+00  3.93378854e-01 -2.55860239e-01 ... -1.89175576e-01
        -3.52438241e-01  4.47871268e-01]
       ...
       [-6.99247956e-01 -4.01002228e-01 -9.55614746e-02 ... -5.12549996e-01
        -2.93750733e-01  6.53216466e-02]
       [-7.55659997e-01  6.40419498e-02 -3.80645603e-01 ... -3.38304751e-02
        -2.81535774e-01  3.27474415e-01]
       [-8.98592114e-01  6.04611039e-01 -2.70986766e-01 ... -3.77177030e-01
         6.39083028e-01  1.36269689e-01]]
    
      [[-2.06768334e-01  4.33980107e-01 -2.79342651e-01 ... -5.97317517e-01
        -3.16711366e-01 -3.49702388e-02]
       [-8.91763270e-01  5.25228903e-02 -1.25209495e-01 ... -2.35961720e-01
        -8.85192119e-03  2.04528570e-01]
       [-6.51450276e-01 -3.80111784e-01 -7.60694802e-01 ... -1.16580009e+00
         4.66391236e-01 -1.49288952e-01]
       ...
       [-8.66602838e-01 -5.96193299e-02 -1.27994597e-01 ... -4.30833064e-02
         1.16992146e-01 -6.62802998e-03]
       [-9.14786756e-01  1.67500958e-01 -2.82980531e-01 ... -6.82374537e-01
        -1.63795985e-02  1.82587206e-01]
       [-5.49697280e-01  6.72760680e-02 -7.42559731e-02 ... -1.59981385e-01
         1.14814062e-02  2.45045304e-01]]
    
      [[-6.60416663e-01  4.52319860e-01  1.04573436e-01 ... -8.84432197e-01
        -1.99507281e-01 -5.85323572e-02]
       [-5.59800625e-01  4.29061443e-01  6.59240410e-02 ... -2.68544644e-01
        -9.68336090e-02  1.81482792e-01]
       [-5.66595554e-01  7.11271226e-01 -4.41362828e-01 ... -4.43202615e-01
         8.01286697e-01 -1.05126597e-01]
       ...
       [-7.27143824e-01  4.69138294e-01 -3.05642545e-01 ... -3.89702499e-01
        -9.79036614e-02  3.28408331e-01]
       [-3.61066341e-01  4.04266417e-01 -2.91205943e-01 ...  5.48217893e-02
        -9.46037620e-02 -1.06996156e-01]
       [-6.50965929e-01  5.33826649e-01 -3.92165482e-01 ... -2.94952631e-01
        -8.13799322e-01  1.03757977e-01]]]]
    ====================================================================================================
    x:
    [[[[4.17022005e-01]
       [7.20324493e-01]
       [1.14374817e-04]
       ...
       [6.23672207e-01]
       [7.50942434e-01]
       [3.48898342e-01]]
    
      [[2.69927892e-01]
       [8.95886218e-01]
       [4.28091190e-01]
       ...
       [1.85762022e-02]
       [7.00221437e-02]
       [4.86345111e-01]]
    
      [[6.06329462e-01]
       [5.68851437e-01]
       [3.17362409e-01]
       ...
       [9.18601778e-01]
       [4.02024891e-04]
       [9.76759149e-01]]
    
      ...
    
      [[5.89549934e-01]
       [3.89137609e-01]
       [5.05975232e-01]
       ...
       [4.35888475e-01]
       [7.89075202e-01]
       [4.66467704e-01]]
    
      [[6.73554921e-01]
       [8.84836452e-01]
       [9.38138449e-01]
       ...
       [7.93970466e-01]
       [2.13784215e-01]
       [6.41105035e-01]]
    
      [[7.31134736e-01]
       [9.50619892e-02]
       [7.00729238e-02]
       ...
       [9.95522026e-01]
       [4.81429517e-01]
       [8.37812754e-01]]]
    
    
     [[[6.03655452e-01]
       [6.64374944e-01]
       [2.72461392e-01]
       ...
       [1.14069927e-01]
       [2.93705095e-01]
       [8.78904978e-03]]
    
      [[2.53263696e-01]
       [8.37712781e-01]
       [8.07756027e-01]
       ...
       [7.37152445e-01]
       [6.00521471e-01]
       [7.37999367e-01]]
    
      [[3.75760828e-01]
       [9.11106703e-01]
       [8.72308594e-01]
       ...
       [9.80268932e-01]
       [1.13198035e-01]
       [4.65678949e-01]]
    
      ...
    
      [[4.94241516e-02]
       [6.34450548e-01]
       [8.93053413e-01]
       ...
       [7.97760317e-01]
       [3.18871974e-01]
       [6.47314782e-01]]
    
      [[4.65136696e-01]
       [5.07669096e-01]
       [4.23295851e-01]
       ...
       [2.98177206e-01]
       [5.32380132e-01]
       [6.12348886e-01]]
    
      [[2.58528146e-01]
       [5.20561003e-02]
       [7.82628170e-01]
       ...
       [8.26242775e-03]
       [7.43071396e-01]
       [3.29652868e-01]]]]

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47684,tf.Variable shape is restricted with copy.deepcopy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8
- CUDA/cuDNN version: ???
- GPU model and memory: GTX 1070

**Describe the current behavior**
`tf.Variable` shape is restricted when using `copy.deepcopy`
```python
>>> x = tf.Variable([[1.2, 3.4]], shape=[None, 2])
>>> x.shape
TensorShape([None, 2])
>>> import copy
>>> copy.deepcopy(x).shape
TensorShape([5, 2])
```

**Describe the expected behavior**
copy has the same shape as original."
47683,"SentencePieceTokenizer.detokenize fails on TPU with ""No registered 'SentencepieceDetokenizeOp' OpKernel for XLA_TPU_JIT devices""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, included below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Collab with TPU
- TensorFlow installed from (source or binary): (pre-installed)
- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1
- Python version: Python 3.7.10

**Describe the current behavior**

Running tensorflow_text.SentencepieceTokenizer.detokenize on a TPU gives ""No registered 'SentencepieceDetokenizeOp' OpKernel for XLA_TPU_JIT devices""

**Describe the expected behavior**

Running tensorflow_text.SentencepieceTokenizer.detokenize on a TPU succeeds, just like on a CPU or GPU device

**Standalone code to reproduce the issue**

```
!pip install tensorflow-text
!pip install sentencepiece

!python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
!python --version

import tensorflow as tf
import tensorflow_text as tf_text
import sentencepiece

resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)

tokenizer_prefix = ""/tmp/tokenizer""
corpus_path = tokenizer_prefix + "".txt""
model_path = tokenizer_prefix + "".model""

with open(corpus_path, ""w"") as f: 
  content = f.write(""a b c d e"")

sentencepiece.SentencePieceTrainer.Train(input=corpus_path, model_prefix=tokenizer_prefix, vocab_size=9, character_coverage=1.0)

with tf.device(""/TPU:0""):
  tokenizer = tf_text.SentencepieceTokenizer(model=tf.io.gfile.GFile(model_path, ""rb"").read(), add_eos=True)
  @tf.function
  def tf_detokenize(input):
    return tokenizer.detokenize(input)
  print(tf_detokenize(tf.constant([1, 2, 3, 4, 5], dtype=tf.int32)))
```

**Other info / logs**

```
InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_tf_detokenize_280}} = __inference_tf_detokenize_280[_XlaMustCompile=true, config_proto=""\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001\202\001\000"", executor_type=""""](dummy_input, dummy_input).
Uncompilable nodes:
SentenceTokenizer/SentenceTokenizer/SentenceTokenizer/SentencepieceDetokenizeOp: unsupported op: No registered 'SentencepieceDetokenizeOp' OpKernel for XLA_TPU_JIT devices compatible with node {{node SentenceTokenizer/SentenceTokenizer/SentenceTokenizer/SentencepieceDetokenizeOp}}
	Stacktrace:
		Node: __inference_tf_detokenize_280, function: 
		Node: SentenceTokenizer/SentenceTokenizer/SentenceTokenizer/SentencepieceDetokenizeOp, function: __inference_tf_detokenize_280

SentenceTokenizer/Reshape: unsupported op: No registered 'Reshape' OpKernel for XLA_TPU_JIT devices compatible with node {{node SentenceTokenizer/Reshape}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, Tshape=DT_INT32
	Stacktrace:
		Node: __inference_tf_detokenize_280, function: 
		Node: SentenceTokenizer/Reshape, function: __inference_tf_detokenize_280

Identity: unsupported op: No registered 'Identity' OpKernel for XLA_TPU_JIT devices compatible with node {{node Identity}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING
	Stacktrace:
		Node: __inference_tf_detokenize_280, function: 
		Node: Identity, function: __inference_tf_detokenize_280

identity_RetVal: unsupported op: No registered '_Retval' OpKernel for XLA_TPU_JIT devices compatible with node {{node identity_RetVal}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, index=0
	Stacktrace:
		Node: __inference_tf_detokenize_280, function: 
		Node: identity_RetVal, function: __inference_tf_detokenize_280
```
"
47682,Building tensorflow 2.4 from source failure: Lowering to LLVM IR failed,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.4
- Python version: Python 3.7
- Bazel version (if compiling from source): 2.3
- GCC/Compiler version (if compiling from source): Clang 11.0
- CUDA/cuDNN version: 11.0
- GPU model and memory: GTX 1080 Ti 11GB

**Describe the current behavior**

When building the tensorflow from source using clang, following issue happens:

```
ERROR: /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/external/org_tensorflow/tensorflow/core/kernels/mlir_generated/BUILD:149:19: compile external/org_tensorflow/tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.7.5.bin failed (Exit 1): linux-sandbox failed: error executing command
  (cd /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/sandbox/linux-sandbox/5469/execroot/ && \
  exec env - \
    TMPDIR=/tmp \
  /home/hzhao/.cache/bazel/_bazel_hzhao/install/bc435791ccf8b3a8a0fe11b973c1ebb7/linux-sandbox -t 15 -w /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/sandbox/linux-sandbox/5469/execroot/ -w /tmp -w /dev/shm -D -- bazel-out/host/bin/external/org_tensorflow/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary '--same_shape=0,1' '--unroll_factors=4' '--tile_sizes=256' '--arch=7.5' '--input=bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/core/kernels/mlir_generated/abs_f16.mlir' '--output=bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.7.5.bin') linux-sandbox failed: error executing command
  (cd /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/sandbox/linux-sandbox/5469/execroot/ && \
  exec env - \
    TMPDIR=/tmp \
  /home/hzhao/.cache/bazel/_bazel_hzhao/install/bc435791ccf8b3a8a0fe11b973c1ebb7/linux-sandbox -t 15 -w /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/sandbox/linux-sandbox/5469/execroot/ -w /tmp -w /dev/shm -D -- bazel-out/host/bin/external/org_tensorflow/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary '--same_shape=0,1' '--unroll_factors=4' '--tile_sizes=256' '--arch=7.5' '--input=bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/core/kernels/mlir_generated/abs_f16.mlir' '--output=bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.7.5.bin')
1615275075.984938474: src/main/tools/linux-sandbox-pid1.cc:208: writable: /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/sandbox/linux-sandbox/5469/execrootexternal/org_tensorflow/tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2021-03-08 23:31:16.021181: E external/org_tensorflow/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc:97] Internal: Lowering to LLVM IR failed.
1615275076.027019459: src/main/tools/linux-sandbox-pid1.cc:410: wait returned pid=2, status=0x100
1615275076.027040792: src/main/tools/linux-sandbox-pid1.cc:428: child exited normally with code 1
1615275076.093379128: src/main/tools/linux-sandbox.cc:233: child exited normally with code 1
Target @org_tensorflow//tensorflow/core/kernels/mlir_generated:abs_f16_kernel failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.442s, Critical Path: 0.12s
INFO: 3 processes: 3 internal.
FAILED: Build did NOT complete successfully
```




**Describe the expected behavior**

Tensorflow can be built without issue.


**Standalone code to reproduce the issue**
Run `tf_to_gpu_binary` with following `abs_f16.mlir`

**Other info / logs** Include any logs or source code that would be helpful to

Content of abs_f16.mlir:
```
func @Abs(%arg0: tensor<?xf16>)
    -> tensor<?xf16> attributes {tf_entry, llvm.emit_c_interface} {
  %0 = ""tf.Abs""(%arg0) : (tensor<?xf16>) -> tensor<?xf16>
  return %0 : tensor<?xf16>
}
```

MLIR reproducer:
```
// configuration: -pass-pipeline='gpu.module(llvm.func(propagate-tf-abi-knowledge{same-shape=0,1}), strip-debuginfo, gpu-kernel-to-blob{arch=7.5 blob-annotation=gpu.binary generate-fatbin=false})'
// note: verifyPasses=true


module attributes {gpu.container_module} {
  func @Abs(%arg0: memref<?xf16>) -> memref<?xf16> attributes {llvm.emit_c_interface, tf_entry} {
    %c256 = constant 256 : index
    %c1024 = constant 1024 : index
    %c0 = constant 0 : index
    %c1 = constant 1 : index
    %0 = dim %arg0, %c0 : memref<?xf16>
    %1 = alloc(%0) : memref<?xf16>
    %2 = cmpi ""sle"", %0, %c0 : index
    %3 = subi %c0, %0 : index
    %4 = subi %0, %c1 : index
    %5 = select %2, %3, %4 : index
    %6 = divi_signed %5, %c1024 : index
    %7 = subi %c0, %6 : index
    %8 = addi %6, %c1 : index
    %9 = select %2, %7, %8 : index
    ""gpu.launch_func""(%9, %c1, %c1, %c256, %c1, %c1, %arg0, %1) {kernel = @Abs_kernel::@Abs_kernel} : (index, index, index, index, index, index, memref<?xf16>, memref<?xf16>) -> ()
    return %1 : memref<?xf16>
  }
  gpu.module @Abs_kernel {
    llvm.func @__nv_fabsf(!llvm.float) -> !llvm.float
    llvm.func @Abs_kernel(%arg0: !llvm.ptr<half>, %arg1: !llvm.ptr<half>, %arg2: !llvm.i64, %arg3: !llvm.i64, %arg4: !llvm.i64, %arg5: !llvm.ptr<half>, %arg6: !llvm.ptr<half>, %arg7: !llvm.i64, %arg8: !llvm.i64, %arg9: !llvm.i64) attributes {gpu.kernel} {
      %0 = llvm.mlir.undef : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %3 = llvm.insertvalue %arg2, %2[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %4 = llvm.insertvalue %arg3, %3[3, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %5 = llvm.insertvalue %arg4, %4[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %6 = llvm.mlir.undef : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %7 = llvm.insertvalue %arg5, %6[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %8 = llvm.insertvalue %arg6, %7[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %9 = llvm.insertvalue %arg7, %8[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %10 = llvm.insertvalue %arg8, %9[3, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %11 = llvm.insertvalue %arg9, %10[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %12 = nvvm.read.ptx.sreg.ctaid.x : !llvm.i32
      %13 = llvm.sext %12 : !llvm.i32 to !llvm.i64
      %14 = nvvm.read.ptx.sreg.ctaid.y : !llvm.i32
      %15 = llvm.sext %14 : !llvm.i32 to !llvm.i64
      %16 = nvvm.read.ptx.sreg.ctaid.z : !llvm.i32
      %17 = llvm.sext %16 : !llvm.i32 to !llvm.i64
      %18 = nvvm.read.ptx.sreg.tid.x : !llvm.i32
      %19 = llvm.sext %18 : !llvm.i32 to !llvm.i64
      %20 = nvvm.read.ptx.sreg.tid.y : !llvm.i32
      %21 = llvm.sext %20 : !llvm.i32 to !llvm.i64
      %22 = nvvm.read.ptx.sreg.tid.z : !llvm.i32
      %23 = llvm.sext %22 : !llvm.i32 to !llvm.i64
      %24 = nvvm.read.ptx.sreg.nctaid.x : !llvm.i32
      %25 = llvm.sext %24 : !llvm.i32 to !llvm.i64
      %26 = nvvm.read.ptx.sreg.nctaid.y : !llvm.i32
      %27 = llvm.sext %26 : !llvm.i32 to !llvm.i64
      %28 = nvvm.read.ptx.sreg.nctaid.z : !llvm.i32
      %29 = llvm.sext %28 : !llvm.i32 to !llvm.i64
      %30 = nvvm.read.ptx.sreg.ntid.x : !llvm.i32
      %31 = llvm.sext %30 : !llvm.i32 to !llvm.i64
      %32 = nvvm.read.ptx.sreg.ntid.y : !llvm.i32
      %33 = llvm.sext %32 : !llvm.i32 to !llvm.i64
      %34 = nvvm.read.ptx.sreg.ntid.z : !llvm.i32
      %35 = llvm.sext %34 : !llvm.i32 to !llvm.i64
      llvm.br ^bb2
    ^bb2:  // pred: ^bb1
      %36 = llvm.mlir.constant(0 : index) : !llvm.i64
      %37 = llvm.extractvalue %5[3, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %38 = llvm.mlir.constant(4 : index) : !llvm.i64
      %39 = llvm.mlir.constant(1 : index) : !llvm.i64
      %40 = llvm.mlir.constant(1024 : index) : !llvm.i64
      %41 = llvm.mul %13, %40 : !llvm.i64
      %42 = llvm.mlir.constant(1024 : index) : !llvm.i64
      %43 = llvm.mlir.constant(-1024 : index) : !llvm.i64
      %44 = llvm.mul %13, %43 : !llvm.i64
      %45 = llvm.add %44, %37 : !llvm.i64
      %46 = llvm.icmp ""slt"" %42, %45 : !llvm.i64
      %47 = llvm.select %46, %42, %45 : !llvm.i1, !llvm.i64
      %48 = llvm.mlir.undef : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %49 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %50 = llvm.bitcast %49 : !llvm.ptr<half> to !llvm.ptr<half>
      %51 = llvm.insertvalue %50, %48[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %52 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %53 = llvm.bitcast %52 : !llvm.ptr<half> to !llvm.ptr<half>
      %54 = llvm.insertvalue %53, %51[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %55 = llvm.extractvalue %5[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %56 = llvm.extractvalue %5[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %57 = llvm.mul %41, %55 : !llvm.i64
      %58 = llvm.add %56, %57 : !llvm.i64
      %59 = llvm.insertvalue %58, %54[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %60 = llvm.insertvalue %47, %59[3, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %61 = llvm.mlir.constant(1 : i64) : !llvm.i64
      %62 = llvm.insertvalue %61, %60[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %63 = llvm.mlir.undef : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %64 = llvm.extractvalue %11[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %65 = llvm.bitcast %64 : !llvm.ptr<half> to !llvm.ptr<half>
      %66 = llvm.insertvalue %65, %63[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %67 = llvm.extractvalue %11[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %68 = llvm.bitcast %67 : !llvm.ptr<half> to !llvm.ptr<half>
      %69 = llvm.insertvalue %68, %66[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %70 = llvm.extractvalue %11[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %71 = llvm.extractvalue %11[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %72 = llvm.mul %41, %70 : !llvm.i64
      %73 = llvm.add %71, %72 : !llvm.i64
      %74 = llvm.insertvalue %73, %69[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %75 = llvm.insertvalue %47, %74[3, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %76 = llvm.mlir.constant(1 : i64) : !llvm.i64
      %77 = llvm.insertvalue %76, %75[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %78 = llvm.mlir.constant(4 : index) : !llvm.i64
      %79 = llvm.mul %19, %78 : !llvm.i64
      %80 = llvm.icmp ""slt"" %79, %47 : !llvm.i64
      llvm.cond_br %80, ^bb3, ^bb13
    ^bb3:  // pred: ^bb2
      %81 = llvm.mlir.constant(4 : index) : !llvm.i64
      %82 = llvm.mlir.constant(-4 : index) : !llvm.i64
      %83 = llvm.mul %19, %82 : !llvm.i64
      %84 = llvm.add %47, %83 : !llvm.i64
      %85 = llvm.icmp ""slt"" %81, %84 : !llvm.i64
      %86 = llvm.select %85, %81, %84 : !llvm.i1, !llvm.i64
      %87 = llvm.icmp ""eq"" %86, %38 : !llvm.i64
      llvm.cond_br %87, ^bb4, ^bb8
    ^bb4:  // pred: ^bb3
      llvm.br ^bb5(%36 : !llvm.i64)
    ^bb5(%88: !llvm.i64):  // 2 preds: ^bb4, ^bb6
      %89 = llvm.icmp ""slt"" %88, %38 : !llvm.i64
      llvm.cond_br %89, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %90 = llvm.add %88, %79 : !llvm.i64
      %91 = llvm.extractvalue %62[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %92 = llvm.extractvalue %62[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %93 = llvm.mlir.constant(1 : index) : !llvm.i64
      %94 = llvm.mul %90, %93 : !llvm.i64
      %95 = llvm.add %92, %94 : !llvm.i64
      %96 = llvm.getelementptr %91[%95] : (!llvm.ptr<half>, !llvm.i64) -> !llvm.ptr<half>
      %97 = llvm.load %96 : !llvm.ptr<half>
      %98 = llvm.fpext %97 : !llvm.half to !llvm.float
      %99 = llvm.call @__nv_fabsf(%98) : (!llvm.float) -> !llvm.float
      %100 = llvm.fptrunc %99 : !llvm.float to !llvm.half
      %101 = llvm.extractvalue %77[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %102 = llvm.extractvalue %77[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %103 = llvm.mlir.constant(1 : index) : !llvm.i64
      %104 = llvm.mul %90, %103 : !llvm.i64
      %105 = llvm.add %102, %104 : !llvm.i64
      %106 = llvm.getelementptr %101[%105] : (!llvm.ptr<half>, !llvm.i64) -> !llvm.ptr<half>
      llvm.store %100, %106 : !llvm.ptr<half>
      %107 = llvm.add %88, %39 : !llvm.i64
      llvm.br ^bb5(%107 : !llvm.i64)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb12
    ^bb8:  // pred: ^bb3
      llvm.br ^bb9(%36 : !llvm.i64)
    ^bb9(%108: !llvm.i64):  // 2 preds: ^bb8, ^bb10
      %109 = llvm.icmp ""slt"" %108, %86 : !llvm.i64
      llvm.cond_br %109, ^bb10, ^bb11
    ^bb10:  // pred: ^bb9
      %110 = llvm.add %108, %79 : !llvm.i64
      %111 = llvm.extractvalue %62[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %112 = llvm.extractvalue %62[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %113 = llvm.mlir.constant(1 : index) : !llvm.i64
      %114 = llvm.mul %110, %113 : !llvm.i64
      %115 = llvm.add %112, %114 : !llvm.i64
      %116 = llvm.getelementptr %111[%115] : (!llvm.ptr<half>, !llvm.i64) -> !llvm.ptr<half>
      %117 = llvm.load %116 : !llvm.ptr<half>
      %118 = llvm.fpext %117 : !llvm.half to !llvm.float
      %119 = llvm.call @__nv_fabsf(%118) : (!llvm.float) -> !llvm.float
      %120 = llvm.fptrunc %119 : !llvm.float to !llvm.half
      %121 = llvm.extractvalue %77[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %122 = llvm.extractvalue %77[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>
      %123 = llvm.mlir.constant(1 : index) : !llvm.i64
      %124 = llvm.mul %110, %123 : !llvm.i64
      %125 = llvm.add %122, %124 : !llvm.i64
      %126 = llvm.getelementptr %121[%125] : (!llvm.ptr<half>, !llvm.i64) -> !llvm.ptr<half>
      llvm.store %120, %126 : !llvm.ptr<half>
      %127 = llvm.add %108, %39 : !llvm.i64
      llvm.br ^bb9(%127 : !llvm.i64)
    ^bb11:  // pred: ^bb9
      llvm.br ^bb12
    ^bb12:  // 2 preds: ^bb7, ^bb11
      llvm.br ^bb13
    ^bb13:  // 2 preds: ^bb2, ^bb12
      llvm.return
    }
  }
}%
```


"
47680,LLVM MLIR Tensorflow Compatibility,"You have a great [table of tested build configurations for tensorflow in general](https://www.tensorflow.org/install/source#tested_build_configurations)  do you have something similar for getting TF's MLIR integration to work.  That is, can you suggest commits from llvm-project to use with commits from this repository that will guarantee a successful build of `mlir:tf-opt` as described in [the tensorflow MLIR readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/README.md)?  In that readme, there is a line suggesting that manual BUILD file changes may be required, for example if ""you are using a different version of LLVM than set in tensorflow/workspace.bzl's LLVM_COMMIT"", but I don't see a reference to `LLVM_COMMIT` in any of the 4 workspace*.bzl files in the tensorflow directory.

Alternatively/Additionally, is there any further guidance toward building `tf-opt` from source than that given in the aforementioned readme?

Any help would be appreciated."
47678,When Tensorflow support python 3.9 with m1 chip,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Mac os 11.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:??
- Python version:3.9.0
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
When could tensorflow support python 3.9 with m1 chip? And in other sense when tensorflow dev summit 2021 convening
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47676,Multi Worker Mirrored Strategy -- math,"In the Multi Worker Mirrored Strategy (seen here https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) it is not clear what is going on with the gradients and backpropogation under the hood (i.e., the math). For example, are gradients being averaged among the workers in the cluster? I cannot seem to figure this out as when I run on one machine without using a distributed strategy and on GPU my training/validation loss and training/validation metric differ significantly when I run on a cluster of machines using the distributed strategy Multi Worker Mirrored Strategy.  I was hoping this would not happen, i.e., you would add machines to the cluster to ideally speed up training, with loss and performance metric (MSE, RMSE) being roughly equal.

Also, if there is some reading/documentation on the math/how this is being implemented, I would appreciate. it. Could not find any so far."
47675,set_weights crashes in custom train_step() method,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.4.1
-   **Python version**: 3.8.0
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 11.0/8
-   **GPU model and memory**: Quadro RTX 6000 (24GB)
-   **Exact command to reproduce**:

```python
class DummyExample(tf.keras.Model):
    def __init__(
        self,
        output_dim: int,
        decay_rate: float
    ):
        super(DummyExample, self).__init__()
        self.output_dim = output_dim
        self.decay = decay_rate
        self.encoder_1 = self.create_encoder()
        self.encoder_2 = self.create_encoder()

    def create_encoder(self):
        encoder = tf.keras.Sequential()
        encoder.add(tf.keras.Input(shape=(4,), dtype=tf.float32))
        encoder.add(tf.keras.layers.Dense(
            units=self.output_dim,
            activation='relu'))
        return encoder

    def call(self, x: tf.Tensor, training: bool = False):
        return self.encoder_1(x, training=training)
    
    def train_step(self, data):
        x = data
        out_2 = self.encoder_2(x, training=True)
        with tf.GradientTape() as tape:
            out_1 = self.encoder_1(x, training=True)
            loss = self.compiled_loss(out_1, out_2)
        enc_1_grads = tape.gradient(loss, self.encoder_1.trainable_weights)
        self.optimizer.apply_gradients(zip(enc_1_grads, self.encoder_1.trainable_weights))
        
        # update encoder_2
        enc_1_weights = self.encoder_1.weights
        enc_2_weights = self.encoder_2.weights
        for i in range(len(enc_1_weights)):
            enc_2_weights[i] = self.decay * enc_2_weights[i] + (1-self.decay) * enc_1_weights[i]
        self.encoder_2.set_weights(enc_2_weights)
        return {'loss': loss}

dummy_model = DummyExample(2, 0.99)
dummy_model.compile(optimizer=tf.keras.optimizers.Adam(3e-4), loss='mse')

x = np.random.normal(size=(100, 4))
ds = tf.data.Dataset.from_tensor_slices(x)
ds = ds.batch(10)

dummy_model.fit(ds, epochs=2)
```
### Describe the problem
The above fit() call for the dummy example raises 
```bash
TypeError: in user code:

    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    <ipython-input-63-77e2fc2e32e6>:36 train_step
        self.encoder_2.set_weights(enc_2_weights)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:1877 set_weights
        backend.batch_set_value(weight_value_tuples)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/backend.py:3706 batch_set_value
        x.assign(np.asarray(value, dtype=dtype(x)))
    /usr/local/lib/python3.8/site-packages/numpy/core/_asarray.py:83 asarray
        return array(a, dtype, copy=False, order=order)

    TypeError: __array__() takes 1 positional argument but 2 were given
``` 

I think set_weights works only with list of numpy arrays, which somehow doesn't seem to work when the train logic is defined in the train_step() function. [This issue](https://github.com/tensorflow/tensorflow/issues/32927#issue-500463734) has a similar problem when the function containing set_weights is decorated by tf.function. Any workaround is greatly appreciated. Thanks in advance.
"
47674,Deallocation not happening when running on GPU in iOS swift,"System information

    OS Platform - iOS, swift
    Mobile device : iPhone, iPad
    TensorFlow installed from (source or binary): Pod.
    TensorFlow version (use command below): 2.3.0

**Execution**
Initialised interpreter once with below code
var delegate = MetalDelegate()
self.interpreter = try Interpreter(modelPath: modelPath, options: nil, delegates: [delegate])
self.interpreter?.allocateTensors()

Then executed the below code each time on running the inference
var inputTensor = try interpreter?.copy(input, toInputAt:0)
try interpreter?.invoke()
var outputTensor = try interpreter?.output(at: 0)

**Issue**
On running the .invoke() in GPU (using MetalDelegate) the memory consumption is increasing and not decreasing. How do i deallocate the unused memory? This is working fine in CPU."
47673,Possible memory leak cloning a model in memory with TF 2.3.1,"Hi,

I created a flask application which loads tensorflow models on startup. When an inference request is coming, the model is cloned in memory for the request like this:

```
model_copy =  tf.keras.models.clone_model(model)
model_copy.build()
model_copy.set_weights(model.get_weights())
```

The flask app runs in docker. After several requests the docker container crashed. With each request the RAM is increasing. After disabling the model cloning and doing the requests with one model in memory with locking, the memory does no more increasing.

**System information**
- OS Platform and Distribution: debian 10
- TensorFlow installed from: binary
- TensorFlow version: v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.6.9
"
47672,How to use a fractionally strided deconvolution layer? Please mention its initialization and also corresponding upsampling of its input,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
47671,More info on the deprecation of predict_x method for Sequential model?,"I traced down to https://github.com/tensorflow/tensorflow/commit/9795964342fbb0b6104b5ae131600d62faddb8c9 which marked predict_x method as deprecated.

But are there more information on the deprecation? Who made the decision? Are there any discussion on the deprecation? Are there any whatsnew/breaking changes items?"
47670,Does tensorflow support AMD?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
47669,Tensorflow 2.3.1 training does not use GPU but CPU with  CUDA 11.2 ,"
I am trying to train a custom dataset with the following backbone 

ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8

I have the following information about the system 

### System information
 Ubuntu 18.04)**:
TensorFlow 2.3.1
TensorFlow version
Python version 3.6
CUDNN 11.2 cuDNN 8.5
GPU K80


I get the following error when I try to train the model 


When I try to train my model it showed the following log when starting training and I see that it does not use GPU at all..


![image](https://user-images.githubusercontent.com/47017344/110425796-f5005980-8072-11eb-99ca-281c03cdeed0.png)


Here is the output of the NVIDA GPU - that is nothing is used

![image](https://user-images.githubusercontent.com/47017344/110425826-00538500-8073-11eb-8035-532a14ab0e27.png)




"
47666,Runtime error of executing HLO when adding SPMD pass,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux n130-024-068 4.14.81.bm.26-amd64 #1 SMP Debian 4.14.81.bm.26 Mon Sep 14 09:46:45 UTC 2020 x86_64 GNU/Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): TF master branch with commit id: a4ac4894d536bde05dafe74a3a3fec3aa0cb93b8
- Python version:3.8.5
- Bazel version (if compiling from source):3.7.2
- GCC/Compiler version (if compiling from source): gcc (Debian 8.3.0-6) 8.3.0
- CUDA/cuDNN version:V11.0.221
- GPU model and memory:V100/32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I want to test SPMD pass. 
The input is a single-device HLO-IR with SPMD sharding.
The program firstly would modify the single-device hlo-ir to 4-device hlo-ir;
Then, it would compile the modified hlo-ir to and executable;
After that, the pjrt client would run the executable on 4 GPUs, and return the result.  
Currently, I use the latest TF code (TF master branch with commit id: a4ac4894d536bde05dafe74a3a3fec3aa0cb93b8), it can successfully compile the code. However, it fails to execute the code and return the expected results.



**Describe the expected behavior**
It should compile the hlo, execute the executable, and return the result successfully.(i.e., [[3,3],[7,7]])

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

there are four steps to reproduce the problem:

**step1: tensorflow/compiler/xla/tools/BUILD**
```bazel
### new added tf_cc_binary to build the pjrt_demo
tf_cc_binary(
    name = ""pjrt_demo"",
    #testonly = True,
    srcs = [""pjrt_client_main.cc""],
    deps = [
        ""@com_google_absl//absl/strings"",
        ""//tensorflow/compiler/xla/pjrt:gpu_device"",
        ""//tensorflow/compiler/xla/pjrt:pjrt_client"",
        ""//tensorflow/compiler/xla/pjrt:cpu_device"",
        ""//tensorflow/compiler/xla/tools:hlo_module_loader"",

        ### SPMD pass
        ""//tensorflow/compiler/xla/service/spmd:spmd_partitioner"",
        ""//tensorflow/compiler/xla/service:hlo_pass_pipeline"",

        ""//tensorflow/compiler/xla/service:computation_layout"",
        ""//tensorflow/compiler/xla/service:layout_assignment"",

        
    ] + if_cuda_or_rocm([
        ""//tensorflow/compiler/xla/service:gpu_plugin"",
    ]),
)

```

**step2: tensorflow/compiler/xla/tools/pjrt_client_main.cc**
new added file to present the pjrt demo in CPP
```CPP
#include <memory>
#include <string>
#include <vector>

#include ""tensorflow/compiler/xla/literal.h""
#include ""tensorflow/compiler/xla/literal_util.h""
#include ""tensorflow/compiler/xla/pjrt/cpu_device.h""
#include ""tensorflow/compiler/xla/pjrt/gpu_device.h""
#include ""tensorflow/compiler/xla/pjrt/pjrt_client.h""
#include ""tensorflow/compiler/xla/status.h""
#include ""tensorflow/compiler/xla/statusor.h""
#include ""tensorflow/compiler/xla/tools/hlo_module_loader.h""
#include ""tensorflow/core/platform/init_main.h""
#include ""tensorflow/core/platform/logging.h""

#include ""tensorflow/compiler/xla/service/spmd/spmd_partitioner.h""
#include ""tensorflow/compiler/xla/service/hlo_pass_pipeline.h""
#include ""tensorflow/compiler/xla/service/hlo_verifier.h""

#include ""tensorflow/compiler/xla/service/computation_layout.h""
#include ""tensorflow/compiler/xla/service/layout_assignment.h""



using namespace xla;
using namespace xla::spmd;

namespace
{
    void AssignLayouts(HloModule* m, ComputationLayout* entry_computation_layout,
                     ChannelLayoutConstraints* channel_constraints = nullptr) {
    LayoutAssignment layout_assignment(
        entry_computation_layout, LayoutAssignment::InstructionCanChangeLayout,
        /*channel_constraints=*/channel_constraints);
    if(!layout_assignment.Run(m).status().ok())
        LOG(FATAL)<<""Layout assign failed"";
  }

  struct PjrtDemoArgs
        {
            PjrtDemoArgs()
                : platform(""GPU""),
                  reference_platform(""default""),
                  print_literals(false),
                  run_test_hlo_passes(true),
                  run_reference_hlo_passes(true),
                  use_large_float_range(false),
                  // TODO(b/68721786): These tolerances are set to match the values in the
                  // isolation test. The goal is to lower these to 0.001.
                  abs_error_bound(0.1),
                  rel_error_bound(0.1),
                  input_format(""hlo""),
                  input_text(""""),
                  iterations(1),
                  print_hlo_ir(false),
                  num_replicas(1),
                  dump_to_file(false),
                  speficy_layout(false),
                  execute_shared(false),
                  enable_spmd_pass(false)
            {
            }
            std::string platform;
            std::string reference_platform;
            bool print_literals;
            bool run_test_hlo_passes;
            bool run_reference_hlo_passes;
            bool use_large_float_range;
            float abs_error_bound;
            float rel_error_bound;
            std::string input_format;
            std::string input_text;
            int iterations;
            bool print_hlo_ir;
            int num_replicas;
            bool dump_to_file;
            bool speficy_layout;
            bool execute_shared;
            bool enable_spmd_pass;
        };

    PjrtDemoArgs get_args(int argc, char** argv)
    {
        PjrtDemoArgs opts;
        std::vector<tensorflow::Flag> flag_list = {
            tensorflow::Flag(
                ""platform"", &opts.platform,
                ""The test platform that the HLO module will be executed on ""
                ""(gpu, cpu, etc).""),
            tensorflow::Flag(
                ""reference_platform"", &opts.reference_platform,
                ""The reference platform that HLO module will be ""
                ""executed on. The result produced on the reference platform will ""
                ""be compared against the result produced on the test platform. A ""
                ""value of 'default' will use the TPU_Interpreter as a reference if ""
                ""the test platform is a TPU, and 'interpreter' otherwise. If the ""
                ""flag value is the empty string, then the module will not be run ""
                ""on a reference platform at all.""),
            tensorflow::Flag(""print_literals"", &opts.print_literals,
                            ""Print the input and result literals to stdout.""),
            tensorflow::Flag(
                ""run_test_hlo_passes"", &opts.run_test_hlo_passes,
                ""Run HLO pass pipeline for the test platform on the HLO module ""
                ""before running the module on the test platform. This should be ""
                ""set to true if the HLO module is unoptimized and set to false if ""
                ""the HLO module already has been optimized.""),
            tensorflow::Flag(
                ""run_reference_hlo_passes"", &opts.run_reference_hlo_passes,
                ""Run HLO pass pipeline for the reference platform on the HLO module ""
                ""before running the module on the reference platform. ""
                ""In general, if the given HLO module was optimized for a platform ""
                ""other ""
                ""than the reference this is necessary because some HLO passes are ""
                ""legalization passes which must be run prior to code generation.""),

            tensorflow::Flag(
                ""use_large_float_range"", &opts.use_large_float_range,
                ""Generate floating point values using a large uniform-log ""
                ""distribution as opposed to a small uniform distribution.""),
            tensorflow::Flag(
                ""abs_error_bound"", &opts.abs_error_bound,
                ""The absolute error bound used when comparing the test and ""
                ""reference results.""),
            tensorflow::Flag(
                ""rel_error_bound"", &opts.rel_error_bound,
                ""The relative error bound used when comparing the test and ""
                ""reference results.""),
            tensorflow::Flag(""input_format"", &opts.input_format,
                            ""The format of the input file. Valid values:\n""
                            ""  hlo : HLO textual format\n""
                            ""  pb : xla::HloProto in binary proto format\n""
                            ""  pbtxt : xla::HloProto in text proto format""),
            tensorflow::Flag(
                ""input_text"", &opts.input_text,
                ""A path to a file containing the HLO module. Can also pass ""
                ""a this as argv[1], but this flag is more explicit.""),
            tensorflow::Flag(
                ""iterations"", &opts.iterations,
                ""The number of times to run the module. Each iteration will be run ""
                ""with different input data.""),
            tensorflow::Flag(
                ""print_hlo_ir"", &opts.print_hlo_ir,
                ""decide whether print the hlo ir or not""),
            tensorflow::Flag(
                ""num_replicas"", &opts.num_replicas,
                ""decide how many replicas to used in data paralleism""),
            tensorflow::Flag(
                ""dump_to_file"", &opts.dump_to_file,
                ""dump to stdio, default is true""),
            tensorflow::Flag(
                ""speficy_layout"", &opts.speficy_layout,
                ""decide whether speficy layout""),
                tensorflow::Flag(
                ""execute_shared"", &opts.execute_shared,
                ""decide whether using executable shared""),
                tensorflow::Flag(
                ""enable_spmd_pass"", &opts.enable_spmd_pass,
                ""decide whether enable spmd pass"")
                
        };

        

        xla::AppendDebugOptionsFlags(&flag_list);
        // The usage string includes the message at the top of the file, the
        // DebugOptions flags and the flags defined above.
        
        bool parse_ok = tensorflow::Flags::Parse(&argc, argv, flag_list);
        tensorflow::port::InitMain("""", &argc, &argv);
        if (!parse_ok)
        {
            LOG(QFATAL) << ""cannot parse cmd data"";
        }

        return opts;
    }
}

int main(int argc, char **argv)
{
    //tensorflow::port::InitMain("""", &argc, &argv);
    PjrtDemoArgs opts = get_args(argc, argv);

    VLOG(0)<<""Using input data: ""<< opts.input_text;
    VLOG(0)<<""Using replicas: "" << opts.num_replicas;

    if(opts.input_text.length()==0)
        LOG(FATAL) << ""please speficy the input data (hlo text): "";

    // Load HloModule from file.
    std::string hlo_filename = opts.input_text;
    std::function<void(xla::HloModuleConfig *)> config_modifier_hook =
        [](xla::HloModuleConfig *config) { config->set_seed(42); };
    std::unique_ptr<xla::HloModule> test_module =
        LoadModuleFromFile(hlo_filename, xla::hlo_module_loader_details::Config(),
                           ""txt"", config_modifier_hook)
            .ValueOrDie();

    int num_devices = opts.num_replicas;

    //if(opts.enable_spmd_pass)
    //{
        //VLOG(0)<<""Enable SPMD pass"";
        // Some tests (BackpropFilter convs) set this flag false to test two
        // different paths of the implementation.
        SpmdPartitionerOptions options;
        options.conv_halo_exchange_always_on_lhs = true;
        options.allow_module_signature_change = true;
        options.choose_faster_windowed_einsum_over_mem = false;

        auto collective_ops_creator =
            GetDefaultCollectiveOpsCreator(num_devices, /*num_replicas=*/opts.num_replicas);
        // Do not use all-gather for pattern-matching purpose, as the partitioner
        // might create reshape/transposes around it.
        collective_ops_creator.create_cross_partition_all_gather = nullptr;

        // NOTE: RUN SPMD Partitioning if necessary
        HloPassPipeline pass(""spmd-partitioning"");
        pass.AddPass<HloVerifier>(/*layout_sensitive=*/false,
                                    /*allow_mixed_precision=*/true);
        pass.AddPass<SpmdPartitioner>(num_devices, /*num_replicas=*/opts.num_replicas, options,
                                        collective_ops_creator);
        pass.AddPass<HloVerifier>(/*layout_sensitive=*/false,
                                    /*allow_mixed_precision=*/true);
    //}

    
    if(opts.speficy_layout)
    {
        VLOG(0)<<""Speficy layout"";
        // layout_assignment_test.cc: 888
        // layoutassignment
        ComputationLayout computation_layout(
            test_module->entry_computation()->ComputeProgramShape(), false);
        VLOG(0) << ""before: computation layout:\n"" << computation_layout.ToString();
        computation_layout.SetToDefaultLayoutIfEmpty();
        VLOG(0) << ""After: computation layout:\n"" << computation_layout.ToString();
        Shape param_shape = ShapeUtil::MakeShape(F32, {2, 2});
        if(computation_layout.mutable_parameter_layout(0)->CopyLayoutFromShape(param_shape)!=Status::OK())
        {
            LOG(FATAL)<<""Error of assigning computation layout for param 0"";
        }

        if(computation_layout.mutable_parameter_layout(1)->CopyLayoutFromShape(param_shape)!=Status::OK())
        {
            LOG(FATAL)<<""Error of assigning computation layout for param 1"";
        }

        computation_layout.mutable_result_layout()->ResetLayout(
            LayoutUtil::MakeLayout({1, 0}));

        ChannelLayoutConstraints channel_constraints;
        AssignLayouts(test_module.get(), &computation_layout, &channel_constraints);

    
        for(int index=1; index < 3; index++)
        {
            VLOG(0)<<""Channel ""<<index<<"": "" << channel_constraints.IsChannelConstrained(index);
            channel_constraints.ConstrainChannel(index, *param_shape.mutable_layout());
            //channel_constraints.LayoutShapeForChannel(param_shape, index);
            if(channel_constraints.IsChannelConstrained(index))
            {
                VLOG(0)<<""set up layout: "" << channel_constraints.LayoutForChannel(index).ToString();
            }
        }
    
        
    }

    VLOG(0)<<""Before spmd pass: \n"" << test_module->ToString();

    if(opts.enable_spmd_pass)
    {
        if (!pass.Run(test_module.get()).status().ok())
        {
            LOG(FATAL) << ""Error of runing hlo pass"";
        }

        VLOG(0)<<""After spmd pass: \n"" << test_module->ToString();
    }

    const xla::HloModuleProto test_module_proto = test_module->ToProto();

    // Run it using JAX C++ Runtime (PJRT).

    // Get a GPU client.
    std::unique_ptr<xla::PjRtClient> client =
        xla::GetGpuClient(/*asynchronous=*/true, xla::GpuAllocatorConfig(),
                          /*distributed_client=*/nullptr, /*node_id=*/0)
            .ValueOrDie();

    LOG(INFO) << ""Compile the code"";

    

    // Compile XlaComputation to PjRtExecutable.
    xla::XlaComputation xla_computation(test_module_proto);
    if(opts.speficy_layout)
    {
        ComputationLayout computation_layout(
            test_module->entry_computation()->ComputeProgramShape(),false);
        VLOG(0) << ""before: computation layout:\n"" << computation_layout.ToString();
        computation_layout.SetToDefaultLayoutIfEmpty();
        VLOG(0) << ""After: computation layout:\n"" << computation_layout.ToString();


        Shape param_shape = ShapeUtil::MakeShape(F32, {2, 2});
        if(computation_layout.mutable_parameter_layout(0)->CopyLayoutFromShape(param_shape)!=Status::OK())
        {
            LOG(FATAL)<<""Error of assigning computation layout for param 0"";
        }

        if(computation_layout.mutable_parameter_layout(1)->CopyLayoutFromShape(param_shape)!=Status::OK())
        {
            LOG(FATAL)<<""Error of assigning computation layout for param 1"";
        }

        computation_layout.mutable_result_layout()->ResetLayout(
            LayoutUtil::MakeLayout({1, 0}));

        ChannelLayoutConstraints channel_constraints;
        AssignLayouts(test_module.get(), &computation_layout, &channel_constraints);

    
        for(int index=1; index < 3; index++)
        {
            VLOG(0)<<""Channel ""<<index<<"": "" << channel_constraints.IsChannelConstrained(index);
            channel_constraints.ConstrainChannel(index, *param_shape.mutable_layout());
            //channel_constraints.LayoutShapeForChannel(param_shape, index);
            if(channel_constraints.IsChannelConstrained(index))
            {
                VLOG(0)<<""set up layout: "" << channel_constraints.LayoutForChannel(index).ToString();
            }
        }
    }

    xla::CompileOptions compile_options;
    compile_options.executable_build_options.mutable_debug_options()
        ->set_xla_gpu_disable_multi_streaming(false);
    compile_options.executable_build_options.mutable_debug_options()
        ->set_xla_gpu_use_random_streams(true);
    compile_options.executable_build_options.set_num_replicas(opts.num_replicas);//cy

    //compile_options.executable_build_options.set_run_id(RunId());
    
    DeviceAssignment device_assignment(opts.num_replicas, 1);
    for(int index=0; index<client->addressable_devices().size();index++)
    {
        PjRtDevice* device = client->addressable_devices().at(index);
        
        device_assignment(index, 0) = device->id();
        VLOG(0)<< ""device id: "" << device->id();
        
    }
    compile_options.executable_build_options.set_device_assignment(
            device_assignment);
    
    std::unique_ptr<xla::PjRtExecutable> executable =
        client->Compile(xla_computation, compile_options).ValueOrDie();

    // Prepare inputs.
    xla::Literal literal_x =
        xla::LiteralUtil::CreateR2<float>({{1.0f, 2.0f}, {3.0f, 4.0f}});
    xla::Literal literal_y =
        xla::LiteralUtil::CreateR2<float>({{1.0f, 1.0f}, {1.0f, 1.0f}});
    
    auto devices_list = client->addressable_devices();
    if(opts.execute_shared)
    {
        VLOG(0)<<""Run iterations: ""<< opts.iterations;
        for(int iter=0; iter < opts.iterations; iter++)
        {
            VLOG(0)<<""enable execute_shared"";
            std::vector<std::vector<std::unique_ptr<xla::PjRtBuffer> > > results;
            VLOG(0)<<""devices_list: "" << devices_list.size();
            //tensorflow::thread::ThreadPool* execution_pool=nullptr;
            tensorflow::thread::ThreadPool pool(tensorflow::Env::Default(), ""replicas"", opts.num_replicas);
            auto a_id = RunId();
            for (int64 index = 0; index < opts.num_replicas; ++index)
            {
                pool.Schedule([&, index] {
                    VLOG(0)<<""Submit task on device: ""<< index;
                    std::unique_ptr<xla::PjRtBuffer> param_x =
                    client->BufferFromHostLiteral(literal_x, devices_list[index])
                        .ValueOrDie();
                    std::unique_ptr<xla::PjRtBuffer> param_y =
                    client->BufferFromHostLiteral(literal_y, devices_list[index])
                        .ValueOrDie();

                    // Execute on GPU.
                    xla::ExecuteOptions execute_options;
                    execute_options.context = (const ExecuteContext*)(&a_id);
                    std::vector<std::unique_ptr<xla::PjRtBuffer> >  _results =
                    executable->ExecuteSharded({{param_x.get(), param_y.get()}}, 
                                                devices_list[index], execute_options)
                                .ValueOrDie();
                    VLOG(0)<<""[Done] task executed on device: ""<< index;
                    // Get result.
                std::shared_ptr<xla::Literal> result_literal =
                    _results[0]->ToLiteral().ValueOrDie();
                LOG(INFO) << ""result = "" << *result_literal;
                });
            }
            VLOG(0) << ""Task are executed done from main thread"";
        }
    return 0;
    }


    std::unique_ptr<xla::PjRtBuffer> param_x =
        client->BufferFromHostLiteral(literal_x, client->addressable_devices()[0])
            .ValueOrDie();
    std::unique_ptr<xla::PjRtBuffer> param_y =
        client->BufferFromHostLiteral(literal_y, client->addressable_devices()[0])
            .ValueOrDie();
    std::unique_ptr<xla::PjRtBuffer> param_x1 =
        client->BufferFromHostLiteral(literal_x, client->addressable_devices()[1])
            .ValueOrDie();
    std::unique_ptr<xla::PjRtBuffer> param_y1 =
        client->BufferFromHostLiteral(literal_y, client->addressable_devices()[1])
            .ValueOrDie();
    std::unique_ptr<xla::PjRtBuffer> param_x2 =
        client->BufferFromHostLiteral(literal_x, client->addressable_devices()[2])
            .ValueOrDie();
    std::unique_ptr<xla::PjRtBuffer> param_y2 =
        client->BufferFromHostLiteral(literal_y, client->addressable_devices()[2])
            .ValueOrDie();
    std::unique_ptr<xla::PjRtBuffer> param_x3 =
        client->BufferFromHostLiteral(literal_x, client->addressable_devices()[3])
            .ValueOrDie();
    std::unique_ptr<xla::PjRtBuffer> param_y3 =
        client->BufferFromHostLiteral(literal_y, client->addressable_devices()[3])
            .ValueOrDie();
    
    // std::vector<const std::vector<PjRtBuffer*> > tmp_buffers;
    // for(int index=0;index<opts.num_replicas;index++)
    // {
    //     std::unique_ptr<xla::PjRtBuffer> param_x =
    //     client->BufferFromHostLiteral(literal_x, client->addressable_devices()[index])
    //         .ValueOrDie();
    //     std::unique_ptr<xla::PjRtBuffer> param_y =
    //     client->BufferFromHostLiteral(literal_y, client->addressable_devices()[index])
    //         .ValueOrDie();

    //     const std::vector<PjRtBuffer*> buf = {param_x.get(), param_y.get()};
    //     tmp_buffers.push_back(std::move(buf));
    // }
    // absl::Span<const std::vector<PjRtBuffer*>> input_buffers = absl::MakeSpan(tmp_buffers);

    // Execute on GPU.
    xla::ExecuteOptions execute_options;
    // One vector<buffer> for each device.
    std::vector<std::vector<std::unique_ptr<xla::PjRtBuffer> > > results =
        executable->Execute({{param_x.get(), param_y.get()}, {param_x1.get(), param_y1.get()}, {param_x2.get(), param_y2.get()}, {param_x3.get(), param_y3.get()}}, execute_options)
            .ValueOrDie();
        //executable->ExecuteSharded({{param_x.get(), param_y.get()}},nullptr, execute_options)
        //    .ValueOrDie();


    // Get result.
    std::shared_ptr<xla::Literal> result_literal =
        results[0][0]->ToLiteral().ValueOrDie();
    LOG(INFO) << ""result = "" << *result_literal;
    return 0;
}
```

**step3: fn_hlo.txt**
a text that indicates a computation (a[2x2] dot b [2x2]) with SPMD sharding
```shell
HloModule module

ENTRY %entry (p0: f32[2,2], p1: f32[2,2]) -> f32[2,2] {
  %p0 = f32[2,2]{1,0} parameter(0), parameter_replication={false}, sharding={maximal device=0}
  %p1 = f32[2,2]{1,0} parameter(1), parameter_replication={false}, sharding={maximal device=0}
  ROOT %my_add = f32[2,2]{1,0} dot(f32[2,2]{1,0} %p0, f32[2,2]{1,0} %p1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, sharding={replicated}
}
```

**step4: compile and run the code**
```bash
./bazel-bin/tensorflow/compiler/xla/tools/pjrt_demo --input_text=PATH-TO-YOUR/fn_hlo.txt --num_replicas=4 --execute_shared --enable_spmd_pass
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The full logs are as follows:

```bash
2021-03-09 11:11:38.345132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-09 11:11:38.349232: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:184] Using input data: tensorflow/compiler/xla/tools/hlo_files/fn_hlo.txt
2021-03-09 11:11:38.349258: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:185] Using replicas: 4
2021-03-09 11:11:38.351253: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:270] Before spmd pass:
HloModule module

ENTRY %entry (p0: f32[2,2], p1: f32[2,2]) -> f32[2,2] {
  %p0 = f32[2,2]{1,0} parameter(0), parameter_replication={false}, sharding={maximal device=0}
  %p1 = f32[2,2]{1,0} parameter(1), parameter_replication={false}, sharding={maximal device=0}
  ROOT %my_add = f32[2,2]{1,0} dot(f32[2,2]{1,0} %p0, f32[2,2]{1,0} %p1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, sharding={replicated}
}


2021-03-09 11:11:38.354686: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:279] After spmd pass:
HloModule module

%add (x: f32[], y: f32[]) -> f32[] {
  %x = f32[] parameter(0)
  %y = f32[] parameter(1)
  ROOT %add = f32[] add(f32[] %x, f32[] %y)
}

%add.1 (x.1: f32[], y.1: f32[]) -> f32[] {
  %x.1 = f32[] parameter(0)
  %y.1 = f32[] parameter(1)
  ROOT %add.1 = f32[] add(f32[] %x.1, f32[] %y.1)
}

ENTRY %entry_spmd (param: f32[2,2], param.1: f32[2,2]) -> f32[2,2] {
  %partition-id = u32[] partition-id()
  %constant = u32[] constant(0)
  %compare.1 = pred[] compare(u32[] %partition-id, u32[] %constant), direction=EQ
  %broadcast.2 = pred[2,2]{1,0} broadcast(pred[] %compare.1), dimensions={}
  %param = f32[2,2]{1,0} parameter(0), parameter_replication={false}, sharding={maximal device=0}
  %constant.1 = f32[] constant(0)
  %broadcast.3 = f32[2,2]{1,0} broadcast(f32[] %constant.1), dimensions={}
  %select.1 = f32[2,2]{1,0} select(pred[2,2]{1,0} %broadcast.2, f32[2,2]{1,0} %param, f32[2,2]{1,0} %broadcast.3)
  %all-reduce.1 = f32[2,2]{1,0} all-reduce(f32[2,2]{1,0} %select.1), channel_id=2, replica_groups={{0},{1},{2},{3}}, to_apply=%add.1
  %param.1 = f32[2,2]{1,0} parameter(1), parameter_replication={false}, sharding={maximal device=0}
  %select = f32[2,2]{1,0} select(pred[2,2]{1,0} %broadcast.2, f32[2,2]{1,0} %param.1, f32[2,2]{1,0} %broadcast.3)
  %all-reduce = f32[2,2]{1,0} all-reduce(f32[2,2]{1,0} %select), channel_id=1, replica_groups={{0},{1},{2},{3}}, to_apply=%add
  ROOT %dot = f32[2,2]{1,0} dot(f32[2,2]{1,0} %all-reduce.1, f32[2,2]{1,0} %all-reduce), lhs_contracting_dims={1}, rhs_contracting_dims={0}
}


2021-03-09 11:11:38.355791: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-09 11:11:39.274557: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x55b7a4c08a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-03-09 11:11:39.274596: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-03-09 11:11:39.274607: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-03-09 11:11:39.274615: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-03-09 11:11:39.274622: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-03-09 11:11:39.276501: I tensorflow/compiler/xla/pjrt/gpu_device.cc:125] XLA backend allocating 30063919104 bytes on device 0 for BFCAllocator.
2021-03-09 11:11:39.276640: I tensorflow/compiler/xla/pjrt/gpu_device.cc:125] XLA backend allocating 30063919104 bytes on device 1 for BFCAllocator.
2021-03-09 11:11:39.276756: I tensorflow/compiler/xla/pjrt/gpu_device.cc:125] XLA backend allocating 30063919104 bytes on device 2 for BFCAllocator.
2021-03-09 11:11:39.276865: I tensorflow/compiler/xla/pjrt/gpu_device.cc:125] XLA backend allocating 29774197555 bytes on device 3 for BFCAllocator.
2021-03-09 11:11:39.283129: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:292] Compile the code
2021-03-09 11:11:39.283303: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:352] device id: 0
2021-03-09 11:11:39.283317: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:352] device id: 1
2021-03-09 11:11:39.283325: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:352] device id: 2
2021-03-09 11:11:39.283332: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:352] device id: 3
2021-03-09 11:11:39.747483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-09 11:11:40.299515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-09 11:11:40.485692: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Unimplemented: Requested AllReduce not implemented on GPU; replica_count: 4; partition_count: 1, group_mode: kCrossReplicaAndPartition, operand_count: 2; NCCL support: 1; first operand array element-type: F32
fish: ./bazel-bin/tensorflow/compiler terminated by signal SIGABRT (Abort)
```"
47665,`ImportError: cannot import name 'Deconvolution3D' from 'keras.layers' (/home/mona/venv/fall/lib/python3.8/site-packages/keras/layers/__init__.py)`,"How should I fix the following error?
`ImportError: cannot import name 'Deconvolution3D' from 'keras.layers' (/home/mona/venv/fall/lib/python3.8/site-packages/keras/layers/__init__.py)`

I have:
```

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-14-8ac95d90a6a0> in <module>
      4 import config
      5 from trainer.fusiondiffroigan import Params,Fusion_Diff_ROI_3DCAE_GAN3D
----> 6 from models import diff_ROI_C3D_AE_no_pool

~/research/code/GAN-fall/Fall-detection/mrfd/models.py in <module>
      5 from keras.layers import Activation, Dropout, Flatten, Dense, Input, Reshape, BatchNormalization
      6 # from keras.layers import Conv3DTranspose as Deconvolution3D
----> 7 from keras.layers import Deconvolution3D
      8 from keras.optimizers import SGD
      9 from keras import regularizers

ImportError: cannot import name 'Deconvolution3D' from 'keras.layers' (/home/mona/venv/fall/lib/python3.8/site-packages/keras/layers/__init__.py)
```

also:

![Screenshot from 2021-03-08 22-22-41](https://user-images.githubusercontent.com/76495162/110413941-dd1ddb00-805c-11eb-8eb7-497ce7f3a0f1.png)
and

```
Python 3.8.5 (default, Jan 27 2021, 15:41:15) 
[GCC 9.3.0] on Linux

$ lsb_release -a
LSB Version:	core-11.1.0ubuntu2-noarch:security-11.1.0ubuntu2-noarch
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04.2 LTS
Release:	20.04
Codename:	focal



```"
47663,Infinite loop in AdjustHue,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 / Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source / binary
- TensorFlow version (use command below): 2.4.1
- Python version: python 3.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): clang 12
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

An infinite loop bug in adjust_hue_op.cc when computing AdjustHue op. It can be triggered easily when argument delta sets as a big floating point value.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1-j_9NiRewoC6DEUk8qS2ceajJ2qbrA7K?usp=sharing

**Other info / logs** 
Code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/adjust_hue_op.cc#L232-L238

`h` may be inf or -inf. Infinite loop in separate while loop.
-inf:
```
tensorflow::AdjustHueOp<Eigen::ThreadPoolDevice, float>::DoCompute(tensorflow::OpKernelContext*, tensorflow::AdjustHueOpBase::ComputeOptions const&)::{lambda(long, long)#1}::operator()(long, long) const (this=<optimized out>,
    start_channel=<optimized out>, end_channel=6) at tensorflow/core/kernels/image/adjust_hue_op.cc:232
232              h += delta_h * kChannelRange;
(gdb) p h
$6 = <optimized out>
(gdb) n
233              while (h < 0) {
(gdb) p h
$7 = -inf
(gdb) n
234                h += kChannelRange;
(gdb)
233              while (h < 0) {
(gdb) p h
$8 = -inf
(gdb)
```
inf:
```
(gdb) n
233              while (h < 0) {
(gdb) p h
$5 = inf
(gdb)
```
@mihaimaruseac Can you help to triage this bug to the right team? Thank you!"
47662,Update aliasing in tf.linalg.band_part docs,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/linalg/band_part

## Description of issue (what needs changing):

### Clear description

https://github.com/tensorflow/tensorflow/blob/56903ed0419882e004d4467dec452684b106e46f/tensorflow/core/api_def/base_api/api_def_MatrixBandPart.pbtxt#L50-L66

`tf.matrix_band_part` in above lines should be `tf.linalg.band_part`. (`tf.matrix_band_part` is moved to `tf.compat.v1.matrix_band_part`)

The below links should also be fixed

* https://github.com/tensorflow/tensorflow/blob/011228639301a8ed60d39c94ab096b074091bb4a/tensorflow/go/op/wrappers.go
* https://github.com/tensorflow/tensorflow/blob/0748d9a3ab284f7c9461985db478d09e2873cf1a/tensorflow/compiler/mlir/tensorflow/ir/tf_generated_ops.td
"
47661,eigh fails with 26x26 zero matrix on GPU with eager execution,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.1
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.7.6
- CUDA/cuDNN version: 11.0
- GPU model and memory: Tesla T4 16GB

**Describe the current behavior**
`tf.linalg.eigh` fails with `InternalError: tensorflow/core/util/cuda_solvers.cc:648: cuSolverDN call failed with status =7 [Op:SelfAdjointEigV2]` on a 26x26 (and larger) zero matrix when used with both GPU and eager execution. It works with other combinations of eager/graph execution and CPU/GPU.

**Describe the expected behavior**
Eigendecomposition should be successfully calculated.

**Standalone code to reproduce the issue**
```
# Needs to have GPU
import tensorflow as tf
tf.linalg.eigh(tf.zeros((26, 26), dtype=tf.float64))
```
Also see gist: https://colab.research.google.com/drive/1x5sfDIGJdDNOgqpaH7OBrnENfgE14ZQ0?usp=sharing
"
47660,tensorflow-gpu==1.14.0 does not support python 3.6.12-debug,"
**System information**
- I haven't written custom code.
- OS Platform and Distribution :Linux Ubuntu 18.04.
- Python version: python 3.6.12-debug

**Describe the current behavior**
I used python 3.6.12-debug built by pyenv:
$ pip -V
pip 18.1 from /home/yawen/.pyenv/versions/3.6.12-debug/lib/python3.6/site-packages/pip (python 3.6)

I wanted to install tensorflow 1.14.0, but I got the following error:
$python -m pip install tensorflow-gpu==1.14.0
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.14.0
ERROR: No matching distribution found for tensorflow-gpu==1.14.0

I checked my whether my python was 32bits or 64bits:
>>> import platform
>>> platform.architecture()
('64bit', '')

It is 64bits, which satisfies the requirement of tensorflow. I don't know why tensorflow does not support python 3.6.12-debug.

**Describe the expected behavior**
I hope I can successfully install tensorflow on python 3.6.12-debug, or any other version of python-debug.


"
47659,tf-model-remediation: Keyword arguments not supported by original model: `['mask']`,"[Reposting ](https://github.com/tensorflow/model-remediation/issues/24) from tensorflow/model-remediation as I was hoping to use MinDiff framework in my work however there hasn't been a response from the team in that repo


**System information**
Relevant Versions:
Running locally for now on Windows 10 Pro Version 10.0.19042 Build 19042
python 3.6.9
tensorflow 2.3.1
tensorflow-model-remediation 0.1.3
transformers 4.3.2


From provided: TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
output: `v2.3.0-54-gfcc4b966f1 2.3.1`

**Describe the current behavior**
Hi I've been trying to debug this for a few days, I'm using the MinDiff remediation similarly to how it's used in the [tutorial](https://www.tensorflow.org/responsible_ai/model_remediation/min_diff/tutorials/min_diff_keras). Using the debugger I see the 'mask' value is None from the caller. 
The base model I am using to remediate unfairness is `TFBertForSequenceClassification` from HuggingFace's transformers library. It compiles to tf.keras, so from the documentation should be compatible with the MinDiff framework

**Describe the expected behavior**
I expect the library to be able to train a mindiff framework with the provided code.

**Standalone code to reproduce the issue**
Code to reproduce (I can't share my data, but I've tried loading the BERT weights from a TF trained Transformers with the same issue. I've also commented out wrapping the TFBertForSequenceClassification in tf.keras model but it does not make a difference to the stacktrace.
```
bert_model = TFBertForSequenceClassification.from_pretrained(path_to_bert, from_pt=True)
# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
# loss = tf.keras.losses.BinaryCrossentropy()
# bert_model.compile(optimizer=optimizer, loss=loss)

min_diff_weight = 1.5 

# Create the dataset that will be passed to the MinDiffModel during training.
dataset = md.keras.utils.input_utils.pack_min_diff_data(
    train_ds_main, train_ds_unpriv, train_ds_priv)

# Wrap the original model in a MinDiffModel, passing in one of the MinDiff
# losses and using the set loss_weight.
min_diff_loss = md.losses.MMDLoss()
model = md.keras.MinDiffModel(bert_model,
                              min_diff_loss,
                              min_diff_weight)

# Compile the model normally after wrapping the original model.  Note that
# this means we use the baseline's model's loss here.
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
loss = tf.keras.losses.BinaryCrossentropy()
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

model.fit(dataset, epochs=epochs)
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Stacktrace:
```
Epoch 1/1000
Traceback (most recent call last):
  File ""*/src/benchmarking/run_model.py"", line 450, in <module>
    main()
  File ""*/src/benchmarking/run_model.py"", line 446, in main
    eval_min_diff_bert(**kwargs)
  File ""*/src/benchmarking/run_model.py"", line 211, in eval_min_diff_bert
    model.fit(dataset, epochs=epochs)
  File ""*\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""*\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""*\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\eager\def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File *\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\eager\def_function.py"", line 823, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""*\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\eager\def_function.py"", line 697, in _initialize
    *args, **kwds))
  File ""*\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\eager\function.py"", line 2855, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""*\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\eager\function.py"", line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""*\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\eager\function.py"", line 3075, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""*\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""*\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\eager\def_function.py"", line 600, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""*\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 973, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    *\anaconda3\envs\mindiff\lib\site-packages\tensorflow\python\keras\engine\training.py:806 train_function  *
        return step_function(self, iterator)
   *\anaconda3\envs\mindiff\lib\site-packages\tensorflow_model_remediation\min_diff\keras\models\min_diff_model.py:473 call  *
        min_diff_loss = self.compute_min_diff_loss(
   *\anaconda3\envs\mindiff\lib\site-packages\tensorflow_model_remediation\min_diff\keras\models\min_diff_model.py:371 compute_min_diff_loss  *
        predictions = self._call_original_model(x, training=training, mask=mask)
    *\anaconda3\envs\mindiff\lib\site-packages\tensorflow_model_remediation\min_diff\keras\models\min_diff_model.py:234 _call_original_model  *
        return self.original_model(inputs, **kwargs)
    *\anaconda3\envs\mindiff\lib\site-packages\transformers-4.2.2-py3.8.egg\transformers\models\bert\modeling_tf_bert.py:1405 call  *
        inputs = input_processing(
    *\anaconda3\envs\mindiff\lib\site-packages\transformers-4.2.2-py3.8.egg\transformers\modeling_tf_utils.py:345 input_processing  *
        raise ValueError(

    ValueError: The following keyword arguments are not supported by this model: ['mask'].
```

Thank you!"
47657,"Converted TF2 model zoo MaskRCNN model to .tflite, however input tensor seems to have incorrect shape","I follow the gist above and converted the `model mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8` to a .tflite model.

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Android
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly: '2.5.0-dev20210308'

### 2. Code

Conversion code:
```
import tensorflow as tf
from tflite_support import metadata as _metadata

model_dir = r'C:\Users\yuh5\PycharmProjects\Convert1\saved_model'
saved_model_dir = 'updated/saved_model'
model = tf.saved_model.load(model_dir)
concrete_func = model.signatures['serving_default']
concrete_func.inputs[0].set_shape([1, 1024, 1024, 3])
tf.saved_model.save(model, saved_model_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, signature_keys=['serving_default'])
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
    f.write(tflite_model)

# Used for adding metadata
populator = _metadata.MetadataPopulator.with_model_file(""model.tflite"")
populator.load_associated_files([""labels.txt""])
populator.populate()
```

### 3. Failure after conversion

The code run though successfully, however, the input tensor of the .tflite model has the shape shown in the image below:

![Capture](https://user-images.githubusercontent.com/5137261/110395357-fd896d80-803b-11eb-9b73-738ce184b080.PNG)

Is this correct? If so, when I use this model to run inference on Android with the demo code for object detection I got the follow error:

`Caused by: java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (serving_default_input_tensor:0) with 3 bytes from a Java Buffer with 12582912 bytes.`

How should I modify the code to use maskRCNN model?

The model file is [here](https://nih.box.com/s/rzbde15y3jjf3pznwgptomacf79cgh7f)
"
47655,Implementing a version of the (Sparse) Categorical Cross Entropy Loss that makes use of Integer-Indexed predictions.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Tensorflow 2.
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

The Categorical Cross Entropy Loss makes use of both one-hot encoded predictions, and one-hot encoded true labels.
Eg, y_true = [ [0,0,1], [1,0,0] ] and y_pred = [ [0,0,1], [0,1,0] ].

The Sparse Categorical Cross Entropy Loss makes use of single integer true labels, but still requires an array of probabilities corresponding to each existing class, per label in the predictions.
Eg, y_true = [2, 0] and y_pred = [ [0.1, 0.2, 0.7], [0.2, 0.85, 0.05] ].

I feel it would be really useful to have a loss function which makes use of single integer-indexed categorical predictions as well as single integer-indexed categorical true labels.
Eg, y_true = [2, 0] and y_pred = [2, 1].

**Will this change the current api? How?**
Not really. It could possibly just require the definition of a new loss function.

**Who will benefit with this feature?**
For one, it would be possible to use Dense models to process two-dimensional data. This could be done by simply encoding one of the concerned dimensions as a series of integer-indexed categories. You could process text data with a Dense model by simply encoding the characters as integer-indexed categories (in contrast to one-hot encoding) and then work on minimizing the outputs of the proposed loss function.

Eg, ""I am"" would simply be [9,0,1,13] (with ' ' as 0). This is evidently amenable to processing by a Dense model.


**Any Other info.**
"
47654, import tensorflow as tf gives an error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version:
- Python version:Python 3.5.6
- Installed using virtualenv? pip? conda?:pip 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

In [1]: import tensorflow as tf
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

C:\Users\Saagarika\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py in <module>()
     22
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26

C:\Users\Saagarika\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py in <module>()
     52
     53 # Protocol buffers
---> 54 from tensorflow.core.framework.graph_pb2 import *
     55 from tensorflow.core.framework.node_def_pb2 import *
     56 from tensorflow.core.framework.summary_pb2 import *

C:\Users\Saagarika\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\core\framework\graph_pb2.py in <module>()
      8 from google.protobuf import reflection as _reflection
      9 from google.protobuf import symbol_database as _symbol_database
---> 10 from google.protobuf import descriptor_pb2
     11 # @@protoc_insertion_point(imports)
     12

C:\Users\Saagarika\Anaconda3\envs\tensorflow\lib\site-packages\google\protobuf\descriptor_pb2.py in <module>()
   1837 FileDescriptorSet = _reflection.GeneratedProtocolMessageType('FileDescriptorSet', (_message.Message,), dict(
   1838   DESCRIPTOR = _FILEDESCRIPTORSET,
-> 1839   __module__ = 'google.protobuf.descriptor_pb2'
   1840   # @@protoc_insertion_point(class_scope:google.protobuf.FileDescriptorSet)
   1841   ))

TypeError: Expected a message Descriptor, got Descriptor
"
47652,"ValueError: Output tensors to a Model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: Tensor(""dense/truediv:0"", shape=(?, 2, 209), dtype=float32). It ","Runs without error locally but it comes to this error in aws SageMaker.
Hier is the model

input_train = np.column_stack((input_cat1, input_cat2, input_num, input_cat3))

os.makedirs(""./data"", exist_ok = True)
np.savez('./data/training', train_input = input_train, train_output=target_cat)

sage_maker_session = sagemaker.Session()
training_input_path = sage_maker_session.upload_data('data/training.npz', key_prefix=prefix + training_folder)
print(training_input_path)
print(training_input_path)

s3://sagemaker-eu-central-1-xxxxxxxxxxx/user_tracking/training/training.npz


**************************************************************************************************************************************
%%writefile train.py

#Nachdem das Programm definiert hat, welche Argumente es bentigt, argparse findet heraus, wie es diese aus sys.argv auslesen kann
import argparse
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, Activation, Dropout, TimeDistributed, RepeatVector
from tensorflow.keras.layers import  Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
import json
import os
import numpy as np
import pandas as pd


if __name__ == ""__main__"":
        
    parser = argparse.ArgumentParser()

    # hyperparameters, die spter eingestellt werden mssen, werden hier als command-line arguments addiert
    parser.add_argument('--epochs', type=int, default=60)
    parser.add_argument('--batch-size', type=int, default=50)
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    parser.add_argument('--training', type=str, default=os.environ['SM_CHANNEL_TRAINING'])
    
    args, _ = parser.parse_known_args()
    
    epochs     = args.epochs
    batch_size = args.batch_size
    model_dir  = args.model_dir
    training_dir   = args.training

    
    input_train =np.load(os.path.join(training_dir, 'training.npz'))['train_input']
    target =np.load(os.path.join(training_dir, 'training.npz'))['train_output']
    
    print (""input_train shape:"", input_train.shape)
    print (""target shape:"", target.shape)
    
    input_cat1 = input_train[:,0].astype(np.int32)
    input_cat2 = input_train[:,1].astype(np.int32)
    input_cat3 = input_train[:,3:].astype(np.int32)
    input_num = input_train[:,2].astype(np.float32)
    
    print (""shape input_cat1:"", input_cat1.shape)
    print (""shape input_cat2:"", input_cat2.shape)
    print (""shape input_cat3:"", input_cat3.shape)
    print (""shape input_num:"", input_num.shape)
    print (""shape target:"", target.shape)
    
    
    n_steps = 2     # number of timesteps in each sample
    num_unique_os = 5                  #len(le_betriebsystem.classes_)+1
    num_unique_browser = 10            #len(le_browser.classes_)+1
    num_unique_actions = 210           #len(le_actionen.classes_)+1
    
    os_emb_size = 32
    browser_emb_size = 32
    actions_emb_size = 64
    
    max_seq_len = 55
    
    #numeric Input 
    numerical_input = tf.keras.Input(shape=(1,), name='numeric_input')
    
    #categorical Input
    os_input = tf.keras.Input(shape=(1,), name='os_input')
    browser_input = tf.keras.Input(shape=(1,), name='browser_input')
    action_input= tf.keras.Input(shape=(max_seq_len,), name='action_input')
    
    emb_os = tf.keras.layers.Embedding(num_unique_os, os_emb_size)(os_input) 
    emb_browser = tf.keras.layers.Embedding(num_unique_browser, browser_emb_size)(browser_input)
    emb_actions = tf.keras.layers.Embedding(num_unique_actions, actions_emb_size)(action_input)
    
    actions_repr = tf.keras.layers.LSTM(300, return_sequences=True)(emb_actions)
    actions_repr = tf.keras.layers.LSTM(200)(emb_actions)
    
    emb_os = tf.squeeze(emb_os, axis=1)
    emb_browser = tf.squeeze(emb_browser, axis=1)
    
    activity_repr = tf.keras.layers.Concatenate()([emb_os, emb_browser, actions_repr, numerical_input])

    x = tf.keras.layers.RepeatVector(n_steps)(activity_repr)
    x = tf.keras.layers.LSTM(288, return_sequences=True)(x) 
    x = tf.keras.layers.LSTM(128, return_sequences=True)(x)

    def last_layer(x):
        x = tf.keras.layers.Dense(num_unique_actions-1, activation='softmax')
        return x
    
    next_n_actions = (tf.keras.layers.Lambda(last_layer))(x)
    
    #next_n_actions = tf.keras.layers.Dense(num_unique_actions-1, activation='softmax')(x)
    

    model = tf.keras.Model(inputs=[numerical_input, os_input, browser_input, action_input], outputs = next_n_actions)
    model.summary()
    
  
    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy']) 
    
    
    
    history = model.fit({'numeric_input': input_num,
            'os_input': input_cat1,
            'browser_input': input_cat2,
            'action_input': input_cat3}, target_cat, batch_size=50, epochs=150)
    
    tf.saved_model.simple_save(
        tf.keras.backend.get_session(),
        os.path.join(model_dir, '1'),
        inputs={'inputs': model.input},
        outputs={t.name: t for t in model.outputs})
   
************************************************************************************************************************************
`train_instance_type='ml.m5.xlarge'
#train_instance_type='local'
tf_version = tf.__version__

tf_estimator = TensorFlow(entry_point='train.py', 
                          role=role,
                          instance_count=1, 
                          instance_type=train_instance_type,
                          framework_version='1.12', 
                          py_version='py3',
                          script_mode=True,
                          hyperparameters={
                              'epochs': 150,
                              'batch-size': 50
                          }
                         )

tf_estimator.fit({'training': training_input_path})`


***********************************************************************************************************************************
2021-03-08 21:49:43 Starting - Starting the training job...
2021-03-08 21:50:07 Starting - Launching requested ML instancesProfilerReport-yyyyyyyyyyy: InProgress
......
2021-03-08 21:51:08 Starting - Preparing the instances for training...
2021-03-08 21:51:38 Downloading - Downloading input data...
2021-03-08 21:52:11 Training - Training image download completed. Training in progress.
2021-03-08 21:52:11 Uploading - Uploading generated training model
2021-03-08 21:52:11 Failed - Training job failed
2021-03-08 21:52:02,113 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training
2021-03-08 21:52:02,118 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-03-08 21:52:02,559 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-03-08 21:52:02,574 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-03-08 21:52:02,585 sagemaker-containers INFO     Invoking user script

Training Env:

{
    ""additional_framework_parameters"": {},
    ""channel_input_dirs"": {
        ""training"": ""/opt/ml/input/data/training""
    },
    ""current_host"": ""algo-1"",
    ""framework_module"": ""sagemaker_tensorflow_container.training:main"",
    ""hosts"": [
        ""algo-1""
    ],
    ""hyperparameters"": {
        ""batch-size"": 50,
        ""model_dir"": ""s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model"",
        ""epochs"": 150
    },
    ""input_config_dir"": ""/opt/ml/input/config"",
    ""input_data_config"": {
        ""training"": {
            ""TrainingInputMode"": ""File"",
            ""S3DistributionType"": ""FullyReplicated"",
            ""RecordWrapperType"": ""None""
        }
    },
    ""input_dir"": ""/opt/ml/input"",
    ""is_master"": true,
    ""job_name"": ""sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210"",
    ""log_level"": 20,
    ""master_hostname"": ""algo-1"",
    ""model_dir"": ""/opt/ml/model"",
    ""module_dir"": ""s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/source/sourcedir.tar.gz"",
    ""module_name"": ""train"",
    ""network_interface_name"": ""eth0"",
    ""num_cpus"": 4,
    ""num_gpus"": 0,
    ""output_data_dir"": ""/opt/ml/output/data"",
    ""output_dir"": ""/opt/ml/output"",
    ""output_intermediate_dir"": ""/opt/ml/output/intermediate"",
    ""resource_config"": {
        ""current_host"": ""algo-1"",
        ""hosts"": [
            ""algo-1""
        ],
        ""network_interface_name"": ""eth0""
    },
    ""user_entry_point"": ""train.py""
}

Environment variables:

SM_HOSTS=[""algo-1""]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={""batch-size"":50,""epochs"":150,""model_dir"":""s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model""}
SM_USER_ENTRY_POINT=train.py
SM_FRAMEWORK_PARAMS={}
SM_RESOURCE_CONFIG={""current_host"":""algo-1"",""hosts"":[""algo-1""],""network_interface_name"":""eth0""}
SM_INPUT_DATA_CONFIG={""training"":{""RecordWrapperType"":""None"",""S3DistributionType"":""FullyReplicated"",""TrainingInputMode"":""File""}}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[""training""]
SM_CURRENT_HOST=algo-1
SM_MODULE_NAME=train
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=4
SM_NUM_GPUS=0
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://sagemaker-eu-central-1-xxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/source/sourcedir.tar.gz
SM_TRAINING_ENV={""additional_framework_parameters"":{},""channel_input_dirs"":{""training"":""/opt/ml/input/data/training""},""current_host"":""algo-1"",""framework_module"":""sagemaker_tensorflow_container.training:main"",""hosts"":[""algo-1""],""hyperparameters"":{""batch-size"":50,""epochs"":150,""model_dir"":""s3://sagemaker-eu-central-1-xxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model""},""input_config_dir"":""/opt/ml/input/config"",""input_data_config"":{""training"":{""RecordWrapperType"":""None"",""S3DistributionType"":""FullyReplicated"",""TrainingInputMode"":""File""}},""input_dir"":""/opt/ml/input"",""is_master"":true,""job_name"":""sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210"",""log_level"":20,""master_hostname"":""algo-1"",""model_dir"":""/opt/ml/model"",""module_dir"":""s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/source/sourcedir.tar.gz"",""module_name"":""train"",""network_interface_name"":""eth0"",""num_cpus"":4,""num_gpus"":0,""output_data_dir"":""/opt/ml/output/data"",""output_dir"":""/opt/ml/output"",""output_intermediate_dir"":""/opt/ml/output/intermediate"",""resource_config"":{""current_host"":""algo-1"",""hosts"":[""algo-1""],""network_interface_name"":""eth0""},""user_entry_point"":""train.py""}
SM_USER_ARGS=[""--batch-size"",""50"",""--epochs"",""150"",""--model_dir"",""s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model""]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_CHANNEL_TRAINING=/opt/ml/input/data/training
SM_HP_BATCH-SIZE=50
SM_HP_MODEL_DIR=s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model
SM_HP_EPOCHS=150
PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages

Invoking script with the following command:

/usr/bin/python train.py --batch-size 50 --epochs 150 --model_dir s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model


input_train shape: (66, 58)
target shape: (66, 2, 209)
shape input_cat1: (66,)
shape input_cat2: (66,)
shape input_cat3: (66, 55)
shape input_num: (66,)
shape target: (66, 2, 209)
Traceback (most recent call last):
  File ""train.py"", line 105, in <module>
    model = tf.keras.Model(inputs=[numerical_input, os_input, browser_input, action_input], outputs = next_n_actions)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 121, in __init__
    super(Model, self).__init__(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py"", line 80, in __init__
    self._init_graph_network(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py"", line 224, in _init_graph_network
    '(thus holding past layer metadata). Found: ' + str(x))
**ValueError: Output tensors to a Model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: Tensor(""dense/truediv:0"", shape=(?, 2, 209), dtype=float32)**
2021-03-08 21:52:04,761 sagemaker-containers ERROR    ExecuteUserScriptError:
Command ""/usr/bin/python train.py --batch-size 50 --epochs 150--model_dir s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model""

****************************************************************************************************************************************
LOCALY I GET THIS

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
os_input (InputLayer)           [(None, 1)]          0                                            
__________________________________________________________________________________________________
browser_input (InputLayer)      [(None, 1)]          0                                            
__________________________________________________________________________________________________
action_input (InputLayer)       [(None, 55)]         0                                            
__________________________________________________________________________________________________
embedding_66 (Embedding)        (None, 1, 32)        160         os_input[0][0]                   
__________________________________________________________________________________________________
embedding_67 (Embedding)        (None, 1, 32)        352         browser_input[0][0]              
__________________________________________________________________________________________________
embedding_68 (Embedding)        (None, 55, 64)       13440       action_input[0][0]               
__________________________________________________________________________________________________
tf.compat.v1.squeeze_16 (TFOpLa (None, 32)           0           embedding_66[0][0]               
__________________________________________________________________________________________________
tf.compat.v1.squeeze_17 (TFOpLa (None, 32)           0           embedding_67[0][0]               
__________________________________________________________________________________________________
lstm_61 (LSTM)                  (None, 200)          212000      embedding_68[0][0]               
__________________________________________________________________________________________________
numeric_input (InputLayer)      [(None, 1)]          0                                            
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 265)          0           tf.compat.v1.squeeze_16[0][0]    
                                                                 tf.compat.v1.squeeze_17[0][0]    
                                                                 lstm_61[0][0]                    
                                                                 numeric_input[0][0]              
__________________________________________________________________________________________________
repeat_vector_19 (RepeatVector) (None, 2, 265)       0           concatenate_36[0][0]             
__________________________________________________________________________________________________
lstm_62 (LSTM)                  (None, 2, 288)       638208      repeat_vector_19[0][0]           
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 2, 209)       60401       lstm_62[0][0]    

                
************************************************************************************************************************************
Total params: 924,561
Trainable params: 924,561
Non-trainable params: 0"
47651,'tf.RGBToHSV' op is neither a custom op nor a flex op,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): pip3 install tensorflow
- TensorFlow version (or github SHA if from source):  2.4.1


**Provide the text output from tflite_convert**

```
2021-03-08 08:34:48.080129: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-08 08:34:48.080319: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-08 08:34:48.324839: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2021-03-08 08:34:48.733404: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)
2021-03-08 08:34:48.733653: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2021-03-08 08:34:48.734625: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2808000000 Hz
2021-03-08 08:34:48.735566: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.006ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.

2021-03-08 08:34:48.946989: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.
2021-03-08 08:34:48.947044: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.
loc(callsite(""model/lambda/RGBToHSV""(""/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"":748:0) at callsite(""/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_image_ops.py"":3270:0 at callsite(""../models.py"":8:0 at callsite(""/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py"":917:0 at callsite(""/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"":1012:0 at callsite(""/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"":560:0 at callsite(""/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"":424:0 at callsite(""/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"":1012:0 at callsite(""/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saving_utils.py"":135:0 at ""/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"":634:0)))))))))): error: 'tf.RGBToHSV' op is neither a custom op nor a flex op
error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
        tf.RGBToHSV {device = """"}
Tensorflow Lite does not support the operation
/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:748:0: error: 'tf.RGBToHSV' op is neither a custom op nor a flex op
/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_image_ops.py:3270:0: note: called from
../models.py:8:0: note: called from
/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py:917:0: note: called from
/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1012:0: note: called from
/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:560:0: note: called from
/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:424:0: note: called from
/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1012:0: note: called from
/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saving_utils.py:135:0: note: called from
/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:634:0: note: called from
<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
        tf.RGBToHSV {device = """"}
```

**Standalone code to reproduce the issue** 
Keras model:
```
from tensorflow import keras
from tensorflow.keras import layers

def hsv_conversion(x):
    return tf.image.rgb_to_hsv(x)

def create_model():
    layer1 = keras.Input((128,64,3))
    x = layers.Lambda(hsv_conversion)(layer1)
    x = layers.Conv2D(16, (5, 5), activation='relu', padding='same')(x)
    x = layers.MaxPooling2D((2,2),  padding='same')(x)
    x = layers.Flatten()(x)
    x = layers.Dense(32, activation='relu')(x)
    x = layers.Dropout(0.4)(x)
    output = layers.Dense(2, activation='softmax')(x)
    model = keras.Model(layer1, output)
    return model
```"
47649,TF 2.4.1 yields NaNs on Large Models using model.fit and Adam as compared to 2.3.0,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7


**Describe the current behavior**
Whenever I train this model on TF 2.4.1, after the second batch, it starts to output NaNs as the loss. However, whenever I run this same exact code on TF 2.3.0, it trains properly. The Adam optimizer has the same hyperparameters in both versions. I'm not sure what would cause the difference. 

**Code to Reproduce NaNs**

```python
import numpy as np
import tensorflow as tf

class MyLayer(tf.keras.layers.Layer):
    def __init__(
        self,
        input_dim: int = None,
        output_dim: int = 512,
        context_dim: int = 5,
        stride: int = 1,
        dilation: int = 1,
        kernel_initializer='glorot_uniform'
    ):
        super(MyLayer, self).__init__()
        self.input_dim = input_dim
        self.conv = tf.keras.layers.Conv1D(filters=output_dim,
                                           kernel_size=context_dim,
                                           strides=stride,
                                           dilation_rate=dilation,
                                           kernel_initializer=kernel_initializer)

    def call(self, x):
        batch_size, num_frames, num_feats = x.shape
        # if self.input_dim:
        #     assert self.input_dim == num_feats
        return self.conv(x)

class Reg(tf.keras.layers.Layer):
    def __init__(self,dropout_rate=0.2):
        super(Reg,self).__init__()
        self.bn = tf.keras.layers.BatchNormalization()
        self.do = tf.keras.layers.Dropout(rate=dropout_rate)
    def call(self,x,training=True):
        x = self.bn(x,training=training)
        return self.do(x,training=training)
        
class MyModel(tf.keras.Model):
    def __init__(self, input_dim, output_dim,dropout_rate=0.2,batch_norm=False, return_xvector=False):
        super().__init__()

        self.input_dim = input_dim
        self.output_dim = output_dim

        self.fc1 = MyLayer(input_dim=self.input_dim,output_dim=512,context_dim=5,dilation=1)
        self.fc2 = MyLayer(input_dim=512  ,output_dim=1536 ,context_dim=3, dilation=2)
        self.fc3 = MyLayer(input_dim=1536 ,output_dim=512  ,context_dim=3, dilation=3)
        self.fc4 = MyLayer(input_dim=512  ,output_dim=512  ,context_dim=1, dilation=1)
        self.fc5 = MyLayer(input_dim=512  ,output_dim=1500 ,context_dim=1, dilation=1)

        self.fc6 = tf.keras.layers.Dense(512)
        self.fc7 = tf.keras.layers.Dense(512)
        self.output_layer = tf.keras.layers.Dense(self.output_dim)

        if batch_norm:
            self.reg1 = Reg(dropout_rate)
            self.reg2 = Reg(dropout_rate)
            self.reg3 = Reg(dropout_rate)
            self.reg4 = Reg(dropout_rate)
            self.reg5 = Reg(dropout_rate)
            self.reg6 = Reg(dropout_rate)
        else:
            self.reg1 = tf.keras.layers.Dropout(rate=dropout_rate)
            self.reg2 = tf.keras.layers.Dropout(rate=dropout_rate)
            self.reg3 = tf.keras.layers.Dropout(rate=dropout_rate)
            self.reg4 = tf.keras.layers.Dropout(rate=dropout_rate)
            self.reg5 = tf.keras.layers.Dropout(rate=dropout_rate)
            self.reg6 = tf.keras.layers.Dropout(rate=dropout_rate)



    def call(self,x,training=True, return_logits=True):
        with tf.name_scope(""Extractor""):
            with tf.name_scope(""Fc1""):
                x = tf.nn.relu(self.fc1(x))
                x = self.reg1(x, training=training)
            with tf.name_scope(""Fc2""):
                x = tf.nn.relu(self.fc2(x))
                x = self.reg2(x, training=training)
            with tf.name_scope(""Fc3""):
                x = tf.nn.relu(self.fc3(x))
                x = self.reg3(x, training=training)
            with tf.name_scope(""Fc4""):
                x = tf.nn.relu(self.fc4(x))
                x = self.reg4(x, training=training)
            with tf.name_scope(""Fc5""):
                x = tf.nn.relu(self.fc5(x))
                x = self.reg5(x, training=training)
            with tf.name_scope(""StatsPool""):
                x = self.statpool(x)

            with tf.name_scope(""Segment6""):
                x = self.fc6(x)
        
        with tf.name_scope(""Classifier""):
            x = tf.nn.relu(x)
            x = self.reg6(x)
            x = tf.nn.relu(self.fc7(x))
            x = self.output_layer(x)
        if return_logits:
            return x 
        else:
            x = self.softmax(x)
            return x

    def statpool(self,x):
        mu = tf.math.reduce_mean(x,axis=1)
        sigma = tf.math.reduce_std(x,axis=1)
        return tf.concat([mu,sigma],1)
        
n_feats = 40
model = MyModel(n_feats,3)

loss_fn =  tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(loss=loss_fn, optimizer=""adam"",metrics = [""accuracy""])

def batch(N=4):
    for i in range(100):
        yield np.random.normal(size=(N,n_feats,200)), np.random.randint(0,3,size=(N))

history = model.fit(x = batch(N=300),
                    validation_data= batch(N=300),
                   )
```"
47647,Plotting a TensorFlow Graph with dot/graphviz,"**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it: Yes

**Describe the feature and the current behavior/state.**

There is [`tf.keras.utils.plot_model`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model), which draws a Keras model using dot/graphviz. It's used in many tutorials/notebooks, and I've found it very useful. However, it only works for Keras models. I could not find a way to plot arbitrary TensorFlow graphs.

I wanted to understand my TensorFlow graph, so [I ended up making `plot_graph`](https://gist.github.com/jameshfisher/f99ad86fc23d2ae7c856ee2f2ec89cd8). It lets you do:

```python
def py_func(x):
  if tf.random.uniform(()) < 0.5:
    x = x*x
  x = tf.cast(x, 'float32')
  return 2*x + 5

tf_func = tf.function(py_func)
tf_concrete_func = tf_func.get_concrete_function(tf.constant(3))
my_graph = tf_concrete_func.graph
plot_graph(my_graph)
```

This generates this image:

![graph](https://user-images.githubusercontent.com/166966/110341720-901d1300-8022-11eb-9397-c583a60cca70.png)

If there's interest, I could polish this and contribute it as a TensorFlow feature.

**Will this change the current api? How?**

No, it would be a standalone additional function, e.g. `tf.graph_util.plot_graph`.

**Who will benefit with this feature?**

* Those wanting to understand their TensorFlow graph visually
* Those wanting to understand the TensorFlow graph language through experimentation
* Tutorial/notebook writers
* Those not using TensorBoard

**Any Other info.**

* I'm aware of TensorBoard, but I haven't really used it. Is everyone using that to visualize their graphs?"
47644,Addition of  kernel variant directory with fast portable kernels,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): n/a
- Tensorflow version (commit SHA if source): 
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**

Currently tflite(u) only has reference kernel implementations (written for clarity / correctness) and target/platform-specific optimized kernels variants.    It lacks efficient portable kernel implementations needed to achieve more competitive performance on Microcontrollers without dedicated support.   This is especially true of smaller Microcontrollers without DSP/NN ISA extensions (e.g. small RISC-V devices) where good portable C++ implementations can achieve close-to-optimal results.

**Please provide the exact sequence of commands/steps when you ran into the problem**

n/a"
47642,Typo issue in tf.keras.losses docs,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy
https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy

## Description of issue (what needs changing):

### Clear description

https://github.com/tensorflow/tensorflow/blob/5b5527d81085db5dadf3886a37fef7c4943ad704/tensorflow/python/keras/losses.py#L587
(BinaryCrossentropy)
https://github.com/tensorflow/tensorflow/blob/5b5527d81085db5dadf3886a37fef7c4943ad704/tensorflow/python/keras/losses.py#L748
(SparseCategoricalCrossentropy)

Above lines should be

```
          **Note - Using from_logits=True may be more numerically stable.**
```

like

https://github.com/tensorflow/tensorflow/blob/5b5527d81085db5dadf3886a37fef7c4943ad704/tensorflow/python/keras/losses.py#L667

(CategoricalCrossentropy)
"
47640,GPU execution not deallocating memory in iOS swift,"**System information**
- OS Platform - iOS, swift
- Mobile device : iPhone, iPad
- TensorFlow installed from (source or binary): Pod.
- TensorFlow version (use command below): 2.3.0

**Issue**
Initialised interpreter once with below code
var delegate = MetalDelegate()
self.interpreter = try Interpreter(modelPath: modelPath, options: nil, delegates: [delegate])
self.interpreter?.allocateTensors()

Then executed the below code each time on running the inference
var inputTensor = try interpreter?.copy(input, toInputAt:0)
try interpreter?.invoke()
var outputTensor = try interpreter?.output(at: 0)

On running the .invoke() in GPU (using MetalDelegate) the memory consumption is increasing and not decreasing. How do i deallocate the unused memory? This is working fine in CPU."
47638,Cannot compile tensorflow lite application when -Werror=undef is actived,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Na
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): Tensorflow lite r2.4
- Python version: Na
- Bazel version (if compiling from source): Na
- GCC/Compiler version (if compiling from source): gcc version 9.2.0 
- CUDA/cuDNN version: Na
- GPU model and memory: Na

**Describe the current behavior**
When I try to compile the minimal.cc code example with the compilation flag -Werror=undef activated I have the following error :

g++ -std=c++17 -pthread -Werror=undef  minimal.cc -o minimal -I./tensorflow/include -L./tensorflow/lib -ltensorflow-lite -ldl
In file included from ./tensorflow/include/tensorflow/lite/allocation.h:27,
                 from ./tensorflow/include/tensorflow/lite/interpreter.h:34,
                 from minimal.cc:16:
./tensorflow/include/tensorflow/lite/c/common.h:87:26: error: ""\__clang_major__"" is not defined, evaluates to 0 [-Werror=undef]
  87 |     defined(HEXAGON) || (\__clang_major__ == 7 && \__clang_minor__ == 1) 
       |                                         ^~~~~~~~~~~~~~~
cc1plus: some warnings being treated as errors


**Describe the expected behavior**

Should compile without error

**Standalone code to reproduce the issue**

To reproduce just compile the code example minimal.cc with the follow command :
`g++ -std=c++17 -pthread -Werror=undef  minimal.cc -o minimal -I<Path to tf-lite include> -L<Path to tf-lite lib> -ltensorflow-lite -ldl`

**Other info / logs** 

It looks that the variable \__clang_major__ was not yet defined, so we found a way to solve this issue by adding (defined(\_clang_)   in the file [common.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/common.h) :

Here is the diff

diff \--git a/tensorflow/include/tensorflow/lite/c/common.h b/tensorflow/include/tensorflow/lite/c/common.h
index 5c2f408..bdcd636 100644
--- a/tensorflow/include/tensorflow/lite/c/common.h
+++ b/tensorflow/include/tensorflow/lite/c/common.h
@@ -84,7 +84,7 @@ typedef struct TfLiteIntArray {
 // https://github.com/google/re2/commit/b94b7cd42e9f02673cd748c1ac1d16db4052514c
 #if (!defined(\__clang__) && defined(\__GNUC__) && \__GNUC__ == 6 && \
      \__GNUC_MINOR__ >= 1) ||                                      \
\-    defined(HEXAGON) || (\__clang_major__ == 7 && \__clang_minor__ == 1)
\+    defined(HEXAGON) || (defined(\_clang_) && \__clang_major__ == 7 && \__clang_minor__ == 1) 
   int data[0];
 #else


With this modification the compilation finish without error

"
47637,Why not use multiple streams for model calculations?,"I see that Tensorflow uses one stream for model calculation by default, why not use multiple streams? Will there be any impact on using multiple streams?
I think that for some OPs that are not dependent on models, using multiple streams will increase the parallelism of calculations, then increasing the end-to-end training speed of the model.
Any advices can help me."
47636,TFLite: Not able to build target riscv,"
Hi,
I am trying to build target riscv, in riscv_makefile.inc at tensorflow/tensorflow/lite/tools/make/targets using toolchain riscv32-unknown-elf- .
I have built the toolchain using following sources at https://github.com/riscv/riscv-gnu-toolchain.git.
I am getting following error: ./tensorflow/lite/shared_library.h:22:10: fatal error: dlfcn.h: No such file or directory.
How can I resolve this?"
47634,How to reproduce Bert with XLA?,"Hi,  how to reproduce Bert with XLAIs there any relevant code provided
![image](https://user-images.githubusercontent.com/33742067/110291744-33702700-8027-11eb-80ac-5ee0f7c96184.png)
https://www.tensorflow.org/xla

I run the code from https://github.com/google-research/bert and enable xla, but it seems that it can't use XLA successfully."
47633,"why my 'workers=8,use_multiprocessing=True' do not work while training?","**Here is my code**
train_model_input=generate_arrays_from_dataframe( data.sample(frac=1) )
history = model.fit(train_model_input,  epochs=1, verbose=1, validation_split=0.0,steps_per_epoch= math.ceil( train_row_len/256),  workers=8,use_multiprocessing=True)  

I find it only 100% CPU-Util not the expect 800%.
![](https://user-images.githubusercontent.com/49393828/110285880-e4be8f00-801e-11eb-8e83-b26b4297729e.png)

And my GPU-Util is 0% although GPU memory is fully used
![](https://user-images.githubusercontent.com/49393828/110285828-ce183800-801e-11eb-9605-1e4289ae39b1.png)

How could I Increase the utilization of my CPU/GPU?
thank you!
"
47632,tensorflow numpy add __setitem__ hook,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):2.4.1
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
``` python
    a = np_array_ops.ones((2,2))
    a[0, 0] = 0.
    self.assertAllEqual([[0.0, 1.0], [1.0, 1.0]], a)
    a = np_array_ops.ones((2,2))
    a[0:2, 0] = [0., 0.]
    self.assertAllEqual([[0.0, 1.0], [0.0, 1.0]], a)
```
**Will this change the current api? How?**
Yes, support item assign likes numpy
**Who will benefit with this feature?**
Developers
**Any Other info.**
I found ```_with_index_update_helper``` in ```tensorflow/python/ops/numpy_ops/np_array_ops.py```, why not enable it?"
47631,ssd_mobilenet_v2 to 8-bit quantized tflite model conversion error,"Tensorflow version : 2.4.1

I have been trying to convert tensorflow model 'ssd_mobilenet_v2' to 8 bit quantized tflite model.
I was successful in converting from tf to just tfllite model ,BUT I was unable to convert to 8 bit quantized tflite model as it shows some errors.
My code is : 


import tensorflow as tf
import numpy as np
import cv2
import os

saved_model_dir = '/home/ssd_mobilenet_v2_2'

folder = '/mscoco_dataset_images'
 
def representative_data_gen() :
 i = 1
 for filename in os.listdir(folder): 
  k = (i/number_of_images)*100
  print(' PROCESSING :  ',k,' %')
  testimage = cv2.imread(os.path.join(folder,filename))
  testimage = np.expand_dims(testimage,0)      
  yield[testimage.astype(np.uint8)]   # doubt,why it is uint8 and not float32
  i = i+1   

onverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir,signature_keys = [""serving_default""]) 

converter.optimizations = [tf.lite.Optimize.DEFAULT]

converter.representative_dataset = representative_data_gen

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8 ,tf.lite.OpsSet.SELECT_TF_OPS] # part after coma is important for preventing tf.size() error

converter.inference_input_type = tf.uint8

converter.inference_output_type = tf.uint8

converter.allow_custom_ops = True #is it required?


tflite_model = converter.convert()

with open('batch_q_model.tflite', 'wb') as f:
  f.write(tflite_model)


**_I'm getting the error as :_**  
**RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor tfl.cast
Empty min/max for tensor tfl.cast**

Why is this error coming?I know it is an issue with related to calibration,i.e,representative dataset,but I've inputted the official mscoco dataset images.
Can someone please tell how to rectify this error 





"
47630,arm_nn_mat_mult_nt_t_s8.c:111: undefined reference to `__SXTB16_RORn,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) NA
- TensorFlow installed from (source or binary):source
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


Could you please help with the following?
Thanks a lot!

Compiling the hello_world example code on the Arduino Nano BLE 33 gives the following error: 
Error compiling for Arduino Nano BLE 33. 
I think it is related to the error below. Please see the partial dump pasted below. 
arm_nn_mat_mult_nt_t_s8.c:137: more undefined references to `__SXTB16_RORn' 

C:\Users\mushtaqsyed\Documents\Arduino\libraries\Arduino_TensorFlowLite\src\tensorflow\lite\micro\tools\make\downloads\kissfft\kiss_fft.c:378:9: warning: incompatible implicit declaration of built-in function 'memcpy'
C:\Users\mushtaqsyed\Documents\Arduino\libraries\Arduino_TensorFlowLite\src\tensorflow\lite\micro\tools\make\downloads\kissfft\kiss_fft.c:378:9: note: include '<string.h>' or provide a declaration of 'memcpy'
C:\Users\mushtaqsyed\Documents\Arduino\libraries\Arduino_TensorFlowLite\src\tensorflow\lite\micro\tools\make\downloads\kissfft\kiss_fft.c:378:9: warning: argument 2 null where non-null expected [-Wnonnull]
         memcpy(fout,tmpbuf,sizeof(kiss_fft_cpx)*st->nfft);
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\mushtaqsyed\Documents\Arduino\libraries\Arduino_TensorFlowLite\src\tensorflow\lite\micro\tools\make\downloads\kissfft\kiss_fft.c:378:9: note: in a call to built-in function 'memcpy'
libraries\Arduino_TensorFlowLite\tensorflow\lite\micro\tools\make\downloads\cmsis\CMSIS\NN\Source\NNSupportFunctions\arm_nn_mat_mult_nt_t_s8.c.o: In function `arm_nn_mat_mult_nt_t_s8':
C:\Users\mushtaqsyed\Documents\Arduino\libraries\Arduino_TensorFlowLite\src\tensorflow\lite\micro\tools\make\downloads\cmsis\CMSIS\NN\Source\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:111: undefined reference to `__SXTB16_RORn'
C:\Users\mushtaqsyed\Documents\Arduino\libraries\Arduino_TensorFlowLite\src\tensorflow\lite\micro\tools\make\downloads\cmsis\CMSIS\NN\Source\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:112: undefined reference to `__SXTB16_RORn'
C:\Users\mushtaqsyed\Documents\Arduino\libraries\Arduino_TensorFlowLite\src\tensorflow\lite\micro\tools\make\downloads\cmsis\CMSIS\NN\Source\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:118: undefined reference to `__SXTB16_RORn'
C:\Users\mushtaqsyed\Documents\Arduino\libraries\Arduino_TensorFlowLite\src\tensorflow\lite\micro\tools\make\downloads\cmsis\CMSIS\NN\Source\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:125: undefined reference to `__SXTB16_RORn'
C:\Users\mushtaqsyed\Documents\Arduino\libraries\Arduino_TensorFlowLite\src\tensorflow\lite\micro\tools\make\downloads\cmsis\CMSIS\NN\Source\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:136: undefined reference to `__SXTB16_RORn'
libraries\Arduino_TensorFlowLite\tensorflow\lite\micro\tools\make\downloads\cmsis\CMSIS\NN\Source\NNSupportFunctions\arm_nn_mat_mult_nt_t_s8.c.o:C:\Users\mushtaqsyed\Documents\Arduino\libraries\Arduino_TensorFlowLite\src\tensorflow\lite\micro\tools\make\downloads\cmsis\CMSIS\NN\Source\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:137: more undefined references to `__SXTB16_RORn' follow
collect2.exe: error: ld returned 1 exit status
exit status 1
Error compiling for board Arduino Nano 33 BLE.

"
47629,AttributeError when concatenating window datasets,"**System information**
- Windows 10
- TensorFlow installed from pip
- TensorFlow version 2.4.1 (git v2.4.0-49-g85c8b2a817f)

**Current behavior**
Concatenating two window datasets throws the following error: `AttributeError: 'DatasetSpec' object has no attribute 'most_specific_compatible_shape'`

**Expected behavior**
The code below returns a new dataset that is the concatenation of the two

**Standalone code to reproduce the issue**
```
import tensorflow as tf

tf.data.Dataset.range(3).window(1).concatenate(tf.data.Dataset.range(3, 6).window(1))
```
"
47627,tensorflow.python.framework.errors_impl.InvalidArgumentError error message details are wrong.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a.
- TensorFlow installed from (source or binary): Binary, using `pip install tensorflow`.
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0.
- Python version: Python 3.6.9.
- Bazel version (if compiling from source): N/a.
- GCC/Compiler version (if compiling from source): N/a.
- CUDA/cuDNN version: N/a.
- GPU model and memory: N/a.

**Describe the current behavior**
Exception:
```
  File ""./tune.py"", line 304, in train2
    keras.callbacks.EarlyStopping(monitor='Alpha', mode=""max"", patience=1, min_delta=0.0001)
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 855, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2943, in __call__
    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1919, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 560, in call
    ctx=ctx)
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float32), but the yielded element was (array([...], dtype=float32), array([...], dtype=float32)).
```

**Describe the expected behavior**
Exception message saying `The expected structure was tf.float32[A][B][C], but the yielded element was tf.float32[D][E][F]`.

**Standalone code to reproduce the issue**
No code example. Came up during hyperparameter search, the exception prevented hyperparameters from saving.

"
47624,keras.callbacks.EarlyStopping fails with RuntimeError: basic_string::_S_construct null not valid.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a.
- TensorFlow installed from (source or binary): Binary, using `pip install tensorflow`.
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0.
- Python version: Python 3.6.9.
- Bazel version (if compiling from source): N/a.
- GCC/Compiler version (if compiling from source): N/a.
- CUDA/cuDNN version: N/a.
- GPU model and memory: N/a.

**Describe the current behavior**
```
Traceback (most recent call last):
  File ""./tune.py"", line 304, in train2
    keras.callbacks.EarlyStopping(monitor='Alpha', mode=""max"", patience=1, min_delta=0.0001)
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 855, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2943, in __call__
    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1919, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 592, in call
    custom_gradient.copy_handle_data(func_graph_output, outputs[i])
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/handle_data_util.py"", line 53, in copy_handle_data
    if (target_t.dtype == dtypes.resource or
  File ""/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py"", line 203, in __eq__
    return self._type_enum == other._type_enum  # pylint: disable=protected-access
RuntimeError: basic_string::_S_construct null not valid
```

**Describe the expected behavior**
No exception, failure is unacceptable.

**Standalone code to reproduce the issue**
The issue happens during hyperparameter tuning. It happened in 4 trials out of 100, all of them pass the same arguments to `keras.callbacks.EarlyStopping`.

**Other info / logs** 
Exception message `basic_string::_S_construct null not valid` strongly suggests that `tensorflow` library passes a null pointer into `std::basic_string` constructor, which is a programming error in `tensorflow` code. 

"
47623,Tensorflow doesn't seem to work on rtx2060,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:none
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): V2.4.0rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.0
- Bazel version (if compiling from source):nonenone
- GCC/Compiler version (if compiling from source):none
- CUDA/cuDNN version : cuda_11.1.1_456.81/cuDNN v8.0.5
- GPU model and memory:RTX20606G
- CPU~~~~~~~~~~~~~:AMD 2600X  16G
You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I try to use tensorflow to construct mtcnn + facenet to realize face recognition, and refer to this https://github.com/kpzhang93/MTCNN_face_detection_alignment
When running mtcnn network, I found that the performance of rtx2060 was very poor. The occupancy rate reported by Windows system was only about 3%, and the occupancy rate queried by NVIDIA SMI was less than 7%.
I thought it was my code problem. Until I ran the MNIST sample, I found that this problem might be universal on my computer

**Describe the expected behavior**
The utilization rate of rtx2060 should be more than 60%. In the case of MNIST, every batch should be used_ The size is set to 256, which seems to only increase the pressure on the video memory.

Through NVIDIA SMI monitoring, the efficiency of rtx2060 is only about 4%
and
I added the running log of my dumnist below. According to the computing power of rtx2060, the time consumed by running this code should be reduced by more than half.

**Standalone code to reproduce the issue**
For the recurrence of MNIST, I do as follows:
```python
import tensorflow as tf
import sys
import os
import datetime
gpu = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpu[0], True)



starttime = datetime.datetime.now()

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0


model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=""relu""),
    tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['sparse_categorical_accuracy'])


checkpoint_save_path = ""./checkpoint/mnist/mnist.ckpt""
if os.path.exists(checkpoint_save_path + '.index'):
    print('------------load the model-----------')
    model.load_weights(checkpoint_save_path)

cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_save_path,
    save_weights_only=True,
    save_best_only=True
)
history = model.fit(x_train, y_train, batch_size=128, epochs=20,
                    validation_data=(x_test, y_test), validation_freq=1,
                    callbacks=[cp_callback])
model.summary()
endtime = datetime.datetime.now()
print((endtime - starttime).seconds)
sys.exit()`


**Other info / logs** Include any logs or source code that would be helpful to
I conducted 20 rounds of tests and the log is as follows:

runfile('E:/IDEA/pythonProject/mnist.py', wdir='E:/IDEA/pythonProject')
2021-03-08 00:27:44.912688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-08 00:27:46.503950: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-08 00:27:46.505289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-03-08 00:27:46.541927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:27:00.0 name: GeForce RTX 2060 computeCapability: 7.5
coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s
2021-03-08 00:27:46.542220: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-08 00:27:46.569789: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-08 00:27:46.569959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-08 00:27:46.575380: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-08 00:27:46.577802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-08 00:27:46.588545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-08 00:27:46.593147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-08 00:27:46.594651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-08 00:27:46.594860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-08 00:27:46.930131: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-08 00:27:46.931110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:27:00.0 name: GeForce RTX 2060 computeCapability: 7.5
coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s
2021-03-08 00:27:46.931392: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-08 00:27:46.931527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-08 00:27:46.931660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-08 00:27:46.931788: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-08 00:27:46.931926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-08 00:27:46.932050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-08 00:27:46.932189: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-08 00:27:46.932316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-08 00:27:46.932480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-08 00:27:47.489887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-08 00:27:47.490098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-08 00:27:47.490232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-08 00:27:47.490534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4720 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:27:00.0, compute capability: 7.5)
2021-03-08 00:27:47.491188: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
------------load the model-----------
2021-03-08 00:27:47.600796: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
Epoch 1/20
2021-03-08 00:27:47.960346: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-08 00:27:48.494384: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
1875/1875 [==============================] - 6s 3ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9939 - val_loss: 0.0778 - val_sparse_categorical_accuracy: 0.9767
Epoch 2/20
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9952 - val_loss: 0.0765 - val_sparse_categorical_accuracy: 0.9793
Epoch 3/20
1875/1875 [==============================] - 5s 3ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.0812 - val_sparse_categorical_accuracy: 0.9766
Epoch 4/20
1875/1875 [==============================] - 5s 3ms/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9968 - val_loss: 0.0808 - val_sparse_categorical_accuracy: 0.9780
Epoch 5/20
1875/1875 [==============================] - 4s 2ms/step - loss: 0.0102 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.0850 - val_sparse_categorical_accuracy: 0.9777
Epoch 6/20
1875/1875 [==============================] - 4s 2ms/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9971 - val_loss: 0.0846 - val_sparse_categorical_accuracy: 0.9782
Epoch 7/20
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.1029 - val_sparse_categorical_accuracy: 0.9765
Epoch 8/20
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0903 - val_sparse_categorical_accuracy: 0.9786
Epoch 9/20
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0076 - sparse_categorical_accuracy: 0.9977 - val_loss: 0.0801 - val_sparse_categorical_accuracy: 0.9811
Epoch 10/20
1875/1875 [==============================] - 4s 2ms/step - loss: 0.0074 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0942 - val_sparse_categorical_accuracy: 0.9792
Epoch 11/20
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0051 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.1002 - val_sparse_categorical_accuracy: 0.9788
Epoch 12/20
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.1038 - val_sparse_categorical_accuracy: 0.9785
Epoch 13/20
1875/1875 [==============================] - 5s 3ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.1138 - val_sparse_categorical_accuracy: 0.9776
Epoch 14/20
1875/1875 [==============================] - 4s 2ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.1081 - val_sparse_categorical_accuracy: 0.9791
Epoch 15/20
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.1001 - val_sparse_categorical_accuracy: 0.9803
Epoch 16/20
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.1246 - val_sparse_categorical_accuracy: 0.9783
Epoch 17/20
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.1079 - val_sparse_categorical_accuracy: 0.9804
Epoch 18/20
1875/1875 [==============================] - 4s 2ms/step - loss: 0.0020 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1111 - val_sparse_categorical_accuracy: 0.9795
Epoch 19/20
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.1124 - val_sparse_categorical_accuracy: 0.9784
Epoch 20/20
1875/1875 [==============================] - 4s 2ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.1386 - val_sparse_categorical_accuracy: 0.9756
Model: ""sequential""

Layer (type)                 Output Shape              Param    
=================================================================
flatten (Flatten)            (None, 784)               0         

dense (Dense)                (None, 128)               100480    

dense_1 (Dense)              (None, 10)                1290      
=================================================================
Total params: 101,770
Trainable params: 101,770
Non-trainable params: 0

94
===========================LOGs END===========================
```"
47622,Build TFLite Micro for RISC-V,"@tensorflow/micro

How do I build TFLite Micro for RISC-V MCU's? I use:
`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mcu_riscv TARGET_ARCH=riscv32_mcu hello_world_bin`
but the binary produced at **_/micro/tools/make/gen/mcu_riscv_riscv32_mcu_default/bin/_** seems to not have used the riscv-gcc toolchain as specified by _**mcu_riscv_makefile.inc**_. An x86-64 executable was built.
"
47619,mobilenet_v3.preprocess_input does nothing!,"## URL(s) with the issue:

* The documentation: https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input
* The source: https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/applications/mobilenet_v3.py#L556-L558

## Description of issue (what needs changing):

This function does nothing: it just returns its first argument. But the docs say it does multiple things: ""The preprocessed data are written over the input data""; ""The inputs pixel values are scaled between -1 and 1"".

The source seems correct ... in the sense that MobileNetV3 appears to work correctly with inputs with ""3 color channels, with values in the range [0, 255]"".

I'm guessing the docs here have just been copy-pasted from [the MobileNetV2 docs](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2/preprocess_input) ...?"
47618,Can we add a check in Model.fit on dataset element_spec?,"**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

I have this code
```python
import tensorflow as tf

length = 500
features = list(range(length))
labels = tf.random.uniform([length], minval=0, maxval=2, dtype=tf.int32)
data = tf.transpose([range(length),
					 tf.random.uniform([length], minval=0, maxval=2, dtype=tf.int32)])

dataset = tf.data.Dataset.from_tensor_slices(data)

dataset.shuffle(length)

train_length = int(length / 5 * 4)
train_data = dataset.take(train_length)
test_data = dataset.skip(train_length)


assert isinstance(train_data.element_spec, tuple) and len(train_data.element_spec) > 0, \
	'When x is dataset, its members must be a tuple of either (inputs, targets) or (inputs, targets, sample_weights). Currently your tuple size is 0.'

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'], run_eagerly=True)
model.fit(train_data.batch(10), validation_data=test_data.batch(10), epochs=10)
```

If we ignore the assert, running the code throws error 

> ValueError: No gradients provided for any variable: ['dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'].

The error reason may not be obvious that train_data doesn't return required examples, ie. ""either (inputs, targets) or (inputs, targets, sample_weights).""


I hope the error message can be improved. Is it ok if we detect `len(train_data.element_spec)` in places like `tensorflow.python.keras.engine.data_adapter.DatasetAdapter._validate_args`? 

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
People writting buggy code, learning dataset, not give correct shape

"
47616,Dynamic preprocessing of image inside model class. ,"I want to take input of image with dynamic size and do the preprocessing on it. Especially, I want to resize the input image with dynamic size to (512x256) and then restore it back to the original input size. 

```
import tensorflow as tf 

def resize(img, current_size, target_size, mask=False):
    frame_h, frame_w = target_size
    image_h, image_w = current_size
    delta_h = tf.abs(image_h - frame_h)
    delta_w = tf.abs(image_w - frame_w)
    resized = tf.image.pad_to_bounding_box(
        img, delta_h // 2, delta_w // 2, frame_h, frame_w
    )
    top, bottom, left, right = delta_h // 2, delta_h // 2, delta_w // 2, delta_w // 2
    return resized, [top, bottom, left, right]

def restore_resize(img, size, paddings, mask=False):
    top, bottom, left, right = paddings
    img = img[:, top : (img.shape[1] - bottom), left : (img.shape[2] - right), :]
    if mask:
        img = tf.image.resize(img, size, method=""nearest"")
    else:
        img = tf.image.resize(img, size, method=""bilinear"")
    return img

def test_model():
   inputs = {""person_orig"": tf.keras.layers.Input([None, None, 3])}
   person = inputs[""person_orig""]
   person_resized = tf.image.resize(person, (512, 256), preserve_aspect_ratio=True)
   output, paddings = resize(
      person_resized,
      (person_resized.shape[1], person_resized.shape[2]),
      (512, 256),
  )
   output = restore_resize(output, (person_resized.shape[1], person_resized.shape[2]), paddings)
   return tf.keras.models.Model(inputs=inputs, outputs=output)

m = test_model()
print(m.summary())




---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-42-dd351506f63b> in <module>()
----> 1 m = test_model()
      2 print(m.summary())

1 frames
<ipython-input-40-ff06df5ad810> in resize(img, current_size, target_size, mask)
      2     frame_h, frame_w = target_size
      3     image_h, image_w = current_size
----> 4     delta_h = tf.abs(image_h - frame_h)
      5     delta_w = tf.abs(image_w - frame_w)
      6     resized = tf.image.pad_to_bounding_box(

TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'
```

---
[Link to reproduce error](https://colab.research.google.com/drive/1b5KEY9XTo6aA9dxii0PdH5DPpLyOn4em?usp=sharing)
"
47615,Error with cude even with tf nightly,"<em>Please make sure that this is a bug. As per our

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I juste wrote `from tensorflow.keras import layers
`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): W10 Family
- TensorFlow installed from (source or binary): Don't
- TensorFlow version (use command below):  tf-estimator-nightly-2.5.0.dev2021030601 
pip install tf-nightly-gpu
- Python version: 3.7
- CUDA/cuDNN version: 11.2
- GPU model and memory: GTX 1660TI 6gb



**Describe the current behavior**
I try to run script using tensorflow . But I have : 

> 2021-03-07 06:41:09.619717: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2021-03-07 06:41:09.634433: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.


**Describe the expected behavior**
I should not have an error.

**Standalone code to reproduce the issue**
Write a script with this code : `from tensorflow.keras import layers
`
pip install tf-nightly-gpu
"
47613,pppppp,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47611,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
47610,Can't Quantize Inputs As Uint8 In TFLM,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution: Windows 10
- TensorFlow installed from Source
- Tensorflow version 2.4.1
- Target platform Visual Studio

**Describe the problem**
I have been troubleshooting problems trying to run inference on a model using Tensorflow Lite for MIcrocontrollers in Visual Studio. I found the issue to be that the lite/micro/kernells/quantize.cc kernel is unable to accept Uint8 inputs, but supports UInt8 outputs. From lite/micro/kernels/quantize.cc in the Prepare() function, UInt8 is not included in the input type check:

```
  TF_LITE_ENSURE(context, input->type == kTfLiteFloat32 ||
                              input->type == kTfLiteInt16 ||
                              input->type == kTfLiteInt8);
  TF_LITE_ENSURE(context, output->type == kTfLiteUInt8 ||
                              output->type == kTfLiteInt8 ||
                              output->type == kTfLiteInt16 ||
                              output->type == kTfLiteInt32);
```
I originally created my quantized model using the conversion code example provided here, which shows using UInt8 for the input:

https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_integer-only_quantization

Should the TFLM implementation of the quantize kernel support UInt8? Or  perhaps the example should be updated to show using Int8?


"
47609,TLFM Problem Compiling micro_allocator.cc In Visual Studio With MSVC,"@tensorflow/micro

**System information**
- Windows 10
- TensorFlow installed from source
- Tensorflow version 2.4.1:
- Target platform: Windows Visual Studio 2019 MSVC. Building with C++ Language Standard Default (ISO C++14 Standard) and C Language Standard Default (Legacy MSVC)

**Describe the problem**
I am working to port TFLM to Visual Studio and almost have it working. I am currently stuck at one issue. In micro_allocator.cc, there is a declaration of a constant TFLiteIntArray:

//tensorflow/lite/micro/micro_allocator.cc line 63:
`const TfLiteIntArray kZeroLengthIntArray = {};`

TFLiteIntArray uses a variable length array at the end.

//tensorflow/lite/c/common.h line 81:
```
typedef struct TfLiteIntArray {
  int size;
// gcc 6.1+ have a bug where flexible members aren't properly handled
// https://github.com/google/re2/commit/b94b7cd42e9f02673cd748c1ac1d16db4052514c
#if (!defined(__clang__) && defined(__GNUC__) && __GNUC__ == 6 && \
     __GNUC_MINOR__ >= 1) ||                                      \
    defined(HEXAGON) || (__clang_major__ == 7 && __clang_minor__ == 1)
  int data[0];
#else
  int data[];
#endif
} TfLiteIntArray;
```

The micro_allocator.cc will not compile in MSVC, as it generates an error [C466](https://docs.microsoft.com/en-us/cpp/error-messages/compiler-errors-1/compiler-error-c2466?f1url=%3FappId%3DDev16IDEF1%26l%3DEN-US%26k%3Dk(C2466)%26rd%3Dtrue&view=msvc-160), cannot allocate an array of constant size 0:

The C466 error can supposedly be disabled with the ""Disable Language Extensions"" compiler setting (/Za). This setting is incompatible with compiler C17, thus I'm using C14. However when I set /Za with C14, the error is still generated for micro_allocator.cc, and creates a lot of problems elsewhere in the project.

There are two places in micro_allocator where the kZeroLengthIntArray object is referenced, in both cases the address of kZeroLengthIntArray is being used when a tensor's shape is found to be null:

```
if (flatbuffer_tensor.shape() == nullptr) {
    // flatbuffer_tensor.shape() can return a nullptr in the case of a scalar
    // tensor.
    result->dims = const_cast<TfLiteIntArray*>(&kZeroLengthIntArray);
  }
```

I have resolved the compiler issue by creating a pointer to an empty zero length int array object, and a function that initializes the object at runtime and returns a pointer to that object:

```
TfLiteIntArray* pZeroLengthIntArray = 0;
TfLiteIntArray* GetZeroLengthIntArray()
{
    if (pZeroLengthIntArray == 0)
    {
        pZeroLengthIntArray = (TfLiteIntArray *)malloc(sizeof(TfLiteIntArray));
        pZeroLengthIntArray->size = 0;
    }
    return pZeroLengthIntArray;
}
```

If a tensor's shape is found to be null, I set its dims pointer to this dynamically allocated object by calling the function:

```
  if (flatbuffer_tensor.shape() == nullptr) {
    // flatbuffer_tensor.shape() can return a nullptr in the case of a scalar
    // tensor.
    result->dims = const_cast<TfLiteIntArray*>(GetZeroLengthIntArray());
  }
```

Question:
Is there some other way to get MSVC to compile micro_allocator.cc? If not, should I submit the above work-around as a general pull-request that will be compatible with MVSC as well as gnu/other compilers?
"
47608,BatchNorm: Documentation: Problem in Inference Equation (and also training),"in the following links 'sqrt' is missing:
https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization#used-in-the-notebooks_1

**During inference:**
instead of  :  (batch - self.moving_mean) / `sqrt`(self.moving_var + epsilon) * gamma + beta
it is written:  (batch - self.moving_mean) / (self.moving_var + epsilon) * gamma + beta.

**During training**
instead of:    (batch - mean(batch)) /`sqrt`(var(batch) + epsilon) * gamma + beta
it is written:  (batch - mean(batch)) / (var(batch) + epsilon) * gamma + beta"
47607,Hexagon 685/690 wrong SoC examples for QCS610/QCS410 on TensorFlow Lite Hexagon delegate ,"On this page:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/hexagon_delegate.md

It states that QCS610, QCS410 are SoC examples for Hexagon 690. However, according to Qualcomm's product info:
https://www.qualcomm.com/products/qcs610

Both SoC QCS610 and QCS410 contain Hexagon 685 rather than Hexagon 690.

"
47606,Could not load dynamic library 'libcudnn.so.7' ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**
- TensorFlow version: 2.2.2
- Python version: python 3.6.13
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version:CUDA 10.1/cuDNN version 7.6.5
- GPU model and memory: GeForce RTX 965M, 8GB



**Describe the problem**
I followed [the cuda installation guide](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) and I also used a **docker container**. Inside the container I tried to run a simple GPU test using the` tf.config.list_physical_devices('GPU')`  and I got the libcudnn error. I am very new to linux and docker so I don't know what infos I should provide to provide more insight to the situation at hand.

> tf.config.list_physical_devices('GPU')
> 2021-03-06 22:01:00.508031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
> 2021-03-06 22:01:00.546788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-06 22:01:00.547204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
> pciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0
> coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s
> 2021-03-06 22:01:00.547439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
> 2021-03-06 22:01:00.549211: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
> 2021-03-06 22:01:00.550911: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
> 2021-03-06 22:01:00.551227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
> 2021-03-06 22:01:00.553568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
> 2021-03-06 22:01:00.554930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
> 2021-03-06 22:01:00.555099: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/include:/usr/local/cuda/lib64:/usr/local/cuda-10.1/lib:/usr/local/cuda-10.1/lib64
> 2021-03-06 22:01:00.555113: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
> Skipping registering GPU devices...
> 

**Any other info**
I have tried adding a bunch of path into my LD_LIBRARY_PATH and ~/.bashrc and also deleting and creating a new symlink in /usr/local/cuda/lib64 for libcudnn.so.7 and libcudnn.so to libcudnn.7.6.5 as per suggested in #20271 and none worked. Other things I have tried is to delete all the libcudnn from /usr/local/cuda-10.1/lib64 and then install the cudnn again both from tar and the debian installation, and it doesn't work either.
"
47603,"Same random seed(s), different tensorflow versions, different results","I'm trying to get the same results using tf 1.15 and tf 2.4.1. Here are 2 examples that I'm expecting to produce similar results. The first example using `tf.layers.conv2d()` and the second using `tf.keras.layers.Conv2D()`. I tried using `kernel_initializer=some_tf_initializer(seed=seed)` and I also tried without using a kernel initializer, still the same. 
___

**TF1**

    import tensorflow as tf
    import numpy as np
    
    
    if __name__ == '__main__':
        seed = 1
        np.random.seed(seed)
        tf.set_random_seed(seed)
        session = tf.InteractiveSession()
        initializer = tf.initializers.orthogonal(1.0, seed)
        x = np.random.random((2, 84, 84, 1))
        xp = tf.placeholder(x.dtype, x.shape)
        result = tf.layers.conv2d(xp, 32, 8, 4, kernel_initializer=initializer)
        session.run(tf.global_variables_initializer())
        print(f'Conv2D output:\n{session.run([result], {xp: x})}\n{100 * ""=""}')
        print(f'x:\n{x}')


**Results in**

    Conv2D output:
    [array([[[[-4.92592295e-01,  5.84946930e-01,  6.45957303e-01, ...,
               3.65328020e-01,  4.03869755e-01, -1.06886940e+00],
             [ 5.37549724e-02,  5.47611947e-01,  3.91425858e-01, ...,
               1.20702194e-01,  9.21035825e-02, -1.10599980e+00],
             [-4.68301348e-01,  8.54547125e-01,  2.99738041e-01, ...,
              -7.47656723e-02, -3.29838560e-01, -6.43979360e-01],
             ...,
             [-3.61929553e-01,  5.70854964e-01,  1.49096153e-01, ...,
               4.05001572e-01, -5.70764458e-01, -8.50054931e-01],
             [-5.23792632e-01,  5.79328891e-01,  4.36402398e-01, ...,
               4.66264733e-01, -8.00687780e-02, -1.05017933e+00],
             [-9.59200260e-01,  9.77023018e-01,  7.22006476e-01, ...,
               3.29199395e-01, -6.48041695e-02, -9.65927090e-01]],
    
            [[-3.05192775e-01,  5.62304495e-01,  2.66985564e-01, ...,
               4.01866526e-01, -5.27853938e-01, -1.36294176e+00],
             [-3.33627733e-01,  8.70909716e-01, -1.90685411e-01, ...,
               5.82781510e-01, -9.54447222e-04, -5.93152081e-01],
             [ 1.05312097e-01,  1.34995604e+00,  4.18403345e-01, ...,
               7.29398371e-01, -1.52729852e-01, -1.44087877e+00],
             ...,
             [-2.65877698e-01,  1.30702457e+00,  2.12075863e-01, ...,
               4.02235128e-01, -4.83213829e-02, -9.84811507e-01],
             [-2.33065665e-01,  8.06240360e-01,  2.04250988e-01, ...,
               5.11907848e-01, -2.97176027e-01, -5.89994050e-01],
             [-6.06616196e-01,  7.52128441e-01,  6.05931022e-01, ...,
               8.25940123e-01, -8.11965401e-01, -1.38039418e+00]],
    
            [[-1.14268620e-02,  9.44562211e-01,  7.30843200e-01, ...,
               7.71483717e-01, -3.62299259e-01, -5.19022458e-01],
             [-1.25248650e-01,  8.31288483e-01,  3.01203507e-01, ...,
               6.10087249e-02, -1.37678504e-01, -1.12305468e+00],
             [-2.37495350e-01,  5.80913482e-01,  2.48314393e-01, ...,
               6.06333851e-01, -3.30380816e-01, -1.00515721e+00],
             ...,
             [ 1.95822525e-01,  9.85462620e-01,  9.30753642e-02, ...,
               5.07316387e-01, -2.72563623e-01, -3.44717273e-01],
             [-7.58518148e-01,  7.18211754e-01,  3.56392246e-01, ...,
               5.41345861e-01, -2.84327606e-01, -5.78967512e-01],
             [-9.36528091e-01,  4.51066108e-01,  3.60469945e-01, ...,
               6.06252149e-01, -9.82606865e-02, -1.12673980e+00]],
    
            ...,
    
            [[-3.23906425e-01,  4.89133765e-01,  5.51718357e-01, ...,
              -1.33718077e-01,  1.23015200e-03, -5.54514943e-01],
             [-5.04556877e-01,  7.36618066e-01,  2.02579467e-01, ...,
              -3.74142562e-01,  6.00494771e-03, -1.31111289e+00],
             [-4.16236220e-01,  5.98196128e-01,  6.97848461e-01, ...,
               6.23122747e-01, -1.07252840e-01, -4.83301913e-01],
             ...,
             [ 1.06692401e-01,  1.73388947e+00,  3.22292122e-01, ...,
              -1.75720933e-01, -2.96764992e-01, -1.06044579e+00],
             [-5.62972482e-01,  1.36246077e+00,  9.13179694e-01, ...,
               6.32824517e-01, -4.73498606e-01, -9.47552266e-01],
             [-5.11697389e-01,  6.66774542e-01,  3.82947609e-01, ...,
               5.67110785e-01, -7.11579248e-03, -7.77461939e-01]],
    
            [[-1.11410557e-01,  1.10325702e+00,  5.50558807e-01, ...,
               5.51697720e-01, -3.75442814e-01, -4.68653786e-01],
             [-3.26447172e-01,  1.19972335e+00,  3.82313314e-01, ...,
               2.20346417e-01, -3.39678412e-01, -8.66913575e-01],
             [-3.98474415e-01,  7.14095329e-01,  8.04559140e-02, ...,
               1.19488448e-01, -2.52660129e-01, -9.91879947e-01],
             ...,
             [-5.70385227e-01,  8.52797253e-01,  4.66251675e-01, ...,
              -3.24400644e-01,  3.20423484e-03, -1.39903236e+00],
             [-2.07675682e-01,  6.29120258e-01,  6.79053796e-01, ...,
               1.07336154e-01,  1.87886060e-01, -3.86794041e-01],
             [-2.67700849e-02,  4.49128504e-01,  3.40760390e-01, ...,
               6.36000637e-01, -3.77890278e-01, -3.30207391e-01]],
    
            [[-3.10700139e-01,  8.18823907e-01,  3.67007768e-03, ...,
               4.90558846e-01, -8.36860336e-01, -1.03869365e+00],
             [-4.88181365e-01,  8.00080028e-01,  3.56097390e-01, ...,
               4.76738039e-01, -3.42056012e-01, -5.30593649e-01],
             [-4.60314405e-01,  7.07439805e-01,  4.22496599e-01, ...,
               5.05574451e-01, -1.24705048e-01, -2.87646769e-01],
             ...,
             [-2.57950491e-01,  1.20126622e+00,  5.56477074e-02, ...,
               6.11127028e-01, -1.93377726e-01, -1.07765356e+00],
             [-6.50455755e-01,  1.29336304e+00,  8.44637456e-01, ...,
               1.81637465e-01, -2.88107846e-01, -4.49444781e-01],
             [-4.21331048e-01,  3.20901642e-01,  8.83963968e-01, ...,
               8.65426315e-01, -4.92428131e-01, -8.49800993e-01]]],
    
    
           [[[-5.16387221e-01,  6.99757393e-01,  7.85561831e-01, ...,
               1.03644383e-01, -3.70012470e-01, -5.32289379e-01],
             [-7.05045607e-02,  6.10474017e-01,  3.49680420e-01, ...,
               7.92201453e-01, -5.39308419e-01, -3.42154387e-01],
             [-4.22583634e-01,  8.01291482e-01,  1.48925846e-01, ...,
               7.68131504e-01, -7.38695500e-01, -1.09516989e+00],
             ...,
             [-7.26389962e-01,  6.26188865e-01,  5.42793169e-01, ...,
               3.52803865e-01, -3.46388144e-01, -7.60162696e-01],
             [-9.04680334e-02,  1.75359623e-01,  8.00145598e-01, ...,
               2.51379785e-01, -2.79905131e-01, -8.72223633e-01],
             [-4.20990087e-01,  3.09422635e-01,  4.11481559e-01, ...,
               3.69259084e-02, -1.60866470e-01, -5.05859647e-01]],
    
            [[-4.38932989e-01,  9.72496331e-01,  2.50341307e-01, ...,
               2.83248632e-01, -3.26580435e-01, -1.03773839e+00],
             [-8.19208455e-01,  8.96083502e-01,  4.24397325e-01, ...,
              -5.64658536e-02, -6.09974865e-01, -1.23144756e+00],
             [-9.45132686e-01,  9.13248242e-01,  8.86299832e-01, ...,
               2.15502049e-01, -5.58705913e-01, -2.25369791e-01],
             ...,
             [-8.56722975e-01,  7.94114365e-01,  5.81065297e-01, ...,
               4.96281428e-01, -1.03129758e+00, -1.05092158e+00],
             [-2.82233182e-02,  9.48533077e-01,  6.68768609e-01, ...,
               5.78499983e-01, -6.95651561e-01, -9.76414384e-01],
             [-8.06302266e-01,  1.05147010e+00,  4.22578075e-01, ...,
               2.94475123e-01,  1.74168406e-01, -1.14952206e+00]],
    
            [[-3.06847178e-01,  9.29719098e-01,  2.68849689e-01, ...,
               3.96790007e-01, -3.59629268e-01, -9.25076133e-01],
             [-2.38372207e-01,  5.78896860e-01,  2.17575482e-01, ...,
              -6.97642069e-02, -2.63976905e-01, -9.10845525e-01],
             [-5.40468423e-01,  1.20147694e+00,  4.04620974e-01, ...,
               1.76649501e-01,  1.46967870e-01, -6.74134783e-01],
             ...,
             [-6.27551970e-01,  5.05884731e-01,  2.10479188e-01, ...,
               3.09780840e-01,  1.46682047e-01, -1.07307698e+00],
             [-6.44653887e-01,  8.90779216e-01,  3.82747674e-01, ...,
               1.72155001e-01, -4.48190676e-01, -8.62522695e-01],
             [-1.50221285e-01,  9.35658435e-01,  3.59703911e-01, ...,
              -8.34232530e-02, -1.89649715e-01, -7.73705172e-01]],
    
            ...,
    
            [[-3.47146995e-01,  6.34597952e-02,  4.73544353e-01, ...,
              -8.20012972e-02, -1.78752555e-01, -1.36296041e+00],
             [-8.95298221e-01,  8.66479595e-01,  5.61457267e-01, ...,
               2.22826074e-01, -4.86448503e-01, -8.56547191e-01],
             [-8.62118822e-01,  4.83077034e-01,  3.60908309e-02, ...,
               3.29259093e-01, -4.08073639e-02, -6.45683881e-01],
             ...,
             [-5.01558985e-01,  4.62753228e-01,  4.01966678e-01, ...,
               6.35652593e-01, -7.45519465e-02, -5.39741380e-01],
             [ 1.29982837e-01,  7.31941977e-01, -2.05750614e-01, ...,
               3.16325608e-01, -3.28495177e-01, -1.09927128e+00],
             [-4.90335504e-01,  4.94757973e-01,  9.40801327e-02, ...,
               5.66634378e-02, -7.30613447e-01, -7.25730750e-01]],
    
            [[-2.75066335e-01,  7.35209409e-01,  5.99839688e-01, ...,
               6.71254510e-02,  7.97677772e-02, -7.72461196e-01],
             [-1.58777502e-01,  7.51857910e-01,  3.80584693e-01, ...,
              -1.01868390e-01, -7.12568409e-02, -6.42932271e-01],
             [-1.45951405e-01,  1.03692197e+00,  4.91333873e-01, ...,
               2.98796942e-01, -5.46416283e-01, -1.04881221e+00],
             ...,
             [-9.75652891e-03,  8.28500896e-01,  3.48450207e-01, ...,
               4.07241092e-01, -2.34265134e-01, -5.27081486e-01],
             [-5.65917326e-01,  7.37496827e-01,  1.65005917e-01, ...,
               6.61606291e-01, -2.20420580e-01, -1.11307865e+00],
             [-2.21167732e-01,  4.83734785e-01,  4.59793140e-01, ...,
               4.19304500e-01, -2.96987262e-01, -2.01353157e-01]],
    
            [[-6.33535626e-01,  1.11787318e+00,  6.41380845e-01, ...,
               1.12652086e-01, -1.99377096e-02, -6.69645437e-01],
             [-1.05662073e-01,  3.87718948e-01,  4.30142659e-01, ...,
               3.27637078e-01, -3.59568870e-01, -9.63431155e-01],
             [-1.26793132e-01,  1.29664648e+00,  3.28922837e-01, ...,
               3.21313848e-01, -7.53446525e-01, -7.93674733e-01],
             ...,
             [-5.32937597e-01,  1.09915270e+00,  6.23443352e-01, ...,
               9.96585500e-01, -6.21343220e-01, -1.01232184e+00],
             [ 5.05322966e-02,  1.18874480e+00,  4.57358272e-01, ...,
               4.80935716e-01, -2.04122013e-01, -1.13864994e+00],
             [-1.03084733e-01,  1.14916096e+00,  2.73508528e-01, ...,
               6.76093153e-01, -3.34324702e-01, -1.28436283e+00]]]])]
    ====================================================================================================
    x:
    [[[[4.17022005e-01]
       [7.20324493e-01]
       [1.14374817e-04]
       ...
       [6.23672207e-01]
       [7.50942434e-01]
       [3.48898342e-01]]
    
      [[2.69927892e-01]
       [8.95886218e-01]
       [4.28091190e-01]
       ...
       [1.85762022e-02]
       [7.00221437e-02]
       [4.86345111e-01]]
    
      [[6.06329462e-01]
       [5.68851437e-01]
       [3.17362409e-01]
       ...
       [9.18601778e-01]
       [4.02024891e-04]
       [9.76759149e-01]]
    
      ...
    
      [[5.89549934e-01]
       [3.89137609e-01]
       [5.05975232e-01]
       ...
       [4.35888475e-01]
       [7.89075202e-01]
       [4.66467704e-01]]
    
      [[6.73554921e-01]
       [8.84836452e-01]
       [9.38138449e-01]
       ...
       [7.93970466e-01]
       [2.13784215e-01]
       [6.41105035e-01]]
    
      [[7.31134736e-01]
       [9.50619892e-02]
       [7.00729238e-02]
       ...
       [9.95522026e-01]
       [4.81429517e-01]
       [8.37812754e-01]]]
    
    
     [[[6.03655452e-01]
       [6.64374944e-01]
       [2.72461392e-01]
       ...
       [1.14069927e-01]
       [2.93705095e-01]
       [8.78904978e-03]]
    
      [[2.53263696e-01]
       [8.37712781e-01]
       [8.07756027e-01]
       ...
       [7.37152445e-01]
       [6.00521471e-01]
       [7.37999367e-01]]
    
      [[3.75760828e-01]
       [9.11106703e-01]
       [8.72308594e-01]
       ...
       [9.80268932e-01]
       [1.13198035e-01]
       [4.65678949e-01]]
    
      ...
    
      [[4.94241516e-02]
       [6.34450548e-01]
       [8.93053413e-01]
       ...
       [7.97760317e-01]
       [3.18871974e-01]
       [6.47314782e-01]]
    
      [[4.65136696e-01]
       [5.07669096e-01]
       [4.23295851e-01]
       ...
       [2.98177206e-01]
       [5.32380132e-01]
       [6.12348886e-01]]
    
      [[2.58528146e-01]
       [5.20561003e-02]
       [7.82628170e-01]
       ...
       [8.26242775e-03]
       [7.43071396e-01]
       [3.29652868e-01]]]]

**TF2**

    import tensorflow as tf
    import numpy as np
    from tensorflow.keras.layers import Conv2D
    
    
    if __name__ == '__main__':
        seed = 1
        np.random.seed(seed)
        tf.random.set_seed(seed)
        x = np.random.random((2, 84, 84, 1))
        initializer = tf.initializers.Orthogonal(1.0, seed)
        print(f'Conv2D output:\n{Conv2D(32, 8, 4, kernel_initializer=initializer)(x)}\n{100 * ""=""}')
        print(f'x:\n{x}')

**Results in**

        Conv2D output:
    [[[[-5.41361034e-01  3.66543412e-01 -1.51497812e-03 ... -5.08555949e-01
        -2.81920075e-01  2.03241315e-02]
       [-5.59882522e-01  4.86802310e-01 -6.99946359e-02 ... -2.40252942e-01
        -3.65630835e-01 -3.15424293e-01]
       [-9.42213356e-01  2.15654269e-01 -1.55082747e-01 ... -4.04906332e-01
        -2.01114044e-01  7.04537705e-02]
       ...
       [-3.46490562e-01  3.19557160e-01 -2.92775959e-01 ... -4.28674221e-01
         2.79810458e-01  4.20070797e-01]
       [-3.45766425e-01  1.90711677e-01 -1.57374702e-02 ... -5.71854293e-01
         4.97741327e-02 -1.16687842e-01]
       [-9.33221936e-01 -1.68129373e-02  1.63893417e-01 ... -4.79621142e-01
        -2.14530781e-01  5.61090052e-01]]
    
      [[-7.58794546e-01  5.37562370e-01 -4.74378794e-01 ... -5.63471198e-01
        -2.08135564e-02  3.88803691e-01]
       [-4.21465009e-01  1.65287077e-01 -4.43225324e-01 ... -1.72018170e-01
        -7.92833045e-02  3.92330065e-02]
       [-4.81306076e-01  3.66574019e-01 -3.64440233e-01 ... -5.52142262e-01
        -2.36726597e-01 -3.12441528e-01]
       ...
       [-8.76132190e-01  3.22705477e-01 -1.90438583e-01 ... -6.52492762e-01
        -6.05543517e-02  3.59102860e-02]
       [-7.17206061e-01  1.32312387e-01 -4.24121648e-01 ...  1.82857260e-01
         1.84026435e-01 -1.85313985e-01]
       [-1.20653558e+00  4.54987772e-02 -2.89574206e-01 ... -3.28905612e-01
         3.50747108e-02  1.20641785e-02]]
    
      [[-5.34970939e-01  7.13559866e-01 -4.85820472e-01 ... -4.59470123e-01
         3.37272674e-01  1.63189009e-01]
       [-7.05033600e-01  3.56267929e-01 -4.30038758e-02 ... -5.83572984e-01
        -1.40957370e-01 -2.53099173e-01]
       [-5.99712431e-01  7.10705340e-01 -3.39112192e-01 ... -4.14105803e-01
         1.87255502e-01 -3.33267421e-01]
       ...
       [-5.80564499e-01  4.36445504e-01  3.36502224e-01 ...  1.45576835e-01
         3.74677926e-01 -3.91695678e-01]
       [-7.22398818e-01  3.15642595e-01 -2.88131565e-01 ... -3.45985293e-01
         2.34268203e-01  3.29410911e-01]
       [-7.46637762e-01  2.26648629e-01 -1.73913926e-01 ... -1.78611025e-01
         7.35538006e-02  2.55084813e-01]]
    
      ...
    
      [[-5.49425662e-01 -3.49880159e-02  4.77346703e-02 ... -4.04859513e-01
         3.71269658e-02 -1.85562521e-02]
       [-8.26125443e-01  1.29296139e-01 -2.88902789e-01 ... -8.50088000e-01
        -5.04021049e-01 -4.97736454e-01]
       [-7.37619460e-01  5.00219822e-01  7.76109993e-02 ... -1.41733378e-01
         2.81554639e-01  3.69791657e-01]
       ...
       [-3.06938559e-01  4.47962463e-01 -2.05980629e-01 ... -8.48388433e-01
        -1.59557834e-01  3.17021906e-02]
       [-3.87109697e-01  9.46836114e-01 -2.71658182e-01 ... -6.23931110e-01
        -2.16598317e-01 -1.88935712e-01]
       [-1.31429803e+00  3.86178017e-01 -6.20340466e-01 ... -3.68769616e-02
         1.81194678e-01 -1.06644318e-01]]
    
      [[-7.55118549e-01 -1.59330755e-01 -1.55888349e-01 ... -8.46641809e-02
         5.47275543e-02 -3.14989388e-01]
       [-7.29794323e-01  1.63924411e-01  5.99410906e-02 ...  1.31567806e-01
        -4.67930377e-01  2.95255750e-01]
       [-6.91133916e-01  5.20268269e-02 -7.29867145e-02 ... -1.88851178e-01
         4.07196075e-01  4.01087821e-01]
       ...
       [-5.79473913e-01  7.94014513e-01 -7.01249093e-02 ... -6.76721931e-01
        -1.01779945e-01 -5.97994268e-01]
       [-6.84648752e-01  1.01270474e-01 -4.07643348e-01 ...  3.85033749e-02
         8.26148912e-02  5.69675528e-02]
       [-4.62479711e-01  4.19429392e-01  1.77284703e-04 ... -1.42741054e-01
         3.54655176e-01  2.56564021e-01]]
    
      [[-2.46302426e-01  7.49420345e-01  8.12041853e-03 ... -3.32499892e-01
         1.33527100e-01  6.35570288e-02]
       [-9.58327591e-01  1.98065817e-01  1.93995520e-01 ... -2.94785231e-01
         2.52389193e-01 -1.20501839e-01]
       [-8.17039490e-01  2.07326993e-01 -5.11587203e-01 ... -4.24753636e-01
         2.88008928e-01  5.03057897e-01]
       ...
       [-7.03616858e-01  3.78316432e-01 -5.60314238e-01 ... -4.05634254e-01
        -3.88694972e-01 -1.93394765e-01]
       [-6.37941658e-01  4.12149638e-01  4.30523872e-01 ... -1.07135192e-01
        -4.06805426e-01  2.13325843e-01]
       [-1.10811067e+00  1.87466890e-01 -4.25169766e-01 ... -3.90125453e-01
        -2.32264772e-01 -1.13592006e-01]]]
    
    
     [[[-1.12212503e+00  4.90197897e-01 -1.32902622e-01 ... -2.71363735e-01
         5.97870303e-03 -1.82653069e-01]
       [-5.94440103e-01 -2.69649085e-02 -8.82392377e-02 ... -2.57950276e-01
        -5.64375184e-02  3.79566044e-01]
       [-9.26148295e-01  7.57718146e-01 -1.35923713e-01 ... -3.23437661e-01
         4.14071977e-01  1.71241298e-01]
       ...
       [-7.43670583e-01  5.69033444e-01  8.93335044e-02 ... -4.33211058e-01
         9.38811079e-02 -2.02697456e-01]
       [-7.01980829e-01  8.19573849e-02  1.15745321e-01 ... -3.05091172e-01
         2.67599583e-01  4.00184661e-01]
       [-1.23418115e-01 -7.21083656e-02 -2.83473641e-01 ... -5.09245217e-01
         1.67642578e-01  3.97953391e-02]]
    
      [[-7.82243788e-01  4.62321520e-01 -1.03323981e-01 ... -6.02696836e-01
         1.12790830e-01  6.60246331e-03]
       [-1.18653631e+00  1.10369611e+00  4.03279841e-01 ... -3.35117340e-01
         1.34733140e-01  1.24090314e-01]
       [-7.56276011e-01  4.68323886e-01  2.55332291e-01 ... -5.64108253e-01
        -1.23212464e-01  9.44344103e-02]
       ...
       [-1.07282424e+00  4.46680576e-01  2.62999147e-01 ... -4.93109912e-01
         1.20379120e-01  2.00786784e-01]
       [-5.91401279e-01 -4.76840436e-02 -6.31224453e-01 ... -1.33297965e-02
        -6.85213029e-01  3.06965083e-01]
       [-1.26961339e+00  4.00966257e-01  1.58335969e-01 ... -4.71471280e-01
        -5.21351993e-01 -2.68614203e-01]]
    
      [[-7.59719312e-01 -2.15684757e-01 -5.51491380e-01 ... -4.05447990e-01
         2.04430401e-01 -4.03630167e-01]
       [-7.50933051e-01 -2.70830095e-01 -4.10136461e-01 ... -2.66372442e-01
        -4.25907433e-01  2.79703408e-01]
       [-1.15558457e+00  1.44730762e-01 -1.11366399e-01 ... -3.16568702e-01
        -1.65987372e-01 -2.50013694e-02]
       ...
       [-1.26355612e+00  1.83538213e-01  2.64952302e-01 ... -4.03630674e-01
        -1.93098515e-01  3.03720146e-01]
       [-1.04506040e+00  9.86333251e-01 -4.76556689e-01 ... -5.65628707e-01
         2.07612868e-02  7.63047487e-02]
       [-1.07622218e+00  2.96826065e-01 -5.04598498e-01 ... -4.09753144e-01
        -2.73265153e-01  6.33615702e-02]]
    
      ...
    
      [[-1.13511002e+00  2.11719826e-01 -8.60134482e-01 ... -3.91803771e-01
        -6.59739316e-01 -2.00912848e-01]
       [-1.29758418e+00  2.52690554e-01 -3.54235232e-01 ... -4.23384011e-01
        -5.52129559e-02 -1.58944473e-01]
       [-1.17037618e+00  3.93378854e-01 -2.55860239e-01 ... -1.89175576e-01
        -3.52438241e-01  4.47871268e-01]
       ...
       [-6.99247956e-01 -4.01002228e-01 -9.55614746e-02 ... -5.12549996e-01
        -2.93750733e-01  6.53216466e-02]
       [-7.55659997e-01  6.40419498e-02 -3.80645603e-01 ... -3.38304751e-02
        -2.81535774e-01  3.27474415e-01]
       [-8.98592114e-01  6.04611039e-01 -2.70986766e-01 ... -3.77177030e-01
         6.39083028e-01  1.36269689e-01]]
    
      [[-2.06768334e-01  4.33980107e-01 -2.79342651e-01 ... -5.97317517e-01
        -3.16711366e-01 -3.49702388e-02]
       [-8.91763270e-01  5.25228903e-02 -1.25209495e-01 ... -2.35961720e-01
        -8.85192119e-03  2.04528570e-01]
       [-6.51450276e-01 -3.80111784e-01 -7.60694802e-01 ... -1.16580009e+00
         4.66391236e-01 -1.49288952e-01]
       ...
       [-8.66602838e-01 -5.96193299e-02 -1.27994597e-01 ... -4.30833064e-02
         1.16992146e-01 -6.62802998e-03]
       [-9.14786756e-01  1.67500958e-01 -2.82980531e-01 ... -6.82374537e-01
        -1.63795985e-02  1.82587206e-01]
       [-5.49697280e-01  6.72760680e-02 -7.42559731e-02 ... -1.59981385e-01
         1.14814062e-02  2.45045304e-01]]
    
      [[-6.60416663e-01  4.52319860e-01  1.04573436e-01 ... -8.84432197e-01
        -1.99507281e-01 -5.85323572e-02]
       [-5.59800625e-01  4.29061443e-01  6.59240410e-02 ... -2.68544644e-01
        -9.68336090e-02  1.81482792e-01]
       [-5.66595554e-01  7.11271226e-01 -4.41362828e-01 ... -4.43202615e-01
         8.01286697e-01 -1.05126597e-01]
       ...
       [-7.27143824e-01  4.69138294e-01 -3.05642545e-01 ... -3.89702499e-01
        -9.79036614e-02  3.28408331e-01]
       [-3.61066341e-01  4.04266417e-01 -2.91205943e-01 ...  5.48217893e-02
        -9.46037620e-02 -1.06996156e-01]
       [-6.50965929e-01  5.33826649e-01 -3.92165482e-01 ... -2.94952631e-01
        -8.13799322e-01  1.03757977e-01]]]]
    ====================================================================================================
    x:
    [[[[4.17022005e-01]
       [7.20324493e-01]
       [1.14374817e-04]
       ...
       [6.23672207e-01]
       [7.50942434e-01]
       [3.48898342e-01]]
    
      [[2.69927892e-01]
       [8.95886218e-01]
       [4.28091190e-01]
       ...
       [1.85762022e-02]
       [7.00221437e-02]
       [4.86345111e-01]]
    
      [[6.06329462e-01]
       [5.68851437e-01]
       [3.17362409e-01]
       ...
       [9.18601778e-01]
       [4.02024891e-04]
       [9.76759149e-01]]
    
      ...
    
      [[5.89549934e-01]
       [3.89137609e-01]
       [5.05975232e-01]
       ...
       [4.35888475e-01]
       [7.89075202e-01]
       [4.66467704e-01]]
    
      [[6.73554921e-01]
       [8.84836452e-01]
       [9.38138449e-01]
       ...
       [7.93970466e-01]
       [2.13784215e-01]
       [6.41105035e-01]]
    
      [[7.31134736e-01]
       [9.50619892e-02]
       [7.00729238e-02]
       ...
       [9.95522026e-01]
       [4.81429517e-01]
       [8.37812754e-01]]]
    
    
     [[[6.03655452e-01]
       [6.64374944e-01]
       [2.72461392e-01]
       ...
       [1.14069927e-01]
       [2.93705095e-01]
       [8.78904978e-03]]
    
      [[2.53263696e-01]
       [8.37712781e-01]
       [8.07756027e-01]
       ...
       [7.37152445e-01]
       [6.00521471e-01]
       [7.37999367e-01]]
    
      [[3.75760828e-01]
       [9.11106703e-01]
       [8.72308594e-01]
       ...
       [9.80268932e-01]
       [1.13198035e-01]
       [4.65678949e-01]]
    
      ...
    
      [[4.94241516e-02]
       [6.34450548e-01]
       [8.93053413e-01]
       ...
       [7.97760317e-01]
       [3.18871974e-01]
       [6.47314782e-01]]
    
      [[4.65136696e-01]
       [5.07669096e-01]
       [4.23295851e-01]
       ...
       [2.98177206e-01]
       [5.32380132e-01]
       [6.12348886e-01]]
    
      [[2.58528146e-01]
       [5.20561003e-02]
       [7.82628170e-01]
       ...
       [8.26242775e-03]
       [7.43071396e-01]
       [3.29652868e-01]]]]
"
47602,Error to load a file related with cuda,"
**System information**
- OS Platform and Distribution : Windows 10 Familly 1909
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install 
- TensorFlow version: 2.3.0
- Python version: 3.7
- Installed using pip
- CUDA/cuDNN version: 11.2
- GPU model and memory: 1660Ti



**Describe the problem**

I installed tensorflow with PIP.  Then I wanted to use use keras. I tried to execute a py file with this code : 
`from tensorflow.keras import layers
`

But I got : 

> 2021-03-06 06:24:10.791991: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2021-03-06 06:24:10.804340: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

And I have already done : 

> pip install tf-nightly-gpu


"
47599,XLA experimental_compile=True fails inside tf.data.Dataset,"The `experimental_compile=True` option for `tf.function` works outside of a tf.data Dataset, but fails once I try to map it over a dataset.

```python
import tensorflow as tf

def log10(x):
    numerator = tf.math.log(x)
    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))
    return numerator / denominator

@tf.function(experimental_compile=True)
def drc(
    inputs,
    threshold=-36.0,
    release_time=80.0,
    ratio=8.0,
    makeup_gain=0.0,
    sample_rate=44100,
):
    # downmix
    buffer = tf.reduce_mean(inputs, axis=1, keepdims=True)
    alpha_release = tf.exp(-1 / (0.001 * sample_rate * release_time))

    y_prev = tf.constant([0.0], dtype=tf.float32)
    length = tf.shape(inputs)[0]
    c = tf.TensorArray(
        dtype=tf.float32,
        size=length,
        element_shape=[1,]
    )

    for i in tf.range(length):
        if tf.abs(buffer[i]) < 0.000001:
            x_g = tf.constant([-120.0], dtype=tf.float32)
        else:
            x_g = tf.multiply(20.0, log10(tf.abs(buffer[i])))

        if x_g > threshold:
            y_g = tf.add(threshold, tf.divide(tf.subtract(buffer[i], threshold), ratio))
        else:
            y_g = x_g

        x_l = tf.subtract(x_g, y_g)

        y_l = tf.add(
            tf.multiply(alpha_release, y_prev), tf.multiply(tf.subtract(1.0, alpha_release), x_l)
        )
        y_l = tf.math.pow(10.0, (makeup_gain - y_l) / 20.0)
        c = c.write(i, y_l)
        y_prev = y_l

    out = c.stack()
    return tf.stack([signal[:, 0] * tf.squeeze(out), signal[:, 1] * tf.squeeze(out)], axis=-1)

signal = tf.random.normal((100000, 2), dtype=tf.float32)
out_tf = drc(signal)  # works

ds = tf.data.Dataset.from_tensor_slices([signal])
ds = ds.map(drc).batch(5)
for b in ds:
  print(b)  # Fails
```
I get the following error:

```
InvalidArgumentError: Function invoked by the following node is not compilable: {{node PartitionedCall}} = PartitionedCall[Tin=[DT_FLOAT, DT_FLOAT], Tout=[DT_FLOAT], _XlaHasReferenceVars=false, _XlaMustCompile=true, _collective_manager_ids=[], _input_hostmem=[], _read_only_resource_inputs=[], config="""", config_proto=""\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001\202\001\000"", executor_type="""", f=__inference_drc_250598[], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](args_0, unknown).
Uncompilable nodes:
PartitionedCall: could not instantiate call: '__inference_drc_250598'
	Stacktrace:
		Node: PartitionedCall, function: 

	 [[PartitionedCall]] [Op:MakeIterator]
```

It works as expected if I set experimental_compile to False. I'm using tensorflow 2.4.0."
47597,Set specific weights to non-trainable instead of the whole layer,"I'm trying to find a way to turn specific weights (variables) from trainable to non-trainable. For example, let's say I load a pre-trained model that includes a `Dense` layer and want to keep the `W` matrix trainable while disabling the bias term. 

I see only examples of how to turn a whole layer:

```python
import tensorflow as tf

# Make a model with 2 layers
layer1 = tf.keras.layers.Dense(3, activation=""relu"")
layer2 = tf.keras.layers.Dense(3, activation=""sigmoid"")
model = tf.keras.Sequential([tf.keras.Input(shape=(3,)), layer1, layer2])

# Freeze the first layer
layer1.trainable = False

model.summary()
```

Returns:

```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 3)                 12        
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 12        
=================================================================
Total params: 24
Trainable params: 12
Non-trainable params: 12
_________________________________________________________________
```

What if, I want to do something like:

```python
import tensorflow as tf

# Make a model with 2 layers
layer1 = tf.keras.layers.Dense(3, activation=""relu"")
layer2 = tf.keras.layers.Dense(3, activation=""sigmoid"")
model = tf.keras.Sequential([tf.keras.Input(shape=(3,)), layer1, layer2])

# Freeze the first layer
layer1.bias.trainable = False

model.summary()

```

This returns the error:


```
layer1.bias.trainable = False.  AttributeError: can't set attribute
```


"
47596,Drastic changes in predictions during training and validation on the SAME data,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.4.1
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I am referring to the [official example on knowledge distillation](https://keras.io/examples/vision/knowledge_distillation/) from Keras. During training the student I get expected results. However just after an iteration of training when I use the SAME dataset to validate the model, I get drastically different results. 

**Describe the expected behavior**

The results on the SAME data should be identical. 

**Standalone code to reproduce the issue**
[Colab Notebook](https://colab.research.google.com/gist/sayakpaul/f1b528c77d26ac951621c2e2258c7b47/verification.ipynb).
"
47595,"Tensorflow Lite Android Object Detection  Mobile SSD models are expected to have exactly 4 outputs, found 8","**Problem Encountered:**

> E/AndroidRuntime:` FATAL EXCEPTION: main Process: org.tensorflow.lite.examples.detection, PID: 14719 java.lang.AssertionError: Error occurred when initializing ObjectDetector: Mobile SSD models are expected to have exactly 4 outputs, found 8

**Problem Description**
- Android Application Source: TensorFlow Lite Object Detection Example from Google
- Error shown when starting the Example Application

**Model Description**
- Custom Model Used? **YES**
- Pre-trained Model Used: ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8
- Inference type: FLOAT
- Number of classes: 4

**System Information**
- OS Platform and Distribution: ( Linux Ubuntu 20.14)
- TensorFlow Version: 2.4.1
- TensorFlow installed from: Pip

**Saved Model conversion commands used**

1. Saved_Model.pb export command:

> python ./exporter_main_v2.py
--input_type image_tensor
--pipeline_config_path ./models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/pipeline.config
--trained_checkpoint_dir ./models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8
--output_directory exported_models/tflite

2. Convert saved model (.pb) to tflite command:
> toco
--saved_model_dir ./exported-models/tflite/saved_model
--emit-select-tf-ops true
--allow_custom_ops
--graph_def_file ./exported-models/tflite/saved_model/saved_model.pb
--output_file ./exported-models/tflite/tflite/detect.tflite
--input_shapes 1,300,300,3
--input_arrays normalized_input_image_tensor
--output_arrays 'TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'
--inference_type=FLOAT
--allow_custom_ops

**Remarks**
I am trying to use a trained custom model on the Google TensorFlow lite provided example. Just that every time I open the application, it returns such an error, Mobile SSD models are expected to have exactly 4 outputs, found 8. The model is trained to identify 4 classes, all stated in the labelmap.txt and pipeline config.

Does anybody have any clue about this error?"
47593,Given an input and get two outputs,"I am using Tensorflow 2.4.0 and Ubuntu 16.04

Given this model

    base_model = tf.keras.applications.EfficientNetB0(weights=""imagenet"", include_top=False)
    base_model.trainable = base_model_trainable
    inputs = tf.keras.Input(shape=(IMG_SIZE,IMG_SIZE,3), name=""input"")
    x = tf.keras.applications.efficientnet.preprocess_input(inputs)
    # more details - https://www.tensorflow.org/tutorials/images/transfer_learning#important_note_about_batchnormalization_layers

    x = base_model(x,
                   training=False)  # training=training is needed only if there are layers with different behavior during training versus inference (e.g. Dropout)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    outputs = tf.keras.layers.Dense(len(class_names), name=""output"")(x)
    model = tf.keras.Model(inputs, outputs)

My objective is given the input to get the value  of `model.get_layer(""efficientnetb0"").output`, 
and `model.output`

if I do (a) (it does not give `model.get_layer(""efficientnetb0"").output`)

    layer_name=""efficientnetb0""

    grad_model = tf.keras.models.Model([model.inputs], [model.output])

or (b) (it requires an additional input after preprocessing)

    grad_model = tf.keras.models.Model([model.inputs, model.get_layer(layer_name).input], [model.get_layer(layer_name).output, model.output])

both works

but if I do

    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])

It gives exception:

    ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='input_1'), name='input_1', description=""created by layer 'input_1'"") at layer ""rescaling"". The following previous layers were accessed without issue: []

It does not make sense to me - is this a bug or is there a way for me to pass in just an input and get two outputs?
"
47589,CMSIS-NN - Missing compiler warning flags,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version:  184cf2d642885013d533227d189232d9a5b7ffab
- Python version: NA
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): ARMCLANG6.15
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the problem**
Compiler warnings are not enabled for CMSIS-NN. I believe it should be the same for other optimized libraries as well.


TFLM kernels source files have the following warning flags enabled: TFLM          -> -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter

CMSIS-NN have these set for warnings: -Wno-type-limits -Wno-unused-private-field

(A fuill example line: arm-none-eabi-gcc -std=c11 -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -DCORTEX_M_GENERIC -DCMSIS_NN -mcpu=cortex-m7+nofp -mfpu=auto -DTF_LITE_MCU_DEBUG_LOG -mthumb -mfloat-abi=soft -funsigned-char -mlittle-endian -Wno-type-limits -Wno-unused-private-field -fomit-frame-pointer -MD -DCPU_M7=1 -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/cmsis -Itensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/ConvolutionFunctions/arm_depthwise_conv_s8_opt.c -o tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m7_default/obj/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/ConvolutionFunctions/arm_depthwise_conv_s8_opt.o
)

 **Is the intention here that , warnings are not enabled for external libraries or is it something that ought to be fixed?**


**Provide the exact sequence of commands / steps that you executed before running into the problem**
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex_m_generic TARGET_ARCH=cortex-m7 OPTIMIZED_KERNEL_DIR=cmsis_nn microlite


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47588,"My python version is 3.6. I successfully installed tensorflow, but when I imported in idle it gives error as following :",">>> import tensorflow
2021-03-05 18:13:41.002427: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2021-03-05 18:13:41.003385: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\sushi\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\sushi\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 41, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\sushi\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\eager\context.py"", line 32, in <module>
    from tensorflow.core.framework import function_pb2
  File ""C:\Users\sushi\AppData\Roaming\Python\Python36\site-packages\tensorflow\core\framework\function_pb2.py"", line 7, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\sushi\AppData\Local\Programs\Python\Python36\lib\site-packages\google\protobuf\descriptor.py"", line 48, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found."
47587,Failed to build TF on Ubuntu 18.04 due to C++ compilation of rule '//tensorflow/stream_executor/cuda:cusparse_stub' failed (Exit 1),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.1
- Python version: 3.8.8
- Installed using virtualenv? pip? conda?: conda (conda version 4.8.2)
- Bazel version (if compiling from source): 3.1.0 (Bazelisk v1.7.5)
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: CUDA Version 10.1.105, cuDNN version 7.6.5
- GPU model and memory: 2 x NVIDIA Quadro RTX 8000 (48 GB per card, 96 GB total)



**Describe the problem**
Bazel build step fails when building from source
**ERROR: /home/user/Downloads/tensorflow-2.4.1/tensorflow/tools/pip_package/BUILD:69:1 C++ compilation of rule '//tensorflow/stream_executor/cuda:cusparse_stub' failed (Exit 1)**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Installing Bazelisk
sudo curl -Lo /usr/local/bin/bazel https://github.com/bazelbuild/bazelisk/releases/download/v1.7.5/bazelisk-linux-amd64
2. Downloaded TF v2.4.1 source
curl -LO https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz
3. tar -xvzf v2.4.1.tar.gz && cd tensorflow-2.4.1
4. Install bazel
bazel version (This command automatically installed bazel version 3.1.0)
5. ./configure.py
Output of configure.py attached in file configure_output.txt
6. bazel build //tensorflow/tools/pip_package:build_pip_package
Output of build attached in file bazel_build_output.txt


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[bazel_build_output.txt](https://github.com/tensorflow/tensorflow/files/6089929/bazel_build_output.txt)
[configure_output.txt](https://github.com/tensorflow/tensorflow/files/6089930/configure_output.txt)
"
47586,[Keras] Model does not learn at all - loss and val_acc remains constant - what other problems can be there?,"I am doing text classification and have made a dummy dataset to aid in debugging. The problem is that the model does not learn/converge. I am trying to predict one out of 20 labels and have tried most fixes on the net to no avail. 
```py
train_text = [['a'],['a'],['a']]
val_text = [['a'],['a'],['a']]
train_label = np.asarray([5,5,5])
val_label = np.asarray([5,5,5])

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

#Implementing a Transformer block as a layer

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation=""relu""), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)


class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super(TokenAndPositionEmbedding, self).__init__()
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

vocab_size = 40000  # Only consider the top 20k words :)
maxlen = 10  # for my task, ideally 500

train_label = tf.keras.utils.to_categorical(train_label, 20)
val_label = tf.keras.utils.to_categorical(val_label, 20)

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000, lower=True, oov_token='<OOV>')
tokenizer.fit_on_texts(train_text)
train_sequences = tokenizer.texts_to_sequences(train_text)

val_sequences = tokenizer.texts_to_sequences(val_text)

train_text = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=maxlen, dtype='int32',
                                           padding='post', truncating='post')

val_text = tf.keras.preprocessing.sequence.pad_sequences(val_sequences, maxlen=maxlen, dtype='int32',
                                           padding='post', truncating='post')

from sklearn.preprocessing import normalize

train_text_n = normalize(train_text)
val_text_n = normalize(val_text)

data_set = tf.data.Dataset.from_tensor_slices((train_text_n , train_label))
data_set

from tensorflow.keras.regularizers import l2
 
embed_dim = 128  # Embedding size for each token
num_heads = 8  # Number of attention heads
ff_dim = 256  # Hidden layer size in feed forward network inside transformer
batch_size = 32
factor = 0.0001
 
inputs = layers.Input(shape=(maxlen,))

embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)
x = embedding_layer(inputs)
transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)

x = transformer_block(x)

x = layers.GlobalAveragePooling1D()(x)

x = layers.Dense(160, activation=""relu"", kernel_initializer='glorot_uniform')(x)
x = layers.Dropout(0.15)(x)
x = layers.BatchNormalization()(x)

x = layers.Dense(90, activation=""relu"", kernel_initializer='glorot_uniform')(x)
x = layers.Dropout(0.1)(x)
x = layers.BatchNormalization()(x)

 
x = layers.Dense(40, activation=""relu"", kernel_initializer='glorot_uniform')(x)
x = layers.Dropout(0.1)(x)

outputs = layers.Dense(20)(x)
 
model = keras.Model(inputs=inputs, outputs=outputs)
 
adamopt = tf.keras.optimizers.Adam(learning_rate=1e-4)
 
 
model.compile(optimizer='adamopt', loss=""categorical_crossentropy"", metrics=[""acc""])
 
model.summary()
 
history = model.fit(
    train_text_n, train_label, batch_size=batch_size, epochs=20, validation_data=(val_text_n, val_label), verbose=1)
```
Any Idea on how to do I fix this network? I am totally stumped after trying to fix it for 4 days :(

I posted this on Stack Overflow also, but seeing the lack of traffic, I doubt I would receive any response https://stackoverflow.com/questions/66471955/network-does-not-learn-what-are-the-other-things-that-can-go-wrong 

> There is a list of all the stuff I have tried on that link ^^^"
47585,"TensorFlow Lite currently doesn't support control flow ops: Merge, Switch.","TensorFlow Lite currently doesn't support control flow ops: Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, EQUAL, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, GREATER_EQUAL, LESS, LOGICAL_OR, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE, UNPACK, WHERE. Here is a list of operators for which you will need custom implementations: DecodeBmp, DecodeGif, DecodeJpeg, DecodePng, DecodeRaw, NonMaxSuppressionV5, ParseSingleExample, Substr.
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
47584,CANNOT FIND the source code of gen_candidate_sampling_ops,"Im reading the source code(python version) of sampled softmax loss(tf.nn.sampled_softmax_loss) since I want to write my own sampled_values, and it leads me to the function log_uniform_candidate_sampler in candidate_sampling_ops.py.

-
@tf_export(
    'random.log_uniform_candidate_sampler',
    v1=[
        'random.log_uniform_candidate_sampler',
        'nn.log_uniform_candidate_sampler'
    ])
@dispatch.add_dispatch_support
@deprecation.deprecated_endpoints('nn.log_uniform_candidate_sampler')
def log_uniform_candidate_sampler(true_classes, num_true, num_sampled, unique,
                                  range_max, seed=None, name=None):

  seed1, seed2 = random_seed.get_seed(seed)
  return gen_candidate_sampling_ops.log_uniform_candidate_sampler(
      true_classes, num_true, num_sampled, unique, range_max, seed=seed1,
      seed2=seed2, name=name)
-

HOWEVER, I cannot find the gen_candidate_sampling_ops.py in source code, which confused me.

[Link:](url)
https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/candidate_sampling_ops.py#L99
"
47583,how can i access the dataset on each worker node when using MultiWorkerMirroredStrategy training?,"when i use MultiWorkerMirroredStrategy to distributed training,
i want to know how the dataset shard with multiple workers in each batch-train,
but there is no way to debug...

thanks for your help!"
47582,Ghost bug on macOS Big Sur - TypeError: tf__gradient_update() missing n required positional arguments,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 11.2.1
- TensorFlow installed from (source or binary): pip (binary)
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.7

```
== check python ===================================================
python version: 3.8.7
python branch: 
python build version: ('default', 'Dec 30 2020 10:14:55')
python compiler version: Clang 12.0.0 (clang-1200.0.32.28)
python implementation: CPython


== check os platform ===============================================

== are we in docker =============================================
No

== compiler =====================================================
Apple clang version 12.0.0 (clang-1200.0.32.29)
Target: x86_64-apple-darwin20.3.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== check pips ===================================================
numpy                      1.19.5
protobuf                   3.14.0
tensorflow                 2.4.1
tensorflow-addons          0.12.0
tensorflow-estimator       2.4.0
tensorflow-probability     0.12.1

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.4.1
tf.version.GIT_VERSION = v2.4.0-49-g85c8b2a817f
tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./collect_tf.sh: line 145: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.4.1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /usr/local/lib/python3.8/site-packages
Required-by: yolo-tf2

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(2, 7, 16, 'final', 0)

== bazel version  ===============================================
```

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
The issue sometimes happens and sometimes it doesn't. It usually occurs when I run some perfectly working code then make some minor changes followed by a re-run. I get the error / a variation of the very same error message referring to missing positional arguments while they are there and there is absolutely nothing wrong.

```
Traceback (most recent call last):
  File ""/Users/emadboctor/Desktop/code/drl-algos/acer.py"", line 273, in <module>
    ]
  File ""/Users/emadboctor/Desktop/code/drl-algos/base_agent.py"", line 346, in fit
    self.train_step()
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 871, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 725, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2969, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3196, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3887, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 977, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:


    TypeError: tf__gradient_update() missing 5 required positional arguments: 'states', 'rewards', 'actions', 'dones', and 'action_probs'
```
Then, the error is usually gone in the following run.

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47579,Build C API on Windows - test failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.4.0
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): MSCV 14.16.27023
- CUDA/cuDNN version: 11.0/8.0.5

**Describe the problem**

I followed the build instructions on [website](https://www.tensorflow.org/install/source_windows) and lib_package [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md)
But it failed at the test:

```
FAILED: //tensorflow/tools/lib_package:libtensorflow_test (Summary)
      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test.log
      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_1.log
      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_2.log
```
See the last section for the logs.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Setup configuration
```
> python ./configure.py

You have bazel 3.1.0 installed.
Please specify the location of python. [Default is D:\ProgramData\Anaconda3\python.exe]:


Found possible Python library paths:
  D:\ProgramData\Anaconda3\lib\site-packages
Please input the desired Python library path to use.  Default is [D:\ProgramData\Anaconda3\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 11.0 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include
Found cuDNN 8 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
```
2. run libtensorflow_test
```
> bazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test

WARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'test' from d:\code\tensorflow_cpp_build\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  Inherited 'build' options: --python_path=D:/ProgramData/Anaconda3/python.exe
INFO: Reading rc options for 'test' from d:\code\tensorflow_cpp_build\.bazelrc:
  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'test' from d:\code\tensorflow_cpp_build\.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=D:/ProgramData/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=D:/ProgramData/Anaconda3/lib/site-packages --python_path=D:/ProgramData/Anaconda3/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Reading rc options for 'test' from d:\code\tensorflow_cpp_build\.bazelrc:
  'test' options: --define open_source_build=true --config=v2
INFO: Reading rc options for 'test' from d:\code\tensorflow_cpp_build\.tf_configure.bazelrc:
  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium
INFO: Found applicable config definition build:short_logs in file d:\code\tensorflow_cpp_build\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\code\tensorflow_cpp_build\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition test:v2 in file d:\code\tensorflow_cpp_build\.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only
INFO: Found applicable config definition build:xla in file d:\code\tensorflow_cpp_build\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file d:\code\tensorflow_cpp_build\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file d:\code\tensorflow_cpp_build\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:v2 in file d:\code\tensorflow_cpp_build\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition test:v2 in file d:\code\tensorflow_cpp_build\.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only
INFO: Found applicable config definition build:opt in file d:\code\tensorflow_cpp_build\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true
INFO: Found applicable config definition build:windows in file d:\code\tensorflow_cpp_build\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\code\tensorflow_cpp_build\.bazelrc: --define framework_shared_object=false
INFO: Analyzed target //tensorflow/tools/lib_package:libtensorflow_test (0 packages loaded, 0 targets configured).
INFO: Found 1 test target...
FAIL: //tensorflow/tools/lib_package:libtensorflow_test (see C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_1.log)
FAIL: //tensorflow/tools/lib_package:libtensorflow_test (see C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_2.log)
FAIL: //tensorflow/tools/lib_package:libtensorflow_test (see C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test.log)

FAILED: //tensorflow/tools/lib_package:libtensorflow_test (Summary)
      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test.log
      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_1.log
      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_2.log
Target //tensorflow/tools/lib_package:libtensorflow_test up-to-date:
  bazel-bin/tensorflow/tools/lib_package/libtensorflow_test
  bazel-bin/tensorflow/tools/lib_package/libtensorflow_test.exe
INFO: Elapsed time: 38948.796s, Critical Path: 1452.51s
INFO: 7616 processes: 7616 local.
INFO: Build completed, 1 test FAILED, 7612 total actions
//tensorflow/tools/lib_package:libtensorflow_test                        FAILED in 3 out of 3 in 16.0s
  Stats over 3 runs: max = 16.0s, min = 2.6s, avg = 7.1s, dev = 6.3s
  C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test.log
  C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_1.log
  C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_2.log

INFO: Build completed, 1 test FAILED, 7612 total actions
```

**Any other info / logs**
[test.log](https://github.com/tensorflow/tensorflow/files/6087959/test.log)
[attempt_1.log](https://github.com/tensorflow/tensorflow/files/6087960/attempt_1.log)
[attempt_2.log](https://github.com/tensorflow/tensorflow/files/6087961/attempt_2.log)
"
47578,Gradient Accumulation with Custom fit in tf.keras?,"## Documents: 
[Customizing fit : TF.Keras](https://keras.io/guides/customizing_what_happens_in_fit/)
This a great article and one of the best deep learning practices. However, with this reading intuition, I was trying to implement Gradiant Accumulation (**GA**) by customizing the `.fit()` method, overriding the `train_step` within `tf.keras.Model` class. But I couldn't implement it yet. 

I don't want it in a custom training loop (like [this](https://stackoverflow.com/questions/59893850/how-to-accumulate-gradients-in-tensorflow-2-0)) but with customized `.fit()`. I also asked in [SO](https://stackoverflow.com/q/66472201/9215780), no response yet. I've found some workaround [here](https://github.com/keras-team/keras/issues/14483), but it's too messy. There should be a convenient way to achieve this. How to achieve this? Please find the gist [here](https://colab.research.google.com/drive/1hr9kIhTlkCxlW_md3CXWREor-6TE6RFq?usp=sharing). 

Also, I know the pros of using **GA** but what are the major **cons** of using it? Why it's not come as a default but an optional feature with the framework?"
47575,Exception related symbols appear to be showing up in the final binary with the xtensa toolchain,"The tests described in this issue are with the RI-2020.4-linux toolchain and the Fusion F1 core.

Build the keyword_benchmark with:
```
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release
```

List all the symbols:
```
XTENSA_CORE=F1_190305_swupgrade xt-nm --print-size --size-sort --radix=d -C tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark
```

We see symbols that appear to be related to exception handling:
```
00082100 00000304 T _Unwind_ForcedUnwind
00070712 00000305 T __cxxabiv1::__vmi_class_type_info::__do_upcast(__cxxabiv1::__class_type_info const*, void const*, __cxxabiv1::__class_type_info::__upcast_result&) const
00076024 00000307 T __divdf3
00082796 00000310 T _Unwind_Resume_or_Rethrow
00104616 00000326 t _xa_nn_dot_product_4_rows_1_vec_mat_aligned_vec_aligned
00080392 00000340 t uw_update_context_1
00081736 00000364 T _Unwind_RaiseException
00082404 00000392 T _Unwind_Resume
00081140 00000408 t uw_advance_context
00080732 00000408 t uw_update_context
00072160 00000413 t get_ttype_entry(lsda_header_info*, unsigned int)
00089100 00000415 T _FDscalex
00085964 00000444 t add_fdes
00086600 00000456 t linear_search_fdes
00085504 00000457 t classify_object_over_fdes
00084100 00000526 t fde_single_encoding_compare
00070064 00000584 T _FExp
00079584 00000806 t uw_frame_state_for
00078136 00001446 t execute_cfa_program
00116320 00002048 b emergency_buffer_72
```

Some of the full command line options (to confirm that we are passing in `-fno-exceptions` when building the .cc files):
```
xt-clang++ -std=c++11 -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DXTENSA -DXTENSA -DNDEBUG -DTF_LITE_STRIP_ERROR_STRINGS -DTF_LITE_MCU_DEBUG_LOG -DTF_LITE_USE_CTIME --xtensa-core=F1_190305_swupgrade -mcoproc -DMAX_RFFT_PWR=9 -DMIN_RFFT_PWR=MAX_RFFT_PWR -DFUSION_F1 -Wno-unused-private-field -DNNLIB_V2 -Wno-shadow -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/kernels/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/nnlib/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/common/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/ndsp/hifi4/include/ -c tensorflow/lite/micro/kernels/xtensa/fully_connected.cc -o tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/obj/tensorflow/lite/micro/kernels/xtensa/fully_connected.o

xt-clang -std=c11 -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DXTENSA -DXTENSA -DNDEBUG -DTF_LITE_STRIP_ERROR_STRINGS -DTF_LITE_MCU_DEBUG_LOG -DTF_LITE_USE_CTIME --xtensa-core=F1_190305_swupgrade -mcoproc -DMAX_RFFT_PWR=9 -DMIN_RFFT_PWR=MAX_RFFT_PWR -DFUSION_F1 -Wno-unused-private-field -DNNLIB_V2 -Wno-shadow -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/kernels/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/nnlib/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/common/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/ndsp/hifi4/include/ -c tensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/ndsp/hifi4/src/scl_tanhf_hifi4.c -o tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/obj/tensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/ndsp/hifi4/src/scl_tanhf_hifi4.o

xt-clang++ -std=c++11 -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DXTENSA -DXTENSA -DNDEBUG -DTF_LITE_STRIP_ERROR_STRINGS -DTF_LITE_MCU_DEBUG_LOG -DTF_LITE_USE_CTIME --xtensa-core=F1_190305_swupgrade -mcoproc -DMAX_RFFT_PWR=9 -DMIN_RFFT_PWR=MAX_RFFT_PWR -DFUSION_F1 -Wno-unused-private-field -DNNLIB_V2 -Wno-shadow -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/kernels/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/nnlib/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/common/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/ndsp/hifi4/include/ -o tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/obj/tensorflow/lite/micro/benchmarks/keyword_benchmark.o tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/obj/tensorflow/lite/micro/benchmarks/keyword_scrambled_model_data.o tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm
```


"
47574, Undefined symbol micro_test::did_test_fail in tensorflow_lite/tensorflow/lite/micro/kernels/conv_test_common,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- Tensorflow version (commit SHA if source): 2.4.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS

The error happens when I try to compile the project in Mbed Studio with tflite micro files generated using make. Steps include:
1. Generate files for Mbed:
```
 make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed OPTIMIZED_KERNEL_DIR=cmsis_nn generate_hello_world_mbed_project
```
2. Copy the tensorflow and third part folders from the gen folder into Mbed project folder in a folder named tensorflow_lite  (the two folders are in tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed). the project directory looks something likes this after copying: 
![Screenshot from 2021-03-04 13-01-10](https://user-images.githubusercontent.com/20337475/110029831-cfc5c100-7ce9-11eb-8f5e-e2dfd03c62cd.png)
The project is configured to run on Nucleo-L476RG.

3. Delete examples folder in tensorflow_lite/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed/tensorflow/lite/micro/

4. Hit 'clean build' button: 
![Screenshot from 2021-03-04 13-02-38](https://user-images.githubusercontent.com/20337475/110029932-eec45300-7ce9-11eb-9445-ee450d3ca050.png)

5. I faced an error with inline TfLiteRegistration Register_FULLY_CONNECTED_INT8()  being redefined. Solution: Comment out the following line in tensorflow_lite/tensorflow/lite/micro/kernels/fully_connected.h:
```
inline TfLiteRegistration Register_FULLY_CONNECTED_INT8() {
return Register_FULLY_CONNECTED();
}
```
6. Hit clean build again. The error this time:
```
Error: L6218E: Undefined symbol micro_test::did_test_fail (referred from BUILD/NUCLEO_L476RG/ARMC6/tensorflow_lite/tensorflow/lite/micro/kernels/conv_test_common.o).

```
I checked conv_test_common.cc in tensorflow_lite/tensorflow/lite/micro/kernels/conv_test_common.cc and it doesnt have any micro_test::did_test_fail function, it does reference ""tensorflow/lite/micro/kernels/conv_test.h"", which references ""tensorflow/lite/micro/testing/micro_test.h"", which does seem to have the following namespace:
```
  namespace micro_test {            \
  int tests_passed;                 \
  int tests_failed;                 \
  bool is_test_complete;            \
  bool did_test_fail;               \
  } 
```
I am not sure why the error is occuring."
47573,Handle null pointer parameters in C functions,"**System information**
- TensorFlow 2.4.0.
- Are you willing to contribute it? Yes.

**Describe the feature and the current behavior/state.**
Currently, when a null pointer (that should not be null) is passed to a C function, segmentation fails, causing the program to be interrupted. It would be good to handle this case to report the error in the `TF_Status` object.

**Will this change the current api? How?**
The `TF_Status` object would contain information related to the segmentation fault error.

**Who will benefit with this feature?**
TensorFlow C API users."
47571,LSTM conversion into Tflite. Error: This is not a valid Tensorflow lite model file -- Android Studio,"### 1. System information

On Google colabs

### 2. Code

`import tensorflow as tf 

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open('model_tensor.tflite', 'wb') as f:
  f.write(tflite_model)
`

### 3. Model Creation
model = keras.Sequential()
model.add(
    keras.layers.Bidirectional(
      keras.layers.LSTM(
          units=128, 
          input_shape=[X_train.shape[1], X_train.shape[2]]
      )
    )
)
model.add(keras.layers.Dropout(rate=0.5))
model.add(keras.layers.Dense(units=128, activation='tanh'))
model.add(keras.layers.Dense(y_train.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])

### 3. Failure after conversion

This is not a valid Tensorflow lite model file -- Android Studio

[Question on StackOverflow](https://stackoverflow.com/questions/66473725/unable-to-import-tensor-flow-lite-model-in-android-after-converting-from-keras-l)"
47570,tensorflow.keras and tensorflow.python.keras,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux centos
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 1.4.1
- Python version: 3.6.2
- Installed using virtualenv? pip? conda?: virtualenv conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):7.2.0
- CUDA/cuDNN version:cudnn-6.0.21-cuda8.0_0
- GPU model and memory:Tesla K20Xm 4G



**Describe the problem**
I was wondering why keras is under /path/tensorflow/python instead of /path/tensorflow/ anymore?
Is it because of new version?

I asked this question is because one package I am trying to use call tensorflow.keras when I import it, while my keras is actually under tensorflow.python.keras, and I want to find the reason before I reinstall everything.

Thanks.  
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47569,TFLite micro hard fault when using tf.reduce_sum,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install tensorflow
- Tensorflow version (commit SHA if source): 2.4.1
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm M4 with Zephyr

**Describe the problem**

I'm trying to export my TF-model with one simple custom layer to TFLite micro and get hardware faults when running it.

I would like to implement a layer that performs this calculation:

```python
def call(self, inputs):
    x = (tf.reduce_sum(tf.math.squared_difference(inputs['vector'],inputs['vector_old'])))
    x = tf.reshape(x,(1,1))
    return x
```
`vector` and `vector_old` have both the shape (1,4) and are float tensors and my output should have the shape (1,1)

**Sequence of commands/steps when I ran into the problem**

Because I get a hard fault I tried the following things:

**A** This works:

```python
def call(self, inputs):
    x = tf.math.subtract( inputs['vector'],inputs['vector_old'], name=None)
    x = tf.math.multiply(x,x)
    #x = tf.reduce_sum(x)
    x = x[0][0]
    x = tf.reshape(x,(1,1))
    return x
```

**B** This doesn't:

```python
def call(self, inputs):
    x = tf.math.subtract( inputs['vector'],inputs['vector_old'], name=None)
    x = tf.math.multiply(x,x)
    x = tf.reduce_sum(x)
    #x = x[0][0]
    x = tf.reshape(x,(1,1))
    return x 
```

**C** This doesn't work also:

```python
def call(self, inputs):
    x = tf.constant(1, shape=(1,1), dtype='float32')
    #x = tf.math.subtract( inputs['vector'],inputs['vector_old'], name=None)
    #x = tf.math.multiply(x,x)
    #x = tf.reduce_sum(x)
    #x = x[0][0]
    #x = tf.reshape(x,(1,1))
    return x
```

**D** But this works without a hard fault:
```python
def call(self, inputs):
    x = tf.math.subtract( inputs['vector'],inputs['vector_old'], name=None)
    x = tf.math.multiply(x,x)
    x1 = tf.math.add(x[0][0], x[0][1], name=None)
    x2 = tf.math.add(x[0][2], x[0][3], name=None)
    x = tf.math.add(x1,x2, name=None)
    x = tf.reshape(x,(1,1))
    return x
```

Thank you for any advice :)

Oliver
"
47568,Update TF with CUDA 11.2 and cuDNN 8.1,"Update TF with CUDA 11.2 and cuDNN 8.1
"
47565,TensorBoard Embedding Projector Metadata filepath format not clear,"
## URL(s) with the issue:

- https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin
- https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard
  - description of argument *embeddings_metadata*; See details link is gone: https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional

## Description of issue (what needs changing):

At the moment i try to use the embedding projector when using multiple runs (with a subfolder named by date for every training run)

I am not able to get the embedding projection including the metadata.tsv file for description labels (instead of just showing the index).

Everything does work as expected when i follow this tutorial for only one run: https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin

The embedding projector does show my custom labels when hovering over points.

But the embedding projector was not able to read the metadata.tsv file when i saved multiple runs in different subfolders.
I tried a lot of combinations (storing the metadata.tsv file in the root logs folder or in the subfolder of the specific run and so on)
It does not recognise the metadata file. So when hovering over points it only shows the index.

At first i used the manual process described in the tensorboard_projector_plugin tutorial.

Later i tried to use the TensorBoard callback with the arguments *embeddings_freq=1, embeddings_metadata=(tried different parameters as dict or string)*

I would be very happy to get clear instruction on how to define the metadata file path in the current version of tensorflow/tensorboard. It seems that a few other people have the same problem. Perhaps it did change with a newer version.

I am not sure how to reference the path:
- relative path from subfolder of run
- relative path from tensorflow log folder root / (experiment root in case of multiple subfolders)
- use backslash or forward slash (does it make a difference when using windows vs. unix)
- how to reference the different layers when using the dict format in the TensorBoard callback instead of a string (in case of different embedding metadata for different embedding layers
  - just use layer name as string which ist defined in model?
  - or use some string with the suffix */.ATTRIBUTES/VARIABLE_VALUE* (as described in the projector tutorial)?

I really tried a lot of things so i would be very happy to finally get a solution or clear description.

Thank you.

Reference links to issues with the same problem:

- https://github.com/tensorflow/tensorflow/issues/33967
- https://stackoverflow.com/questions/41708106/linking-tensorboard-embedding-metadata-to-checkpoint
- https://stackoverflow.com/questions/42679552/how-can-i-select-which-checkpoint-to-view-in-tensorboards-embeddings-tab"
47564,"pip3 install tensorflow-cpu fails on AArch64, Fedora 33 ","I guess it is because there is no 'wheel' defined yet. 'pip3 install tensorflow' is available, works. Is there a difference, if you have no (dedicated) GPU? "
47563,"""Status: device kernel image is invalid"" with A100","

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source (pip)
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2 (also have 10.1 installed)
- GPU model and memory: A100-SXM4-40GB

**Describe the current behavior**
```
import tensorflow as tf
tf.constant(0)
```

yields the error ""Status: device kernel image is invalid"".  [This issue](https://github.com/tensorflow/tensorflow/issues/41990)  mentions that tensorflow==2.3 no longer support some older GPUs, but this shouldn't apply to this case. Is it some cuda library confusion? 



**Detailed log**

```
Python 3.6.3 (default, Mar 20 2018, 13:50:41)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2021-03-04 14:19:04.766920: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
>>> x = tf.constant(0)
2021-03-04 14:19:31.998272: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-03-04 14:19:32.186620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: A100-SXM4-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s
2021-03-04 14:19:32.188740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:
pciBusID: 0000:41:00.0 name: A100-SXM4-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s
2021-03-04 14:19:32.190843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties:
pciBusID: 0000:81:00.0 name: A100-SXM4-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s
2021-03-04 14:19:32.192913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 3 with properties:
pciBusID: 0000:c1:00.0 name: A100-SXM4-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s
2021-03-04 14:19:32.192970: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-03-04 14:19:32.195229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-03-04 14:19:32.196491: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-03-04 14:19:32.196916: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-03-04 14:19:32.198940: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-03-04 14:19:32.200061: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-03-04 14:19:32.204798: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-03-04 14:19:32.221788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1, 2, 3
2021-03-04 14:19:32.222380: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-04 14:19:32.238093: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3200000000 Hz
2021-03-04 14:19:32.246203: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x484daa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-03-04 14:19:32.246307: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-03-04 14:19:32.624138: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x40cc380 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-03-04 14:19:32.624199: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): A100-SXM4-40GB, Compute Capability 8.0
2021-03-04 14:19:32.624218: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): A100-SXM4-40GB, Compute Capability 8.0
2021-03-04 14:19:32.624235: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): A100-SXM4-40GB, Compute Capability 8.0
2021-03-04 14:19:32.624286: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): A100-SXM4-40GB, Compute Capability 8.0
2021-03-04 14:19:32.633961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: A100-SXM4-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s
2021-03-04 14:19:32.636050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:
pciBusID: 0000:41:00.0 name: A100-SXM4-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s
2021-03-04 14:19:32.638161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties:
pciBusID: 0000:81:00.0 name: A100-SXM4-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s
2021-03-04 14:19:32.640212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 3 with properties:
pciBusID: 0000:c1:00.0 name: A100-SXM4-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s
2021-03-04 14:19:32.640284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-03-04 14:19:32.640334: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-03-04 14:19:32.640364: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-03-04 14:19:32.640394: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-03-04 14:19:32.640421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-03-04 14:19:32.640449: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-03-04 14:19:32.640476: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-03-04 14:19:32.656584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1, 2, 3
2021-03-04 14:19:32.656644: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/app-root/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
    allow_broadcast=True)
  File ""/opt/app-root/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 275, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File ""/opt/app-root/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 300, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/opt/app-root/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 97, in convert_to_eager_tensor
    ctx.ensure_initialized()
  File ""/opt/app-root/lib/python3.6/site-packages/tensorflow/python/eager/context.py"", line 539, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid

```
"
47562,Didnt find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3' when trying to run on Android,"### 1. System information

- OS Platform and Distribution (Windows 10):
- TensorFlow installation (pip package):
- TensorFlow library (2.3.1, pip package):

### 2. Code

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)



### 3. Failure after conversion
# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""model.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()  #### Returns error here

RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 151 (FlexSize) failed to prepare.

"
47561,tf.train.Checkpoint.read breaking inside a @tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 11.2.1
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8

**Describe the current behavior**
""Tensor is unhashable. Instead, use tensor.ref() as the key."" error when running tf.train.Checkpoint.read() inside a @tf.function.
I am running this on jupyter notebook and I am getting the error whenever I run the following function.
word2vec.load_variables.get_concrete_function((tf.TensorSpec([], tf.string, name=""file"")))

**Describe the expected behavior**
It was expected to work without errors as the tf.train.Checkpoint.write() does and nothing in the documentation indicates that the tf.train.Checkpoint().read() should be different.

**Standalone code to reproduce the issue**

    import tensorflow as tf    
    from tensorflow.keras import Model
    from tensorflow.keras.layers import Dot, Embedding, Flatten

    class Word2Vec(Model):
        def __init__(self, vocab_size, embedding_dim):
            super(Word2Vec, self).__init__()
            self.target_embedding = Embedding(vocab_size, 
                                          embedding_dim,
                                          input_length=1,
                                          name=""w2v_embedding"", )

            self.context_embedding = Embedding(vocab_size, 
                                           embedding_dim, 
                                           input_length=5)
            self.dots = Dot(axes=(3,2))
            self.flatten = Flatten()

        @tf.function
        def call(self, pair):
            target, context = pair
            we = self.target_embedding(target)
            ce = self.context_embedding(context)
            dots = self.dots([ce, we])
            return self.flatten(dots)
    
        @tf.function
        def save_variables(self, file):
            tf.train.Checkpoint(step=self.variables).write(""variables"")
            return 0
    
         @tf.function
        def load_variables(self, file):
            tf.train.Checkpoint(step=self.variables).read(""variables"")
            return 0
    
    #Word2Vec
    embedding_dim = 300
    vocab_size = 50000
    word2vec = Word2Vec(vocab_size, embedding_dim)
    opt = tf.keras.optimizers.SGD(learning_rate=1.0)
    word2vec.compile(optimizer=opt,
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

    target = tf.constant([[1]])
    context = tf.constant([[[2],[3],[4],[5],[6]]])
    predict_example = (target,context)
    word2vec(predict_example)

    call_output = word2vec.call.get_concrete_function((tf.TensorSpec([None,1], tf.int32, name='target'), tf.TensorSpec([None,5,1], tf.int32, name='context')))
    save_variables = word2vec.save_variables.get_concrete_function((tf.TensorSpec([], tf.string, name=""file"")))
    load_variables = word2vec.load_variables.get_concrete_function((tf.TensorSpec([], tf.string, name=""file"")))

    word2vec.save(""w2v_shakespeare.tf"", save_format=""tf"", signatures={'predict': call_output, 'save': save_variables, 'load': load_variables})
`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

TypeError: in user code:

    <ipython-input-2-f49e7051c166>:31 load_variables  *
        tf.train.Checkpoint(step=self.variables).read(""variables"")
    /Users/maurodlt/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py:2148 read  **
        return self._saver.restore(save_path=save_path, options=options)
    /Users/maurodlt/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py:1322 restore
        file_prefix_feed_dict = {self._file_prefix_placeholder: save_path}
    /Users/maurodlt/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:830 __hash__
        raise TypeError(""Tensor is unhashable. ""

    TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.


[traceback_error.txt](https://github.com/tensorflow/tensorflow/files/6084255/traceback_error.txt)
**Background** 
I am creating a TF model in Python which I am training using the C API. After training in the C API, I want to save and load the trained model. Considering the limitations in the C API, I need to run @tf.functions defined in python to save and load my model. However, the only function that I found that saves variables and can be used inside a @tf.function is the tf.train.Checkpoint.write() which can only be read using the tf.train.Checkpoint.read(), which is returning the reported error.

Thanks : ) 

"
47560,passing constants to costume rnn cell not working,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro 20.2.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):2.4.1
- Python version:3.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.2
- GPU model and memory: GeForce GTX 1080;  6858 MB memory

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
when passing the ""constants"" argument to a tf.keras.layers.RNN model instance with a costume rnn cell, the python script crashes
**Describe the expected behavior**
the ""constants"" argument is passed to the costume RNN cells call function, and no error occurs

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class dummy_rnn_cell(tf.keras.Model):
  def __init__(self, units):
    super(dummy_rnn_cell, self).__init__()
    self.gru = tf.keras.layers.GRUCell(units,
                                   recurrent_initializer='glorot_uniform')
    self.units = self.gru.units
    self.state_size = self.gru.state_size
    self.output_size = self.gru.output_size

  def call(self, input_at_t, states_at_t, constants):
    output, state = self.gru(input_at_t, states_at_t)
    return output, state
  
  def get_initial_state(inputs=None, batch_size=None, dtype=None):
      return self.gru.get_initial_state(inputs, batch_size, dtype)

class dummy_model(tf.keras.Model):
    def __init__(self, units):
        super(dummy_model, self).__init__()
        self.rnn = tf.keras.layers.RNN(dummy_rnn_cell(units), True)

    def call(self, inp):
        seq = inp[0]
        const = inp[1]
        out = self.rnn(seq, constants=const)
        return out

model = dummy_model(1)
seqs = [[[[1],[2]],[[1],[2]]],[[[1],[2]],[[1],[2]]]]
consts = [[[3],[3]],[[3],[3]]]
dataset = tf.data.Dataset.from_tensor_slices((seqs,consts))
for inp in dataset:
    print(model(inp))
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
2021-03-04 13:59:52.420241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-04 13:59:53.466141: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-04 13:59:53.466826: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-04 13:59:53.492760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-04 13:59:53.493343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:0d:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.797GHz coreCount: 20 deviceMemorySize: 7.91GiB deviceMemoryBandwidth: 298.32GiB/s
2021-03-04 13:59:53.493366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-04 13:59:53.495658: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-04 13:59:53.495704: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-04 13:59:53.496468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-04 13:59:53.496635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-04 13:59:53.497347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2021-03-04 13:59:53.497917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-04 13:59:53.498048: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-04 13:59:53.498137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-04 13:59:53.498663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-04 13:59:53.499094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-04 13:59:53.499318: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-04 13:59:53.500255: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-04 13:59:53.500322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-04 13:59:53.500769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:0d:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.797GHz coreCount: 20 deviceMemorySize: 7.91GiB deviceMemoryBandwidth: 298.32GiB/s
2021-03-04 13:59:53.500786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-04 13:59:53.500798: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-04 13:59:53.500809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-04 13:59:53.500820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-04 13:59:53.500830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-04 13:59:53.500840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2021-03-04 13:59:53.500850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-04 13:59:53.500859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-04 13:59:53.500902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-04 13:59:53.501396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-04 13:59:53.501948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-04 13:59:53.501973: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-04 13:59:53.884072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-04 13:59:53.884104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-04 13:59:53.884110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-04 13:59:53.884278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-04 13:59:53.884769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-04 13:59:53.885213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-04 13:59:53.885636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6858 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:0d:00.0, compute capability: 6.1)
2021-03-04 13:59:53.885875: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Traceback (most recent call last):
  File ""/home/moritz/Projects/Bitblade/tensorflow/./dummy.py"", line 39, in <module>
    print(model(inp))
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/home/moritz/Projects/Bitblade/tensorflow/./dummy.py"", line 31, in call
    out = self.rnn(seq, constants=const)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 717, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1008, in __call__
    self._maybe_build(inputs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 2710, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 578, in build
    self.cell.build(step_input_shape)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py"", line 407, in build
    raise ValueError(
ValueError: Currently, you cannot build your model if it has positional or keyword arguments that are not inputs to the model, but are required for its `call` method. Instead, in order to instantiate and build your model, `call` your model on real tensor data with all expected call arguments.
```
"
47559,Fails to run the model in an Android app,"Hi, I've created a model using keras. The model works fine in google-colab but when I attach it to my app in Android Studio I get the following comment:

`java.lang.AssertionError: Error occurred when initializing ImageClassifier: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.`

My app works fine with a different model so I believe the problem is in the model (in the metadata to my understanding).

This is the metadata code:
# Creates model info.
model_meta = _metadata_fb.ModelMetadataT()
model_meta.name = ""Pedastrian Lights classifier""
model_meta.description = (""checks what color is the light if exists in frame"")
model_meta.version = ""v1""
model_meta.author = ""Neta Kletshevsky""
# Creates input info.
input_meta = _metadata_fb.TensorMetadataT()

# Creates output info.
output_meta = _metadata_fb.TensorMetadataT()
input_meta.name = ""image""
input_meta.description = (
    ""Input image to be classified."".format(160, 160))
input_meta.content = _metadata_fb.ContentT()
input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()
input_meta.content.contentProperties.colorSpace = (
    _metadata_fb.ColorSpaceType.RGB)
input_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.ImageProperties)
input_normalization = _metadata_fb.ProcessUnitT()
input_normalization.optionsType = (
    _metadata_fb.ProcessUnitOptions.NormalizationOptions)
input_normalization.options = _metadata_fb.NormalizationOptionsT()
input_normalization.options.mean = [127.5]
input_normalization.options.std = [127.5]
input_meta.processUnits = [input_normalization]
input_stats = _metadata_fb.StatsT()
input_stats.max = [255]
input_stats.min = [0]
input_meta.stats = input_stats

# Creates output info.
output_meta = _metadata_fb.TensorMetadataT()
output_meta.name = ""probability""
output_meta.description = ""Probabilities of the 1001 labels respectively.""
output_meta.content = _metadata_fb.ContentT()
output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()
output_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.FeatureProperties)
output_stats = _metadata_fb.StatsT()
output_stats.max = [1.0]
output_stats.min = [0.0]
output_meta.stats = output_stats
#label_file = _metadata_fb.AssociatedFileT()
#label_file.name = ""labels""#label_file.description = ""Labels for objects that the model can recognize.""
#label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS
#output_meta.associatedFiles = [label_file]

# Creates subgraph info.
subgraph = _metadata_fb.SubGraphMetadataT()
subgraph.inputTensorMetadata = [input_meta]
subgraph.outputTensorMetadata = [output_meta]
model_meta.subgraphMetadata = [subgraph]

b = flatbuffers.Builder(0)
b.Finish(
    model_meta.Pack(b),
    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)
metadata_buf = b.Output()

populator = _metadata.MetadataPopulator.with_model_file(""converted_model.tflite"")
populator.load_metadata_buffer(metadata_buf)
#populator.load_associated_files('/content/drive/MyDrive/android/assets')
populator.populate()

**What NormalizationOptions do I need to add and how?**"
47558,Unable to create custom Wide and Deep model with shared inputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Bundled along with Colab
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.10

**Current behavior**
I wanted to create a Bayesian W&D model with MC Dropout (hence, cannot use the standard Tensorflow model). Separate inputs from a CSV file are provided to the Wide & Deep parts of the network. However, there are a few overlaps in the input (e.g. ""discount"" column in the CSV dataset is used for a crossed column in the Wide part & as one-hot encoded categorical feature column in the Deep part). BUT, I'm getting a Graph Disconnected error although the inputs are perfect during model building. 

**Standalone code to reproduce the issue**

Given the following `wide_inputs` and `deep_inputs`:

**wide_inputs:**
{'campaign_category': <KerasTensor: shape=(None,) dtype=string (created by layer 'campaign_category')>,
 'discount': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'discount')>,
 'free_shipping': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'free_shipping')>,
 'frequency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'frequency_score')>,
 'has_urgency': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'has_urgency')>,
 'is_discount_mentioned': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_discount_mentioned')>,
 'is_exclusive': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_exclusive')>,
 'is_one_for_free': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_one_for_free')>,
 'recency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'recency_score')>,
 'retention_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'retention_score')>,
 'riid': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid')>,
 'sends_since_last_open': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sends_since_last_open')>,
 'sent_dayofweek': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_dayofweek')>,
 'sent_hr': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_hr')>,
 'sent_week': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_week')>,
 'sl_contains_price': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sl_contains_price')>}


**deep_inputs:**
{'discount': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'discount')>,
 'free_shipping': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'free_shipping')>,
 'frequency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'frequency_score')>,
 'has_urgency': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'has_urgency')>,
 'is_discount_mentioned': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_discount_mentioned')>,
 'is_exclusive': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_exclusive')>,
 'is_one_for_free': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_one_for_free')>,
 'promo': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'promo')>,
 'recency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'recency_score')>,
 'retention_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'retention_score')>,
 'riid': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid')>,
 'sale': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sale')>,
 'sends_since_last_open': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sends_since_last_open')>,
 'sent_dayofweek': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_dayofweek')>,
 'sent_hr': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_hr')>,
 'sent_week': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sent_week')>,
 'sl_contains_price': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sl_contains_price')>}

```python
def build_bayesian_wide_and_deep_model(wide_inputs, wide_feature_columns, 
                        deep_inputs, dnn_feature_columns, dnn_hidden_units, 
                        multihead_count = 64, p_value=0.5):

    #Build the Deep Network
    deep = tf.keras.layers.DenseFeatures(dnn_feature_columns, name='deep_inputs')(deep_inputs)    

    for layerno, numnodes in enumerate(dnn_hidden_units):
        deep = tf.keras.layers.Dense(numnodes, activation='relu', name='dnn_{}'.format(layerno+1))(deep)
    
    deep = tf.keras.Model(inputs=deep_inputs, outputs=deep)
    
    #Build the Wide Network
    wide = tf.keras.layers.DenseFeatures(wide_feature_columns, name='wide_inputs')(wide_inputs)
    wide = tf.keras.layers.Dense(1, activation=""linear"", name=""wide_output"")(wide)
    wide = tf.keras.Model(inputs=wide_inputs, outputs=wide)

    #Concatenate the Wide & Deep
    both = tf.keras.layers.concatenate([deep.output, wide.output], name='both')

    #Create the multi-head layer
    multihead_pre_dropout = tf.keras.layers.Dropout(p_value)(both, training=True)
    multihead = tf.keras.layers.Dense(multihead_count, activation='relu', name='multihead')(multihead_pre_dropout)
    multihead_dropout = tf.keras.layers.Dropout(p_value)(multihead, training=True)

    #Create the output layer
    output = tf.keras.layers.Dense(2, activation='softmax', name='optimal_action')(multihead_dropout)
    
    #Convert the 2 inputs dictionary into a single list
    full_inputs = (wide.input).copy()
    full_inputs.update(deep.input)
    model = tf.keras.Model(inputs=full_inputs, outputs=output)
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

bwdmodel = build_bayesian_wide_and_deep_model(wide_inputs = crossed_columns_input, wide_feature_columns = crossed_columns,  deep_inputs = deep_columns_input, dnn_feature_columns = deep_columns, dnn_hidden_units = [512, 256, 128], multihead_count = 64, p_value=0.5)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The traceback is as follows

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-33-5f875643195f> in <module>()
     39 bwdmodel = build_bayesian_wide_and_deep_model(wide_inputs = crossed_columns_input, wide_feature_columns = crossed_columns, 
     40                                deep_inputs = deep_columns_input, dnn_feature_columns = deep_columns,
---> 41                                dnn_hidden_units = [512, 256, 128], multihead_count = 64, p_value=0.5)
     42 tf.keras.utils.plot_model(bwdmodel, '/content/drive/MyDrive/Bandit_Project/models/bayesian_w&d.png', show_shapes=False, rankdir='LR')

5 frames
<ipython-input-33-5f875643195f> in build_bayesian_wide_and_deep_model(wide_inputs, wide_feature_columns, deep_inputs, dnn_feature_columns, dnn_hidden_units, multihead_count, p_value)
     31     full_inputs.update(deep.input)
     32     #pdb.set_trace()
---> 33     model = tf.keras.Model(inputs=full_inputs, outputs=output)
     34     model.compile(optimizer='adam',
     35                   loss='categorical_crossentropy',

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    515     self._self_setattr_tracking = False  # pylint: disable=protected-access
    516     try:
--> 517       result = method(self, *args, **kwargs)
    518     finally:
    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in __init__(self, inputs, outputs, name, trainable, **kwargs)
    118     generic_utils.validate_kwargs(kwargs, {})
119     super(Functional, self).__init__(name=name, trainable=trainable)
--> 120     self._init_graph_network(inputs, outputs)
    121 
    122   @trackable.no_automatic_dependency_tracking

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    515     self._self_setattr_tracking = False  # pylint: disable=protected-access
    516     try:
--> 517       result = method(self, *args, **kwargs)
    518     finally:
    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in _init_graph_network(self, inputs, outputs)
    202     # Keep track of the network's nodes and layers.
    203     nodes, nodes_by_depth, layers, _ = _map_graph_network(
--> 204         self.inputs, self.outputs)
    205     self._network_nodes = nodes
    206     self._nodes_by_depth = nodes_by_depth

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in _map_graph_network(inputs, outputs)
    988                              'The following previous layers '
    989                              'were accessed without issue: ' +
--> 990                              str(layers_with_complete_input))
    991         for x in nest.flatten(node.outputs):
    992           computable_tensors.add(id(x))

ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.int64, name='discount'), name='discount', description=""created by layer 'discount'"") at layer ""wide_inputs"". The following previous layers were accessed without issue: ['deep_inputs', 'dnn_1']
```
"
47557,What's the correct matcher of 'tf.nn.conv2d_transpose' in quantization aware training ?,"To some reason, I have to use TF1.X (actually TF1.14.0) to quantize a custom model.
As general,
```
g = tf.get_default_graph()
tf.contrib.quantize.create_training_graph(input_graph=g,quant_delay=0)
```
was used to create fakequant min/max variable and I could find the min/max value of 'tf.nn.conv2d_transpose' in checkpoint. But when
```
tf.contrib.quantize.create_eval_graph()
```
created the Fakequant Evaluation Graph, 'tf.nn.conv2d_transpose' was not added fake quantization nodes. I tried transpose only, transpose --> relu6, transpose --> bias --> relu6, transpose --> bacthnorm --> relu6, but all of them did not work.

### **Thus the direct question: What's the correct matcher of 'tf.nn.conv2d_transpose' in quantization aware training ? Is there any resources documented the correct matchers of every op ?**"
47556,"Lambda(tf.identity) fails to deserialize, while Lambda(lambda t: tf.identity(t)) works fine","Note: I'm not sure if this belongs here or the Keras bug tracker. I'm filing this report here because I use `tf.keras`, not `keras`.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): bundled in Colab
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.10

**Describe the current behavior**

The following snippet works fine:

```python
import tensorflow as tf
keras = tf.keras

model_input = keras.layers.Input(shape=[256], dtype=tf.float32)

model_output = keras.layers.Lambda(
    lambda t: tf.identity(t),
    name='identity_in_lambda',
)(model_input)

model = keras.Model(inputs=model_input, outputs=model_output, name='test_model')

model.save('test_model.h5')

loaded_model = keras.models.load_model('test_model.h5')
```

If, however, I replace `lambda t: tf.identity(t)` with just `tf.identity`, deserialization crashes with a rather long traceback:

```pytb
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py in wrapper(*args, **kwargs)
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):

TypeError: 'str' object is not callable

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
14 frames
<ipython-input-4-ba5e25a08d15> in <module>()
     10 model.save('test_model_broken.h5')
     11 
---> 12 loaded_model = keras.models.load_model('test_model_broken.h5')

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)
    205           (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
    206         return hdf5_format.load_model_from_hdf5(filepath, custom_objects,
--> 207                                                 compile)
    208 
    209       filepath = path_to_string(filepath)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)
    182     model_config = json_utils.decode(model_config.decode('utf-8'))
    183     model = model_config_lib.model_from_config(model_config,
--> 184                                                custom_objects=custom_objects)
    185 
    186     # set weights

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)
     62                     '`Sequential.from_config(config)`?')
     63   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top
---> 64   return deserialize(config, custom_objects=custom_objects)
     65 
     66 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)
    175       module_objects=LOCAL.ALL_OBJECTS,
    176       custom_objects=custom_objects,
--> 177       printable_module_name='layer')

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    356             custom_objects=dict(
    357                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 358                 list(custom_objects.items())))
    359       with CustomObjectScope(custom_objects):
    360         return cls.from_config(cls_config)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in from_config(cls, config, custom_objects)
    667     """"""
    668     input_tensors, output_tensors, created_layers = reconstruct_from_config(
--> 669         config, custom_objects)
    670     model = cls(inputs=input_tensors, outputs=output_tensors,
    671                 name=config.get('name'))

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in reconstruct_from_config(config, custom_objects, created_layers)
   1283       if layer in unprocessed_nodes:
   1284         for node_data in unprocessed_nodes.pop(layer):
-> 1285           process_node(layer, node_data)
   1286 
   1287   input_tensors = []

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in process_node(layer, node_data)
   1231         input_tensors = (
   1232             base_layer_utils.unnest_if_single_tensor(input_tensors))
-> 1233       output_tensors = layer(input_tensors, **kwargs)
   1234 
   1235       # Update node index map.

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    950     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):
    951       return self._functional_construction_call(inputs, args, kwargs,
--> 952                                                 input_list)
    953 
    954     # Maintains info about the `Layer.call` stack.

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)
   1089         # Check input assumptions set after layer building, e.g. input shape.
   1090         outputs = self._keras_tensor_symbolic_call(
-> 1091             inputs, input_masks, args, kwargs)
   1092 
   1093         if outputs is None:

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)
    820       return nest.map_structure(keras_tensor.KerasTensor, output_signature)
    821     else:
--> 822       return self._infer_output_signature(inputs, args, kwargs, input_masks)
    823 
    824   def _infer_output_signature(self, inputs, args, kwargs, input_masks):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)
    861           # TODO(kaftan): do we maybe_build here, or have we already done it?
    862           self._maybe_build(inputs)
--> 863           outputs = call_fn(inputs, *args, **kwargs)
    864 
    865         self._handle_activity_regularization(inputs, outputs)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, mask, training)
    915     with backprop.GradientTape(watch_accessed_variables=True) as tape,\
    916         variable_scope.variable_creator_scope(_variable_creator):
--> 917       result = self.function(inputs, **kwargs)
    918     self._check_variables(created_variables, tape.watched_variables())
    919     return result

/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py in wrapper(*args, **kwargs)
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a
    204       # TypeError, when given unexpected types.  So we need to catch both.
--> 205       result = dispatch(wrapper, args, kwargs)
    206       if result is not OpDispatcher.NOT_SUPPORTED:
    207         return result

TypeError: 'module' object is not callable
```

**Describe the expected behavior**

Replacing `lambda t: tf.identity(t)` with `tf.identity` should not change behavior.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1K24uCFzIBeYW2bPBuu01cRHNRQ9vuld1?usp=sharing"
47555,Optimize TFLite argmin/argmax,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Pixel 3a Android 11
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): nightly

**Standalone code to reproduce the issue** 

```python
model = tf.keras.Sequential([
  tf.keras.Input(shape=(512,), batch_size=1, dtype=tf.int32),
  tf.keras.layers.Embedding(1000, 128),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.Dense(5000),
  tf.keras.layers.Lambda(lambda x: tf.math.argmax(x, axis=-1)),
])
```

Given a simple sequence model with 5000 output labels, ArgMax takes as much time as computation-intense ops like FullyConnected and LSTM. Here is my benchmark on pixel 3a

```
                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]
        UNIDIRECTIONAL_SEQUENCE_LSTM            1           27.552          33.447%         33.447%          0.000              1
                 FULLY_CONNECTED                1           27.474          33.353%         66.800%          0.000              1
                         ARG_MAX                1           24.868          30.189%         96.989%          0.000              1
                         RESHAPE                1            2.343           2.844%         99.834%          0.000              1
                          GATHER                1            0.137           0.166%        100.000%          0.000              1
```

Looks like compiler is not smart enough to vectorize naive-for-loop argmin-max reduction. In order to improve the performance, we should at least specialize the implementation when `axis=dims-1`, that is, `inner_size=1` in the following code snippet. If the request is valid, I can try to handcraft some Neon intrinsics to see if the performance is better.

https://github.com/tensorflow/tensorflow/blob/ea49c350b3b812bd852ae8456d2c8cc29aff6c95/tensorflow/lite/kernels/internal/reference/arg_min_max.h#L49-L63"
47554,"WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.","`WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.
`

![image](https://user-images.githubusercontent.com/4510984/109937386-d2d0a980-7d09-11eb-8e3c-b83cb07023f9.png)

![image](https://user-images.githubusercontent.com/4510984/109937397-d6643080-7d09-11eb-9206-c47efa47bf46.png)
"
47553,A sentence in Basic Image Classification Tutorial is not clear,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/tutorials/keras/classification#make_predictions

## Description of issue (what needs changing): 
The statement, **The model's linear outputs, [logits](https://developers.google.com/machine-learning/glossary#logits).** can't be understood.

More explanation can be added on why the Outputs are Linear."
47552,Hyperlinks can be added for improved understand-ability of Basic Image Classification Tutorial ,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/tutorials/keras/classification#import_the_fashion_mnist_dataset
https://www.tensorflow.org/tutorials/keras/classification#set_up_the_layers
https://www.tensorflow.org/tutorials/keras/classification#compile_the_model
https://www.tensorflow.org/tutorials/keras/classification#feed_the_model

## Description of issue (what needs changing):
1. The phrase, **`load the Fashion MNIST data`** In the statement, **`Import and load the Fashion MNIST data directly from TensorFlow:`**, can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data

2. The word, **`layer`**, In the statement, **`The basic building block of a neural network is the layer`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/layers

3. For better readability, The arguments in the Dense Layers can be named i.e., `tf.keras.layers.Dense(128)` can be replaced with `tf.keras.layers.Dense(units = 128)`

4. The word **`compile`** in the statement, **`These are added during the model's compile step:`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile

5. The word **`Loss Function`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/losses

6. The word **`Optimizer`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/optimizers

7. The word **`Metrics`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/metrics

8. **`model.fit`** in the statement, **`To start training, call the model.fit method`** can be linked to https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit

### Clear description

Adding the Hyperlinks for each API helps the beginners understand the functionality."
47551,Wrong output values in TFLite_Detection_PostProcess operation when max_classes_per_detection > 1,"**System information**

OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from: pypi
TensorFlow version: 1.15.2
Python version: 3.6.9

**Describe the current behavior**

When you add the NMS operation at exporting the TFlite graph with the option `max_classes_per_detection > 1`, you'll get more than one class per detection as expected, but the class IDs and their corresponding scores are not consecutive in the output array. Instead, the output arrays have a position without value, that only contains garbage memory.

For example, with `max_classes_per_detection = 2`, you get:

output[0] = detection1_top1_class_id
output[1] = garbage
output[2] = garbage
output[3] = detection1_top2_class_id
output[4] = garbage
output[5] = garbage
...

**Describe the expected behavior**

output[0] = detection1_top1_class_id
output[1] = detection1_top2_class_id
output[2] = detection2_top1_class_id
...

**Standalone code to reproduce the issue**

Export the model with:

```bash
python3 export_tflite_ssd_graph.py \
    --pipeline_config_path=configs/ssdlite_mobiledet_cpu_320x320_coco_sync_4x4.config \
    --trained_checkpoint_prefix=ssdlite/model.ckpt-100000 \
    --output_directory=ssdlite/tflite \
    --max_detections=10 \
    --max_classes_per_detection=2 \
    --add_postprocessing_op=true
```

```
# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path='ssdlite/tflite/model.tflite'))
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test model
input_shape = input_details[0]['shape']
input_data = load_image_into_numpy_array(image_paths[0])
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data
count = int(interpreter.get_tensor(output_details[3]['index'])[0])
boxes = interpreter.get_tensor(output_details[0]['index'])[:, :count]
classes = interpreter.get_tensor(output_details[1]['index'])[:, :count]
scores = interpreter.get_tensor(output_details[2]['index'])[:, :count]

# classes[1] and scores[1] will contain random numbers from unassigned memory slots
```

I think that the error stems from this line of code:

https://github.com/tensorflow/tensorflow/blob/127adf71b4a995265cdcc960de38c164c93d53a9/tensorflow/lite/kernels/detection_postprocess.cc#L710

`box_offset` should have the value of `output_box_index` (which is already incremented in the inner loop where the top classes of the detection are assigned) or move the line `output_box_index++;` out of the inner loop to really make it account for the number of detections"
47550,Validation fails while model fit,"**System information**
- OS Platform and Distribution: Linux Mint 19.3
- TensorFlow installed from: pypi
- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/6081336/tf_env.txt)

**Describe the current behavior**
Train finished complete, but validation fails

**Describe the expected behavior**
Train and validation succeeded

**Standalone code to reproduce the issue**
I have some TF.Example files and load it as:
```python
def get_dataset(files_pattern: str, strategy: tf.distribute.Strategy, batch_size: int):
    global_batch_size = _get_global_batch_size(strategy, batch_size)
    dataset = tf.data.TFRecordDataset(tf.io.gfile.glob(files_pattern))
    dataset = dataset.shuffle(10 * batch_size)
    # dataset = dataset.repeat()
    dataset = dataset.batch(global_batch_size)
    dataset = dataset.map(_parse_example)  # vectorised tf.train.Example parsing
    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

    options = dataset_ops.Options()
    options.experimental_distribute.auto_shard_policy = dataset_ops.distribute_options.AutoShardPolicy.DATA
    dataset = dataset.with_options(options)

    return dataset
```

Model training:
```python
def run_local(train_pattern, eval_pattern, batch_size, model_dir, log_path):
    strategy = tf.distribute.OneDeviceStrategy(device=""/cpu:0"")

    train_ds = data.get_dataset(train_pattern, strategy, batch_size)
    eval_ds = data.get_dataset(eval_pattern, strategy, batch_size)

    with strategy.scope():
        model = _build_keras_model(
            hidden_units=DNN_HIDDEN_UNITS,
            learning_rate=LEARNING_RATE)

    profile_options = tf.profiler.experimental.ProfilerOptions(
        host_tracer_level=1, python_tracer_level=1, device_tracer_level=1,
    )
    tf.profiler.experimental.start(log_path, options=profile_options)

    # Write logs to path
    callbacks = get_callbacks(log_path, 0)

    model.fit(
        train_ds,
        validation_data=eval_ds,
        callbacks=callbacks)

    tf.profiler.experimental.stop()

    _save_model(model_dir, model, strategy)
```

**Other info / logs**
```
Train on None steps
   1000/Unknown - 492s 491ms/step - batch: 499.5000 - size: 1.0000 - loss: 0.0650 - tp: 32328.0000 - fp: 28630.0000 - tn: 9838817.0000 - fn: 100225.0000 - accuracy: 0.9871 - precision: 0.5303 - recall: 0.2439 - auc: 0.8881 - mse: 0.0125 - binary_crossentropy: 0.0646/home/local/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
Traceback (most recent call last):
  File ""/snap/pycharm-community/226/plugins/python-ce/helpers/pydev/pydevd.py"", line 1477, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/snap/pycharm-community/226/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/local/projects/***/model/local_train.py"", line 12, in <module>
    run_local(train_pattern, eval_pattern, batch_size, model_path, log_path)
  File ""/home/local/projects/***/model/keras.py"", line 310, in run_local
    model.fit(
  File ""/home/local/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 789, in fit
    return func.fit(
  File ""/home/local/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_distributed_v1.py"", line 675, in fit
    return training_arrays_v1.fit_loop(
  File ""/home/local/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py"", line 417, in model_iteration
    val_results = model_iteration(
  File ""/home/local/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py"", line 347, in model_iteration
    index_array = np.arange(num_samples_or_steps)
TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'
```"
47549,How can i secure my .tflite model on edge device?,"I have built the TensorFlow model and converted it into a .tflite model, I want to deploy my .tflite model into an edge device(android or IoT).
Final goal: I want to protect my model from being a copy or reverse engineered and also the user is only able to use this model in my android app or IoT device.
how can I do that?"
47548,Different behavior between eager execution and tf.function,"Hello Tensorflow community

I have a loss function with a few logarithms in it:

```
def NegLogProb(a, b, targ):
    return -tf.reduce_mean(tf.reduce_sum(tf.math.log(a*b) + (a-1)*tf.math.log(targ+1e-8) + (b-1)*tf.math.log((1-targ**a)+1e-8), axis=-1))
```

This is a form of the beta distribution. My network outputs parameters _a_ and _b_: the range for both outputs is >=(1+1e-8). _targ_ is a spectrum whose values are normalized from 0-1. There are three terms in this equation. Where necessary you can see that I added 1e-8 to avoid log(0). I believe I have put in all the safeguards to prevent the logarithm from failing.

This equation works when running in eager execution, but if I create a graph out of it, the equation will always output inf followed by NaN. Can anyone account for the fact that the behavior changes when creating a graph?

I am using Tensorflow version 2.1.0

Many thanks in advance."
47547,TRTconverted tensorRT model pb too large,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu-18.04.x86_64
- TensorFlow installed from (source or binary): build from source 
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: cuda=10.1, cuDnn=7.6.4
- GPU model and memory:
- TensorRT version: 6.0.1

**Describe the current behavior**
the converted tensorRT model (pb file) is too large (**9.5M --> 1.1G**, see below)

**Describe the expected behavior**
I guess the converted model could be larger than the original (9.5M), but should not be **too large**

**Standalone code to reproduce the issue**
```
from tensorflow.python.compiler.tensorrt import trt_convert as trt

conversion_params = trt.TrtConversionParams(precision_mode=trt.TrtPrecisionMode.FP32)

input_saved_model_dir = 'bert_finetune_20210303'
output_saved_model_dir = 'bert_finetune_20210303_fp32'

converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir, conversion_params=conversion_params)
converter.convert()
converter.save(output_saved_model_dir)
```

the size bewteen converted and the original:

```sh
# the original
4.0K	./bert_finetune_20210303/assets
9.5M	./bert_finetune_20210303/saved_model.pb
387M	./bert_finetune_20210303/variables
397M	./bert_finetune_20210303

# precision_mode=trt.TrtPrecisionMode.FP16
4.0K	./bert_finetune_20210303_fp16/assets
1.1G	./bert_finetune_20210303/saved_model.pb
387M	./bert_finetune_20210303_fp16/variables
1.5G   	./bert_finetune_20210303_fp16

# precision_mode=trt.TrtPrecisionMode.FP32
4.0K	./bert_finetune_20210303_fp32/assets
1.1G	./bert_finetune_20210303_fp32/saved_model.pb
387M	./bert_finetune_20210303_fp32/variables
1.5G 	./bert_finetune_20210303_fp32
```

anyone could help me, thanks a lot.

**NOTE**: the model used here: https://drive.google.com/file/d/1sSu08U4-c-iopi5SGLfPUWP68MTpC5wg/view?usp=sharing"
47546,About reshape op on TF Lite nnapi delegate,"Android NN API only supported tensor rank: up to 4. So maybe need to add rank restriction on TF Lite nnapi delegate. If not, some models which have reshape op(rank > 4) will be failed on Android platform.

For example:
tensorflow/lite/delegates/nnapi/nnapi_delegate.cc:

```c++
bool NNAPIDelegateKernel::Validate(
    const TfLiteContext* context, int builtin_code, int version,
    int android_sdk_version, const TfLiteNode* node,
    bool is_accelerator_specified,
    std::vector<NNAPIValidationFailure>* map_failures) {
...
case kTfLiteBuiltinReshape: {
...
      // add these lines for rank restriction
      const auto& input = context->tensors[node->inputs->data[0]];
      Expect(input.dims->size <= 4,
             NNAPIValidationFailureType::kUnsupportedOperandRank,
             ""Input rank should be <= 4"", &val_ctx);
...
}
...
}
```
"
47545,tf.data.zip and tf.data.cache throws cachefile AlreadyExistsError and/or deletes valid cachefiles,"**System information**
Running on Ubuntu 20.04 using the docker container 2.4.1-gpu-jupyter.
Also occurs on Google Colab environment running TF 2.4.1

**Describe the current behavior**
`tf.data.Dataset.cache` and `tf.data.Dataset.zip` do not operate well together in certain situations, where there is a hierarchy between datasets but nevertheless they are both desired and cached. Imagine the scenario of an audio data pipeline where client extracts spectrograms and uses those spectrograms to extract MFCCs. Furthermore, the user wants to retain the spectrogram dataset at the end of the pipeline and includes in the final `zip` operation. Now, if the spectrogram dataset is cached to disk to allow for faster feature extraction later (say with different hyperparameters), user will be faced the exception: 
```
AlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists 
```
Currently the user can sort of get behind this behavior by simply moving the `.cache` operation on the intermediary dataset to after the `.map` operation that creates the conflicting dataset. This will introduce a performance hit, however, unless all the datasets  upstream of the intermediary dataset are cached. Which brings us to the second issue.
Even if the upstream datasets are cached, there is a bug/behavior that is causing the cache files to be deleted after an iteration over the entire zipped dataset is completed. This second bug essentially renders the workaround useless when the user wants to cache both the downstream and upstream datasets.

**Describe the expected behavior**
The expected or ideal behavior would be the ability of caching&retaining intermediary dataset without a problem. This would make intuitive sense for users building multi-step pipelines with the `tf.data` API.

**Standalone code to reproduce the issue**
The notebook below contains the problematic case as well as the workaround described (and its demise). 
[Link to Google Colab Notebook](https://colab.research.google.com/drive/1MOOfB5eJKAEyNTSxHY1vci6Unc5J2mi0?usp=sharing)

**Other info / logs** 
```
---------------------------------------------------------------------------
AlreadyExistsError                        Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   2112       ctx.executor = executor_new
-> 2113       yield
   2114     finally:

9 frames
AlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('spectrogram_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1614817563 [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

AlreadyExistsError                        Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/executor.py in wait(self)
     67   def wait(self):
     68     """"""Waits for ops dispatched in this executor to finish.""""""
---> 69     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     70 
     71   def clear_error(self):
```
"
47544,Obtain the output of intermediate layer (Functional API) and use it in SubClassed API,"# Documents 

In the [keras doc](https://keras.io/getting_started/faq/), it says that if we want to pick the **output of the model** (sequential and functional), all we need to do as follows: 

```python
model = ...  # create the original model

layer_name = 'my_layer'
intermediate_layer_model = keras.Model(inputs=model.input,
                                       outputs=model.get_layer(layer_name).output)
intermediate_output = intermediate_layer_model(data)
```

So, here we get two models, the `intermediate_layer_model` is the sub-model of its parent model. And they're independent as well. Likewise, if we get the **output feature maps** of the parent model (or base model), and **do some operation** with it and get some output feature maps from this operation, then we can also impute **this output feature maps** to the parent model.

```python

input = tf.keras.Input(shape=(size,size,3))
model = tf.keras.applications.DenseNet121(input_tensor = input)

layer_name = ""conv1_block1"" # for example 
output_feat_maps = SomeOperationLayer()(model.get_layer(layer_name).output)  

# assume, they're able to add up
base = Add()([model.output, output_feat_maps])

# bind all 
imputed_model = tf.keras.Model(inputs=[model.input], outputs=base)
```
So, in this way we have one modified model. It's quite easy with functional API. All the `keras` imagenet models are written with functional API (mostly). In model subclassing API, we can use these models. My concern here is, what to do if we need the intermediate output feature maps of these functional API models' inside `call` function. 

```python
class Subclass(tf.keras.Model): 
    def __init__(self, dim):
         super(Subclass, self).__init__()
         self.dim = dim
         self.base = DenseNet121(input_shape=self.dim)

         # building new model with the desired output layer of base model 
         self.mid_layer_model = tf.keras.Model(self.base.inputs, 
							         self.base.get_layer(layer_name).output)

    def call(self, inputs):
         # forward with base model 
         x = self.base(inputs)

         # forward with mid_layer_model 
         mid_feat = self.mid_layer_model(inputs)

         # do some op with it 
         mid_x = SomeOperationLayer()(mid_feat)
         
         # assume, they're able to add up
         out = tf.keras.layers.add([x, mid_x])

         return out 
```

The issue is, here we've **two models** and it's not desirable. Here we simply want the intermediate output feature maps of the base model. What can we do to get one modified subclasses model? Creating a new model with desired intermediate out works but by the same time, we repeat the same operation i.e the base model and its subset model. Maybe I'm missing something obvious, please add up something. "
47537,tf.keras.backend.in_train_phase / tf.keras.backend.in_test_phase not documented,"Tensorflow 2.4.1

tf.keras.backend.in_train_phase / tf.keras.backend.in_test_phase not documented. 

Reference tf 2.4.1 docs here on the tf.keras.backend lib.  You will see neither function appears.

https://www.tensorflow.org/api_docs/python/tf/keras/backend"
47536,Native C++ Tensorflow Lite project build failing on Raspberry Pi,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Following [this guide](https://www.tensorflow.org/lite/guide/build_cmake) to build from source
- TensorFlow version: TensorFlow Lite
- Python version: N/A, trying to create a C++ project
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): N/A, using CMake 3.20.0-rc2
- GCC/Compiler version (if compiling from source): arm-linux-gnueabihf 8.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Right now, I'm trying to use the TensorFlow C++ API. I am able to build the library using cmake without any fatal errors using this [this guide](https://www.tensorflow.org/lite/guide/build_cmake). However, when I try to build the minimal script (code provided) and run the make command (CMakeLists provided), I get a fatal error (traceback provided). 

**Any other info / logs**
Commands ran to build TensorFlow using CMake:
```
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
mkdir tflite_build
cd tflite_build
cmake ../tensorflow_src/tensorflow/lite -DCMAKE_BUILD_TYPE=Debug -DTFLITE_ENABLE_XNNPACK=OFF
cmake --build . -j3
```

Commands ran to build project using CMake:
```
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Debug ..
cmake --build . -j
```

`CMakeLists.txt`:
```
cmake_minimum_required(VERSION 3.13)
project(minimal C CXX)

set(TENSORFLOW_SOURCE_DIR ""/home/pi/tensorflow_src"")

add_subdirectory(
  ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite""
  ""${CMAKE_CURRENT_BINARY_DIR}/tensorflow-lite""
  EXCLUDE_FROM_ALL
)

set(CMAKE_CXX_STANDARD 11)
add_executable(minimal
  minimal.c
)
target_link_libraries(minimal
  tensorflow-lite
  ${CMAKE_DL_LIBS}
)
```

`minimal.c`:
```
#include <cstdio>
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/optional_debug_tools.h""

// This is an example that is minimal to read a model
// from disk and perform inference. There is no data being loaded
// that is up to you to add as a user.
//
// NOTE: Do not add any dependencies to this that cannot be built with
// the minimal makefile. This example must remain trivial to build with
// the minimal build tool.
//
// Usage: minimal <tflite model>

#define TFLITE_MINIMAL_CHECK(x)                              \
  if (!(x)) {                                                \
    fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \
    exit(1);                                                 \
  }

int main(int argc, char* argv[]) {
  if (argc != 2) {
    fprintf(stderr, ""minimal <tflite model>\n"");
    return 1;
  }
  const char* filename = argv[1];

  // Load model
  std::unique_ptr<tflite::FlatBufferModel> model =
      tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);

  // Build the interpreter with the InterpreterBuilder.
  // Note: all Interpreters should be built with the InterpreterBuilder,
  // which allocates memory for the Intrepter and does various set up
  // tasks so that the Interpreter can read the provided model.
  tflite::ops::builtin::BuiltinOpResolver resolver;
  tflite::InterpreterBuilder builder(*model, resolver);
  std::unique_ptr<tflite::Interpreter> interpreter;
  builder(&interpreter);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);

  // Allocate tensor buffers.
  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);
  printf(""=== Pre-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());

  // Fill input buffers
  // TODO(user): Insert code to fill input tensors.
  // Note: The buffer of the input tensor with index `i` of type T can
  // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`

  // Run inference
  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);
  printf(""\n\n=== Post-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());

  // Read output buffers
  // TODO(user): Insert getting data out code.
  // Note: The buffer of the output tensor with index `i` of type T can
  // be accessed with `T* output = interpreter->typed_output_tensor<T>(i);`

  return 0;
}
```

Traceback:
```
Consolidate compiler generated dependencies of target absl_spinlock_wait
Consolidate compiler generated dependencies of target absl_log_severity
Consolidate compiler generated dependencies of target pthreadpool
Consolidate compiler generated dependencies of target absl_dynamic_annotations
Consolidate compiler generated dependencies of target absl_city
Consolidate compiler generated dependencies of target absl_int128
Consolidate compiler generated dependencies of target absl_time_zone
Consolidate compiler generated dependencies of target farmhash
Consolidate compiler generated dependencies of target absl_civil_time
Consolidate compiler generated dependencies of target flatbuffers
Consolidate compiler generated dependencies of target fft2d_fftsg
Consolidate compiler generated dependencies of target ruy
[  0%] Built target absl_spinlock_wait
[  0%] Built target absl_log_severity
[  0%] Built target absl_dynamic_annotations
[  1%] Built target absl_city
[  1%] Built target pthreadpool
[  1%] Built target farmhash
[  1%] Built target absl_int128
[  1%] Built target absl_civil_time
[  3%] Built target flatbuffers
[  3%] Built target fft2d_fftsg
[  3%] Built target absl_time_zone
Consolidate compiler generated dependencies of target clog
Consolidate compiler generated dependencies of target absl_raw_logging_internal
Consolidate compiler generated dependencies of target fft2d_fftsg2d
[  3%] Built target clog
[  3%] Built target absl_raw_logging_internal
[  3%] Built target fft2d_fftsg2d
Consolidate compiler generated dependencies of target cpuinfo
[  7%] Built target ruy
Consolidate compiler generated dependencies of target absl_throw_delegate
Consolidate compiler generated dependencies of target absl_debugging_internal
Consolidate compiler generated dependencies of target absl_strings_internal
Consolidate compiler generated dependencies of target absl_base
Consolidate compiler generated dependencies of target absl_bad_optional_access
Consolidate compiler generated dependencies of target absl_bad_variant_access
[  7%] Built target absl_throw_delegate
[  9%] Built target cpuinfo
[  9%] Built target absl_strings_internal
[  9%] Built target absl_debugging_internal
[  9%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_base.dir/internal/cycleclock.cc.o
[  9%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_base.dir/internal/sysinfo.cc.o
[  9%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_base.dir/internal/spinlock.cc.o
[  9%] Built target absl_bad_optional_access
[  9%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_base.dir/internal/thread_identity.cc.o
[  9%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_base.dir/internal/unscaledcycleclock.cc.o
[  9%] Built target absl_bad_variant_access
Consolidate compiler generated dependencies of target absl_stacktrace
[ 10%] Built target absl_stacktrace
Scanning dependencies of target XNNPACK
Consolidate compiler generated dependencies of target XNNPACK
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-dwconv/gen/up8x9-minmax-neon-mul16.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-dwconv/gen/up16x9-minmax-neon-mul16.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-dwconv/gen/up24x9-minmax-neon-mul16.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-dwconv/gen/up32x9-minmax-neon-mul16.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8-minmax-neon-mlal-lane.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/4x8-minmax-neon-mlal-lane.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/2x16-minmax-neon-mlal-lane.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x16-minmax-neon-mlal-lane.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-igemm/gen/1x8-minmax-neon-mlal-lane.c.o
[ 10%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-igemm/gen/4x8-minmax-neon-mlal-lane.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-igemm/gen/1x16-minmax-neon-mlal-lane.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-requantization/precise-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-igemm/gen/2x16-minmax-neon-mlal-lane.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-requantization/fp32-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-requantization/q31-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-avgpool/9p8x-minmax-neon-c8.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-avgpool/9x-minmax-neon-c8.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-dwconv/up8x9-minmax-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-gavgpool/7x-minmax-neon-c8.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-gavgpool/7p7x-minmax-neon-c8.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-gemm/4x8-minmax-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-gemm/8x8-minmax-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-requantization/precise-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-igemm/8x8-minmax-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-igemm/4x8-minmax-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-requantization/fp32-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-requantization/q31-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/u8-maxpool/9p8x-minmax-neon-c16.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qu8-vadd/minmax-neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/u8-rmax/neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-fill/neon.c.o
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/u8-clamp/neon-x64.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-packx/x4-neon-st4.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-pad/neon.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-zip/x3-neon.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-unpool/neon.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-zip/x2-neon.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-zip/x4-neon.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x8-zip/x2-neon.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x32-zip/xm-neon.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x8-zip/x3-neon.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x8-zip/x4-neon.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/x8-zip/xm-neon.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundne-neon-addsub.c.o
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c: In function xnn_qs8_gavgpool_minmax_ukernel_7p7x__neon_c24_acc2:
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c:437:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
     ^~~~~~~~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c:437:51: error: incompatible type for argument 1 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                   ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
               ~~~~~~~~~~^~~
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundd-neon-addsub.c.o
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c:437:77: error: incompatible type for argument 2 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
                              ~~~~~~~~~~^~~
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundu-neon-addsub.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundd-neon-cvt.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundz-neon-addsub.c.o
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundu-neon-cvt.c.o
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c: In function xnn_qs8_gavgpool_minmax_ukernel_7x__neon_c32_acc2:
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:261:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
     ^~~~~~~~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:261:51: error: incompatible type for argument 1 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                   ^~~~~~~~~~~~~~~~~~~~~~~~
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundz-neon-cvt.c.o
make[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8322: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c24-acc2.c.o] Error 1
make[2]: *** Waiting for unfinished jobs....
[ 14%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/sigmoid-neon-frac-p9-p10-nr1recps.c.o
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c: In function xnn_qs8_gavgpool_minmax_ukernel_7p7x__neon_c16_acc2:
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
               ~~~~~~~~~~^~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:261:77: error: incompatible type for argument 2 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
                              ~~~~~~~~~~^~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:262:51: error: incompatible type for argument 1 of vcombine_s16
     int8x16_t voutGHIJKLMNOPQRSTUV = vcombine_s16(vqmovn_s16(vaccGHIJKLMN), vqmovn_s16(vaccOPQRSTUV));
                                                   ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
               ~~~~~~~~~~^~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:262:77: error: incompatible type for argument 2 of vcombine_s16
     int8x16_t voutGHIJKLMNOPQRSTUV = vcombine_s16(vqmovn_s16(vaccGHIJKLMN), vqmovn_s16(vaccOPQRSTUV));
                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
                              ~~~~~~~~~~^~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c:294:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
     ^~~~~~~~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c:294:51: error: incompatible type for argument 1 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                   ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
               ~~~~~~~~~~^~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c:294:77: error: incompatible type for argument 2 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
                              ~~~~~~~~~~^~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c: In function xnn_qs8_gavgpool_minmax_ukernel_7x__neon_c24_acc2:
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c: In function xnn_qs8_gavgpool_minmax_ukernel_7x__neon_c16_acc2:
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c:174:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
     ^~~~~~~~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c:174:51: error: incompatible type for argument 1 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                   ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
               ~~~~~~~~~~^~~
make[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8280: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c32-acc2.c.o] Error 1
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c:218:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
     ^~~~~~~~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c:174:77: error: incompatible type for argument 2 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c:218:51: error: incompatible type for argument 1 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                   ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
                              ~~~~~~~~~~^~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
               ~~~~~~~~~~^~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c:218:77: error: incompatible type for argument 2 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
                              ~~~~~~~~~~^~~
make[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8308: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c16-acc2.c.o] Error 1
make[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8252: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c16-acc2.c.o] Error 1
make[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8266: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7x-minmax-neon-c24-acc2.c.o] Error 1
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c: In function xnn_qs8_gavgpool_minmax_ukernel_7p7x__neon_c32_acc2:
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:518:5: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
     ^~~~~~~~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:518:51: error: incompatible type for argument 1 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                   ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
               ~~~~~~~~~~^~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:518:77: error: incompatible type for argument 2 of vcombine_s16
     int8x16_t vout0123456789ABCDEF = vcombine_s16(vqmovn_s16(vacc01234567), vqmovn_s16(vacc89ABCDEF));
                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
                              ~~~~~~~~~~^~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:519:51: error: incompatible type for argument 1 of vcombine_s16
     int8x16_t voutGHIJKLMNOPQRSTUV = vcombine_s16(vqmovn_s16(vaccGHIJKLMN), vqmovn_s16(vaccOPQRSTUV));
                                                   ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:25: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
               ~~~~~~~~~~^~~
/home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:519:77: error: incompatible type for argument 2 of vcombine_s16
     int8x16_t voutGHIJKLMNOPQRSTUV = vcombine_s16(vqmovn_s16(vaccGHIJKLMN), vqmovn_s16(vaccOPQRSTUV));
                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/pi/.../cpp/build/xnnpack/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c:12:
/usr/lib/gcc/arm-linux-gnueabihf/8/include/arm_neon.h:7141:40: note: expected int16x4_t but argument is of type int8x8_t
 vcombine_s16 (int16x4_t __a, int16x4_t __b)
                              ~~~~~~~~~~^~~
make[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:8336: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gavgpool/gen/7p7x-minmax-neon-c32-acc2.c.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:14888: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 14%] Linking CXX static library libabsl_base.a
[ 14%] Built target absl_base
make: *** [Makefile:156: all] Error 2
```"
47535,"Error grabbing variables after a forward pass using autograph, dropout and GRUCell","**System information**
- TensorFlow version: 2.4.0

**Describe the current behavior**
```
TypeError: '<' not supported between instances of 'FuncGraph' and '_WeakReferencableClass'

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)

ValueError: Error processing property '_dropout_mask_cache' of <ContextValueCache at 0x7fa31138ab10>
```

**Describe the expected behavior**
the variable method can safely be called after performing a forward pass with autograph and dropout.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow

class GRU(tf.Module):
  def __init__(self):
    super(GRU, self).__init__()
    self.cell = tf.keras.layers.GRUCell(units=2, dropout=0.1, recurrent_dropout=0.1)

  @tf.function
  def bad_infer(self, inputs, states):
    o, h = self.cell(inputs=inputs, states=states, training=True)
    return o, h

  def good_infer(self, inputs, states):
    o, h = self.cell(inputs=inputs, states=states, training=True)
    return o, h

gru = GRU()
inputs = tf.ones((1, 2))
states = gru.cell.get_initial_state(inputs)
targets = tf.ones(1, 2)

gru.good_infer(inputs=inputs, states=states)
gru.variables

gru.bad_infer(inputs=inputs, states=states)
gru.variables
```

[colab notebook](https://colab.research.google.com/drive/1kEjy5l8r5GbAKCPFtgFdHDLmVYKob_YY?usp=sharing)

guessing @qlzh727 may know the issue
"
47534,Dataset not ending when created inside predict_step function.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.6
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.9 

**Describe the current behavior**
When overriding the `predict_step` of a `tf.keras.Model` and creating a `Dataset` inside, it will not end.

**Describe the expected behavior**
`Dataset` should end.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1lKF1KxUdUDjJq_DTfle5HZITWIcICgdG?usp=sharing
"
47533,tf.device breaks when using tf.data.Dataset.from_generator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2 / 8.0.4
- GPU model and memory: GeForce RTX 3090 24GB RAM


**Describe the current behavior**
When using `tf.device` inside a generator which is used for dataset creation (using `tf.data.Dataset.from_generator`), the device passed to `tf.device` is not guaranteed to be used.

**Describe the expected behavior**
Tensors created from operations within the scope of `tf.device` should be placed on the given device, unless the operations don't support that device. Anyhow, tensor placement should be deterministic.

**Standalone code to reproduce the issue**
```
import tensorflow as tf

def gen():
    with tf.device(""CPU""):
        i = 0
        for i in tf.range(5000):
            i = tf.add(i, 1)
            print(i.device)
            yield i

d = tf.data.Dataset.from_generator(gen, output_types=(tf.int32))

for i in d:
    print(i)
```
Sample output of the above code:
```
...
/job:localhost/replica:0/task:0/device:GPU:0
tf.Tensor(4993, shape=(), dtype=int32)
/job:localhost/replica:0/task:0/device:CPU:0
tf.Tensor(4994, shape=(), dtype=int32)
/job:localhost/replica:0/task:0/device:GPU:0
tf.Tensor(4995, shape=(), dtype=int32)
/job:localhost/replica:0/task:0/device:CPU:0
tf.Tensor(4996, shape=(), dtype=int32)
/job:localhost/replica:0/task:0/device:GPU:0
tf.Tensor(4997, shape=(), dtype=int32
...
```
In some cases, the above code will also throw a RuntimeError: ""Exiting device scope without proper scope nesting"". I'll discuss why this happens later.

The following sample code **does not** reproduce the bug (which is expected, as discussed below):
(I moved the `tf.device` call inside the loop).
```
import tensorflow as tf

def gen():
    i = 0
    for i in tf.range(5000):
        with tf.device(""CPU""):
            i = tf.add(i, 1)
            print(i.device)
            yield i

d = tf.data.Dataset.from_generator(gen, output_types=(tf.int32))

for i in d:
    print(i)
```


**Detailed explanation of cause**
The issue is caused by the interaction of the implementations of `tf.device` and `tf.data`.
`tf.device` is a context manager. On entrance, it sets the `device_name` to the given device, and on exit - it restores it to its previous value. However, the `device_name` is a thread-local variable, stored in the context's thread_local_data. You can see the implementation [here](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/eager/context.py#L1749).
The problem is that, due to the multi-threaded nature of `tf.data`, the python generator used by our dataset might run from different threads. I think it is only guaranteed to run on the same thread until the next `yield` statement (I didn't make sure, but it seems like it).
Thus, if we call `tf.device`, then call `yield`, and then resume execution on a different thread - our context's thread local data would contain a different `device_name`.

In the second code snippet I provided the issue does not reproduce, because each time we return from a `yield` statement, we set the device scope again.

The RuntimeError I mentioned above, stems from [here](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/eager/context.py#L1797), and it happens when the context manager exits with a different thread, rather than the one it entered with."
47532,working with sparse input,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) colab
- TensorFlow installed from (source or binary): colab
- TensorFlow version (use command below): 2.4.1
- Python version: 3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
Sparse input works with dense and dropout layers as of v2.4.1. However, when you stack dense with dropout, it throws an error. If you try to reshape sparse input, you also get an error.

**Describe the expected behavior**
Sparse input works just like dense input.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

see [gist](https://colab.research.google.com/gist/hoondy/74b5c6cca796d0229941868b43c66592/git210303.ipynb)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

spawn from #25980"
47531,Unable to access resources using tf.data.experimental.service,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.19.160-2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.7 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
tf resources used in mapped functions are not available to workers when using `tf.data.experimental.service.distribute` to process a dataset. When the dataset is iterated over locally, this does not produce an error.

**Describe the expected behavior**
The expected behavior would be that locally run calls to `Dataset.map` and ones which use `tf.data.experimental.service.distribute` work equally.

**Standalone code to reproduce the issue**
```
import tensorflow as tf

keys = 'abcdefghijklmnopqrstuvwxyz'
table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(list(keys), range(len(keys))), -1)

ds = tf.data.Dataset.from_tensor_slices([['a', 'b'], ['c', 'd'], ['e','f']])
ds = ds.map(lambda x: table.lookup(x))

# When True, this produces an error, but works when set to False
USE_DATA_SERVICE = True
if USE_DATA_SERVICE:
  # Set up data service
  dispatcher = tf.data.experimental.service.DispatchServer()
  dispatcher_address = dispatcher.target.split(""://"")[1]
  workers = [
    tf.data.experimental.service.WorkerServer(
      tf.data.experimental.service.WorkerConfig(
          dispatcher_address=dispatcher_address)) for _ in range(2)
  ]

  processing_mode = ""parallel_epochs""
  ds = ds.apply(tf.data.experimental.service.distribute(
    processing_mode=processing_mode, service=dispatcher.target))

ds.__iter__().get_next()
```

**Other info / logs** Include any logs or source code that would be helpful to
```
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   2112       ctx.executor = executor_new
-> 2113       yield
   2114     finally:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    732           output_types=self._flat_output_types,
--> 733           output_shapes=self._flat_output_shapes)
    734 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py in iterator_get_next(iterator, output_types, output_shapes, name)
   2578     except _core._NotOkStatusException as e:
-> 2579       _ops.raise_from_not_ok_status(e, name)
   2580     except _core._FallbackException:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6861   # pylint: disable=protected-access
-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)
   6863   # pylint: enable=protected-access

/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

NotFoundError: Failed to get element: Container localhost does not exist. (Could not find resource: localhost/28)
	 [[{{node None_Lookup/LookupTableFindV2}}]] [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-2-e1a0821191f4> in <module>
     22     processing_mode=processing_mode, service=dispatcher.target))
     23 
---> 24 ds.__iter__().get_next()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in get_next(self)
    798 
    799   def get_next(self):
--> 800     return self._next_internal()
    801 
    802   def get_next_as_optional(self):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    737         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    738       except AttributeError:
--> 739         return structure.from_compatible_tensor_list(self._element_spec, ret)
    740 
    741   @property

/opt/conda/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)
    128                 value = type()
    129             try:
--> 130                 self.gen.throw(type, value, traceback)
    131             except StopIteration as exc:
    132                 # Suppress StopIteration *unless* it's the same exception that

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   2114     finally:
   2115       ctx.executor = executor_old
-> 2116       executor_new.wait()
   2117 
   2118 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/executor.py in wait(self)
     67   def wait(self):
     68     """"""Waits for ops dispatched in this executor to finish.""""""
---> 69     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     70 
     71   def clear_error(self):

NotFoundError: Failed to get element: Container localhost does not exist. (Could not find resource: localhost/28)
	 [[{{node None_Lookup/LookupTableFindV2}}]]
```
"
47530,Failed to get convolution algorithm [CUDA 11.0 - cuDNN 8.0.4 - Python 3.8],"**System information**
- Windows 10
- PC
- TensorFlow installed from (source or binary): via pip
- TensorFlow version: 2.4.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: Cuda 11.0 | cuDNN 8.0.4
- GPU model and memory:
RTX 2060 
6GB


**Describe the problem**
I Coded a CNN in Tensorflow on CPU it works without any problems, then i wanted more performance so i tryd the GPU version , after 2 days of figuring out wich cuda bla bla

now i have no idea to fix this , this is my first Neuronal Network and i only wanted to Check is a Cat or a Dog on the Picture.

i hope someone can help me

i got everytime the following error :
> 2021-03-03 15:23:58.929966: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
> 2021-03-03 15:23:58.930097: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
> 2021-03-03 15:23:58.930188: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 2021-03-03 15:23:59.186498: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
> 2021-03-03 15:23:59.186672: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
> 2021-03-03 15:23:59.186834: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
> 2021-03-03 15:23:59.189687: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
> 2021-03-03 15:23:59.190360: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
> 2021-03-03 15:23:59.190471: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.


**Any other info / logs**
the traindata paths and labes are in a SQL database and the sql manger build the full path
i testet it on CPU -> works without Problems

my Programcode itself (Pastebin):
https://pastebin.com/cNeGByBm
The Full Log :
https://pastebin.com/fe3D9Ubk
"
47529,Estimator#export_saved_model section links to irrelevant guide or guide missing relevant section,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_saved_model
https://www.tensorflow.org/guide/saved_model#savedmodels_from_estimators

## Description of issue (what needs changing):

### The api_docs link has a link to the guide/saved_model section. The section referenced does not exist in the guide, nor does any mention of the estimator method export_saved_model.

"
47528,Error while Compile to the model into TFLite model,"I use it in google Colab

[Here is notebook link 1](https://colab.research.google.com/drive/1voMaXutl7pOcybTZKwtjSjU_5aqncBdp?usp=sharing)
[Here is notebook link 2](https://nbviewer.jupyter.org/github/samyon7/Convert_TFLite_Problem/blob/master/Untitled243.ipynb)

Was that because I used the branching model?"
47526,Need for Normalization of Pixel Data can be mentioned in Basic Image Classification Tutorial,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/tutorials/keras/classification#preprocess_the_data

## Description of issue (what needs changing): 
The Tutorial states 

> Scale these values to a range of 0 to 1 before feeding them to the neural network model.

It would be clear to the beginners if the reason for **`Normalization`** has been specified because there has been questions out of confusion. Example [Question 1](https://datascience.stackexchange.com/questions/39929/why-normalize-when-all-features-are-on-the-same-scale), [Question 2](https://stackoverflow.com/questions/20486700/why-do-we-always-divide-rgb-values-by-255) and [Question 3](https://datascience.stackexchange.com/questions/29958/when-inputting-image-rgb-values-to-mlp-should-i-divide-by-255)"
47525,[TFLite] Segmentation fault on inference due to int overflow in im2col,"Hello,

The convolution kernel uses im2col to optimize some convolutions and [uses](https://github.com/tensorflow/tensorflow/blob/7677422a56f18c10e3827e29dc8da8a7b23fbede/tensorflow/lite/kernels/conv.cc#L453) an intermediate buffer of size `batches*output_height*output_width*(input_depth*filter_height*filter_width)` for that.

If the size of this buffer overflows an `int`, there is a risk of integer overflow in the `Im2col` [method](https://github.com/tensorflow/tensorflow/blob/7677422a56f18c10e3827e29dc8da8a7b23fbede/tensorflow/lite/kernels/internal/optimized/im2col_utils.h#L247) which uses `int` as type for indexing.

Such large buffers can quickly happen in super resolution models with large output resolution. The script bellow is based on the [How to generate super resolution images using TensorFlow Lite on Android](https://blog.tensorflow.org/2020/12/how-to-generate-super-resolution-images-using-tensorflow-lite-on-android.html) tutorial and illustrates the problem. It uses a larger 700x700 input image to generate a 2800x2800 output image.

The last convolution needs an im2col buffer of size `1*2800*2800*(32*3*3) = 2 257 920 000` which is larger than the 32-bit `int` used for indexing in `Im2col` and result in a `Segmentation fault` with the following stack trace due to some of the indexes going negative:
```
#0  __memmove_avx_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:366
#1  0x00007fffae09dc86 in void tflite::optimized_ops::Im2col<signed char>(tflite::ConvParams const&, int, int, int const*, int, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char*) () from ~/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#2  0x00007fffae0c7cdd in tflite::optimized_ops::HybridConvPerChannel(tflite::ConvParams const&, float*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::RuntimeShape const&, signed char*, float const*, int*, tflite::RuntimeShape const&, int*, int*, bool*, tflite::CpuBackendContext*) () from ~/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#3  0x00007fffae0c861f in TfLiteStatus tflite::ops::builtin::conv::EvalHybridPerChannel<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*) ()
```

The test was done using the latest `tf-nightly-2.5.0.dev20210303`.

```python
import os
import tensorflow as tf
import tensorflow_hub as hub

input_shape = (1, 700, 700, 3)

model = hub.load(""https://tfhub.dev/captain-pool/esrgan-tf2/1"")
concrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
concrete_func.inputs[0].set_shape(input_shape)

converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

interpreter = tf.lite.Interpreter(
    model_content=tflite_model, num_threads=os.cpu_count()
)
interpreter.allocate_tensors()
interpreter.invoke()
```

Forcing the usage of the reference kernels which don't use the im2col optimization circumvent the problem.

One way to solve it would be to use larger integers for indexing or disabling the im2col optimization when the intermediate buffer is too large.

Thibaut"
47524,Increase GPU memory usage in tensorflow 2.4.1 versus tensorflow 2.3.2,"**System information**
- OS Platform and Distribution: Ubuntu 18.04.5
- TensorFlow installed from: tensorflow dockers _tensorflow:2.3.2-gpu_ and _tensorflow:2.4.1-gpu_
- TensorFlow version (use command below): 2.3.2 and 2.4.1
- Python version: 3.6.9
- CUDA/cuDNN version: for **2.3.2**: CUDA 10.1.243 CUDNN 7.6.4.38-1 for **2.4.1**: CUDA 11.0.3 CUDNN 8.0.4.30-1
- GPU model and memory: GeForce RTX 2070 SUPER, 7979MiB

**Describe the current behavior**
After upgrading to tensorflow 2.4.1 GPU memory consumption is increased. Training with the same batch size as for tensorflow 2.3.2 results in GPU running out of memory. 

The script below works fine when run with
`sudo docker run --rm -v ${PWD}:/data --gpus all tensorflow/tensorflow:2.3.2-gpu python3 /data/script.py`

but crashes when run with
`sudo docker run --rm -v ${PWD}:/data --gpus all tensorflow/tensorflow:2.4.1-gpu python3 /data/script.py`

When using tensorflow 2.4.1 the batch size must be lower to 3 for the script to run without crashing.

**Describe the expected behavior**
The script below should be possible to run with tensorflow 2.4.1 without crashing due to GPU running out of memory.

**Standalone code to reproduce the issue**
script.py: 
```
import numpy as np
import tensorflow as tf
from tensorflow.python.keras import Input
from tensorflow.python.keras.layers import Conv3D, Activation
from tensorflow.python.keras.models import Model


for gpu in tf.config.experimental.list_physical_devices('GPU'):
    tf.config.experimental.set_memory_growth(gpu, True)


def get_model():
    inputs = Input((100, 100, 100, 1), name='image3d')
    x = Conv3D(80, kernel_size=(3, 3, 3), padding='valid', activation='relu')(inputs)

    class_scores = Conv3D(1, kernel_size=(1, 1, 1))(x)
    out = Activation('softmax', name='out')(class_scores)

    return Model(inputs=inputs, outputs=out, name='net')


batch_size = 8

model = get_model()
x = np.empty((batch_size, 100, 100, 100, 1), dtype=np.float32)
y = np.empty((batch_size, 98, 98, 98, 1), dtype=np.float32)

model.compile('sgd', 'mse')
model.fit(x, y, batch_size=batch_size)
```

works fine when run with
`sudo docker run --rm -v ${PWD}:/data --gpus all tensorflow/tensorflow:2.3.2-gpu python3 /data/script.py`

but crashes when run with
`sudo docker run --rm -v ${PWD}:/data --gpus all tensorflow/tensorflow:2.4.1-gpu python3 /data/script.py`

When using tensorflow 2.4.1 the batch size must be lower to 3 for the script to run without crashing.

**Other info / logs** 
output when executing 
`sudo docker run --rm -v ${PWD}:/data --gpus all tensorflow/tensorflow:2.4.1-gpu python3 /data/script.py`
[trace-2.4.1.log](https://github.com/tensorflow/tensorflow/files/6075341/trace-2.4.1.log)
"
47523,getting error while adding metadata to object detection tflite model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or GitHub SHA, if built from source): tf-nightly

### 2. Code

Reference Link- https://www.tensorflow.org/lite/convert/metadata

I am trying to add metadata into my tflite model as metadata is required for the android app. I am trying the following method but getting error.

```bash
python ./metadata_writer_for_image_classifier.py \
    --model_file=./model_without_metadata/mobilenet_v1_0.75_160_quantized.tflite \
    --label_file=./model_without_metadata/labels.txt \
    --export_directory=model_with_metadata
```

Google-Colab Link for refernce-
https://colab.research.google.com/drive/1MkPxvYWVWdygFYxDk8U-jXDBhEjsSnMa#scrollTo=q3AtK7ErmrVl

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

```bash
File ""metadata_writer_for_image_classifier.py"", line 176, in main
    ""The model info for, {0}, is not defined yet."".format(model_basename))
ValueError: The model info for, , is not defined yet.
```

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47522,Model Accuracy Goes Static When Arbitrary Lines Are Added,"**System information**
- I have written custom code, repo link below.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 Home, build 19041.804**
- TensorFlow installed from (source or binary): source, **Anaconda3**
- TensorFlow version (use command below): **2.1.0**
- Python version: **3.7.7**
- CUDA/cuDNN version: 10.1
- GPU model and memory: EVGA Geforce GTX 1080 SC, 8GB

**Describe the current behavior**
Adding arbitrary comment and print lines above the model training function causes accuracy to become static and not improve past 0.5024.  Removing the lines allows it to train correctly.  

Please see these screenshots for a better explanation:
https://imgur.com/a/weV5mgL

**Describe the expected behavior**
Model should train correctly regardless of arbitrary lines.

**Standalone code to reproduce the issue**
repo: https://github.com/Cnotey/TF_Issue_003

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

I have a hard time believing this is a TF issue, or an issue with my code.  It seems like this is a compatibility issue somehow.
"
47520,TensorBoard callback: step is always 0 for batch_steps_per_second,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-51814-gb08c055e840 2.5.0-dev20210301
- Python version: 3.8.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
The current tf-nightly (for 2.5.0) has  a new parameter  `write_steps_per_second` to tf.keras.callbacks.TensorBoard.
If that is used with `update_freq` > 0, the resulting batch_steps_per_second all have `0` as step value, which makes the graph unreadable in tensorboard.

**Describe the expected behavior**
The step number is the corresponding training step

**Standalone code to reproduce the issue**
```
import shutil
import glob
import tensorflow as tf
from tensorflow.core.util import event_pb2

workdir = 'tmp_workdir/'
shutil.rmtree(workdir, ignore_errors=True)

x = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [0, 1, 1, 0]

inputs = tf.keras.Input(shape=(2,))
outputs = tf.keras.layers.Dense(units=1, activation='sigmoid')(inputs)
model = tf.keras.Model(inputs=inputs, outputs=outputs)

model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())

model.fit(x=x, y=y, batch_size=2, epochs=2, callbacks=[
    tf.keras.callbacks.TensorBoard(log_dir=workdir, write_steps_per_second=True, update_freq=1,
                            profile_batch=0)])

max_step = 0
for record in tf.data.TFRecordDataset(glob.glob(workdir + 'train/events.out*')[0]):
    event = event_pb2.Event.FromString(record.numpy())
    if event.HasField('summary') and event.summary.value[0].tag == 'batch_steps_per_second':
        max_step = max(max_step, event.step)

if max_step == 0:
    print(""FAILURE"")
else:
    print(""OK"")
```

**Other info / logs** 
The step number 0 comes from passing `step.value()` instead of `step` in _push_writer. There is a TODO saying `# TODO(b/151339474): Fix deadlock when not using .value() here.` above that line - I suppose the details are in some internal google bug database.

But as the `batch_steps_per_second` event is only written during training, this can easily be fixed by explicitly passing  self._train_step  as step when writing it.

I'll add a pull request."
47517,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"**System information**
- windows 10
- TensorFlow installed using pip
- tensorflow-2.0.0
- Python 3.6.2

Hey, I've been trying to use [this](https://www.youtube.com/watch?v=wypVcNIH6D4) tutorial to get a simple chatbot running but am running into this issue when I try compiling main.py file:

`Traceback (most recent call last):
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 6, in <module>
    import tflearn
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tflearn\__init__.py"", line 4, in <module>
    import tensorflow.compat.v1 as tf
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Micha\anaconda3\envs\chatbot\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.`

"
47513,Missing documentation for bucket_by_sequence_length api,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/data/experimental/ops/grouping.py
Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):
I think `bucket_by_sequence_length` is a very important api, but it's lack of documentation example makes it usage quiet restricted to some users.
### Clear description
This API is very useful in NLP where we can batch the data efficiently keeping in mind the memory constraints that is involved. 
This API can pad the data based on the longest sequence in that particular batch in place of padding the complete dataset with a constant number.
For example, why should someone use this method? How is it useful?
As I said above it can be used for large dataset as well as small as it will reduce the padding of the dataset.
### Correct links

Is the link to the source code correct?
https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/data/experimental/ops/grouping.py
### Parameters defined
Yes
Are all parameters defined and formatted correctly?
Yes
### Returns defined
Yes
Are return values defined?
Yes
### Raises listed and defined
Yes
Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises
Yes
### Usage example
There is only one example in the nightly build that doesn't involve all the parameter such as `padded_shapes , padding_values, pad_to_bucket_boundary, no_padding, drop_remainder`. Basically it doesn't involve a close to real scenario.
Is there a usage example?
Yes, there is one example.

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?
I will be willing to contribute and I can have the PR ready within a day, as I use this API, if the other contributors agree to it. If you accept this issue, you can assign it to me.


Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
47509,Scores are always inverse of each other in person_detection example with OV7670 Camera,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Version: 20H2, OS Build: 19042.804
- TensorFlow installed from (source or binary): Arduino IDE 1.8.13 Library Manager
- Tensorflow version (commit SHA if source): 2.4.0-ALPHA (Arduino IDE 1.8.13 Library Manager)
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano 33 BLE Sense

**Describe the problem**
When using the person_detection example with the OV7670 camera (not from ArduCAM) and the edited image provider below, person_score and no_person_score are always inverse of each other. For example, 1 and -1, -8 and 8, etc. Inference does not fail.

**Please provide the exact sequence of commands/steps when you ran into the problem**
- Made a copy of person_detection example from Arduino_TensorFlowLite library
- Deleted all SparkFun Edge and HM01B0 related files
- Edited arduino_image_provider.cpp to the following for use with OV7670
- No other files were edited

```
/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.

  Licensed under the Apache License, Version 2.0 (the ""License"");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an ""AS IS"" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
  ==============================================================================*/

#include ""image_provider.h""

#if defined(ARDUINO) && !defined(ARDUINO_ARDUINO_NANO33BLE)
#define ARDUINO_EXCLUDE_CODE
#endif  // defined(ARDUINO) && !defined(ARDUINO_ARDUINO_NANO33BLE)

#ifndef ARDUINO_EXCLUDE_CODE

//Ov767X Library
#include <Arduino_OV767X.h>

#define CAMERA_WIDTH 160
#define CAMERA_HEIGHT 120

int bytesPerFrame;

byte data[CAMERA_WIDTH * CAMERA_HEIGHT]; //unsigned
//int8_t data[CAMERA_WIDTH * CAMERA_HEIGHT]; //signed

#define START_Y 0
#define END_Y 96
#define START_X 0
#define END_X 96

//Get the camera module ready
TfLiteStatus InitCamera(tflite::ErrorReporter* error_reporter) {
  TF_LITE_REPORT_ERROR(error_reporter, ""Attempting to start 0V767X"");
  if (!Camera.begin(QQVGA, GRAYSCALE, 5)) {
    TF_LITE_REPORT_ERROR(error_reporter, ""Failed to initialize OV767X"");
    delay(1000);
    return kTfLiteError;
  }
  bytesPerFrame = Camera.width() * Camera.height() * Camera.bytesPerPixel();
  delay(100);
  return kTfLiteOk;
}

//Begin the capture and wait for it to finish
TfLiteStatus PerformCapture(tflite::ErrorReporter* error_reporter) {
  TF_LITE_REPORT_ERROR(error_reporter, ""Starting capture"");
  // Start capture
  Camera.readFrame(data);
  TF_LITE_REPORT_ERROR(error_reporter, ""Frame read"");
  delay(50);
  return kTfLiteOk;
}

TfLiteStatus ReadData(tflite::ErrorReporter* error_reporter) {
  //  TF_LITE_REPORT_ERROR(error_reporter, ""%d bytes per frame from OV767X"", bytesPerFrame);
  //  delayMicroseconds(15);
  //  TF_LITE_REPORT_ERROR(error_reporter, ""Exiting read data function"");
  return kTfLiteOk;
}

//Crop frame
TfLiteStatus DecodeAndProcessImage(tflite::ErrorReporter* error_reporter,
                                   int image_width, int image_height,
                                   int8_t* image_data) {
  TF_LITE_REPORT_ERROR(error_reporter, ""Cropping image"");

  //Crop image
  uint32_t index = 0;
  for (int y = START_Y; y < END_Y; y++) {
    for (int x = START_X; x < END_X; x++) {
      image_data[index++] = static_cast<int8_t>(data[(y * image_width) + x] - 128); //Unsigned to signed conversion
    }
  }

  TF_LITE_REPORT_ERROR(error_reporter, ""Image processed"");
  return kTfLiteOk;
}

// Get an image from the camera module
TfLiteStatus GetImage(tflite::ErrorReporter* error_reporter, int image_width,
                      int image_height, int channels, int8_t* image_data) {
  static bool g_is_camera_initialized = false;
  if (!g_is_camera_initialized) {
    TfLiteStatus init_status = InitCamera(error_reporter);
    if (init_status != kTfLiteOk) {
      TF_LITE_REPORT_ERROR(error_reporter, ""InitCamera failed"");
      return init_status;
    }
    g_is_camera_initialized = true;
  }

  TfLiteStatus capture_status = PerformCapture(error_reporter);
  if (capture_status != kTfLiteOk) {
    TF_LITE_REPORT_ERROR(error_reporter, ""PerformCapture failed"");
    return capture_status;
  }

  TfLiteStatus read_data_status = ReadData(error_reporter);
  if (read_data_status != kTfLiteOk) {
    TF_LITE_REPORT_ERROR(error_reporter, ""ReadData failed"");
    return read_data_status;
  }

  TfLiteStatus decode_status = DecodeAndProcessImage(
                                 error_reporter, image_width, image_height, image_data);
  if (decode_status != kTfLiteOk) {
    TF_LITE_REPORT_ERROR(error_reporter, ""DecodeAndProcessImage failed"");
    return decode_status;
  }

  return kTfLiteOk;
}

#endif  // ARDUINO_EXCLUDE_CODE
```
Did I write the image provider incorrectly, or is this an issue with the original example as well?"
47505,install_pip_packages.sh specifies incompatible version combo for pylint and astroid,"This PR https://github.com/tensorflow/tensorflow/pull/46046 introduces what seems to be an incompatible version combination for pylint and astroid.

see this line : https://github.com/tensorflow/tensorflow/pull/46046/files#diff-c8d38bb153d3e8107f198a493594dba5a8725176bbc6069304aec7fcec2fabc6R89

The nightly TF ROCm build (which uses the `install_pip_packages.sh` script when creating the docker container), ran into the following error
```
ERROR: Cannot install astroid==2.5 and pylint==2.6.2 because these package versions have conflicting dependencies.

The conflict is caused by:
    The user requested astroid==2.5
    pylint 2.6.2 depends on astroid<2.5 and >=2.4.0

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies
```

The complete build log can be seen here : http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly/630/console

/cc @bhack "
47504,LSTMs with XNNPACK: ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.4.1


**Provide the text output from tflite_convert**

The conversion is actually correct, the problem occurs when trying to use the model with a delegate that requires static-sized tensors. I'm currently testing with XNNPACK Delegate on the tflite benchmark whose output is:
```
STARTING!
Log parameter values verbosely: [0]
Graph: [encoder.tflite]
Enable op profiling: [1]
Use xnnpack: [1]
Loaded model encoder.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
Failed to apply XNNPACK delegate.
Benchmarking failed.
```

**Standalone code to reproduce the issue** 
The problem can be reproduced by using the profiler as following with the model attached:
```
./linux_x86-64_benchmark_model --graph=encoder.tflite --enable_op_profiling=true --use_xnnpack=true
```

https://drive.google.com/file/d/1xZOAS67A0szLJEqdPTC8vtISZXvKcbO6/view?usp=sharing

**Any other info / logs**


I converted an encoder-decoder model for machine translation using two separate tflite models respectively for the encoder and the decoder.

The encoder uses BiLSTM and generates the error in the title. The same error doesn't occur if I use unroll=True, but this is unfeasible for me as I need input sequences of length 100. In this case, I don't get the error but then the usage of RAM explodes.

The decoder, on the other hand, uses input of length 1 and LSTMCell instead of LSTM, and it doesn't produce the error.

Is there a way to make the operation static-length without using unroll?
I have already tried to set the full shape of the input and output shapes of every layer.
Moreover, it is a keras model but I wrap it into a tf.function for which I set the full input shape in the signature, and then I do the conversion from the concrete function but also this doesn't help to get a static size tensor. Using from_saved_model or from_keras_model doesn't help either."
47503,"`tf.distribute.MirroredStrategy(),` but getting unbalance GPU usage","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes, with reference.  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.6.9
- CUDA/cuDNN version: CUDA 11 and cuDNN 8
- GPU model and memory: 3 GPU's 32GB each (same GPU model)


**Describe the current behavior**
Trying to use `tf.distribute.MirroredStrategy(),` but getting unbalance GPU usage. The 1st GPU is used most of the time and other 2 GPU's are seldom used for the same code, same training instance. 

**Describe the expected behavior**
get almost uniform usage of all 3 GPU (for better performance).

**Standalone code to reproduce the issue**
https://github.com/rrklearn2020/keras-retinanet 

Running directly from the repository with tf.distribute.MirroredStrategy():
`keras_retinanet/bin/train_v2.py coco /path/to/MS/COCO`


I also tried the ""tf.data.Dataset.from_generator"" by adding below lines (currently not added to train_v2.py due to error)
```
train_generator=tf.data.Dataset.from_generator(train_generator)
validation_generator=tf.data.Dataset.from_generator(validation_generator)
```
But getting an error as shown below.
```
raise TypeError(""`generator` must be callable."")
TypeError: `generator` must be callable.
```


 Please guide me to get almost uniform usage of all 3 GPU (for better performance).

"
47502,Error when using quantization aware training with custom activation function,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Custom code

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10

- TensorFlow installed from (source or binary):
Anaconda Navigator

- TensorFlow version (use command below):
tensorflow-gpu 2.3.0

- Python version:
3.7.9

- CUDA/cuDNN version:
cudatoolkit 10.1.243
cudnn 7.6.5

- GPU model and memory:
GTX 1070 8GB

**Describe the current behavior**
I have a model with a custom activation function, ReLU6, as defined below. When using tfmot.quantization.keras.quantize_model I get the following error output:

> ValueError: Unable to clone model. This generally happens if you used custom Keras layers or objects in your model. Please specify them via `quantize_scope` for your calls to `quantize_model` and `quantize_apply`.

When simply using 'relu' I don't get this error.

Activation function:
`def _relu6(x):
    return activations.relu(x, max_value=6.0)`

Usage:
`x = Activation(_relu6)(x)`


**Describe the expected behavior**
I need to be able to quantize a model with the relu6 activation function. I am not sure how to approach this problem.

**Standalone code to reproduce the issue**
Creating any model with the custom activation function as above seems to yields this error.

**Other info / logs** Include any logs or source code that would be helpful to

"
47501,Tensorflow failed to build due to error C2039 C3861 on windows with MSVC,"Hi All,

Tensorflow failed to build due to error C2039 C3861 on windows with MSVC. It can be reproduced on latest master branch commit. Could you please help take a look at this issue? Thanks in advance!

Environment
Tensorflow(latest master branch revision from github) + VS 2019 + Windows Server 2016

Note: 
build.log in build.zip attachment.
[build.zip](https://github.com/tensorflow/tensorflow/files/6067995/build.zip)

tensorflow_set_bazel_version.patch in tensorflow_set_bazel_version.zip
[tensorflow_set_bazel_version.zip](https://github.com/tensorflow/tensorflow/files/6068012/tensorflow_set_bazel_version.zip)

Repro steps:
1. Open x64 native tools command prompt for VS2019
2. git clone https://github.com/tensorflow/tensorflow F:\gitP\tensorflow\tensorflow
3. git submodule init
4. git submodule update --recursive
5. Apply tensorflow_set_bazel_version.patch in attachment tensorflow_set_bazel_version.zip
6. cd F:\gitP\tensorflow\tensorflow
7. pip3 install six numpy wheel
8. pip3 install keras_applications==1.0.6 --no-deps
9. pip3 install keras_preprocessing==1.0.5 --no-deps
10. set PATH=F:\gitP\tensorflow\tensorflow\..\tools;%path%
11. set PATH=F:\gitP\tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
12. yes """" 2>nul | python ./configure.py
13. set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC
14. set BAZEL_VC_FULL_VERSION=14.28.29333
15. bazel --output_user_root F:\bazelTemp build --config=opt --subcommands //tensorflow/tools/pip_package:build_pip_package

Actual result:
SUBCOMMAND: # //tensorflow/core/platform:error [action 'Compiling tensorflow/core/platform/error.cc', configuration: b0b8ead46223da3f6648bba4bb6404871eb4a1826bc4e150e3d35eeda28b2f74]
SUBCOMMAND: # //tensorflow/core/platform:error [action 'Linking tensorflow/core/platform/error.lib', configuration: b0b8ead46223da3f6648bba4bb6404871eb4a1826bc4e150e3d35eeda28b2f74]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\include\complex(674): error C2039: 'copysign': is not a member of '`global namespace''
C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\include\complex(674): error C3861: 'copysign': identifier not found
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 9803.671s, Critical Path: 681.63s
INFO: 9288 processes: 9288 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
##[debug] Command #6 exited with code [1].
##[error] Detected error code [1]."
47498,-1073741795,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



ImportError: Traceback (most recent call last):
  File ""C:\Users\Junaid\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Junaid\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Junaid\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Junaid\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Junaid\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

- first I run the command line pip install tensorflow==2.2.0
- then while importing the tensorflow it throws the above error

![image](https://user-images.githubusercontent.com/72540582/109615543-079dfe80-7b5a-11eb-82d9-9e33fd4b8909.png)

"
47497,rtx3090 tf2.4.1 error with single machine multi-card,"strategy is tf.distribute.MirroredStrategy, I train the model on two cards, it reports that
F tensorflow/stream_executor/cuda/cuda_driver.cc:1289] Check failed: context != nullptr success should entail non-null context.
how can I solve it?
"
47496,zero gradient for higher-order derivatives when using tf.function and tf.scan,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f
- Python version: 3.0

**Describe the current behavior**
When using tf.function decorator on functions involving tf.scan, the second-order derivative goes to zero.

**Describe the expected behavior**
I would expect the gradient not to be zero when tf.function is used.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1M5-ua3LXgrc8QpwtcwY5tEsQ1gSkY9TD"
47495,keras lstm layer produces different results in tf 1 and tf 2 due to different default activation,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- TensorFlow version:
1. TF 1.0: v1.15.4-39-g3db52be7be8 1.15.5
2. TF 2.0: v2.4.0-49-g85c8b2a817f 2.4.1


**Describe the current behavior**
The LSTM layer in tf 1.0 uses ""hard_sigmoid"" as the default recurrent activation: https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/keras/layers/recurrent.py#L2489

while it uses  ""sigmoid"" in tf 2.0: https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/recurrent_v2.py#L1077

As a result, when I try to execute the same code (without specifying recurrent_activation flag explicitly) in tf 1 and 2, I get slightly different results and took quite a while to find the cause.

**Describe the expected behavior**
Same code should produce the same result in both versions. Or some prompt should be given for easier debugging.
"
47494,https://www.tensorflow.org/guide/estimators - HTTP Error 404: Page not Found,"Looks like this shows a 404 to me at the time of writing
https://www.tensorflow.org/guide/estimators
"
47493,Cosine annealing learning rate scheduler with minimum learning rate boundary,"**System information**
- TensorFlow version (you are using): Tensorflow 2.x
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
In the nightly build, we have API called [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay)  which schedule decaying learning rate. However, we cannot set the range of learning rate incremental decay. For example, this current API allow us to change learning from alpha then decrease it in cosine manner and beyond. My proposal is to decrease the learning rate incrementally in cosine manner from alpha to beta.
note - alpha : initial learning rate , beta : final learning rate

**Will this change the current api? How?**
It could be implemented in tf.keras.optimizers.schedules.CosineDecay or implemented in different API name.

**Who will benefit with this feature?**
Anyone who want to use cosine annealing learning rate and worry about vanishing gradient issue.

"
47492,GPUOptions C++ free(): invalid pointer,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary): N/A
- TensorFlow version (use command below):1.14
- Python version:3.6
- Bazel version (if compiling from source):0.24.1
- GCC/Compiler version (if compiling from source):7.5.0
- CUDA/cuDNN version:10.0/7.4.2
- GPU model and memory: 2070 8G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
c++
int main()
{   
    tensorflow::Session* session;
    tensorflow::Status status;
    ConfigProto configProto;
    GPUOptions gpuOptions;

    gpuOptions.set_per_process_gpu_memory_fraction(0.5);
    configProto.set_allocated_gpu_options(&gpuOptions);
}

it will report free(): invalid pointer after the code is finished.

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47488,Untraced function warning and Model parsing failure for Keras TCN Regressor (TF Lite),"**The error:** TF Lite converter throws an untraced function warning when trying to convert a temporal CNN (built using the widely used Keras TCN library: https://github.com/philipperemy/keras-tcn ), and throws in model parsing error when trying to do post-training quantization

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): Pip (python 3.8.8)
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.3.0 (TF Base), 2.4.0 (TF-GPU)

### 2. Code
Part 1, converting pretrained TF model to TF Lite Model:
```
import os
os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ[""CUDA_VISIBLE_DEVICES""]=""1""
from tensorflow.python.keras.backend import set_session
import tensorflow as tf
config = tf.compat.v1.ConfigProto() 
config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU
config.log_device_placement = True  # to log device placement (on which device the operation ran)
sess = tf.compat.v1.Session(config=config)
set_session(sess)  # set this TensorFlow session as the default
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from gtda.time_series import SlidingWindow
import matplotlib.pyplot as plt
from math import atan2, pi, sqrt

from tensorflow.keras.layers import Dense, MaxPooling1D, Flatten
from tensorflow.keras import Input, Model
from tensorflow.keras.callbacks import ModelCheckpoint
from tcn import TCN, tcn_full_summary
from tensorflow.keras.models import load_model

model = load_model('best_joint_new.hdf5',custom_objects={'TCN':TCN})
converter = tf.lite.TFLiteConverter.from_keras_model(model)
model_no_quant_tflite = converter.convert()
open('best_joint.tflite', ""wb"").write(model_no_quant_tflite)
```
Part 2: Post training quantization

trainX is a nX200X6 matrix of floating point values, n can be any integer.
```
def representative_dataset():
    for i in range(trainX.shape[0]):
        yield ([trainX[i]])
```
Same converter used as before.
```
# Set the optimization flag.
converter.optimizations = [tf.lite.Optimize.DEFAULT]
# Enforce integer only quantization
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
# Provide a representative dataset to ensure we quantize correctly.
converter.representative_dataset = representative_dataset
model_quant_tflite = converter.convert()
# Save the model to disk
open('best_joint_quant.tflite', ""wb"").write(model_quant_tflite)
```

### 3. Failure after conversion
Part 1 (conversion successful but produces the following warning)
```
WARNING:absl:Found untraced functions such as residual_block_0_layer_call_and_return_conditional_losses, residual_block_0_layer_call_fn, residual_block_1_layer_call_and_return_conditional_losses, residual_block_1_layer_call_fn, residual_block_2_layer_call_and_return_conditional_losses while saving (showing 5 of 325). These functions will not be directly callable after loading.
WARNING:absl:Found untraced functions such as residual_block_0_layer_call_and_return_conditional_losses, residual_block_0_layer_call_fn, residual_block_1_layer_call_and_return_conditional_losses, residual_block_1_layer_call_fn, residual_block_2_layer_call_and_return_conditional_losses while saving (showing 5 of 325). These functions will not be directly callable after loading.
```
Part 2 (quantization fails)
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/anaconda3/envs/tflite/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py in __init__(self, model_content)
     57       self._calibrator = (
---> 58           _calibration_wrapper.CalibrationWrapper(model_content))
     59     except Exception as e:

TypeError: pybind11::init(): factory function returned nullptr

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-7-f34a9c965790> in <module>
      7 # Provide a representative dataset to ensure we quantize correctly.
      8 converter.representative_dataset = representative_dataset
----> 9 model_quant_tflite = converter.convert()
     10 # Save the model to disk
     11 open('best_joint_quant.tflite', ""wb"").write(model_quant_tflite)

~/anaconda3/envs/tflite/lib/python3.8/site-packages/tensorflow/lite/python/lite.py in convert(self)
    871           graph=frozen_func.graph)
    872 
--> 873     return super(TFLiteKerasModelConverterV2,
    874                  self).convert(graph_def, input_tensors, output_tensors)
    875 

~/anaconda3/envs/tflite/lib/python3.8/site-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)
    630     calibrate_and_quantize, flags = quant_mode.quantizer_flags()
    631     if calibrate_and_quantize:
--> 632       result = self._calibrate_quantize_model(result, **flags)
    633 
    634     flags_modify_model_io_type = quant_mode.flags_modify_model_io_type(

~/anaconda3/envs/tflite/lib/python3.8/site-packages/tensorflow/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type, activations_type, allow_float)
    447     # Add intermediate tensors to the model if needed.
    448     result = _calibrator.add_intermediate_tensors(result)
--> 449     calibrate_quantize = _calibrator.Calibrator(result)
    450     if self._experimental_calibrate_only or self._experimental_new_quantizer:
    451       calibrated = calibrate_quantize.calibrate(

~/anaconda3/envs/tflite/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py in __init__(self, model_content)
     58           _calibration_wrapper.CalibrationWrapper(model_content))
     59     except Exception as e:
---> 60       raise ValueError(""Failed to parse the model: %s."" % e)
     61     if not self._calibrator:
     62       raise ValueError(""Failed to parse the model."")

ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.

```

### 4. The Master Model Architecture
Input: 6 channels of 200-sample floating point values
Output: 2 scalars (floating point)

![Screenshot from 2021-03-01 17-57-39](https://user-images.githubusercontent.com/20337475/109585138-c7863f80-7ab7-11eb-9268-cfc13a7a2fa2.png)

### 5. Link to original HDF5 model:
https://drive.google.com/file/d/1GFRgMUkIVatSsUWgnzee_jjeF6D3l-A-/view?usp=sharing "
47487,Can't save/load a Keras model's optimizer weights when using SavedModel format,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): No
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.10

**Describe the current behavior**
When saving a keras model in TensorFlow SavedModel format using `model.save` optimizer weights are not saved, even when specifying `include_optimizer=True` in the `model.save` call  and `compiled=True` in the `tf.keras.models.load_model` call. This is crucial for continuing training from a checkpoint with adaptive optimizers.
Looks like the optimizer object gets saved and loaded along with its parameters. Only the weights are missing.

Weights **are** saved as expected when saving in `h5` format.

**Describe the expected behavior**
Optimizer weights should be saved when specifying `include_optimizer=True`.

**Standalone code to reproduce the issue**
Reproduced in [this Colab](https://colab.research.google.com/drive/1bEZif1c6xzHdcGO4SQQMB9q0DMDo8jbI?usp=sharing)
"
47486,Building tensorflow_cc target (cpu-only) from scratch with xla failing (TF1.15.5),"### System information

-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
-   **TensorFlow installed from (source or binary)**: source
-   **TensorFlow version (use command below)**: 1.15
-   **Python version**: 3.6
-   **Bazel version (if compiling from source)**: 0.26.1
-   **GCC/Compiler version (if compiling from source)**: 7.5.0 (set using --action env GCC_HOST_COMPILER_PATH)

### Source code / logs
Execution platform: @bazel_tools//platforms:host_platform
tensorflow/compiler/xla/service/generic_transfer_manager.cc: In member function 'virtual tensorflow::Status xla::GenericTransferManager::WriteSingleTupleIndexTable(stream_executor::Stream*, absl::Span<const stream_executor::DeviceMemoryBase>, const xla::Shape&, stream_executor::DeviceMemoryBase*)':
tensorflow/compiler/xla/service/generic_transfer_manager.cc:56:47: error: expected ',' before '{' token
   stream->ThenDoHostCallback([element_pointers{std::move(element_pointers)}]() {
                                               ^
tensorflow/compiler/xla/service/generic_transfer_manager.cc:56:47: error: expected identifier before '{' token
Target //tensorflow:tensorflow_cc failed to build
INFO: Elapsed time: 1827.953s, Critical Path: 236.32s
INFO: 5317 processes: 5317 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully"
47485,flatbuffers download:,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 10:
- TensorFlow installed from (source or binary):Binary
- TensorFlow version: 3 (Lite)
- Python version: 3.9
- Installed using virtualenv? pip? conda?: cloned it. from github

**Describe the problem**
 flatbuffers download is failing.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

$ make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test
SYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc
syswgetrc = C:\Program Files (x86)\GnuWin32/etc/wgetrc
--2021-03-01 22:41:11--  http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip
Resolving mirror.tensorflow.org... 172.217.6.48
Connecting to mirror.tensorflow.org|172.217.6.48|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1760478 (1.7M) [application/zip]
Saving to: `/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip'

     0K .......... .......... .......... .......... ..........  2%  526K 3s
    50K .......... .......... .......... .......... ..........  5%  855K 2s
   100K .......... .......... .......... .......... ..........  8% 1.33M 2s
   150K .......... .......... .......... .......... .......... 11% 2.38M 2s
   200K .......... .......... .......... .......... .......... 14%  612K 2s
   250K .......... .......... .......... .......... .......... 17% 63.8M 1s
   300K .......... .......... .......... .......... .......... 20%  444K 2s
   350K .......... .......... .......... .......... .......... 23% 16.7M 1s
   400K .......... .......... .......... .......... .......... 26% 1.03M 1s
   450K .......... .......... .......... .......... .......... 29% 1.62M 1s
   500K .......... .......... .......... .......... .......... 31%  340K 1s
   550K .......... .......... .......... .......... .......... 34% 89.0M 1s
   600K .......... .......... .......... .......... .......... 37%  106M 1s
   650K .......... .......... .......... .......... .......... 40% 1.37M 1s
   700K .......... .......... .......... .......... .......... 43% 4.26M 1s
   750K .......... .......... .......... .......... .......... 46%  697K 1s
   800K .......... .......... .......... .......... .......... 49% 1.48M 1s
   850K .......... .......... .......... .......... .......... 52% 1.04M 1s
   900K .......... .......... .......... .......... .......... 55% 1.81M 1s
   950K .......... .......... .......... .......... .......... 58% 1.79M 1s
  1000K .......... .......... .......... .......... .......... 61% 1.73M 1s
  1050K .......... .......... .......... .......... .......... 63% 1.44M 1s
  1100K .......... .......... .......... .......... .......... 66% 1.31M 0s
  1150K .......... .......... .......... .......... .......... 69% 1.58M 0s
  1200K .......... .......... .......... .......... .......... 72% 1.40M 0s
  1250K .......... .......... .......... .......... .......... 75% 1.70M 0s
  1300K .......... .......... .......... .......... .......... 78%  525K 0s
  1350K .......... .......... .......... .......... .......... 81% 71.7M 0s
  1400K .......... .......... .......... .......... .......... 84% 2.75M 0s
  1450K .......... .......... .......... .......... .......... 87% 1.14M 0s
  1500K .......... .......... .......... .......... .......... 90%  888K 0s
  1550K .......... .......... .......... .......... .......... 93% 6.11M 0s
  1600K .......... .......... .......... .......... .......... 95% 1.66M 0s
  1650K .......... .......... .......... .......... .......... 98%  892K 0s
  1700K .......... .........                                  100% 2.13M=1.4s

2021-03-01 22:41:13 (1.20 MB/s) - `/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip' saved [1760478/1760478]

tensorflow/lite/micro/tools/make/bash_helpers.sh: line 29: /tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: No such file or directory
tensorflow/lite/micro/tools/make/Makefile:525: *** Something went wrong with the flatbuffers download: Bad checksum. Expected: aa9adc93eb9b33fa1a2a90969e48baee, Got: .  Stop.

The file dca12522a9f9e37f126ab925fd385c807ab4f84e.zip does exist under c:/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip
"
47479,Saving model in TF 2.4,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 11.1 cudnn 8.0.5
- GPU model and memory: A100, 40GB

**Describe the current behavior**

Saving a model in TF 2.4 omits sub-layers and generates warning messages about ""untraced function.""

**Describe the expected behavior**

No warning and the sub-layers will be directly callable after loading.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

EXAMPLE:
```
import tensorflow as tf
import numpy as np

print(tf.version.GIT_VERSION, tf.version.VERSION) ### v2.4.0-49-g85c8b2a817f 2.4.1

'''
simple autoencoder example
'''

class Encoder(tf.keras.layers.Layer):
    def __init__(self, name=""encoder"", **kwargs):
        super(Encoder, self).__init__(name=name, **kwargs)
        self.enc = tf.keras.layers.Dense(units=20, activation=""relu"", name='encoder_layer')

    def call(self, inputs):
        x = inputs
        x = self.enc(x)
        return x

class Decoder(tf.keras.layers.Layer):
    def __init__(self, name=""decoder"", **kwargs):
        super(Decoder, self).__init__(name=name, **kwargs)
        self.dec = tf.keras.layers.Dense(units=1000, activation=""sigmoid"", name='decoder_layer')

    def call(self, inputs):
        x = inputs
        x = self.dec(x)
        return x
    
class AutoEncoder(tf.keras.Model):
    def __init__(self, name=""autoencoder"", **kwargs):
        super(AutoEncoder, self).__init__(name=name, **kwargs)
        self.encoder = Encoder()
        self.decoder = Decoder()
    
    def call(self, inputs):
        x = self.encoder(inputs)
        x = self.decoder(x)
        return x

    def train_step(self, data):
        x, y = data
        with tf.GradientTape() as tape:
            y_pred = self(x, training=True)
            mse_loss = tf.reduce_mean(tf.reduce_sum(tf.square(y - y_pred), axis=-1), axis=-1) # mean squared error
        gradients = tape.gradient(mse_loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        return {""loss"": mse_loss}
    
'''
example run
'''
inp = np.random.rand(10000,1000)
ae = AutoEncoder()
ae.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3))
ae.fit(inp, inp)

'''
saving throws warning message ""Found untraced functions ... These functions will not be directly callable after loading.""
This is new in TF 2.4.
TF 2.3 did not have this issue.
'''
ae.save('tmp')
```
OUTPUT:
```
v2.4.0-49-g85c8b2a817f 2.4.1
313/313 [==============================] - 1s 1ms/step - loss: 83.3821
WARNING:absl:Found untraced functions such as encoder_layer_layer_call_and_return_conditional_losses, encoder_layer_layer_call_fn, decoder_layer_layer_call_and_return_conditional_losses, decoder_layer_layer_call_fn, encoder_layer_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.
WARNING:absl:Found untraced functions such as encoder_layer_layer_call_and_return_conditional_losses, encoder_layer_layer_call_fn, decoder_layer_layer_call_and_return_conditional_losses, decoder_layer_layer_call_fn, encoder_layer_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: tmp/assets
INFO:tensorflow:Assets written to: tmp/assets
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Spawn from issue #44541"
47478,Tensorflow Lite Custom Object detection Model Error in Android app,"Could you please help to solve this error?

I am testing a custom Object Detection model using TensorFlow Lite in Android App according to the documentation, but I have an error when the library tries to recognize an image.

I am using the Tensorflow lite sample app: `https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android`

**Using Task Library:** `https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector`

**dependency version implementation:** `'org.tensorflow:tensorflow-lite-task-vision:0.1.0'`

When this method is executed, this error is obtained::

**method**
` List<Detection> results = objectDetector.detect(TensorImage.fromBitmap(bitmap));`

error
```
Abort message: 'JNI DETECTED ERROR IN APPLICATION: JNI NewStringUTF called with pending exception java.lang.NoSuchMethodError: no static method Lorg/tensorflow/lite/support/label/Category;.create(Ljava/lang/String;Ljava/lang/String;F)Lorg/tensorflow/lite/support/label/Category;""
        at java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detectNative(long, java.nio.ByteBuffer, int, int, int) (ObjectDetector.java:-2)
        at java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detect(org.tensorflow.lite.support.image.TensorImage, org.tensorflow.lite.task.core.vision.ImageProcessingOptions) (ObjectDetector.java:312)
        at java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detect(org.tensorflow.lite.support.image.TensorImage) (ObjectDetector.java:292)
        at java.util.List org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(android.graphics.Bitmap) (TFLiteObjectDetectionAPIModel.java:87)
        at void org.tensorflow.lite.examples.detection.DetectorActivity$2.run() (DetectorActivity.java:187)
        at void android.os.Handler.handleCallback(android.os.Message) (Handler.java:938)
        at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:99)
        at void android.os.Looper.loop() (Looper.java:223)
        at void android.os.HandlerThread.run() (HandlerThread.java:67)
    
        in call to NewStringUTF
        from java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detectNative(long, java.nio.ByteBuffer, int, int, int)'

```
"
47475,Saving and loading a model using tf.keras and CategoricalCrossentropy with from_logits=True restores an incorrect model.,"Only incorrect when using CategoricalCrossentropy with from_logits=True and sigmoid outputs. Using from_logits=False and softmax outputs is fine, as is BinaryCrossenropy with from_logits=True and sigmoid outputs.

Big problem since CategoricalCrossentropy is most popular loss, and from_logits=True is recommended usage due to numerical stability.

Model is saved using ModelCheckpoint, val_accuracy, max. Trained using Adam.

Saved model loads successfully but predictions do not match the validation accuracy reported by tf when the model was saved, always much worse but does indicate partial training rather than random weights. 

Loaded model object reports correct from_logits value.

The first saved model usually gives correct result, the second save is often is wrong, third and beyond are always wrong.

Even a simple models displays this behavior.

Tested on stable and nightly tf, same result. Using tf.keras, not standalone keras."
47474,"""import tensorflow"" configures Python logging if tensorboard is not installed","**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7, Python 3.8 from MacPorts, Python venv
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary wheel `tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl` from PyPI (using `python -m pip install tensorflow`) 
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version:  N/A
- GPU model and memory: Radeon Pro 560X 4 GB, Intel UHD Graphics 630 1536 MB

**Describe the current behavior**

After uninstalling `tensorboard` from the Python virtual environment with `pip uninstall tensorboard`, executing `import tensorflow` at the prompt causes logging to be configured: in particular, a `StreamHandler` handler is added to the root logger.

As a result, in a large Python GUI (PyQt5) application, an `import tensorflow` at application startup time resulted in many log messages unrelated to tensorflow being printed to the console; without importing tensorflow, those log messages weren't visible. (Note that the application had set the level of the root logger to `logging.DEBUG`, and added its own root-level loggers, under the assumption that it would be the only part of the Python environment working with the configuration for the root-level logger.)

**Describe the expected behavior**

Following usual logging best practices (libraries should only emit log messages but not configure logging; log configuration should be left to the application using the library), `import tensorflow` does not affect the root logger.

**Standalone code to reproduce the issue**

Here's an interpreter session demonstrating the issue

```python
Python 3.8.8 (default, Feb 22 2021, 09:21:28) 
[Clang 12.0.0 (clang-1200.0.32.28)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import logging
>>> root_logger = logging.root
>>> root_logger.handlers
[]
>>> import tensorflow
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.summary API due to missing TensorBoard installation.
>>> root_logger.handlers  # expected an empty list
[<StreamHandler <stderr> (NOTSET)>]
```

**Additional information**

This looks like a shallow bug and an easy fix. Here's the troublesome code: https://github.com/tensorflow/tensorflow/blob/c023a829167aa4a233a36479cd338ef9784b6990/tensorflow/compat_template.__init__.py#L43

Diagnosis: because the Python logging system hadn't already been configured at the point of the `import tensorflow` execution, and because `tensorboard` wasn't available,`_logging.warning` was then called as above: that then implicitly configured logging by calling `logging.basicConfig`.

A simple solution would be to set up a logger in the normal way (`logger = logging.getLogger(__name__)`) and use `logger.warning` instead of `logging.warning`; that way, implicit configuration of the logging machinery is avoided.

We were able to work around this problem in our application by making sure that we configured logging (adding a null handler to the root logger) _before_ importing `tensorflow`, so that `logging.basicConfig` was no longer invoked.

Another workaround was to ensure that `tensorboard` was installed, but since we don't generally need the functionality provided by `tensorboard`, our packaging system leaves `tensorboard` out as a explicit dependency of `tensorflow`. If the recommendation is that `tensorboard` should be treated as a required dependency of `tensorflow`, that would be good to know."
47473,tf-nightly-gpu builds from February are crashing on 'tf.keras model.fit()' for Nvidia RTX 3090; January builds performance is slower than 2080ti on : 2Layer Bidirectional LSTM + 3 Dense Layers,"**System information**
OS Windows 10 Pro : 1909 18363.1379
CUDA TOOLKIT : 11.1
CUDNN: 8.1.0
Latest GPU Driver 461.72

GPU : MSI RTX 3090 Gaming X Trio
CPU i7 3930k 32GB ram

using Anaconda
python 3.7.9
tf-nightly-gpu 2.5.0.dev20210114
tf-nightly 2.5.0.dev20210114

**Describe the problem**

=> The above config allows me to run my code LSTM code in tensor flow (extract below):
However if i create an environment in Anaconda by cloning the above and pip --upgrade to a tf-nightly build more recent, including todays build
of 2.5.0.dev20210228 ... when the code reaches tf.model.fit .. it crashes out with no error reporting at all.

The reason why i tried upgrading the tf-nightly build : currently the model training time is about the same as on my RTX 2080ti and sometimes even slower which i thought was down to issues with the tf-nightly january Build and Ampere.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
The code can be run from CMD conda command prompt or from a terminal in Visual Studio Code and it crashes as soon as the model.fit line is reached.


**Any other info / logs**
code block:

model = tf.keras.models.Sequential([
#input shape equals term points in curve
tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True),input_shape=(n_days,n_cols)),
tf.keras.layers.Dropout(0.2),

tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
tf.keras.layers.Dropout(0.2),

tf.keras.layers.Dense(64, activation='relu',kernel_initializer=initializer),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Dropout(0.2),
    
tf.keras.layers.Dense(64, activation='relu',kernel_initializer=initializer, bias_initializer=output_bias),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Dropout(0.2),

tf.keras.layers.Dense(t_cols, activation='sigmoid', bias_initializer=output_bias)])
#list of keras metrics to compute in model
METRICS = [
keras.metrics.BinaryAccuracy(name='binary_accuracy'),
keras.metrics.Precision(name='precision'),
keras.metrics.Recall(name='recall'),
keras.metrics.AUC(name='auc'),
]

model.compile(optimizer=Adam(learning_rate=lng_rate),
loss='binary_crossentropy',
metrics=METRICS)

history=model.fit(
X_train,
Y_train,
epochs=training_epochs,
batch_size=training_batch_size,
verbose=1,
class_weight=label_weights[0],
validation_data=(X_validation,Y_validation))"
47470,Large overhead when py_function returns tuple of lists,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.8 (verified in 3.6.9 as well)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Given the code in the gist https://gist.github.com/kretes/02dc479ab7a63a8b5a559c5e7df89598 - I see that the version of the `py_function` that return a tuple of lists with data is introducing some large overhead on top of the actual operation. It takes ~10 seconds to finish the iteration in this code, while the other two versions (returning just tuple, or returning tuple of ndarrays) is done within ~2 secs.

**Describe the expected behavior**
There should be no difference between returning a tuple with data, or wrapping them up with one more nesting level.

**Standalone code to reproduce the issue**
https://gist.github.com/kretes/02dc479ab7a63a8b5a559c5e7df89598 - code and logs from run on my side. I verified this in multiple installations, the recent one in jupyter/tensorflow docker image"
47469,Tensorflow WARNING: tensorflow:Gradients do not exist for variables,"I am trying to implement the VQ-VAE using TensorFlow 2.X. 
I borrow the code of the VQ-VAE training from [this github][1] which is written in TF 2x. and I'm working on merging the Keras code of PixelCNN training & sampling from [this tutorial][2] to the VQ-VAE TF 2.X code.


For the training of VQ-VAE, I think it was fine. But when I want to get the quantization vectors from the trained VQ-VAE and use the quantization vectors to further train the PixelCNN. I always get a warning:

    

> WARNING:tensorflow:Gradients do not exist for variables
> ['conv2d_block/batch_normalization/gamma:0',
> 'conv2d_block/batch_normalization/beta:0',
> 'conv2d_block/conv2d/kernel:0', 'conv2d_block/conv2d/bias:0',
> 'conv2d_block_1/batch_normalization_1/gamma:0',
> 'conv2d_block_1/batch_normalization_1/beta:0',
> 'conv2d_block_1/conv2d_1/kernel:0', 'conv2d_block_1/conv2d_1/bias:0',
> 'conv2d_block_2/batch_normalization_2/gamma:0',
> 'conv2d_block_2/batch_normalization_2/beta:0',
> 'conv2d_block_2/conv2d_2/kernel:0', 'conv2d_block_2/conv2d_2/bias:0',
> 'conv2d_block_3/batch_normalization_3/gamma:0',
> 'conv2d_block_3/batch_normalization_3/beta:0',
> 'conv2d_block_3/conv2d_3/kernel:0', 'conv2d_block_3/conv2d_3/bias:0',
> 'conv2d_block_4/batch_normalization_4/gamma:0',
> 'conv2d_block_4/batch_normalization_4/beta:0',
> 'conv2d_block_4/conv2d_4/kernel:0', 'conv2d_block_4/conv2d_4/bias:0',
> 'conv2d_block_5/batch_normalization_5/gamma:0',
> 'conv2d_block_5/batch_normalization_5/beta:0',
> 'conv2d_block_5/conv2d_5/kernel:0', 'conv2d_block_5/conv2d_5/bias:0',
> 'conv2d_block_6/batch_normalization_6/gamma:0',
> 'conv2d_block_6/batch_normalization_6/beta:0',
> 'conv2d_block_6/conv2d_6/kernel:0', 'conv2d_block_6/conv2d_6/bias:0',
> 'conv2d_block_7/batch_normalization_7/gamma:0',
> 'conv2d_block_7/batch_normalization_7/beta:0',
> 'conv2d_block_7/conv2d_7/kernel:0', 'conv2d_block_7/conv2d_7/bias:0',
> 'conv2d_block_8/batch_normalization_8/gamma:0',
> 'conv2d_block_8/batch_normalization_8/beta:0',
> 'conv2d_block_8/conv2d_8/kernel:0', 'conv2d_block_8/conv2d_8/bias:0',
> 'conv2d_block_9/batch_normalization_9/gamma:0',
> 'conv2d_block_9/batch_normalization_9/beta:0',
> 'conv2d_block_9/conv2d_9/kernel:0', 'conv2d_block_9/conv2d_9/bias:0',
> 'conv2d_block_10/instance_normalization/scale:0',
> 'conv2d_block_10/instance_normalization/offset:0',
> 'conv2d_block_10/conv2d_10/kernel:0',
> 'conv2d_block_10/conv2d_10/bias:0',
> 'conv2d_block_11/instance_normalization_1/scale:0',
> 'conv2d_block_11/instance_normalization_1/offset:0',
> 'conv2d_block_11/conv2d_11/kernel:0',
> 'conv2d_block_11/conv2d_11/bias:0',
> 'conv2d_block_12/instance_normalization_2/scale:0',
> 'conv2d_block_12/instance_normalization_2/offset:0',
> 'conv2d_block_12/conv2d_12/kernel:0',
> 'conv2d_block_12/conv2d_12/bias:0',
> 'conv2d_block_13/instance_normalization_3/scale:0',
> 'conv2d_block_13/instance_normalization_3/offset:0',
> 'conv2d_block_13/conv2d_13/kernel:0',
> 'conv2d_block_13/conv2d_13/bias:0',
> 'conv2d_block_14/instance_normalization_4/scale:0',
> 'conv2d_block_14/instance_normalization_4/offset:0',
> 'conv2d_block_14/conv2d_14/kernel:0',
> 'conv2d_block_14/conv2d_14/bias:0',
> 'conv2d_block_15/instance_normalization_5/scale:0',
> 'conv2d_block_15/instance_normalization_5/offset:0',
> 'conv2d_block_15/conv2d_15/kernel:0',
> 'conv2d_block_15/conv2d_15/bias:0',
> 'conv2d_block_16/instance_normalization_6/scale:0',
> 'conv2d_block_16/instance_normalization_6/offset:0',
> 'conv2d_block_16/conv2d_16/kernel:0',
> 'conv2d_block_16/conv2d_16/bias:0',
> 'conv2d_block_17/instance_normalization_7/scale:0',
> 'conv2d_block_17/instance_normalization_7/offset:0',
> 'conv2d_block_17/conv2d_17/kernel:0',
> 'conv2d_block_17/conv2d_17/bias:0',
> 'conv2d_block_18/instance_normalization_8/scale:0',
> 'conv2d_block_18/instance_normalization_8/offset:0',
> 'conv2d_block_18/conv2d_18/kernel:0',
> 'conv2d_block_18/conv2d_18/bias:0',
> 'conv2d_block_19/conv2d_19/kernel:0',
> 'conv2d_block_19/conv2d_19/bias:0', 'conv2d_20/kernel:0',
> 'conv2d_20/bias:0', 'v_masked_conv_1/W_v:0', 'h_masked_conv_1/W_h:0',
> 'v_masked_conv_2/W_v:0', 'h_masked_conv_2/W_h:0',
> 'v_masked_conv_3/W_v:0', 'h_masked_conv_3/W_h:0',
> 'v_masked_conv_4/W_v:0', 'h_masked_conv_4/W_h:0',
> 'v_masked_conv_5/W_v:0', 'h_masked_conv_5/W_h:0',
> 'v_masked_conv_6/W_v:0', 'h_masked_conv_6/W_h:0',
> 'v_masked_conv_7/W_v:0', 'h_masked_conv_7/W_h:0',
> 'v_masked_conv_8/W_v:0', 'h_masked_conv_8/W_h:0',
> 'v_masked_conv_9/W_v:0', 'h_masked_conv_9/W_h:0',
> 'v_masked_conv_10/W_v:0', 'h_masked_conv_10/W_h:0',
> 'v_masked_conv_11/W_v:0', 'h_masked_conv_11/W_h:0',
> 'v_masked_conv_12/W_v:0', 'h_masked_conv_12/W_h:0'] when minimizing
> the loss.

  The following is what I have for my codes.

        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  
        x_train = (x_train[:200]/ 255.).astype(""float32"")
        x_test = (x_test[:200] / 255.).astype(""float32"")
        
        #training vqvae 
        model = Autoencoder(config)
        model.fit(x_train, config)
        _, vq_return = model.call(x_train, training=False)
        quantize = vq_return['quantize']

where the `fit` function is I newly defined for the Autoencoder model (actually it should be the VQ-VAE mode), the rests are the same as what it has in [the git][3]. 

For the PixelCNN part, I modified the above Keras tutorial to have Lambda layer as classes:

    class SamplingLayer(tf.keras.layers.Layer):
        def __init__(self, model, **kwargs):  
            super(SamplingLayer, self).__init__()
            self.model = model
        def call(self, encoding_indices, training=False):
            vq = self.model.vq
            return vq.quantize(encoding_indices)
    
    class CodesSampler(tf.keras.Model):
        def __init__(self, **kwargs):
            super(CodesSampler, self).__init__()
    
        def call(self, model, size, training=False):
            sampling_layer = SamplingLayer(model)
            indices = tf.keras.layers.Input(shape=(size, size), name='codes_sampler_inputs', dtype='int32')
            z_q = sampling_layer(indices)
            codes_sampler = tf.keras.Model(inputs=indices, outputs=z_q, name=""codes_sampler"")
            return codes_sampler
and the rest of Pixel CNN model is basically the same as the tutorial 

    '''Learning a prior over the latent space'''
    class GateLayer(tf.keras.layers.Layer):
        def __init__(self, **kwargs):
            super(GateLayer, self).__init__()
            # self.name = name
        def call(self, inputs):
            """"""Gated activations""""""
            x, y = tf.split(inputs, 2, axis=-1)
            return Kb.tanh(x) * Kb.sigmoid(y)
    
    
    class MaskedConv2D(tf.keras.layers.Layer):
        """"""Masked convolution""""""
        def __init__(self, kernel_size, out_dim, direction, mode, **kwargs):
            self.direction = direction     # Horizontal or vertical
            self.mode = mode               # Mask type ""a"" or ""b""
            self.kernel_size = kernel_size
            self.out_dim = out_dim
            super(MaskedConv2D, self).__init__(**kwargs)
        
        def build(self, input_shape):   
            filter_mid_y = self.kernel_size[0] // 2
            filter_mid_x = self.kernel_size[1] // 2        
            in_dim = int(input_shape[-1])
            w_shape = [self.kernel_size[0], self.kernel_size[1], in_dim, self.out_dim]
            mask_filter = np.ones(w_shape, dtype=np.float32)
            # Build the mask
            if self.direction == ""h"":
                mask_filter[filter_mid_y + 1:, :, :, :] = 0.
                mask_filter[filter_mid_y, filter_mid_x + 1:, :, :] = 0.
            elif self.direction == ""v"":
                if self.mode == 'a':
                    mask_filter[filter_mid_y:, :, :, :] = 0.
                elif self.mode == 'b':
                    mask_filter[filter_mid_y+1:, :, :, :] = 0.0
            if self.mode == 'a':
                mask_filter[filter_mid_y, filter_mid_x, :, :] = 0.0
            # Create convolution layer parameters with masked kernel
            self.W = mask_filter * self.add_weight(""W_{}"".format(self.direction), w_shape, trainable=True)
            self.b = self.add_weight(""v_b"", [self.out_dim,], trainable=True)
        
        def call(self, inputs):
            return tf.keras.backend.conv2d(inputs, self.W, strides=(1, 1)) + self.b
    
        
    def gated_masked_conv2d(v_stack_in, h_stack_in, out_dim, kernel, mask='b', residual=True, i=0):
        """"""Basic Gated-PixelCNN block. 
           This is an improvement over PixelRNN to avoid ""blind spots"", i.e. pixels missingt from the
           field of view. It works by having two parallel stacks, for the vertical and horizontal direction, 
           each being masked  to only see the appropriate context pixels.
        """"""
        kernel_size = (kernel // 2 + 1, kernel)
        padding = (kernel // 2, kernel // 2)
        v_gate = GateLayer(name=""v_gate_{}"".format(i))
        v_stack = tf.keras.layers.ZeroPadding2D(padding=padding, name=""v_pad_{}"".format(i))(v_stack_in)
        v_stack = MaskedConv2D(kernel_size, out_dim * 2, ""v"", mask, name=""v_masked_conv_{}"".format(i))(v_stack)
        v_stack = v_stack[:, :int(v_stack_in.get_shape()[-3]), :, :]
        v_stack_out = v_gate(v_stack)
        
        kernel_size = (1, kernel // 2 + 1)
        padding = (0, kernel // 2)
        h_gate = GateLayer(name=""h_gate_{}"".format(i))
        h_stack = tf.keras.layers.ZeroPadding2D(padding=padding, name=""h_pad_{}"".format(i))(h_stack_in)
        h_stack = MaskedConv2D(kernel_size, out_dim * 2, ""h"", mask, name=""h_masked_conv_{}"".format(i))(h_stack)
        h_stack = h_stack[:, :, :int(h_stack_in.get_shape()[-2]), :]
        h_stack_1 = tf.keras.layers.Conv2D(filters=out_dim * 2, kernel_size=1, strides=(1, 1), name=""v_to_h_{}"".format(i))(v_stack)
        h_stack_out = h_gate(h_stack + h_stack_1)
        
        h_stack_out =  tf.keras.layers.Conv2D(filters=out_dim, kernel_size=1, strides=(1, 1), name=""res_conv_{}"".format(i))(h_stack_out)
        if residual:
            h_stack_out += h_stack_in
        return v_stack_out, h_stack_out
    
    
    def build_pixelcnn(codes_sampler, k, size, num_layers, num_feature_maps=32):
        pixelcnn_prior_inputs = tf.keras.layers.Input(shape=(size, size), name='pixelcnn_prior_inputs', dtype=tf.int32)
        z_q = codes_sampler(pixelcnn_prior_inputs, size) # maps indices (z_train in the implementation) to the actual codebook
        
        v_stack_in, h_stack_in = z_q, z_q
        for i in range(num_layers):
            mask = 'b' if i > 0 else 'a'
            kernel_size = 3 if i > 0 else 7
            residual = True if i > 0 else False
            v_stack_in, h_stack_in = gated_masked_conv2d(v_stack_in, h_stack_in, num_feature_maps,
                                                         kernel=kernel_size, residual=residual, i=i + 1)
    
        fc1 = tf.keras.layers.Conv2D(filters=num_feature_maps, kernel_size=1, name=""fc1"")(h_stack_in)
        fc2 = tf.keras.layers.Conv2D(filters=k, kernel_size=1, name=""fc2"")(fc1) 
        # outputs logits for probabilities of codebook indices for each cell
    
        pixelcnn_prior = tf.keras.Model(inputs=pixelcnn_prior_inputs, outputs=fc2, name='pixelcnn-prior')
    
        # Distribution to sample from the pixelcnn
        dist = tfp.distributions.Categorical(logits=fc2)
        sampled = dist.sample()
        prior_sampler = tf.keras.Model(inputs=pixelcnn_prior_inputs, outputs=sampled, name='pixelcnn-prior-sampler')
        return pixelcnn_prior, prior_sampler
    
    
    ##%%time
    # Train the PixelCNN and monitor prediction accuracy
    def accuracy(y_true, y_pred):
        size = int(y_pred.get_shape()[-2])
        k = int(y_pred.get_shape()[-1])
        y_true = tf.reshape(y_true, (-1, size * size))
        y_pred = tf.reshape(y_pred, (-1, size * size, k))
        return Kb.cast(Kb.equal(y_true, Kb.cast(Kb.argmax(y_pred, axis=-1), Kb.floatx())), Kb.floatx())

The last is training PixelCNN using the quantized vectors:

    z_train = model.call(x_train, training=False)[1]['encoding_indices']
    pixelcnn_prior, prior_sampler = build_pixelcnn(codes_sampler, NUM_LATENT_K, SIZE, 
                                                   PIXELCNN_NUM_BLOCKS, PIXELCNN_NUM_FEATURE_MAPS)
    pixelcnn_prior.summary()
    pixelcnn_prior.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[accuracy],
                           optimizer=tf.keras.optimizers.Adam(PIXELCNN_LEARNING_RATE))
    prior_history = pixelcnn_prior.fit(z_train, z_train, epochs=PIXELCNN_NUM_EPOCHS, 
                                       batch_size=PIXELCNN_BATCH_SIZE, verbose=1)



It's also weird that when I plot the summary tables of the Keras tutorial and my modified TF2.X codes, there are different in: `codes_sampler` and `Non-trainable params: 8,064`.

The top is the summary of the Keras tutorial with most of the common parts omitted:

    Model: ""pixelcnn-prior""
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    pixelcnn_prior_inputs (InputLay [(None, 8, 8)]       0                                            
    __________________________________________________________________________________________________
    codes_sampler (Model)           (None, 8, 8, 256)    0           pixelcnn_prior_inputs[0][0]      
    __________________________________________________________________________________________________
    v_pad_1 (ZeroPadding2D)         (None, 14, 14, 256)  0           codes_sampler[1][0]              
    __________________________________________________________________________________________________
    h_pad_1 (ZeroPadding2D)         (None, 8, 14, 256)   0           codes_sampler[1][0]              
    __________________________________________________________________________________________________
    v_masked_conv_1 (MaskedConv2D)  (None, 11, 8, 64)    458816      v_pad_1[0][0]                    
    __________________________________________________________________________________________________
    h_masked_conv_1 (MaskedConv2D)  (None, 8, 11, 64)    65600       h_pad_1[0][0]                    
    __________________________________________________________________________________________________
    tf_op_layer_strided_slice_4 (Te [(None, 8, 8, 64)]   0           v_masked_conv_1[0][0]            
    __________________________________________________________________________________________________
    tf_op_layer_strided_slice_5 (Te [(None, 8, 8, 64)]   0           h_masked_conv_1[0][0]            
    __________________________________________________________________________________________________
    v_to_h_1 (Conv2D)               (None, 8, 8, 64)     4160        tf_op_layer_strided_slice_4[0][0]
    __________________________________________________________________________________________________
    tf_op_layer_add (TensorFlowOpLa [(None, 8, 8, 64)]   0           tf_op_layer_strided_slice_5[0][0]
                                                                     v_to_h_1[0][0]                   
    __________________________________________________________________________________________________
    h_gate_1 (Lambda)               (None, 8, 8, 32)     0           tf_op_layer_add[0][0]            
    __________________________________________________________________________________________________
    v_gate_1 (Lambda)               (None, 8, 8, 32)     0           tf_op_layer_strided_slice_4[0][0]


and 

    ==================================================================================================
    Total params: 786,592
    Trainable params: 786,592
    Non-trainable params: 0

For my modified TF2.X code, 

    Model: ""pixelcnn-prior""
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    pixelcnn_prior_inputs (InputLay [(None, 4, 4)]       0                                            
    __________________________________________________________________________________________________
    codes_sampler (Functional)      (None, 4, 4, 256)    21824387    pixelcnn_prior_inputs[0][0]      
    __________________________________________________________________________________________________
    v_pad_1 (ZeroPadding2D)         (None, 10, 10, 256)  0           codes_sampler[0][0]              
    __________________________________________________________________________________________________
    h_pad_1 (ZeroPadding2D)         (None, 4, 10, 256)   0           codes_sampler[0][0]              
    __________________________________________________________________________________________________
    v_masked_conv_1 (MaskedConv2D)  (None, 7, 4, 64)     458816      v_pad_1[0][0]                    
    __________________________________________________________________________________________________
    h_masked_conv_1 (MaskedConv2D)  (None, 4, 7, 64)     65600       h_pad_1[0][0]                    
    __________________________________________________________________________________________________
    tf.__operators__.getitem (Slici (None, 4, 4, 64)     0           v_masked_conv_1[0][0]            
    __________________________________________________________________________________________________
    tf.__operators__.getitem_1 (Sli (None, 4, 4, 64)     0           h_masked_conv_1[0][0]            
    __________________________________________________________________________________________________
    v_to_h_1 (Conv2D)               (None, 4, 4, 64)     4160        tf.__operators__.getitem[0][0]   
    __________________________________________________________________________________________________
    tf.__operators__.add (TFOpLambd (None, 4, 4, 64)     0           tf.__operators__.getitem_1[0][0] 
                                                                     v_to_h_1[0][0]                   
    __________________________________________________________________________________________________
    gate_layer_1 (GateLayer)        (None, 4, 4, 32)     0           tf.__operators__.add[0][0]       
    __________________________________________________________________________________________________
    gate_layer (GateLayer)          (None, 4, 4, 32)     0           tf.__operators__.getitem[0][0]   

and 

    ==================================================================================================
    Total params: 22,610,979
    Trainable params: 22,602,915
    Non-trainable params: 8,064


Could you please let me know where I did wrong? I've been tried with different things but they cannot sort things out... 

  [1]: https://github.com/iomanker/VQVAE-TF2
  [2]: https://www.kaggle.com/ameroyer/keras-vq-vae-for-image-generation/comments
  [3]: https://github.com/iomanker/VQVAE-TF2/blob/master/vqvae.py

"
47468,how to Training .TextGrid file by tensorflow,"now, i have get some audio label file , that  read .wav file and use Annotate -TextGrid(sentence) button operator by Praat tool;
open file  :

`File type = ""ooTextFile""
Object class = ""TextGrid""

xmin = 0 
xmax = 17.275351473922903 
tiers? <exists> 
size = 1 
item []: 
    item [1]:
        class = ""IntervalTier"" 
        name = ""silences"" 
        xmin = 0 
        xmax = 17.275351473922903 
        intervals: size = 19 
        intervals [1]:
            xmin = 0 
            xmax = 1.9536757369614515 
            text = ""silent"" 
        intervals [2]:
            xmin = 1.9536757369614515 
            xmax = 3.4176757369614514 
            text = ""sounding"" 
        intervals [3]:
            xmin = 3.4176757369614514 
            xmax = 4.281675736961452 
            text = ""silent"" 
        intervals [4]:
            xmin = 4.281675736961452 
            xmax = 5.081675736961452 
            text = ""sounding"" `

---------
but  i do not how to use the file with tensorflow , 
give me a dir,   3Q;"
47467,Flatbuffers Checksum Issue Building With Make,"**System information**
- No custom code
- Windows 10 
- TensorFlow installed from source
- TensorFlow current version


**Describe the current behavior**

Using make to build Tensorflow Lite example projects fails using make for several projects. On Windows, setting command prompt to the tensorflow repository directory, and following the README for tensorflow\lite\micro\examples\magic_wand:

`make -f tensorflow/lite/micro/tools/make/Makefile test_magic_wand_test`

That produces the following results:


```
C:\Users\jtork\CMake\tensorflow_src>make -f tensorflow/lite/micro/tools/make/Makefile test_magic_wand_test
FIND: Parameter format not correct
FIND: Parameter format not correct
--2021-02-28 19:42:41--  http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip
Resolving mirror.tensorflow.org (mirror.tensorflow.org)... 142.250.113.128, 2607:f8b0:4023:1000::80
Connecting to mirror.tensorflow.org (mirror.tensorflow.org)|142.250.113.128|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1760478 (1.7M) [application/zip]
Saving to: '/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip'

/tmp/dca12522a9f9e37f126ab925 100%[=================================================>]   1.68M  8.87MB/s    in 0.2s

2021-02-28 19:42:41 (8.87 MB/s) - '/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip' saved [1760478/1760478]

/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 1: $'PK\003\004': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 2: nL8Q5: command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 3: nL8Q?: command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 4: $'\bnL8Qw\030B\005\352\002\304\005V': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 4: $'\367\257XY': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'\002T': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'v\355\236\215\265M4\036w]\027\262\036p\250t5\256\367\245f': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'\225L\347\313l\036\020\350\341\320\265\254\3218\002~\264B\323\302\371\016XC\2408\313': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'@\244\315\217\v\2105': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'\301\2173H2\037.\342,\311N\223\233d}\231^\257\341': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'^\255\342\345:\231g\220\256': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'m\237\220Q7\037\324\242\304R\234R\372T\235\317k\347T\347\316\334\235\353F\352\217g\317q': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: 6m@0cs/d j4E)8&z@-i#hPoqX6
l{Ay+Ff%325sD%k<JW90ubOO_@Y0!mbhR(KH2@L90j63J^%,Gj-\liWA27/1Ue'jYku9#3(ld7Z4KEzy}]O,m'>!(}Wt|^8Zuv{Jt+{%^k<~.Z46v9   vZ{7o)H\/     eCw@B2#-{0l>9t)s_P2     PK
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 11: $'OO\337\367\312\262\204\0365\3341P]\034\327\025ZL4\237t\364]r\363\024c\020\202\216\214?h\210\254N\326\b\a\275\221z]P\022\301c\177\346\2608\274\350\234\3230tl4y\006\244\272h.y\371\214i\201,': command not found
nL8Q>   flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.bazelci/UTl_PK
nL8Q=!XK      flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.bazelci/presubmit.ymlUTl_J*ILL-RI,I-.*iEV
Iy%f& X}|IbQzjI1DHWAIOOO        KjML/PK
nL8Q4%_0B    flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.clang-formatUTl_mMn@>)[v*RP\p+ynQE: No such file or directory
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 11: $'\034Z\303\211\346\236\223\377\b\277\211\215l\270\247\326\b\257ot[\005\235:\232\004\235\243\246\277\231\304m\242\3711\277\356\361\235$\235\264\031Y\363\351\2365\363\334\356\004\343\232-_\346_\332+\355[\264\371\021\324J\234\256\377B\252\252\002\370\004PK\003\004': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 12: $'\bnL8Q\33088\322\236\337B': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 13: $'\302@\020D\373': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: $'\021\301\336\316\322?\020': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: rTbLlXNQtK*4: No such file or directory
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: $'\202\v%Q\256\253\333\341^\211ulh': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 17: $'f\345eey\206\310?S\336\253wZ\270pF\223\304\2026\306\3236\375{\351\003PK\003\004': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 18: $'\bnL8Q\324\240P3\211\375A': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: command substitution: line 19: syntax error near unexpected token `)'
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: command substitution: line 19: `1DdmJ.[^3o=&9bxB 0j<k$x)(b""1 Pxl'
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: $'\305\202\205\270\312\0261\315\274\304RT\373t\363\206\036t\371\261\204d\340V\327\257v\230\036\236\332\300U0\330_\246\345\f\343L\336\027\027mg\252\333\256\230\235g\376\201I\275\317\037PK\003\004': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 17: $'\bnL8Q\026\345\365\275OUC': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 18: $'\200': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 18: $'\365\370\243\371PK\003\004': command not found
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 19: flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.github/UTl_PK: No such file or directory
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 20: syntax error near unexpected token `('
/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 20: nL8Q}BN flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.github/ISSUE_TEMPLATE.mdUTl_UAO0o(v@ABp  H^]`CI!=!hr>^#0zUCq1,|a\#RjGjB.+!r'     9yBD?KRIGkX$gB9BF^9Aqu1bj1=HO,!-9GsW      =L<\]Uhzu[S7dvT@JeY            q)Zr; ,nVs6{4=;I%fPK'
tensorflow/lite/micro/tools/make/Makefile:525: *** Something went wrong with the flatbuffers download: Bad checksum. Expected: aa9adc93eb9b33fa1a2a90969e48baee, Got: .  Stop.
```

I believe this same issue occurs for all make options I have tried.



**Describe the expected behavior**

All dependencies should be resolved and the project should build. 

"
47466,RuntimeError: Mixing different tf.distribute.Strategy objects while using tf.distribute.MirroredStrategy(),"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04)
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): 1.15
- Python version:3.6.9
- CUDA/cuDNN version: 10
- GPU model and memory: TitanXP

TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`: v1.15.4-39-g3db52be 1.15.

**Describe the current behavior**
Referring to : https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/bin/train.py, I was trying to create a code and also converting it to multi-GPU with tf.distribute.MirroredStrategy(). The code works with out tf.distribute.MirroredStrategy() in single Titan XP, but once I use the below code its shows error as shown below. 
**Describe the expected behavior**
Using the tf.distribute.MirroredStrategy()

`strategy=tf.distribute.MirroredStrategy()
    with strategy.scope():
           model....
           model.compile`

The above structure should run with multi-gpu

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Referring to https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/bin/train.py#L107, replaced with the above code. 

But error `RuntimeError: Mixing different tf.distribute.Strategy objects: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategyV1 object at 0x7f36a44f72e8> is not <tensorflow.python.distribute.mirrored_strategy.MirroredStrategyV1 object at 0x7f36247d0128>`"
47465,Correct range of input values for MobileNet,"Hello,

I want to use the implementation of MobileNetV3 (either the MobileNet_large or MobileNet_small version) in my project. I'm using the TensorFlow v2.4.1.

 I get confused when I read the documentation about the range of input values for MobileNet-based models. According to this [link](https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5)  the expected range of values for the model is [0,1]. However, according to this other [link](https://www.tensorflow.org/tutorials/images/transfer_learning), the correct range of values is [-1, 1]. A similar issue was already reported at this [link](https://github.com/tensorflow/hub/issues/637).

On the other hand, to my surprise, the function preprocess_input of the module [MobileNetV3](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/applications/mobilenet_v3.py#L556-L558) does not apply any change over the input; i.e. the function is returning directly the input.

```
@keras_export('keras.applications.mobilenet_v3.preprocess_input')
def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument
  return x

```

So, what should be the correct range of input values for MobileNetV3?
"
47464,Suspected duplicate code in resize_bilinear_op.cc,"I suspect these two code blocks are doing very similar tasks and can potentially be merged: 

- [Code block 1](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/image/resize_bilinear_op.cc;l=254;drc=9e274c0b2ff75f64a97c9aec57aa59b030c5a01b;bpv=1;bpt=0)
- [Code block 2](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/image/resize_bilinear_op.cc;l=265-283;drc=9e274c0b2ff75f64a97c9aec57aa59b030c5a01b;bpv=1;bpt=0)

The only difference which I can notice is that the code block 1 **might** benefit from sequential cache read. But I think the compiler should assist to achieve the same efficiency for the code block 2. If so, we should merge these two blocks to eliminate duplicate code. "
47462,Problem with TensorFlow model example ,"Hi,

I would like to practice with the TensorFlow model example (https://www.tensorflow.org/lite/guide/ops_custom#create_a_tensorflow_model). In particular, I'm interested to repeat the error 
""Error: Some of the operators in the model are not supported by the standard TensorFlow Lite runtime...... Here is
a list of operators for which you will need custom implementations: Sin."" 

How can I do this using, for example, tf.lite.TFLiteConverter.from_concrete_functions()?
Thanks in advance.
"
47461,minimize total loss without step loss,"I want to minimize total loss without step loss (each loss no back-propagation), how to implement it?    
total_loss = 0
idx = 0
for data in train_datas:
       pre = model(data)
       loss = metric(pre, gt)
       total_loss = total_loss + loss
       idx = idx + 1
       if idx %10 == 0:
              train_ops = optimizer(total_loss)
              sess.run(train_ops)"
47460,unable to train model by using TPU(free TPU),"I have prepared pipe line to train x-rays image classification model, train dataset is not balanced so did balance it by using resampling methods see the below link for more information:
https://www.tensorflow.org/guide/data#resampling

please see the error msg I got when fit the model:
_""""UnavailableError: 2 root error(s) found.
  (0) Unavailable: Socket closed
  (1) Unavailable: Unable to find a context_id matching the specified one (12465293436922014039). Perhaps the worker was restarted, or the context was GC'd?
0 successful operations.
0 derived errors ignored._

Please advise?"
47459,"Error ""failed to connect to all addresses"" when iterating dataset on TPU with Colab or Kaggle","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I'm currently testing out this example [EfficientDet_TPU](https://www.kaggle.com/davidtong/efficientdet-tpu-train-1-epoch-in-4-min)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Kaggle and Google Colab
- TensorFlow version (use command below): 2.4
- Python version: 3.7

**Describe the current behavior**
Unable to iterate dataset
![Screenshot from 2021-02-28 16-35-06](https://user-images.githubusercontent.com/19550237/109412442-f8e10b80-79e2-11eb-9822-c07aa298818a.png)


**Describe the expected behavior**
Able to iterate dataset

**Standalone code to reproduce the issue**
 You can directly try this example, the tfrecords is provided as well. [EfficientDet_TPU](https://www.kaggle.com/davidtong/efficientdet-tpu-train-1-epoch-in-4-min)

**Other info / logs** 
`UnavailableError                          Traceback (most recent call last)
<ipython-input-33-15ac5583c702> in <module>
----> 1 for batch_x, (batch_regression, batch_classification) in train_dataset_encode:
      2     print(batch_x)
      3     break

/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in __next__(self)
    745   def __next__(self):
    746     try:
--> 747       return self._next_internal()
    748     except errors.OutOfRangeError:
    749       raise StopIteration

/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    737         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    738       except AttributeError:
--> 739         return structure.from_compatible_tensor_list(self._element_spec, ret)
    740 
    741   @property

/opt/conda/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)
    117         if type is None:
    118             try:
--> 119                 next(self.gen)
    120             except StopIteration:
    121                 return False

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   2114     finally:
   2115       ctx.executor = executor_old
-> 2116       executor_new.wait()
   2117 
   2118 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/executor.py in wait(self)
     67   def wait(self):
     68     """"""Waits for ops dispatched in this executor to finish.""""""
---> 69     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     70 
     71   def clear_error(self):

UnavailableError: failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1614500451.196304845"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":4143,""referenced_errors"":[{""created"":""@1614500451.108464009"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":398,""grpc_status"":14}]}
`
"
47458,issue with tf.data.experimental.rejection_resample method,"when I run the below block of code i got this error:
""Shape must be rank 3 but is rank 2 for '{{node Tile}} = Tile[T=DT_FLOAT, Tmultiples=DT_INT32](ExpandDims, Tile/multiples)' with input shapes: [1,11,11], [2].""

I need to balance the ""train_dataset"" which has 11 labels by using the follwoing line of codes:

1) create class_fun:
```
def class_func(features, label):
  return label
```

2)create resampler object with targer_dist:
```
resampler = tf.data.experimental.rejection_resample(
    class_func, target_dist=[0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1])

```
When run the below line of code I got the error which I have mentioned it above :

`resample_ds = train_dataset.unbatch().apply(resampler)
`
Please advise?"
47457,TF-TRT: batch dimension is failing in FasterRCNN,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): Nvidia docker image  `nvcr.io/nvidia/tensorflow:20.11-tf1-py3`
- TensorFlow version (use command below): 1.15.4
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:  10.2 / 7.6.5 
- GPU model and memory: T4


**Describe the current behavior**
I am using the script `https://github.com/tensorflow/tensorrt/tree/r1.14%2B/tftrt/examples/object_detection` to optimize the model faster_rcnn_inception_v2, although the test was completed successfully I got the below warning about` incomplete shapes`: 
```
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
**1752 ops no flops stats due to incomplete shapes.
Parsing Inputs...
Incomplete shape.**
.
.
.
orm/default/dso_loader.cc:49] Successfully opened dynamic library libnvinfer.so.7
WARNING:tensorflow:INT8 precision mode with calibration is supported with dynamic TRT ops only. Disregarding is_dynamic_op parameter.
.
.
.
DONE (t=1.91s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.390
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.265
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.262
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.446
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.300
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.314
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553
{
    ""avg_latency_ms"": 246.9332789430524,
    ""avg_throughput_fps"": 32.397415343295854,
    ""map"": 0.24546921626010776
}
ASSERTION PASSED: statistics['map'] > (0.243 - 0.01)
```


The test was completed successfully; however, when I tried to deploy the model with Nvidia DeepStream I got the below input shape errors, even though the engine was created with max batch size 8, it seems the object_detection.py script didn't assign the parameter input_shape for some operations
```
  (0) Invalid argument: Input shape axis 0 must equal 8, got shape [5,600,1024,3]
         [[{{node Preprocessor/unstack}}]]
         [[SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression_4/unstack/_2859]]
  (1) I**nvalid argument: Input shape axis 0 must equal 8, got shape [5,600,1024,3]**
         [[{{node Preprocessor/unstack}}]]
```

Hi @samikama / @trevor-m , could you please suggest how to solve this input shapes issue?, also it seems to be related to `NMS` custom ops . A similar case was reported [here](https://github.com/tensorflow/tensorflow/issues/21081)
"
47456,how to set C++ standard version for tensenflow as another bazel project dependency,"**System information**
- ubuntu20.04
- TensorFlow version:2.4.0
- Python version:3.8
- Bazel version (if compiling from source):3.7.2
- GCC/Compiler version (if compiling from source):9.3

**Describe the problem**
my project depend on tensorflow, compile tensorflow need std=c++14 and above, I set --cxxopt=""-std=c++14"" in bazel build command for my project, but this option doesn't work for tensorflow repository, I notice -std=c++14 effective for my own project and some other external repository

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
external/org_tensorflow/tensorflow/core/platform/default/port.cc:360:46: error: could not convert '{9223372036854775807, 9223372036854775807}' from '<brace-enclosed initializer list>' to 'tensorflow::port::MemoryInfo'
  360 |   MemoryInfo mem_info = {INT64_MAX, INT64_MAX};
      |                                              ^
      |                                              |
      |                                              <brace-enclosed initializer list>

"
47454,Batch Normalization fails as kernel constraint for Conv layers when using mixed precision,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7


**Describe the current behavior**
When using `tf.keras.layers.BatchNormalization()` as a constraint in a conv layer using mixed precision, the model cannot train

**Describe the expected behavior**
Using `tf.keras.layers.BatchNormalization()` as a constraint in a conv layer behaves the same regardless of using mixed precision or not.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
[Colab link](https://colab.research.google.com/drive/1IFWFwRrYUQx7Kw0I_KdtrKtVhvZbI7hy?usp=sharing).

I've included a few notes in comments to show that this issue is isolated to conv layers when using mixed precision.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

It looks like the issue is in the loss_scale_optimizer.

```txt
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:498 minimize
        return self.apply_gradients(grads_and_vars, name=name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:712 apply_gradients
        args=(grads_and_vars, name, experimental_aggregate_gradients))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2941 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2948 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:745 _apply_gradients_cross_replica  **
        do_not_apply_fn)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/smart_cond.py:59 smart_cond
        name=name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:538 new_func
        return func(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:1180 cond
        return cond_v2.cond_v2(pred, true_fn, false_fn, name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/cond_v2.py:89 cond_v2
        op_return_value=pred)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:990 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:732 apply_fn
        args=(grads, wrapped_vars, name, experimental_aggregate_gradients))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:755 _apply_gradients
        experimental_aggregate_gradients=experimental_aggregate_gradients)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:635 apply_gradients
        ""name"": name,
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2941 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2948 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:683 _distributed_apply  **
        var, apply_grad_to_update_var, args=(grad,), group=False))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2494 update
        return self._update(var, fn, args, kwargs, group)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3431 _update
        return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3437 _update_non_slot
        result = fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:661 apply_grad_to_update_var  **
        return var.assign(var.constraint(var))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/autocast_variable.py:237 assign
        name, read_value)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/autocast_variable.py:209 _apply_assign_update
        assign_op = update_fn(value, use_locking, name, False)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:882 assign
        value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py:163 wrapped
        return func(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1509 convert_to_tensor
        (dtype.name, value.dtype.name, value))

    ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: <tf.Tensor 'cond_1/SGD/SGD/update/batch_normalization_2/FusedBatchNormV3:0' shape=(3, 3, 1, 32) dtype=float16>
```"
47453,[_Derived_]RecvAsync is cancelled Error when training with LSTM on tf-gpu ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
```
model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, store.embedding_dim,
                                  batch_input_shape=[batch_size, None]),
        tf.keras.layers.Dropout(store.dropout_rate),
        tf.keras.layers.LSTM(store.rnn_units,
                             return_sequences=True,
                             stateful=True,
                             recurrent_initializer=store.rnn_initializer),
        tf.keras.layers.Dropout(store.dropout_rate),
        tf.keras.layers.LSTM(store.rnn_units,
                             return_sequences=True,
                             stateful=True,
                             recurrent_initializer=store.rnn_initializer),
        tf.keras.layers.Dropout(store.dropout_rate),
        tf.keras.layers.Dense(vocab_size)
    ])
    model.compile(optimizer=optimizer, loss=loss, metrics=[""accuracy""])

```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version (use command below):  v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.8
- CUDA/cuDNN version: CUDA 11.0.3 / CUDNN 8.0.5.77
- GPU model and memory: GTX 1070 (8GB)




**Describe the current behavior**
I've been working on a LSTM model, when training with $ `model.fit()` , it runs for 6 epochs and then gives this error 

```
  File ""C:\Users\Me\anaconda3\envs\tf_gpu24\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""C:\Users\Me\anaconda3\envs\tf_gpu24\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\Me\anaconda3\envs\tf_gpu24\lib\site-packages\tensorflow\python\eager\def_function.py"", line 855, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""C:\Users\Me\anaconda3\envs\tf_gpu24\lib\site-packages\tensorflow\python\eager\function.py"", line 2942, in __call__
    return graph_function._call_flat(
  File ""C:\Users\Me\anaconda3\envs\tf_gpu24\lib\site-packages\tensorflow\python\eager\function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""C:\Users\Me\anaconda3\envs\tf_gpu24\lib\site-packages\tensorflow\python\eager\function.py"", line 555, in call
    outputs = execute.execute(
  File ""C:\Users\Me\anaconda3\envs\tf_gpu24\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.
         [[{{node gradient_tape/sequential/embedding/embedding_lookup/Reshape/_20}}]] [Op:__inference_train_function_4800]

Function call stack:
train_function
```



**Describe the expected behavior**
Model should complete training without issue.
I know the code is fine because I trained on the same code with no issue in an old environment I was using 2 months ago. It also runs fine in a CPU only tensorflow environment.  


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem.

```
Epoch 1/50
2021-02-27 14:50:38.552734: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-02-27 14:50:38.882403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-02-27 14:50:39.546250: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-02-27 14:50:39.794953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
37/37 [==============================] - 7s 55ms/step - loss: 7.0684 - accuracy: 0.1270
Epoch 2/50
37/37 [==============================] - 2s 54ms/step - loss: 4.8889 - accuracy: 0.1828
Epoch 3/50
37/37 [==============================] - 2s 54ms/step - loss: 4.7884 - accuracy: 0.1666
Epoch 4/50
37/37 [==============================] - 2s 54ms/step - loss: 4.6866 - accuracy: 0.1480
Epoch 5/50
37/37 [==============================] - 2s 55ms/step - loss: 4.5179 - accuracy: 0.1630
Epoch 6/50
17/37 [============>.................] - ETA: 1s - loss: 4.2505 - accuracy: 0.14842021-02-27 14:50:55.955000: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(2004): 'cudnnRNNBackwardWeights( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), output_desc.handles(), output_data.opaque(), workspace.opaque(), workspace.size(), rnn_desc.params_handle(), params_backprop_data->opaque(), reserve_space_data->opaque(), reserve_space_data->size())'
2021-02-27 14:50:55.955194: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1926 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 256, 256, 1, 100, 64, 256]
2021-02-27 14:50:55,957 : MainThread : INFO : Saving model history to model_history.csv
2021-02-27 14:50:55,961 : MainThread : INFO : Saving model to D:\project\project_engine\fftest_checkpoints\batch_0\synthetic
Traceback (most recent call last):
  File ""runTrain.py"", line 65, in <module>
    model.train()
  ...
  ... 
  ...
  File ""D:\project\project_engine\runTrain.py"", line 201, in train_rnn
    model.fit(dataset, epochs=store.epochs, callbacks=_callbacks)
  File ""C:\Users\Me\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""C:\Users\Me\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\Me\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\eager\def_function.py"", line 855, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""C:\Users\Me\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\eager\function.py"", line 2942, in __call__
    return graph_function._call_flat(
  File ""C:\Users\Me\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\eager\function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""C:\Users\Me\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\eager\function.py"", line 555, in call
    outputs = execute.execute(
  File ""C:\Users\Me\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.
         [[{{node gradient_tape/sequential/embedding/embedding_lookup/Reshape/_20}}]] [Op:__inference_train_function_4800]

Function call stack:
train_function
```
"
47452,Empty Interpreter Input/Output Buffer Pointers,"### 1. System information

- OS Platform and Distribution: Windows 10, building in Visual Studio
- TensorFlow installation: Model trained from PIP package (Tensorflow 2.4.1). TFLite interpreter building 2.0.0 (haven't been able to build TFLite 2.4.1 in Visual Studio yet)
- TensorFlow library: 2.4.1 (Train, conversion) and 2.0.0 (inference)

### 2. Code

My model uses the following layers: 
First stage, the stage I'm having problems:
TimeDistributed(Conv2D), TimeDistributed(MaxPooling2D), TimeDistributed(BatchNormalization), TimeDistributed(Flatten), TimeDistributed(Dense), 

Second stage (evaluated independently, haven't gotten to it yet):
Bidirectional(GRU), Reshape, Dense, Dropout.

I am quantizing this model. The inference code is shown below.

`	FS_FILE* pModelFile;
	char ModelStage1Filename[] = ""\\Models\\cls_stg1.tflite"";
	char ModelStage2Filename[] = ""\\Models\\cls_stg2.tflite"";
	int BytesRead;

	pModelFile = fs_fopen(ModelStage1Filename, ""r"");
	if (pModelFile != 0)
	{
		Stage1ModelSize = fs_fread(ModelStage1ModelBinary, 1, sizeof(ModelStage1ModelBinary), pModelFile);
		fs_fclose(pModelFile);
	}
	pModelFile = fs_fopen(ModelStage2Filename, ""r"");
	if (pModelFile != 0)
	{
		Stage1ModelSize = fs_fread(ModelStage2ModelBinary, 1, sizeof(ModelStage2ModelBinary), pModelFile);
		fs_fclose(pModelFile);
	}

	pMicroErrorReporter = new tflite::MicroErrorReporter();
	pErrorReporter = pMicroErrorReporter;


	if (Stage1ModelSize > 0)
	{
		Stage1Model = tflite::GetModel(ModelStage1ModelBinary);
		if (Stage1Model->version() != TFLITE_SCHEMA_VERSION)
		{
			/*TF_LITE_REPORT_ERROR(error_reporter,
				""Model provided is schema version %d not equal ""
				""to supported version %d.\n"",
				model->version(), TFLITE_SCHEMA_VERSION);*/
		}
		else
		{
			pStage1Interpreter = new tflite::MicroInterpreter(Stage1Model, resolver, stage_1_tensor_arena, stage_1_tensor_arena_size, pErrorReporter);
			if (pStage1Interpreter != 0)
			{
				pStage1Interpreter->AllocateTensors();
				// Obtain a pointer to the model's input tensor
				TfLiteTensor* input = pStage1Interpreter->input(0);
				if (input != 0)
				{
					if (input->dims->size == 4 &&
						input->type == kTfLiteUInt8 &&
						input->dims->data[0] == 1 &&
						input->dims->data[1] == 37 &&
						input->dims->data[2] == 256 &&
						input->dims->data[3] == 1)
					{
						TfLiteTensor* output = pStage1Interpreter->output(0);
						if (output != 0)
						{
							if (output->dims->size == 2 &&
								output->type == kTfLiteUInt8 &&
								output->dims->data[0] == 1 &&
								output->dims->data[1] == 64)
							{
								// All Good!
								Stage1Good = 1;
								pStage1Input = pStage1Interpreter->typed_input_tensor<unsigned char>(0);
								pStage1Output = pStage1Interpreter->typed_output_tensor<unsigned char>(0);
							}
						}
					}
				}
			}
		}
	}
	else
	{
	}`



### 3. Failure after conversion

My fully trained model is split into two parts, a non-RNN first stage and an RNN second stage. When I run the inference code above to prepare to do inference on the first stage, everything looks good, the input and output tensor shapes and sizes are correct. But executing the lines below, I get null pointers, so I am unable to perform inference.

pStage1Input = pStage1Interpreter->typed_input_tensor<unsigned char>(0);
pStage1Output = pStage1Interpreter->typed_output_tensor<unsigned char>(0);

When I use this code and setup with a simple model with a single dense layer, I get valid pointers. 

### 4. (optional) RNN conversion support
This first stage does not include the RNN (the Bidirectional(GRU) ) parts of the full model.

### 5. (optional) Any other info / logs
I have tried to pull in Tensorflow Lite 2.4.1 into my Visual Studio project, however I am currently unable to resolve all of the dependencies, as the Windows make process seemed to work before in version 2.0.0 but not now in 2.4.1. There is a failure getting flatbuffers. So I don't know for sure if there is an issue with the difference in training/converting in 2.4.1 and performing inference using 2.0.0.
"
47451,Tensorflow Addons connected components not working properly,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.243
- GPU model and memory: GEForce GTX 1080 Ti, 11264 MB Dedicated Video Memory (I'm using CPU)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
TFA connected components should produce 2 components with the attached script. It is not.

**Describe the expected behavior**
TFA result should show same number of components as Scipy.ndimage.measurements.label, as claimed in the documentation


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

""""""
    TFA Addons Connected Components check

    Using:
    TensorFlow 2.4.0
    Python 3.8.5
    TFA 0.12.1
    Scipy 1.6.1
""""""

import tensorflow
import numpy as np
import tensorflow_addons as tfa
import matplotlib.pyplot as plt
import scipy.ndimage.measurements as meas


with tensorflow.device('/CPU:0'):

    M = 500
    img = np.zeros((M, M))

    # One square
    top = 50
    bottom = 270
    left = 300
    right = 450
    img[np.ix_(np.arange(top, bottom + 1), np.arange(left, right + 1))] = 1

    # Second one
    top = 100
    bottom = 150
    left = 200
    right = 250
    img[np.ix_(np.arange(top, bottom + 1), np.arange(left, right + 1))] = 1

    # Convert image to tensor
    img = tensorflow.convert_to_tensor(img)
    imgtf = tensorflow.expand_dims(img, -1)

    # Connected components with TFA
    d_img_tfa = tfa.image.connected_components(imgtf).numpy()

    # Plot it
    plt.figure()
    plt.title('TFA Connected Components Image')
    _ = plt.imshow(d_img_tfa)
    plt.show(block=False)

    # Connected components with scipy
    d_img_scipy, _ = meas.label(imgtf)

    # Count number of non-background components (subtract 1 for background)
    num_comp_tfa = np.unique(d_img_tfa.flatten()).size - 1
    num_comp_scipy = np.unique(d_img_scipy.flatten()).size - 1

    # These images should be the same
    print(""\nTFA connected components has %d components."" % num_comp_tfa)
    print(""Scipy connected components has %d components.\n"" % num_comp_scipy)

    # Plot
    plt.figure()
    plt.title('Scipy Connected Components Image')
    _ = plt.imshow(d_img_scipy)
    plt.show(block=True)


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47450,Keras model saving erroring: TypeError: get_config() missing 1 required positional argument: 'self',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: unknown
- TensorFlow installed from (source or binary): binary, conda
- TensorFlow version (use command below): 2.2
- Python version: 3.8
- CUDA/cuDNN version: 7.6

Bug demonstrated here:
https://stackoverflow.com/questions/57154799/keras-model-saving-erroring-typeerror-get-config-missing-1-required-position

Saving model that has weights initialized by tf.keras.initializers.zeros instead of tf.keras.initializers.Zeros() does not work because tf.keras.initializers.zeros is a class while tf.keras.initializers.Zeros() is an instance. Tf tries to look at fields of class but can't because it is not an instance. I cannot save my own model like this, and instead, have to save the weights and python code to construct it.

I get the following error after calling model.save(filename) on my tf.keras Model:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-52-4285f74de661> in <module>
----> 1 ae.save(
      2     os.path.join(folder, 'model'),
      3     include_optimizer=False
      4 )

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)
   1049     ```
   1050     """"""
-> 1051     save.save_model(self, filepath, overwrite, include_optimizer, save_format,
   1052                     signatures, options)
   1053 

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)
    135         model, filepath, overwrite, include_optimizer)
    136   else:
--> 137     saved_model_save.save(model, filepath, overwrite, include_optimizer,
    138                           signatures, options)
    139 

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)
     76     # we use the default replica context here.
     77     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access
---> 78       save_lib.save(model, filepath, signatures, options)
     79 
     80   if not include_optimizer:

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)
    948   meta_graph_def = saved_model.meta_graphs.add()
    949 
--> 950   _, exported_graph, object_saver, asset_info = _build_meta_graph(
    951       obj, export_dir, signatures, options, meta_graph_def)
    952   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)
   1034         function_aliases[fdef.name] = alias
   1035 
-> 1036   object_graph_proto = _serialize_object_graph(saveable_view,
   1037                                                asset_info.asset_index)
   1038   meta_graph_def.object_graph_def.CopyFrom(object_graph_proto)

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in _serialize_object_graph(saveable_view, asset_file_def_index)
    694 
    695   for obj, obj_proto in zip(saveable_view.nodes, proto.nodes):
--> 696     _write_object_proto(obj, obj_proto, asset_file_def_index,
    697                         saveable_view.function_name_map)
    698   return proto

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in _write_object_proto(obj, proto, asset_file_def_index, function_name_map)
    735           version=versions_pb2.VersionDef(
    736               producer=1, min_consumer=1, bad_consumers=[]),
--> 737           metadata=obj._tracking_metadata)
    738       # pylint:enable=protected-access
    739     proto.user_object.CopyFrom(registered_type_proto)

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _tracking_metadata(self)
   2740   @property
   2741   def _tracking_metadata(self):
-> 2742     return self._trackable_saved_model_saver.tracking_metadata
   2743 
   2744   def _list_extra_dependencies_for_serialization(self, serialization_cache):

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py in tracking_metadata(self)
     52     # TODO(kathywu): check that serialized JSON can be loaded (e.g., if an
     53     # object is in the python property)
---> 54     return json_utils.Encoder().encode(self.python_properties)
     55 
     56   def list_extra_dependencies_for_serialization(self, serialization_cache):

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in python_properties(self)
     39   def python_properties(self):
     40     # TODO(kathywu): Add python property validator
---> 41     return self._python_properties_internal()
     42 
     43   def _python_properties_internal(self):

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py in _python_properties_internal(self)
     33 
     34   def _python_properties_internal(self):
---> 35     metadata = super(ModelSavedModelSaver, self)._python_properties_internal()
     36     metadata.update(
     37         saving_utils.model_metadata(

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/network_serialization.py in _python_properties_internal(self)
     31 
     32   def _python_properties_internal(self):
---> 33     metadata = super(NetworkSavedModelSaver, self)._python_properties_internal()
     34 
     35     # Network stateful property is dependent on the child layers.

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _python_properties_internal(self)
     55         stateful=self.obj.stateful)
     56 
---> 57     metadata.update(get_config(self.obj))
     58     if self.obj.input_spec is not None:
     59       # Layer's input_spec has already been type-checked in the property setter.

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in get_config(obj)
    113     # When loading, the program will attempt to revive the object from config,
    114     # and if that fails, the object will be revived from the SavedModel.
--> 115     config = generic_utils.serialize_keras_object(obj)['config']
    116 
    117   if config is not None:

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py in serialize_keras_object(instance)
    268     name = get_registered_name(instance.__class__)
    269     try:
--> 270       config = instance.get_config()
    271     except NotImplementedError as e:
    272       if _SKIP_FAILED_SERIALIZATION:

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py in get_config(self)
    966     if not self._is_graph_network:
    967       raise NotImplementedError
--> 968     return copy.deepcopy(get_network_config(self))
    969 
    970   @classmethod

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py in get_network_config(network, serialize_layer_fn)
   2117           filtered_inbound_nodes.append(node_data)
   2118 
-> 2119     layer_config = serialize_layer_fn(layer)
   2120     layer_config['name'] = layer.name
   2121     layer_config['inbound_nodes'] = filtered_inbound_nodes

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py in serialize_keras_object(instance)
    268     name = get_registered_name(instance.__class__)
    269     try:
--> 270       config = instance.get_config()
    271     except NotImplementedError as e:
    272       if _SKIP_FAILED_SERIALIZATION:

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py in get_config(self)
    261         'activation': activations.serialize(self.activation),
    262         'use_bias': self.use_bias,
--> 263         'kernel_initializer': initializers.serialize(self.kernel_initializer),
    264         'bias_initializer': initializers.serialize(self.bias_initializer),
    265         'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/initializers.py in serialize(initializer)
    164 @keras_export('keras.initializers.serialize')
    165 def serialize(initializer):
--> 166   return serialize_keras_object(initializer)
    167 
    168 

~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py in serialize_keras_object(instance)
    268     name = get_registered_name(instance.__class__)
    269     try:
--> 270       config = instance.get_config()
    271     except NotImplementedError as e:
    272       if _SKIP_FAILED_SERIALIZATION:

TypeError: get_config() missing 1 required positional argument: 'self'
```

"
47448,custom op on gpu/dsp,"The official website just tell me how to custom ops on CPU, i wonder if theres something could teach me how to do this on GPU/DSP.
And we need  **_libhexagon_nn_skel.*.so""_** to use DSP. If i custom ops on hexagon delegate, do i have to compile new  .so through hexagon SDK ,or just build AAR and use the old .so files?
Thanks a lot!!!"
47447,Quantize for pooling op,"The int8 pooling(avg and max) does not have rescale because operators have the same scale and zero point for
input and output tensors. 
I'm wondering how to keep same scale, since distributions for different for intput and output tensors after max or avg operation.
![image](https://user-images.githubusercontent.com/19774920/109381833-97a03600-7917-11eb-8d9d-c87d2d6be964.png)
"
47445,"""ValueError: No gradients provided "" error in TF v2.4.1, works fine on v2.3.1","**System information**
Python : 3.6.3, TF: 2.4.1, TF probability: 0.12.1
Python : 3.6.9, TF: 2.3.1, TF probability: 0.11.1
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu and CentOS
- TensorFlow installed from (source or binary): Pip
- TensorFlow version (use command below): 2.4.1 and 2.3.1
- Python version: 3.6.9 and 3.6.3
- GPU model and memory: None

**Describe the current behavior**
I have copied simple Gaussian process example from the tensorflow probability site. It works fine on version 2.3.1 but yields 
`ValueError: No gradients provided for any variable:` error on tensorflow 2.4.1

The example file  colab has been attached alongwith.

**Describe the expected behavior**
Both shall result in identical behaviour. And should produce gradients.

**Standalone code to reproduce the issue**
Reproduced here:

https://colab.research.google.com/drive/1Tx-46aoF4i5mHMuJ7Et13B1km7u6i7YV#scrollTo=RXrq442mMGJD

Example adapted from here:

https://www.tensorflow.org/probability/examples/Gaussian_Process_Regression_In_TFP

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Error:

```
---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

<ipython-input-1-6493a18eac89> in <module>()
    116                             observation_noise_variance_var)
    117   grads = tape.gradient(loss, trainable_variables)
--> 118   optimizer.apply_gradients(zip(grads, trainable_variables))
    119   lls_[i] = loss
    120 

1 frames

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)
    596       RuntimeError: If called in a cross-replica context.
    597     """"""
--> 598     grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    599     var_list = [v for (_, v) in grads_and_vars]
    600 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/utils.py in filter_empty_gradients(grads_and_vars)
     77   if not filtered:
     78     raise ValueError(""No gradients provided for any variable: %s."" %
---> 79                      ([v.name for _, v in grads_and_vars],))
     80   if vars_with_empty_grads:
     81     logging.warning(

ValueError: No gradients provided for any variable: ['amplitude:0', 'length_scale:0', 'observation_noise_variance_var:0'].
```"
47442,AttributeError: module 'tensorflow' has no attribute 'Session' with tensorflow 2.4.1,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **windows 10**
- TensorFlow installed from (source or binary): installed from source
- TensorFlow version (use command below): **2.4.1**
- Python version: **3.8.5**
- CUDA/cuDNN version: Cuda 11.2/11.1
- GPU model and memory: **Intel(R) UHD Graphics 630
NVIDIA GeForce GTX 1050**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


v2.4.0-49-g85c8b2a817f 2.4.1


**Describe the current behavior**

I installed the tensorflow-gpu:
```
conda create -n tensorflow-gpu
activate tensorflow-gpu
pip install tensorflow-gpu

activate tensorflow-gpu

python
import tensorflow as tf
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

I got this error: AttributeError: module 'tensorflow' has no attribute 'Session'
```



**Describe the expected behavior**

The Session() should be executed correctly.

"
47441,tensorflow-gpu 2.2.0 doesn't recognize Nvidia MX130 GPU,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10 Build 19042
- TensorFlow installed from (source or binary): `pip install tensorflow-gpu==2.2.0` in conda environment.
- TensorFlow version: 2.2.0
- Python version: 3.7.9
- Installed using pip.
- CUDA/cuDNN version: `CUDA 10.1.243`, `cuDNN 7.6.5`
- GPU model and memory: GeForce MX130, 2GB

**Problem Description:**
TensorFlow 2.2.0 doesn't recognize my GPU.

**Provide the exact sequence of commands / steps that you executed before running into the problem:**

1. Installed [CUDA 10.1 (Update 2)](https://developer.nvidia.com/cuda-10.1-download-archive-update2?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal) to `D:\Program Files\CUDA\v10.1`.
2. Extracted [cuDNN 7.6.5](https://developer.nvidia.com/rdp/cudnn-archive#a-collapse765-101) to `D:\cuda`.
3. Set the correct path, as instructed in [https://www.tensorflow.org/install/gpu for TensorFlow 2.2.0](https://web.archive.org/web/20200603082035if_/https://www.tensorflow.org/install/gpu#software_requirements).
```batch
SET PATH=D:\Program Files\CUDA\v10.1\bin;%PATH%
SET PATH=D:\Program Files\CUDA\v10.1\extras\CUPTI\lib64;%PATH%
SET PATH=D:\Program Files\CUDA\v10.1\include;%PATH%
SET PATH=D:\cuda\bin;%PATH%
```

4. In Python, imported TensorFlow and tried to check for the GPU:
```python
import tensorflow as tf
tf.test.is_built_with_cuda()
tf.config.experimental.list_physical_devices('gpu')
```
Output is:
```python
>>> import tensorflow as tf
2021-02-26 19:58:38.967202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll

>>> tf.test.is_built_with_cuda()
True

>>> tf.config.experimental.list_physical_devices('gpu')
2021-02-26 21:14:53.525272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-02-26 21:14:54.056784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2021-02-26 21:14:54.057159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-02-26 21:14:54.068576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-02-26 21:14:54.076899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-02-26 21:14:54.079739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-02-26 21:14:54.090196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-02-26 21:14:54.095665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-02-26 21:14:55.099130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-02-26 21:14:55.300935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
[]
```

Not a duplicate of [issue #41892](https://github.com/tensorflow/tensorflow/issues/41892), as I use Windows 10.

Edit:
As @ymodak suggested:
```python
import tensorflow as tf
tf.config.list_logical_devices('GPU')
```
does return:
```python
>>> import tensorflow as tf
2021-02-27 14:21:36.675965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll

>>> tf.config.list_logical_devices('GPU')
2021-02-27 14:26:59.304811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-02-27 14:26:59.803139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2021-02-27 14:26:59.803491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-02-27 14:27:00.179875: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-02-27 14:27:00.224576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-02-27 14:27:00.247267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-02-27 14:27:00.290612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-02-27 14:27:00.313676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-02-27 14:27:00.834454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-02-27 14:27:01.102483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2021-02-27 14:27:01.111434: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2021-02-27 14:27:01.160065: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e9c1582e60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-02-27 14:27:01.160517: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-02-27 14:27:01.207202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2021-02-27 14:27:01.207683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-02-27 14:27:01.212166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-02-27 14:27:01.215100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-02-27 14:27:01.216437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-02-27 14:27:01.219085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-02-27 14:27:01.220720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-02-27 14:27:01.221751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-02-27 14:27:01.223134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2021-02-27 14:27:16.215597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-27 14:27:16.216163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2021-02-27 14:27:16.221461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2021-02-27 14:27:16.304146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1377 MB memory) -> physical GPU (device: 0, name: GeForce MX130, pci bus id: 0000:02:00.0, compute capability: 5.0)
2021-02-27 14:27:16.432698: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e9dec28fe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-02-27 14:27:16.433279: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX130, Compute Capability 5.0
[LogicalDevice(name='/device:GPU:0', device_type='GPU')]
```
_but_ in Jupyter notebook it returns:
```python
[]
```
_AND_ it still does not train my model using the GPU."
47436,"Distributed computing, extreme distribution latency ","tf 2.2
python 3.7

I'm trying to distribute a training step over multiple GPU and although I'm not shown any error and the memory seems to be correctly allocated to all the GPUs.
I am experiencing extreme latency in the distribution and collection:
```
@tf.function
def distributed_train_step(dist_inputs):
    per_replica_losses = mirrored_strategy.run(model._train_step, args=(dist_inputs,))
    return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                                    axis=None)
dist_dataset = mirrored_strategy.experimental_distribute_dataset(train_dataset.dataset)
for dist_input in dist_dataset:
    output = distributed_train_step(dist_input)
    print(f'\r{output}')
```
Specifically it seems that a lot of time is spent for this operation (~5 seconds)
```
mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                                    axis=None)
```
as well as between calls of 
```
mirrored_strategy.run(model._train_step, args=(dist_inputs,))
```
where I very rudimentally print ""inside train step"" at the beginning of model._train_step and I see a print every ~ 1s per GPU (sequentially), so ~3s in total.
In a single GPU settings this train step (with the same total batch size) is performed in less than a second. 

When starting training everything seems fine

```
2021-02-26 13:31:14.658769: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-02-26 13:31:14.714487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:3b:00.0 name: Quadro RTX 5000 computeCapability: 7.5
coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 417.29GiB/s
2021-02-26 13:31:14.715401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:
pciBusID: 0000:af:00.0 name: Quadro RTX 5000 computeCapability: 7.5
coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 417.29GiB/s
2021-02-26 13:31:14.715624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-02-26 13:31:14.717261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2021-02-26 13:31:14.718644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2021-02-26 13:31:14.718874: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2021-02-26 13:31:14.720288: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2021-02-26 13:31:14.721085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2021-02-26 13:31:14.724158: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-02-26 13:31:14.727302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2021-02-26 13:31:14.727631: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2021-02-26 13:31:14.734379: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3200000000 Hz
2021-02-26 13:31:14.736749: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56099e0aead0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-02-26 13:31:14.736805: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-02-26 13:31:14.933093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:3b:00.0 name: Quadro RTX 5000 computeCapability: 7.5
coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 417.29GiB/s
2021-02-26 13:31:14.933919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:
pciBusID: 0000:af:00.0 name: Quadro RTX 5000 computeCapability: 7.5
coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 417.29GiB/s
2021-02-26 13:31:14.933976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-02-26 13:31:14.933988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2021-02-26 13:31:14.933999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2021-02-26 13:31:14.934009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2021-02-26 13:31:14.934019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2021-02-26 13:31:14.934029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2021-02-26 13:31:14.934040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-02-26 13:31:14.937073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2021-02-26 13:31:14.937121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-02-26 13:31:14.938810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-26 13:31:14.938835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1
2021-02-26 13:31:14.938840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y
2021-02-26 13:31:14.938843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N
2021-02-26 13:31:14.941865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15037 MB memory) -> physical GPU (device: 0, name: Quadro RTX 5000, pci bus id: 0000:3b:00.0, compute capability: 7.5)
2021-02-26 13:31:14.943851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15037 MB memory) -> physical GPU (device: 1, name: Quadro RTX 5000, pci bus id: 0000:af:00.0, compute capability: 7.5)
2021-02-26 13:31:14.945733: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56099d69c690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-02-26 13:31:14.945750: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 5000, Compute Capability 7.5
2021-02-26 13:31:14.945754: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Quadro RTX 5000, Compute Capability 7.5
2021-02-26 13:31:40.674717: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
```
Loss is actually decreasing so backprop seems to work just fine.

Here are the relevant code snippets:
```
mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    opt = tf.keras.optimizers.Adam(0.0001,
                                   beta_1=0.9,
                                   beta_2=0.98,
                                   epsilon=1e-9)
    model = config_manager.get_model()
    model.loss_weights = [1., 1.]
    model.compile(loss=[masked_mean_absolute_error,
                        new_scaled_crossentropy(index=2, scaling=8)],
                  loss_weights=model.loss_weights,
                  optimizer=opt)
```
all the loss functions have a SUM  reduction strategy

where train_dataset.dataset is this binned_data
```
dataset = tf.data.Dataset.from_generator(lambda: self._datagen(shuffle),
                                                 output_types=output_types)

binned_data = dataset.apply(
    tf.data.experimental.bucket_by_sequence_length(
        len_function,
        bucket_boundaries=bucket_boundaries,
        bucket_batch_sizes=bucket_batch_sizes,
        padded_shapes=padded_shapes,
        drop_remainder=drop_remainder,
        padding_values=padding_values
    ))
```

and model._train_step is
```
    def _train_step(self, dist_inputs):
        mel, phonemes, stop, sample_name = dist_inputs
        tar_inp = mel[:, :-1]
        tar_real = mel[:, 1:]
        tar_stop_prob = stop[:, 1:]

        mel_len = int(tf.shape(tar_inp)[1])
        tar_mel = tar_inp[:, 0::self.r, :]

        with tf.GradientTape() as tape:
            model_out = self.__call__(inputs=phonemes,
                                      targets=tar_mel,
                                      training=True)
            loss, loss_vals = weighted_sum_losses((tar_real,
                                                   tar_stop_prob),
                                                  (model_out['mel'][:, :mel_len, :],
                                                   model_out['stop_prob'][:, :mel_len, :]),
                                                  self.loss,
                                                  self.loss_weights)
            loss = loss / 64.
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        return loss
```

Everything seems according to the guide https://www.tensorflow.org/guide/distributed_training "
47433,Model.predict accepts tensors of incorrect rank,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab (Ubuntu 18.04)
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1
- Python version: 3.7.10

**Describe the current behavior**
* When defining a model with `tf.keras.Input(shape=[1])`, `model.predict` should only accept a tensor of shape `[batch,1]`, but it incorrectly accepts a tensor of shape `[batch]` (and appears to reshape it to `[batch, 1]`).
* When defining a model with `tf.keras.Input(shape=[2])`, `model.predict` should only accept a tensor of shape `[batch,2]`, but it incorrectly accepts a tensor of shape `[1, batch, 2]`. The erroneous extra dimension is retained in the value returned by `model.predict`.

(These two bugs feel closely related, which is why I'm reporting them together. If not, I can file two separate issues.)

**Describe the expected behavior**
When defining a model with `tf.keras.Input(shape=[d1,d2,...,dn])`, `model.predict` should only accept a tensor of shape `[batch,d1,d2,...,dn]`. All other shapes should raise an exception.

**Standalone code to reproduce the issue**
[The issues are shown in this Colab notebook.](https://colab.research.google.com/drive/1qP7wa2b7-8Sb5Z5ODNEXWZeQ0cDc-ttY?usp=sharing)

**Other info / logs**
If this is expected behavior, [it should be documented here](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict). It is not. And I hope it is not expected behavior.
"
47431,Failed to run models with libtensorflow_jni and tensorflow text together,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.1
- Python version:  n/a
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): gcc7
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
When using [libtensorflow_jni.so](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/BUILD#L437) with [tensorflow text](https://github.com/tensorflow/text), it crashes.

Same issue: https://github.com/tensorflow/java/issues/82

**Any other info / logs**
```
2021-02-17 19:20:45.283001: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at wordpiece_kernel.cc:204 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorflow6lookup15LookupInterfaceE got N10tensorflow6lookup15LookupInterfaceE
```"
47430,Cannot import name 'boosted_trees_test' from 'tensorflow_estimator.python.estimator.canned' ,"When I am trying to import boosted_trees_test, I face the following error:


`cannot import name 'boosted_trees_test' from 'tensorflow_estimator.python.estimator.canned' (C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow_estimator\python\estimator\canned\__init__.py)
`


Also, I could not find the boosted_trees_test in the latest TF package (2.4)"
47428,Normalization layer supresses ValueError when model is called with bad input shape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab notebook, and everywhere else tested
- TensorFlow installed from (source or binary):  Colab notebook, and everywhere else tested
- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1
- Python version: 3.7.10

**Describe the current behavior**
After adding a `tf.keras.layers.experimental.preprocessing.Normalization` layer to a model, badly shaped input is accepted by the model's `.predict`, generating only a warning log line, and returning some nonsense output.

**Describe the expected behavior**
After adding a `tf.keras.layers.experimental.preprocessing.Normalization` layer to a model, badly shaped input should be rejected by the model's `.predict`, raising a `ValueError`.

**Standalone code to reproduce the issue**
[Here is a Colab notebook that shows the issue](https://colab.research.google.com/drive/1ud_9UtZaTwmlL6WTnJkWMXG162DJ57kr?usp=sharing)"
47427,Pass for Dialect conversion from Tflite to TF  ,"Does there exists a Pass that can take back for Dialect conversion from Tflite to TF, TFlite->TF
We need something which preserves the quantization info generated during Tflite and detours back to TF where we can lower it down to HLO and Linalg thereon.
We don't need a new Dialect but just a Pass that can visit the Tflite IR and reconvert it back to TF getting back the Quantization info.
                CustomPass
TF->TFlite->TF->HLO->Linalg
What would be the other available ways to achieve this."
47426,In EXPO load coco-ssd model speed very slow,"Hello, I'm running a demo test in expo, but I found coco-ssh model loading is really slow or nothing happens. By the way I'm in China, is it caused by internet restrictions? Please give some help, thanks. 
this.model = await cocossd.load() // loading, no response

My code:
import React from 'react'
import {
  StyleSheet,
  Text,
  View,
  ActivityIndicator,
  StatusBar,
  Image,
  TouchableOpacity
} from 'react-native'
import * as tf from '@tensorflow/tfjs'
import { fetch } from '@tensorflow/tfjs-react-native'
import * as cocossd from '@tensorflow-models/coco-ssd'
import * as jpeg from 'jpeg-js'
import * as ImagePicker from 'expo-image-picker'
import Constants from 'expo-constants'
import * as Permissions from 'expo-permissions'

class App extends React.Component {
  state = {
    isTfReady: false,
    isModelReady: false,
    predictions: null,
    image: null
  }

  async componentDidMount() {
    await tf.ready()
    this.setState({
      isTfReady: true
    })
    this.model = await cocossd.load()
    this.setState({ isModelReady: true })
    this.getPermissionAsync()
  }

  getPermissionAsync = async () => {
    if (Constants.platform.ios) {
      const { status } = await Permissions.askAsync(Permissions.CAMERA_ROLL)
      if (status !== 'granted') {
        alert('Sorry, we need camera roll permissions to make this work!')
      }
    }
  }

  imageToTensor(rawImageData) {
    const TO_UINT8ARRAY = true
    const { width, height, data } = jpeg.decode(rawImageData, TO_UINT8ARRAY)
    // Drop the alpha channel info for mobilenet
    const buffer = new Uint8Array(width * height * 3)
    let offset = 0 // offset into original data
    for (let i = 0; i < buffer.length; i += 3) {
      buffer[i] = data[offset]
      buffer[i + 1] = data[offset + 1]
      buffer[i + 2] = data[offset + 2]

      offset += 4
    }

    return tf.tensor3d(buffer, [height, width, 3])
  }

  classifyImage = async () => {
    try {
      const imageAssetPath = Image.resolveAssetSource(this.state.image)
      const response = await fetch(imageAssetPath.uri, {}, { isBinary: true })
      const rawImageData = await response.arrayBuffer()
      const imageTensor = this.imageToTensor(rawImageData)
      const predictions = await this.model.classify(imageTensor)
      this.setState({ predictions })
      console.log(predictions)
    } catch (error) {
      console.log(error)
    }
  }

  selectImage = async () => {
    try {
      let response = await ImagePicker.launchImageLibraryAsync({
        mediaTypes: ImagePicker.MediaTypeOptions.All,
        allowsEditing: true,
        aspect: [4, 3]
      })

      if (!response.cancelled) {
        const source = { uri: response.uri }
        this.setState({ image: source })
        this.classifyImage()
      }
    } catch (error) {
      console.log(error)
    }
  }

  renderPrediction = prediction => {
    return (
      <Text key={prediction.className} style={styles.text}>
        {prediction.className}
      </Text>
    )
  }

  render() {
    const { isTfReady, isModelReady, predictions, image } = this.state

    return (
      <View style={styles.container}>
        <StatusBar barStyle='light-content' />
        <View style={styles.loadingContainer}>
          <Text style={styles.text}>
            TFJS ready? {isTfReady ? <Text></Text> : ''}
          </Text>

          <View style={styles.loadingModelContainer}>
            <Text style={styles.text}>Model ready? </Text>
            {isModelReady ? (
              <Text style={styles.text}></Text>
            ) : (
              <ActivityIndicator size='small' />
            )}
          </View>
        </View>
        <TouchableOpacity
          style={styles.imageWrapper}
          onPress={isModelReady ? this.selectImage : undefined}>
          {image && <Image source={image} style={styles.imageContainer} />}

          {isModelReady && !image && (
            <Text style={styles.transparentText}>Tap to choose image</Text>
          )}
        </TouchableOpacity>
        <View style={styles.predictionWrapper}>
          {isModelReady && image && (
            <Text style={styles.text}>
              Predictions: {predictions ? '' : 'Predicting...'}
            </Text>
          )}
          {isModelReady &&
            predictions &&
            predictions.map(p => this.renderPrediction(p))}
        </View>
        <View style={styles.footer}>
          <Text style={styles.poweredBy}>Powered by:</Text>
          <Image source={require('./assets/tfjs.jpg')} style={styles.tfLogo} />
        </View>
      </View>
    )
  }
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#171f24',
    alignItems: 'center'
  },
  loadingContainer: {
    marginTop: 80,
    justifyContent: 'center'
  },
  text: {
    color: '#ffffff',
    fontSize: 16
  },
  loadingModelContainer: {
    flexDirection: 'row',
    marginTop: 10
  },
  imageWrapper: {
    width: 280,
    height: 280,
    padding: 10,
    borderColor: '#cf667f',
    borderWidth: 5,
    borderStyle: 'dashed',
    marginTop: 40,
    marginBottom: 10,
    position: 'relative',
    justifyContent: 'center',
    alignItems: 'center'
  },
  imageContainer: {
    width: 250,
    height: 250,
    position: 'absolute',
    top: 10,
    left: 10,
    bottom: 10,
    right: 10
  },
  predictionWrapper: {
    height: 100,
    width: '100%',
    flexDirection: 'column',
    alignItems: 'center'
  },
  transparentText: {
    color: '#ffffff',
    opacity: 0.7
  },
  footer: {
    marginTop: 40
  },
  poweredBy: {
    fontSize: 20,
    color: '#e69e34',
    marginBottom: 6
  },
  tfLogo: {
    width: 125,
    height: 70
  }
})

export default App


package.json:
{
  ""dependencies"": {
    ""expo-gl"": ""~9.2.0"",
    ""jpeg-js"": ""0.3.6"",
    ""expo-constants"": ""~9.3.3"",
    ""@tensorflow/tfjs"": ""1.2.11"",
    ""expo-permissions"": ""~10.0.0"",
    ""expo-image-picker"": ""~9.2.0"",
    ""@tensorflow/tfjs-core"": ""^1.2.8"",
    ""react-native-image-picker"": ""1.1.0"",
    ""@tensorflow/tfjs-converter"": ""1.2.1"",
    ""@react-native-community/blur"": ""3.3.1"",
    ""@tensorflow-models/mobilenet"": ""2.0.4"",
    ""@tensorflow/tfjs-react-native"": ""0.1.0-alpha.2"",
    ""@react-native-community/async-storage"": ""~1.12.0"",
    ""@tensorflow-models/coco-ssd"": ""^2.2.1""
  }
}
"
47425,Disable unroll batch matmul pass,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): nightly

### 2. Code

https://colab.research.google.com/drive/1VxiMyDs5B_Oxcg29EmPSu9qgvPejCWqx?usp=sharing

### 3. Failure after conversion

It's all successful. But with unrolling (large batch dim), the model will be significant larger, and probably decrease the performance. I suppose it's just a workaround when tflite does not have batch matmul kernel, but we have it now."
47424,SELU activation in TFlite micro,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): **2.5**
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.** Currently no good alternative exists for deployment if micro. Batch Norm for micros seem to have som issues in certain Cortex M4 platforms.

**Will this change the current api? How?** It will add SELU activation to the supported ops in TF Lite Micro

**Who will benefit with this feature?** Embedded developers, interested in building NN's with multple layers.

**Any Other info.**
SELU activation as per [Klaumbauer et.al](https://arxiv.org/abs/1706.02515) provides self normalising, similar to Batch Normalisation, but requiring less compute, as it is an activation function. 
At the time of writing we haven't found alternatives, to running self normalising on microprocessor implementations. 

The application on wich the operator is desired is a predicitive maintainence system we are developing.
Our model is trained on field data captured from a large number of rotating machines, where the data mean and variance differs a lot, from from machine to machine, caused by mechanical tolerances and different mechanical transfer functions. Thus a degree of normalisation through the network is required, if we are to use identical models for detecting emerging defect in rotating components. 
We are using a convolutional model, fed with raw data, to be able to capture transient events, as opposed to traditional FFT based feature extraction, based on [this proposal](https://www.researchgate.net/publication/314247372_A_New_Deep_Learning_Model_for_Fault_Diagnosis_with_Good_Anti-Noise_and_Domain_Adaptation_Ability_on_Raw_Vibration_Signals) 

From what i can see the SELU could be implemented similarily to the ELU `/lite/kernels/elu.cc` that uses a lookuptable when performing inference, and the selu itself, works as sketched below  :


```
#define ALPHA   1.6733f
#define LAMBDA  1.0507f

inline void Selu(const RuntimeShape& input_shape, const float* input_data,
                const RuntimeShape& output_shape, float* output_data) {
  const int flat_size = MatchingFlatSize(input_shape, output_shape);
  for (int i = 0; i < flat_size; ++i) {
    const float val = input_data[i];
    output_data[i] = val < 0.0f ? LAMBDA * (ALPHA * std::expm1(val) - ALPHA) : LAMBDA * val;
  }
}
```
"
47422,conflict numpy version between tensorflow and tf-nighty,"When I install tf-nighty as below command:
`pip3 install tf-nightly`


the error appear as below:

```
Requirement already satisfied: zipp>=0.5 in /home/hoaphan/.local/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tb-nightly~=2.4.0.a->tf-nightly) (3.4.0)
Installing collected packages: numpy
  Attempting uninstall: numpy
    Found existing installation: numpy 1.18.5
    Uninstalling numpy-1.18.5:
      Successfully uninstalled numpy-1.18.5
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.19.5 which is incompatible.
Successfully installed numpy-1.19.5
```
this mean:
the tensorflow requires numpy <1.19.0
but the tf-nighty requires numpy 1.19.5

so I cannot install tensorflow and tf-nighty at same time,
so please help me solve this issue."
47421,"NotFoundError: It shows libcudart.so.8.0, but I am using CUDA 10.0.","Hello everyone.
I wanted to use TensorFlow 1.13.1, so I installed it in the following environment.
-----system information-----
OS:Ubuntu18.04.5 64bit
CPU:Intel Core  i7-10700 @2.90Ghz x 16
RAM:16GB
GPU:Gefoce RTX2060
CUDA:10.0.130
cuDNN:7.4
GPU driver:460.32.03
python2.7
-----------------------------------

-----The following error message will be displayed-----
tensorflow.python.framework.errors_impl.NotFoundError: libcudart.so.8.0: cannot open shared object file: No such file or directory
-------------------------------------------------
The TensorFlow website describes it as compatible with Python 2.7, 3.3 to 3.7 GCC 4.8 Bazel 0.19.2 cuDNN7.4 CUDA10.0. Why am I being suggested a CUDA 8.0 environment?
I have tried to install other versions of TensorFlow, but they all give me the same error.
Can someone please help me?
Thank you
"
47420,evaluate,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
47415,Changes to MklConvFwdPrimitiveFactory to support Arm Compute Library backend,"**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
At the moment MklConvFwdPrimitiveFactory reuses already created oneDNN primitives, stored in the vector `fwd_primitives`, with the help of [GetConvFwd](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/mkl/mkl_conv_ops.cc#L401-L404) function which creates a key from convolution parameters stored in MklConvFwdParams and compares this key with the keys of existing primitives. However, when oneDNN is built with [Arm Compute Library](https://github.com/ARM-software/ComputeLibrary) (ACL, see also oneDNN [build options](https://oneapi-src.github.io/oneDNN/dev_guide_build_options.html)) there is specific requirement to create a separate primitive per constant weights tensor, this is a restriction implied by ACL code design which does not allow to use primitive caching mechanism in MklConvFwdPrimitiveFactory as it is. The solution may be to add a new entry, `void* filter_address`, to [MklConvFwdParams](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/mkl/mkl_conv_ops.cc#L58-L93) struct and to include the address of weights to the key in [CreateKey](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/mkl/mkl_conv_ops.cc#L361-L399) function:

    ...
    key_creator.AddAsKey(convFwdDims.filter_dims);
    #ifdef DNNL_AARCH64_USE_ACL
    key_creator.AddAsKey(convFwdDims.filter_address);
    #endif
    key_creator.AddAsKey(convFwdDims.bias_dims);
    ...

**Will this change the current api? How?**
The only API change is the addition of a new field to [CreateKey](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/mkl/mkl_conv_ops.cc#L361-L399) and [MklConvFwdParams](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/mkl/mkl_conv_ops.cc#L58-L93) in `mkl_conv_ops.cc`, this is related to the limitations of ACL which is designed to create a separate primitive per weights tensor.

**Who will benefit with this feature?**
The users who run inference regime with TensorFlow on AArch64-based machines.

**Any Other info.**
This Issue is raised mainly to get feedback from maintainers of the oneDNN to TensorFlow integration."
47411,No algorithm worked on Ubuntu 20.04 LTS with CUDA 11.0 and Tensorflow 2.4.1,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes. It's the tensorflow.keras version of [mnist_convnet](https://keras.io/examples/vision/mnist_convnet/)
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04 LTS
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: installed via pip inside conda environment
-   **TensorFlow version (use command below)**: 2.4.1
-   **Python version**: 3.8.8
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: 11.0/8.04
-   **GPU model and memory**: Nvidia RTX 2080 Super Max-Q, 8G
-   **Exact command to reproduce**: See [mnist_convnet](https://keras.io/examples/vision/mnist_convnet/) (in which I have replaced all keras with tensorflow.keras)

### Describe the problem

I have installed tensorflow inside the conda base environment via `pip install --upgrade pip` and `pip install --upgrade tensorflow`, and the latter version is 2.4.1 (which should support CUDA 11.0). But as I tested the code from [mnist_convnet](https://keras.io/examples/vision/mnist_convnet/) (in which I have replaced all keras with tensorflow.keras), the code doesn't work and showed
```
NotFoundError:  No algorithm worked!
	 [[node sequential/conv2d/Conv2D (defined at <ipython-input-4-e30d719c0a87>:6) ]] [Op:__inference_train_function_728]

Function call stack:
train_function
```

Here are some strange things about it. 
1. I have no problem with running a dense neural network, as shown in [keras_example](https://www.tensorflow.org/datasets/keras_example). The training was smooth and fast.
2. I have no problem with CUDA 10.1. I mean after uninstall ingCUDA 11.0 and all associated CUDNN, and installing CUDA 10.1 and corresponding CUDNN, then the code in [mnist_convnet](https://keras.io/examples/vision/mnist_convnet/) worked.

### Source code / logs
```python
import numpy as np
from tensorflow import keras
import tensorflow as tf
from tensorflow.keras import layers


# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype(""float32"") / 255
x_test = x_test.astype(""float32"") / 255
# Make sure images have shape (28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""softmax""),
    ]
)
model.summary()

batch_size = 128
epochs = 15

model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
```
Then the error came:
```
NotFoundError                             Traceback (most recent call last)
<ipython-input-4-e30d719c0a87> in <module>
      4 model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])
      5 
----> 6 model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-> 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830       new_tracing_count = self.experimental_get_tracing_count()

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    886         # Lifting succeeded, so variables are initialized and we can run the
    887         # stateless function.
--> 888         return self._stateless_fn(*args, **kwds)
    889     else:
    890       _, _, _, filtered_flat_args = \

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2940       (graph_function,
   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
-> 2942     return graph_function._call_flat(
   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   2944 

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1916         and executing_eagerly):
   1917       # No tape is watching; skip to running the function.
-> 1918       return self._build_call_outputs(self._inference_function.call(
   1919           ctx, args, cancellation_manager=cancellation_manager))
   1920     forward_backward = self._select_forward_and_backward_functions(

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    553       with _InterpolateFunctionError(self):
    554         if cancellation_manager is None:
--> 555           outputs = execute.execute(
    556               str(self.signature.name),
    557               num_outputs=self._num_outputs,

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57   try:
     58     ctx.ensure_initialized()
---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

NotFoundError:  No algorithm worked!
	 [[node sequential/conv2d/Conv2D (defined at <ipython-input-4-e30d719c0a87>:6) ]] [Op:__inference_train_function_728]

Function call stack:
train_function
```


"
47407,Different outputs from GradientTape and CropAndResizeGradImage ,"**System information**
== tensorflow installed from info ==================
Name: tensorflow
Version: 2.4.1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/user/.venv/lib/python3.6/site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 6, 8, 'final', 0)


**Describe the current behavior**
I am calculating gradients using GradientTape.gradient() and comparing those against CropAndResizeGradImage().

**Describe the expected behavior**
This check passes for some configurations (like one given on https://www.tensorflow.org/api_docs/python/tf/image/crop_and_resize#example) not always.

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

def test_crop_and_resize(BATCH_SIZE=1, NUM_BOXES=41, IMAGE_DIM=15, CHANNELS=6, CROP_SIZE=8):

    image = tf.random.normal(shape=(BATCH_SIZE, IMAGE_DIM, IMAGE_DIM, CHANNELS) )
    boxes = tf.random.uniform(shape=(NUM_BOXES, 4))

    box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0, maxval=BATCH_SIZE, dtype=tf.int32)

    with tf.GradientTape() as g:
        g.watch(image)
        output = tf.image.crop_and_resize(image, boxes, box_indices, (CROP_SIZE, CROP_SIZE))

    grad_in = tf.random.normal(output.shape)
    grad_out = g.gradient(output, image, grad_in)
    grad_out_raw = tf.raw_ops.CropAndResizeGradImage(grads=grad_in, boxes=boxes, box_ind=box_indices, image_size=image.shape, T=tf.float32, method='bilinear')
    np.testing.assert_array_equal(grad_out.numpy(), grad_out_raw.numpy())

test_crop_and_resize()
```

```
E       AssertionError: 
E       Arrays are not equal
E       
E       Mismatched elements: 962 / 1350 (71.3%)
E       Max absolute difference: 3.6710215
E       Max relative difference: 320.43555
E        x: array([[[[ 4.556600e-01,  3.979062e-01,  1.036789e-01,  4.944029e-01,
E                  2.423330e-01,  1.300183e-01],
E                [ 5.906850e-01,  3.972412e-01,  2.205431e-01,  1.333644e+00,...
E        y: array([[[[ 4.556600e-01,  3.979062e-01,  1.036789e-01,  4.944029e-01,
E                  2.423330e-01,  1.300183e-01],
E                [ 5.906850e-01,  3.972412e-01,  2.205431e-01,  1.333644e+00,...
```"
47406,How to run a model training command in redhat linux slurm file?,"<em>Hello , I am trying to run a model training command  for object detection in cluster server. For this reason, I need to run a final command in a particular directory . As I am working on a cluster server, I am creating a slurm file. After the basic instructions, I  wrote the directory where the training command will be executed and now I am unable to connect the training command with the slurm file.</em>

![Slurrm _ssd](https://user-images.githubusercontent.com/68661510/109196179-ff605080-7760-11eb-963d-8e46fbb6fcd7.png)
"
47405,Login loop caused by unmet packages in gpu Ubuntu16.04 installation guide,"**System information**
- OS Platform and Distribution Linux Ubuntu 16.04:
- GeForce GTX TITAN 2070
Other inapplicable cause I following a guide: 
https://www.tensorflow.org/install/gpu?hl=ur#ubuntu_1604_cuda_110

**Describe the problem**
23th line 
`sudo apt-get install --no-install-recommends     cuda-11-0     libcudnn8=8.0.4.30-1+cuda11.0      libcudnn8-dev=8.0.4.30-1+cuda11.0`

leads to
`Reading package lists... Done
Building dependency tree       
Reading state information... Done
libcudnn8-dev is already the newest version (8.0.4.30-1+cuda11.0).
libcudnn8 is already the newest version (8.0.4.30-1+cuda11.0).
The following additional packages will be installed:
  cuda-demo-suite-11-0 cuda-drivers cuda-drivers-460 cuda-runtime-11-0
  nvidia-460 nvidia-460-dev
Recommended packages:
  nvidia-prime | bumblebee
The following packages will be REMOVED:
  nvidia-450
The following NEW packages will be installed:
  cuda-11-0 cuda-demo-suite-11-0 cuda-drivers cuda-drivers-460
  cuda-runtime-11-0 nvidia-460 nvidia-460-dev
0 upgraded, 7 newly installed, 1 to remove and 66 not upgraded.
Need to get 0 B/168 MB of archives.
After this operation, 93,2 MB of additional disk space will be used.
Do you want to continue? [Y/n] `

If 'yes' is choosen next reboot leads to login loop because of
""NVIDIA NVML Driver/library version mismatch""
nvidia 450 was installed at 19th line and nvidia 460 after 23th line

Deleting nvidia 460 allowing to login normally,with nvidia smi giving 450 version,
but 
```
from tensorflow.python.client import device_lib
device_lib.list_local_devices()
```
Still don't show accessible GPU.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
https://www.tensorflow.org/install/gpu?hl=ur#ubuntu_1604_cuda_110
except 19th line
`sudo apt-get install --no-install-recommends nvidia-driver-450`
due to
https://github.com/tensorflow/tensorflow/issues/47402
instead of it 
`sudo apt-get install --no-install-recommends nvidia-450`
And
`sudo apt-get update --allow-unauthenticated` before 2nd line
due to
`W: Failed to fetch https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/InRelease  Could not resolve host: developer.download.nvidia.com`
"
47404,"Layer .input_shape and .output_shape wrongly claim ""The layer has never been called""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2
- TensorFlow installed from (source or binary): https://github.com/apple/tensorflow_macos
- TensorFlow version (use command below): v1.12.1-44680-gc3fea33a21 2.4.0-rc0
- Python version: 3.8.6

**Describe the current behavior**
For a layer `l`, `l.input_shape` and `l.output_shape` raise the error ""AttributeError: The layer has never been called and thus has no defined input shape"", even though the layer has definitely been called.

**Describe the expected behavior**
After a layer `l` has been called, `l.input_shape` and `l.output_shape` return shapes.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
l = tf.keras.layers.Dense(units=2)
l(tf.ones((5,3)))  # Call the layer, defining the input shape
print(l.input_shape)  # Expected: (None, 3)
print(l.output_shape)  # Expected: (None, 2)
```

**Other info / logs**
* It's possible I'm using the API wrongly. But I'm pretty sure the claim ""The layer has never been called"" is false.
* This works as expected if I wrap the layer in a `tf.keras.Sequential`. Is there an assumption that the layer is inside a `tf.keras.Model`?"
47403,Tensorflow and flask has an error loading model.,"Hello 
According to this closed issue, https://github.com/tensorflow/tensorflow/issues/44467
I tried to install h5py == 2.10.0 version on my venv
But still got same error

![image](https://user-images.githubusercontent.com/75870530/109180833-46346180-77c6-11eb-9988-8925782f5bc8.png)
![image](https://user-images.githubusercontent.com/75870530/109180849-4cc2d900-77c6-11eb-83a7-0c35caad603b.png)
![image](https://user-images.githubusercontent.com/75870530/109180894-54827d80-77c6-11eb-9629-c21b75c8b590.png)

Please help me
Thanks All!"
47402,"Install gpu Ubuntu16.04 misses crucial steps: ""unable to locate package nvidia-driver-450""","## URL(s) with the issue:
https://www.tensorflow.org/install/gpu?hl=ur#ubuntu_1604_cuda_110
## Description of issue (what needs changing):

19th line:
`sudo apt-get install --no-install-recommends nvidia-driver-450`
leads to error: 
`E: Unable to locate package nvidia-driver-450`

"
47401,Burst mode execution in NNAPI,"How can we use the burst mode of execution while using the NNAPI delegate? There seems to be no API exposed to select the mode of execution and by default, the NNAPI delegate selects only between async and sync mode based on version. If we want to select a mode of execution through an android application, what should be the approach? "
47400,Compile TensorFlow Lite for iOS Simulator on Apple Silicon,"TensorFlowLite isn't available for iOS Simulator running on Apple Silicon.

It seems that this depends on [Bazel supporting this](https://github.com/bazelbuild/rules_apple/issues/980), as well as on TensorFlowLite being built as a .xcframework (fat frameworks have an arm64 slice, but cannot have an arm64-mac slice).

"
47399,Unable to build TFLite for Microcontrollers on Mac OS ,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac OS Mojave (version: 10.14.6)
- TensorFlow installed from (source or binary): from source 
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): zephyr_vexriscv (from [this tutorial](https://blog.tensorflow.org/2020/06/running-and-testing-tf-lite-on-microcontrollers.html)

**Describe the problem**
I have created my own test project base on [this tutorial](https://blog.tensorflow.org/2020/06/running-and-testing-tf-lite-on-microcontrollers.html), but when I build it using the following command: 
**make -f tensorflow/lite/micro/tools/make/Makefile TARGET=""zephyr_vexriscv"" ruby1_bin**
 
I have got this error message:
tensorflow/lite/micro/tools/make/Makefile:417: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:417: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/zephyr_vexriscv_x86_64/bin/ruby1 tensorflow/lite/micro/tools/make/gen/zephyr_vexriscv_x86_64/obj/tensorflow/lite/micro/examples/ruby1/main.o tensorflow/lite/micro/tools/make/gen/zephyr_vexriscv_x86_64/obj/tensorflow/lite/micro/examples/ruby1/model_28.o tensorflow/lite/micro/tools/make/gen/zephyr_vexriscv_x86_64/lib/libtensorflow-microlite.a  -lm
Undefined symbols for architecture x86_64:
  ""_stbi_load"", referenced from:
      _main in main.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [tensorflow/lite/micro/examples//ruby1/Makefile.inc:13: tensorflow/lite/micro/tools/make/gen/zephyr_vexriscv_x86_64/bin/ruby1] Error 1

"
47396,BestExporter exports worse model when training tasks restart.,"BestExporter is widely used in my company to avoid model degradation with time. But BestExporter exports worse model when training task restart.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution: Distributed training.
- TensorFlow installed from (source or binary):binary.
- TensorFlow version (use command below): All versions(TF1.14, TF1.15, TF2.x).

**Describe the current behavior**

There are scenes for example we need to restart training tasks: 

1. continuous training(not online streaming training): Training with new data and check if its satisfies the best exporter condition.
2. Failure restart: In local or distributed training, worker failure is common, thanks to the checkpoint, we can restart failed task.

For example, old best AUC is 0.85, but when we restart training task,  [self._has_exported](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/exporter.py#L295) in BestExporter is False, and TF will export new saved model ignoring whether current eval_result(for example AUC=0.79) is better than best_result(AUC=0.80)  loaded from event files created by last running task or not. And next all eval_results(for example AUC=0.80, AUC=0.81) are worse than old AUC=0.85 unfortunately which means all newer saved_models are worse than old one and may cause worse online business metrics.

This bug was imported in [issue](https://github.com/tensorflow/estimator/pull/41)  and  [this commit](https://github.com/tensorflow/estimator/commit/43921b4552d1c30acc31d3b5989112cb397383e0#diff-5e0426432b8f40323ca256ef0d11b27ec3a7b4cc9d4042d228b17d0b13d73cfb).

**Describe the expected behavior**

Export REAL best model.

**Standalone code to reproduce the issue**

Has been proved in production environment.

"
47395,tflite api doesnt return input shape in c++,"I am using a functional model which  I converted it to tflite  in tf2.4.0 but I dont get shape of input tensor in c++ but Its  visible in python

```
interpreter = tf.lite.Interpreter(model_path=""model.tflite"")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
print(input_details)
print(input_details[0]['shape'])  ##Returns  shape of input tensor [1  100 100 1]
```

```
std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""model.tflite"");
tflite::ops::builtin::BuiltinOpResolver resolver;
tflite::InterpreterBuilder builder(*model, resolver);
std::unique_ptr<tflite::Interpreter> interpreter;
builder(&interpreter);
interpreter->AllocateTensors();
std::cout <<""Input variable name: ""<<interpreter->GetInputName(0)<<std::endl; // Works
std::vector<int> inputs_vec = interpreter->inputs();  // Returns [0]. Size of vector is 1


```
"
47394,Simple Example with CIFAR-10. ImageDataGenerator + 'sparse_categorical_crossentropy',"**System information**
- TensorFlow version (use command below): 2.4

I try example CIFAR-10 with ImageDataGenerator. But it doesn't works. 

My custom CNN Model is not training when I use **ImageDataGenerator + sparse_categorical_crossentropy**.
However, when I use **ImageDataGenerator + categorical_crossentropy**, it works well.

what happen in this code?

Thank you and my code is below.

------------------------------------------

```
from tensorflow.keras.datasets import cifar10
import numpy as np

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

x_mean = np.mean(x_train, axis = (0, 1, 2))
x_std = np.std(x_train, axis = (0, 1, 2))

x_train = (x_train - x_mean) / x_std
x_test = (x_test - x_mean) / x_std

from sklearn.model_selection import train_test_split

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, 
                                                  test_size = 0.3, random_state = 777)

print('data ready~')

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(horizontal_flip = True,
                                   zoom_range = 0.2,
                                   width_shift_range = 0.1,
                                   height_shift_range = 0.1,
                                   rotation_range = 30,
                                   fill_mode = 'nearest'
                                  )
val_datagen = ImageDataGenerator()

batch_size = 32

train_generator = train_datagen.flow(x_train, y_train,
                                    batch_size = batch_size)
val_generator = val_datagen.flow(x_val, y_val,
                                batch_size = batch_size)


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Activation, BatchNormalization
from tensorflow.keras.optimizers import Adam

model = Sequential()

model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', input_shape = (32, 32, 3)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))

model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))

model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))

model.add(Flatten())
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dense(10, activation = 'softmax'))

model.compile(optimizer = Adam(1e-4),
             loss = 'sparse_categorical_crossentropy',
             metrics = ['acc'])

def get_step(train_len, batch_size):
    if(train_len % batch_size > 0):
        return train_len // batch_size + 1
    else:
        return train_len // batch_size

history = model.fit(train_generator,
                    epochs = 100,
                    steps_per_epoch = get_step(len(x_train), batch_size),
                    validation_data = val_generator,
                    validation_steps = get_step(len(x_val), batch_size))
```
"
47393,EagerTensor implements `__len__` but not `len()`.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux merry 5.8.0-43-generic #49~20.04.1-Ubuntu SMP Fri Feb 5 09:57:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux`
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version (use command below): `v2.4.0-49-g85c8b2a817f 2.4.1`
- Python version: `3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0]`

**Describe the current behavior**
The EagerTensor implements `__len__` but does not provide the `len(.)` function.

**Describe the expected behavior**
If `__len__` is a attribute, the `len(.)` function should work.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
import sys

print(tf.version.GIT_VERSION, tf.version.VERSION) # v2.4.0-49-g85c8b2a817f 2.4.1
print(sys.version) # 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0]
x = np.dtype('float32').type(3.0)

sigmoid = tf.keras.layers.Activation(activation=""sigmoid"")
actX = sigmoid(x)

print(type(x)) # <class 'numpy.float32'>
print(type(actX)) # <class 'tensorflow.python.framework.ops.EagerTensor'>

if hasattr(actX, ""__len__""):
    print(len(actX))
else:
    print(""Has no length"")
```
yield the following error:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-a34ef039f8c4> in <module>
      9 
     10 if hasattr(actX, ""__len__""):
---> 11     print(len(actX))
     12 else:
     13     print(""Has no length"")

~/Projects/NotebooksEnvs/py38Env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in __len__(self)
   1020     """"""Returns the length of the first dimension in the Tensor.""""""
   1021     if not self.shape.ndims:
-> 1022       raise TypeError(""Scalar tensor has no `len()`"")
   1023     # pylint: disable=protected-access
   1024     try:

TypeError: Scalar tensor has no `len()`
```


"
47392,Did not open image after detecting object using tensorflow2,"I am learning object [detetction with tensorflow2](https://gilberttanner.com/blog/tensorflow-object-detection-with-tensorflow-2-creating-a-custom-model) and my model trained successfully.

But whenever I trying to [Test trained model on test images](https://colab.research.google.com/github/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model/blob/master/Tensorflow_2_Object_Detection_Train_model.ipynb#scrollTo=5tGVwzpLxvSv) in my _VScode_, image doesn't open.

### Here is my code:

```
import io
import os
import cv2
import scipy.misc
import numpy as np
import six
import time
import glob
from IPython.display import display

from six import BytesIO

import matplotlib
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageFont

import tensorflow as tf
from object_detection.utils import ops as utils_ops
from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as vis_util

def load_image_into_numpy_array(path):
  """"""Load an image from file into a numpy array.

  Puts image into numpy array to feed into tensorflow graph.
  Note that by convention we put it into a numpy array with shape
  (height, width, channels), where channels=3 for RGB.

  Args:
    path: a file path (this can be local or on colossus)

  Returns:
    uint8 numpy array with shape (img_height, img_width, 3)
  """"""
  img_data = tf.io.gfile.GFile(path, 'rb').read()
  image = Image.open(BytesIO(img_data))
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)

category_index = label_map_util.create_category_index_from_labelmap('training/label_map.pbtxt', use_display_name=True)

tf.keras.backend.clear_session()
model = tf.saved_model.load(f'inference_graph/saved_model')

def run_inference_for_single_image(model, image):
  image = np.asarray(image)
  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.
  input_tensor = tf.convert_to_tensor(image)
  # The model expects a batch of images, so add an axis with `tf.newaxis`.
  input_tensor = input_tensor[tf.newaxis,...]

  # Run inference
  model_fn = model.signatures['serving_default']
  output_dict = model_fn(input_tensor)

  # All outputs are batches tensors.
  # Convert to numpy arrays, and take index [0] to remove the batch dimension.
  # We're only interested in the first num_detections.
  num_detections = int(output_dict.pop('num_detections'))
  output_dict = {key:value[0, :num_detections].numpy() 
                 for key,value in output_dict.items()}
  output_dict['num_detections'] = num_detections

  # detection_classes should be ints.
  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)
   
  # Handle models with masks:
  if 'detection_masks' in output_dict:
    # Reframe the the bbox mask to the image size.
    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
              output_dict['detection_masks'], output_dict['detection_boxes'],
               image.shape[0], image.shape[1])      
    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,
                                       tf.uint8)
    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()
    
  return output_dict
  
for image_path in glob.glob('images/test/*.jpg'):
  image_np = load_image_into_numpy_array(image_path)
  output_dict = run_inference_for_single_image(model, image_np)
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks_reframed', None),
      use_normalized_coordinates=True,
      line_thickness=8)
  display(Image.fromarray(image_np))
```

### After run the python file I got below output in cmd:

> <PIL.Image.Image image mode=RGB size=500x625 at 0x2090689B040>
> <PIL.Image.Image image mode=RGB size=242x239 at 0x2090689B040>
> <PIL.Image.Image image mode=RGB size=238x238 at 0x2090689B040>
> <PIL.Image.Image image mode=RGB size=489x368 at 0x2090689B040>
> <PIL.Image.Image image mode=RGB size=433x358 at 0x2090689B040>
> <PIL.Image.Image image mode=RGB size=486x486 at 0x2090689B040>
> <PIL.Image.Image image mode=RGB size=492x487 at 0x2090689B040>
> <PIL.Image.Image image mode=RGB size=155x357 at 0x2090689B040>

**_I didn't get expected output which is shown [here](https://colab.research.google.com/github/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model/blob/master/Tensorflow_2_Object_Detection_Train_model.ipynb#scrollTo=EEX-m3P1yp4y&line=5&uniqifier=1)._** Please help me to solve this."
47391,tf.errors.UnknownError should include __cause__,"**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

When a [`tf.errors.UnknownError`](https://www.tensorflow.org/api_docs/python/tf/errors/UnknownError) is raised, the `exception.__cause__` is not set (it is `None`), so if someone wants to find the underlying error, the best they can do is trying to infer it through the `exception.message` attribute, which is suboptimal at best.

`exception.__cause__` should be set to the chained exception when known. See https://docs.python.org/3/library/exceptions.html.

**Will this change the current api? How?**

It will change the `tf.errors.UnknownError` behaviour to include more information.

**Who will benefit with this feature?**

Everyone who wants more control over `UnknownError`. For example, it is currently not possible for me to detect that the `UnknownError` is caused due to a `PIL.UnidentifiedImageError` unless I find the string in the `message`.

**Any Other info.**

Code causing the `UnknownError` that prompted the request:

```python
def my_generator(img):
    img = Image.open(io.BytesIO(img))  # <- here's the `PIL.UnidentifiedImageError`
    img = tf.image.convert_image_dtype(img, tf.float32)
    yield img, tf.constant([[]], dtype=tf.float32), tf.constant([], dtype=tf.int32)

ds = tf.data.Dataset.from_generator(lambda: my_generator((image_bytes))
# ...trying to use ds...
```
"
47390,TypeError: tensor_equals() missing 1 required positional argument: 'other',"
![image](https://user-images.githubusercontent.com/18486587/109113616-895ce900-7762-11eb-9882-5f86a866d871.png)

On loading the saved model, it gives an error. 
Tensorflow version is 2.4.1
[Notebook to reproduce it. ](https://colab.research.google.com/drive/1w5QAIeG6rUpSWUbHEJJjNLfp1FYzCwff?usp=sharing)"
47387,"Unable to migrate TF1 code to TF2, tape.gradient returns None","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos big sur
- TensorFlow installed from (source or binary): binary, pip
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.7

**Describe the current behavior**
I'm migrating code from TF1 that uses `tf.gradients()` for doing a custom gradient calculation. I'm trying to get the same results I get with TF1 in TF2 using `tf.GradientTape()` however, no matter what I do including:
- I tried to use `tape.watch()` and the issue persists.
- I tried manually creating a `tf.Variable()` with `trainable=True`, watch the variable and the issue persists.
- I tried using `tf.gradients()` within a `tf.function` and `tf.compat.v1.gradients()` if there is any difference at all, and the issue persists

Here's a jupyter [notebook](https://colab.research.google.com/drive/1eeEc9eIlvMVAJbueIfhpgh569pi-lGx4?usp=sharing) with the full code to be able to reproduce the issue.

Here's the [code](https://github.com/openai/baselines/blob/master/baselines/acer/acer.py) I'm migrating. Check lines [156 - 176]. Below is the part of interest:

```
    g = tf.gradients(-loss, f)  # loss being a float and f being a (m, n) tensor
    k = -f_pol / (f + eps)  # f_pol another (m, n) tensor and eps a float
    k_dot_g = tf.reduce_sum(k * g, axis=-1)
    adj = tf.maximum(
        0.0,
        (tf.reduce_sum(k * g, axis=-1) - delta)
        / (tf.reduce_sum(tf.square(k), axis=-1) + eps),
    )
    g = g - tf.reshape(adj, [nenvs * nsteps, 1]) * k
    grads_f = -g / (nenvs * nsteps)
    grads_policy = tf.gradients(f, params, grads_f)  # params being the model parameters
```

and here's a simplified version of what I'm trying to do:

    with tf.GradientTape() as tape:
        f = calculate_f()
        f_pol = calculate_f_pol()
        others = do_further_calculations()
        loss = calculate_loss()
    g = tape.gradient(-loss, f)
    print(g)

results in:

    None

**Describe the expected behavior**

As far as I understand `tf.GradientTape()` is the TF2 alternative to `tf.gradients()`. I'm trying to replicate the exact same results in TF2 and it doesn't work, so this implies either there is something wrong with my code or it is a bug. This is not the first time it happens with someone, I found numerous other complains including closed issues and neither includes a solution I have not tried.
 
**Standalone code to reproduce the issue**
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

jupyter [notebook](https://colab.research.google.com/drive/1eeEc9eIlvMVAJbueIfhpgh569pi-lGx4?usp=sharing) 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47386,[XLA:GPU] Build failure (bazel cache end up being corrupt),"This commit:

```
commit 9f8d5d0ef4e990ab2398d573a14aaa8ed02a10e2
Author: Rahul Joshi <jurahul@google.com>
Date:   Mon Feb 22 08:41:59 2021 -0800

    [MLIR:LHLO] Add optional call target arg mapping to LMHLO CustomCall operations.

    - XLA:HLO -> LMHLO conversion drops all token arguments and return values, however
      custom calls that users write still expect to get buffer pointers for these token types.
    - To be able to support this, add an optional call target argument mapping attribute to
      LMHLO custom calls. When this attribute is present, it indicates the number of
      arguments and returns that the custom call expects and also indicates which LMHLO
      arg() or output() maps to which arg or result number of the custom call.

    PiperOrigin-RevId: 358826664
    Change-Id: I36e839e9ff5b73890715b71717a4c13631955fba
```

Introduced a compilation regression. Before that commit, this command line works:

```
bazel test -c opt --config=cuda //tensorflow/compiler/xla/tests:conv_depthwise_test_gpu //tensorflow/compiler/xla/tests:grouped_convolution_test_gpu
```

Starting at this commit, it gives this error:
```
ERROR: /home/fbastien/github/tensorflow-tf2-upstream2/tensorflow/compiler/mlir/hlo/BUILD:485:11: undeclared inclusion(s) in rule '//tensorflow/compiler/mlir/hlo:lhlo':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/lhlo_ops_structs.cc':
  'bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'
  'bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc'
cc1plus: warning: command line option -Wno-pointer-sign is valid for C/ObjC but not for C++
```

@jurahul that authored it."
47383,ModifyGraphWithDelegate(delegate) running very slow (ubuntu with USB accelerator),"Hi, 
I wrote a minimal C++ code loading the mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite
and doing some inferencing.
When running on a ubuntu-16.04 notebook (i7, SSD, 12G) with USB accelerator,
interpreter_->ModifyGraphWithDelegate
runs substantially (50x) slower than the same(see below) code on the dev-board.

Dev-board:
Code line: interpreter_->ModifyGraphWithDelegate({delegate, edgetpu_free_delegate});
libedgetpu version: libedgetpu1-std:arm64                14.1
run time: 0.07sec (incl. loading network etc)

Ubuntu 16.04, 
Code line: interpreter_->ModifyGraphWithDelegate(delegate);
[ the version above with the 2 delegates can't be compiled]
libedgetpu: libedgetpu1-max/coral-edgetpu-stable,now 15.0 amd64 [installed]
TENSORFLOW: tensorflow_src cloned today and built tflite library
runtime:  3.2sec (!)

After that, the inference is running about only half as fast as on the devboard. 
(slower USB speed??? or problem with the delegate?)

Code snippet:
 std::unique_ptr<tflite::FlatBufferModel> model;
  model = tflite::FlatBufferModel::BuildFromFile(model_path.c_str());

  tflite::ops::builtin::BuiltinOpResolver resolver;
  tflite::InterpreterBuilder(*model, resolver)(&interpreter_);

  size_t num_devices;
  std::unique_ptr<edgetpu_device, decltype(&edgetpu_free_devices)> devices(
      edgetpu_list_devices(&num_devices), &edgetpu_free_devices);
  TFLITE_MINIMAL_CHECK(num_devices);
  const auto& device = devices.get()[0];
  auto* delegate = edgetpu_create_delegate(device.type, device.path, nullptr, 0);
  ON DEVBOARD:  interpreter_->ModifyGraphWithDelegate({delegate, edgetpu_free_delegate});
  ON UBUNTU/USB: interpreter_->ModifyGraphWithDelegate(delegate);

Thank you!
-johann

"
47381,Generating the descriptors of constant shape,"The local features generated by tensorflowv1 is different from tensorflowv2.5

```
def generate_vectors(paths):
    
    ops.reset_default_graph()
    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.FATAL)

    model = hub.Module('https://tfhub.dev/google/delf/1')

    image_placeholder = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 3), name='input_image')

    module_inputs = {
        'image': image_placeholder,
        'score_threshold': 100.0,
        'image_scales': [0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0],
        'max_feature_num': 1000,
    }

    module_outputs = model(module_inputs, as_dict=True)
    image_tf = paths_to_image_loader(list(paths))
    img_paths = []
    with tf.compat.v1.train.MonitoredSession() as sess:
        features = []

        for i in tqdm(range(len(paths))):
            image = sess.run(image_tf)
            local_feature = sess.run([module_outputs['locations'], module_outputs['descriptors']],
                                     feed_dict={image_placeholder: image})

```

The local_feature[1] and local_feature[0] has got a constant shape of 1000 with the above code that has the legacy cide,.

I used the latest code to generate vectors as follow,

```
def run_delf(image):
    np_image = np.array(image)
    float_image = tf.image.convert_image_dtype(np_image, tf.float32)

    return delf(
        image=float_image,
        score_threshold=tf.constant(100.0),
        image_scales=tf.constant([0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0]),
        max_feature_num=tf.constant(1000))
```

Unfortunately `result[""locations""].numpy()` and `result[""descriptors""].numpy()` always has got a different shapes for every image inputs while it is intended to have 1000 features. How can I fix it?"
47376,No version of tensorflow will install on python 3.7.9 x64 / pip 20.1,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `OS X 11.1 (PC)`
- TensorFlow installed from (source or binary): `pip`
- TensorFlow version: `1.15 or any other`
- Python version: `3.7.9`
- Installed using virtualenv? pip? conda?: `pip`

**Describe the problem**

installed python 3.7.9 as compatible platform with TF 1.15 via `pyenv`
no version of tf will install at all:

```
:~$ python3 -VV
Python 3.7.9 (default, Feb 24 2021, 13:04:10)
[Clang 12.0.0 (clang-1200.0.32.29)]
:~$ python3 -c 'import sys;print(""%x"" % sys.maxsize, sys.maxsize > 2**32)'
7fffffffffffffff True
:~$ python3 -m pip install tensorflow==1.15
ERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: none)
ERROR: No matching distribution found for tensorflow==1.15
```
same output with just `pip3 install tensorflow`

Is there something else I am overlooking as far the python/pip version/build type?
"
47373,"AttributeError: Can't set the attribute ""name"" when building bidirectional layer with stackedrnncell","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
On Jupyter Notebook, Python 3.6, Tensorflow Version 2.4

**Describe the current behavior**
I am currently trying to build a bidirectional layer using stackedrnncell. However, I am running to an attribute error but the error message is not helping me in debugging this error. 

**Describe the expected behavior**
Would expect the model to be build. 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

lstm_1 = tf.keras.layers.LSTMCell(128)
lstm_2 = tf.keras.layers.LSTMCell(128)
lstm_fw_cell = tf.keras.layers.StackedRNNCells([lstm_1, lstm_2])
lstm_fw = tf.keras.layers.RNN(lstm_fw_cell,unroll=True)

lstm_3 = tf.keras.layers.LSTMCell(128)
lstm_4 = tf.keras.layers.LSTMCell(128)
lstm_bw_cell= tf.keras.layers.StackedRNNCells([lstm_3, lstm_4])
lstm_bw = tf.keras.layers.LSTM(lstm_bw_cell,go_backwards=True,unroll=True)

model = Sequential()
model.add(Bidirectional(lstm_fw, backward_layer=lstm_bw))
model.add(Dense(4, activation=""softmax""))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

![image](https://user-images.githubusercontent.com/41911024/109015802-7a3d5300-76f0-11eb-9ed7-b6a7b3f34f04.png)
"
47371,tf_upgrade_v2: UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 385: character maps to <undefined>,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6.1
- CUDA/cuDNN version: 11.1
- GPU model and memory: RTX3070 8GB

I tried to update my project with script tf_upgrade_v2 and command:
```
tf_upgrade_v2  --intree D:\Work\in_dir --outtree D:\Work\out_dir --reportfile D:\Work\report.txt --print_all
```
but I get error:
```
Traceback (most recent call last):
  File ""c:\python36\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\python36\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\python36\Scripts\tf_upgrade_v2.exe\__main__.py"", line 7, in <module>
  File ""c:\python36\lib\site-packages\tensorflow\tools\compatibility\tf_upgrade_v2_main.py"", line 178, in main
    args.input_tree, output_tree, args.copy_other_files)
  File ""c:\python36\lib\site-packages\tensorflow\tools\compatibility\ast_edits.py"", line 1076, in process_tree
    _, l_report, l_errors = self.process_file(input_path, output_path)
  File ""c:\python36\lib\site-packages\tensorflow\tools\compatibility\ast_edits.py"", line 921, in process_file
    temp_file)
  File ""c:\python36\lib\site-packages\tensorflow\tools\compatibility\ast_edits.py"", line 982, in process_opened_file
    lines = in_file.readlines()
  File ""c:\python36\lib\encodings\cp1250.py"", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 385: character maps to <undefined>
```

How to check which file in my project is troublesome?
"
47370,True loss values,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.15.3 and 2.4.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
The log loss values of the training dataset recorded during model fitting are not the actual loss values obtained when evaluating the resulting model. I explain this througly in https://github.com/sparklingdew/keras_loss_discrepancy.

**Will this change the current api? How?**
The only change that I suggest is to give the actual loss value for the training dataset, rather than this other internal value which is difficult to understand. It should be calculated similarly to the validation loss, which is obtained from evaluating the resulting model. I understand that this may go in detriment to running time. In that case, this new feature could be optional.

**Who will benefit with this feature?**
NN users will be benefitted, since model tunning is performed based on loss and accuaracy results. If these values are misleading, the models are difficult to tune.

**Any Other info.**
Many thanks for your time!"
47369,"sorry for mistake reference issue, please delete it",
47368,Tensorflow is ignoring `tf.config.set_soft_device_placement(True)`,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (GPU)
- TensorFlow installed from (source or binary): Google Colab (GPU)
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.10
- CUDA/cuDNN version: Google Colab (GPU)
- GPU model and memory: Google Colab (GPU)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:

**Describe the current behaviour**

I'm training a neural network with embedding layer using GPU-distributed strategy (TF's MirroredStrategy), and I'm getting the following error:

```
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
GatherV2: GPU CPU 
Cast: GPU CPU 
Const: GPU CPU 
ResourceSparseApplyAdagradV2: CPU 
_Arg: GPU CPU 
ReadVariableOp: GPU CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  model_6_user_embedding_embedding_lookup_readvariableop_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  adagrad_adagrad_update_1_update_0_resourcesparseapplyadagradv2_accum (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  model_6/User-Embedding/embedding_lookup/ReadVariableOp (ReadVariableOp) 
  model_6/User-Embedding/embedding_lookup/axis (Const) 
  model_6/User-Embedding/embedding_lookup (GatherV2) 
  gradient_tape/model_6/User-Embedding/embedding_lookup/Shape (Const) 
  gradient_tape/model_6/User-Embedding/embedding_lookup/Cast (Cast) 
  Adagrad/Adagrad/update_1/update_0/ResourceSparseApplyAdagradV2 (ResourceSparseApplyAdagradV2) /job:localhost/replica:0/task:0/device:GPU:0

     [[{{node model_6/User-Embedding/embedding_lookup/ReadVariableOp}}]] [Op:__inference_train_function_2997]
```


From the error log, it looks like the embedding_lookup/ReadVariableOp is not GPU compatible - which I can understand why. 

But I've set `tf.config.set_soft_device_placement(True)` so I'm expecting TF to use CPU for CPU only operations, and that doesn't seemed to have worked. Is TF ignoring that config setting? Or does that setting doesn't yet support some other conditions? 


**Describe the expected behaviour**

I'd expect MirroredStrategy to default to CPU for CPU only computations by setting `tf.config.set_soft_device_placement(True)` but that doesn't seem to be happening.


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.


Link to Google Colab notebook:
https://colab.research.google.com/drive/1ZN1HzSTTfvA_zstuI-EsKjw7Max1f73v?usp=sharing

Dataset can be found on Kaggle:
https://www.kaggle.com/zygmunt/goodbooks-10k


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Full log can be seen on Google Colab by downloading the dataset from kaggle, uploading to google colab, and running the code on GPU. The neural network is really small -takes less than 5 seconds to execute the whole notebook.

"
47366,metrics values don't match between custom metric function and model.fit callbacks,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Google Colab default installed
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**  
returned metrics value from custom metrics functions and callback printed metrics from model.fit doesn't match.

**Describe the expected behavior**  
returned metrics value from custom metrics functions and callback printed metrics from model.fit match.

**Standalone code to reproduce the issue**  
[Google Colab gist](https://gist.github.com/asterisk37n/9e8c492fc7f01a9f37f9d5fb33ee3406)

**Other info / logs**  
I opened a quesiton on stackoverflow.  
[https://stackoverflow.com/questions/66311611/when-and-how-keras-calculate-metrics-for-each-batch-of-samples](https://stackoverflow.com/questions/66311611/when-and-how-keras-calculate-metrics-for-each-batch-of-samples)"
47365,Getting ERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- MacBook pro
- processor: 2GHz Quad-Core Intel Core i5,  
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I'm successfully able to install 1.8.0, 
getting below error with all available wheel from mac os, supported wheel for my mac is cp39, which is not avilable. 
tensorflow-2.4.1-cp36-cp36m-macosx_10_11_x86_64.whl is not a supported wheel on this platform.


**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47364,ssd mobilenet model size is not correct using tflite converter python api,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1

### 2. Code

My script [SSD mobilenet convert to TF lite colab script](https://colab.research.google.com/drive/1qaGW7ViN0fW2z4bqRKL4sd8X-HIq5Vm2?usp=sharing)


### 3. Failure after conversion

convert offical SSD MobileNet v2 320x320 model using python api from [tf2_detection_zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md), conversion is successful, but tflite file size is too small (only 6.4 MB) and run on mobile extremely slow (900 ms).

And I found following result:

* if **convert to saved model and tflite both using tf 2.4**
   * using tflite converter python api is 6.4M
   * using command line tool (tflite_converter) is 324 bytes
   * **both file is NOT correct**
* if **convert to saved model using TF 2.3 and convert to tflite using tf 2.4**
   * using tflite converter python api is 6.4M
   * using command line tool (tflite_converter) is 24M
   * **the command line tool transform result is correct** which run on mobile very fast (100 ms)!

saved model convert script

```bash=
python object_detection/export_tflite_graph_tf2.py --pipeline_config_path ./ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config --trained_checkpoint_dir ./ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ --output_directory ./output/
```

tflite converter python api convert code

```python=
converter = tf.lite.TFLiteConverter.from_saved_model('output/saved_model/',signature_keys=[""serving_default""])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()

with tf.io.gfile.GFile('output/detect_using_api.tflite', 'wb') as f:
  f.write(tflite_model)
```

command line tool convert script

```bash=
tflite_convert --output_file output/detect_using_command.tflite --saved_model_dir output/saved_model/ --experimental_new_converter True
```

My question is

Why output result is different between using tflite converter python api and command line tool?

### 5. (optional) Any other info / logs
N/A
"
47363,Any suggestion to use tensorflow feasibly?,"I am currently doing a research while it is based on a project written with Tensorflow. While I could not implement the dynamic time wrapping easily since it is a dynamic method. With pytorch I could implement it very quickly (since pytorch supports dynamic graph). 

Could anyone help me choose the correct version and documents of Tensorflow that supports dynamic graph? I checked on Tensorflow official website while the document is kind of messy. "
47362,Ampere bfloat16 support,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.1/built from head
- Are you willing to contribute it (Yes/No): If you point me at the necessary code, I'll take a crack at it.



**Describe the feature and the current behavior/state.**
I attempted to use bfloat16 training by setting:

```
tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')
```

I got this error message:

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Conv2D' used by {{node model/conv2d/Conv2D}} with these attrs: [dilations=[1, 1, 1, 1], T=DT_BFLOAT16, data_format=""NHWC"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, explicit_paddings=[], padding=""SAME""]
Registered devices: [CPU, GPU]
Registered kernels:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]
  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
```

I read the [cuDNN release notes](https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html#rel-810) which said that support for bfloat16 came with cuDNN version 8.1, so I built tensorflow from scratch, using CUDA 11.2 and cuDNN 8.1.0.77.

However, I got the same error message.  I've tried this on a trival example (the fashion mnist tutorial) with the same result, so it's not some advanced kernel that I'm using in my code.

I am a little confused that I'm getting this error message even when running with XLA, since the error message says that XLA_GPU_JIT supports DT_BFLOAT16.


**Will this change the current api? How?**
No, you can already specify a bfloat16 mixed precision policy.
**Who will benefit with this feature?**
Anyone with Ampere gpus who wants the speed of 16 bit floats and the numerical stability of bfloat16.
"
47361,Support tf.IsFinite lowering,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): nightly

### 2. Code

```python
import tensorflow as tf

@tf.function
def func(x):
  return tf.is_finite(x)

converter = tf.lite.TFLiteConverter.from_concrete_functions([func.get_concrete_function(tf.TensorSpec(shape=(1, 2, 3)))])
converter.convert()
```

### 3. Failure after conversion

It will fail if flex is not enabled. However, `tf.is_inf()` is supported without flex because it is lowered to other ops in `lower_tf.cc`. It should be easy to support lowering pattern of `tf.is_finite`. Note that `tf.is_finite` is also being used in `tf.keras.layers.Softmax` with multi-axes reduction. I understand that I can enable flex to use it, but it should be a minimum work to lower it directly.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/advanced_activations.py#L328-L345"
47360,Having issue with NotImplemented Tensor to NumPy,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
Arch Linux
Tensorflow version 2.4.1
Python version 3.9.1

I am doing the [TensorFlow tutorial](https://www.tensorflow.org/tutorials/images/classification#data_augmentation) and am getting NotImplemented errors at the Data Augmentation stage:

```
data_augmentation = keras.Sequential(
    [
        layers.experimental.preprocessing.RandomFlip(""horizontal"",
                                                     input_shape=(img_height,
                                                                  img_width,
                                                                  3)),
        layers.experimental.preprocessing.RandomRotation(0.1),
        layers.experimental.preprocessing.RandomZoom(0.1),
    ]
)
```


Error Trace:
```
Traceback (most recent call last):
  File ""/home/yathavan/Documents/tensorflow/flowers/flowers.py"", line 136, in <module>
    data_augmentation = keras.Sequential(
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py"", line 517, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py"", line 144, in __init__
    self.add(layer)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py"", line 517, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py"", line 223, in add
    output_tensor = layer(self.outputs[0])
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 951, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1090, in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 822, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 863, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py"", line 866, in call
    output = control_flow_util.smart_cond(training, random_rotated_inputs,
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/utils/control_flow_util.py"", line 114, in smart_cond
    return smart_module.smart_cond(
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/framework/smart_cond.py"", line 54, in smart_cond
    return true_fn()
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py"", line 861, in random_rotated_inputs
    get_rotation_matrix(angles, img_hd, img_wd),
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py"", line 757, in get_rotation_matrix
    array_ops.zeros((num_angles, 2), dtypes.float32),
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 2819, in wrapped
    tensor = fun(*args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 2868, in zeros
    output = _constant_if_small(zero, shape, dtype, name)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 2804, in _constant_if_small
    if np.prod(shape) < 1000:
  File ""<__array_function__ internals>"", line 5, in prod
  File ""/usr/lib/python3.9/site-packages/numpy/core/fromnumeric.py"", line 3030, in prod
    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
  File ""/usr/lib/python3.9/site-packages/numpy/core/fromnumeric.py"", line 87, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 852, in __array__
    raise NotImplementedError(
NotImplementedError: Cannot convert a symbolic Tensor (random_rotation/rotation_matrix/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```"
47359,"Im trying to list my gpus, but only show gpu0  and do not show gpu1 that is my nvidia","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
47358,The comparing with memory by 1080 ti and 3090,"- Unet by tensorflow.keras (not include tf.compat.v1 code)
- Both of these are ubuntu18
- 2.4.0 (A computer) 2.0.0 (B computer)
- Both of these are python 3.6.12
- 11.1/8.0.5 (A computer) 10.0/7.6.5 (B computer)
- 3090 24265MiB (A computer) 1080 ti 11178MiB (B computer)
- Driver version 460.39

### current behavior

I ran the code using Unet to do segmentation project.
I found the using of memory is very diffecent.
It is the setting of model when I had trained the model:

batch_size = 20
epochs = 2

When I had trained the model, I inspect the htop and nvidia-smi:

The using resource by 3090 (A computer)
cpu memory 4.4G -> 12G
gpu memory 457MiB -> 18307MiB 

The using resource by 1080 ti: (B computer)
cpu memory 0.79G  -> 3.4G
gpu memory 0MiB -> 10867MiB

### expected behavior

I expected 3090(tf2.4) should be faster than 1080(tf2.0).

Thank you to read this issue!"
47357,tflite interpreter on android crashes on a model downloaded from tfhub,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- TensorFlow installed from (source or binary): maven build
- TensorFlow version (or github SHA if from source): ``org.tensorflow:tensorflow-lite:2.4.0``


**Provide the text output from tflite_convert**

```
# N/A - as the model is downloaded from tfhub
```

I am trying the simple style-transfer application using tflite. I am downloading the ``style-predict`` and ``style-transfer`` models from tfhub: https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/fp16/predict/1

The problem is that when using the ``style transfer`` model, the app crashes with the following:

```
signal 6 (SIGABRT), code -1 (SI_QUEUE), fault addr --------
Abort message: '/buildbot/src/android/ndk-release-r18/external/libcxx/../../external/libcxxabi/src/abort_message.cpp:73: abort_message: assertion ""terminating with uncaught exception of type std::bad_alloc: std::bad_alloc"" failed'
    eax 00000000  ebx 00001e3b  ecx 00001e3b  edx 00000006
    edi e83c333e  esi ff88e7f0
    ebp ea340ad0  esp ff88e798  eip ea340ad9
backtrace:
      #00 pc 00000ad9  [vdso] (__kernel_vsyscall+9)
      #01 pc 00092328  /apex/com.android.runtime/lib/bionic/libc.so (syscall+40) (BuildId: 76290498408016ad14f4b98c3ab6c65c)
      #02 pc 000ad651  /apex/com.android.runtime/lib/bionic/libc.so (abort+193) (BuildId: 76290498408016ad14f4b98c3ab6c65c)
      #03 pc 000adb88  /apex/com.android.runtime/lib/bionic/libc.so (__assert2+56) (BuildId: 76290498408016ad14f4b98c3ab6c65c)
      #04 pc 002ba9a4  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #05 pc 002baab7  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #06 pc 002b82d9  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #07 pc 002b7c0e  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #08 pc 002b7b63  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #09 pc 002ba578  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #10 pc 000f7e76  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #11 pc 000fedd9  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #12 pc 000fd993  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #13 pc 000fc1e8  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #14 pc 000f1280  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #15 pc 000efe3b  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #16 pc 0012dfab  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #17 pc 0012c6a5  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #18 pc 000ee7d6  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #19 pc 0028acc0  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #20 pc 0028e326  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so
      #21 pc 0004224f  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+63)
      #22 pc 00144f67  /apex/com.android.runtime/lib/libart.so (art_quick_generic_jni_trampoline+71) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #23 pc 0013e9a2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_static_stub+418) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #24 pc 00149a7a  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #25 pc 00332502  /apex/com.android.runtime/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+386) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #26 pc 0032c19c  /apex/com.android.runtime/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+988) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #27 pc 00684d03  /apex/com.android.runtime/lib/libart.so (MterpInvokeStatic+643) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #28 pc 001389a1  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_static+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #29 pc 00296a70  [anon:dalvik-classes.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.run+156)
      #30 pc 00681adc  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+1612) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #31 pc 00138821  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #32 pc 0029603a  [anon:dalvik-classes.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk] (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs+10)
      #33 pc 00681adc  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+1612) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #34 pc 00138821  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #35 pc 00007b6e  [anon:dalvik-classes2.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk!classes2.dex] (com.companyName.mytflite_app.MainActivity.runTransform+102)
      #36 pc 006845ac  /apex/com.android.runtime/lib/libart.so (MterpInvokeDirect+1324) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #37 pc 00138921  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_direct+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #38 pc 00007bc0  [anon:dalvik-classes2.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk!classes2.dex] (com.companyName.mytflite_app.MainActivity.StyleTransfer+16)
      #39 pc 006845ac  /apex/com.android.runtime/lib/libart.so (MterpInvokeDirect+1324) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #40 pc 00138921  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_direct+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #41 pc 000080cc  [anon:dalvik-classes2.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk!classes2.dex] (com.companyName.mytflite_app.MainActivity.lambda$configureFlutterEngine$0$MainActivity+236)
      #42 pc 00681adc  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+1612) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #43 pc 00138821  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #44 pc 00007944  [anon:dalvik-classes2.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk!classes2.dex] (com.companyName.mytflite_app.-$$Lambda$MainActivity$rQ-TPneqhLnrlPUSt3FsQ1A3iA8.onMethodCall+4)
      #45 pc 006837bc  /apex/com.android.runtime/lib/libart.so (MterpInvokeInterface+1980) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #46 pc 00138a21  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_interface+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #47 pc 001fc346  [anon:dalvik-classes.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk] (io.flutter.plugin.common.MethodChannel$IncomingMethodCallHandler.onMessage+34)
      #48 pc 006837bc  /apex/com.android.runtime/lib/libart.so (MterpInvokeInterface+1980) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #49 pc 00138a21  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_interface+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #50 pc 001f245a  [anon:dalvik-classes.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk] (io.flutter.embedding.engine.dart.DartMessenger.handleMessageFromDart+114)
      #51 pc 006837bc  /apex/com.android.runtime/lib/libart.so (MterpInvokeInterface+1980) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #52 pc 00138a21  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_interface+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #53 pc 001f1128  [anon:dalvik-classes.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk] (io.flutter.embedding.engine.FlutterJNI.handlePlatformMessage+8)
      #54 pc 002f8e0a  /apex/com.android.runtime/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEbb.llvm.12194892193087984976+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #55 pc 002ffcc5  /apex/com.android.runtime/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+181) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #56 pc 0066fbd9  /apex/com.android.runtime/lib/libart.so (artQuickToInterpreterBridge+1209) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #57 pc 0014503d  /apex/com.android.runtime/lib/libart.so (art_quick_to_interpreter_bridge+77) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #58 pc 0013e7d2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_stub+338) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #59 pc 00149a69  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+281) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #60 pc 0055a513  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+99) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #61 pc 0055bc7a  /apex/com.android.runtime/lib/libart.so (art::InvokeVirtualOrInterfaceWithVarArgs(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, char*)+474) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #62 pc 0040962f  /apex/com.android.runtime/lib/libart.so (art::JNI::CallVoidMethodV(_JNIEnv*, _jobject*, _jmethodID*, char*)+943) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #63 pc 003d8f44  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::CheckJNI::CallMethodV(char const*, _JNIEnv*, _jobject*, _jclass*, _jmethodID*, char*, art::Primitive::Type, art::InvokeType)+1700) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #64 pc 003c53f9  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::CheckJNI::CallVoidMethodV(_JNIEnv*, _jobject*, _jmethodID*, char*)+73) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #65 pc 0123628f  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)
      #66 pc 012361c3  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)
      #67 pc 0123073e  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)
      #68 pc 012b14a3  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)
      #69 pc 01255e05  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)
      #70 pc 01258bf3  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)
      #71 pc 01258b04  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)
      #72 pc 0125f3fa  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)
      #73 pc 0125f428  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)
      #74 pc 00018487  /system/lib/libutils.so (android::SimpleLooperCallback::handleEvent(int, int, void*)+39) (BuildId: 288ba3aff5b46dbd7e74be954af88b83)
      #75 pc 00019414  /system/lib/libutils.so (android::Looper::pollInner(int)+1044) (BuildId: 288ba3aff5b46dbd7e74be954af88b83)
      #76 pc 00018f4e  /system/lib/libutils.so (android::Looper::pollOnce(int, int*, int*, void**)+62) (BuildId: 288ba3aff5b46dbd7e74be954af88b83)
      #77 pc 0013299b  /system/lib/libandroid_runtime.so (android::android_os_MessageQueue_nativePollOnce(_JNIEnv*, _jobject*, long long, int)+59) (BuildId: 6ceb9761bceb97a18c92f8a4b7072247)
      #78 pc 002b86f8  /system/framework/x86/boot-framework.oat (art_jni_trampoline+136) (BuildId: ff6ec03dd8445d20788424c92ba8ea28ad0f54f4)
      #79 pc 02001f56  /memfd:/jit-cache (deleted) (android.os.MessageQueue.next+230)
      #80 pc 0013e7d2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_stub+338) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #81 pc 00149a69  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+281) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #82 pc 00332502  /apex/com.android.runtime/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+386) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #83 pc 0032c19c  /apex/com.android.runtime/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+988) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #84 pc 0068186d  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+989) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #85 pc 00138821  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #86 pc 00319d8a  /system/framework/framework.jar (android.os.Looper.loop+130)
      #87 pc 00684f6c  /apex/com.android.runtime/lib/libart.so (MterpInvokeStatic+1260) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #88 pc 001389a1  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_static+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #89 pc 0018945e  /system/framework/framework.jar (android.app.ActivityThread.main+194)
      #90 pc 002f8e0a  /apex/com.android.runtime/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEbb.llvm.12194892193087984976+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #91 pc 002ffcc5  /apex/com.android.runtime/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+181) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #92 pc 0066fbd9  /apex/com.android.runtime/lib/libart.so (artQuickToInterpreterBridge+1209) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #93 pc 0014503d  /apex/com.android.runtime/lib/libart.so (art_quick_to_interpreter_bridge+77) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #94 pc 0013e9a2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_static_stub+418) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #95 pc 00149a7a  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #96 pc 0055a513  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+99) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #97 pc 0055c32f  /apex/com.android.runtime/lib/libart.so (art::InvokeMethod(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jobject*, _jobject*, unsigned int)+1327) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #98 pc 004c9153  /apex/com.android.runtime/lib/libart.so (art::Method_invoke(_JNIEnv*, _jobject*, _jobject*, _jobjectArray*)+83) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #99 pc 000c6bf8  /system/framework/x86/boot.oat (art_jni_trampoline+168) (BuildId: 7913dbaef2e8d9971cb7619ef0d566987f8326a7)
      #100 pc 0013e7d2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_stub+338) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #101 pc 00149a69  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+281) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #102 pc 00332502  /apex/com.android.runtime/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+386) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #103 pc 0032c19c  /apex/com.android.runtime/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+988) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #104 pc 0068186d  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+989) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #105 pc 00138821  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #106 pc 0034cd36  /system/framework/framework.jar (com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run+22)
      #107 pc 002f8e0a  /apex/com.android.runtime/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEbb.llvm.12194892193087984976+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #108 pc 002ffcc5  /apex/com.android.runtime/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+181) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #109 pc 0066fbd9  /apex/com.android.runtime/lib/libart.so (artQuickToInterpreterBridge+1209) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #110 pc 0014503d  /apex/com.android.runtime/lib/libart.so (art_quick_to_interpreter_bridge+77) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #111 pc 00998b08  /system/framework/x86/boot-framework.oat (com.android.internal.os.ZygoteInit.main+1816) (BuildId: ff6ec03dd8445d20788424c92ba8ea28ad0f54f4)
      #112 pc 0013e9a2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_static_stub+418) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #113 pc 00149a7a  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #114 pc 0055a513  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+99) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #115 pc 0055a1ae  /apex/com.android.runtime/lib/libart.so (art::InvokeWithVarArgs(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, char*)+430) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #116 pc 004305cd  /apex/com.android.runtime/lib/libart.so (art::JNI::CallStaticVoidMethodV(_JNIEnv*, _jclass*, _jmethodID*, char*)+893) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #117 pc 003d93bf  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::CheckJNI::CallMethodV(char const*, _JNIEnv*, _jobject*, _jclass*, _jmethodID*, char*, art::Primitive::Type, art::InvokeType)+2847) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #118 pc 003c7509  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::CheckJNI::CallStaticVoidMethodV(_JNIEnv*, _jclass*, _jmethodID*, char*)+73) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)
      #119 pc 000b25fe  /system/lib/libandroid_runtime.so (_JNIEnv::CallStaticVoidMethod(_jclass*, _jmethodID*, ...)+62) (BuildId: 6ceb9761bceb97a18c92f8a4b7072247)
      #120 pc 000b628a  /system/lib/libandroid_runtime.so (android::AndroidRuntime::start(char const*, android::Vector<android::String8> const&, bool)+794) (BuildId: 6ceb9761bceb97a18c92f8a4b7072247)
      #121 pc 00003632  /system/bin/app_process32 (main+1490) (BuildId: b7a60bc7d078521421fd5a8d201915ae)
      #122 pc 000898e8  /apex/com.android.runtime/lib/bionic/libc.so (__libc_init+120) (BuildId: 76290498408016ad14f4b98c3ab6c65c)
```

The ``style-predict`` model works fine and generates the expected output. But when I try to use the input image and the ``bottleneck`` from the ``style-predict`` in the ``style-transfer`` model the app crashes. 

Whats interesting is that when I use the [other style transfer model that is not based on inceptionv3](https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/fp16/transfer/1), the app works fine and everything works great. But I want to make the inception model work as its result is far superior.

Below is my code in java that I am using to make this work:

```java
    protected Interpreter predictInterpreter;
    protected Interpreter transformInterpreter;
    protected Interpreter.Options interpreterOptions = new Interpreter.Options();
    private final static int IMAGE_MEAN = 0;
    private final static int IMAGE_STD = 255;
    private final static int STYLE_IMG_SIZE = 256;
    private final static int CONTENT_IMG_SIZE = 384;
    private final static int DIM_BATCH_SIZE = 1;
    private final static int DIM_PIXEL_SIZE = 3;

// non-relevant code removed for easy reading

try {
            interpreterOptions.setNumThreads(4);
            predictInterpreter = new Interpreter(loadModelFile(""inceptionv3_fp16_predict_1.tflite""), interpreterOptions);
            transformInterpreter = new Interpreter(loadModelFile(""inceptionv3_fp16_transfer_1.tflite""), interpreterOptions);
        } catch (Exception e) {
            e.printStackTrace();
        }

// input contentImage is already downscaled to 384*384 and ``bottleneck`` is the output from the ``style-predict`` model.
private Bitmap runTransform(Interpreter tflite, Bitmap contentImage, ByteBuffer bottleneck) {
        TensorImage inputTensorImage = getInputTensorImage(tflite, contentImage);
        Object[] inputs = new Object[2];
        inputs[0] = inputTensorImage.getBuffer();
        inputs[1] = bottleneck;

        int[] outputShape =
                new int[] {DIM_BATCH_SIZE, CONTENT_IMG_SIZE, CONTENT_IMG_SIZE, DIM_PIXEL_SIZE};
        DataType outputDataType = tflite.getOutputTensor(/* outputTensorIndex */ 0).dataType();
        TensorBuffer outputTensorBuffer = TensorBuffer.createFixedSize(outputShape, outputDataType);
        Map<Integer, Object> outputs = new HashMap<>();
        outputs.put(0, outputTensorBuffer.getBuffer());
        tflite.runForMultipleInputsOutputs(inputs, outputs);    /// on this line app crashes with above error messages.
        Bitmap outputBitmap = convertOutputToBmp(outputTensorBuffer.getFloatArray());
        return outputBitmap;
    }

private TensorImage getInputTensorImage(Interpreter tflite, Bitmap inputBitmap) {
        DataType imageDataType = tflite.getInputTensor(/* imageTensorIndex */0).dataType();
        TensorImage inputTensorImage = new TensorImage(imageDataType);
        inputTensorImage.load(inputBitmap);
        ImageProcessor imageProcessor =
                new ImageProcessor.Builder().add(new NormalizeOp(IMAGE_MEAN, IMAGE_STD)).build();
        return imageProcessor.process(inputTensorImage);
    }
```

I hope someone can help me figure out what the problem is please."
47354,ImportError traceback in VScode,"ImportError                               Traceback (most recent call last)
~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\pywrap_tensorflow.py in swig_import_helper()
     17         try:
---> 18             fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
     19         except ImportError:

C:\Program Files\Python39\lib\imp.py in find_module(name, path)
    295     else:
--> 296         raise ImportError(_ERR_MSG.format(name), name=name)
    297 

ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

ModuleNotFoundError                       Traceback (most recent call last)
~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\__init__.py in <module>
     53     # use `dlopen()` for dynamic loading.
---> 54     from tensorflow.python import pywrap_tensorflow
     55 except ImportError:

~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow = swig_import_helper()
     29     del swig_import_helper

~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\pywrap_tensorflow.py in swig_import_helper()
     19         except ImportError:
---> 20             import _pywrap_tensorflow
     21             return _pywrap_tensorflow

ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
c:\Users\tarang mehta\Desktop\introtodeeplearning-master\Deep_lizard\First_ANN.py in 
----> 2 import tensorflow as tf
      3 from tensorflow import keras
      4 from tensorflow.keras.models import Sequential
      5 from tensorflow.keras.layers import Activation, Dense
      6 from tensorflow.keras.optimizers import Adam

~\AppData\Roaming\Python\Python39\site-packages\tensorflow\__init__.py in <module>
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\__init__.py in <module>
     58 please exit the tensorflow source tree, and relaunch your python interpreter
     59 from there."""""" % traceback.format_exc()
---> 60   raise ImportError(msg)
     61 
     62 # Protocol buffers

ImportError: Traceback (most recent call last):
  File ""C:\Users\tarang mehta\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Program Files\Python39\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\tarang mehta\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\tarang mehta\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\tarang mehta\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
2 cells were canceled due to an error in the previous cell."
47353,"TensorFlow 2.4 Contains References to nocopts, which is No Longer Compatible with Bazel","In e5f8043742f927ed0e1711bb48f3e1a153b7a997 and ddde447e792231cdf83b435b0eeb59dd59bf4044, among other commits, support for `nocopts` was removed to enable the upgrade of Bazel to 1.0 and above.

However, there are still a few references to `nocopts` in the r2.4 branch:
```
$ git grep nocopts
tensorflow/lite/micro/testing/micro_test.bzl:        nocopts = """",
tensorflow/lite/micro/testing/micro_test.bzl:        nocopts: list of gcc compilation flags to remove for this rule
tensorflow/lite/micro/testing/micro_test.bzl:        nocopts = nocopts,
tensorflow/tensorflow.bzl:    # -fno-exceptions in nocopts breaks compilation if header modules are enabled.
tensorflow/tensorflow.bzl:    # -fno-exceptions in nocopts breaks compilation if header modules are enabled.
```

These should probably be removed for consistency and to avoid confusion."
47352,When will tensorflow be compatible with CUDA 11.2?,"- TensorFlow version: 2.5 (nightly)
- GPU: RTX 3070
- CUDA version: 11.2
- cuDNN version: 8

Right now, tensorflow is unable to recognize my GPU, so I'm not able to take advantage of the hardware. I read that others have been having the same issue with the newer RTX cards and it's due to tensorflow not supporting the latest CUDA versions. Please make this update, so we can leverage these newer cards. They're pretty hard to get right now, so it was disappointing to find out I couldn't use it.
"
47348,Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: no
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: source
-   **TensorFlow version (use command below)**: 2.4.0
-   **Python version**: 3.8.8
-   **Bazel version (if compiling from source)**: 3.1.0
-   **GCC/Compiler version (if compiling from source)**: MSVC 2019
-   **CUDA/cuDNN version**: 11.0.3/8.0.5
-   **GPU model and memory**: GTX 1650 Ti, 4GB
-   **Exact command to reproduce**:

### Describe the problem
Bazel fails to download 1.7.336.tar.gz, then unable to unzip an empty simple_console_for_windows.zip (size is 0b).

### Source code / logs
```PS C:\tensorflow> bazel build //tensorflow/tools/pip_package:build_pip_package
PS C:\tensorflow> bazel build //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from c:\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Python38/python.exe
INFO: Reading rc options for 'build' from c:\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from c:\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Python38/python.exe --action_env PYTHON_LIB_PATH=C:/Python38/lib/site-packages --python_path=C:/Python38/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file c:\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file c:\tensorflow\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file c:\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:windows in file c:\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\tensorflow\.bazelrc: --define framework_shared_object=false
**WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found**
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (410 packages loaded, 26438 targets configured).
INFO: Found 1 target...
Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
  bazel-bin/tensorflow/tools/pip_package/build_pip_package.exe
INFO: Elapsed time: 6455.931s, Critical Path: 785.27s
INFO: 10307 processes: 10307 local.
INFO: Build completed successfully, 14648 total actions
PS C:\tensorflow> bazel-bin\tensorflow\tools\pip_package\build_pip_package C:/tmp/tensorflow_pkg
Tue Feb 23 16:54:05 CEST 2021 : === Preparing sources in dir: /tmp/tmp.ZkbeuBDQsm
Unzipping simple_console_for_windows.zip to create runfiles tree...
[./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip]
  End-of-central-directory signature not found.  Either this file is not
  a zipfile, or it constitutes one disk of a multi-part archive.  In the
  latter case the central directory and zipfile comment will be found on
  the last disk(s) of this archive.
unzip:  cannot find zipfile directory in one of ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip or
        ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.zip, and cannot find ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.ZIP, period.```
"
47347,Custom objects in tf.keras have conflicting interface,"Custom tf.keras layers (tf.2.4.)  use `call` for invocation.  However, custom constraints use `__call__`.  This seems confusing.  Furthermore, if you implement a constraint using `call` instead of `__ call __` no warning or error given. I wonder if its reasonable to expect a Not Implemented error to occur in this case of `__call__` missing.

Ref: 
1. Custom layer documentation: https://keras.io/guides/making_new_layers_and_models_via_subclassing/
2. Custom constraint documentation: https://keras.io/api/layers/constraints/

EDIT 1. Fixed link.
EDIT 2. Added tensorflow version. Updated ticket title to reflect tf.keras."
47346,Proposal to use newer oneDNN version in the core,"**System information**
- TensorFlow version (you are using): v2.4.1/r2.4
- Are you willing to contribute it: It depends on TF Community plans

**Describe the feature and the current behavior/state.**
At the moment TensorFlow employs [oneDNN v1.6.4](https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/workspace.bzl#L172-L181) for core computationally intensive routines. Meanwhile oneDNN is approaching [to v2.2](https://github.com/oneapi-src/oneDNN/milestones) release. May you please clarify if there are any plans for the upcoming TF releases to move to a newer oneDNN version?

**Will this change the current api? How?**
It should not change the current API, I built TensorFlow with oneDNN master and it worked fine.

**Who will benefit with this feature?**
Newer versions of oneDNN have better support for and performance on AArch64, the main benefits will be for the users running TensorFlow on AArch64-based machines.

**Any Other info.**
This issue is mainly a question to understand TensorFlow Community plans for this topic."
47345,support of TFlite of Snapdragon888 dsp,"Hi @shuki-k 
Sadly this is not possible.
These new chips uses a newer version of HVX that is different than what Hexagon delegate supports.

_Originally posted by @karimnosseir in https://github.com/tensorflow/tensorflow/issues/47246#issuecomment-783807697_"
47344,In-batch-negatives and distributed traning,"I'm training a model using triplet loss. This means that my loss function aims to minimize the distance to a positive observation (let's say a sentence with the same semantic meaning) and maximize the distance to the negative observation. 
Im using [TripletHardLoss](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/losses/triplet.py#L348) from tensorflow-addons to do so. TripletHardLoss mines the batch to find the positive observation with the largest distance and the negative observation with the smallest distance in the batch and then applies the triplet loss.

Now when I train this model using `tf.distribute.MirroredStrategy` on a batch of 128 sentences and 8 GPUs the batches on my devices will only contain 14 (not counting the positive observation and the observation itself) potential negatives for each datapoint in the batch.

Is there any distribution strategy to concatenate all the 128 embeddings, run the triplet mining on this global batch, distribute the observation losses back to the devices and calculate the gradients (or even calculate the gradients on the device that runs the triplet mining) to utilize the negatives of the global batch?"
47342,How to convert a TF1 model with custom_getter to TF2,"I need to do this very same calculation in TF2 which ends up by calculating `v1` and `v2` which are softmax output and the same softmax output but calculated using a 
`tf.train.ExponentialMovingAverage` `average()` method. The original example can be found [here](https://github.com/openai/baselines/blob/master/baselines/acer/acer.py) at line 88 but I simplified the code in the example here for readability.

**TF1 code:**

    import tensorflow as tf
    
    
    with tf.variable_scope('my_model', reuse=tf.AUTO_REUSE):
        step_model = StepModel(step_param)  # step_param is a placeholder
        train_model = TrainModel(train_param)  # train_param is a placeholder
    
    params = tf.trainable_variables('my_model')
    ema = tf.train.ExponentialMovingAverage(0.99)
    ema_apply_op = ema.apply(params)
    
    def custom_getter(getter, *args, **kwargs):
        return ema.average(getter(*args, **kwargs))
    
    with tf.variable_scope(""my_model"", custom_getter=custom_getter, reuse=True):
        avg_model = AveragingModel(avg_param)  # avg_param is a placeholder
    
    train_model_p = tf.nn.softmax(train_model.pi)
    avg_model_p = tf.nn.softmax(avg_model.pi)
    common_param = some_param
    with tf.get_default_session() as sess:
        feed_dict = {
            train_model.train_param: common_param,
            avg_model.avg_param: common_param,
        }
        ops = [train_model_p, avg_model_p]
        v1, v2 = sess.run(ops, feed_dict)[1:]

If I want to keep a moving average of the trainable variables in TF2 without worrying about `v1` and `v2`, I'd do:


    from tensorflow.keras.optimizers import Adam
    from tensorflow_addons.optimizers import MovingAverage
    from tensorflow.keras.models import Model


and then I'd wrap the `tf.keras.optimizers.Optimizer()` being used:

    
    model = Model(...)
    optim = MovingAverage(Adam())
    model.compile(optimizer=optim)
    model.fit(...)

But I need to get the values of `v1` and `v2` for further calculations.
    "
47340,Calling model.test_on_batch after model.evaluate returns corrupted values for the loss and the metrics,"**System information**
- Google colab with tf 2.4.1 (v2.4.1-0-g85c8b2a817f )
- with CPU or GPU runtimes, it does not matter

**Describe the current behavior**

Calling `model.test_on_batch` after calling `model.evaluate` gives incorrect results.

**Describe the expected behavior**

Calling `model.test_on_batch` should  return the a value that does not depend on whether`model.evaluate`  was called before.

**Standalone code to reproduce the issue**

Let's define a randomly initialized model evaluated on randomly generated data. If we call `model.test_on_batch` directly, everything is fine:

```python
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

rng = np.random.RandomState(0)
batch_size = 32
n_samples, n_features = batch_size * 10, 5
X = rng.normal(size=(n_samples, n_features))
y = rng.randint(low=0, high=2, size=X.shape[0])

model = Sequential([Dense(1, input_shape=(n_features,),
                          activation=""sigmoid"")])
model.compile(optimizer=""adam"", loss='binary_crossentropy',
              metrics=['accuracy'])

print(""model.test_on_batch without model.evaluate"")
for i in range(3):
    loss, acc = model.test_on_batch(X[:batch_size], y[:batch_size])
    print(loss, acc)
```

output:

```
model.test_on_batch without model.evaluate
6.294709205627441 0.5625
6.294709205627441 0.5625
6.294709205627441 0.5625
```

If we then call `evaluate`, the fist call of `model.test_on_batch` return an incorrect value:

```python
normal_loss, normal_acc = model.evaluate(
    X, y, batch_size=batch_size, verbose=0)

print(""model.test_on_batch *after* model.evaluate"")
for i in range(3):
    loss, acc = model.test_on_batch(X[:batch_size], y[:batch_size])
    print(loss, acc)
```

output:

```
model.test_on_batch *after* model.evaluate
6.585799694061279 0.4801136255264282
6.294709205627441 0.5625
6.294709205627441 0.5625
```

Note: this is a minimal reproducer of the report found in the comments of a similar standalone keras issue: https://github.com/keras-team/keras/issues/14086

It is probably also the cause or at least related to corrupted loss/metric values reported when using `evaluate_generator`:
https://github.com/keras-team/keras/issues/13780"
47339,"ModelCheckPoint callback creates "".png"" file instead of "".h5"" file ","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memgory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
if we already have a saved model say ""model_name.h5"" in models directory and then we use ModelCheckPoint callback and provide the same name(""model_name.h5"") to save the model. After the training is finished we get ""model_name.png"" file instead of ""model_name.h5"" file. 

**Describe the expected behavior**
The user should get ""model_name.h5"" file instead of ""model_name.png"" after training.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1rBD6SZwjU9wgxfsgNn5COeaDmCpGI9IQ#scrollTo=0qqWjf-v1lVw

Note: run the training cell twice. first time to generate ""model_name.h5"" file then when you run the training cell second time, we can observe ""model_name.png"" is generated 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47337,"SparseTensorDenseMatMul adjoint_a takes transpose, not adjoint","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
On CPU (haven't checked GPU), the `adjoint_a` argument to `SparseTensorDenseMatMul` op (accessed via `tf.sparse.sparse_dense_matmul`) takes the transpose of the argument but not the conjugate.

Looks like the issue is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.h#L55; shouldn't that be taking a conjugate?

**Describe the expected behavior**
`adjoint_a` takes the adjoint of `a`.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
x = tf.sparse.from_dense([[1j]])
y = tf.constant([[1]], dtype=tf.complex128)
print(tf.sparse.sparse_dense_matmul(x, y, adjoint_a=True).numpy()) # Gives [[1j]]
```
See gist here: https://colab.research.google.com/drive/1qRZo4q1qwwt_pt6EfxRJPVohXmDUrdvN?usp=sharing
"
47336,How to extend self attention in transformer with local attention,"Hello everyone.. I'm trying (Vaswani et al, 2017) for machine translation task. Some of the recent findings (""https://www.aclweb.org/anthology/P19-1295/"" and ""https://www.aclweb.org/anthology/D19-5622/"" ) show that number of heads can extract redundant features as original work by Vaswani et al, 2017 used global attention. These papers suggested to use local attention along with global attention. So, I'm trying to extent Vaswani et al, 2017 with local attention as suggested by ""https://www.aclweb.org/anthology/D19-5622/"". 
In the paper, suggested global mask as follows:
![global mask](https://user-images.githubusercontent.com/60576100/108811216-df9b2200-75d2-11eb-9ca4-55c3e4a398c2.png)

and local mask as: 
![local mask](https://user-images.githubusercontent.com/60576100/108811141-b67a9180-75d2-11eb-89af-2aed43ace7a1.png)



I have created global and local mask as follows: 
        seq_len = tf.shape(seq)[1]
        global_mask = tf.zeros((tf.shape(seq)[0], tf.shape(seq)[1]))
                
        local_mask = global_mask
        local_mask=tf.Variable(local_mask)
        for i in range(0, tf.shape(seq)[0]):
          for j in range(0,tf.shape(seq)[1]):
            if not (i-w<=j and j<=i+w):
              local_mask[i,j].assign(-1e9) 
        local_mask = tf.convert_to_tensor(local_mask)

       global_mask=global_mask[:,tf.newaxis,tf.newaxis,:]
       local_mask=local_mask[:,tf.newaxis,tf.newaxis,:]

"
47335,Slower on a Nvidia 1080TI GPU than on CPU ,"**System information**
- Linux Ubuntu 18.04
- TensorFlow installed from binary
- TensorFlow version 2.4.1
- Python version : 3.8
- CUDA V 11.0/cuDNN version 8:
- GPU model: Nvidia 1080TI 11gb RAM

Running same script is 2x slower on GPU than CPU 

Steps to reproduce the problem :

obj = DeepFace.analyze(img_path = ""target.jpg"", actions = ['age', 'gender', 'race'])

This framework uses several models but in this case VGG Face was used for analysis .

Log from GPU run  :

2021-02-23 04:35:18.719513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-02-23 04:35:18.719590: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-02-23 04:35:19.596969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-23 04:35:19.597049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-02-23 04:35:19.597068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-02-23 04:35:19.597404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-23 04:35:19.598430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-23 04:35:19.599326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-23 04:35:19.600166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10271 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:00:05.0, compute capability: 6.1)
Action: age:   0%|                                                                                                                     | 0/3 [00:00<?, ?it/s]2021-02-23 04:35:23.409541: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-02-23 04:35:23.410483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199965000 Hz
2021-02-23 04:35:23.581168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-02-23 04:35:24.535674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-02-23 04:35:24.910561: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
Action: race: 100%|| 3/3 [00:05<00:00,  1.80s/it]
29  years old  asian     Woman
10.368096351623535  seconds

************** CPU RUn log :

2021-02-22 21:37:27.379054: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-22 21:37:27.382967: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
2021-02-22 21:37:27.385962: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
2021-02-22 21:37:27.394354: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-101PVLV
2021-02-22 21:37:27.398139: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-101PVLV
2021-02-22 21:37:27.400591: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-02-22 21:37:27.408451: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Action: age:   0%|                                                                               | 0/3 [00:00<?, ?it/s]2021-02-22 21:37:31.366069: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
Action: race: 100%|| 3/3 [00:01<00:00,  1.63it/s]
29  years old  asian     Woman
5.869304418563843  seconds

"
47334,ERROR: Didn't find op for builtin opcode 'ADD' version '1',"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, with MCU Expresso
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 2.3.1
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33, etc.): IMX RT 1060

**Describe the problem**
The application as per [this code](https://github.com/kavyaprasad22/MNV3) is saved as .pb file and converted to .tflite using [this convert.py](https://github.com/kavyaprasad22/MNV3/blob/main/conver.py). After uploading this to IMX RT 1060, the debugger throws an ```ERROR: Didn't find op for built-in opcode 'ADD' version '1'```. Is the opcode ADD failed to  be converted by the tflite or is it because of any missing header files in the client end?

**Please provide the exact sequence of commands/steps when you ran into the problem**
```
python train_RJ.py
python conver.py
``` 
after this converted to *.h using ``` xxd -i model.tflite > model.h``` 
and uploaded with MCU Xpresso to IMX RT 1060 using tensorflow_lite_cifar_10 example provided under eiq folder from SDK 2.9.1 build from [this builder](https://mcuxpresso.nxp.com/en/builder?hw=EVK-MIMXRT1060).


"
47331,Adabound feature request ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**
[Adabound](https://arxiv.org/abs/1902.09843) is a powerful optimizer for training models combining the advantages of SGD and Adam/momentum. Currently, there are no official TF/pytorch implementations available. Some projects in PYPI [keras-adabound](https://pypi.org/project/keras-adabound/) [torch-optimizer](https://pypi.org/project/torch-optimizer/) provides implementaions, but why not support an official Adabound and amsbound optimizer!

**Will this change the current api? How?**

**Who will benefit with this feature?**
Everyone hoping to experiment adabound for accelerated training.
**Any Other info.**
"
47330,memory increasing slowly and largely at the beginning of model.fit() in tf.keras,"**System information**
    Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows
    TensorFlow installed from (source or binary):github release
    TensorFlow version (use command below):2.3.1
    Python version:3.8.1
    CUDA/cuDNN version:10.1, 7.6.
    GPU model and memory: RTX2080Ti x4 
**Describe the current behavior**
It takes to much time for memory increasing before keras logs come and sometimes shows W:""multi-device function optimization failure""
**Describe the expected behavior**
Less time for memory increasing

**Code to reproduce the issue**

> main.py:

```python

    import tensorflow as tf
    import tensorflow_addons as tfa
    import tensorflow_datasets as tfds
    from tensorflow.keras.callbacks import TensorBoard
    from model import VisionTransformer
    AUTOTUNE = tf.data.experimental.AUTOTUNE

    ds = tfds.load(""imagenet_resized/32x32"", as_supervised=True)
    ds_train = (
        ds[""train""]
        .cache()
        .shuffle(5 * 4096)
        .batch(4096)
        .prefetch(AUTOTUNE)
    )
    ds_test = (
        ds[""validation""]
        .cache()
        .batch(4096)
        .prefetch(AUTOTUNE)
    )

    strategy = tf.distribute.MirroredStrategy()

    with strategy.scope():
        model = VisionTransformer(
            image_size=32,
            patch_size=4,
            num_layers=4,
            num_classes=1000,
            d_model=64,
            num_heads=4,
            mlp_dim=128,
            channels=3,
            dropout=0.1,
        )
        model.compile(
            loss=tf.keras.losses.SparseCategoricalCrossentropy(
                from_logits=True
            ),
            optimizer=tfa.optimizers.AdamW(
                learning_rate=3e-4, weight_decay=1e-4
            ),
            metrics=[""accuracy""],
        )

    model.fit(
        ds_train,
        validation_data=ds_test,
        epochs=300,
    )
`

> model.py:

`
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.layers import (
    Dense,
    Dropout,
    LayerNormalization,
)
from tensorflow.keras.layers.experimental.preprocessing import Rescaling

class MultiHeadSelfAttention(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads=8):
        super(MultiHeadSelfAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        if embed_dim % num_heads != 0:
            raise ValueError(
                f""embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}""
            )
        self.projection_dim = embed_dim // num_heads
        self.query_dense = Dense(embed_dim)
        self.key_dense = Dense(embed_dim)
        self.value_dense = Dense(embed_dim)
        self.combine_heads = Dense(embed_dim)

    def attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output, weights

    def separate_heads(self, x, batch_size):
        x = tf.reshape(
            x, (batch_size, -1, self.num_heads, self.projection_dim)
        )
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        query = self.query_dense(inputs)
        key = self.key_dense(inputs)
        value = self.value_dense(inputs)
        query = self.separate_heads(query, batch_size)
        key = self.separate_heads(key, batch_size)
        value = self.separate_heads(value, batch_size)
        attention, weights = self.attention(query, key, value)
        attention = tf.transpose(attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(
            attention, (batch_size, -1, self.embed_dim)
        )
        output = self.combine_heads(concat_attention)
        return output


class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadSelfAttention(embed_dim, num_heads)
        self.mlp = tf.keras.Sequential(
            [
                Dense(mlp_dim, activation=tfa.activations.gelu),
                Dropout(dropout),
                Dense(embed_dim),
                Dropout(dropout),
            ]
        )
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(dropout)
        self.dropout2 = Dropout(dropout)

    def call(self, inputs, training):
        inputs_norm = self.layernorm1(inputs)
        attn_output = self.att(inputs_norm)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = attn_output + inputs
        out1_norm = self.layernorm2(out1)
        mlp_output = self.mlp(out1_norm)
        mlp_output = self.dropout2(mlp_output, training=training)
        return mlp_output + out1

class VisionTransformer(tf.keras.Model):
    def __init__(
        self,
        image_size,
        patch_size,
        num_layers,
        num_classes,
        d_model,
        num_heads,
        mlp_dim,
        channels=3,
        dropout=0.1,
    ):
        super(VisionTransformer, self).__init__()
        num_patches = (image_size // patch_size) ** 2
        self.patch_dim = channels * patch_size ** 2
        self.patch_size = patch_size
        self.d_model = d_model
        self.num_layers = num_layers

        self.rescale = Rescaling(1.0 / 255)
        self.pos_emb = self.add_weight(
            ""pos_emb"", shape=(1, num_patches + 1, d_model)
        )
        self.class_emb = self.add_weight(""class_emb"", shape=(1, 1, d_model))
        self.patch_proj = Dense(d_model)
        self.enc_layers = [
            TransformerBlock(d_model, num_heads, mlp_dim, dropout)
            for _ in range(num_layers)
        ]
        self.mlp_head = tf.keras.Sequential(
            [
                LayerNormalization(epsilon=1e-6),
                Dense(mlp_dim, activation=tfa.activations.gelu),
                Dropout(dropout),
                Dense(num_classes),
            ]
        )

    def extract_patches(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding=""VALID"",
        )
        patches = tf.reshape(patches, [batch_size, -1, self.patch_dim])
        return patches

    def call(self, x, training):
        batch_size = tf.shape(x)[0]
        x = self.rescale(x)
        patches = self.extract_patches(x)
        x = self.patch_proj(patches)

        class_emb = tf.broadcast_to(
            self.class_emb, [batch_size, 1, self.d_model]
        )
        x = tf.concat([class_emb, x], axis=1)
        x = x + self.pos_emb

        for layer in self.enc_layers:
            x = layer(x, training)

        # First (class token) is used for classification
        x = self.mlp_head(x[:, 0])
        return x
`
**Other info / logs**

> Epoch 1/20
2021-02-23 09:54:36.039640: W tensorflow/core/common_runtime/process_function_library_runtime.cc:675] Ignoring multi-device function optimization failure: Deadline exceeded: meta_optimizer exceeded deadline.
```
Usually it takes too much time from 'Epoch 1/20' log to progress bar log. By the way, could you tell me what things the increasing memory consists of?"
47329,ImportError: cannot import name 'keras_modules_injection',"**System information**
- Have I written custom code and  used `from tensorflow.python.keras.applications import keras_modules_injection` in Tensorflow 2.0, it worked, now I changed to Tensorflow 2.4, and facing error `ImportError: cannot import name 'keras_modules_injection'`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): 2.4.1
- Python version: Python 3.6.9
- CUDA/cuDNN version: CUDA 11
- GPU model and memory: Titan XP

Checking TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

`2021-02-22 20:44:00.220774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
v2.4.0-49-g85c8b2a817f 2.4.1`


**Describe the current behavior**
python test_tf_imports.py 
2021-02-22 20:24:25.627138: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File ""test_tf_imports.py"", line 2, in <module>
    from tensorflow.python.keras.applications import keras_modules_injection
ImportError: cannot import name 'keras_modules_injection'

**Describe the expected behavior**
With the line of code `from tensorflow.python.keras.applications import keras_modules_injection` which was running in TF2.0, should run in TF2.4.x

**Standalone code to reproduce the issue**
from tensorflow.python.keras.applications import keras_modules_injection
from tensorflow.python.util.tf_export import keras_export

@keras_export('keras.applications.resnet50.ResNet50',
              'keras.applications.ResNet50')
@keras_modules_injection
def ResNet50(*args, **kwargs):
  return resnet50.ResNet50(*args, **kwargs)
"
47328,tf.nn.depth_to_space with NCHW argument works in TF 2.3 but fails in TF 2.4,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Intel CPU, Macbook Pro 2019

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
In TF 2.4, the tf.nn.depth_to_space command fails with any data_format parameter of ""NCHW"". 

```
python test_d2l.py
2021-02-22 15:37:05.145218: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-22 15:37:05.145510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""test_d2l.py"", line 4, in <module>
    tf.nn.depth_to_space(x, block_size=2, data_format=""NCHW"")
  File ""/usr/local/anaconda3/envs/keras2onnx/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/local/anaconda3/envs/keras2onnx/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 3922, in depth_to_space_v2
    return gen_array_ops.depth_to_space(input, block_size, data_format, name=name)
  File ""/usr/local/anaconda3/envs/keras2onnx/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1625, in depth_to_space
    return depth_to_space_eager_fallback(
  File ""/usr/local/anaconda3/envs/keras2onnx/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1661, in depth_to_space_eager_fallback
    _result = _execute.execute(b""DepthToSpace"", 1, inputs=_inputs_flat,
  File ""/usr/local/anaconda3/envs/keras2onnx/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node DepthToSpace}} = DepthToSpace[T=DT_FLOAT, block_size=2, data_format=""NCHW""]
All kernels registered for op DepthToSpace:
  device='CPU'; T in [DT_UINT64]; data_format in [""NHWC""]
  device='CPU'; T in [DT_INT64]; data_format in [""NHWC""]
  device='CPU'; T in [DT_UINT32]; data_format in [""NHWC""]
  device='CPU'; T in [DT_UINT16]; data_format in [""NHWC""]
  device='CPU'; T in [DT_INT16]; data_format in [""NHWC""]
  device='CPU'; T in [DT_UINT8]; data_format in [""NHWC""]
  device='CPU'; T in [DT_INT8]; data_format in [""NHWC""]
  device='CPU'; T in [DT_INT32]; data_format in [""NHWC""]
  device='CPU'; T in [DT_HALF]; data_format in [""NHWC""]
  device='CPU'; T in [DT_BFLOAT16]; data_format in [""NHWC""]
  device='CPU'; T in [DT_FLOAT]; data_format in [""NHWC""]
  device='CPU'; T in [DT_DOUBLE]; data_format in [""NHWC""]
  device='CPU'; T in [DT_COMPLEX64]; data_format in [""NHWC""]
  device='CPU'; T in [DT_COMPLEX128]; data_format in [""NHWC""]
  device='CPU'; T in [DT_BOOL]; data_format in [""NHWC""]
  device='CPU'; T in [DT_STRING]; data_format in [""NHWC""]
  device='CPU'; T in [DT_RESOURCE]; data_format in [""NHWC""]
  device='CPU'; T in [DT_VARIANT]; data_format in [""NHWC""]
 [Op:DepthToSpace]
```


**Describe the expected behavior**
Code block below runs without error in TF 2.3 (compiles cluster with XLA). 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
import numpy as np
x =  np.random.rand(3, 4, 6, 8).astype(np.float32)
tf.nn.depth_to_space(x, block_size=2, data_format=""NCHW"")
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47326,Unable to convert mT5 model to tflite (tensorflow.GraphDef exceeds maximum protobuf size of 2GB),"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library : 2.3.1

### 2. Code
```
import transformers
from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM, TFAutoModelForSeq2SeqLM
import tensorflow as tf

model_name = ""google/mt5-base""
config = AutoConfig.from_pretrained(
    model_name
)
tokenizer = AutoTokenizer.from_pretrained(
    model_name
)
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    from_pt=True,
    config=config
)

input_spec = tf.TensorSpec([1, 64], tf.int16)
model._set_inputs(input_spec, training=False)
print(model.inputs)
print(model.outputs)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.float32
converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""mt5_base_en_to_fa.tflite"", ""wb"").write(tflite_model)
```

### 3. Error Message:
```
ValueError                                Traceback (most recent call last)
<ipython-input-78-e62147be515c> in <module>
----> 1 tflite_model = converter.convert()
      2 open(""mt5_base_en_to_fa.tflite"", ""wb"").write(tflite_model)

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)
    807     frozen_func, graph_def = (
    808         _convert_to_constants.convert_variables_to_constants_v2_as_graph(
--> 809             self._funcs[0], lower_control_flow=False))
    810 
    811     input_tensors = [

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)
   1107 
   1108   frozen_func = _construct_concrete_function(func, output_graph_def,
-> 1109                                              converted_input_indices)
   1110   return frozen_func, output_graph_def
   1111 

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py in _construct_concrete_function(func, output_graph_def, converted_input_indices)
    999   new_func = wrap_function.function_from_graph_def(output_graph_def,
   1000                                                    new_input_names,
-> 1001                                                    new_output_names)
   1002 
   1003   # Manually propagate shape for input tensors where the shape is not correctly

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in function_from_graph_def(graph_def, inputs, outputs)
    648     importer.import_graph_def(graph_def, name="""")
    649 
--> 650   wrapped_import = wrap_function(_imports_graph_def, [])
    651   import_graph = wrapped_import.graph
    652   return wrapped_import.prune(

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)
    626           signature=signature,
    627           add_control_dependencies=False,
--> 628           collections={}),
    629       variable_holder=holder,
    630       signature=signature)

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in __call__(self, *args, **kwargs)
     85 
     86   def __call__(self, *args, **kwargs):
---> 87     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
     88 
     89   def call_with_variable_creator_scope(self, fn):

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in wrapped(*args, **kwargs)
     91     def wrapped(*args, **kwargs):
     92       with variable_scope.variable_creator_scope(self.variable_creator_scope):
---> 93         return fn(*args, **kwargs)
     94 
     95     return wrapped

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in _imports_graph_def()
    646 
    647   def _imports_graph_def():
--> 648     importer.import_graph_def(graph_def, name="""")
    649 
    650   wrapped_import = wrap_function(_imports_graph_def, [])

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--> 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/framework/importer.py in import_graph_def(***failed resolving arguments***)
    403       return_elements=return_elements,
    404       name=name,
--> 405       producer_op_list=producer_op_list)
    406 
    407 

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)
    492   # _ProcessNewOps.
    493   with graph._mutation_lock():  # pylint: disable=protected-access
--> 494     with c_api_util.tf_buffer(graph_def.SerializeToString()) as serialized:
    495       try:
    496         results = c_api.TF_GraphImportGraphDefWithResults(

ValueError: Message tensorflow.GraphDef exceeds maximum protobuf size of 2GB: 2330463933
```

### 4. Note:
1. I realized that the suuport for models larger than 2GB is added to ONNX converter ([here](https://github.com/onnx/tensorflow-onnx/pull/1090) ). I was wondering if any similar fix is added for tflite converter as well? If not, how can I overcome this error?
2. I have been to convert ""google/mt5-small"" model to tflite, but not the mt5-base as it is a larger model.

Thanks."
47325,Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string,"Opening a new bug per @nikitamaia's suggestion. See discussion in #28007, in which she confirmed the bug. Bug only presents when using GPU, not CPU.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 2.4.1/8
- GPU model and memory: GeForce GTX 1050 Ti computeCapability: 6.1, 3.95GiB

**Describe the current behavior**
Error during function call for compiled function:
`Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string`

**Describe the expected behavior**
Compiled function call should succeed like uncompiled function call.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf


def _get_char_flags(char_tensor):
    char = char_tensor.numpy().decode()
    return char.isalpha(), char.isspace()


@tf.function  # <--------------- Runs fine with this line commented out.
def tf_split_text(text):
    tf.assert_rank(text, 0)
    tf.debugging.assert_type(text, tf.string)

    chars = tf.strings.unicode_split(text, input_encoding='UTF-8')
    is_alpha, is_space = tf.map_fn(lambda char: tf.py_function(_get_char_flags, char, Tout=[tf.bool, tf.bool]),
                                   (chars,), dtype=tf.bool, parallel_iterations=True,
                                   fn_output_signature=[tf.bool, tf.bool])

    is_alpha = tf.concat([is_alpha, [False]], axis=0)
    is_space = tf.concat([is_space, [True]], axis=0)

    is_special = ~(is_alpha | is_space)
    is_non_alpha = ~is_alpha
    is_non_space = ~is_space

    was_special = tf.concat([[False], is_special[:-1]], axis=0)
    was_non_alpha = tf.concat([[True], is_non_alpha[:-1]], axis=0)
    was_non_space = tf.concat([[False], is_non_space[:-1]], axis=0)

    any_to_special = is_special
    non_alpha_to_non_space = was_non_alpha & is_non_space
    token_start_flags = any_to_special | non_alpha_to_non_space
    token_start_indices = tf.where(token_start_flags)[:, 0]

    special_to_any = was_special
    non_space_to_non_alpha = was_non_space & is_non_alpha
    token_end_flags = special_to_any | non_space_to_non_alpha
    token_end_indices = tf.where(token_end_flags)[:, 0]

    tf.debugging.assert_equal(tf.size(token_start_indices), tf.size(token_end_indices))

    preceding_space = tf.concat([[False], is_space[:-1]], axis=0)

    tokens = tf.strings.substr(text, token_start_indices, token_end_indices - token_start_indices, unit='UTF8_CHAR')
    has_preceding_space = tf.gather(preceding_space, token_start_indices)

    tf.assert_rank(has_preceding_space, 1)
    tf.assert_rank(tokens, 1)
    tf.assert_equal(tf.reduce_sum(tf.map_fn(tf.strings.length, tokens, fn_output_signature=tf.int32)) +
                    tf.reduce_sum(tf.cast(has_preceding_space, tf.int32)),
                    tf.strings.length(text))
    return has_preceding_space, tokens


text = tf.constant('hi there', tf.string)
preceding_spaces, words = tf_split_text(text)
print(words)
```

**Other info / logs**

Traceback:
```python
Traceback (most recent call last):
  File ""/home/hosford42/PycharmProjects/ImageParser/error.py"", line 56, in <module>
    preceding_spaces, words = tf_split_text(text)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py"", line 894, in _call
    return self._concrete_stateful_fn._call_flat(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 555, in call
    outputs = execute.execute(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
	 [[{{node map_1/TensorArrayUnstack/TensorListFromTensor/_96}}]]
	 [[map_1/while/loop_body_control/_61/_107]]
  (1) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
	 [[{{node map_1/TensorArrayUnstack/TensorListFromTensor/_96}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_tf_split_text_265]

Function call stack:
tf_split_text -> tf_split_text
```
"
47324,Using GridSearchCV in a regression with Keras:  TypeError: cannot pickle '_thread.RLock' object,"I'm trying to use GridSearchCV in a regression with a Keras neural network.
The data I'm using is the Boston Housing Price dataset, which was loaded directly from keras `boston_housing.load_data()`. The following is a code snippet of I'm trying to do.

    def build_model():
        model=models.Sequential()
        model.add(layers.Dense(64,activation=""relu"",
                              input_shape=(train_data_norm.shape[1],)))
        model.add(layers.Dense(64,activation=""relu""))
        model.add(layers.Dense(1))
        model.compile(optimizer='rmsprop',loss=""mse"",metrics=[""mae""])
        return model 
    
    from sklearn.model_selection import GridSearchCV
    
    from keras.wrappers.scikit_learn import KerasRegressor
    
    model=KerasRegressor(build_fn=build_model(),epochs=30)
    
    param_grid = {""epochs"":list(range(1,51))}
    
    grid_model=GridSearchCV(model,param_grid,cv=4)
    
    grid_model.fit(train_data_norm, train_targets)

And I get the following error message: 

    TypeError Traceback (most recent call last)
    <ipython-input-185-3fa5fc34b6b0> in <module>
          9 grid_model=GridSearchCV(model,param_grid,cv=4)
         10 
    ---> 11 grid_model.fit(train_data_norm, train_targets)
    
    ~\anaconda3\envs\PythonCPU\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
         70                           FutureWarning)
         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
    ---> 72         return f(**kwargs)
         73     return inner_f
         74 
    
    ~\anaconda3\envs\PythonCPU\lib\site-packages\sklearn\model_selection\_search.py in fit(self, X, y, groups, **fit_params)
        679         n_splits = cv.get_n_splits(X, y, groups)
        680 
    --> 681         base_estimator = clone(self.estimator)
        682 
        683         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
    
    ~\anaconda3\envs\PythonCPU\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
         70                           FutureWarning)
         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
    ---> 72         return f(**kwargs)
         73     return inner_f
         74 
    
    ~\anaconda3\envs\PythonCPU\lib\site-packages\sklearn\base.py in clone(estimator, safe)
         85     new_object_params = estimator.get_params(deep=False)
         86     for name, param in new_object_params.items():
    ---> 87         new_object_params[name] = clone(param, safe=False)
         88     new_object = klass(**new_object_params)
         89     params_set = new_object.get_params(deep=False)
    
    ~\anaconda3\envs\PythonCPU\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
         70                           FutureWarning)
         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
    ---> 72         return f(**kwargs)
         73     return inner_f
         74 
    
    ~\anaconda3\envs\PythonCPU\lib\site-packages\sklearn\base.py in clone(estimator, safe)
         69     elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):
         70         if not safe:
    ---> 71             return copy.deepcopy(estimator)
         72         else:
         73             if isinstance(estimator, type):
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        170                     y = x
        171                 else:
    --> 172                     y = _reconstruct(x, memo, *rv)
        173 
        174     # If is its own copy, don't memoize.
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
        268     if state is not None:
        269         if deep:
    --> 270             state = deepcopy(state, memo)
        271         if hasattr(y, '__setstate__'):
        272             y.__setstate__(state)
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        144     copier = _deepcopy_dispatch.get(cls)
        145     if copier is not None:
    --> 146         y = copier(x, memo)
        147     else:
        148         if issubclass(cls, type):
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
        228     memo[id(x)] = y
        229     for key, value in x.items():
    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)
        231     return y
        232 d[dict] = _deepcopy_dict
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        144     copier = _deepcopy_dispatch.get(cls)
        145     if copier is not None:
    --> 146         y = copier(x, memo)
        147     else:
        148         if issubclass(cls, type):
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _deepcopy_list(x, memo, deepcopy)
        203     append = y.append
        204     for a in x:
    --> 205         append(deepcopy(a, memo))
        206     return y
        207 d[list] = _deepcopy_list
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        170                     y = x
        171                 else:
    --> 172                     y = _reconstruct(x, memo, *rv)
        173 
        174     # If is its own copy, don't memoize.
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
        268     if state is not None:
        269         if deep:
    --> 270             state = deepcopy(state, memo)
        271         if hasattr(y, '__setstate__'):
        272             y.__setstate__(state)
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        144     copier = _deepcopy_dispatch.get(cls)
        145     if copier is not None:
    --> 146         y = copier(x, memo)
        147     else:
        148         if issubclass(cls, type):
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
        228     memo[id(x)] = y
        229     for key, value in x.items():
    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)
        231     return y
        232 d[dict] = _deepcopy_dict
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        144     copier = _deepcopy_dispatch.get(cls)
        145     if copier is not None:
    --> 146         y = copier(x, memo)
        147     else:
        148         if issubclass(cls, type):
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _deepcopy_list(x, memo, deepcopy)
        203     append = y.append
        204     for a in x:
    --> 205         append(deepcopy(a, memo))
        206     return y
        207 d[list] = _deepcopy_list
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        170                     y = x
        171                 else:
    --> 172                     y = _reconstruct(x, memo, *rv)
        173 
        174     # If is its own copy, don't memoize.
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
        268     if state is not None:
        269         if deep:
    --> 270             state = deepcopy(state, memo)
        271         if hasattr(y, '__setstate__'):
        272             y.__setstate__(state)
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        144     copier = _deepcopy_dispatch.get(cls)
        145     if copier is not None:
    --> 146         y = copier(x, memo)
        147     else:
        148         if issubclass(cls, type):
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
        228     memo[id(x)] = y
        229     for key, value in x.items():
    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)
        231     return y
        232 d[dict] = _deepcopy_dict
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        170                     y = x
        171                 else:
    --> 172                     y = _reconstruct(x, memo, *rv)
        173 
        174     # If is its own copy, don't memoize.
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
        268     if state is not None:
        269         if deep:
    --> 270             state = deepcopy(state, memo)
        271         if hasattr(y, '__setstate__'):
        272             y.__setstate__(state)
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        144     copier = _deepcopy_dispatch.get(cls)
        145     if copier is not None:
    --> 146         y = copier(x, memo)
        147     else:
        148         if issubclass(cls, type):
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
        228     memo[id(x)] = y
        229     for key, value in x.items():
    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)
        231     return y
        232 d[dict] = _deepcopy_dict
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        170                     y = x
        171                 else:
    --> 172                     y = _reconstruct(x, memo, *rv)
        173 
        174     # If is its own copy, don't memoize.
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
        268     if state is not None:
        269         if deep:
    --> 270             state = deepcopy(state, memo)
        271         if hasattr(y, '__setstate__'):
        272             y.__setstate__(state)
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        144     copier = _deepcopy_dispatch.get(cls)
        145     if copier is not None:
    --> 146         y = copier(x, memo)
        147     else:
        148         if issubclass(cls, type):
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
        228     memo[id(x)] = y
        229     for key, value in x.items():
    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)
        231     return y
        232 d[dict] = _deepcopy_dict
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        170                     y = x
        171                 else:
    --> 172                     y = _reconstruct(x, memo, *rv)
        173 
        174     # If is its own copy, don't memoize.
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
        268     if state is not None:
        269         if deep:
    --> 270             state = deepcopy(state, memo)
        271         if hasattr(y, '__setstate__'):
        272             y.__setstate__(state)
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        144     copier = _deepcopy_dispatch.get(cls)
        145     if copier is not None:
    --> 146         y = copier(x, memo)
        147     else:
        148         if issubclass(cls, type):
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
        228     memo[id(x)] = y
        229     for key, value in x.items():
    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)
        231     return y
        232 d[dict] = _deepcopy_dict
    
    ~\anaconda3\envs\PythonCPU\lib\copy.py in deepcopy(x, memo, _nil)
        159                     reductor = getattr(x, ""__reduce_ex__"", None)
        160                     if reductor is not None:
    --> 161                         rv = reductor(4)
        162                     else:
        163                         reductor = getattr(x, ""__reduce__"", None)
    
    TypeError: cannot pickle '_thread.RLock' object


"
47323,Getting negative loss function in Autoregressive,"Hi

I'm using Autoregressive for density estimation, the same example that is written in this page:

https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/AutoregressiveNetwork?version=nightly

However, when I increase the dimension (event_shape) of the data that I want to find its density, from two to four for example, I start getting negative loss function!

Can anyone help me with this issue.
Thanks

"
47322,Value error with DELF,"I use the following code to compute delf,

```
import argparse
from glob import glob

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.python.framework import ops
from tensorflow.python.framework.ops import disable_eager_execution
from tqdm import tqdm

disable_eager_execution()
tf.compat.v1.disable_v2_behavior()

class DeepDELF:

    def __init__(self, input_path):
        ops.reset_default_graph()

        m = hub.Module('https://tfhub.dev/google/delf/1')

        # The module operates on a single image at a time, so define a placeholder to
        # feed an arbitrary image in.
        self.image_placeholder = tf.compat.v1.placeholder(
            tf.float32, shape=(None, None, 3), name='input_image')

        module_inputs = {
            'image': self.image_placeholder,
            'score_threshold': 100.0,
            'image_scales': [0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0],
            'max_feature_num': 1000,
        }

        self.module_outputs = m(module_inputs, as_dict=True)
        self.image_tf = self.image_input_fn(glob(input_path + '/*'))
        self.path_list = glob(input_path + '/*')

    def extract(self):
        with tf.compat.v1.train.MonitoredSession() as sess:
            results_dict = {}  # Stores the locations and their descriptors for each image
            for image_path in tqdm(self.path_list):
                image = sess.run(self.image_tf)
                print('Extracting locations and descriptors from %s' % image_path)
                results_dict[image_path] = sess.run(
                    [self.module_outputs['locations'], self.module_outputs['descriptors']],
                    feed_dict={self.image_placeholder: image})
            return results_dict

    def image_input_fn(self, image_files):
        filename_queue = tf.compat.v1.train.string_input_producer(
            image_files, shuffle=False)
        reader = tf.compat.v1.WholeFileReader()
        _, value = reader.read(filename_queue)
        image_tf = tf.image.decode_jpeg(value, channels=3)
        return tf.image.convert_image_dtype(image_tf, tf.float32)


def main(args):
    path = args['input_path']
    extrator = None
    extractor = DeepDELF(path)
    results_dict = extractor.extract()
    results_dict2 = extractor.extract()
    print(""Shape feature: "", results_dict.keys())
    print(""Shape feature 2: "", results_dict2.keys())


def args_parser():
    parser = argparse.ArgumentParser(description=""Methods extract image."")
    parser.add_argument('-i', '--input_path', 
                        help=""The path of the input image."")
    return vars(parser.parse_args())


if __name__ == ""__main__"":
    args = args_parser()
    # End default optional arguments
    # Print info arguments
    print(""Extract feature from image."".upper().center(100))
    print(str(""-"" * 63).center(100))
    print(""|{:<30}:\n|{:<30}|"".format(""Image path"", args['input_path']).center(100))
    print(str(""-"" * 63).center(100))

    main(args)

```

Unfortunately, it throws the following error

```
    raise ValueError(not_null_err)
ValueError: string_input_producer requires a non-null input tensor
```

How can I fix this?"
47321,Keras fit with generators not executing in the main thread when setting workers=0,"Can be reproduced on colab with TensorFlow (`v2.4.1-0-g85c8b2a817f 2.4.1`)

**Describe the current behavior**

Generator code is executed in other threads when using `model.fit`.

From the docs, it looks like that setting workers=0 would execute the generator code in the main thread.

> workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. **If 0, will execute the generator on the main thread**.

This doesn't seem to work though as only the first iterations seems to be executed in the main thread.

For example:

```
import tensorflow as tf
import threading
model = tf.keras.Sequential([tf.keras.layers.Dense(1)])
model.compile(loss = ""mse"", optimizer = ""adam"")

def gen ():
  for i in range(100):
    print(threading.current_thread())
    yield (tf.random.normal(shape=(100,1)), tf.random.normal(shape = (100,)))

model.fit(gen(), epochs = 1, workers = 0, verbose = 0, steps_per_epoch = 3, max_queue_size=0)
```

I get:

```
<_MainThread(MainThread, started 140516450817920)>
<_DummyThread(Dummy-4, started daemon 140514717599488)>
<_DummyThread(Dummy-5, started daemon 140514709206784)>
<tensorflow.python.keras.callbacks.History at 0x7fcc1d7223c8>
```

**Describe the expected behavior**

I'd expect that all calls from the generator to be executed in the main thread. This is problematic because in my use case the generator is not thread safe and crashes the program when executed from other threads. 

Note: I have opened a [SO question](https://stackoverflow.com/questions/66320198/keras-fit-with-generator-function-always-execute-in-the-main-thread) but I feel this behavior should be treated as a bug.
"
47319,TPU with bfloat16 does not support tf.image.resize with nearest,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.9
- GPU model and memory: TPUv3


![](https://user-images.githubusercontent.com/3354448/108751857-b4d2ae80-757d-11eb-8d7f-467085464eca.png)
"
47316,BUG: gfile.rmtree is raises a NotFound error for ram:// filesystems,"Tested via tf-nightly on 2020-02-22 (YYYY-MM-DD).

Code to reproduce:

```python
from tensorflow.io import gfile

gfile.mkdir(""ram://deletethisdir"")
gfile.rmtree(""ram://deletethisdir"")  # raises a NotFoundError
```

Colab notebook: https://colab.research.google.com/drive/13bWmaJ40aeQKd2pUfNtnfwaaUP1M_IVL?authuser=1#"
47313,BUG: gfile.walk return full paths instead of filenames for ram:// filesystems,"Tested via tf-nightly on 2020-02-22 (YYYY-MM-DD).

Code to reproduce:

```python3
from tensorflow.io import gfile

gfile.mkdir(""ram://testdir"")
with gfile.GFile(""ram://testdir/file.txt"", ""w"") as f:
    f.write(""test"")

print(list(gfile.walk(""ram://testdir"")))
```

The third element is `['ram://testdir/file.txt']`, but it should be just `['file.txt']`

Colab notebook: https://colab.research.google.com/drive/13bWmaJ40aeQKd2pUfNtnfwaaUP1M_IVL?authuser=1#"
47312,CUDA 11.2/TF 2.5.0 Failed to load libcudnn.so.8,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.5.0-dev20210218
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: pip
- GCC/Compiler version (if compiling from source): 10.2.0
- CUDA/cuDNN version: 11.2
- GPU model and memory: NVIDIA GeForce RTX 2060 Super



I have gone through every installation guide, installed/reinstalled TF many times, as well as cuda and cudnn. But everytime i run:
```

import tensorflow as tf
tf.config.list_physical_devices()
```
I get:
```

2021-02-22 10:08:07.383300: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-02-22 10:08:11.985232: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-02-22 10:08:12.014912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-22 10:08:12.015228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1770] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.695GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2021-02-22 10:08:12.015246: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-02-22 10:08:12.016727: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-02-22 10:08:12.016757: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-02-22 10:08:12.017340: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-02-22 10:08:12.032485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-02-22 10:08:12.050575: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-02-22 10:08:12.051062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-02-22 10:08:12.051213: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-02-22 10:08:12.051239: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1803] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...

Out[3]: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]

```
I have three cuda folders in /usr/local/, cuda, cuda-11 and cuda-11.2 and they all have the 'libcudnn.so.8' file.
"
47311,TypeError: Cannot convert a symbolic Keras input/output to a numpy array.,"I dont quite understand why i'm getting this error. i came back to this project after updating some things and now my code wont work. Below is the code for my model. Any idea how I can avoid this? 

```
from tensorflow import keras
import tensorflow.keras.backend as K
import tensorflow as tf

LEARNING_RATE = 1e-4
HIDDEN_SIZE = 32
CLIPPING = 0.2
LOSS = 1e-5


# PPO loss function
def PPO_loss(advantage, old_prediction):
    def loss(y_true, y_pred):
        prob = y_true * y_pred
        old_prob = y_true * old_prediction
        r = prob/(old_prob + 1e-10)

        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - CLIPPING, max_value=1 + CLIPPING) * advantage) + LOSS * -(prob * K.log(prob + 1e-10)))

    return loss


class PPO:
    def __init__(self, statesize, num_intruders, actionsize, valuesize):
        self.statesize = statesize
        self.num_intruders = num_intruders
        self.actionsize = 5
        self.valuesize = valuesize

        self.model = self.__build_linear__()

    def __build_linear__(self):
        # Input of the aircraft of focus
        _input = keras.layers.Input(
            shape=(self.statesize,), name='input_state')

        # This is the input for the n_closest aircraft
        _input_context = keras.layers.Input(
            shape=(self.num_intruders, 7), name='input_context')

        # Empty layer
        empty = keras.layers.Input(shape=(HIDDEN_SIZE,), name='empty')

        # Input for advantages
        advantage = keras.layers.Input(shape=(1,), name=""advantage"")

        # Input old prediction
        old_prediction = keras.layers.Input(
            shape=(self.actionsize,), name='old_predictions')

        # Flatten the context layer (As context is passed as an n*m tensor)
        flatten_context = keras.layers.Flatten()(_input_context)

        # Hidden Layers

        # 1st hidden applies to the context only
        h1 = keras.layers.Dense(
            HIDDEN_SIZE, activation='relu')(flatten_context)

        # Combine the input and the context
        combine = keras.layers.concatenate([_input, h1], axis=1)

        # Hidden layers 2 & 3 apply to all inputs
        h2 = keras.layers.Dense(256, activation='relu')(combine)
        h3 = keras.layers.Dense(256, activation='relu')(h2)

        # Output layer
        out = keras.layers.Dense(self.actionsize+1, activation=None)(h3)

        # Policy and value layer processing
        policy = keras.layers.Lambda(
            lambda x: x[:, :self.actionsize], output_shape=(self.actionsize,))(out)
        value = keras.layers.Lambda(
            lambda x: x[:, self.actionsize:], output_shape=(self.valuesize,))(out)

        # Policy and value outputs
        policy_out = keras.layers.Activation(
            'softmax', name='policy_out')(policy)
        value_out = keras.layers.Activation(
            'linear', name='value_out')(value)

        # Optimizer
        optimizer = keras.optimizers.Adam(lr=LEARNING_RATE)

        # Produce the model
        model = keras.models.Model(inputs=[
                                   _input, _input_context, empty, advantage, old_prediction], outputs=[policy_out, value_out])

        self.estimator = keras.models.Model(
            inputs=[_input, _input_context, empty], outputs=[policy_out, value_out])

        # Compile the model

        model.compile(optimizer=optimizer, loss={'policy_out': PPO_loss(
            advantage=advantage, old_prediction=old_prediction), 'value_out': 'mse'})

        print(model.summary())
        return model

```
"
47309,How to fix 'Tidx' type in TensorFlowOpLayer (Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6.1
- CUDA/cuDNN version: 11.1
- GPU model and memory: RTX3070 8GB

After upgrading to a newer version of TensorFlow (1.14 -> 2.4.0) in my project based on MaskRCNN I get an error when trying read model with model_from_json (tf.keras.model.model_from_json):
```ValueError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean/Mean' using Op<name=Mean; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>```
"
47308,Need a easy way to let different weights have different LR in Keras.,"**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes, may be


**Describe the feature and the current behavior/state.**
At nowtimes, if I want to train model and use defferent LR for weights, I need to give up `Model.Fit()`.


**Will this change the current api? How?**
Yes, probably:
- Layer.add_weight() may have extra param ""Init_LR""
- Weights may add attribute Init_LR
- Some others.


**Who will benefit with this feature?**
Anyone who want to use this.
Developer who also use PyTorch."
47307,Building v2.3.1 from sources with avx512 support on a system without avx512,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.1
- Python version: 2.7.15
- Installed using virtualenv? pip? conda?: build from sources
- Bazel version (if compiling from source): 3.7.0
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: without cuda
- GPU model and memory: without GPU



**Describe the problem**
I am able to build and run TF 2.3.1 using `-msse3`. I am now trying to build it for `-march=skylake-avx512` on a system which is not `skylake-avx512`. As the build system does not support avx512 instruction sets so during the build phase it crashes with error like

```
ERROR: tensorflow-2.3.1/tensorflow/compiler/mlir/tensorflow/BUILD:171:7: Generating code from table: transforms/canonicalize.td //tensorflow
/compiler/mlir/tensorflow:tensorflow_canonicalize_inc_gen__gen_rewriters_genrule failed (Illegal instruction): bash failed: error executing command
```

I think this is because many internal tools e.g.  `bazel-out/k8-opt/bin/tensorflow/compiler/mlir/xla/operator_writer_gen` were build with `-march=skylake-avx512` flags.  I was wondering if there is a way to build these internal mlir tools with `-march=native` only

"
47306,TopKV2 InvalidArgumentError: k must be scalar,"Running on Google Colab
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- TensorFlow version: 2.4.1
- Python version: 3.6.9
- CUDA/cuDNN version: 10.1

### Current behaviour

I am trying to use tf.raw_ops.TopKV2 to get the top k values of my output, with k varying per row.

As stated in the documentation k expects ""A Tensor of type int32. 0-D.  Number of top elements to look for along the last dimension (along each row for matrices)."" 

However, it gives me InvalidArgumentError: k must be scalar, got shape [2] [Op:TopKV2]

### Code


```
import numpy as np
import tensorflow as tf

input = np.array([[0,1,2], [5,4,3]])
k = np.array([1,2])

tf.raw_ops.TopKV2(input=input, k=k)
```
Trace:
```
InvalidArgumentError                      Traceback (most recent call last)

<ipython-input-11-245902c47f71> in <module>()
      5 k = np.array([1,2])
      6 
----> 7 tf.raw_ops.TopKV2(input=input, k=k)

3 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_export.py in wrapper(*args, **kwargs)
    402           'Please pass these args as kwargs instead.'
    403           .format(f=f.__name__, kwargs=f_argspec.args))
--> 404     return f(**kwargs)
    405 
    406   return tf_decorator.make_decorator(f, wrapper, decorator_argspec=f_argspec)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py in top_kv2(input, k, sorted, name)
  11426       return _result
  11427     except _core._NotOkStatusException as e:
> 11428       _ops.raise_from_not_ok_status(e, name)
  11429     except _core._FallbackException:
  11430       pass

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6860   message = e.message + ("" name: "" + name if name is not None else """")
   6861   # pylint: disable=protected-access
-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)
   6863   # pylint: enable=protected-access
   6864 

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: k must be scalar, got shape [2] [Op:TopKV2]
```
### Expected behaviour

I expect topKV2 to accept a list/array/tensor for the k parameter. Returning value 2, index 2 for the first row and values 5 4, indices 0 1 for the second row.

### Extra

Additionally, the documentation example use of TopKV2 is
```
tf.raw_ops.TopKV2(
    input, k, sorted=True, name=None
)
```

However, providing the input and k arguments directly gives the following error:
top_kv2 only takes keyword args (possible keys: ['input', 'k', 'sorted', 'name']). Please pass these args as kwargs instead.


"
47305,"tutorial URL outdated, should probably be ""www.tensorflow.org/tutorials/audio/simple_audio""","The url 
`https://www.tensorflow.org/tutorials/audio_recognition`
in line 21 of `tensorflow/tensorflow/examples/speech_commands/train.py` is outdated and produces an 404 error. 

It should probably be:
`https://www.tensorflow.org/tutorials/audio/simple_audio`
"
47303,Add compatability for h5py 3,"**System information**
- TensorFlow version (you are using): 2.4
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
We are developing some software that relies heavily on both H5Py and TensorFlow and require the latest release of each. In terms of functionality the code will run however if the user installs both separately but the setup.py will throw errors reading conflicts in dependencies.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Those who wish to have a python module that relies on H5Py 3 and TensorFlow

**Any Other info.**
"
47301,`saved_model_cli show` report a error when apply it to a bert fine-tuned model,"**System information**
- Have I written custom code: No
- OS Platform and Distribution: Ubuntu 16.04.2 LTS
- TensorFlow installed from binary 
- TensorFlow version v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.7.9
- CUDA/cuDNN version: 11.0
- GPU model and memory: Quadro P5000 16G

**Describe the current behavior**

I am fine-tune a bert model for text classification, and there are preprocess procedure inside the model. The pre-trained model and preprocess model are all from tfhub so it should be fine, and code written by me is all about read file, build model with a few dense layers, fit, evaluate, predict and save.

After all these, I want to use `saved_model_cli` before serving like introduced in [this tutorial](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#save_your_model)

When using command

```bash
$ saved_model_cli show --dir training_checkpoints_sm/ --all
2021-02-22 11:38:35.613708: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is: 

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['text'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: serving_default_text:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['classifier'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall_2:0
  Method name is: tensorflow/serving/predict
Traceback (most recent call last):
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3929, in _get_op_def
    return self._op_def_cache[type]
KeyError: 'RegexSplitWithOffsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 890, in load_internal
    ckpt_options, filters)
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 132, in __init__
    meta_graph.graph_def.library))
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py"", line 340, in load_function_def_library
    func_graph = function_def_lib.function_def_to_graph(copy)
  File ""/home/kingsoft/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/framework/function_def_to_graph.py"", line 59, in function_def_to_graph
    fdef, input_shapes)
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/framework/function_def_to_graph.py"", line 220, in function_def_to_graph_def
    op_def = default_graph._get_op_def(node_def.op)  # pylint: disable=protected-access
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3934, in _get_op_def
    buf)
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'RegexSplitWithOffsets' in binary running on k8s-w-10-13-84-8. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/user/anaconda3/envs/gang/bin/saved_model_cli"", line 8, in <module>
    sys.exit(main())
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 1192, in main
    args.func(args)
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 719, in show
    _show_all(args.dir)
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 307, in _show_all
    _show_defined_functions(saved_model_dir)
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 187, in _show_defined_functions
    trackable_object = load.load(saved_model_dir)
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 859, in load
    return load_internal(export_dir, tags, options)[""root""]
  File ""/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 893, in load_internal
    str(err) + ""\n If trying to load on a different device from the ""
FileNotFoundError: Op type not registered 'RegexSplitWithOffsets' in binary running on k8s-w-10-13-84-8. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
 If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'.
```

This error `KeyError: 'RegexSplitWithOffsets'` really looks like when I forget to import `tensorflow_text` in my code.

```python
import tensorflow_text as text  # Registers the ops.
```

**Describe the expected behavior**

Expected behavior is I can import tensorflow_text when using this tool `saved_model_cli`.

**Standalone code to reproduce the issue**
```python
def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessor = hub.KerasLayer(
        ""https://tfhub.dev/tensorflow/bert_zh_preprocess/3"", name=""preprocessor"")
    encoder_inputs = preprocessor(text_input)
    encoder = hub.KerasLayer(
        ""https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/3"",
        trainable=True,
        name=""BERT_encoder"")
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)
    return tf.keras.Model(text_input, net)

...

history = classifier_model.fit(x=train_dataset,
                               epochs=epochs,
                               callbacks=[checkpoint_callback, early_stopping],
                               validation_data=val_dataset,
                               class_weight=class_weight)

...

classifier_model.save(checkpoint_dir + ""_sm"")

...

```

Can I successfully use `saved_model_cli`? or skip this part in that tutorial maybe?
"
47299,TFLM / example: micro_speech / mbed compile -m DISCO_F746NG -t GCC_ARM collect2: error: ld returned 1 exit status,"@tensorflow/micro

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1
- TensorFlow installed from (source or binary): source
- TensorFlow version: TensorFlow/Lite/Micro
- Python version: Python 2.7.17 and Python3 3.6.9
- Installed using virtualenv? pip? conda?: pip, and no virtualenv, no conda
- GCC/Compiler version (if compiling from source): gcc 7.5.0
- mbed version: 1.10.5
- Target platform: DISCO_F746NG

**Describe the problem**
TensorFlow Lite Micro cross compile for micro speech example at DISCO_F746NG platform. It was 100% completed in compile, but report link error.
I follow the advice from [issue 46721](https://github.com/tensorflow/tensorflow/issues/46721) and modify the tensorflow/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc as the following, see also [link](https://github.com/marconi1964/tensorflow/blob/example/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc)

`# Settings for the Discovery STM32F746NG board.`
`ifeq ($(TARGET),$(filter $(TARGET),mbed))`

`  micro_speech_MBED_PROJECT_FILES += \`
`    AUDIO_DISCO_F746NG.lib \`
`    BSP_DISCO_F746NG.lib \`
`    SDRAM_DISCO_F746NG.lib \`
`    LCD_DISCO_F746NG.lib`
`  MICRO_SPEECH_SRCS += \`
`	tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc \`
`	tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.cc`

`endif`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
The procedures are
`$ cd tensorflow`
`$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed generate_micro_speech_mbed_project`
`$ mbed config root .`
`$ mbed deploy`
`$ mbed compile -m DISCO_F746NG -t GCC_ARM`
collect2: error: ld returned 1 exit status

[mbed] ERROR: ""/usr/bin/python"" returned error.
       Code: 1
       Path: ""/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed""
       Command: ""/usr/bin/python -u /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM""
       Tip: You could retry the last command with ""-v"" flag for verbose output

Below is the complete log of last command with -v flag.

**Any other info / logs**
`ubuntu@ubuntu:~/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed$ /usr/bin/python -u /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM -v`

[Warning] @,: Compiler version mismatch: Have 7.3.1; expected version >= 9.0.0 and < 10.0.0
Building project mbed (DISCO_F746NG, GCC_ARM)
Scan: mbed
Macros: -DDEVICE_CRC=1 -DTARGET_STM32F746xG -DDEVICE_I2C_ASYNCH=1 -DDEVICE_EMAC=1 -D__MBED__=1 -DDEVICE_I2CSLAVE=1 -D__FPU_PRESENT=1 -DDEVICE_PORTOUT=1 -DDEVICE_PORTINOUT=1 -D__MBED_CMSIS_RTOS_CM -DTARGET_DISCO_F746NG -DCOMPONENT_FLASHIAP=1 -DTARGET_STM32F7 -DDEVICE_MPU=1 -DDEVICE_SERIAL_ASYNCH=1 -D__CMSIS_RTOS -DTARGET_N25Q128A -DTOOLCHAIN_GCC -DDEVICE_CAN=1 -DARM_MATH_CM7 -DTARGET_CORTEX_M -DTARGET_LIKE_CORTEX_M7 -DDEVICE_RTC=1 -DDEVICE_ANALOGOUT=1 -DDEVICE_QSPI=1 -DTARGET_M7 -DCOMPONENT_PSA_SRV_IMPL=1 -DEXTRA_IDLE_STACK_REQUIRED -DDEVICE_LPTICKER=1 -DDEVICE_PWMOUT=1 -DDEVICE_SPI_ASYNCH=1 -DMBED_TICKLESS -DUSE_FULL_LL_DRIVER -DCOMPONENT_QSPIF=1 -DTARGET_CORTEX -DDEVICE_I2C=1 -DTRANSACTION_QUEUE_SIZE_SPI=2 -DDEVICE_USBDEVICE=1 -DDEVICE_STDIO_MESSAGES=1 -D__CORTEX_M7 -DTARGET_FAMILY_STM32 -DUSE_HAL_DRIVER -DTARGET_FF_ARDUINO -DHSE_VALUE=25000000 -DTARGET_RELEASE -DTARGET_STM -DTARGET_NAME=DISCO_F746NG -DDEVICE_SERIAL_FC=1 -DCOMPONENT_PSA_SRV_EMUL=1 -DDEVICE_USTICKER=1 -DDEVICE_WATCHDOG=1 -DDEVICE_TRNG=1 -DTARGET_LIKE_MBED -DTARGET_RTOS_M4_M7 -DDEVICE_SLEEP=1 -DTOOLCHAIN_GCC_ARM -DMBED_BUILD_TIMESTAMP=1613922146.79 -DDEVICE_RESET_REASON=1 -DDEVICE_SPI=1 -DCOMPONENT_NSPE=1 -DDEVICE_INTERRUPTIN=1 -DDEVICE_SPISLAVE=1 -DDEVICE_ANALOGIN=1 -DDEVICE_SERIAL=1 -DDEVICE_FLASH=1 -DDEVICE_PORTIN=1 -DSTM32F746xx
Link: mbed
Preproc: arm-none-eabi-cpp -E -P ./mbed-os/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F746xG/device/TOOLCHAIN_GCC_ARM/STM32F746xG.ld -Wl,--gc-sections -Wl,--wrap,main -Wl,--wrap,_malloc_r -Wl,--wrap,_free_r -Wl,--wrap,_realloc_r -Wl,--wrap,_memalign_r -Wl,--wrap,_calloc_r -Wl,--wrap,exit -Wl,--wrap,atexit -Wl,-n -Wl,--wrap,printf -Wl,--wrap,sprintf -Wl,--wrap,snprintf -Wl,--wrap,vprintf -Wl,--wrap,vsprintf -Wl,--wrap,vsnprintf -Wl,--wrap,fprintf -Wl,--wrap,vfprintf -mcpu=cortex-m7 -mthumb -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -fmessage-length=0 -fno-exceptions -ffunction-sections -fdata-sections -funsigned-char -MMD -fomit-frame-pointer -Os -g -DMBED_TRAP_ERRORS_ENABLED=1 -DMBED_MINIMAL_PRINTF -mcpu=cortex-m7 -mthumb -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -DMBED_ROM_START=0x8000000 -DMBED_ROM_SIZE=0x100000 -DMBED_ROM1_START=0x200000 -DMBED_ROM1_SIZE=0x100000 -DMBED_RAM_START=0x20010000 -DMBED_RAM_SIZE=0x40000 -DMBED_RAM1_START=0x20000000 -DMBED_RAM1_SIZE=0x10000 -DMBED_BOOT_STACK_SIZE=4096 -DXIP_ENABLE=0 -o ./BUILD/DISCO_F746NG/GCC_ARM/.link_script.ld -DDEVICE_CRC=1 -DTARGET_STM32F746xG -DDEVICE_I2C_ASYNCH=1 -DDEVICE_EMAC=1 -D__MBED__=1 -DDEVICE_I2CSLAVE=1 -D__FPU_PRESENT=1 -DDEVICE_PORTOUT=1 -DDEVICE_PORTINOUT=1 -D__MBED_CMSIS_RTOS_CM -DTARGET_DISCO_F746NG -DCOMPONENT_FLASHIAP=1 -DTARGET_STM32F7 -DDEVICE_MPU=1 -DDEVICE_SERIAL_ASYNCH=1 -D__CMSIS_RTOS -DTARGET_N25Q128A -DTOOLCHAIN_GCC -DDEVICE_CAN=1 -DARM_MATH_CM7 -DTARGET_CORTEX_M -DTARGET_LIKE_CORTEX_M7 -DDEVICE_RTC=1 -DDEVICE_ANALOGOUT=1 -DDEVICE_QSPI=1 -DTARGET_M7 -DCOMPONENT_PSA_SRV_IMPL=1 -DEXTRA_IDLE_STACK_REQUIRED -DDEVICE_LPTICKER=1 -DDEVICE_PWMOUT=1 -DDEVICE_SPI_ASYNCH=1 -DMBED_TICKLESS -DUSE_FULL_LL_DRIVER -DCOMPONENT_QSPIF=1 -DTARGET_CORTEX -DDEVICE_I2C=1 -DTRANSACTION_QUEUE_SIZE_SPI=2 -DDEVICE_USBDEVICE=1 -DDEVICE_STDIO_MESSAGES=1 -D__CORTEX_M7 -DTARGET_FAMILY_STM32 -DUSE_HAL_DRIVER -DTARGET_FF_ARDUINO -DHSE_VALUE=25000000 -DTARGET_RELEASE -DTARGET_STM -DTARGET_NAME=DISCO_F746NG -DDEVICE_SERIAL_FC=1 -DCOMPONENT_PSA_SRV_EMUL=1 -DDEVICE_USTICKER=1 -DDEVICE_WATCHDOG=1 -DDEVICE_TRNG=1 -DTARGET_LIKE_MBED -DTARGET_RTOS_M4_M7 -DDEVICE_SLEEP=1 -DTOOLCHAIN_GCC_ARM -DMBED_BUILD_TIMESTAMP=1613922146.79 -DDEVICE_RESET_REASON=1 -DDEVICE_SPI=1 -DCOMPONENT_NSPE=1 -DDEVICE_INTERRUPTIN=1 -DDEVICE_SPISLAVE=1 -DDEVICE_ANALOGIN=1 -DDEVICE_SERIAL=1 -DDEVICE_FLASH=1 -DDEVICE_PORTIN=1 -DSTM32F746xx @./BUILD/DISCO_F746NG/GCC_ARM/.includes_d41d8cd98f00b204e9800998ecf8427e.txt -include ./BUILD/DISCO_F746NG/GCC_ARM/mbed_config.h
[DEBUG] Return: 0
Link: arm-none-eabi-gcc @./BUILD/DISCO_F746NG/GCC_ARM/.link_options.txt
[DEBUG] Return: 1
[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)':
[DEBUG] Errors: /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:154: multiple definition of `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)'
[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:28: first defined here
[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `LatestAudioTimestamp()':
[DEBUG] Errors: /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:181: multiple definition of `LatestAudioTimestamp()'
[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:37: first defined here
[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.o: In function `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)':
[DEBUG] Errors: /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.cc:26: multiple definition of `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)'
[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/command_responder.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/command_responder.cc:23: first defined here
[DEBUG] Errors: collect2: error: ld returned 1 exit status
Traceback (most recent call last):
  File ""/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/make.py"", line 78, in wrapped_build_project
    *args, **kwargs
  File ""/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/build_api.py"", line 610, in build_project
    res = toolchain.link_program(resources, build_path, name)
  File ""/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/toolchains/mbed_toolchain.py"", line 778, in link_program
    self.link(elf, objects, libraries, lib_dirs, linker_script)
  File ""/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/toolchains/gcc.py"", line 357, in link
    self.default_cmd(cmd)
  File ""/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/toolchains/mbed_toolchain.py"", line 830, in default_cmd
    raise ToolException(stderr)
ToolException: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)':
/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:154: multiple definition of `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)'
BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:28: first defined here
BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `LatestAudioTimestamp()':
/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:181: multiple definition of `LatestAudioTimestamp()'
BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:37: first defined here
BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.o: In function `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)':
/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.cc:26: multiple definition of `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)'
BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/command_responder.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/command_responder.cc:23: first defined here
collect2: error: ld returned 1 exit status"
47297,Load Train Model From Checkpoint - NotFoundError,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- TensorFlow installed from (source or binary):
- https://github.com/tensorflow/models
- TensorFlow version: 2
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: conda and pip

**Describe the problem**
I have downloaded and installed tensorflow, and I'm attempting to train a custom model, but keep getting runtime errors or notfounderrors to do with tenorflow lib files.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
`WORKSPACE_PATH = 'Tensorflow/workspace'
SCRIPTS_PATH = 'Tensorflow/scripts'
APIMODEL_PATH = 'Tensorflow/models'
ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'
IMAGE_PATH = WORKSPACE_PATH+'/images'
MODEL_PATH = WORKSPACE_PATH+'/models'
PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'
CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'
CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'

labels = [{'name':'title', 'id':1}, {'name':'xaxis', 'id':2}, {'name':'yaxis', 'id':3}, {'name':'bar', 'id':4}, {'name':'key', 'id':5}]

with open(ANNOTATION_PATH + '\label_map.pbtxt', 'w') as f:
    for label in labels:
        f.write('item { \n')
        f.write('\tname:\'{}\'\n'.format(label['name']))
        f.write('\tid:{}\n'.format(label['id']))
        f.write('}\n')

!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}
!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x{IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}

!cd Tensorflow && git clone https://github.com/tensorflow/models

CUSTOM_MODEL_NAME = 'my_ssd_mobnet' 
!mkdir {'Tensorflow\workspace\models\\'+CUSTOM_MODEL_NAME}
!cp {PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config'} {MODEL_PATH+'/'+CUSTOM_MODEL_NAME}

import tensorflow as tf
from object_detection.utils import config_util
from object_detection.protos import pipeline_pb2
from google.protobuf import text_format

CONFIG_PATH = MODEL_PATH+'/'+CUSTOM_MODEL_NAME+'/pipeline.config'

config = config_util.get_configs_from_pipeline_file(CONFIG_PATH)
config
{'model': ssd {
   num_classes: 90
   image_resizer {
     fixed_shape_resizer {
       height: 320
       width: 320
     }
   }
   feature_extractor {
     type: ""ssd_mobilenet_v2_fpn_keras""
     depth_multiplier: 1.0
     min_depth: 16
     conv_hyperparams {
       regularizer {
         l2_regularizer {
           weight: 3.9999998989515007e-05
         }
       }
       initializer {
         random_normal_initializer {
           mean: 0.0
           stddev: 0.009999999776482582
         }
       }
       activation: RELU_6
       batch_norm {
         decay: 0.996999979019165
         scale: true
         epsilon: 0.0010000000474974513
       }
     }
     use_depthwise: true
     override_base_feature_extractor_hyperparams: true
     fpn {
       min_level: 3
       max_level: 7
       additional_layer_depth: 128
     }
   }
   box_coder {
     faster_rcnn_box_coder {
       y_scale: 10.0
       x_scale: 10.0
       height_scale: 5.0
       width_scale: 5.0
     }
   }
   matcher {
     argmax_matcher {
       matched_threshold: 0.5
       unmatched_threshold: 0.5
       ignore_thresholds: false
       negatives_lower_than_unmatched: true
       force_match_for_each_row: true
       use_matmul_gather: true
     }
   }
   similarity_calculator {
     iou_similarity {
     }
   }
   box_predictor {
     weight_shared_convolutional_box_predictor {
       conv_hyperparams {
         regularizer {
           l2_regularizer {
             weight: 3.9999998989515007e-05
           }
         }
         initializer {
           random_normal_initializer {
             mean: 0.0
             stddev: 0.009999999776482582
           }
         }
         activation: RELU_6
         batch_norm {
           decay: 0.996999979019165
           scale: true
           epsilon: 0.0010000000474974513
         }
       }
       depth: 128
       num_layers_before_predictor: 4
       kernel_size: 3
       class_prediction_bias_init: -4.599999904632568
       share_prediction_tower: true
       use_depthwise: true
     }
   }
   anchor_generator {
     multiscale_anchor_generator {
       min_level: 3
       max_level: 7
       anchor_scale: 4.0
       aspect_ratios: 1.0
       aspect_ratios: 2.0
       aspect_ratios: 0.5
       scales_per_octave: 2
     }
   }
   post_processing {
     batch_non_max_suppression {
       score_threshold: 9.99999993922529e-09
       iou_threshold: 0.6000000238418579
       max_detections_per_class: 100
       max_total_detections: 100
       use_static_shapes: false
     }
     score_converter: SIGMOID
   }
   normalize_loss_by_num_matches: true
   loss {
     localization_loss {
       weighted_smooth_l1 {
       }
     }
     classification_loss {
       weighted_sigmoid_focal {
         gamma: 2.0
         alpha: 0.25
       }
     }
     classification_weight: 1.0
     localization_weight: 1.0
   }
   encode_background_as_zeros: true
   normalize_loc_loss_by_codesize: true
   inplace_batchnorm_update: true
   freeze_batchnorm: false
 }, 'train_config': batch_size: 128
 data_augmentation_options {
   random_horizontal_flip {
   }
 }
 data_augmentation_options {
   random_crop_image {
     min_object_covered: 0.0
     min_aspect_ratio: 0.75
     max_aspect_ratio: 3.0
     min_area: 0.75
     max_area: 1.0
     overlap_thresh: 0.0
   }
 }
 sync_replicas: true
 optimizer {
   momentum_optimizer {
     learning_rate {
       cosine_decay_learning_rate {
         learning_rate_base: 0.07999999821186066
         total_steps: 50000
         warmup_learning_rate: 0.026666000485420227
         warmup_steps: 1000
       }
     }
     momentum_optimizer_value: 0.8999999761581421
   }
   use_moving_average: false
 }
 fine_tune_checkpoint: ""PATH_TO_BE_CONFIGURED""
 num_steps: 50000
 startup_delay_steps: 0.0
 replicas_to_aggregate: 8
 max_number_of_boxes: 100
 unpad_groundtruth_tensors: false
 fine_tune_checkpoint_type: ""classification""
 fine_tune_checkpoint_version: V2, 'train_input_config': label_map_path: ""PATH_TO_BE_CONFIGURED""
 tf_record_input_reader {
   input_path: ""PATH_TO_BE_CONFIGURED""
 }, 'eval_config': metrics_set: ""coco_detection_metrics""
 use_moving_averages: false, 'eval_input_configs': [label_map_path: ""PATH_TO_BE_CONFIGURED""
 shuffle: false
 num_epochs: 1
 tf_record_input_reader {
   input_path: ""PATH_TO_BE_CONFIGURED""
 }
 ], 'eval_input_config': label_map_path: ""PATH_TO_BE_CONFIGURED""
 shuffle: false
 num_epochs: 1
 tf_record_input_reader {
   input_path: ""PATH_TO_BE_CONFIGURED""
 }}
pipeline_config = pipeline_p

pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
with tf.io.gfile.GFile(CONFIG_PATH, ""r"") as f:                                                                                                                                                                                                                     
    proto_str = f.read()                                                                                                                                                                                                                                          
    text_format.Merge(proto_str, pipeline_config)  
pipeline_config.model.ssd.num_classes = 2
pipeline_config.train_config.batch_size = 4
pipeline_config.train_config.fine_tune_checkpoint = PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'
pipeline_config.train_config.fine_tune_checkpoint_type = ""detection""
pipeline_config.train_input_reader.label_map_path= ANNOTATION_PATH + '/label_map.pbtxt'
pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/train.record']
pipeline_config.eval_input_reader[0].label_map_path = ANNOTATION_PATH + '/label_map.pbtxt'
pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/test.record']

config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        
with tf.io.gfile.GFile(CONFIG_PATH, ""wb"") as f:                                                                                                                                                                                                                     
    f.write(config_text)   

print(""""""python {}/research/object_detection/model_main_tf2.py --model_dir={}/{} --pipeline_config_path={}/{}/pipeline.config --num_train_steps=5000"""""".format(APIMODEL_PATH, MODEL_PATH,CUSTOM_MODEL_NAME,MODEL_PATH,CUSTOM_MODEL_NAME))

import os
from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as viz_utils
from object_detection.builders import model_builder

# Load pipeline config and build a detection model
configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)
detection_model = model_builder.build(model_config=configs['model'], is_training=False)

# Restore checkpoint
ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()

@tf.function
def detect_fn(image):
    image, shapes = detection_model.preprocess(image)
    prediction_dict = detection_model.predict(image, shapes)
    detections = detection_model.postprocess(prediction_dict, shapes)
    return detections
#---------------------This Is Where The error Happens ----------------------------------------`


**Error Message**
`---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\py_checkpoint_reader.py in NewCheckpointReader(filepattern)
     94   try:
---> 95     return CheckpointReader(compat.as_bytes(filepattern))
     96   # TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the

RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\tracking\util.py in restore(self, save_path, options)
   2259     try:
-> 2260       status = self.read(save_path, options=options)
   2261     except errors_impl.NotFoundError:

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\tracking\util.py in read(self, save_path, options)
   2147     options = options or checkpoint_options.CheckpointOptions()
-> 2148     return self._saver.restore(save_path=save_path, options=options)
   2149 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\tracking\util.py in restore(self, save_path, options)
   1291       return InitializationOnlyStatus(self._graph_view, ops.uid())
-> 1292     reader = py_checkpoint_reader.NewCheckpointReader(save_path)
   1293     graph_building = not context.executing_eagerly()

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\py_checkpoint_reader.py in NewCheckpointReader(filepattern)
     98   except RuntimeError as e:
---> 99     error_translator(e)

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\py_checkpoint_reader.py in error_translator(e)
     34       'matching files for') in error_message:
---> 35     raise errors_impl.NotFoundError(None, None, error_message)
     36   elif 'Sliced checkpoints are not supported' in error_message or (

NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-17-f5bf27ce595e> in <module>
      5 # Restore checkpoint
      6 ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
----> 7 ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()
      8 
      9 @tf.function

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\tracking\util.py in restore(self, save_path, options)
   2263           None, None,
   2264           ""Could not find checkpoint or SavedModel at {}.""
-> 2265           .format(orig_save_path))
   2266     # Create the save counter now so it gets initialized with other variables
   2267     # when graph building. Creating it earlier would lead to errors when using,

NotFoundError: Could not find checkpoint or SavedModel at Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6.`"
47294,Cant compile because there is no cuda.h,"
**System information**
- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installed from (source or binary): source 5534f5d3208
- TensorFlow version: 5534f5d3208
- Python version: 3.8.5
- Installed using: conda
- Bazel version (if compiling from source): 3.7.2 (installed with go get )
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- CUDA/cuDNN version: 
- GPU model and memory: GeForce RTX 3090 24267MiB
- NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2 



**Describe the problem**

Cant compile because there is no cuda.h

**Provide the exact sequence of commands / steps that you executed before running into the problem**

##  Bug

Can't compile with bazel because there is no cuda.h

## To Reproduce

Use this command inside tensorflow folder

```
(xla)  ~/Documents/github/pytorch/xla/third_party/tensorflow [:5534f5d3208|1] 
00:04 $ bazel build --define framework_shared_object=false -c opt --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --cxxopt=-std=c++14 --cxxopt=-Wno-c++11-narrowing --cxxopt=-DXLA_CUDA=1 --config=cuda //tensorflow/compiler/xla/xla_client:libxla_computation_client.so
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Found applicable config definition build:short_logs in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:linux in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Repository local_config_cuda instantiated at:
  /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/WORKSPACE:12:10: in <toplevel>
  /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/workspace2.bzl:13:20: in workspace
  /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/workspace.bzl:95:19: in tf_repositories
Repository rule cuda_configure defined at:
  /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl:1430:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
	File ""/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1400, column 38, in _cuda_autoconf_impl
		_create_local_cuda_repository(repository_ctx)
	File ""/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl"", line 977, column 35, in _create_local_cuda_repository
		cuda_config = _get_cuda_config(repository_ctx, find_cuda_config_script)
	File ""/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl"", line 666, column 30, in _get_cuda_config
		config = find_cuda_config(repository_ctx, find_cuda_config_script, [""cuda"", ""cudnn""])
	File ""/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl"", line 643, column 41, in find_cuda_config
		exec_result = _exec_find_cuda_config(repository_ctx, script_path, cuda_libraries)
	File ""/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl"", line 637, column 19, in _exec_find_cuda_config
		return execute(repository_ctx, [python_bin, ""-c"", decompress_and_execute_cmd])
	File ""/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/remote_config/common.bzl"", line 217, column 13, in execute
		fail(
Error in fail: Repository command failed
Could not find any cuda.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
        '/lib'
        '/lib/i386-linux-gnu'
        '/lib/i386-linux-gnu/i686/sse2'
        '/lib/i386-linux-gnu/sse2'
        '/lib/x86_64-linux-gnu'
        '/lib32'
        '/usr'
        '/usr/lib/x86_64-linux-gnu/libfakeroot'
        '/usr/local/cuda'
        '/usr/local/cuda-11.1/targets/x86_64-linux/lib'
        '/usr/local/lib'
ERROR: Skipping '//tensorflow/compiler/xla/xla_client:libxla_computation_client.so': no such package '@local_config_cuda//cuda': Repository command failed
Could not find any cuda.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
        '/lib'
        '/lib/i386-linux-gnu'
        '/lib/i386-linux-gnu/i686/sse2'
        '/lib/i386-linux-gnu/sse2'
        '/lib/x86_64-linux-gnu'
        '/lib32'
        '/usr'
        '/usr/lib/x86_64-linux-gnu/libfakeroot'
        '/usr/local/cuda'
        '/usr/local/cuda-11.1/targets/x86_64-linux/lib'
        '/usr/local/lib'
WARNING: Target pattern parsing failed.
ERROR: no such package '@local_config_cuda//cuda': Repository command failed
Could not find any cuda.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
        '/lib'
        '/lib/i386-linux-gnu'
        '/lib/i386-linux-gnu/i686/sse2'
        '/lib/i386-linux-gnu/sse2'
        '/lib/x86_64-linux-gnu'
        '/lib32'
        '/usr'
        '/usr/lib/x86_64-linux-gnu/libfakeroot'
        '/usr/local/cuda'
        '/usr/local/cuda-11.1/targets/x86_64-linux/lib'
        '/usr/local/lib'
INFO: Elapsed time: 0.215s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/compiler/xla/xla_client

```



**Any other info / logs**

```
$ cat tf_env.txt

== check python ===================================================
python version: 3.8.5
python branch: 
python build version: ('default', 'Sep  4 2020 07:30:14')
python compiler version: GCC 7.3.0
python implementation: CPython


== check os platform ===============================================

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                    1.20.1
protobuf                 3.15.1

== check for virtualenv =========================================
False

== tensorflow import ============================================
    147497:	find library=libpthread.so.0 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/haswell:/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/x86_64:/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls:/home/tyoc213/miniconda3/envs/xla/bin/../lib/haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/bin/../lib/haswell:/home/tyoc213/miniconda3/envs/xla/bin/../lib/x86_64:/home/tyoc213/miniconda3/envs/xla/bin/../lib		(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/haswell/x86_64/libpthread.so.0
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/haswell/libpthread.so.0
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/x86_64/libpthread.so.0
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/libpthread.so.0
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/haswell/x86_64/libpthread.so.0
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/haswell/libpthread.so.0
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/x86_64/libpthread.so.0
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libpthread.so.0
    147497:	 search cache=/etc/ld.so.cache
    147497:	  trying file=/lib/x86_64-linux-gnu/libpthread.so.0
    147497:	
    147497:	find library=libc.so.6 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib		(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libc.so.6
    147497:	 search cache=/etc/ld.so.cache
    147497:	  trying file=/lib/x86_64-linux-gnu/libc.so.6
    147497:	
    147497:	find library=libdl.so.2 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib		(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libdl.so.2
    147497:	 search cache=/etc/ld.so.cache
    147497:	  trying file=/lib/x86_64-linux-gnu/libdl.so.2
    147497:	
    147497:	find library=libutil.so.1 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib		(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libutil.so.1
    147497:	 search cache=/etc/ld.so.cache
    147497:	  trying file=/lib/x86_64-linux-gnu/libutil.so.1
    147497:	
    147497:	find library=librt.so.1 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib		(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/librt.so.1
    147497:	 search cache=/etc/ld.so.cache
    147497:	  trying file=/lib/x86_64-linux-gnu/librt.so.1
    147497:	
    147497:	find library=libm.so.6 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib		(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libm.so.6
    147497:	 search cache=/etc/ld.so.cache
    147497:	  trying file=/lib/x86_64-linux-gnu/libm.so.6
    147497:	
    147497:	
    147497:	calling init: /lib/x86_64-linux-gnu/libpthread.so.0
    147497:	
    147497:	
    147497:	calling init: /lib/x86_64-linux-gnu/libc.so.6
    147497:	
    147497:	
    147497:	calling init: /lib/x86_64-linux-gnu/libm.so.6
    147497:	
    147497:	
    147497:	calling init: /lib/x86_64-linux-gnu/librt.so.1
    147497:	
    147497:	
    147497:	calling init: /lib/x86_64-linux-gnu/libutil.so.1
    147497:	
    147497:	
    147497:	calling init: /lib/x86_64-linux-gnu/libdl.so.2
    147497:	
    147497:	
    147497:	initialize program: /home/tyoc213/miniconda3/envs/xla/bin/python
    147497:	
    147497:	
    147497:	transferring control: /home/tyoc213/miniconda3/envs/xla/bin/python
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	find library=libffi.so.7 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/haswell:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../haswell:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../..(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/haswell/x86_64/libffi.so.7
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/haswell/libffi.so.7
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/x86_64/libffi.so.7
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/libffi.so.7
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../haswell/x86_64/libffi.so.7
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../haswell/libffi.so.7
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../x86_64/libffi.so.7
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libffi.so.7
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libffi.so.7
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/termios.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/fcntl.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	find library=libopenblasp-r0-5bebc122.3.13.dev.so [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/haswell:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/haswell:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs		(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/haswell/x86_64/libopenblasp-r0-5bebc122.3.13.dev.so
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/haswell/libopenblasp-r0-5bebc122.3.13.dev.so
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/x86_64/libopenblasp-r0-5bebc122.3.13.dev.so
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/libopenblasp-r0-5bebc122.3.13.dev.so
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/haswell/x86_64/libopenblasp-r0-5bebc122.3.13.dev.so
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/haswell/libopenblasp-r0-5bebc122.3.13.dev.so
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/x86_64/libopenblasp-r0-5bebc122.3.13.dev.so
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so
    147497:	
    147497:	find library=libgfortran-2e0d59d6.so.5.0.0 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs		(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0
    147497:	
    147497:	find library=libquadmath-2d0c479f.so.0.0.0 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs		(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0
    147497:	
    147497:	find library=libz-eb09ad1d.so.1.2.3 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs		(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3
    147497:	
    147497:	find library=libgcc_s.so.1 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs		(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libgcc_s.so.1
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib		(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libgcc_s.so.1
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/bin/../lib/libgcc_s.so.1
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/linalg/lapack_lite.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	find library=libz.so.1 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../..		(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libz.so.1
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libz.so.1
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	find library=liblzma.so.5 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../..		(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../liblzma.so.5
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../liblzma.so.5
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	find library=libcrypto.so.1.1 [0]; searching
    147497:	 search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../..		(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)
    147497:	  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libcrypto.so.1.1
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libcrypto.so.1.1
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so
    147497:	
    147497:	
    147497:	calling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so
    147497:	
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/python/eager/context.py"", line 32, in <module>
    from tensorflow.core.framework import function_pb2
ImportError: cannot import name 'function_pb2' from 'tensorflow.core.framework' (unknown location)
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/bin/python [0]
    147497:	
    147497:	
    147497:	calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libffi.so.7 [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/termios.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/fcntl.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/linalg/lapack_lite.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0 [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/bin/../lib/libgcc_s.so.1 [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3 [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0 [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../liblzma.so.5 [0]
    147497:	
    147497:	
    147497:	calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libz.so.1 [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libcrypto.so.1.1 [0]
    147497:	
    147497:	
    147497:	calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]
    147497:	
    147497:	
    147497:	calling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so [0]
    147497:	
    147497:	
    147497:	calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]
    147497:	

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Sun Feb 21 00:14:33 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 3090    Off  | 00000000:02:00.0  On |                  N/A |
| 35%   33C    P8    24W / 350W |    453MiB / 24267MiB |     12%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      9920      G   /usr/lib/xorg/Xorg                 35MiB |
|    0   N/A  N/A     10224      G   /usr/lib/xorg/Xorg                209MiB |
|    0   N/A  N/A     10395      G   /usr/bin/gnome-shell               71MiB |
|    0   N/A  N/A     10421      G   ...mviewer/tv_bin/TeamViewer        4MiB |
|    0   N/A  N/A     13811      G   ...AAAAAAAA== --shared-files       33MiB |
|    0   N/A  N/A     16812      G   ...gAAAAAAAAA --shared-files       48MiB |
|    0   N/A  N/A     19304      G   ...AAAAAAAAA= --shared-files       23MiB |
|    0   N/A  N/A     92089      G   /usr/lib/firefox/firefox            4MiB |
|    0   N/A  N/A    119133      G   /usr/lib/firefox/firefox            4MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1.74

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 8, 5, 'final', 0)

== bazel version  ===============================================
Bazelisk version: development
Build label: 3.7.2
Build time: Thu Dec 17 16:57:23 2020 (1608224243)
Build timestamp: 1608224243
Build timestamp as int: 1608224243

```
"
47292,Can the GradCAM being integrate to savedModel format?,"**System information**
Colaboratory
Tensorflow 2.4
Tested on below version
v2.4.0-49-g85c8b2a817f 2.4.1
v2.3.0-54-gfcc4b966f1 2.3.1

**Describe the current behavior**
I'm trying to make a savedModel that can generate GradCAM heatmap, however it shows errors that the tape.gradient or tf.gradients is None can't be compute, Does anyone can help with this?


**Describe the expected behavior**
Expected that can save to savedModel .

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
This is the colaboratory notebook.

https://colab.research.google.com/drive/1uX4T_DZWaptZ4pF2akawB5FYYMgRtX7y?authuser=1#scrollTo=QBsBrZ5JEk-N

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47291,micro: port op LOG_SOFTMAX from lite,"@tensorflow/micro

This issue tracks my work porting operator LOG_SOFTMAX from lite to micro.

The port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:

PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
PR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences
PR 3: Copy operator from lite to micro making minimal changes and not including in the build
PR 4: Delete extra code from the micro copy of the operator
PR 5: Port micro copy of operator as necessary and add a corresponding test
"
47290,micro: port op CUMSUM from lite,"@tensorflow/micro

This issue tracks my work porting operator CUMSUM from lite to micro.

The port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:

PR 1 (step 1): Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
PR 2 (step 2): Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences

The next 3 steps are combined into a single PR3 with separate commits:

(step 3): Copy operator from lite to micro making minimal changes and not including in the build
(step 4): Delete extra code from the micro copy of the operator
(step 5): Port micro copy of operator as necessary and add a corresponding test"
47289,tf.keras.layers.ZeroPadding2D crashes (segfault),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):2.1.0
- Python version:3.7.6
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A

**Describe the current behavior**
tf.keras.layers.ZeroPadding2D crashes (segfault) when `padding` is large and input contains 0 in shape.

**Describe the expected behavior**
Expect graceful exception messages instead of crash

**Standalone code to reproduce the issue**
~~~python
import numpy as np
import tensorflow as tf
layer = tf.keras.layers.ZeroPadding2D(padding=420958214)
layer(np.ones((0, 4, 4, 4)))
~~~

Output
~~~python
Segmentation fault (core dumped)
~~~

"
47288,"    ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', ...]","This is not the first time I encounter this weird error, I keep getting every now and then and I don't know what causes it / how to fix it. I always end up trying alternative approaches until it's gone. Here's a colab [notebook](https://colab.research.google.com/drive/1eeEc9eIlvMVAJbueIfhpgh569pi-lGx4?usp=sharing) with the full code.

Training function:

```
    def train_step(self):
        with tf.GradientTape() as tape:
            (
                _,
                rewards,
                actions,
                value_logits,
                dones,
                _,
                entropies,
                actor_logits,
            ) = tf.numpy_function(
                self.np_train_step, [], [tf.float32 for _ in range(8)]
            )
            action_probs = tf.nn.softmax(actor_logits)
            values = tf.reduce_sum(action_probs * value_logits, axis=-1)
            action_indices = self.get_action_indices(self.batch_indices, actions)
            selected_probs = tf.gather_nd(action_probs, action_indices)
            selected_logits = tf.gather_nd(value_logits, action_indices)
            importance_ratio = action_probs / (action_probs + self.epsilon)
            action_importance = tf.gather_nd(importance_ratio, action_indices)
            returns = tf.numpy_function(
                self.calculate_returns,
                [rewards, values, dones, selected_logits, action_importance],
                tf.float32,
            )
            loss = self.compute_loss(
                returns,
                values,
                entropies,
                action_importance,
                value_logits,
                importance_ratio,
                action_probs,
                selected_probs,
                selected_logits,
            )
        grads = tape.gradient(loss, self.model.trainable_variables)
        if self.grad_norm is not None:
            grads, _ = tf.clip_by_global_norm(grads, self.grad_norm)
        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))
```
Results in:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-13-65446c8a838c> in <module>()
      7 o = MovingAverage(Adam(7e-4))
      8 agn = ACER(env, m, optimizer=o)
----> 9 agn.fit(19)

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, ""ag_error_metadata""):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

ValueError: in user code:

    <ipython-input-11-cc50c030ee4d>:151 train_step  *
        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))
    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/optimizers/average_wrapper.py:56 apply_gradients  *
        return super().apply_gradients(grads_and_vars, name, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:598 apply_gradients  **
        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/utils.py:79 filter_empty_gradients
        ([v.name for _, v in grads_and_vars],))

    ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'dense/kernel:0', 'dense/bias:0'].
```
"
47287, OSError: SavedModel file does not exist at: model_resnet152V2.h5/{saved_model.pbtxt|saved_model.pb},"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\saved_model\loader_impl.py"", line 113, in parse_saved_model
    constants.SAVED_MODEL_FILENAME_PB))

OSError: SavedModel file does not exist at: model_resnet152V2.h5/{saved_model.pbtxt|saved_model.pb}

### Source code / logs

from __future__ import division, print_function
# coding=utf-8
import sys
import os
import glob
import re
import numpy as np
import tensorflow as tf
import tensorflow as tf

from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.2
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
# Keras
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image

# Flask utils
from flask import Flask, redirect, url_for, request, render_template
from werkzeug.utils import secure_filename
#from gevent.pywsgi import WSGIServer

# Define a flask app
app = Flask(__name__)

# Model saved with Keras model.save()
MODEL_PATH ='model_resnet152V2.h5'

# Load your trained model
model = load_model(MODEL_PATH)




def model_predict(img_path, model):
    print(img_path)
    img = image.load_img(img_path, target_size=(224, 224))

    # Preprocessing the image
    x = image.img_to_array(img)
    # x = np.true_divide(x, 255)
    ## Scaling
    x=x/255
    x = np.expand_dims(x, axis=0)
   

    # Be careful how your trained model deals with the input
    # otherwise, it won't make correct prediction!
   # x = preprocess_input(x)

    preds = model.predict(x)
    preds=np.argmax(preds, axis=1)
    if preds==0:
        preds=""The leaf is diseased cotton leaf""
    elif preds==1:
        preds=""The leaf is diseased cotton plant""
    elif preds==2:
        preds=""The leaf is fresh cotton leaf""
    else:
        preds=""The leaf is fresh cotton plant""
        
    
    
    return preds


@app.route('/', methods=['GET'])
def index():
    # Main page
    return render_template('index.html')


@app.route('/predict', methods=['GET', 'POST'])
def upload():
    if request.method == 'POST':
        # Get the file from post request
        f = request.files['file']

        # Save the file to ./uploads
        basepath = os.path.dirname(__file__)
        file_path = os.path.join(
            basepath, 'uploads', secure_filename(f.filename))
        f.save(file_path)

        # Make prediction
        preds = model_predict(file_path, model)
        result=preds
        return result
    return None


if __name__ == '__main__':
    app.run(port=5001,debug=True)

"
47286,The Example Code in tf.feature_column.numeric_column is not Executable/Complete/Self-Sufficient,"Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column#example

Description of issue (what needs changing):
The example code cannot be Executed as it is. We have to add the namespaces by searching in the **`tensorflow.org`** site.

Complete/Self-Sufficient/Stand-Alone example code will be very helpful, especially for the New Developers."
47285,`tf.keras.backend` python api docs don't include most symbols exported,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/backend


## Description of issue (what needs changing):
### Clear description

`tensorflow.keras.backend` is a re-export module; The generated api document page for `tf.keras.backend` only exposes a very small portion of the exports.


For example, why should someone use this method? How is it useful?

### Correct links

But all of these methods _should_ be included:

```
In [8]: print(dir(tf.keras.backend))
['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_sys', 'abs', 'all', 'any', 'arange', 'argmax', 'argmin', 'backend', 'batch_dot', 'batch_flatten', 'batch_get_value', 'batch_normalization', 'batch_set_value', 'bias_add', 'binary_crossentropy', 'cast', 'cast_to_floatx', 'categorical_crossentropy', 'clear_session', 'clip', 'concatenate', 'constant', 'conv1d', 'conv2d', 'conv2d_transpose', 'conv3d', 'cos', 'count_params', 'ctc_batch_cost', 'ctc_decode', 'ctc_label_dense_to_sparse', 'cumprod', 'cumsum', 'depthwise_conv2d', 'dot', 'dropout', 'dtype', 'elu', 'epsilon', 'equal', 'eval', 'exp', 'expand_dims', 'eye', 'flatten', 'floatx', 'foldl', 'foldr', 'function', 'gather', 'get_uid', 'get_value', 'gradients', 'greater', 'greater_equal', 'hard_sigmoid', 'image_data_format', 'in_test_phase', 'in_top_k', 'in_train_phase', 'int_shape', 'is_keras_tensor', 'is_sparse', 'l2_normalize', 'learning_phase', 'learning_phase_scope', 'less', 'less_equal', 'local_conv1d', 'local_conv2d', 'log', 'manual_variable_initialization', 'map_fn', 'max', 'maximum', 'mean', 'min', 'minimum', 'moving_average_update', 'name_scope', 'ndim', 'normalize_batch_in_training', 'not_equal', 'one_hot', 'ones', 'ones_like', 'permute_dimensions', 'placeholder', 'pool2d', 'pool3d', 'pow', 'print_tensor', 'prod', 'random_bernoulli', 'random_binomial', 'random_normal', 'random_normal_variable', 'random_uniform', 'random_uniform_variable', 'relu', 'repeat', 'repeat_elements', 'reset_uids', 'reshape', 'resize_images', 'resize_volumes', 'reverse', 'rnn', 'round', 'separable_conv2d', 'set_epsilon', 'set_floatx', 'set_image_data_format', 'set_learning_phase', 'set_value', 'shape', 'sigmoid', 'sign', 'sin', 'softmax', 'softplus', 'softsign', 'sparse_categorical_crossentropy', 'spatial_2d_padding', 'spatial_3d_padding', 'sqrt', 'square', 'squeeze', 'stack', 'std', 'stop_gradient', 'sum', 'switch', 'tanh', 'temporal_padding', 'tile', 'to_dense', 'transpose', 'truncated_normal', 'update', 'update_add', 'update_sub', 'var', 'variable', 'zeros', 'zeros_like']
```

### Submit a pull request?

I don't know how the doc generator works; so no
"
47283,Tensorflow 2.4.1 build failing with MKL enabled,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
Using the following dockerfile to compile TF:
[Dockerfile.txt](https://github.com/tensorflow/tensorflow/files/6014553/Dockerfile.txt)

**Describe the problem**
The build fails when run with `--config=mkl` and succeeds when this option is omitted.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
See attached Dockerfile  (NVIDIA proprietary files are mentioned, but not included)

**Any other info / logs**

The error traceback looks like this:
```
#INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (409 packages loaded, 31937 targets configured).
#INFO: Found 1 target...
#[0 / 293] [Prepa] BazelWorkspaceStatusAction stable-status.txt ... (7 actions, 0 running)
#[80 / 2,141] Executing genrule @local_config_cuda//cuda:cuda-include [for host]; 30s local ... (12 actions, 10 running)
#[122 / 2,157] Executing genrule @local_config_cuda//cuda:cuda-include [for host]; 57s local ... (12 actions, 11 running)
#[183 / 2,208] Executing genrule @local_config_cuda//cuda:cuda-include [for host]; 88s local ... (11 actions, 10 running)
#[1,955 / 2,623] Compiling external/com_google_protobuf/src/google/protobuf/text_format.cc [for host]; 2s local ... (12 actions, 11 running)
#[2,540 / 3,542] Compiling external/llvm-project/llvm/lib/Support/CommandLine.cpp [for host]; 2s local ... (12 actions, 11 running)
#[2,819 / 3,680] Compiling external/llvm-project/llvm/utils/TableGen/GlobalISelEmitter.cpp [for host]; 5s local ... (12 actions, 11 running)
#[3,635 / 7,799] Compiling external/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFVerifier.cpp [for host]; 9s local ... (12 actions running)
#[3,788 / 7,799] Compiling external/llvm-project/llvm/lib/Transforms/Instrumentation/PGOInstrumentation.cpp [for host]; 7s local ... (12 actions, 11 running)
#[3,966 / 7,799] Compiling external/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp [for host]; 20s local ... (12 actions, 11 running)
#[4,129 / 7,799] Compiling external/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp [for host]; 6s local ... (12 actions, 11 running)
#[4,363 / 8,003] Compiling external/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp [for host]; 10s local ... (12 actions running)
#[5,419 / 9,077] Compiling external/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp [for host]; 25s local ... (12 actions running)
#[6,657 / 9,123] Compiling external/llvm-project/mlir/lib/Dialect/SPIRV/SPIRVOps.cpp [for host]; 27s local ... (12 actions running)
#[7,199 / 9,160] Compiling external/mkl_dnn_v1/src/cpu/cpu_reorder.cpp [for host]; 55s local ... (12 actions running)
#[7,481 / 9,226] Compiling external/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp [for host]; 25s local ... (12 actions running)
#[7,991 / 9,748] Compiling external/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp [for host]; 5s local ... (12 actions running)
#[9,334 / 11,861] Compiling external/mkl_dnn_v1/src/cpu/cpu_reorder.cpp [for host]; 71s local ... (12 actions running)
#[11,414 / 16,245] Compiling external/llvm-project/llvm/lib/Passes/PassBuilder.cpp [for host]; 58s local ... (12 actions, 11 running)
#ERROR: /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/external/llvm_openmp/BUILD.bazel:25:1: Executing genrule @llvm_openmp//:kmp_i18n_default failed (Exit 2): bash failed: error executing command
#  (cd /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/execroot/org_tensorflow && \
#  exec env - \
#    LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-11.1/lib64 \
#    PATH=/opt/rh/devtoolset-7/root/usr/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda-11.1/bin \
#  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; perl external/llvm_openmp/runtime/tools/message-converter.pl --os=lin --prefix=kmp_i18n --default=bazel-out/host/bin/external/llvm_openmp/include/kmp_i18n_default.inc external/llvm_openmp/runtime/src/i18n/en_US.txt')
#Execution platform: @local_execution_config_platform//:platform
#Can't locate Data/Dumper.pm in @INC (@INC contains: /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/external/llvm_openmp/runtime/tools/lib /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/external/llvm_openmp/runtime/tools/lib/tools.pm line 80.
#BEGIN failed--compilation aborted at /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/external/llvm_openmp/runtime/tools/lib/tools.pm line 80.
#Compilation failed in require at external/llvm_openmp/runtime/tools/message-converter.pl line 22.
#BEGIN failed--compilation aborted at external/llvm_openmp/runtime/tools/message-converter.pl line 22.
#Target //tensorflow/tools/pip_package:build_pip_package failed to build
#ERROR: /native/tensorflow/python/tools/BUILD:143:1 Executing genrule @llvm_openmp//:kmp_i18n_default failed (Exit 2): bash failed: error executing command
#  (cd /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/execroot/org_tensorflow && \
#  exec env - \
#    LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-11.1/lib64 \
#    PATH=/opt/rh/devtoolset-7/root/usr/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda-11.1/bin \
#  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; perl external/llvm_openmp/runtime/tools/message-converter.pl --os=lin --prefix=kmp_i18n --default=bazel-out/host/bin/external/llvm_openmp/include/kmp_i18n_default.inc external/llvm_openmp/runtime/src/i18n/en_US.txt')
#Execution platform: @local_execution_config_platform//:platform
#INFO: Elapsed time: 2046.582s, Critical Path: 116.95s
#INFO: 7156 processes: 7156 local.
#FAILED: Build did NOT complete successfully
#FAILED: Build did NOT complete successfully
```


"
47282,whether tensorflow2.0 suppert bert pretraining?,"Question: Where tensorflow2.0 suppert bert pretraining?
    Version:  tensorflow2.4
   Base on my data, I want to train new pretrain-model with bert. However, When I run run_pretraining.py file, ad error occurred.


Traceback (most recent call last):
  File ""run_pretraining.py"", line 493, in <module>
    tf.compat.v1.app.run()
  File ""/usr/local/python3/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/python3/lib/python3.8/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/usr/local/python3/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""run_pretraining.py"", line 429, in main
    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2
AttributeError: module 'tensorflow' has no attribute 'contrib'_**


So, I don't know how to deal with this problem and whether tensorflow2.0+ support bert pretraining again with my data.
Finally, hope someone can give me answer and I will appreciate you, thanks!


"
47281,"""classes"" not working on flow_from_dataframe","The classes parameter works on flow_from_directory but not flow_from_dataframe in TF 2.4.

```py
train_generator = train_datagen.flow_from_dataframe(dataframe=df,
                                                    x_col=""img_path"",
                                                    y_col=""class"",
                                                    classes=[""rika"", ""risa"",""yui"", ""akane"",""neru""],
                                                    target_size=(200, 200),
                                                    batch_size=32,
                                                    class_mode='categorical', shuffle=False)
```

![](https://i.imgur.com/TyrbHcW.png)

The `train_generator.class_indices` output are not correspond my  classes lists.

Please correct me if I am wrong. Thanks!"
47278,Why is there no model parallelism api,"
Why is there no model parallelism api

All the strategy are data parallelism. If I want to use model parallelism, do I have to use tensorflow1. X

That is what we need now."
47277,tf.raw_ops.Send/Recv fail within MultiWorkerMirroredStrategy,"I want to use `tf.raw_ops.Send` and `tf.raw_ops.Recv` to transfer tensor between different machines, but the `Recv` fails after `Send` operation is finished.

Here is the error log:
```shell
2021-02-20 01:42:11.421182: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-02-20 01:42:11.424571: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2194875000 Hz
WARNING:tensorflow:/job:worker/replica:0/task:0 seems down, retrying 1/3
WARNING:tensorflow:/job:worker/replica:0/task:0 seems down, retrying 2/3
ERROR:tensorflow:Cluster check alive failed, /job:worker/replica:0/task:0 is down, aborting collectives: Deadline Exceeded
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1613785391.258566931"",""description"":""Deadline Exceeded"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/deadline/deadline_filter.cc"",""file_line"":69,""grpc_status"":4}
```

And here is my code to reproduce this issue:
```python
def test_multi_worker_mirrored_strategy_send_recv(args):
    # specify cluster resolver
    import os, json

    # set visible gpus
    os.environ['CUDA_VISIBLE_DEVICES']= """"

    # set log leverl
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # 1 show all, 2 for warning and error, 3 for only error

    port = ""12345""
    ips = [host1, host2]
    os.environ['TF_CONFIG'] = json.dumps({
        'cluster': { ""worker"": [ip + "":"" + port for ip in ips] },
        ""task"" : {'type': 'worker', ""index"": args.task_id}
    })
    resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()

    # create stratey
    options = tf.distribute.experimental.CommunicationOptions(
        implementation=tf.distribute.experimental.CommunicationImplementation.AUTO)
    strategy = tf.distribute.MultiWorkerMirroredStrategy(resolver, options)

    print(""num_accelerators = "", strategy.cluster_resolver.num_accelerators())
    print(""task_type = %s, task_id = %s"" %(strategy.cluster_resolver.task_type, strategy.cluster_resolver.task_id))

    @tf.function
    def _test_step(task_id):
        send_device = ""/job:worker/replica:0/task:0/device:CPU:0""
        recv_device = ""/job:worker/replica:0/task:1/device:CPU:0""
        if task_id == 0: # send
            tensor = tf.constant([1.0, 2.0], dtype=tf.float32)
            tf.raw_ops.Send(tensor=tensor, 
                            tensor_name=""test_send_tensor"", 
                            send_device=send_device, 
                            send_device_incarnation=123, 
                            recv_device=recv_device,
                            client_terminated=True)
            print(""Send tensor from: "", tensor.device)
        else: # recv
            tensor = tf.raw_ops.Recv(tensor_type=tf.float32, 
                                     tensor_name=""test_send_tensor"", 
                                     send_device=send_device, 
                                     send_device_incarnation=123, 
                                     recv_device=recv_device,
                                     client_terminated=False)
            print(""Recv tensor from: "", tensor.device)
        return tensor

    tensor = strategy.run(_test_step, args=(args.task_id,))
    print(tensor)

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description='test')
    parser.add_argument('--task_id', type=int, 
                        help='specify the task id in Multi Worker Mirrored Strategy cluster.',
                        required=True)
    args = parser.parse_args()

    test_multi_worker_mirrored_strategy_send_recv(args)
```"
47275,HDF5 locking issues when using concurrent.futures.ProcessPoolExecutor,"I was working in TensorFlow and concurrent futures on **Windows 10** using anaconda.  Below is the MWE:

    import tensorflow as tf
    from tensorflow import keras
    
    import numpy as np
    import concurrent.futures 
    import time

    def simple_model():
        model = keras.models.Sequential([
            keras.layers.Dense(units = 10, input_shape = [1]),
            keras.layers.Dense(units = 1, activation = 'sigmoid')
        ])
        model.compile(optimizer = 'sgd', loss = 'mean_squared_error')
        return model
    
    def clone_model(model):
        model_clone = tf.keras.models.clone_model(model)
        model_clone.set_weights(model.get_weights())
        return model_clone
    
    def work(model_path, seq):
        # model = clone_model(model)# model_list[model_id]
        # print(model)
        # import tensorflow as tf
        model = tf.keras.models.load_model(model_path)
        return model.predict(seq)
    
    def workers(model, num_of_seq = 4):
        seqences = np.arange(0,num_of_seq*10).reshape(num_of_seq, -1)
        model_savepath = './simple_model.h5'
        model.save(model_savepath)
        path_list = [model_savepath for _ in range(num_of_seq)]
    
        with concurrent.futures.ProcessPoolExecutor(max_workers=None) as executor:        
            t0 = time.perf_counter()
            # model_list = [clone_model(model) for _ in range(num_of_seq)]
            index_list = np.arange(1, num_of_seq)
            # [clone_model(model) for _ in range(num_of_seq)]
            # print(model_list)
            future_to_samples = {executor.submit(work, path, seq): seq for path, seq in zip(path_list,seqences)}
        Seq_out = []
        for future in concurrent.futures.as_completed(future_to_samples):
            out = future.result()
            Seq_out.append(out)
        t1 = time.perf_counter()
        print(t1-t0)
        return np.reshape(Seq_out, (-1, )), t1-t0
    
    
    
    if __name__ == '__main__':
        model = simple_model()
        num_of_seq = 400
        # model_list = [clone_model(model) for _ in range(4)]
        out = workers(model, num_of_seq=num_of_seq)
        print(out)

returns:

    2021-02-19 16:31:13.665341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]
    15.456169300000003
    (array([1., 1., 1., ..., 1., 1., 1.], dtype=float32), 15.456169300000003)

When I try to run the script in Ubuntu it keeps running with no output and until I force quit the process. And some time it gives the following error:

    OSError: Unable to open file (unable to lock file, errno = 37, error message = 'No locks available')

What I have tried:

 1. I used `conda tf-gpu export > environment.yml` to get all the installed files to a different Windows 10 and saw the same behavior.
2. Made a new environment in Windows 10 (where the code is working) with various different TensorFlow versions. All TF-2.x versions worked out.
3. Tried the same code on Docker by pulling TensorFlow images. Same issues
4. The issue is with locking the HDF5 file, so tried: https://stackoverflow.com/questions/57310333/can-we-disable-h5py-file-locking-for-python-file-like-object. It did not work

Similar issues have been reported in:

 - https://stackoverflow.com/questions/49438814/opening-already-opened-hdf5-file-in-write-mode-using-h5py
 -  https://github.com/h5py/h5py/issues/1066
 -  https://github.com/h5py/h5py/issues/1101
 -  https://github.com/keras-team/keras/issues/11796
 - https://github.com/keras-team/keras/issues/11101
"
47274,tf.convert_to_tensor crashes(segfault) ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):2.1.0
- Python version:3.7.6
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A

**Describe the current behavior**
`tf.convert_to_tensor` crashes(segfault) when integer `dtype` passed

**Describe the expected behavior**
Expect graceful exception messages instead of crash


**Standalone code to reproduce the issue**
~~~python
import tensorflow as tf
import numpy as np
tf.convert_to_tensor(value=np.array((10)), dtype=20)
~~~

Output:
~~~python
Segmentation fault (core dumped)
~~~


"
47271,Export tensorflow-lite target in CMake build,"**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
Currently it is only possible to integrate the tensorflow-lite CMake build as a source build into an external project.
See [https://www.tensorflow.org/lite/guide/build_cmake](https://www.tensorflow.org/lite/guide/build_cmake): ""Create a CMake project which uses TensorFlow Lite"".
Additionally the compiled libtensorflow-lite.a does not include all dependencies (ruy, absl):
See _""Note: This generates a static library libtensorflow-lite.a in the current directory but the library isn't self-contained since all the transitive dependencies are not included.""_. This makes it hard to manually import the prebuilt libary into new projects.

The suggestion would be to export the tensorflow-lite target using CMakes importing and exporting feature ([https://cmake.org/cmake/help/git-stage/guide/importing-exporting/index.html](https://cmake.org/cmake/help/git-stage/guide/importing-exporting/index.html)). All dependencies, like ruy and absl, should be either bundled into the tensorflow-lite library or also be exported.

**Will this change the current api? How?**
This will add the possibility to import the tensorflow-lite CMake build into a separate external project using find_package() and without an additional source build. No modification, only an addition.

**Who will benefit with this feature?**
- Users of the Tensorflow Lite C/C++ API
- Teams can build tensorflow-lite once and distribute the library in a package to all members
- Allows to deliver a prebuilt tensorflow-lite and be easily integrated into projects
"
47270,LinearOperatorKronecker for SparseTensors,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tensorflow 2.2(gpu) & 2.3, linux64, anaconda
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Currently there is no kronecker product operator that works on sparse tensors. Calling tf.linalg.LinearOperatorKronecker on sparse tensors raises `AttributeError: 'SparseTensor' object has no attribute 'is_non_singular'`

**Will this change the current api? How?**
Possibly?

LinearOperatorKronecker checks attributes: `.is_non_singular`, `.is_self_adjoint`, and `.is_self_adjoint`, and then calls `.extend` during initialization, and looks for `.name`. SparseTensor doesn't have any of these. It's probably easier to build a kronecker product operator specifically for sparse tensors.

**Who will benefit with this feature?**
Kronecker products on sparse tensors can be useful in building coefficient matrices for finite difference simulations. I could see wide application building grey box models for use in physics, industrial engineering, aerospace, weather, and so on. Pretty much anyone looking to solve PDEs on a regular grid."
47269,Minimal example that exercises the tf.function(experimental_implements) functionality?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip package built from source
- TensorFlow library (version, if pip package or github SHA, if built from source): tag v2.4.1

### 2. Code

```
import tensorflow as tf

@tf.function(experimental_implements='custom_op')
def custom_op(x, y):
  return tf.matmul(x, y)

@tf.function
def func_2(x):
  return tf.exp(x)

@tf.function
def model(x, y):
  return func_2(custom_op(x, y))

x = tf.Variable([[1.0, 2.0], [3.0,4.0]])
y = tf.Variable([[1.0, 2.0], [3.0,4.0]])
converter = tf.lite.TFLiteConverter.from_concrete_functions([custom_op.get_concrete_function(x,y)])
tflite_model = converter.convert()

with open('model.tflite', 'wb') as f:
  f.write(tflite_model)

```

### 3. Failure after conversion

According to documentation [here](https://www.tensorflow.org/lite/convert/operation_fusion), decorating the _custom_op_ function with _tf.function(experimental_implements='custom_op')_ should call _PrepareCompositeFunctionsPass::ConvertTFImplements_ function inside _tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc_ so that one can write customer conversion code. 

So far I have been unable to achieve that, this function never gets called (easily checked by adding a exit(-1); at the beginning of the function. The code snippet in the aforementioned documentation does not compile because it relies on unmentioned other code, so it can't be used as an example either.
"
47268,BoostedTreeClassifier does not support EmbeddingColumns : EmbeddingColumns has no attribute 'dtype',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary (pip install)
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Including an EmbeddingColumns feature in the data results in the following error

`AttributeError: 'EmbeddingColumn' object has no attribute 'dtype'`

**Describe the expected behavior**
Documentation of BoostedTreesClassifier states that the `feature_columns` argument ""should be instances of classes derived from **FeatureColumn**"" EmbeddingColumns looks to be derived from **FeatureColumns** and so should result in no errors.

**Standalone code to reproduce the issue**

```python
import numpy as np
import pandas as pd
import tensorflow as tf

dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')

y_train = dftrain.pop('survived')

CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',
                       'alone']
NUMERIC_COLUMNS = ['age', 'fare']

def one_hot_cat_column(feature_name, vocab):
  return tf.feature_column.indicator_column(
      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,
                                                 vocab))
feature_columns = []
for feature_name in CATEGORICAL_COLUMNS:
  vocabulary = dftrain[feature_name].unique()
  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, 
                                                          dtype=tf.float32))

embark = tf.feature_column.categorical_column_with_vocabulary_list('embark_town', dftrain['embark_town'].unique())
feature_columns.append(tf.feature_column.embedding_column(embark, dimension=5))
                       
est = tf.estimator.BoostedTreesClassifier(feature_columns,
                                         n_batches_per_layer=1)

NUM_EXAMPLES = len(y_train)
def make_input_fn(X, y, n_epochs=None, shuffle=True):
    def input_fn():
        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))
        if shuffle:
            dataset = dataset.shuffle(NUM_EXAMPLES)
        dataset = dataset.repeat(n_epochs)
        dataset = dataset.batch(NUM_EXAMPLES)
        return dataset
    return input_fn

train_input_fn = make_input_fn(dftrain, y_train)

est.train(train_input_fn)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-25-67c937169652> in <module>
     44 train_input_fn = make_input_fn(dftrain, y_train)
     45 
---> 46 est.train(train_input_fn)

~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    347 
    348       saving_listeners = _check_listeners_type(saving_listeners)
--> 349       loss = self._train_model(input_fn, hooks, saving_listeners)
    350       logging.info('Loss for final step: %s.', loss)
    351       return self

~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1173       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1174     else:
-> 1175       return self._train_model_default(input_fn, hooks, saving_listeners)
   1176 
   1177   def _train_model_default(self, input_fn, hooks, saving_listeners):

~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
   1201           self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))
   1202       worker_hooks.extend(input_hooks)
-> 1203       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,
   1204                                            self.config)
   1205       global_step_tensor = tf.compat.v1.train.get_global_step(g)

~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1161 
   1162     logging.info('Calling model_fn.')
-> 1163     model_fn_results = self._model_fn(features=features, **kwargs)
   1164     logging.info('Done calling model_fn.')
   1165 

~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py in _model_fn(features, labels, mode, config)
   2079 
   2080     def _model_fn(features, labels, mode, config):
-> 2081       return _bt_model_fn(
   2082           features,
   2083           labels,

~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py in _bt_model_fn(features, labels, mode, head, feature_columns, tree_hparams, n_batches_per_layer, config, closed_form_grad_and_hess_fn, example_id_column_name, weight_column, train_in_memory, name)
   1147   logits_dimension = head.logits_dimension
   1148   sorted_feature_columns = sorted(feature_columns, key=lambda tc: tc.name)
-> 1149   float_columns = _get_float_feature_columns(sorted_feature_columns)
   1150 
   1151   with ops.name_scope(name) as name:

~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py in _get_float_feature_columns(sorted_feature_columns)
    111   float_columns = []
    112   for feature_column in sorted_feature_columns:
--> 113     if _is_numeric_column(feature_column):
    114       float_columns.append(feature_column)
    115   return float_columns

~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py in _is_numeric_column(feature_column)
     77                 (feature_column_lib.DenseColumn, fc_old._DenseColumn)):
     78     # NOTE: GBDT requires that all DenseColumns expose a dtype attribute
---> 79     return feature_column.dtype.is_floating
     80   else:
     81     raise ValueError('Encountered unexpected column {}'.format(feature_column))

AttributeError: 'EmbeddingColumn' object has no attribute 'dtype'
```"
47267,Non-deterministic graph when custom_gradient has watched variables,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7.8.2003
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

`tf.custom_gradient()` keeps track of variables that are not part of the inputs and returns gradients for them. When there are multiple such variables, the created graph (in non-eager mode) is non-deterministic. The reason is that watched variable references are stored in a frozenset and since these references are essentially ids that have different ordering from one python invocation to another, the generated graph is different across different runs.

**Describe the expected behavior**

The generated graph in the aforementioned case should be deterministic across different runs.

**Standalone code to reproduce the issue**

Run the python script multiple times and diff the generated graphdefs. They are going to be different.

```bash
$ python run.py 1  # creates tf_graph.pbtxt.1
$ python run.py 2  # creates tf_graph.pbtxt.2
$ python run.py 3  # creates tf_graph.pbtxt.3
```

```python
# run.py
import sys
import tensorflow as tf
import tensorflow.compat.v1 as v1


g = v1.Graph()
with g.as_default():
    # Create a bunch of variables that are captured in custom_gradient
    captured = [tf.Variable(float(i)) for i in range(1, 20)]

    @tf.custom_gradient
    def FuncMult(x):
        def GradMult(*dys, variables=None):
            return (
                4. * sum(captured) * dys[0],
                [(i + 1) * x * y for i in range(len(variables))]
            )

        return x * sum(captured), GradMult

    x = tf.Variable(6.)
    y = FuncMult(x)
    grad = tf.gradients(y, [x])

graph_def = g.as_graph_def(add_shapes=True)
with open(f""tf_graph.pbtxt.{sys.argv[1]}"", ""w"") as f:
    f.write(str(graph_def))
```

**Fix**

Please see #47266 for a potential fix of this issue.

"
47265,Successive prediction (loop) in keras model generate NaN values,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 on google cloud
- TensorFlow installed from (source or binary): conda installation
- TensorFlow version (use command below):2.4.0/2.4.1
- Python version: Python 3.6.12
- Bazel version (if compiling from source):0.15.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.0
- GPU model and memory: K80 on google cloud

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Running successive prediction generates NAN values from the second iteration
**Describe the expected behavior**
No NaN values
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
from typing import Tuple, Optional, List

import cv2
import tensorflow as tf
import numpy as np

from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
from numpy import ndarray
from tensorflow import Tensor


def darknet53(input_data: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
    """"""
    :darknet53: darknet53 model

    :param input_data: input data tensorflow
    :type input_data: Tensor
    :return:
    darknet53 outputs
    """"""

    input_data = convolutional(input_data, (3, 3, 3, 32))
    input_data = convolutional(input_data, (3, 3, 32, 64),
                                      downsample=True)

    for i in range(1):
        input_data = residual_block(input_data, 64, 32, 64)

    input_data = convolutional(input_data, (3, 3, 64, 128),
                                      downsample=True)

    for i in range(2):
        input_data = residual_block(input_data, 128, 64, 128)

    input_data = convolutional(input_data, (3, 3, 128, 256),
                                      downsample=True)

    for i in range(8):
        input_data = residual_block(input_data, 256, 128, 256)

    route_1 = input_data
    input_data = convolutional(input_data, (3, 3, 256, 512),
                                      downsample=True)

    for i in range(8):
        input_data = residual_block(input_data, 512, 256, 512)

    route_2 = input_data
    input_data = convolutional(input_data, (3, 3, 512, 1024),
                                      downsample=True)

    for i in range(4):
        input_data = residual_block(input_data, 1024, 512, 1024)

    return route_1, route_2, input_data


class BatchNormalization(tf.keras.layers.BatchNormalization):
    """"""
    ""Frozen state"" and ""inference mode"" are two separate concepts.
    `layer.trainable = False` is to freeze the layer, so the layer will use
    stored moving `var` and `mean` in the ""inference mode"", and both `gama`
    and `beta` will not be updated !
    """"""

    def call(self, x, training=False):
        if not training:
            training = tf.constant(False)
        training = tf.logical_and(training, self.trainable)
        return super().call(x, training)


def convolutional(input_layer: Tensor, filters_shape: Tuple,
                  downsample: bool = False,
                  activate: bool = True,
                  bn: bool = True):
    """"""
    :convolutional: custom convolution layer

    :param input_layer: input layer
    :type input_layer: Input

    :param filters_shape: filters shape
    :type filters_shape: tuple

    :param downsample: downsample flag
    :type downsample: bool

    :param activate: leaky relu
    :type activate: bbol

    :param bn: batchnorm flag
    :type bn: bool

    :return:
    convolution tensor
    """"""
    if downsample:
        input_layer = tf.keras.layers.ZeroPadding2D(((1, 0), (1, 0)))(
            input_layer)
        padding = 'valid'
        strides = 2
    else:
        strides = 1
        padding = 'same'

    conv = tf.keras.layers.Conv2D(
        filters=filters_shape[-1],
        kernel_size=filters_shape[0],
        strides=strides, padding=padding,
        use_bias=not bn,
        kernel_regularizer=tf.keras.regularizers.l2(0.0005),
        kernel_initializer=tf.random_normal_initializer(stddev=0.01),
        bias_initializer=tf.constant_initializer(0.))(input_layer)

    if bn: conv = BatchNormalization()(conv)
    if activate == True: conv = tf.nn.leaky_relu(conv, alpha=0.1)

    return conv


def residual_block(input_layer: Tensor, input_channel: Tensor,
                   filter_num1, filter_num2) -> Tensor:
    """"""
    :residual_block: residual block function
    :param input_layer: input tensor
    :type input_layer: Tensor

    :param input_channel: input tensor
    :type input_channel: Tensor

    :param filter_num1: size of input channels
    :type filter_num1: int

    :param filter_num2: size of output channels
    :type filter_num2: int
    :return:
    output of residual block
    """"""
    short_cut = input_layer
    conv = convolutional(input_layer,
                         filters_shape=(1, 1, input_channel, filter_num1))
    conv = convolutional(conv, filters_shape=(3, 3, filter_num1, filter_num2))

    residual_output = short_cut + conv
    return residual_output


def upsample(input_layer: Tensor):
    """"""
    :upsample: upsample function
    :param input_layer: input layer tensor
    :type input_layer: Tensor
    :return:
    resized tensor
    """"""
    return tf.image.resize(input_layer, (
        input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='nearest')


def image_preporcess(image: ndarray, target_size: int,
                     gt_boxes: Optional[ndarray] = None) -> Tensor:
    """"""
    :image_preprocess: scale with padding the image to a target size
    :param image: RGB image
    :type image: ndarray
    :param target_size: image target size
    :type target_size: int
    :param gt_boxes: ndarray
    :return:
    """"""
    ih, iw = target_size
    h, w, _ = image.shape

    scale = min(iw / w, ih / h)
    nw, nh = int(scale * w), int(scale * h)
    image_resized = cv2.resize(image, (nw, nh))

    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)
    dw, dh = (iw - nw) // 2, (ih - nh) // 2
    image_paded[dh:nh + dh, dw:nw + dw, :] = image_resized
    image_paded = image_paded / 255.

    if gt_boxes is None:
        return image_paded

    else:
        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw
        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh
        return image_paded, gt_boxes


def bboxes_iou(boxes1, boxes2) -> float:
    """"""
    :bboxes_iou: intersection over union between two bboxes.

    :param boxes1: bbox
    :type boxes1: ndarray
    :param boxes2:bbox
    :type boxes2: ndarray

    :return:
    intersection over union score
    """"""
    boxes1 = np.array(boxes1)
    boxes2 = np.array(boxes2)

    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (
            boxes1[..., 3] - boxes1[..., 1])
    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (
            boxes2[..., 3] - boxes2[..., 1])

    left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])
    right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])

    inter_section = np.maximum(right_down - left_up, 0.0)
    inter_area = inter_section[..., 0] * inter_section[..., 1]
    union_area = boxes1_area + boxes2_area - inter_area
    ious = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)

    return ious


def nms(bboxes: list, iou_threshold: float,
        sigma: float = 0.3, method: str = 'nms'):
    """"""
    :nms: non-maximum suppression.

    :param bboxes: bbox coordinates.
                   (xmin, ymin, xmax, ymax, score, class)
    :type bboxes: list

    :param iou_threshold: threshold to remove duplicated bbox
    :type iou_threshold: float

    :param sigma: sigma value for softnms
    :param method: nms or soft-nms
    :return:
    ndarray of best bbox

    :Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf
          https://github.com/bharatsingh430/soft-nms
    """"""
    classes_in_img = list(set(bboxes[:, 5]))
    best_bboxes = []

    for cls in classes_in_img:
        cls_mask = (bboxes[:, 5] == cls)
        cls_bboxes = bboxes[cls_mask]

        while len(cls_bboxes) > 0:
            max_ind = np.argmax(cls_bboxes[:, 4])
            best_bbox = cls_bboxes[max_ind]
            best_bboxes.append(best_bbox)
            cls_bboxes = np.concatenate(
                [cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])
            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])
            weight = np.ones((len(iou),), dtype=np.float32)

            assert method in ['nms', 'soft-nms']

            if method == 'nms':
                iou_mask = iou > iou_threshold
                weight[iou_mask] = 0.0

            if method == 'soft-nms':
                weight = np.exp(-(1.0 * iou ** 2 / sigma))

            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight
            score_mask = cls_bboxes[:, 4] > 0.
            cls_bboxes = cls_bboxes[score_mask]

    return best_bboxes


def postprocess_boxes(pred_bbox: ndarray,
                      org_img_shape: int,
                      input_size: int,
                      score_threshold: float) -> ndarray:
    """"""
    :postprocess_boxes: post precessing the bboxes.
    :param pred_bbox: bboxes
    :type pred_bbox: ndarray

    :param org_img_shape: original image shape
    :type org_img_shape: int

    :param input_size: image input size
    :type input_size: int

    :param score_threshold: threshold to discard some invalid bboxes
    :type score_threshold: float
    :return:
    """"""
    valid_scale = [0, np.inf]
    pred_bbox = np.array(pred_bbox)

    pred_xywh = pred_bbox[:, 0:4]
    pred_conf = pred_bbox[:, 4]
    pred_prob = pred_bbox[:, 5:]

    # # (1) (x, y, w, h) --> (xmin, ymin, xmax, ymax)
    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,
                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5],
                               axis=-1)
    # # (2) (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)
    org_h, org_w = org_img_shape
    resize_ratio = min(input_size / org_w, input_size / org_h)

    dw = (input_size - resize_ratio * org_w) / 2
    dh = (input_size - resize_ratio * org_h) / 2

    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio
    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio

    # # (3) clip some boxes those are out of range
    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),
                                np.minimum(pred_coor[:, 2:],
                                           [org_w - 1, org_h - 1])], axis=-1)
    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]),
                                 (pred_coor[:, 1] > pred_coor[:, 3]))
    pred_coor[invalid_mask] = 0

    # # (4) discard some invalid boxes
    bboxes_scale = np.sqrt(
        np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))
    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale),
                                (bboxes_scale < valid_scale[1]))

    # # (5) discard some boxes with low scores
    classes = np.argmax(pred_prob, axis=-1)
    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]
    score_mask = scores > score_threshold
    mask = np.logical_and(scale_mask, score_mask)
    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]

    return np.concatenate(
        [coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)


def get_bbox_coords(arr: ndarray) -> ndarray:
    """"""
    :get_bbox_coords: get bboxes coordinates.

    :param arr: bbox of polygons arr
    :type arr: ndarray
    :return:
    """"""
    x_min, x_max = min(arr[:, 0]), max(arr[:, 0])
    y_min, y_max = min(arr[:, 1]), max(arr[:, 1])

    return np.array([[x_min, y_min], [x_max, y_max]])


def get_object(model: Model, input_image: ndarray,
              input_size: int = 416) -> ndarray:
    """"""
    :crop_card: run yolov3 to crop the card region.
    :param model: keras Model
    :type model_path: Model
    :param input_image: RGB image
    :type input_image: ndarray

    :param input_size:input size
    :return:
    card crop
    """"""
    original_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)
    original_image_size = original_image.shape[:2]
    image_data = image_preporcess(np.copy(original_image),
                                  [input_size, input_size])
    image_data = image_data[np.newaxis, ...].astype(np.float32)

    pred_bbox = model.predict(image_data)
    print(pred_bbox)
    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]
    pred_bbox = tf.concat(pred_bbox, axis=0)
    bboxes = postprocess_boxes(pred_bbox, original_image_size,
                               input_size, 0.3)
    # bboxes format bboxes: [x_min, y_min, x_max, y_max, probability, cls_id]
    bboxes = nms(bboxes, 0.1, method='nms')
    # take only the first crop.
    if len(bboxes) != 0:
        bbox_1 = bboxes[0]
        x_min, y_min, x_max, y_max = int(bbox_1[0]), int(bbox_1[1]), int(
            bbox_1[2]), int(bbox_1[3])
        # crop the original image and get the card.
        h, w = original_image.shape[:2]
        crop = original_image[max(0, y_min - 10):min(h, y_max + 10),
               max(0, x_min - 10):min(w, x_max + 10)]
        # save crop
        # cv2.imwrite(""crop_id_card.png"", crop)
        print('crop done!')
    else:
        crop = None
    return crop


NUM_CLASS = 1
ANCHORS = np.array(
    [1.25, 1.625, 2.0, 3.75, 4.125, 2.875, 1.875, 3.8125, 3.875, 2.8125,
     3.6875, 7.4375, 3.625, 2.8125, 4.875, 6.1875, 11.65625, 10.1875
     ]).reshape(3, 3, 2)
STRIDES = np.array([8, 16, 32])
IOU_LOSS_THRESH = 0.5


def YOLOv3(input_layer: Input) -> List:
    """"""
    Run Yolo3 model.
    :param input_layer: input layer of yolo3
    :type input_layer: Tensor
    :return: list of features maps
    """"""
    route_1, route_2, conv = darknet53(input_layer)

    conv = convolutional(conv, (1, 1, 1024, 512))
    conv = convolutional(conv, (3, 3, 512, 1024))
    conv = convolutional(conv, (1, 1, 1024, 512))
    conv = convolutional(conv, (3, 3, 512, 1024))
    conv = convolutional(conv, (1, 1, 1024, 512))

    conv_lobj_branch = convolutional(conv, (3, 3, 512, 1024))
    conv_lbbox = convolutional(conv_lobj_branch,
                                      (1, 1, 1024, 3 * (NUM_CLASS + 5)),
                                      activate=False, bn=False)

    conv = convolutional(conv, (1, 1, 512, 256))
    conv = upsample(conv)

    conv = tf.concat([conv, route_2], axis=-1)

    conv = convolutional(conv, (1, 1, 768, 256))
    conv = convolutional(conv, (3, 3, 256, 512))
    conv = convolutional(conv, (1, 1, 512, 256))
    conv = convolutional(conv, (3, 3, 256, 512))
    conv = convolutional(conv, (1, 1, 512, 256))

    conv_mobj_branch = convolutional(conv, (3, 3, 256, 512))
    conv_mbbox = convolutional(conv_mobj_branch,
                                      (1, 1, 512, 3 * (NUM_CLASS + 5)),
                                      activate=False, bn=False)

    conv = convolutional(conv, (1, 1, 256, 128))
    conv = upsample(conv)

    conv = tf.concat([conv, route_1], axis=-1)

    conv = convolutional(conv, (1, 1, 384, 128))
    conv = convolutional(conv, (3, 3, 128, 256))
    conv = convolutional(conv, (1, 1, 256, 128))
    conv = convolutional(conv, (3, 3, 128, 256))
    conv = convolutional(conv, (1, 1, 256, 128))

    conv_sobj_branch = convolutional(conv, (3, 3, 128, 256))
    conv_sbbox = convolutional(conv_sobj_branch,
                                      (1, 1, 256, 3 * (NUM_CLASS + 5)),
                                      activate=False, bn=False)

    return [conv_sbbox, conv_mbbox, conv_lbbox]


def decode(conv_output, i: int = 0) -> Tensor:
    """"""
    :decode: Decode the output of Yolo3 to get the bbox.

    :param conv_output: output feature map
    :type conv_output: Tensor

    :param i: anchors index
    :type i: int

    :return: tensor of shape [batch_size, output_size,
     output_size, anchor_per_scale, 5 + num_classes]
            contains (x, y, w, h, score, probability)
    """"""

    conv_shape = tf.shape(conv_output)
    batch_size = conv_shape[0]
    output_size = conv_shape[1]

    conv_output = tf.reshape(conv_output, (
        batch_size, output_size, output_size, 3, 5 + NUM_CLASS))

    conv_raw_dxdy = conv_output[:, :, :, :, 0:2]
    conv_raw_dwdh = conv_output[:, :, :, :, 2:4]
    conv_raw_conf = conv_output[:, :, :, :, 4:5]
    conv_raw_prob = conv_output[:, :, :, :, 5:]

    y = tf.tile(tf.range(output_size, dtype=tf.int32)[:, tf.newaxis],
                [1, output_size])
    x = tf.tile(tf.range(output_size, dtype=tf.int32)[tf.newaxis, :],
                [output_size, 1])

    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)
    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :],
                      [batch_size, 1, 1, 3, 1])
    xy_grid = tf.cast(xy_grid, tf.float32)

    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]
    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]
    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)

    pred_conf = tf.sigmoid(conv_raw_conf)
    pred_prob = tf.sigmoid(conv_raw_prob)

    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)


def bbox_iou(boxes1: ndarray, boxes2: ndarray) -> float:
    """"""
    :bbox_iou: intersection over union between two bboxes.

    :param boxes1: bbox
    :type boxes1: ndarray
    :param boxes2:bbox
    :type boxes2: ndarray

    :return:
    intersection over union score
    """"""
    boxes1_area = boxes1[..., 2] * boxes1[..., 3]
    boxes2_area = boxes2[..., 2] * boxes2[..., 3]

    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,
                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)
    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,
                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)

    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])
    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])

    inter_section = tf.maximum(right_down - left_up, 0.0)
    inter_area = inter_section[..., 0] * inter_section[..., 1]
    union_area = boxes1_area + boxes2_area - inter_area

    return 1.0 * inter_area / union_area


def bbox_giou(boxes1, boxes2):
    """"""
    :bbox_giou: Generalized intersection over union between two bboxes.

    :param boxes1: bbox
    :type boxes1: ndarray
    :param boxes2:bbox
    :type boxes2: ndarray

    :return:
    intersection over union score
    """"""
    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,
                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)
    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,
                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)

    boxes1 = tf.concat([tf.minimum(boxes1[..., :2], boxes1[..., 2:]),
                        tf.maximum(boxes1[..., :2], boxes1[..., 2:])], axis=-1)
    boxes2 = tf.concat([tf.minimum(boxes2[..., :2], boxes2[..., 2:]),
                        tf.maximum(boxes2[..., :2], boxes2[..., 2:])], axis=-1)

    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (
            boxes1[..., 3] - boxes1[..., 1])
    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (
            boxes2[..., 3] - boxes2[..., 1])

    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])
    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])

    inter_section = tf.maximum(right_down - left_up, 0.0)
    inter_area = inter_section[..., 0] * inter_section[..., 1]
    union_area = boxes1_area + boxes2_area - inter_area + 1e-10
    iou = inter_area / union_area

    enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])
    enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])
    enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)
    enclose_area = enclose[..., 0] * enclose[..., 1]
    giou = iou - 1.0 * (enclose_area - union_area) / enclose_area

    return giou


def compute_loss(pred: Tensor, conv: Tensor, label: Tensor,
                 bboxes: Tensor, i: int = 0) -> Tuple[Tensor, Tensor, Tensor]:
    """"""
    :compute_loss: compute loss of yolo3
    :param pred: predict tensor
    :type pred: Tensor
    :param conv: target tensor
    :type conv: Tensor
    :param label: label tensor
    :type label: Tensor
    :param bboxes: bboxes tensor
    :type bboxes: Tensor
    :param i: index
    :type i: int
    :return:
    tuple of losses tensors
    """"""
    conv_shape = tf.shape(conv)
    batch_size = conv_shape[0]
    output_size = conv_shape[1]
    input_size = STRIDES[i] * output_size
    conv = tf.reshape(conv,
                      (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))

    conv_raw_conf = conv[:, :, :, :, 4:5]
    conv_raw_prob = conv[:, :, :, :, 5:]

    pred_xywh = pred[:, :, :, :, 0:4]
    pred_conf = pred[:, :, :, :, 4:5]

    label_xywh = label[:, :, :, :, 0:4]
    respond_bbox = label[:, :, :, :, 4:5]
    label_prob = label[:, :, :, :, 5:]

    giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)
    input_size = tf.cast(input_size, tf.float32)

    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:,
                                                                :, :, :,
                                                                3:4] / (
                              input_size ** 2)
    giou_loss = respond_bbox * bbox_loss_scale * (1 - giou)

    iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :],
                   bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])
    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)

    respond_bgd = (1.0 - respond_bbox) * tf.cast(max_iou < IOU_LOSS_THRESH,
                                                 tf.float32)

    conf_focal = tf.pow(respond_bbox - pred_conf, 2)

    conf_loss = conf_focal * (
            respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(
        labels=respond_bbox, logits=conv_raw_conf)
            +
            respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(
        labels=respond_bbox, logits=conv_raw_conf)
    )

    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(
        labels=label_prob, logits=conv_raw_prob)

    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1, 2, 3, 4]))
    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1, 2, 3, 4]))
    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1, 2, 3, 4]))

    return giou_loss, conf_loss, prob_loss


def create_yolo3_model(input_size: int = 416):
    input_layer = tf.keras.layers.Input([input_size, input_size, 3])
    feature_maps = YOLOv3(input_layer)

    bbox_tensors = []
    for i, fm in enumerate(feature_maps):
        bbox_tensor = decode(fm, i)
        bbox_tensors.append(bbox_tensor)
    model = tf.keras.Model(input_layer, bbox_tensors)
    return model


if __name__ == ""__main__"":
    model = create_yolo3_model()

    images = np.random.uniform(0, 255., (10, 416, 416, 3))
    images = images.astype(np.float32)
    for idx, image in enumerate(images):
        # here i'll print pred_bbox variable in get_object
        # We remark that the first iteration is good but the rest is NaN
        print(f""Iteration : {idx}"")
        get_object(model, input_image=image)
```
"
47264,Keras layers do not track tf.Module (not conforming to SOLID principles),"This is a reduction of https://github.com/tensorflow/probability/issues/946 to an issue with TensorFlow by itself.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.7.9

**Describe the current behavior**
tf.keras.layers.Layer is a subclass of tf.Module, but I cannot replace a use of tf.Module with tf.keras.layers.Layer, violating the Liskov substitution principle - which is a real issue for downstream projects as demonstrated by https://github.com/tensorflow/probability/issues/946.

**Describe the expected behavior**
I can substitute a `tf.keras.layers.Layer` anywhere I need a `tf.Module`. Specifically, `layer.trainable_variables` and `layer.trainable_weights` discover all Variable instances that are in sub-Modules, not just in sub-Layers.

Ideally, `.trainable_variables` would have a consistent return type between Module and Layer.

**Standalone code to reproduce the issue**
```python
module = tf.Module()
module.submodule = tf.Module()
module.submodule.var = tf.Variable(1.0)
assert module.trainable_variables == (module.submodule.var,)  # as expected

layer = tf.keras.layers.Layer()
assert isinstance(layer, tf.Module)  # passes
layer.sublayer = tf.keras.layers.Layer()
layer.sublayer.var = tf.Variable(1.0)
assert layer.trainable_variables == [layer.sublayer.var]  # acceptable

layer = tf.keras.layers.Layer()
layer.submodule = tf.Module()
layer.submodule.var = tf.Variable(1.0)
assert list(layer.trainable_variables) == [layer.submodule.var]  # FAILS
```"
47263,(Numpy v1.20+ compat.): Cannot convert a symbolic Tensor (sequential_1/random_rotation_1/rotation_matrix/strided_slice:0) to a numpy array,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and no. I ahve writeen custom code, bu the code that throws exception _is_ provided by tensorflow
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tf-nightly-gpu v2.5.0-dev20201214
- Python version: 3.8.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.0.130/7.5.1
- GPU model and memory:  nvidia Tesla T4 15109 MiB



**Describe the current behavior**
i have been running the same scripts for months, and i just started having this issue today when i re-run them. Last time they worked was 7 days ago (i even have the git commit uploading the then freshly executed script)

the only thing that changed in between was an updated version of numpy

again, this exact code worked without a hitch a week ago

**Describe the expected behavior**
the code shoould run fine


**Standalone code to reproduce the issue**

import tensorflow as tf
from tensorflow import keras

inference_model = tf.keras.models.load_model('./S2_models/inference_model.h5')

 Adding data augmentation
data_augmentation = tf.keras.Sequential([
tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)
])

inputs = tf.keras.Input(shape=(224, 398, 3))
x = data_augmentation(inputs)
outputs = inference_model(x, training=False)
training_model = tf.keras.Model(inputs, outputs)

**Other info / logs** 

NotImplementedError                       Traceback (most recent call last)
<ipython-input-11-4e12e0617af3> in <module>
      1 inputs = tf.keras.Input(shape=(224, 398, 3))
----> 2 x = data_augmentation(inputs)
      3 outputs = inference_model(x, training=False)
      4 training_model = tf.keras.Model(inputs, outputs)

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    955     # >> model = tf.keras.Model(inputs, outputs)
    956     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):
--> 957       return self._functional_construction_call(inputs, args, kwargs,
    958                                                 input_list)
    959 

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)
   1094           layer=self, inputs=inputs, build_graph=True, training=training_value):
   1095         # Check input assumptions set after layer building, e.g. input shape.
-> 1096         outputs = self._keras_tensor_symbolic_call(
   1097             inputs, input_masks, args, kwargs)
   1098 

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)
    826       return nest.map_structure(keras_tensor.KerasTensor, output_signature)
    827     else:
--> 828       return self._infer_output_signature(inputs, args, kwargs, input_masks)
    829 
    830   def _infer_output_signature(self, inputs, args, kwargs, input_masks):

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)
    867           # TODO(kaftan): do we maybe_build here, or have we already done it?
    868           self._maybe_build(inputs)
--> 869           outputs = call_fn(inputs, *args, **kwargs)
    870 
    871         self._handle_activity_regularization(inputs, outputs)

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in call(self, inputs, training, mask)
    396         kwargs['training'] = training
    397 
--> 398       outputs = layer(inputs, **kwargs)
    399 
    400       if len(nest.flatten(outputs)) != 1:

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1016         with autocast_variable.enable_auto_cast_variables(
   1017             self._compute_dtype_object):
-> 1018           outputs = call_fn(inputs, *args, **kwargs)
   1019 
   1020         if self._activity_regularizer:

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in call(self, inputs, training)
    866           interpolation=self.interpolation)
    867 
--> 868     output = control_flow_util.smart_cond(training, random_rotated_inputs,
    869                                           lambda: inputs)
    870     output.set_shape(inputs.shape)

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/utils/control_flow_util.py in smart_cond(pred, true_fn, false_fn, name)
    112     return control_flow_ops.cond(
    113         pred, true_fn=true_fn, false_fn=false_fn, name=name)
--> 114   return smart_module.smart_cond(
    115       pred, true_fn=true_fn, false_fn=false_fn, name=name)
    116 

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     52   if pred_value is not None:
     53     if pred_value:
---> 54       return true_fn()
     55     else:
     56       return false_fn()

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in random_rotated_inputs()
    861       return transform(
    862           inputs,
--> 863           get_rotation_matrix(angles, img_hd, img_wd),
    864           fill_mode=self.fill_mode,
    865           fill_value=self.fill_value,

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in get_rotation_matrix(angles, image_height, image_width, name)
    756             math_ops.cos(angles)[:, None],
    757             y_offset[:, None],
--> 758             array_ops.zeros((num_angles, 2), dtypes.float32),
    759         ],
    760         axis=1)

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    204     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    205     try:
--> 206       return target(*args, **kwargs)
    207     except (TypeError, ValueError):
    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in wrapped(*args, **kwargs)
   2905 
   2906   def wrapped(*args, **kwargs):
-> 2907     tensor = fun(*args, **kwargs)
   2908     tensor._is_zeros_tensor = True
   2909     return tensor

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in zeros(shape, dtype, name)
   2954           # Create a constant if it won't be very big. Otherwise create a fill
   2955           # op to prevent serialized GraphDefs from becoming too large.
-> 2956           output = _constant_if_small(zero, shape, dtype, name)
   2957           if output is not None:
   2958             return output

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in _constant_if_small(value, shape, dtype, name)
   2890 def _constant_if_small(value, shape, dtype, name):
   2891   try:
-> 2892     if np.prod(shape) < 1000:
   2893       return constant(value, shape=shape, dtype=dtype, name=name)
   2894   except TypeError:

<__array_function__ internals> in prod(*args, **kwargs)

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims, initial, where)
   3028     10
   3029     """"""
-> 3030     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
   3031                           keepdims=keepdims, initial=initial, where=where)
   3032 

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     85                 return reduction(axis=axis, out=out, **passkwargs)
     86 
---> 87     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
     88 
     89 

~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in __array__(self)
    851 
    852   def __array__(self):
--> 853     raise NotImplementedError(
    854         ""Cannot convert a symbolic Tensor ({}) to a numpy array.""
    855         "" This error may indicate that you're trying to pass a Tensor to""

NotImplementedError: Cannot convert a symbolic Tensor (sequential_1/random_rotation_1/rotation_matrix/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
"
47262,EfficientnetB1 :ValueError: You are trying to load a weight file containing 185 layers into a model with 184 layers ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
yes- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Colab - OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Source- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
3.6- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
after using your code efficientnet_weight_update_util.py to  converted pretrained weights model.ckpt to efficientnetb1_notop.h5

please help, im getting the mentioned error
model = EfficientNetB1(weights=""efficientnetb1_notop.h5"", include_top=False)

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47261,[Code provided] Weird tf.function behaviour with boolean inputs after trace,"When passed a tensor (dtype = tf.bool) from an tf.function to another tf.function, its behaviour become weird after first time call (trace).

https://colab.research.google.com/drive/1id1t9L8xypGbavcA2hqtNEDQ4VFwJ40I?usp=sharing"
47260,EfficientnetB1 :ValueError: You are trying to load a weight file containing 185 layers into a model with 184 layers ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
yes- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Colab - OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Source- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
3.6- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
after using your code efficientnet_weight_update_util.py to  converted pretrained weights model.ckpt to efficientnetb1_notop.h5

please help, im getting the mentioned error
model = EfficientNetB1(weights=""efficientnetb1_notop.h5"", include_top=False)

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47259,tflite building is taking hours,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): yocto
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version: 2.4 
- Python version: 3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.7
- GCC/Compiler version (if compiling from source): 10.2
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I am trying to compile tensorflow-lite and benchmark_model from source using yocto. I already managed to do it using a yocto recipe based on the Makefile. But the Makefile support is limited, so I need to compile tensorflow-lite and benchmark_model using bazel. But bazel build takes several hours!  

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I started from http://git.yoctoproject.org/cgit/cgit.cgi/meta-tensorflow/ and slightly modified http://git.yoctoproject.org/cgit/cgit.cgi/meta-tensorflow/tree/recipes-framework/tensorflow/tensorflow_2.4.0.bb to compile tensorflow-lite instead of tensorflow. Basically I just modified the do_compile so now it is:
```
do_compile () {
    export CT_NAME=$(echo ${HOST_PREFIX} | rev | cut -c 2- | rev)
    unset CC
    ${BAZEL} build \
        --jobs=40 \
        ${TF_ARGS_EXTRA} \
        -c opt \
        --cpu=${BAZEL_TARGET_CPU} \
        --crosstool_top=@local_config_yocto_compiler//:toolchain \
        --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
        --copt -DTF_LITE_DISABLE_X86_NEON --copt -DCL_DELEGATE_NO_GL \
        //tensorflow/lite:libtensorflowlite.so \
        //tensorflow/lite/tools/benchmark:benchmark_model \
        //tensorflow/tools/pip_package:build_pip_package \
        ${TF_TARGET_EXTRA}
}
```
I also updated the do_install function with the correct path (tensorflow --> tensorflow-lite)

And the recipe works. I was able to install `benchmark_model` on the target and run it. 
The issue here is that, the build takes literally 3 hours to complete while with the Makefile it tooks only few minutes. 
I am building on a very powerfull machine with 40 cores and 189G of RAM. When using `htop` to monitor cpu load, I see only a few processes although I added `--jobs=40` on the bazel build command. 

Am I missing something here? 
Thank you for your help

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47257,MultiWorkerMirroredStrategy rendezvous Did not find key ,"Hi, I want to use rendezvous to transfer tensor between two machines within the scope of MultiWorkerMirroredStrategy, but it complains an Internal error `Did not find key my_tensor_name`.

What is the device name different machines used to identify each other in the cluster? Can two machines identify each other with such device name? 
```c++
const char* dst_device_name = ""/job:worker/replica:0/task:1/device:CPU:0"";
const char* src_device_name= ""/job:worker/replica:0/task:0/device:CPU:0"";
```

BTW, `typeid(*OpKernelContext->rendezvous()).name()` show `SimpleRendezvous` in one machine and `IntraProcessRendezvous` for another machine. Why not `*Remote*Rendezvous` is created? 

Here is my code snippet:
1. my custom op
```c++
#include ""tensorflow/core/framework/op_kernel.h""
#include <chrono>
#include <thread>

namespace tensorflow {

using GPUDevice = Eigen::GpuDevice;
using CPUDevice = Eigen::ThreadPoolDevice; 

class TestRendezvousMultiWorkerOp : public AsyncOpKernel {
public:
    explicit TestRendezvousMultiWorkerOp(OpKernelConstruction* ctx) : AsyncOpKernel(ctx) {
        OP_REQUIRES_OK(ctx, ctx->GetAttr(""task_id"", &task_id_));
        OP_REQUIRES(ctx, task_id_ >= 0, 
            errors::Internal(""task_id should be >= 0, but got "", task_id_));
    }
    void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {
        OP_REQUIRES_ASYNC(ctx, ctx->rendezvous() != nullptr,
            errors::Internal(""Op kernel context needs to provide a rendezvous.""), done);

        const char* dst_device_name = ""/job:worker/replica:0/task:1/device:CPU:0"";
        const char* src_device_name = ""/job:worker/replica:0/task:0/device:CPU:0"";

        std::string send_key = Rendezvous::CreateKey(
                            src_device_name /*src_device*/,
                            123/*src_incarnation*/,
                            dst_device_name /*dst_device*/,
                            ""test_rendezvous_string""/*name*/,
                            FrameAndIter(0, 0)/*frame_and_iter*/);

        Rendezvous::ParsedKey parsed_key;
        OP_REQUIRES_OK_ASYNC(ctx, Rendezvous::ParseKey(send_key, &parsed_key), done);
        
        Rendezvous::Args args;

        const Tensor* input_tensor = nullptr;
        OP_REQUIRES_OK_ASYNC(ctx, ctx->input(""value"", &input_tensor), done);
        Tensor* out_tensor = nullptr;
        OP_REQUIRES_OK_ASYNC(ctx, ctx->allocate_output(0, input_tensor->shape(), &out_tensor), done);

        if (task_id_ == 0) {
            OP_REQUIRES_OK_ASYNC(ctx, ctx->rendezvous()->Send(parsed_key, args, *input_tensor, ctx->is_input_dead()), done);
            std::memcpy(out_tensor->data(), input_tensor->data(), input_tensor->TotalBytes());
            std::this_thread::sleep_for(std::chrono::seconds(5));
        } else {
            bool is_dead = false;
            OP_REQUIRES_OK_ASYNC(ctx, ctx->rendezvous()->Recv(parsed_key, args, out_tensor, &is_dead), done);
        }

        done();
        
    }
private:
    int task_id_;
};

REGISTER_KERNEL_BUILDER(Name(""TestRendezvousMultiWorker"").Device(DEVICE_CPU), 
                        TestRendezvousMultiWorkerOp);

} // namespace tensorflow
```

2. my python script
```python
import tensorflow as tf
import os, json

os.environ['CUDA_VISIBLE_DEVICES'] = """"

os.environ[""TF_CONFIG""] = json.dumps({
    ""cluster"": {""worker"": [""host1:port"", ""host2:port""]},
    ""task"": {""type"": ""worker"", ""index"": args.task_id}
})

resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()
strategy = tf.distribute.MultiWorkerMirroredStrategy(resolver)

import my_custom_lib

@tf.function
def _test_step(task_id):
    value = tf.constant([1.0, 2.0])
    out = my_custom_lib.test_rendezvous_multi_worker(value, task_id=task_id)
    return out

out = strategy.run(_test_step, args=(args.task_id,))
print(out)
```

3. the python3 command
```python
python3 file.py --task_id=0 # on machine 1
python3 file.py --task_id=1 # on machine 2
```"
47256,[Grappler] Optimise branches with constant predicates. ,"I am using tf 1.15. When I try to optimize a graph using grappler and get an optimized graph_def, I see that branches on constant predicates are not folded. Is this something that is not supported?
"
47254,"Why TF do not support strides = [1,0] or [0,1] for conv2d?","It will mean the kernel moves on only one direction. 
Are there alternative way to do it?
Thank you very much."
47253,tensorflow is shown on listing packages using pip list or conda list but you can't import in Python,"Same issue happened even with using pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.3.0-cp38-cp38-macosx_10_14_x86_64.whl

as mentioned in https://www.tensorflow.org/install/pip#package-location

tensorflow is shown on listing packages using pip list or conda list but you can't import in Python

When I try to import TensorFlow the kernel restarting and then I have the message 

[SpyderKernelApp] WARNING | No such comm: 7843964a725e11eb900a1e29c0108246
[SpyderKernelApp] WARNING | No such comm: 81501ed4725e11eb900a1e29c0108246"
47252,Same issue happened even with using  pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.3.0-cp38-cp38-macosx_10_14_x86_64.whl  as mentioned in https://www.tensorflow.org/install/pip#package-location  tensorflow is shown on listing packages using pip list or conda list but you can't import in Python,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47249,CUDA_ERROR_ILLEGAL_ADDRESS after upgrading form TF 2.3.1 to 2.3.2 on Windows 10,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: no
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: -
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: v2.3.1-38-g9edbe5075f7 2.3.2
-   **Python version**: 3.7.5
-   **Bazel version (if compiling from source)**: -
-   **GCC/Compiler version (if compiling from source)**: -
-   **CUDA/cuDNN version**: cuda_10.1.105_418.96_win10.exe / cudnn-10.1-windows10-x64-v7.6.5.32.zip
-   **GPU model and memory**: Geforce RTX 2060, 6 GB RAM
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm running a CNN in TF.
On TF 2.3.1 it runs fine.
(On TF 2.4 it runs out of memory - another filed bug.)
Now I tried it on TF 2.3.2 (upgraded both tensorflow and tensorflow-gpu). At Epoch 740/800 it gives the bellow CUDA_ERROR_ILLEGAL_ADDRESS error.
Nothing changed, compared to the previous runs, only the TF version.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Epoch 739/800
61/61 [==============================] - 12s 200ms/step - loss: 0.0245 - accuracy: 0.9929 - val_loss: 0.2726 - val_accuracy: 0.9485
Epoch 740/800
50/61 [=======================>......] - ETA: 2s - loss: 0.0192 - accuracy: 0.99282021-02-18 20:55:36.002311: E tensorflow/stream_executor/cuda/cuda_driver.cc:951] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered :: 0x00007FFA1C753DA5
        tensorflow::CurrentStackTrace
0x00007FFA1C4A286E      tensorflow::CostGraphDef_Node::set_is_final
0x00007FFA1C64AA1E      stream_executor::StreamExecutor::SetDeviceSharedMemoryConfig
0x00007FFA1A1DB796      tensorflow::StepStats::internal_default_instance
0x00007FFA1A1ECFC4      google::protobuf::RepeatedPtrField<tensorflow::InterconnectLink>::Add
0x00007FFA020A0177      std::vector<tensorflow::DtypeAndPartialTensorShape,std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=
0x00007FFA0207B0BB      absl::lts_2020_02_25::Span<tensorflow::Tensor const >::end
0x00007FFA01FF3ACF      TFE_TensorHandleResolve
0x00007FFA01F90E83      TFE_Py_TensorShapeSlice
0x00007FFA01F8E6FA      std::_Tree<std::_Tmap_traits<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char> >,0>,tensorflow::monitoring::CounterCell,std::less<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char>
0x00007FFA529A2AB7      PyMethodDef_RawFastCallKeywords
0x00007FFA529A318C      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3963      PyEval_EvalFrameDefault
0x00007FFA529A32E3      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3963      PyEval_EvalFrameDefault
0x00007FFA529A32E3      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3963      PyEval_EvalFrameDefault
0x00007FFA529A0886      PyEval_EvalCodeWithName
0x00007FFA5299FBEA      PyFunction_FastCallDict
0x00007FFA529B4DCD      PyObject_IsAbstract
0x00007FFA529A46D5      PyEval_EvalFrameDefault
0x00007FFA529A0886      PyEval_EvalCodeWithName
0x00007FFA529A33FC      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3A0F      PyEval_EvalFrameDefault
0x00007FFA529A0886      PyEval_EvalCodeWithName
0x00007FFA529A33FC      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3E12      PyEval_EvalFrameDefault
0x00007FFA529A32E3      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3E12      PyEval_EvalFrameDefault
0x00007FFA529A0886      PyEval_EvalCodeWithName
0x00007FFA529A33FC      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3963      PyEval_EvalFrameDefault
0x00007FFA529A0886      PyEval_EvalCodeWithName
0x00007FFA529A33FC      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3A0F      PyEval_EvalFrameDefault
0x00007FFA529A32E3      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3963      PyEval_EvalFrameDefault
0x00007FFA529A32E3      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3963      PyEval_EvalFrameDefault
0x00007FFA529A0886      PyEval_EvalCodeWithName
0x00007FFA529A33FC      PyMethodDef_RawFastCallKeywords
0x00007FFA529A455A      PyEval_EvalFrameDefault
0x00007FFA529A0886      PyEval_EvalCodeWithName
0x00007FFA529A33FC      PyMethodDef_RawFastCallKeywords
0x00007FFA529A3963      PyEval_EvalFrameDefault
0x00007FFA529A0886      PyEval_EvalCodeWithName
0x00007FFA5299FBEA      PyFunction_FastCallDict
0x00007FFA529B4DCD      PyObject_IsAbstract
0x00007FFA529A46D5      PyEval_EvalFrameDefault
0x00007FFA529A0886      PyEval_EvalCodeWithName
0x00007FFA529A33FC      PyMethodDef_RawFastCallKeywords
0x00007FFA529A455A      PyEval_EvalFrameDefault
0x00007FFA529A0886      PyEval_EvalCodeWithName
0x00007FFA529C0CFF      PyEval_EvalCodeEx
0x00007FFA529C0C5D      PyEval_EvalCode
0x00007FFA529C0C07      PyArena_Free
0x00007FFA52B24A95      PyRun_FileExFlags
0x00007FFA52B252BC      PyRun_SimpleFileExFlags
0x00007FFA52B24963      PyRun_AnyFileExFlags
0x00007FFA52A71113      Py_UnixMain
0x00007FFA52A711BB      Py_UnixMain
0x00007FFA529F8C00      PyErr_NoMemory
0x00007FFA529A5AC3      Py_Main
0x00007FFA529A5A9E      Py_Main

Traceback (most recent call last):
  File ""train.py"", line 103, in <module>
    epochs=EPOCHS, verbose=1)
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1103, in fit
    callbacks.on_train_batch_end(end_step, logs)
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 440, in on_train_batch_end
    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 289, in _call_batch_hook
    self._call_batch_end_hook(mode, batch, logs)
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 309, in _call_batch_end_hook
    self._call_batch_hook_helper(hook_name, batch, logs)
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 342, in _call_batch_hook_helper
    hook(batch, logs)
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 961, in on_train_batch_end
    self._batch_update_progbar(batch, logs)
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 1016, in _batch_update_progbar
    logs = tf_utils.to_numpy_or_python_type(logs)
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\keras\utils\tf_utils.py"", line 537, in to_numpy_or_python_type
    return nest.map_structure(_to_single_numpy_or_python_type, tensors)
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\util\nest.py"", line 635, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\util\nest.py"", line 635, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\keras\utils\tf_utils.py"", line 533, in _to_single_numpy_or_python_type
    x = t.numpy()
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\framework\ops.py"", line 1063, in numpy
    maybe_arr = self._numpy()  # pylint: disable=protected-access
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\framework\ops.py"", line 1031, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: GPU sync failed
2021-02-18 20:55:36.255246: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
         [[{{node PyFunc}}]]

"
47248,"ValueError: Expected `model` argument to be a functional `Model` instance, but got a subclass model instead.","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Maybe, unless there's some way to achieve the same thing already

**Describe the feature and the current behavior/state.**

I need to copy a keras model and there is no way that I know of which can be done unless the model **is not** a `tf.keras.models.Model()` subclass. 

**Note:** The use `copy.deepcopy()` will work without giving any errors however it will result in another error whenever the copy is used:

Results in:

    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:569 _run_internal_graph  **
        assert x_id in tensor_dict, 'Could not compute output ' + str(x)
    
    AssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None), name='tf.math.argmax/ArgMax:0', description=""created by layer 'tf.math.argmax'"")

Example:

    import tensorflow as tf
    
    class MyModel(tf.keras.Model):
    
      def __init__(self):
        super(MyModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
        self.dropout = tf.keras.layers.Dropout(0.5)
    
      def call(self, inputs, training=False):
        x = self.dense1(inputs)
        if training:
          x = self.dropout(x, training=training)
        return self.dense2(x)
    
    
    if __name__ == '__main__':
        model1 = MyModel()
        model2 = tf.keras.models.clone_model(model1)

Results in:

    Traceback (most recent call last):
      File ""/Users/emadboctor/Library/Application Support/JetBrains/PyCharm2020.3/scratches/scratch.py"", line 600, in <module>
        model2 = tf.keras.models.clone_model(model1)
      File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/models.py"", line 430, in clone_model
        return _clone_functional_model(
      File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/models.py"", line 171, in _clone_functional_model
        raise ValueError('Expected `model` argument '
    ValueError: Expected `model` argument to be a functional `Model` instance, but got a subclass model instead.

**Will this change the current api? How?** I don't know

**Who will benefit with this feature?** Whoever wants to create n copies of a custom model without writing n lines for achieving the same thing.

**Any Other info.**
"
47246,TensorFlow lite support for snapdragon 888/875 dsp,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Mobile device Snapdragon 875/888
- TensorFlow version 2.4.0:



Problem description:

when running my tesorflow lite app that need to use the hexagon dsp delegate.
I get the following error:

""WARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
INFO: Hexagon Delegate is not supported""

from https://www.tensorflow.org/lite/performance/hexagon_delegate I can't see that you support the snapdragon version I'm using 875/888 , is the Hexagon delegate doesn't support this new snapdragon device or am I'm missing something ?
When will it be supported ?


Thanks
Shuki
"
47245,Illegal instruction (core dumped),"**System information**
- OS Platform and Distribution: Ubuntu 20.04 LTS
- TensorFlow installed from: source
- TensorFlow version: 2.4.1
- Python version: 3.8.5
- Installed using: pip
- Bazel version: 4.0.0
- GCC/Compiler version: 8.4.0
- CUDA/cuDNN version: 11.1/8
- GPU model and memory: GeForce GTX 1080 Ti (11.7G)

**Describe the problem**
At first, I tried installing `tensorflow` from `pip` repositories, but it crashed with `Illegal instruction (core dumped)` upon importing. I've tracked this down to my CPU (Intel G4400) not supporting AVX instructions (https://github.com/tensorflow/tensorflow/issues/17411#issuecomment-608027554), so I built tensorflow from source, using the tools described above. I can successfully import tensorflow now, but when I try running the `deepdream` tutorial: https://www.tensorflow.org/tutorials/generative/deepdream.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Downloaded notebook from https://www.tensorflow.org/tutorials/generative/deepdream
2. `jupyter notebook`
3. Click ""Run All""
4. `Illegal instruction (core dumped)`

Alternatively:
1. Downloaded notebook from https://www.tensorflow.org/tutorials/generative/deepdream
2. `ipython nbconvert --to script deepdream.ipynb`
3. `python3 deepdream.py`
4. `Illegal instruction (core dumped)`

**Any other info / logs**
This is the full log of the ""alternative"" sequence:

```
$ python3 deepdream.py                                                                                                                                                                                                                          
2021-02-18 15:28:33.944636: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-02-18 15:28:35.342188: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-18 15:28:35.342976: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-02-18 15:28:35.372123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.372799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.645GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2021-02-18 15:28:35.372953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.373537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-02-18 15:28:35.373661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.374241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: 
pciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-02-18 15:28:35.374363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.374945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: 
pciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-02-18 15:28:35.375035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-02-18 15:28:35.419511: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-02-18 15:28:35.419904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-02-18 15:28:35.430928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-02-18 15:28:35.431822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-02-18 15:28:35.440650: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2021-02-18 15:28:35.453660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-02-18 15:28:35.454131: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-02-18 15:28:35.454448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.456315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.458227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.460016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.461794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.463552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.465343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.467095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:35.468689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3
2021-02-18 15:28:35.469399: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-02-18 15:28:35.469726: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-18 15:28:36.139881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.140462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.645GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2021-02-18 15:28:36.140558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.141120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-02-18 15:28:36.141188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.141733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: 
pciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-02-18 15:28:36.141793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.142330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: 
pciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-02-18 15:28:36.142348: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-02-18 15:28:36.142369: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-02-18 15:28:36.142380: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-02-18 15:28:36.142389: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-02-18 15:28:36.142399: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-02-18 15:28:36.142408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2021-02-18 15:28:36.142418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-02-18 15:28:36.142427: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-02-18 15:28:36.142482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.143058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.143631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.144209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.144794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.145431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.146029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.146610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:36.147133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3
2021-02-18 15:28:36.147166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-02-18 15:28:40.439754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-18 15:28:40.439843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2 3 
2021-02-18 15:28:40.439851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y Y Y 
2021-02-18 15:28:40.439856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N Y Y 
2021-02-18 15:28:40.439859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   Y Y N Y 
2021-02-18 15:28:40.439863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   Y Y Y N 
2021-02-18 15:28:40.440129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.440819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.441455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.442052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.442640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.443222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.443766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6064 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-02-18 15:28:40.444097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.444766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.445324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 6122 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
2021-02-18 15:28:40.445584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.446200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.446752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 6122 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)
2021-02-18 15:28:40.446992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.447637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-18 15:28:40.448199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 6122 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
2021-02-18 15:28:44.046400: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-02-18 15:28:44.124782: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3299990000 Hz
2021-02-18 15:28:44.980720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
Illegal instruction (core dumped)
```

(I believe this is a `build/install` problem rather than a bug because I've already encountered this error in the context of an install issue)"
47244,Cannot assign a device for operation using Google Cloud TPU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
The codebase can be found at https://github.com/google/compare_gan.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu on Google Cloud.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Not applicable.
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
TF v1.15
- Python version:
3.8
- Bazel version (if compiling from source):
Not applicable.
- GCC/Compiler version (if compiling from source):
Not applicable.
- CUDA/cuDNN version:
Not applicable.
- GPU model and memory:
TPU v3-8

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
v1.15.0-rc3-22-g590d6ee 1.15.0
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
The original issue can be found at [here](https://github.com/google/compare_gan/issues/53). I used the exact same software setup as stated in the CompareGAN repo, and ran it on V3-8 TPU with 4 vCPU VM. 
 
I had the following error: 
 
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation input_pipeline_task0/TensorSliceDataset: node input_pipeline_task0/TensorSliceDataset (defined at /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py:1748)  was explicitly assigned to /job:worker/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.

It looks to me that this error is caused by different naming conventions from older TF. The original repo uses TensorFlow 1.15, while the firmware of TPU/VM might have been updated over time, thus, it may not properly recognize the device name. I tried a workaround as suggested here (https://github.com/google/compare_gan/issues/53), but using a GPU instance can be very expensive and it goes against the reason of using TPUs. 

Therefore, I wonder if there is a way to rename the device from TensorFlow, or manually assign the device for input_pipeline handling?

**Describe the expected behavior**
I expect TF to recognize the device name properly. 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

You can use the following command if you have a TPU:
```python3 compare_gan/main.py --use_tpu=True  --gin_config=/home/xxx/compare_gan/example_configs/resnet_cifar10.gin ```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
 File ""/home/xxx/.local/lib/python3.5/site-packages/tensorflow_datasets/core/api_utils.py"", line 52, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""/home/xxx/.local/lib/python3.5/site-packages/tensorflow_datasets/core/dataset_builder.py"", line 236, in download_and_prepare
    self.info.compute_dynamic_properties()
  File ""/home/xxx/.local/lib/python3.5/site-packages/tensorflow_datasets/core/dataset_info.py"", line 245, in compute_dynamic_properties
    self._compute_dynamic_properties(self._builder)
  File ""/home/xxx/.local/lib/python3.5/site-packages/tensorflow_datasets/core/dataset_info.py"", line 258, in _compute_dynamic_properties
    builder, split_name)
  File ""/home/xxx/.local/lib/python3.5/site-packages/tensorflow_datasets/core/dataset_info.py"", line 447, in get_dataset_feature_statistics
    for example in tqdm.tqdm(np_dataset, unit="" examples""):
  File ""/usr/local/lib/python3.5/dist-packages/tqdm/std.py"", line 1107, in __iter__
    for obj in iterable:
  File ""/home/xxx/.local/lib/python3.5/site-packages/tensorflow_datasets/core/dataset_utils.py"", line 98, in _graph_dataset_iterator
    yield sess.run(ds_item)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation input_pipeline_task0/TensorSliceDataset: node input_pipeline_task0/TensorSliceDataset (defined at /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py:1748)  was explicitly assigned to /job:worker/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.
         [[input_pipeline_task0/TensorSliceDataset]]
```"
47242,Cannot convert a symbolic Tensor (gru/strided_slice:0) to a numpy array.,"**System information**
- OS Platform and Distribution: Arch Linux
- TensorFlow installed from package: python-tensorflow
- TensorFlow version: unknown 2.4.1
- Python version: 3.9.1
- CUDA/cuDNN version: No GPU

**Standalone code to reproduce the issue**
```python
from tensorflow.keras.layers import Embedding, Input, GRU

x = Input(shape=(None,))
x = Embedding(input_dim=50, output_dim=16, mask_zero=True)(x)
x = GRU(units=256)(x)
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""/home/jnphilipp/Nextcloud/Code/jnphilipp/deep_learning/test3.py"", line 5, in <module>
    x = GRU(units=256)(x)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 660, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 951, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1090, in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 822, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 863, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent_v2.py"", line 439, in call
    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 859, in _process_inputs
    initial_state = self.get_initial_state(inputs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 642, in get_initial_state
    init_state = get_initial_state_fn(
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 1948, in get_initial_state
    return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 2987, in _generate_zero_filled_state_for_cell
    return _generate_zero_filled_state(batch_size, cell.state_size, dtype)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 3005, in _generate_zero_filled_state
    return create_zeros(state_size)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 3000, in create_zeros
    return array_ops.zeros(init_state_size, dtype=dtype)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 2819, in wrapped
    tensor = fun(*args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 2868, in zeros
    output = _constant_if_small(zero, shape, dtype, name)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 2804, in _constant_if_small
    if np.prod(shape) < 1000:
  File ""<__array_function__ internals>"", line 5, in prod
  File ""/usr/lib/python3.9/site-packages/numpy/core/fromnumeric.py"", line 3030, in prod
    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
  File ""/usr/lib/python3.9/site-packages/numpy/core/fromnumeric.py"", line 87, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 852, in __array__
    raise NotImplementedError(
NotImplementedError: Cannot convert a symbolic Tensor (gru/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```
"
47241,add tflite_micro with cmake files and zephyr modules,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (Linux Ubuntu 16.04):
- TensorFlow installed from (source):
- Tensorflow version (e85120faff5febebc6ac527b8cf977333a8dd6f4):
- Target platform (Arm Mbed OS):

**Describe the problem**

tflite only have makefile support, so add cmake support will make tf easy to be integarted.

**Please provide the exact sequence of commands/steps when you ran into the problem**
I use Zephyr RTOS as example, 
https://github.com/hakehuang/zephyr/commits/tf_int

cd samples/hello_world/
west build -b mimxrt1060_evk


"
47239,ParameterServerStrategy supporting GPU workers,"**Describe the feature and the current behavior/state.**
Hi TF team,

we can train very efficiently our models thanks to the PS strategy using tf 1.15. 
We are moving our codebase to 2.3 but it looks like you don't support GPU workers in the PS strategy right now.
Do you have any (even tentative) dates for shipping that feature?

**Will this change the current api? How?**

**Who will benefit from this feature?**

**Any Other info.**
"
47237,CMSIS-NN fully_connected kernel does not support null pointer bias,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
arm_fully_connected_s8 supports null pointer bias. Therefore should the null pointer checks be removed in CMSIS-NN kernel.

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
47236,How do I profile layers in tensorflow instead of ops? Is there a way to profile only layers in tensorflow?,
47235,Failed to convert QAT model to tflite when I use tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- TensorFlow installation (pip package or built from source):pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):2.3.1

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
When I use tf.lite.TFLiteConverter to convert keras ACTIVATIONS_INT16_WEIGHTS_INT8 QAT model to tflite fileThere are some bugs and the conversion is failed.
This is my convert code and Bugs.
**quant_converter1 = tf.lite.TFLiteConverter.from_keras_model(model1_quant)
quant_converter1.optimizations = [tf.lite.Optimize.DEFAULT]
quant_converter1.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
quant_tflite_model1 = quant_converter1.convert()
with open(model1_tflite_quant, 'wb') as f:
  f.write(quant_tflite_model1)**


![as](https://user-images.githubusercontent.com/12438262/108315281-17f5c700-71b3-11eb-9ec9-f0e0d1114f9f.png)
**I updated the TensorFlow to the latest stable version v2.4.1 and also  face the same error**  @amahendrakar



"
47233,Add NFNets to keras.applications,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes, as much as possible but probably need help.

**Describe the feature and the current behavior/state.**

DeepMind recently introduced their new ""Normalizer-Free"" [NFNets](https://arxiv.org/abs/2102.06171), matching the test accuracy of EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and their largest models attain a new state-of-the-art top-1 accuracy of 86.5%. It seems fitting that these would be available in keras since they surpass EfficientNets which are already available.

**Will this change the current api? How?**

This would introduce new APIs i.e., `tensorflow.keras.applications.NFNetF0` through to `tensorflow.keras.applications.NFNetF6`.

**Who will benefit with this feature?**

Anyone doing image classification, including transfer learning and fine tuning; a large portion of the machine learning community!

**Any Other info.**

DeepMind's [implementation](https://github.com/deepmind/deepmind-research/tree/master/nfnets)"
47232,Edited as it was spam,Edited as it was spam
47230,Using TensorArray for storing large dataset of interactions,"**Reading from TensorArray:**
 
```python
def __init__(self, size):
        self.obs_buf = tf.TensorArray(tf.float32, size=size, clear_after_read=False)
        self.obs2_buf = tf.TensorArray(tf.float32, size=size, clear_after_read=False)
        self.act_buf = tf.TensorArray(tf.float32, size=size, clear_after_read=False)
        self.rew_buf = tf.TensorArray(tf.float32, size=size, clear_after_read=False)
        self.done_buf = tf.TensorArray(tf.float32, size=size, clear_after_read=False)

 def get_sample(self, batch_size):
            idxs = tf.random.uniform(shape=[batch_size], maxval=self.size, dtype=tf.int32)
            tf.print(idxs)
            return self.obs_buf.gather(indices=idxs),     \     # HERE IS THE ISSUE
                   self.act_buf.gather(indices=idxs),     \
                   self.rew_buf.gather(indices=idxs),     \
                   self.obs2_buf.gather(indices=idxs),    \
                   self.done_buf.gather(indices=idxs)
```

**Usage:**

```python
@tf.function
def train(self, rpm, batch_size, gradient_steps):
    for gradient_step in tf.range(1, gradient_steps + 1):
        obs, act, rew, next_obs, done = rpm.get_sample(batch_size)

        with tf.GradientTape() as tape:
            ...
```
**Issue:**

> Traceback (most recent call last):
>   File "".\main.py"", line 130, in <module>
>     rl_training.train()
>   File ""C:\Users\user\Documents\Projects\rl-toolkit\rl_training.py"", line 129, in train
>     self._rpm, self.batch_size, self.gradient_steps, logging_wandb=self.logging_wandb
>   File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
>     result = self._call(*args, **kwds)
>   File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\def_function.py"", line 871, in _call
>     self._initialize(args, kwds, add_initializers_to=initializers)
>   File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\def_function.py"", line 726, in _initialize
>     *args, **kwds))
>   File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 2969, in _get_concrete_function_internal_garbage_collected
>     graph_function, _ = self._maybe_define_function(args, kwargs)
>   File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 3361, in _maybe_define_function
>     graph_function = self._create_graph_function(args, kwargs)
>   File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 3206, in _create_graph_function
>     capture_by_value=self._capture_by_value),
>   File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 990, in func_graph_from_py_func
>     func_outputs = python_func(*func_args, **func_kwargs)
>   File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\def_function.py"", line 634, in wrapped_fn
>     out = weak_wrapped_fn().__wrapped__(*args, **kwds)
>   File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 3887, in bound_method_wrapper
>     return wrapped_fn(*args, **kwargs)
>   File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 977, in wrapper
>     raise e.ag_error_metadata.to_exception(e)
> tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:
> 
>     C:\Users\user\Documents\Projects\rl-toolkit\policy\sac\sac.py:183 update  *
>         obs, act, rew, next_obs, done = rpm.get_sample(batch_size)
>     C:\Users\user\Documents\Projects\rl-toolkit\utils\replay_buffer.py:39 __call__  *
>         return self.obs_buf.gather(indices=idxs),                    self.act_buf.gather(indices=idxs),                    self.rew_buf.gather(indices=idxs),                    self.obs2_buf.gather(indices=idxs),                   self.done_buf.gather(indices=idxs)
>     C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\tensor_array_ops.py:1190 gather  **
>         return self._implementation.gather(indices, name=name)
>     C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\tensor_array_ops.py:861 gather
>         return array_ops.stack([self._maybe_zero(i) for i in indices])
>     C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py:505 __iter__
>         self._disallow_iteration()
>     C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py:498 _disallow_iteration
>         self._disallow_when_autograph_enabled(""iterating over `tf.Tensor`"")
>     C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py:476 _disallow_when_autograph_enabled
>         "" indicate you are trying to use an unsupported feature."".format(task))
> 
>     OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.

Why I cannot using TensorArray in this context? And what alternatives I have?

TF version: 2.4.1
Python version:  3.6.8"
47229,"Building TFLite WHL on Pi Zero (armv6l) - ""warning: requested alignment 16 is larger than 8 [-Wattributes]""","**System information**
- OS Platform and Distribution: Raspbian GNU/Linux 10 (buster) - **Fresh install**
- Pi Zero (armv6l)
- TensorFlow version: 2.4.0
- Python version: 3.7.3
- Installed with pip



Upon trying to build the TFLite Python wheel for my Pi Zero, `build_pip_package.sh ` spits out warnings and I believe causes my Pi to crash (green light is flashing consistently, my unix terminal I'm using to ssh to the Pi is unresponsive or extremely slow). I let it go the 2nd time around, and the Pi recovered, but I couldn't find the wheel anywhere on the Pi. I'm assuming it didn't actually finish and crashed instead, with ample recovery time after.

Here's what I did to try to build TFLite:
```
sudo apt install swig libjpeg-dev zlib1g-dev python3-dev python3-numpy
pip install numpy pybind11 && pip3 install numpy pybind11
./tensorflow/lite/tools/make/download_dependencies.sh #sh command did not work for these commands, error included in log
./tensorflow/lite/tools/pip_package/build_pip_package.sh
```


**Logs**
```
./tensorflow/lite/tools/make/download_dependencies.sh 
downloading https://gitlab.com/libeigen/eigen/-/archive/011e0db31d1bed8b7f73662be6d57d9f30fa457a/eigen-011e0db31d1bed8b7f73662be6d57d9f30fa457a.tar.gz
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 2604k    0 2604k    0     0  1851k      0 --:--:--  0:00:01 --:--:-- 1849k
checking sha256 of tensorflow/lite/tools/make/downloads/eigen
/tmp/tmp.KDDmNAnmqI/eigen-011e0db31d1bed8b7f73662be6d57d9f30fa457a.tar.gz: OK
downloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/gemmlowp/archive/fda83bdc38b118cc6b56753bd540caa49e570745.zip
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  914k  100  914k    0     0  1793k      0 --:--:-- --:--:-- --:--:-- 1797k
checking sha256 of tensorflow/lite/tools/make/downloads/gemmlowp
/tmp/tmp.ZohJqyHeDH/fda83bdc38b118cc6b56753bd540caa49e570745.zip: OK
Archive:  /tmp/tmp.ZohJqyHeDH/fda83bdc38b118cc6b56753bd540caa49e570745.zip
```
...
```
pi@raspberrypi:~/tensorflow $ sh tensorflow/lite/tools/pip_package/build_pip_package.sh
tensorflow/lite/tools/pip_package/build_pip_package.sh: 18: tensorflow/lite/tools/pip_package/build_pip_package.sh: Bad substitution
+ cd 
+ pwd
+ SCRIPT_DIR=/home/pi/tensorflow
+ PYTHON=python3
+ VERSION_SUFFIX=
+ export TENSORFLOW_DIR=/home/pi/tensorflow/../../../..
+ TENSORFLOW_LITE_DIR=/home/pi/tensorflow/../../../../tensorflow/lite
+ + + grepcut _VERSION =  /home/pi/tensorflow/../../../../tensorflow/tools/pip_package/setup.py -d=sed
 -f2 s/[ '-]//g

grep: /home/pi/tensorflow/../../../../tensorflow/tools/pip_package/setup.py: No such file or directory
+ TENSORFLOW_VERSION=
+ export PACKAGE_VERSION=
+ BUILD_DIR=/home/pi/tensorflow/gen/tflite_pip/python3
+ rm -rf /home/pi/tensorflow/gen/tflite_pip/python3
+ mkdir -p /home/pi/tensorflow/gen/tflite_pip/python3/tflite_runtime
+ cp -r /home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/debian /home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/setup.py /home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /home/pi/tensorflow/../../../../tensorflow/lite/python/interpreter_wrapper /home/pi/tensorflow/gen/tflite_pip/python3
cp: cannot stat '/home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/debian': No such file or directory
cp: cannot stat '/home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/setup.py': No such file or directory
cp: cannot stat '/home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in': No such file or directory
cp: cannot stat '/home/pi/tensorflow/../../../../tensorflow/lite/python/interpreter_wrapper': No such file or directory
```
...
```
pi@raspberrypi:~/tensorflow $ ./tensorflow/lite/tools/pip_package/build_pip_package.sh
+++ dirname ./tensorflow/lite/tools/pip_package/build_pip_package.sh
++ cd ./tensorflow/lite/tools/pip_package
++ pwd
+ SCRIPT_DIR=/home/pi/tensorflow/tensorflow/lite/tools/pip_package
+ PYTHON=python3
+ VERSION_SUFFIX=
+ export TENSORFLOW_DIR=/home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../..
+ TENSORFLOW_DIR=/home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../..
+ TENSORFLOW_LITE_DIR=/home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite
++ cut -d= -f2
++ grep '_VERSION = ' /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py
++ sed 's/[ '\''-]//g'
+ TENSORFLOW_VERSION=2.4.0
+ export PACKAGE_VERSION=2.4.0
+ PACKAGE_VERSION=2.4.0
+ BUILD_DIR=/home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ rm -rf /home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ mkdir -p /home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime
+ cp -r /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup.py /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ cp /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime
+ echo '__version__ = '\''2.4.0'\'''
++ git -C /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe
fatal: not a git repository (or any of the parent directories): .git
+ echo '__git_version__ = '\'''\'''
+ cd /home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ case ""${TENSORFLOW_TARGET}"" in
+ [[ -n '' ]]
+ python3 setup.py bdist bdist_wheel
running bdist
running bdist_dumb
running build
running build_py
running build_ext
make: Entering directory '/home/pi/tensorflow'
g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/activations.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/linux_armv6l/obj/tensorflow/lite/kernels/activations.o
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:94,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h: In member function Packet Eigen::internal::UniformRandomGenerator<T>::packetOp(Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h:213:40: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX T values[packetSize];
                                        ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h: In member function Packet Eigen::internal::NormalRandomGenerator<T>::packetOp(Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h:322:40: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX T values[packetSize];
                                        ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:104,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In member function Eigen::TensorReductionEvaluatorBase<const Eigen::TensorReductionOp<Op, Dims, XprType, MakePointer_>, Device>::PacketReturnType Eigen::TensorReductionEvaluatorBase<const Eigen::TensorReductionOp<Op, Dims, XprType, MakePointer_>, Device>::packet(Eigen::TensorReductionEvaluatorBase<const Eigen::TensorReductionOp<Op, Dims, XprType, MakePointer_>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:811:93: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                             ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:107,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h: In member function Eigen::TensorEvaluator<const Eigen::TensorConcatenationOp<Axis, LhsXprType, RhsXprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorConcatenationOp<Axis, LhsXprType, RhsXprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorConcatenationOp<Axis, LhsXprType, RhsXprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h:271:54: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX CoeffReturnType values[packetSize];
                                                      ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h: In member function void Eigen::TensorEvaluator<Eigen::TensorConcatenationOp<Axis, LhsXprType, RhsXprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorConcatenationOp<Axis, LhsXprType, RhsXprType>, Device>::Index, const PacketReturnType&):
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h:384:54: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX CoeffReturnType values[packetSize];
                                                      ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:108,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h: In member function typename Eigen::internal::enable_if<(Eigen::internal::unpacket_traits<Packet>::size == packet_size), PacketT>::type Eigen::internal::BaseTensorContractionMapper<Scalar, Index, side, Tensor, nocontract_t, contract_t, packet_size, inner_dim_contiguous, inner_dim_reordered, Alignment, MakePointer_>::load(Index, Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h:327:44: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX Scalar data[packet_size];
                                            ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h: In member function typename Eigen::internal::enable_if<(Eigen::internal::unpacket_traits<Packet>::size != packet_size), PacketT>::type Eigen::internal::BaseTensorContractionMapper<Scalar, Index, side, Tensor, nocontract_t, contract_t, packet_size, inner_dim_contiguous, inner_dim_reordered, Alignment, MakePointer_>::load(Index, Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h:347:54: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX Scalar data[requested_packet_size];
                                                      ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h: In member function PacketT Eigen::internal::BaseTensorContractionMapper<Scalar, Index, side, Tensor, nocontract_t, contract_t, 1, inner_dim_contiguous, inner_dim_reordered, Alignment, MakePointer_>::loadPacket(Index, Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h:393:34: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX Scalar data[1];
                                  ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h: In member function PacketT Eigen::internal::BaseTensorContractionMapper<Scalar, Index, side, Tensor, nocontract_t, contract_t, 1, inner_dim_contiguous, inner_dim_reordered, Alignment, MakePointer_>::load(Index, Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h:399:34: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX Scalar data[1];
                                  ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:113,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h: In member function TgtPacket Eigen::PacketConverter<TensorEvaluator, SrcPacket, TgtPacket, 1, TgtCoeffRatio>::packet(Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:160:95: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX typename internal::unpacket_traits<TgtPacket>::type values[TgtPacketSize];
                                                                                               ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h: In static member function static TargetPacket Eigen::internal::PacketConv<SrcPacket, TargetPacket, LoadMode, ActuallyVectorize, IsSameT>::run(const Eigen::TensorEvaluator<ArgType, Device>&, Eigen::Index):
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:260:88: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<TargetType>::type values[PacketSize];
                                                                                        ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h: In static member function static TargetPacket Eigen::internal::PacketConv<SrcPacket, TargetPacket, LoadMode, false, true>::run(const Eigen::TensorEvaluator<ArgType, Device>&, Eigen::Index):
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:292:88: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<TargetType>::type values[PacketSize];
                                                                                        ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:114,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConvolution.h: In member function Eigen::TensorEvaluator<const Eigen::TensorConvolutionOp<Dimensions, InputXprType, KernelXprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorConvolutionOp<Dimensions, InputXprType, KernelXprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorConvolutionOp<Dimensions, InputXprType, KernelXprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConvolution.h:448:45: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX Scalar data[PacketSize];
                                             ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:116,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorPatch.h: In member function Eigen::TensorEvaluator<const Eigen::TensorPatchOp<PatchDim, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorPatchOp<PatchDim, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorPatchOp<PatchDim, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorPatch.h:250:56: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX CoeffReturnType values[PacketSize];
                                                        ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:117,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h: In member function Eigen::TensorEvaluator<const Eigen::TensorImagePatchOp<Rows, Cols, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorImagePatchOp<Rows, Cols, XprType>, Device>::packetWithPossibleZero(Eigen::TensorEvaluator<const Eigen::TensorImagePatchOp<Rows, Cols, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h:543:93: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                             ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:118,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorVolumePatch.h: In member function Eigen::TensorEvaluator<const Eigen::TensorVolumePatchOp<Planes, Rows, Cols, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorVolumePatchOp<Planes, Rows, Cols, XprType>, Device>::packetWithPossibleZero(Eigen::TensorEvaluator<const Eigen::TensorVolumePatchOp<Planes, Rows, Cols, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorVolumePatch.h:546:93: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                             ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:119,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h: In member function Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::packetOneByNByOne(Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:375:93: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                             ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h: In member function Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::packetOneByN(Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:429:95: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                               ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h: In member function Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::packetNByOne(Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:447:93: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                             ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h: In member function Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::packetColMajor(Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:521:95: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                               ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h: In member function Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::packetRowMajor(Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:579:95: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                               ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:120,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h: In member function Eigen::TensorEvaluator<const Eigen::TensorChippingOp<DimId, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorChippingOp<DimId, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorChippingOp<DimId, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h:243:95: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                               ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h:263:97: warning: requested alignment 16 is larger than 8 [-Wattributes]
         EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                                 ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h: In member function void Eigen::TensorEvaluator<Eigen::TensorChippingOp<DimId, XprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorChippingOp<DimId, XprType>, Device>::Index, const PacketReturnType&):
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h:469:95: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                               ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h:489:97: warning: requested alignment 16 is larger than 8 [-Wattributes]
         EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                                 ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:121,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorInflation.h: In member function Eigen::TensorEvaluator<const Eigen::TensorInflationOp<Strides, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorInflationOp<Strides, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorInflationOp<Strides, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorInflation.h:205:93: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                             ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:123,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h: In member function Eigen::TensorEvaluator<const Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:628:95: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[packetSize];
                                                                                               ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h: In member function void Eigen::TensorEvaluator<Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>, Device>::Index, const PacketReturnType&):
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:823:56: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX CoeffReturnType values[packetSize];
                                                        ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:124,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorPadding.h: In member function Eigen::TensorEvaluator<const Eigen::TensorPaddingOp<PaddingDimensions, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorPaddingOp<PaddingDimensions, XprType>, Device>::packetWithPossibleZero(Eigen::TensorEvaluator<const Eigen::TensorPaddingOp<PaddingDimensions, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorPadding.h:683:93: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                             ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:125,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h: In member function Eigen::TensorEvaluator<const Eigen::TensorReverseOp<ReverseDimensions, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorReverseOp<ReverseDimensions, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorReverseOp<ReverseDimensions, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h:238:78: warning: requested alignment 16 is larger than 8 [-Wattributes]
                                                             values[PacketSize];
                                                                              ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h: In member function void Eigen::TensorEvaluator<Eigen::TensorReverseOp<ReverseDimensions, XprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorReverseOp<ReverseDimensions, XprType>, Device>::Index, const PacketReturnType&):
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h:470:54: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX CoeffReturnType values[PacketSize];
                                                      ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:126,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h: In static member function static Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::PacketLoader<LoadMode, Self, ImplPacketAccess>::Run(const Self&, Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::Index):
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h:211:95: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                               ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h: In static member function static Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::PacketLoader<LoadMode, Self, true>::Run(const Self&, Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::Index):
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h:228:97: warning: requested alignment 16 is larger than 8 [-Wattributes]
         EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                                 ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h: In member function void Eigen::TensorEvaluator<Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::Index, const PacketReturnType&):
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h:416:93: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                             ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:127,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h: In member function Eigen::TensorEvaluator<const Eigen::TensorStridingOp<Strides, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorStridingOp<Strides, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorStridingOp<Strides, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h:212:95: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
                                                                                               ^
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h: In member function void Eigen::TensorEvaluator<Eigen::TensorStridingOp<Strides, XprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorStridingOp<Strides, XprType>, Device>::Index, const PacketReturnType&):
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h:347:47: warning: requested alignment 16 is larger than 8 [-Wattributes]
       EIGEN_ALIGN_MAX Scalar values[PacketSize];
                                               ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:131,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorGenerator.h: In member function Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorGenerator.h:159:93: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[packetSize];
                                                                                             ^
In file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:134,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,
                 from tensorflow/lite/kernels/activations.cc:29:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorTrace.h: In member function Eigen::TensorEvaluator<const Eigen::TensorTraceOp<Dims, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorTraceOp<Dims, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorTraceOp<Dims, XprType>, Device>::Index) const:
/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorTrace.h:250:93: warning: requested alignment 16 is larger than 8 [-Wattributes]
     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];
```
"
47224,"Keras cannot restore custom functions for subclassed Model, but for subclassed Layer custom functions can be restored","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Keras cannot restore custom functions for subclassed Model, even with custom_object arguments; but for subclassed Layer, custom functions can be restored. See two code snippet attached.

**Describe the expected behavior**
I expect the consistent behavior between subclassed Model and subclassed Layer

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
 import tensorflow as tf
 import numpy as np

 class MyModel(tf.keras.models.Model):

   def __init__(self, **kargs):
     super(MyModel, self).__init__(**kargs)
     self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
     self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

   def call(self, inputs):
     x = self.dense1(inputs)
     return self.dense2(x)

   def about(self):
       print(""123"")

 model = MyModel()

 x = np.random.random((2, 3))
 model(x)

 model_file = ""model_file""
 model.save(model_file)

 loaded_model = tf.keras.models.load_model(model_file, custom_objects={""MyModel"": MyModel})
 loaded_model.about()
```

```
 import tensorflow as tf
 import numpy as np


 class MyDense(tf.keras.layers.Dense):
     def __init__(self, units, **kargs):
         super(MyDense, self).__init__(units=units, **kargs)
     def call(self, inputs):
         return super(MyDense, self).call(inputs)
     def my_units(self):
         return self.units
     @classmethod
     def convert_to_p(cls, p_inputs):
         print(""now converting to p"")

 x = tf.keras.Input(shape=(10,))
 dense1 = MyDense(units=20)(x)
 y = MyDense(units=30)(dense1)
 model = tf.keras.models.Model(inputs=[x], outputs=[y])
 # tf.keras.utils.plot_model(model)
 outputs = model(np.random.random((2, 10)))
 model_path = ""./foo""
 model.save(model_path)
 loaded_model = tf.keras.models.load_model(model_path, custom_objects={""MyDense"": MyDense})
 for layer in loaded_model.layers:
     if hasattr(layer, ""my_units""):
         print(layer.name, layer.my_units())
         layer.convert_to_pynchpin([1])
         print()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47223,Long prediction time when using tensorflow lite optimize,"I wrote a model based on the VGG16 using mostly TensorFlow documentation. After training, I saved the model into a .h5 file.
When I run the prediction, it takes about a second and a half to return the results. As I try to optimize the prediction time using `tensorflow.lite.Optimize.DEFAULT` (code available below) the prediction time increases drastically. I'm talking about almost 600 times slower than the normal code (when I don't use the Optimize). Why is that? Shouldn't optimize suppose to decrease the prediction time instead of increasing it? I used the *default* optimizer, *latency* optimizer, and *size* optimizer without any change in the result.
I used both CPU and GPU versions of TensorFlow without any significant change in the result.

**My system is:**
-  Intel Core i7 9700H
-  16 Gb DDR4 2300Mhz
-  GTX 1660ti (mobile version)
-  Windows 10 (latest build)

The code:
```
import tensorflow as tf
import os
import time
import numpy as np
from tensorflow import keras
import pathlib

saved_model_dir= 'model/'
saved_modelh5 = 'model.h5'
dataset_path = 'bound box dataset/img'
out_path = 'converted_model.tflite'
num_calibration_steps = 1

print(""Im doing something"")

converter = tf.lite.TFLiteConverter.from_keras_model(keras.models.load_model(saved_modelh5))  
converter.optimizations = [tf.lite.Optimize.DEFAULT  ]
tflite_model = converter.convert()


#------------------------------------------------------------
#with open(out_path, ""wb"")as f:
print('converted')
tflite_model_file = pathlib.Path(out_path)
tflite_model_file.write_bytes(tflite_model)
print('Saved')


input_data = np.random.rand(1,512,512,3).astype(np.float32)

interpreter = tf.lite.Interpreter( model_content = tflite_model)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test model on random input data.
t = time.time()
input_shape = input_details[0]['shape']
#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
print('start invoke')
interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])

t = time.time() - t
print('predict time:',t)
```



"
47222,LSTM - Issue with model fitting using ragged tensor,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution : Linux Ubuntu 16.04 / Google Colab
- TensorFlow installed from (source or binary): Google Colab
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.9
- CUDA/cuDNN version: Not used
- GPU model and memory: None

** Problem Description**

I'm working on classification of ECG signals into 4 classes. Each sequence to be classified has potentially different lengths. Sequences are stored into a ragged tensor. 

The model is based on LSTM layers. Unfortunatly when I try to fit the model using the function ""fit"" there is a problem about matrix size incompatibility:

```
InvalidArgumentError:  Matrix size-incompatible: In[0]: [32,18000], In[1]: [18286,256]
	 [[node sequential/LSTM_1/lstm_cell/MatMul (defined at <ipython-input-7-36997f53d4b2>:19) ]] [Op:__inference_train_function_6985]
```
Nevertheless, when I use a subset of this exact dataframe (25 sequence of 8500+), the training is going well to the end. 

I have verified, there are no Nan of None in my sequences. 

**Describe the current behavior**

The training of the model returns an error concerning matrix size incompatibility. The training can be performed when using a subset of the dataset.

**Describe the expected behavior**

Model can be trained well with the entire dataset

**Standalone code to reproduce the issue**

_Notebook link :_  https://colab.research.google.com/drive/1RdtafxvD_5nqh9-xuG6o8WL4GOd9IZnN?authuser=1#scrollTo=Dzq3tRK9cwbe

_Data link :_ https://drive.google.com/file/d/1V9nNxMehtiwAjNVS-8usTP6Jol9jC6et/view?usp=sharing

**Other info / logs** 

```
Epoch 1/10
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-7-36997f53d4b2> in <module>()
     17 model.summary()
     18 
---> 19 model.fit(xx,yy, epochs=10) # --> Doesn t working with the full dataset, matrix size error

6 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

InvalidArgumentError:  Matrix size-incompatible: In[0]: [32,18000], In[1]: [18286,256]
	 [[node sequential/LSTM_1/lstm_cell/MatMul (defined at <ipython-input-7-36997f53d4b2>:19) ]] [Op:__inference_train_function_6985]

Function call stack:
train_function
```
Thanks in advance 


"
47220,"Does TensorFlow do some optimization (like merge a conv and BN into one conv) when converting to TFLite? If yes, is there any other layers merge optimization Tensorflow did?","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No):Yes


**Describe the feature and the current behavior/state.**
What kind of layers merge does Tensorflow do when converting to TFLite? (Like merge consecutive conv and BN into a new conv). I am working on some optimization part of TFLite and If Tensorflow already did some layers merge optimization part then I do not need to redo this part.

**Will this change the current api? How?**
No
**Who will benefit with this feature?**
Anyone who would like to do optimizations to TFLite
**Any Other info.**
"
47218,I have problem with python code in google colab,"I write this code:
basemodel.fit(X_train,y_train,epochs=20,validation_split=.1,callbacks=call_back)
but don't work this error:
Epoch 1/20
ValueError Traceback (most recent call last)
in ()
----> 1 basemodel.fit(X_train,y_train,epochs=20,validation_split=0.1,callbacks=call_back)

9 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
975 except Exception as e: # pylint:disable=broad-except
976 if hasattr(e, ""ag_error_metadata""):
--> 977 raise e.ag_error_metadata.to_exception(e)
978 else:
979 raise"
47217,DSP HiFi4 support (delegate) for TF-Lite,"I wonder if there is any support (experimental) for HiFi4 DSP of Cadance ([Xtensa HiFi4 Audio DSP](https://ip.cadence.com/uploads/928/TIP_PB_HiFi_DSP_FINAL-pdf)), and possibly check there is an execution mechanism, probably delegate executor for this DSP.  
So question or request is: Is there any support available or would be possible to add HiFi4 delegate or another mechanism to leverage from HiF4 DSP?"
47216,TypeError if set the weights to the current weights via `set_weights`,"
**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 20.04 `Linux XXX 5.8.0-43-generic #49~20.04.1-Ubuntu SMP Fri Feb 5 09:57:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux`
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version (use command below): `v2.4.0-49-g85c8b2a817f 2.4.1`
- Python version: `3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0]`

**Describe the current behavior**
I get the error `TypeError: __array__() takes 1 positional argument but 2 were given`. Hence it might be related to bug #46840. 


**Describe the expected behavior**
Should just set the weight to what they are right now.

**Standalone code to reproduce the issue**
```
import tensorflow as tf

denseLayer = tf.keras.layers.Dense(1,activation=""relu"")
denseLayer.build(input_shape=(4))

denseLayer.set_weights(denseLayer.weights)
```
Link: [Colab](https://colab.research.google.com/drive/1bwHmMvktEsLzOK-x8fsmIJBiNJGfsM9F?usp=sharing)

**Other info / logs** 
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-21-99e968378f7a> in <module>
      4 denseLayer.build(input_shape=(4))
      5 
----> 6 denseLayer.set_weights(denseLayer.weights)

~/Projects/NotebooksEnvs/py38Env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in set_weights(self, weights)
   1875         weight_index += 1
   1876 
-> 1877     backend.batch_set_value(weight_value_tuples)
   1878 
   1879   def get_weights(self):

~/Projects/NotebooksEnvs/py38Env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/Projects/NotebooksEnvs/py38Env/lib/python3.8/site-packages/tensorflow/python/keras/backend.py in batch_set_value(tuples)
   3704   if ops.executing_eagerly_outside_functions():
   3705     for x, value in tuples:
-> 3706       x.assign(np.asarray(value, dtype=dtype(x)))
   3707   else:
   3708     with get_graph().as_default():

~/Projects/NotebooksEnvs/py38Env/lib/python3.8/site-packages/numpy/core/_asarray.py in asarray(a, dtype, order)
     81 
     82     """"""
---> 83     return array(a, dtype, copy=False, order=order)
     84 
     85 

TypeError: __array__() takes 1 positional argument but 2 were given
```
"
47214,"ValueError: No gradients provided for any variable: ['conv1_conv/kernel:0', 'conv1_conv/bias:0', 'conv2_block1_preact_bn/gamma:0', 'conv2_block1_preact_bn/beta:0', 'conv2_block1_1_conv/kernel:0',","I cannot understand how I can solve this issue.  I would be very grateful for help!  

https://colab.research.google.com/drive/1ZFrfT7BPV7S75WD0082VTswNkdYqYSWf?usp=sharing"
47213,Tensorflow Lite QAT: Double QUANT/DEQUANT in half-quantized and failure in full-quantized model conversion,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04.1-Ubuntu
- TensorFlow installation (pip package or built from source): pip install tensorflow==2.3.0-gpu
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

import tensorflow as tf

import tensorflow_model_optimization as tfmot

from constants import *
from dataset_loader import DatasetLoader

import cv2
from constants import *
import numpy as np

import numpy as np
import pathlib
from imutils import paths
import argparse

if __name__ == '__main__':

    ap = argparse.ArgumentParser()
    ap.add_argument(""-m"", ""--model"", type=str, default=""none"", help=""name of pre-trained network to use"")
    ap.add_argument(""-q"", ""--qat"", type=int, default=1, help=""use qat"")
    args = vars(ap.parse_args())

    #
    # LOAD MODEL
    #
    print(""[INFO] load model from file... {}"".format(args[""model""]))
    model = tf.keras.models.load_model(args[""model""])

    # LOAD DATASET
    dl = DatasetLoader()
    dl.load()
    trainX = dl.images
    valX   = dl.images_test
    trainY = dl.labels
    valY   = dl.labels_test

    # .npy data are UINT8_T, SIZE_FACExSIZE_FACE, convert to float
    trainX = trainX.astype(""float"")
    valX = valX.astype(""float"")

    #
    # QAT
    #
    if(args[""qat""]):
        print(""[INFO] build QAT model"")

        quantize_model = tfmot.quantization.keras.quantize_model

        q_aware_model = quantize_model(model)

        q_aware_model.compile(optimizer='adam',
                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])

        q_aware_model.fit( trainX,
                           trainY,
                           validation_data=(valX, valY),
                           shuffle=True,                       
                           batch_size=256, 
                           epochs=1, 
                           validation_split=0.1)

        baseline_model_accuracy = model.evaluate( valX, valY, verbose=0)
        q_aware_model_accuracy = q_aware_model.evaluate(valX,valY, verbose=0)

        _, baseline_model_accuracy = model.evaluate(
            valX, valY, verbose=0)

        _, q_aware_model_accuracy = q_aware_model.evaluate(
            valX, valY, verbose=0)

        print(baseline_model_accuracy )
        print(q_aware_model_accuracy)


    if(args[""qat""]):
        print(""\n[INFO] set converter..."")
        my_model=q_aware_model
    else:
        print(""\n[INFO] set converter..."")
        my_model = model

    converter = tf.lite.TFLiteConverter.from_keras_model( my_model ) 


    print(""\n[INFO] convert to TFLITE/FLOAT32"")
    tflite_model = converter.convert()

    #
    # STORE: tflite model where all the parameters are FLOAT32
    #
    tflite_models_dir = pathlib.Path(""./output"")
    tflite_models_dir.mkdir(exist_ok=True, parents=True)

    tflite_model_file = tflite_models_dir/""fer_model_float.tflite""
    print(""[INFO] writing TFLITE/FLOAT32    : {}"".format(tflite_model_file))
    tflite_model_file.write_bytes(tflite_model)


    #
    # STORE: tflite model where all the parameters are UINT8
    #
    converter2 = tf.lite.TFLiteConverter.from_keras_model(my_model ) 
    
    representative_ds = tf.data.Dataset.from_tensor_slices((trainX)).batch(32)

    def representative_data_float_gen():
      for input_value in representative_ds.take(100):
        yield [input_value]

    # convert again, in/out & parameters are now UINT8
    converter2.optimizations = [tf.lite.Optimize.DEFAULT]
    converter2.representative_dataset = representative_data_float_gen

    print(""\n[INFO] convert to TFLITE/HALF-QUANT"")
    tflite_model_quant      = converter2.convert()
    tflite_model_quant_file = tflite_models_dir/""fer_model_half-quant.tflite""
    print(""[INFO] writing TFLITE/HALF-QUANT : {}"".format(tflite_model_quant_file))
    tflite_model_quant_file.write_bytes(tflite_model_quant)


    #
    # STORE: tflite model where all the parameters are still using float32
    #
    converter3 = tf.lite.TFLiteConverter.from_keras_model(my_model ) 
    converter3.representative_dataset      = representative_data_float_gen
    converter3.optimizations               = [tf.lite.Optimize.DEFAULT]
    converter3.target_spec.supported_ops   = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter3.target_spec.supported_types = [tf.int8]    
    converter3.inference_input_type        = tf.uint8
    converter3.inference_output_type       = tf.uint8

    print(""\n[INFO] convert to TFLITE/FULL-QUANT"")
    tflite_model_quant_io = converter3.convert()
    tflite_model_quant_io_file = tflite_models_dir/""fer_model_full-quant.tflite""
    print(""[INFO] writing TFLITE/FULL-QUANT : {}"".format(tflite_model_quant_io_file))
    tflite_model_quant_io_file.write_bytes(tflite_model_quant_io)


    # END
    print(""\nbye.\n\n"")

The keras model is very simple:

class SimpleNet:
    @staticmethod
    def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9):

        inputShape = (height, width, depth)
        chanDim = -1

        if K.image_data_format() == ""channels_first"":
            inputShape = (depth, height, width)
            chanDim = 1


        model = tf.keras.Sequential()

        model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu', input_shape=inputShape ))
        model.add(MaxPooling2D((3, 3), strides=(2, 2)))
        model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))
        model.add(MaxPooling2D((3, 3), strides=(2, 2)))
        model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))
    
        model.add(Flatten())
        model.add(Dropout(0.3))
        model.add(Dense(512, kernel_regularizer=l2(reg)))
        model.add(Dropout(0.3))
        model.add(Dense(256, kernel_regularizer=l2(reg)))
        model.add(Dropout(0.3))
        model.add(Dense(classes, activation=""softmax""))        

        return model

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:
I'm training the keras model and the converting to tflite using QAT as per official documentation.

Conversion to half-quantization is successful but the .tflite model has n.2 QUANTIZE at the input and n.2 DEQUANTIZE at the output.

Conversion to full-quantized model fails to convert but I cannot detect what op is not supported, here is my console log

[INFO] load model from file... ./output/fer_model_float.hdf5
[INFO] build QAT model
69/69 [==============================] - 3s 43ms/step - loss: 1.8564 - accuracy: 0.6415 - val_loss: 1.7545 - val_accuracy: 0.6627
0.6648070812225342
0.6155276894569397

[INFO] set converter...

[INFO] convert to TFLITE/FLOAT32
[INFO] writing TFLITE/FLOAT32    : output/fer_model_float.tflite

[INFO] convert to TFLITE/HALF-QUANT
[INFO] writing TFLITE/HALF-QUANT : output/fer_model_half-quant.tflite

[INFO] convert to TFLITE/FULL-QUANT
Traceback (most recent call last):
  File ""./tflite.py"", line 217, in <module>
    tflite_model_quant_io_file.write_bytes(tflite_model_quant_io)
  File ""/home/gfilippi/.virtualenvs/tf23/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 831, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File ""/home/gfilippi/.virtualenvs/tf23/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 638, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""/home/gfilippi/.virtualenvs/tf23/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 452, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File ""/home/gfilippi/.virtualenvs/tf23/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 98, in calibrate_and_quantize
    np.dtype(activations_type.as_numpy_dtype()).num)
RuntimeError: Quantization not yet supported for op:


### 5. (optional) Any other info / logs
this seems similar to the problem documented here:

https://github.com/tensorflow/tensorflow/issues/41308

![double_q-deq](https://user-images.githubusercontent.com/1153348/108218102-99f2db00-7134-11eb-827a-26a09017a89f.png)
"
47212,tensorflow.python.framework.errors_impl.NotFoundError: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringB5cxx11EPNS_15OpKernelContextEb,"I encounter a bug that occurs when I try to import tensorflow in a conda environment (python 3.8). All my attempts so far, to even revert to a previous version or recreate the environment from scratch with different version of tensorflow, failed so far.
The issue is similar to https://github.com/tensorflow/tensorflow/issues/42084 but I didn't manage to get a fix out of it, even some others have the same issue (and the end was ""It's better to open a new issue and post the full details.""), so here I am!

# About the system
A Kubuntu (Linux) computer, version 20.04. The installation step are close to https://www.tensorflow.org/install/pip#ubuntu-macos, adapted to the current version of Kubuntu for the nvidia dependencies.
Pilots Nvidia Updated
CUDA/cuDNN version: 11.00
TF installed from conda (Binary)
TF version 2.2.0 or 2.3.0

# Step to reproduce
Launch python 3.8
```
>>> import tensorflow
2021-02-17 10:35:57.687845: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/me/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 436, in <module>
    _ll.load_library(_main_dir)
  File ""/home/me/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 153, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: /home/me/.conda/envs/CasComplexe/lib/python3.8/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringB5cxx11EPNS_15OpKernelContextEb
```

"
47211,Support for tf.where without x and y arguments in XLA,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 10 / Linux Ubuntu 20.04 / Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: conda 3.8.5
- CUDA/cuDNN version: 11.0
- GPU model and memory: RTX 2080 Super

**Describe the current behavior**
The following error is raised, either the code is run on CPU or GPU:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/anaconda3/envs/jplu-transformers/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/opt/anaconda3/envs/jplu-transformers/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 895, in _call
    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access
  File ""/opt/anaconda3/envs/jplu-transformers/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1919, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/opt/anaconda3/envs/jplu-transformers/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 560, in call
    ctx=ctx)
  File ""/opt/anaconda3/envs/jplu-transformers/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_where_7}} = __inference_where_7[_XlaMustCompile=true, config_proto=""\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0012\005*\0010J\0008\001\202\001\000"", executor_type=""""](dummy_input).
Uncompilable nodes:
Where: unsupported op: No registered 'Where' OpKernel for XLA_GPU_JIT devices compatible with node {{node Where}}
        Stacktrace:
                Node: __inference_where_7, function:
                Node: Where, function: __inference_where_7
 [Op:__inference_where_7]
```

**Describe the expected behavior**
I'm expecting to have the Where op compliant with XLA on GPU and CPU as stated by @blakehechtman in https://github.com/tensorflow/tpu/issues/584 and get the following result:
```python
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]], dtype=int64)>
```

**Standalone code to reproduce the issue**
Here a piece of code to reproduce the issue:
```python
import tensorflow as tf

mask = tf.constant([True, True, False])

@tf.function(experimental_compile=True)
def test():
  return tf.where(mask)

test()
```

The context is that I need to have the indices of the `True` values in a mask in order to be able to use the `tf.gather_nd`, `tf.scatter_nd` and `tf.tensor_scatter_nd_update` methods."
47210,Use of tensorflow_addons instead of tf.contrib,"I am using tensorflow2. As we know tensorflow2 is not supporting tf.contrib and tf.contrib is move to tensorflow_addons.
But I don't know how to use tesorflow_addons instead of tf.contrib

```
import tensorflow as tf
import tensorflow_addons as tfa

slim_example_decoder = tf.contrib.slim.tfexample_decoder
```

I want to write `slim_example_decoder = tf.contrib.slim.tfexample_decoder` using **tesorflow_addons**.
can anyone have any idea?"
47208,Replacing activation in a pretrained network,"I'm trying to replace swish activation with relu activation in pretrained TF model EfficientNetB0. EfficientNetB0 uses swish activation in Conv2D and Activation layers. I could not find any relevant resources on how to do this. Is there any TF API to do this? Below is the code that I'm working on to replace swish activations with relu activations. Any pointers/help on replacing the activation is much appreciated.

```
import tensorflow as tf
from tensorflow.keras.layers import ReLU

def replace_swish_with_relu(model):
    '''
    Modify passed model by replacing swish activation with relu
    '''
    for layer in tuple(model.layers):
        layer_type = type(layer).__name__
        if hasattr(layer, 'activation') and layer.activation.__name__ == 'swish':
            print(layer_type, layer.activation.__name__)
            if layer_type == ""Conv2D"":
                # conv layer with swish activation.
                # Do something
                layer.activation = ReLU() # This didn't work
            else:
                # activation layer
                # Do something
                layer = tf.keras.layers.Activation('relu', name=layer.name + ""_relu"") # This didn't work
    return model

# load pretrained efficientNet
model = tf.keras.applications.EfficientNetB0(
    include_top=True, weights='imagenet', input_tensor=None,
    input_shape=(224, 224, 3), pooling=None, classes=1000,
    classifier_activation='softmax')

# convert swish activation to relu activation
model = replace_swish_with_relu(model)
model.save(""efficientNet-relu"")
```"
47207,TFLite converter produces wrong output shape_signature if RNN / LSTM output layer in model (becomes all unknown dimensions),"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly==2.5.0-dev20210216 and tensorflow>=2.4.0

### 2. Code

- [Collab notebook](https://colab.research.google.com/drive/1csQPhZpdidop2OS3JOGEGtCTU1JWwiWU?usp=sharing)

- Code:
```py
import tensorflow as tf
from pprint import pprint

def convertModelToTFLite(path, outPath):

    print(f""[INFO] Using tensorflow v{tf.__version__}"")
    converter = tf.lite.TFLiteConverter.from_saved_model(path)
    converter.experimental_new_converter = True
    tfliteModel = converter.convert()

    # Save the model
    with open(outPath, 'wb') as f:
        f.write(tfliteModel)

units = 256
savePath = ""my_model""

# creating model
i = tf.keras.Input(shape=(1, 521), name='input')
x = tf.keras.layers.Dense(units)(i)
x = tf.keras.layers.LSTM(units, return_sequences=True)(x)
# x = tf.keras.layers.Dense(units)(x)  # <-------- Having a dense output layer gives correct  output signature 

# saving as SavedModel
model = tf.keras.models.Model(inputs=[i], outputs=[x])
model.save(savePath, save_format='tf')

convertModelToTFLite(savePath, f""{savePath}.tflite"", args.optimize)
ip = tf.lite.Interpreter(f""{savePath}.tflite"")

print(model.summary())
pprint(ip.get_output_details())
```

### 3. Failure after conversion

- Conversion **is** successful and model **works** if I resize the output tensor to expected dimensions and reallocate tensors
- **But** I expect the shape signature of the output to be correct, and have the last dimension (feature dimension) be known and equal to the number of units / cells in the LSTM.

- With an LSTM or RNN layer as the output layer to a `keras.Model`, the TFLite model has the unexpected output shape signature :
```
  'shape_signature': array([-1, -1, -1], dtype=int32),      <---------- All -1s
``` 

- However with a Dense layer, the output has the expected shape signature:
```
  'shape_signature': array([ -1,  -1, 256], dtype=int32),
```


### 5. (optional) Any other info / logs
- Output details and model summary **with a RNN / LSTM output layer**

```
[{'dtype': <class 'numpy.float32'>,
  'index': 42,
  'name': 'StatefulPartitionedCall:0',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
                              'scales': array([], dtype=float32),
                              'zero_points': array([], dtype=int32)},
  'shape': array([1, 1, 1], dtype=int32),
  'shape_signature': array([-1, -1, -1], dtype=int32),      <---------- All -1s
  'sparsity_parameters': {}}]

___________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 1, 521)]          0         
_________________________________________________________________
dense_3 (Dense)              (None, 1, 256)            133632    
_________________________________________________________________
lstm_3 (LSTM)                (None, 1, 256)            525312    
=================================================================
```

- Output details and model summary **with Dense output layer**

```
[{'dtype': <class 'numpy.float32'>,
  'index': 51,
  'name': 'StatefulPartitionedCall:0',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
                              'scales': array([], dtype=float32),
                              'zero_points': array([], dtype=int32)},
  'shape': array([  1,   1, 256], dtype=int32),
  'shape_signature': array([ -1,  -1, 256], dtype=int32),
  'sparsity_parameters': {}}]

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================

input (InputLayer)           [(None, 1, 521)]          0
_________________________________________________________________
dense (Dense)                (None, 1, 256)            133632
_________________________________________________________________
lstm (LSTM)                  (None, 1, 256)            525312
_________________________________________________________________
dense_1 (Dense)              (None, 1, 256)            65792
=================================================================
```"
47205,"""No matching distribution found for tensorflow"" when installing with pip on macOS 11","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2.1, x86-64
- TensorFlow installed from (source or binary): `pip3 install tensorflow`
- TensorFlow version: latest
- Python version: 3.8
- Installed using: pip3

**Describe the problem**

Installation with `pip` fails with:

```shell
$ pip3 install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```bash
# TensorFlow apparently does not yet support 3.9, so fall back to 3.8
brew install python@3.8
brew link --overwrite python@3.8

pip3 install tensorflow
```

**Any other info / logs**

Here is verbose output:

```shell
$ pip3 install --upgrade -v tensorflow
Using pip 20.2.4 from /usr/local/lib/python3.8/site-packages/pip (python 3.8)
Non-user install because site-packages writeable
Created temporary directory: /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-ephem-wheel-cache-v942l1u1
Created temporary directory: /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-req-tracker-vv4x81gu
Initialized build tracking at /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-req-tracker-vv4x81gu
Created build tracker: /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-req-tracker-vv4x81gu
Entered build tracker: /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-req-tracker-vv4x81gu
Created temporary directory: /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-install-wju8g2vw
1 location(s) to search for versions of tensorflow:
* https://pypi.org/simple/tensorflow/
Fetching project page and analyzing links: https://pypi.org/simple/tensorflow/
Getting page https://pypi.org/simple/tensorflow/
Found index url https://pypi.org/simple
Looking up ""https://pypi.org/simple/tensorflow/"" in the cache
Request header has ""max_age"" as 0, cache bypassed
Starting new HTTPS connection (1): pypi.org:443
https://pypi.org:443 ""GET /simple/tensorflow/ HTTP/1.1"" 304 0
  Skipping link: none of the wheel's tags match: cp27-cp27m-macosx_10_11_x86_64: https://files.pythonhosted.org/packages/90/cf/1d1e12f9f39b6a0ed1c49792ef5ce7615dddc2ce7287fc83ede0dddb9b3c/tensorflow-0.12.0rc0-cp27-cp27m-macosx_10_11_x86_64.whl#sha256=feaf06c7df5c0a480654bf1f38dd4d3b809c7315502a7d9f295033f9d2bd9b13 (from https://pypi.org/simple/tensorflow/)
... OMITTING EXTREMELY LONG LIST OF WHEELS ...
  Skipping link: none of the wheel's tags match: cp38-cp38-win_amd64: https://files.pythonhosted.org/packages/ad/fc/fccaa149d7ccc165de01d62d19e5e9492e87ad23a7106f6dfe132800ca6f/tensorflow-2.4.1-cp38-cp38-win_amd64.whl#sha256=eedcf578afde5e6e69c75d796bed41093451cd1ab54afb438760e40fb74a09de (from https://pypi.org/simple/tensorflow/)
Given no hashes to check 0 links for project 'tensorflow': discarding no candidates
Exception information:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py"", line 228, in _main
    status = self.run(options, args)
  File ""/usr/local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper
    return func(self, options, args)
  File ""/usr/local/lib/python3.8/site-packages/pip/_internal/commands/install.py"", line 323, in run
    requirement_set = resolver.resolve(
  File ""/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/legacy/resolver.py"", line 183, in resolve
    discovered_reqs.extend(self._resolve_one(requirement_set, req))
  File ""/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/legacy/resolver.py"", line 388, in _resolve_one
    abstract_dist = self._get_abstract_dist_for(req_to_install)
  File ""/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/legacy/resolver.py"", line 339, in _get_abstract_dist_for
    self._populate_link(req)
  File ""/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/legacy/resolver.py"", line 305, in _populate_link
    req.link = self._find_requirement_link(req)
  File ""/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/legacy/resolver.py"", line 270, in _find_requirement_link
    best_candidate = self.finder.find_requirement(req, upgrade)
  File ""/usr/local/lib/python3.8/site-packages/pip/_internal/index/package_finder.py"", line 928, in find_requirement
    raise DistributionNotFound(
pip._internal.exceptions.DistributionNotFound: No matching distribution found for tensorflow
Removed build tracker: '/private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-req-tracker-vv4x81gu'
```

The cause seems to be that it expects at least one wheel whose tags match my machine, but none do. My guess is that this is due to using macOS 11. All lines look like `macosx_10_*_x86_64`, but I guess it's looking for a line matching `macosx_11_*_x86_64`. (Apologies: I know nothing about the python ecosystem; this is just my educated guess.)"
47204,AttributeError: module 'tensorflow' has no attribute 'contrib',"I am learning object-detection from [this link](https://pythonprogramming.net/training-custom-objects-tensorflow-object-detection-api-tutorial/).

But I **got an error** while run command in **anaconda prompt**:

(base) R:\SEMESTER 8\SC\eye-detection\models\research\object_detection\legacy>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config
2021-02-17 15:30:15.419573: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2021-02-17 15:30:15.419775: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""train.py"", line 49, in <module>
    from object_detection.builders import dataset_builder
  File ""R:\Anaconda\anaconda\lib\site-packages\object_detection\builders\dataset_builder.py"", line 27, in <module>
    from object_detection.data_decoders import tf_example_decoder
  File ""R:\Anaconda\anaconda\lib\site-packages\object_detection\data_decoders\tf_example_decoder.py"", line 27, in <module>
    slim_example_decoder = tf.contrib.slim.tfexample_decoder
AttributeError: module 'tensorflow' has no attribute 'contrib'

### What I have tried:

- I tried to automatically upgrade script to tensorflow2 using [single python file command](https://www.tensorflow.org/guide/upgrade#single_file). But didn't work for me.
(base) R:\Anaconda\anaconda\Lib\site-packages\object_detection\data_decoders>**tf_upgrade_v2 --infile tf_example_decoder.py --outfile tf_example_decoder.py**
2021-02-17 15:14:35.610773: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2021-02-17 15:14:35.611176: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
ERROR line 27:23: Using member tf.contrib.slim.tfexample_decoder in deprecated module tf.contrib. tf.contrib.slim.tfexample_decoder cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
ERROR line 62:23: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
ERROR line 63:20: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
ERROR line 71:31: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
ERROR line 72:20: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
TensorFlow 2.0 Upgrade Script
**Converted 1 files
Detected 5 issues that require attention
File: tf_example_decoder.py**
tf_example_decoder.py:27:23: ERROR: Using member tf.contrib.slim.tfexample_decoder in deprecated module tf.contrib. tf.contrib.slim.tfexample_decoder cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
tf_example_decoder.py:62:23: ERROR: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
tf_example_decoder.py:63:20: ERROR: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
tf_example_decoder.py:71:31: ERROR: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
tf_example_decoder.py:72:20: ERROR: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
Make sure to read the detailed log 'report.txt'

- tried to install downgrade tesorflow version but not supported in python 3.9.1
- tried to run this code in cmd also but got same error.

**_Tensorflow Version: 2.4.1
python version: 3.9.1
windows: 10
conda 4.9.2_**

Can anyone know proper solution for this?"
47203,Training with MultiWorkerMirroredStrategy for machines with heterogeneous number of GPUs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04**
- TensorFlow installed from (source or binary): **From docker image tensorflow:latest-gpu**
- TensorFlow version (use command below): **2.4.1**
- Python version: **3.6.9**
- CUDA/cuDNN version: **CUDA 11.2**
- GPU model and memory: **chief (2x Quadro P2000), worker (4x RTX 2080)** 

**Describe the current behavior**
With the [keras tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras), I made the relevant changes to my own model. 

I encounter the following error when the worker runs the main.py and starts the training:

> tensorflow.python.framework.errors_impl.InternalError: Collective Op has group_size 8 and group_key 1 but that group has size 4
> Encountered` when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors.

However, the code runs the training fine when I limit the number of CUDA visible GPUs for each machine to 2 (such that worker and chief have the same number of GPUs).

**Describe the expected behavior**
I want to be able to utilise all 6 GPUs for training.

**Standalone code to reproduce the issue**
docker command for chief and worker (change index to 1 for worker):
```
docker run --gpus all -it -p 12345:12345 -v /home:/home tensorflow/tensorflow:latest-gpu
export TF_CONFIG='{""cluster"": {""worker"": [""x.x.x.x:12345"", ""x.x.x.x:12345""]}, ""task"": {""index"": 0, ""type"": ""worker""}}'
python main.py
```

**Other info / logs** "
47201,RE: Some questions,"Hi I have some questions in regards to this project:

1. How does this differ to OpenCV?
2. Am I able to also code in C++?
3. Does it support AMD OpenCL?"
47200,How can I get min/max information from TFlite Intepreter?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):2.4.1
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
I can see the min/max information by using netron open .tflite file. I need those two information to calculate radix. However, I cannot get min/max from any API provided in TFLite Intepreter. I guess to use get_tensor_details(). However I can only get the scale and zero point. Is there any way I can access min/max?

**Will this change the current api? How?**
Yes, adding one or two items in get_tensor_details()
**Who will benefit with this feature?**
Anyone that wants to calculate radix from quantized models in TFLite
**Any Other info.**
"
47198,pip Install Error: don't could find version that satisfies the requirement tensorflow,"------------------------

### System information

-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **Python version**: 3.9.1 64x

I tried to 

```bash
pip install https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz
```
or
```bash
pip install tensorflow
```
on VirtualEnv created via VSCode and I am getting the following errors:

```bash
ERROR: Could not find a version that satisfies the requirement tensorflow
ERROR: No matching distribution found for tensorflow
```
or

```bash
Collecting https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz
  Using cached https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz
    ERROR: Command errored out with exit status 1:
     command: 'c:\users\venic\onedrive\kaggle\fashion_ai\.venv\scripts\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\venic\\AppData\\Local\\Temp\\pip-req-build-72uc4pr3\\setup.py'""'""'; __file__='""'""'C:\\Users\\venic\\AppData\\Local\\Temp\\pip-req-build-72uc4pr3\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base 'C:\Users\venic\AppData\Local\Temp\pip-pip-egg-info-z0d8x7xg'
    Complete output (5 lines):
      File ""C:\Users\venic\AppData\Local\Programs\Python\Python39\lib\tokenize.py"", line 392, in open
        buffer = _builtin_open(filename, 'rb')
    FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\venic\\AppData\\Local\\Temp\\pip-req-build-72uc4pr3\\setup.py'
    ----------------------------------------
```

Pip version = 21.0.1
"
47197,Multi-thread support for TF2 ParameterServerStrategy,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
After building a sample model based on tutorial(https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy), the performance is way less compare to TF 1 PS strategy(multi client version) and checking the coordinator resource utilization, it turns out that only single core is heavily utilized in a multi-core machine for coordinator. It seems that even though in workers are maintained in separate thread(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/coordinator/cluster_coordinator.py#L742), the actual executions are still sequential. 

Is this caused by GIL? what's the recommended approach to improve the performance?

**Describe the expected behavior**
Multi-thread / multi-core support for ParameterServerStrategy coordinator.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
N/A
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47196,I have problem with python code in google colab,"I write this code:
basemodel.fit(X_train,y_train,epochs=20,validation_split=.1,callbacks=call_back)
but don't work this error:
Epoch 1/20
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-34-337a90849ebd> in <module>()
----> 1 basemodel.fit(X_train,y_train,epochs=20,validation_split=0.1,callbacks=call_back)

9 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, ""ag_error_metadata""):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise"
47195,Error while starting the training for custom dataset using Tensor flow object detection API,"Trying to train a model with custom dataset using the link :
https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html
this Object detection API. 
I followed all the steps mentioned in the documentation and the training should have been stated after running the command:
>python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config
but instead this error is coming up. 
Please help 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip installed
- TensorFlow version (use command below): 2.4
- Python version: 3.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 11.0.2, CUDnn 8.0.5
- GPU model and memory: Nvidia Quadro T2000


Error log from cmd: 


(tensorflow) C:\Saurav\Work\SESA\YardHealthStick\tensorflow\workspace\training_demo>python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config
2021-02-16 21:41:58.726218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-02-16 21:42:03.204172: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-16 21:42:03.208849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-02-16 21:42:03.255184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro T2000 computeCapability: 7.5
coreClock: 1.785GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s
2021-02-16 21:42:03.262012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-02-16 21:42:03.275486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-02-16 21:42:03.279686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-02-16 21:42:03.287813: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-02-16 21:42:03.294632: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-02-16 21:42:03.312945: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-02-16 21:42:03.322076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-02-16 21:42:03.328164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-02-16 21:42:03.330916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-02-16 21:42:03.333579: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-02-16 21:42:03.346264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro T2000 computeCapability: 7.5
coreClock: 1.785GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s
2021-02-16 21:42:03.353121: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-02-16 21:42:03.356164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-02-16 21:42:03.358468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-02-16 21:42:03.361200: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-02-16 21:42:03.365114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-02-16 21:42:03.367316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-02-16 21:42:03.370458: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-02-16 21:42:03.373566: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-02-16 21:42:03.376127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-02-16 21:42:04.480554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-16 21:42:04.484849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-02-16 21:42:04.487720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-02-16 21:42:04.490792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2903 MB memory) -> physical GPU (device: 0, name: Quadro T2000, pci bus id: 0000:01:00.0, compute capability: 7.5)
2021-02-16 21:42:04.499549: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I0216 21:42:04.510176 15720 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
INFO:tensorflow:Maybe overwriting train_steps: None
I0216 21:42:04.516160 15720 config_util.py:552] Maybe overwriting train_steps: None
INFO:tensorflow:Maybe overwriting use_bfloat16: False
I0216 21:42:04.517184 15720 config_util.py:552] Maybe overwriting use_bfloat16: False
WARNING:tensorflow:From C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py:530: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
W0216 21:42:04.714892 15720 deprecation.py:333] From C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py:530: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
INFO:tensorflow:Reading unweighted datasets: ['annotations/train.record']
I0216 21:42:04.721902 15720 dataset_builder.py:163] Reading unweighted datasets: ['annotations/train.record']
INFO:tensorflow:Reading record datasets for input file: ['annotations/train.record']
I0216 21:42:04.723216 15720 dataset_builder.py:80] Reading record datasets for input file: ['annotations/train.record']
INFO:tensorflow:Number of filenames to read: 1
I0216 21:42:04.723867 15720 dataset_builder.py:81] Number of filenames to read: 1
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W0216 21:42:04.724863 15720 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.
W0216 21:42:04.730878 15720 deprecation.py:333] From C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.
WARNING:tensorflow:From C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W0216 21:42:04.758770 15720 deprecation.py:333] From C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
Traceback (most recent call last):
  File ""model_main_tf2.py"", line 113, in <module>
    tf.compat.v1.app.run()
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\absl\app.py"", line 300, in run
    _run_main(main, args)
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""model_main_tf2.py"", line 104, in main
    model_lib_v2.train_loop(
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py"", line 530, in train_loop
    train_input = strategy.experimental_distribute_datasets_from_function(
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 340, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1143, in experimental_distribute_datasets_from_function
    return self.distribute_datasets_from_function(dataset_fn, options)
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1134, in distribute_datasets_from_function
    return self._extended._distribute_datasets_from_function(  # pylint: disable=protected-access
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\mirrored_strategy.py"", line 545, in _distribute_datasets_from_function
    return input_lib.get_distributed_datasets_from_function(
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 161, in get_distributed_datasets_from_function
    return DistributedDatasetsFromFunction(dataset_fn, input_workers,
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1272, in __init__
    _create_datasets_from_function_with_input_context(
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1936, in _create_datasets_from_function_with_input_context
    dataset = dataset_fn(ctx)
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py"", line 521, in train_dataset_fn
    train_input = inputs.train_input(
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\inputs.py"", line 893, in train_input
    dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py"", line 251, in build
    dataset = dataset_map_fn(dataset, decoder.decode, batch_size,
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py"", line 236, in dataset_map_fn
    dataset = dataset.map_with_legacy_function(
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 340, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2679, in map_with_legacy_function
    ParallelMapDataset(
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 4242, in __init__
    self._map_func = StructuredFunctionWrapper(
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3493, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\function.py"", line 546, in add_to_graph
    self._create_definition_if_needed()
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\function.py"", line 378, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\function.py"", line 400, in _create_definition_if_needed_impl
    temp_graph = func_graph_from_py_func(
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\function.py"", line 971, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3485, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3453, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
  File ""C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 670, in wrapper
    raise e.ag_error_metadata.to_exception(e)
NotImplementedError: in user code:

    C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\object_detection\data_decoders\tf_example_decoder.py:524 default_groundtruth_weights  *
        [tf.shape(tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]],
    C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:201 wrapper  **
        return target(*args, **kwargs)
    C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\array_ops.py:3120 ones
        output = _constant_if_small(one, shape, dtype, name)
    C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\array_ops.py:2804 _constant_if_small
        if np.prod(shape) < 1000:
    <__array_function__ internals>:5 prod

    C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\numpy\core\fromnumeric.py:3030 prod
        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
    C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\numpy\core\fromnumeric.py:87 _wrapreduction
        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
    C:\Users\ytl2tnk\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py:852 __array__
        raise NotImplementedError(

    NotImplementedError: Cannot convert a symbolic Tensor (cond_2/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported

"
47194,Bug related to Martin Grnes Attend Tutorial,"I know that TF 1 is no longer supported but if I test this notebook related to Martin Grner Attention Based Classification Tutorial: https://github.com/conversationai/conversationai-models/blob/master/attention-tutorial/Attention_Model_Tutorial.ipynb

And add the following lines to the bi_rnn_model Function after the encoding was calculated:

Original Code
```
encoding, alphas = attend(outputs, 
                            hparams['attention_size'], 
                            hparams['attention_depth'])
```
To add code.

```
  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    
    xev = encoding.eval()

```
The eval Method will hang forever. This happens in session.py:

```
def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,
                          run_metadata):
    return tf_session.TF_SessionRun_wrapper(
        self._session, options, feed_dict, fetch_list, target_list,
        run_metadata)

```


Am I doing it completely wrong or is it an old TF1 Bug???

Kind regards,

Dirk"
47193,Non exact-zero value for fake quantizer ops,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.5
- CUDA/cuDNN version: 11.0 / 8.1.0
- GPU model and memory: Tesla T4 and GeForce GTX 1650

**Describe the current behavior**

The [tf.quantization.fake_quant_with_min_max_vars](https://www.tensorflow.org/api_docs/python/tf/quantization/fake_quant_with_min_max_vars) (and similar) does not include an exact zero result. Depending on the `min` and `max` parameters these values are actually slightly above zero or below zero, e.g. -0.0000002 or 0.0000006.

**Describe the expected behavior**

An exact zero should be returned and it should not be dependent on the `min` and `max` parameters. Or at least, this was the behaviour in pre-TF 2.4.0 tensorflow versions on the CPU. It is also mentioned in [this article in section 2.1](https://arxiv.org/abs/1712.05877).

**Standalone code to reproduce the issue**

The simplest snippet to reproduce the issue is:
```python
print(tf.quantization.fake_quant_with_min_max_vars(0.0, -25, 20, num_bits=8))
```
with TF 2.4.1 on (CPU or GPU) I get:
```python
tf.Tensor(-3.874302e-07, shape=(), dtype=float32)
```
with TF 2.3.2 on the CPU I get:
```python
tf.Tensor(0.0, shape=(), dtype=float32)
```
and with TF 2.3.2 on the GPU I get:
```python
tf.Tensor(-3.874302e-07, shape=(), dtype=float32)
```

A more realistic example would look something like this:
```python
data = tf.random.normal(shape=(4096,), mean=0.0, stddev=1.0)
result = tf.quantization.fake_quant_with_min_max_vars(data, -21, 23, num_bits=8).numpy()
print([result[index] for index in range(result.shape[0]) if -1e-4 < result[index] < 1e-4][0])
```

Colab link: https://colab.research.google.com/drive/1swMaNOZpQp1psllRguWH13YHYOpf2Kqa?usp=sharing

**Other info / logs**

I've tested this in various settings on various machines, and it seems to work in certain scenarios:

| platform | TF 2.2.x | TF 2.3.x | TF 2.4.x |
|------------|-------------|------------|-------------|
| Mac      |   |   |  |
| Ubuntu 20.04 CPU |  |  |  |
| Ubuntu 20.04 GPU |  |  |  | 

Here  means it is not exact zero, and  means it is exact zero.

It also seems to work if the function is enclosed in a `tf.function` and XLA is enabled by passing in `experimental_compile=True`.

I could not find any recent commits to the relevant files:
* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fake_quant_ops.cc
* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fake_quant_ops_gpu.cu.cc
* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fake_quant_ops_functor.h

So, most likely the issue is in one of the TF utility functions or in an external library (e.g. Eigen) or in the way the TensorFlow wheels are compiled."
47192,Bidirection Masked LSTM breaks in graph mode.,"tf version: 2.4.1
python: 3.6

Using the Bidirectional layer is returning the following error in graph mode
```
ValueError: Shape must be rank 1 but is rank 2 for '{{node cond/ReverseSequence}} = ReverseSequence[T=DT_FLOAT, Tlen=DT_INT32, batch_dim=0, seq_dim=1](cond/ReverseSequence/inputs, cond/Sum)' with input shapes: [?,?,1], [?,1].
```
To reproduce:
```
import tensorflow as tf


class Bad(tf.keras.models.Model):
    def __init__(self):
        super(Bad, self).__init__()
        # self.rec_net = tf.keras.layers.LSTM(10)
        self.rec_net = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10))
    
    @tf.function(input_signature=
                 [tf.TensorSpec(shape=(None, None, 1), dtype=tf.float32),
                  tf.TensorSpec(shape=(None, None, 1), dtype=tf.int32), ])
    def call(self, x, mask):
        return self.rec_net(x, mask=tf.cast(mask, tf.bool), training=False)
        # return self.rec_net(x, training=False)


if __name__ == '__main__':
    inp = tf.random.uniform((3, 4, 1))
    mask = tf.convert_to_tensor([[[1], [1], [1], [1]],
                                 [[1], [1], [1], [0]],
                                 [[1], [1], [0], [0]]])
    
    b = Bad()
    out = b(inp, mask)
```

It works if any of tf.function, bidirectional or mask is removed.
Also works if a GRU is used."
47191,CUDA devices are not recognized,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.3.0
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: Anaconda 4.8.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1.243/7.6.5
- GPU model and memory: NVIDIA Quadro P2000 4GB

**Describe the problem**
- The list of available GPU is empty!
- TensorFlow runs only on CPU

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`conda env create -f tf-gpu-test.yaml`
`conda activate tf-gpu-test`
`python tf-gpu-test.py`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Files: [tf-gpu-test.zip](https://github.com/tensorflow/tensorflow/files/5989857/tf-gpu-test.zip)
CUDA: []
CPU: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
47188,Error with running tensorflow after installation,"I just installed tensorflow 2.4.1.  I use python 3.6 and there were no problem when I installed tensorflow. But the error comes up exactly after importing Tensorflow when I run:

import tensorflow as tf
I will get:

ITraceback (most recent call last):
  File ""C:\Users\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/MultiCNN.py"", line 9, in <module>
    import tensorflow as tf
  File ""C:\Users\Python36\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Python36\site-packages\tensorflow\python\__init__.py"", line 39, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.

Failed to load the native TensorFlow runtime.



"
47187,tf.keras.layers.Embedding updating embeddings of nearby indices even though they were not updated in the code,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.9


I am using tf.keras.layers.embedding on a dictionary size of 20 million. For indices greater than 16777216, updates happening in one embedding gets reflected in some other embedding .i.e I am getting same final embeddings for indices above 16777216 in groups of 3.
eg. final embeddings for 16777216, 16777217, 16777218 are equal, 16777220, 16777221, 16777222 are equal and so on.

Somehow higher indices are getting mapped to same entry in the look up table, or may be some hash collision.

Simplified reproducible code can be found [here.](https://colab.research.google.com/drive/1m-zz4mlPMqOZ_fpz9GpwHTllqH3uqM4g?usp=sharing)

<img width=""1422"" alt=""Screenshot 2021-02-16 at 8 36 26 PM"" src=""https://user-images.githubusercontent.com/69950926/108081744-4d0cf700-7097-11eb-847c-b93e7897e089.png"">


Also, on a side note, this behaviour is not observed in tf version 2.1.0, embedding layer works fine there.
Any help will be really appreciated, as this is resulting in serious performance issue.
"
47186,Add tf.nn.log_gaussian_loss,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Implement the Gaussian negative log-likelihood loss for the tf.nn class, similar to the [log poisson loss](https://www.tensorflow.org/api_docs/python/tf/nn/log_poisson_loss). See also the [PyTorch version](https://pytorch.org/docs/master/generated/torch.nn.GaussianNLLLoss.html). 

**Will this change the current api? How?**
tf.nn class will have the log_gaussian_loss function, which could look like:
`tf.nn.log_gaussian_loss(
    targets, inputs, variances, compute_full_loss=False, name=None
)`
to keep in line with the `log_poisson_loss` implementation.

**Who will benefit with this feature?**
This loss function is used extensively in Bayesian deep ensembles, see for example [Lakshminarayanan et al. 2017](https://scholar.google.co.uk/citations?user=QYn8RbgAAAAJ&hl=en&oi=sra#d=gs_md_cita-d&u=%2Fcitations%3Fview_op%3Dview_citation%26hl%3Den%26user%3DQYn8RbgAAAAJ%26citation_for_view%3DQYn8RbgAAAAJ%3AhCrLmN-GePgC%26tzom%3D0), though the loss function has been around since the 90s (Mean and Variance Estimation, [Nix et al. 1994](https://ieeexplore.ieee.org/document/374138)).

**Any Other info.**
"
47185,Cannot import Tensorflow Lite model with an LSTM layer to Android Studio,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, Google Colaboratory
- TensorFlow installed from (source or binary): Bundled with Google Colaboratory
- TensorFlow version (use command below): 2.5.0 (latest, installed from nightly, but also occurs on the latest stable version 2.4)
- Python version: 3.6 (Bundled with Google Colaboratory)

**Describe the current behavior**
I have created a Tensorflow Lite model using[ the official guide](https://www.tensorflow.org/lite/convert/index#convert_a_keras_model_), and then tried to import it to Android Studio using [another official guide](https://www.tensorflow.org/lite/guide/android#use_android_studio_ml_model_binding). However, Android Studio will not make the ""Next"" button available, while displaying an error saying ""This is not a valid Tensorflow Lite model file"".

**Describe the expected behavior**
The model should have been imported, as it follows your own official guides.

**Standalone code to reproduce the issue**

1. Create the minimal model I have been using the following Google Colaboratory notebook: https://colab.research.google.com/drive/1VAsXLGHHF4tYxJbq4VtjkccdkVm09hkH?usp=sharing
2. Download the resulting file to your local computer
3. In any Android Studio project, right-click on any module and select New => Other => Tensorflow Lite model
4. Choose the file you downloaded in Step 2
5. The error message will be displayed. (This can be seen in the attached image)

**Other info / logs** 
I opened the same bug with Android Studio in the following [Link](https://issuetracker.google.com/issues/180311612). You can see the Android Studio details and screenshots of the bug in the link.

Also note that removing the LSTM layer causes Android Studio to recognize the model as valid. It's the inclusion of the LSTM layer that causes the problem."
47184,Conv2D operator not XLA compliant with Windows 10,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: conda 3.8.5
- CUDA/cuDNN version: 11.0
- GPU model and memory: RTX 2080 super

**Describe the current behavior**
The following error is raised, either the code is run on CPU or GPU:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\eager\def_function.py"", line 894, in _call
    return self._concrete_stateful_fn._call_flat(
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\eager\function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\eager\function.py"", line 555, in call
    outputs = execute.execute(
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_test_9}} = __inference_test_9[_XlaMustCompile=true, config_proto=""\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0012\005*\0010J\0008\001\202\001\000"", executor_type=""""](dummy_input, dummy_input).
Uncompilable nodes:
Conv2D: unsupported op: No registered 'Conv2D' OpKernel for XLA_GPU_JIT devices compatible with node {{node Conv2D}}
        Stacktrace:
                Node: __inference_test_9, function:
                Node: Conv2D, function: __inference_test_9
 [Op:__inference_test_9]
```

**Describe the expected behavior**
I'm expecting to have the `Conv2D` op compliant with XLA on GPU and CPU as stated in the TF documentation https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/g3doc/gpu_supported_ops.md and get the following result:
```
<tf.Tensor: shape=(1, 4, 4, 2), dtype=float32, numpy=
array([[[[10.        ,  1.8999999 ],
         [ 9.999999  ,  2.1999998 ],
         [ 5.999999  ,  1.5999999 ],
         [ 6.        ,  2.        ]],

        [[11.999998  ,  1.3999999 ],
         [14.999998  ,  2.1999998 ],
         [12.999999  ,  2.6999998 ],
         [12.999998  ,  1.7       ]],

        [[ 6.999999  ,  1.6999998 ],
         [10.999998  ,  1.3       ],
         [15.999998  ,  1.3       ],
         [ 7.        ,  1.        ]],

        [[10.        ,  0.59999996],
         [ 7.        ,  1.3999999 ],
         [ 4.        ,  1.5       ],
         [ 6.9999995 ,  1.4000001 ]]]], dtype=float32)>
```

**Standalone code to reproduce the issue**
Here a piece of code to reproduce the issue:
```python
import tensorflow as tf
import numpy as np

x_in = np.array([[
  [[2], [1], [2], [0], [1]],
  [[1], [3], [2], [2], [3]],
  [[1], [1], [3], [3], [0]],
  [[2], [2], [0], [1], [1]],
  [[0], [0], [3], [1], [2]], ]])

kernel_in = np.array([
 [ [[2, 0.1]], [[3, 0.2]] ],
 [ [[0, 0.3]],[[1, 0.4]] ], ])

x = tf.constant(x_in, dtype=tf.float32)
kernel = tf.constant(kernel_in, dtype=tf.float32)

@tf.function(experimental_compile=True)
def run_xla():
  return tf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding='VALID')

run_xla()
```

This issue appears only on Windows, but not on Linux, then it might be an OS related issue."
47183,"Crash when creating Dataset from RaggedTensor, parallel function calls","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, see in colab below

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Docker image tensorflow/tensorflow:2.4.1-gpu-jupyter

- TensorFlow installed from (source or binary):
binary

- TensorFlow version (use command below):
2.4.1

- Python version:
3.6.9

- GPU model and memory:
Tesla V100 with 16GB

**Describe the current behavior**

Running the script in the colab below crashes:
- at random times (but typically soon after launching the script)
- **most often with a segfault without further info**, sometimes with a variety of error such as
  - tensorflow.python.framework.errors_impl.InvalidArgumentError: PartialTensorShape: Incompatible ranks during merge: 1 vs. 0
         [[{{node TensorListConcatV2}}]] [Op:IteratorGetNext]
  - F tensorflow/core/framework/tensor_shape.cc:466] Check failed: end <= dims() (1 vs. 0)
  - invalid fastbin entry (free)
  - InvalidArgumentError: Trying to concat list with only uninitialized tensors but element_shape_except_first_dim_ is not fully defined: []
	 [[{{node TensorListConcatV2}}]]

**Describe the expected behavior**

No crashes, the expected output is 
2
0
2
0
...

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1TA2kXxM5mQXDKI5-mcY28IuTl4zO4Syv?usp=sharing

**Other info**
The problem does not occur if
- **the num_parallel_calls is set to 1**
and/or
- the ragged tensor is changed to an ordinary tensor
and/or
- the tf.fill is changed to a constant tensor
"
47182,TF repo preprocessing for EfficientNet differs from that contained in Keras repo.,"Hi all,

I've noticed that the code for EfficientNet image preprocessing contained [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/efficientnet.py#L530) is different from the one contained [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/efficientnet.py#L740).
In the first link (from keras-team) standard ImageNet preprocessing is performed, while in the second link (from tensorflow) nothing is done.

Which version of the code should I trust for reusing the EfficientNet checkpoints publicly released by Google?

Thanks for your time!"
47179,unique operation on strings returns bogus indices,"**System information**
- Have I written custom code: No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.9
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.8.
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1.243
- GPU model and memory: None

**Describe the current behavior**

Executing the test that runs `unique` on strings/chars returns invalid values for the index tensor. Some values are plain out of bounds (very big or negative) and others seem to point to wrong values. This very much looks like some kind of memory corruption.
So far this has only been reproducible on a cascade lake CPU, no GPUs in the system but TF was built with CUDA support.

**Describe the expected behavior**

Valid values returned

**Standalone code to reproduce the issue**

```
    import numpy as np
    from tensorflow.python.ops import array_ops

    def testString():
        indx = np.random.randint(65, high=122, size=7000)
        x = [chr(i) for i in indx]
        y, idx = array_ops.unique(x)
        tf_y, tf_idx = (y.numpy(), idx.numpy())
        print(','.join(str(i) for i in tf_idx))

        assert len(x) == len(tf_idx)
        assert len(tf_y) == len(np.unique(x))

        for i in range(len(x)):
            assert x[i] == tf_y[tf_idx[i]].decode('ascii')

    testString()
```
This test was extracted from the TF test suite as-is.
Errors include the last assert failing as well as errors like `IndexError: index 7566446 is out of bounds for axis 0 with size 57`

**Other info / logs**

For a part of the log from the TF test suite see https://gist.github.com/boegel/26f768c82080e593add3924fc7bc76cf
The `print` of `tf_idx` shows things like:
`-15829468,126746879,293189,50530320,291592,3,-16616924,58720256,66085632,66085120,1,0,7352690,946952,1538050,51149349,10,126746628,11,10,7,33583628,-16551388,126747138,126746628,50491941,101480976,126746879,16742418,14,20,752147,19,134209791,126749222,17,126747141,3840,16,2,33585676,18,387021328,70664,219117604,486963728,27,66343024,50530320,24,50756133,27,126762239,4096,7,-16223708,126749187,33977217,20,9,4,293189,20,4,40,24,23,8,13,-16025575,31,26,2375,40,22,126749187,8,19,36,6,26,126746627,22,36,133292287,133292287,44,21,34,36,13,31,26,13,24,43,10,-16354780,31,41,419459596,38,39,8,1,43,2,44,1,4,11,2,45,7,22,41,29,6,18,39,126746630,5,8,3,1,26,14,40,36,32,8,988163,25,7,42,18,3,41,50,32,2,8,22,22,17,39,18,5,21,30,14,22,34,43,43,48,6,24,50,20,32,24,29,35,8,40,9,16,18,46,50,18,7,13,31,37,47,12,2,27,39,6,37,38,44,45,29,16,0,2,31,1,29,8,26,9,30,32,66084976,29,29,29,24,15,39,35,49,37,30,0,27,39,386169362,9,41,4,7,126749211,5,24,46,4,0,49 [...]` (remaining values look ok)


To stress that: This seems to be highly system dependent. We compile with `-march=native` and see it only on the cascade lake system. The string test is the only one that fails, all other unique tests succeed.
This sounds to me like some case of undefined behavior where an optimization corrupts the absl hashmap used."
47178,'An error ocurred while starting the kernel' while training on gpu in Spyder!,"I want to train a Bidirectional LSTM model on my GPU but this is what I get after 2-3 epochs:
`An error ocurred while starting the kernel
2021 09:06:13.678667: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021 09:06:16.489943: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021 09:06:16.492166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021 09:06:17.272286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro M2000M computeCapability: 5.0
coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s
2021 09:06:17.274418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021 09:06:17.308431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021 09:06:17.308767: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021 09:06:17.327392: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021 09:06:17.331839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021 09:06:17.345486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021 09:06:17.363282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021 09:06:17.364864: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021 09:06:17.365316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021 09:06:48.969791: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations: AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021 09:06:49.127162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro M2000M computeCapability: 5.0
coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s
2021 09:06:49.127794: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021 09:06:49.128172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021 09:06:49.128483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021 09:06:49.128781: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021 09:06:49.129096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021 09:06:49.129508: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021 09:06:49.130288: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021 09:06:49.130789: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021 09:06:49.131620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021 09:06:49.885459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021 09:06:49.885798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] 0
2021 09:06:49.885996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: N
2021 09:06:49.886418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 3023 MB memory) > physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2021 09:06:49.887589: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021 09:07:13.591865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro M2000M computeCapability: 5.0
coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s
2021 09:07:13.592482: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021 09:07:13.592765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021 09:07:13.593045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021 09:07:13.593372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021 09:07:13.593648: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021 09:07:13.593932: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021 09:07:13.594485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021 09:07:13.594999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021 09:07:13.595489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021 09:07:13.596239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021 09:07:13.596684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] 0
2021 09:07:13.597243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: N
2021 09:07:13.598111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3023 MB memory) > physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2021 09:07:13.598930: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021 09:07:14.536186: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021 09:07:20.533796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021 09:07:20.874353: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021 09:08:09.760699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro M2000M computeCapability: 5.0
coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s
2021 09:08:09.761518: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021 09:08:09.761935: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021 09:08:09.762304: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021 09:08:09.762632: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021 09:08:09.762953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021 09:08:09.763477: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021 09:08:09.763869: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021 09:08:09.764196: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021 09:08:09.764560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021 09:08:09.764888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021 09:08:09.765192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] 0
2021 09:08:09.765382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: N
2021 09:08:09.765714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 3023 MB memory) > physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2021 09:08:09.766262: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021 09:12:27.232789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro M2000M computeCapability: 5.0
coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s
2021 09:12:27.233388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021 09:12:27.234009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021 09:12:27.234656: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021 09:12:27.235187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021 09:12:27.235952: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021 09:12:27.236577: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021 09:12:27.237076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021 09:12:27.237535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021 09:12:27.238326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021 09:12:27.239011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021 09:12:27.239505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] 0
2021 09:12:27.239812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: N
2021 09:12:27.240130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 3023 MB memory) > physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2021 09:12:27.240670: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021 09:18:44.349103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro M2000M computeCapability: 5.0
coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s
2021 09:18:44.349690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021 09:18:44.349989: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021 09:18:44.350288: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021 09:18:44.350653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021 09:18:44.350996: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021 09:18:44.351675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021 09:18:44.352405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021 09:18:44.353289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021 09:18:44.354195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021 09:18:44.355185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021 09:18:44.356032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] 0
2021 09:18:44.356764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: N
2021 09:18:44.357599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 3023 MB memory) > physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2021 09:18:44.358700: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021 09:19:11.594693: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTOmode, and switching to DATAbased sharding, instead of FILEbased sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: ""FlatMapDataset/_9""
op: ""FlatMapDataset""
input: ""PrefetchDataset/_8""
attr {
key: ""Targuments""
value {
list {
}
}
}
attr {
key: ""f""
value {
func {
name: ""__inference_Dataset_flat_map_slice_batch_indices_20559""
}
}
}
attr {
key: ""output_shapes""
value {
list {
shape {
dim {
size: 
}
}
}
}
}
attr {
key: ""output_types""
value {
list {
type: DT_INT64
}
}
}
. Consider either turning off autosharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2021 10:01:56.865667: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTOmode, and switching to DATAbased sharding, instead of FILEbased sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: ""FlatMapDataset/_9""
op: ""FlatMapDataset""
input: ""PrefetchDataset/_8""
attr {
key: ""Targuments""
value {
list {
}
}
}
attr {
key: ""f""
value {
func {
name: ""__inference_Dataset_flat_map_slice_batch_indices_27929""
}
}
}
attr {
key: ""output_shapes""
value {
list {
shape {
dim {
size: 
}
}
}
}
}
attr {
key: ""output_types""
value {
list {
type: DT_INT64
}
}
}
. Consider either turning off autosharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2021 10:02:00.920677: F tensorflow/core/kernels/split_lib_gpu.cu.cc:205] NonOKstatus: GpuLaunchKernel(SplitOpKernel, config.block_count, config.thread_per_block, 0, d.stream(), input, prefix_dim_size, split_dim_size, suffix_dim_size, output_ptr_data) status: Internal: unspecified launch failure
`
GPU model: Quadro M2000M
Tensorflow 2.4.1
Windows 10
CUDA 11
cudNN 8.6"
47177,tensorflow_model_server  taking a lot of time in deploying model,"Command is running from hours 
nohup tensorflow_model_server \
  --rest_api_port=9090 \
  --model_name=seman_model \
  --model_base_path=""${Model_Directory}"" >server1.log 2>&1"
47176,Need to get the version info from the model,"I have downloaded / created tfLite Model.

How do i know the version of the tfLite Model from just the flatbuffer model(*.tflite ) file..
Can i fetch the version information from this model.

Another query - we have schema_generated.h file in every tfLite release. (tensorflow/lite/schema)
I could not see any version information in this file. 

Can i fetch the version of tfLite from this file ."
47175,"Model benchmark binary throws ""unconsumed cmdline flag: --graph"" error","**System information**
- Windows 10
- ADB
- Binary downloaded from [here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model_performance_options)

**Describe the current behavior**
ADB Shell input and output:
```
CPH1920:/data/local/tmp $ ./android_aarch64_benchmark_model_performance_options --graph test
STARTING!
Unconsumed cmdline flags: --graph test
WARNING: unrecognized commandline flag: --graph
WARNING: unrecognized commandline flag: test
The list of TFLite runtime options to be benchmarked: [all]
Please specify the name of your TF Lite input file with --graph
Please specify the name of your TF Lite input file with --graph
Please specify the name of your TF Lite input file with --graph
Please specify the name of your TF Lite input file with --graph
Please specify the name of your TF Lite input file with --graph
Please specify the name of your TF Lite input file with --graph
Please specify the name of your TF Lite input file with --graph
Please specify the name of your TF Lite input file with --graph
Please specify the name of your TF Lite input file with --graph
Please specify the name of your TF Lite input file with --graph

==============Summary of All Runs w/ Different Performance Options==============
                  gpu-fp16:  failed!
          cpu w/ 1 threads:  failed!
cpu w/ 2 threads (xnnpack):  failed!
cpu w/ 4 threads (xnnpack):  failed!
               gpu-default:  failed!
     nnapi(w/o accel name):  failed!
          cpu w/ 4 threads:  failed!
cpu w/ 1 threads (xnnpack):  failed!
          cpu w/ 2 threads:  failed!
            dsp w/ hexagon:  failed!
```
**Describe the expected behavior**
Should not be asking for `--graph` flag if it is already provided."
47174,EfficientNet models from TensorFlow.Keras not being reproducible on GPU,"After downloading an EfficientNet model from [tensorflow.keras.applications.efficientnet][1], and retraining it on our own data, I've noticed that the results are not reproducible. The results are reproducible for other architectures like *VGG16*, *ResNet101*, *InceptionV3*, and *InceptionResNetV2*, but not for any of the *EfficientNetBx* models.

Please note that I've set all the following seeds, and even have tensorflow-determinism:

    random.seed(1)
    np.random.seed(1)
    tf.random.set_seed(1)
    os.environ['TF_CUDNN_DETERMINISTIC'] = TRUE
    os.environ['TF_DETERMINISTIC_OPS'] = TRUE

TensorFlow Version:  tensorflow-gpu==2.3

  [1]: https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet"
47173,`ValueError: None values not supported` when using the Estimator API,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 33
-   **TensorFlow installed from (source or binary)**: Attempted both
-   **TensorFlow version (use command below)**: Attempted v2.4.1-0-g85c8b2a817f 2.4.1 (source) and v2.4.0-49-g85c8b2a817f 2.4.1 (binary)
-   **Python version**: Python 3.9.1 and Python 3.7.7 :: Intel(R) Corporation
-   **Bazel version (if compiling from source)**: Used Bazelisk, which listed 3.1.0 as the version it's using
-   **GCC/Compiler version (if compiling from source)**: gcc (GCC) 10.2.1 20201125 (Red Hat 10.2.1-9)
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**: Attached code snippet

### Describe the problem
I have been attempting to use TensorFlow to build a random forest classifier, in this case using the built-in boosted trees classifier. I have attempted two different datasets, but the same general idea in the code. After these attempts, I'm thinking I might be hitting a bug of some sort
The code errors out with: `ValueError: None values not supported.` in a way that's extremely unclear what's going on

Test1.py:
```
Traceback (most recent call last):
  File ""/home/user/tensorflow-random-forest/test.py"", line 38, in <module>
    estimator.train(input_fn=create_input_fn('train'), max_steps=100)
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1203, in _train_model_default
    estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1163, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py"", line 2081, in _model_fn
    return _bt_model_fn(
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py"", line 1259, in _bt_model_fn
    batch_size = tf.compat.v1.shape(labels)[0]
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 649, in shape
    return shape_internal(input, name, optimize=True, out_type=out_type)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 673, in shape_internal
    input = ops.convert_to_tensor(input)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/profiler/trace.py"", line 163, in wrapped
    return func(*args, **kwargs)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 1540, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 339, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 281, in _constant_impl
    tensor_util.make_tensor_proto(
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/tensor_util.py"", line 445, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.
```

Test2.py:
```
Traceback (most recent call last):
  File ""/home/user/tensorflow-random-forest/test2.py"", line 61, in <module>
    estimator.train(input_fn=create_input_fn(train=True))
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1203, in _train_model_default
    estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1163, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py"", line 2233, in _model_fn
    return _bt_model_fn(
  File ""/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py"", line 1222, in _bt_model_fn
    batch_size = tf.compat.v1.shape(labels)[0]
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 649, in shape
    return shape_internal(input, name, optimize=True, out_type=out_type)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 673, in shape_internal
    input = ops.convert_to_tensor(input)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/profiler/trace.py"", line 163, in wrapped
    return func(*args, **kwargs)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 1540, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 339, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 281, in _constant_impl
    tensor_util.make_tensor_proto(
  File ""/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/tensor_util.py"", line 445, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.
```



### Source code / logs
These two snippets produce the same type of error, it's possible I'm making some fundamental coding mistake, but I'm not seeing how.
(github wanted me to change the .py to .txt to attach them, just remove to run)
[test2.py.txt](https://github.com/tensorflow/tensorflow/files/5984443/test2.py.txt)
[test.py.txt](https://github.com/tensorflow/tensorflow/files/5984444/test.py.txt)
"
47172,How to compute gradient of output wrt input in Tensorflow 2.0 when there are multi-labels?,"In my case, the input shape is (None, 16) (sample size, feature size), and my output is (None, 3). It means that my DNN is a mapping from input space to 3-D output space. In that case how can I use the tape to calculate the gradients of output w.r.t input?
Here is my code:
```
import tensorflow as tf
x_grad = tf.Variable(gpi_input, dtype=tf.float32)
with tf.GradientTape(persistent=True) as tape:
    pred = best_model(x_grad)
grads = tape.gradient(pred, x_grad)
```
where ```pred``` is of [None, 3] and ```x_grad``` is of [None, 16]. But what I got is a tensor of shape [None, 16] which means ```grads``` is of shape [None, 16]. But I want to know the gradients of each output w.r.t to each input. So what should I do?


The ```best_model``` has been already trained and we cannot retrain a model from input to first label, the second label and the third label. So how can I get gradients of every label given the model I have in my hand?

Thanks a lot! "
47171,Slow as_numpy_iterator() and StringLookup.adapt() with make_csv_dataset(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, attached below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 under WSL 2 and Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Doesn't apply.
- TensorFlow installed from (source or binary): Binary.
- TensorFlow version (use command below): v1.12.1-50923-g01fe322bbb5 2.5.0-dev20210215 (also tried 2.4.1 via pip and colab)
- Python version: 3.8.6
- Bazel version (if compiling from source): Doesn't apply.
- GCC/Compiler version (if compiling from source): Doesn't apply.
- CUDA/cuDNN version: Doesn't apply.
- GPU model and memory: Doesn't apply. CPU only.


**Describe the current behavior**
Using the dataset API to read CSV data (via `list(data.as_numpy_iterator())`) takes around 4-5x longer compared to regular pandas' `read_csv()`.
Running a `StringLookup.adapt(...)` is also slow. (Made even worse when multiple adapt()s need to run?) But can't be easily compared to pandas' unique() since the data is in memory by then.
Setting `num_parallel_reads` didn't really make a difference.

The used dataset has 678014 lines with 30MB worth of data.

**Describe the expected behavior**
Being near pandas read performance would be nice or faster via num_parallel_reads. ;-) This should make `adapt()` fast as well, hopefully.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1wySl43lLZ5P3ntjpD3TmYn0ZcYrTYve-?usp=sharing

```python
import tensorflow as tf
import pandas as pd
import logging
from pathlib import Path

FORMAT = '%(asctime)-15s %(message)s'
logging.basicConfig(format=FORMAT, level = logging.INFO)

path = ""https://www.openml.org/data/get_csv/20649148/freMTPL2freq.arff""
if not Path(""freMTPL2freq.csv"").exists():
  logging.info(""Loading data file"")
  df = pd.read_csv(path)
  df.to_csv(""freMTPL2freq.csv"")
else:
  logging.info(""Data file already exists"")
  df = pd.read_csv(path)

logging.info(""Opening data"")
data = tf.data.experimental.make_csv_dataset(""freMTPL2freq.csv"",
                                             num_rows_for_inference=100,
                                             batch_size=256,
                                             shuffle=False,
                                             sloppy=False,
                                             #num_parallel_reads=4,
                                             num_epochs=1,)
# this is even slower?
#data = tf.data.Dataset.from_tensor_slices(df.to_dict('list'))

logging.info(""Reading data"")
# This part is slower than expected
tmp = list(data.as_numpy_iterator())
logging.info(""Done reading data"")
lookup = tf.keras.layers.experimental.preprocessing.StringLookup()
feature_ds = data.map(lambda x: x['Area'])
logging.info(""Starting adapt"")
# This part is also slower than expected
lookup.adapt(feature_ds)
logging.info(""Finished adapt"")

# Commented out IPython magic to ensure Python compatibility.
# %timeit tmp = list(data.as_numpy_iterator())

# Commented out IPython magic to ensure Python compatibility.
# %timeit lookup.adapt(feature_ds)

# Commented out IPython magic to ensure Python compatibility.
# %timeit df[""Area""].unique()

# Commented out IPython magic to ensure Python compatibility.
# %timeit pd.read_csv(path)
```"
47170,Can't compile Tensorarray with XLA.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CPU
- GPU model and memory: N/A

**Describe the current behavior**

I have some code that is expensive to compile in XLA (~10 minutes). I am looking for a way to speed up this compilation. I noticed that my codebase produces a lot of computational graph nodes in one of the for loops. Therefore I plan to replace this for-loop with a `tf.while_loop`. In order to do that, I need to use TensorArrays. When I do that the compilation fails.

If I understand correctly, I have to build the Tensorarray inside the XLA context due to XLA limitations. I believe the issue might be caused by the fact that on compile time the shape of the tensors have to be known and the compiler assumes all the shapes within the tensoarray will be shared.

When I change the shapes of the tensors in such a way that all of them have the same size, the code compiles fine.

Then a question arises: Is there a way to construct a `tf.while_loop` (or a similiar construct) with some kind of a data structure so that I don't compile most of the iterations?

**Describe the expected behavior**

The XLA compilation runs smoothly and my compilation time goes down.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1gpzRRggNNcuYDnCWy-VLXNvLi3O81tAN?usp=sharing

```
import tensorflow as tf

array = [
    tf.random.uniform((19,1,3,2)),
    tf.random.uniform((19,1,2,2)),
    tf.random.uniform((3,1,3,2)),
    tf.random.uniform((18,1,2,2)),
]

size = len(array)

@tf.function(experimental_compile=True) # The same happens with autograph=False
def add(array):
    
    tensor_array = tf.TensorArray(
        dtype=tf.float32,
        size=size,
        infer_shape=False,
        element_shape=tf.TensorShape([None, 1, None, 2]),
    )
    for i in range(size):
        tensor_array = tensor_array.write(i, array[i])
    # There would be a while_loop here

r = add(array)
```

**Other info / logs**

```
---------------------------------------------------------------------------

InternalError                             Traceback (most recent call last)

<ipython-input-1-c5dfc3274b00> in <module>()
     23     # There would be a while_loop here
     24 
---> 25 r = add(array)

4 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    893       # If we did not create any variables the trace we have is good enough.
    894       return self._concrete_stateful_fn._call_flat(
--> 895           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access
    896 
    897     def fn_with_cond(inner_args, inner_kwds, inner_filtered_flat_args):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1917       # No tape is watching; skip to running the function.
   1918       return self._build_call_outputs(self._inference_function.call(
-> 1919           ctx, args, cancellation_manager=cancellation_manager))
   1920     forward_backward = self._select_forward_and_backward_functions(
   1921         args,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    558               inputs=args,
    559               attrs=attrs,
--> 560               ctx=ctx)
    561         else:
    562           outputs = execute.execute_with_cancellation(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

InternalError: Invalid TensorList shape: element_type: TUPLE
tuple_shapes {
  element_type: F32
  dimensions: 4
  dimensions: 19
  dimensions: 1
  dimensions: 3
  dimensions: 2
  layout {
    minor_to_major: 4
    minor_to_major: 3
    minor_to_major: 2
    minor_to_major: 1
    minor_to_major: 0
    format: DENSE
  }
  is_dynamic_dimension: false
  is_dynamic_dimension: false
  is_dynamic_dimension: false
  is_dynamic_dimension: false
  is_dynamic_dimension: false
}
tuple_shapes {
  element_type: S32
  layout {
    format: DENSE
  }
}
, expected: element_type: TUPLE
tuple_shapes {
  element_type: F32
  dimensions: 4
  dimensions: 19
  dimensions: 1
  dimensions: 2
  dimensions: 2
  layout {
    minor_to_major: 4
    minor_to_major: 3
    minor_to_major: 2
    minor_to_major: 1
    minor_to_major: 0
    format: DENSE
  }
  is_dynamic_dimension: false
  is_dynamic_dimension: false
  is_dynamic_dimension: false
  is_dynamic_dimension: false
  is_dynamic_dimension: false
}
tuple_shapes {
  element_type: S32
  layout {
    format: DENSE
  }
}

	 [[{{node TensorArrayV2Write_1/TensorListSetItem}}]] [Op:__inference_add_26]
```"
47168,TFLite GPU Delegate Fails ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Google Pixel XL
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0,  2.5.0-dev20210212
- Python version: 3.6.12
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 7.1.3-1+cuda11.0
- GPU model and memory: NVIDIA Tesla V100, 16GB

**Describe the current behavior**
I am attempting to use the GPU delegate with a TFLite model on an Android device. I am testing using the benchmark app from: https://www.tensorflow.org/lite/performance/measurement.

I am able to benchmark performance using the default (None/CPU) and XNNPACK delegates, but am running into an error when attempting to use the GPU delegate (--use_gpu=true). 

With an undefined (None) batch size, I get the error: 
```
Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
```

With a batch size of 1 (to avoid dynamic shapes):
```python
inputs = tf.keras.Input(shape=(48,), dtype=tf.int32, batch_size=1)
```

or forcing the input shape after saving:
``` python
model = tf.saved_model.load(save_dir)
concrete_func = model.signatures[
  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]

# set to batch size 1 so there are no dynamic shapes
concrete_func.inputs[0].set_shape([1, 48])
```

I am getting: 
```
02-15 10:04:31.620 32268 32268 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/test.tflite               --num_threads=4              --use_gpu=true
02-15 10:04:31.628 32268 32268 I tflite  : Initialized TensorFlow Lite runtime.
02-15 10:04:31.630 32268 32268 I tflite  : Created TensorFlow Lite delegate for GPU.
02-15 10:04:31.631 32268 32268 E tflite  : Following operations are not supported by GPU delegate:
02-15 10:04:31.631 32268 32268 E tflite  : DEQUANTIZE:
02-15 10:04:31.631 32268 32268 E tflite  : GATHER: Operation is not supported.
02-15 10:04:31.631 32268 32268 E tflite  : 233 operations will run on the GPU, and the remaining 2 operations will run on the CPU.
02-15 10:04:31.632 32268 32268 E tflite  : TfLiteGpuDelegate Init: ADD: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got 48x100
02-15 10:04:31.632 32268 32268 I tflite  : Created 0 GPU delegate kernels.
02-15 10:04:31.632 32268 32268 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized
02-15 10:04:31.632 32268 32268 E tflite  : Node number 235 (TfLiteGpuDelegateV2) failed to prepare.
02-15 10:04:31.632 32268 32268 E tflite  : Restored original execution plan after delegate application failure.
```
**Describe the expected behavior**
The GPU delegate does not raise an error, and the delegate is successful.

**Standalone code to reproduce the issue**
``` python
import numpy as np
import tensorflow as tf

# from: https://github.com/keras-team/keras-io/blob/master/examples/nlp/text_classification_with_transformer.py
class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential(
            [tf.keras.layers.Dense(ff_dim, activation=""relu""), tf.keras.layers.Dense(embed_dim),]
        )
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# from: https://github.com/keras-team/keras-io/blob/master/examples/nlp/text_classification_with_transformer.py
class TokenAndPositionEmbedding(tf.keras.layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super(TokenAndPositionEmbedding, self).__init__()
        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions
    
X_train = np.ones((10, 48))
X_test = np.ones((10,48))
y_train = np.ones(10)
y_test = np.ones(10)

inputs = tf.keras.Input(shape=(48,), dtype=tf.int32, batch_size=1)

embeddings = TokenAndPositionEmbedding(
            maxlen=48, vocab_size=10, embed_dim=100)(inputs)

sequence_layer = TransformerBlock(100, 12, 16)(embeddings)
global_pool = tf.keras.layers.GlobalAveragePooling1D()(sequence_layer)
dense_layer = tf.keras.layers.Dense(16, activation='relu')(global_pool)
pred = tf.keras.layers.Dense(2, activation='softmax', name=""prediction"")(dense_layer)

model = tf.keras.Model(inputs, outputs=pred)

model.compile(optimizer=""Adam"",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=[""accuracy""],
             )

model.fit(
    x=X_train,
    y=y_train,
    epochs=1,
    steps_per_epoch=10,
    verbose=1,
)

converter = tf.lite.TFLiteConverter.from_keras_model(model)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]

converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS,
]

tflite_model = converter.convert()

with open('models/test.tflite', 'wb') as f:
    f.write(tflite_model)
```
"
47166,Failed to build Tensorflow Lite for Win32 using CMake,"**System information**
- OS Platform and Distribution: Windows - 10.0.19041 - AMD64
- TensorFlow installed from: tag v2.4.1 (85c8b2a) or master (bf157579cf0)
- TensorFlow version: 2.4.1
- CMake version: 3.17.2
- GCC/Compiler version: MSVC 19.27.29111.0
- CUDA/cuDNN version: Not used
- GPU model and memory: Not used

This is probably not relevant since I'm building with CMake, but follow my Bazel environment information:
- Python version: 3.8.3 (x64 and x86)
- Installed using virtualenv: tried with both Python x86 venv and with x64 venv
- Bazel version: 3.1.0




**My problem:**
I'm currently tying to build Tensorflow Lite library to be used by my library that should support both Linux and Windows, x86 and x64. Since Bazel does not builds for x86 out of the box I was following the [documentation](https://www.tensorflow.org/lite/guide/build_cmake) to build TF Lite using CMake, including its source with _add_subdirectory()_ .
It works great for for x64 builds (-A x64) but when I change it to windows x86 (-A Win32) it always fails building _gemmlowp_.
I also tried to build via cmake on command line, using the tensorflow/lite/CMakeLists.txt to build the static library, but get the same error.
**I wonder if it was even supposed to be supported** and if so, if it is a issue to be handled by tensorflow or [gemmlowp](https://github.com/google/gemmlowp). I can also see some pthread releated errors on CMakeError.log


**Build steps:**

-  cmake -A Win32 -DCMAKE_BUILD_TYPE=Release -S ..\tensorflow\lite\ -B .
-  cmake --build . --config Release
...
  ...
  expand_dims.cc
  fake_quant.cc
  Generating Code...
**C:\workspace\tensorflow\build-tflite\gemmlowp\internal\output.h(176): fatal error C1001: Internal compiler error.** [C :\workspace\tensorflow\build-tflite\tensorflow-lite.vcxproj]
  (compiler file 'd:\agent\_work\7\s\src\vctools\Compiler\Utc\src\p2\main.c', line 195)
   To work around this problem, try simplifying or changing the program near the locations listed above.
  If possible please provide a repro here: https://developercommunity.visualstudio.com
  Please choose the Technical Support command on the Visual C++
   Help menu, or open the Technical Support help file for more information

**Other logs**

**CMakeError.log**

Determining if the include file pthread.h exists failed with the following output:
Change Dir: C:/workspace/tensorflow/build-tflite/CMakeFiles/CMakeTmp

...

Copyright (C) Microsoft Corporation. All rights reserved.

  Microsoft (R) C/C++ Optimizing Compiler Version 19.27.29111 for x86

  CheckIncludeFile.c

  Copyright (C) Microsoft Corporation.  All rights reserved.

  cl /c /I""C:\workspace\vcpkg\scripts\buildsystems\msbuild\..\..\..\installed\x86-windows\include"" /Zi /W1 /WX- /diagnostics:column /Od /Ob0 /Oy- /D WIN32 /D _WINDOWS /D ""CMAKE_INTDIR=\""Debug\"""" /D _MBCS /Gm- /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Fo""cmTC_d958e.dir\Debug\\"" /Fd""cmTC_d958e.dir\Debug\vc142.pdb"" /Gd /TC /analyze- /errorReport:queue ""C:\workspace\tensorflow\build-tflite\CMakeFiles\CMakeTmp\CheckIncludeFile.c""

C:\workspace\tensorflow\build-tflite\CMakeFiles\CMakeTmp\CheckIncludeFile.c(1,10): **fatal error C1083: Cannot open include file: 'pthread.h': No such file or directory** [C:\workspace\tensorflow\build-tflite\CMakeFiles\CMakeTmp\cmTC_d958e.vcxproj]

Performing C++ SOURCE FILE Test EIGEN_COMPILER_SUPPORT_CPP11 failed with the following output:
Change Dir: C:/workspace/tensorflow/build-tflite/CMakeFiles/CMakeTmp

...

  Microsoft (R) C/C++ Optimizing Compiler Version 19.27.29111 for x86
  src.cxx
  Copyright (C) Microsoft Corporation.  All rights reserved.

  cl /c /I""C:\workspace\vcpkg\scripts\buildsystems\msbuild\..\..\..\installed\x86-windows\include"" /Zi /W1 /WX- /diagnostics:column /Od /Ob0 /Oy- /D WIN32 /D _WINDOWS /D EIGEN_COMPILER_SUPPORT_CPP11 /D ""CMAKE_INTDIR=\""Debug\"""" /D _MBCS /Gm- /EHsc /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /GR /Fo""cmTC_94317.dir\Debug\\"" /Fd""cmTC_94317.dir\Debug\vc142.pdb"" /Gd /TP /analyze- /errorReport:queue  -std=c++11 ""C:\workspace\tensorflow\build-tflite\CMakeFiles\CMakeTmp\src.cxx""

cl : **command line warning D9002: ignoring unknown option '-std=c++11'** [C:\workspace\tensorflow\build-tflite\CMakeFiles\CMakeTmp\cmTC_94317.vcxproj]

Source file was:
int main(int argc, char* argv[]) { return (int)__builtin_expect(0, 0); }
**Checking whether the ASM compiler is GNU using ""--version"" did not match ""(GNU assembler)|(GCC)|(Free Software Foundation)"":**
Microsoft (R) C/C++ Optimizing Compiler Version 19.27.29111 for x86
Copyright (C) Microsoft Corporation.  All rights reserved.



Thanks in advance, cheers!"
47163,Error while training a model using spark and elephas estimator- Please suggest,"I am still getting the same error while executing on spark cluster 

estimator = ElephasEstimator()
estimator.set_keras_model_config(model.to_yaml())
estimator.set_optimizer_config(sgd_conf)
estimator.set_mode(""synchronous"")
estimator.set_loss(""mae"")
estimator.set_metrics(['mse'])
estimator.set_epochs(epochs)
estimator.set_batch_size(batch_size)
estimator.set_validation_split(0.1)
estimator.set_categorical_labels(False)
pipeline = Pipeline(stages=[estimator])
fitted_pipeline = pipeline.fit(df)

<img width=""633"" alt=""Capture1"" src=""https://user-images.githubusercontent.com/53963317/107957497-b3761480-6fc6-11eb-8247-e0ba551f10da.PNG"">
<img width=""751"" alt=""Capture2"" src=""https://user-images.githubusercontent.com/53963317/107957501-b53fd800-6fc6-11eb-9c2c-ce335eadfd9e.PNG"">

"
47162,Compiled metrics cannot deal with dictionaries in update_state,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.7.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0/8.1.0
- GPU model and memory: RTX2080Ti 11GB

**Describe the current behavior**
I have written a custom metric, subclassing from tf.keras.metrics.Metric. I also have a custom model,
subclassing from tf.keras.Model. I compile the model, passing this custom metric. When doing a single train step,
I update the state of the custom metric with prediction and target as **dictionaries**.

However, the autograph operation turns these dictionaries into lists, making it impossible for me to access the right item in the dictionary within the metric.

An error is thrown:

```
File ""bug.py"", line 41, in <module>
    model.train_step([])
  File ""bug.py"", line 17, in train_step
    self.compiled_metrics.update_state(y_true, y_pred)
  File ""/home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py"", line 408, in update_state
    metric_obj.update_state(y_t, y_p, sample_weight=mask)
  File ""/home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/keras/utils/metrics_utils.py"", line 90, in decorated
    update_op = update_state_fn(*args, **kwargs)
  File ""/home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py"", line 177, in update_state_fn
    return ag_update_state(*args, **kwargs)
  File ""/home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 670, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    bug.py:27 update_state  *
        self.value.assign_add((y_true[""a""] - y_pred[""a""]) ** 2)
    /home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1009 _slice_helper
        _check_index(s)
    /home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:886 _check_index
        raise TypeError(_SLICE_TYPE_ERROR + "", got {!r}"".format(idx))

    TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'a'
```

**Describe the expected behavior**
I expect the dictionaries passed to the compiled metric container's `update_state` function to be preserved as dictionaries instead of converted into keyless lists.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1DCYM2W3C5H2StxrdPeqqsvVGBtbVfNfJ?usp=sharing

```
import numpy as np
import tensorflow as tf

class Model(tf.keras.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, x, training=True):
        # this model returns a dictionary of values
        return {
            ""a"": tf.constant([5]),
            ""b"": tf.constant([5]),
        }

    def train_step(self, data):
        # each sample also consists of a dictionary of values
        y_true = {""a"": tf.constant([3]),
                  ""b"": tf.constant([4])}
        y_pred = self(4)

        # the metrics on this compiled metric container each care about
        # only one of the values in the dictionaries y_true/y_pred
        self.compiled_metrics.update_state(y_true, y_pred)

        return {m.name: m.result() for m in self.metrics}

class MetricForA(tf.keras.metrics.Metric):
    def __init__(self, name=""metricForA"", **kwargs):
        super(MetricForA, self).__init__(name=name, **kwargs)
        self.value = self.add_weight(name='value', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.value.assign_add((y_true[""a""] - y_pred[""a""]) ** 2)

    def result(self):
        return self.value

    def reset_states(self):
        self.value.assign(0)

class MetricForB(tf.keras.metrics.Metric):
    def __init__(self, name=""metricForB"", **kwargs):
        super(MetricForB, self).__init__(name=name, **kwargs)
        self.value = self.add_weight(name='value', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.value.assign_add((y_true[""b""] - y_pred[""b""]) ** 2)

    def result(self):
        return self.value

    def reset_states(self):
        self.value.assign(0)

if __name__ == ""__main__"":
    model = Model()
    metrics = [MetricForA(), MetricForB()]

    model.compile(metrics=metrics)

    # this results in an error, showing us that the dictionaries passed to update_state
    # are no longer dictionaries
    model.train_step([])
```"
47161,Keras fails to load saved model / properly infer dtypes in `tf.math.maximum`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Keras fails to load model due to problems with inferring data types in `tf.math.maximum`. In particular: take the input to the network to be `float32`, then cast the tensor to `float64` and feed into maximum layer (with the second input to that layer being `float64` constant tensor) - creating this model is successful. Save that model (below I used SavedModel format, but .h5 case is similar) and try to load it - error is raised.

Moreover, the error is raised only when the constant tensor is passed as first input to `tf.math.maxiumum` and does not occur if it is passed as the second input - see the Colab notebook I attached below.

I believe this is strictly Keras-related, as I was able to successfully convert this failing-to-load model to to .tflite version (with proper options of converter set, using `from_saved_model` method) and this tflite model works and its' data types are correct.

**Describe the expected behavior**
The model should load properly.

**Standalone code to reproduce the issue**

This code snipper should reproduce the issue:

```python
import tensorflow as tf
from tensorflow import keras

inp = keras.Input(shape=(1))
x = tf.cast(inp, dtype=tf.float64)
a = tf.constant(1.0, dtype=tf.float64)
x = tf.maximum(a, x)
out = tf.cast(x, dtype=tf.float32)

model = keras.models.Model(inp, out)
model.summary()

model.save('dummy_model')
del model

loaded_model = tf.keras.models.load_model('dummy_model')
```

BTW: When using `x = tf.maximum(x, a)` instead of `x = tf.maximum(a, x)` in the example above, the error is not raised!

See also slightly more elaborate [Colab notebook](https://colab.research.google.com/drive/1ysEjTkfkCDUzMQDrlbRGPfP8d8M5VZu-?usp=sharing).

**Other info / logs** 
```python                                                                  
2021-02-15 12:45:01.772147: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-15 12:45:01.772422: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
tf.cast (TFOpLambda)         (None, 1)                 0         
_________________________________________________________________
tf.math.maximum (TFOpLambda) (None, 1)                 0         
_________________________________________________________________
tf.cast_1 (TFOpLambda)       (None, 1)                 0         
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________
2021-02-15 12:45:01.835730: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Traceback (most recent call last):
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 517, in _apply_op_helper
    values = ops.convert_to_tensor(
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py"", line 163, in wrapped
    return func(*args, **kwargs)
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1507, in convert_to_tensor
    raise ValueError(
ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float64: <tf.Tensor 'Placeholder:0' shape=(None, 1) dtype=float64>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tmp.py"", line 32, in <module>
    loaded_model = tf.keras.models.load_model('dummy_model')
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py"", line 212, in load_model
    return saved_model_load.load(filepath, compile, options)
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 147, in load
    keras_loader.finalize_objects()
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 600, in finalize_objects
    self._reconstruct_all_models()
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 619, in _reconstruct_all_models
    self._reconstruct_model(model_id, model, layers)
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 665, in _reconstruct_model
    created_layers) = functional_lib.reconstruct_from_config(
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 1285, in reconstruct_from_config
    process_node(layer, node_data)
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 1233, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 951, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1090, in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 822, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 863, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py"", line 1327, in _call_wrapper
    return self._call_wrapper(*args, **kwargs)
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py"", line 1359, in _call_wrapper
    result = self.function(*args, **kwargs)
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5704, in maximum
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File ""/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 555, in _apply_op_helper
    raise TypeError(
TypeError: Input 'y' of 'Maximum' Op has type float64 that does not match type float32 of argument 'x'.
```
"
47160,"Same dataset(180 GB), why training 1 epoch on 2 nodes(16 gpus) takes around 10 minutes, training on 4 nodes(32 gpus) only take 33 seconds?","# System Information:
TensorFlow version (use command below): 2.1.1
Python version: 3.7
mpi/openmpi: 4.0.3-gnu-9.2.0
compiler/gnu/: 9.2.0
CUDA version: 10.1.243
GPU model and memory:  Tesla V100-SXM2-32GB


# Description:

I tested the code with multiple nodes in a cluster, and the training time for one epoch has sudden decrease when use 4 nodes, and I have no idea why it happens. The codes tries to read from 343 csv files(around 180 gb in total), and each csv file has 14406 records, each record has 4860 features, and 8 labels to be predicted. The 8 labels are numbers, thus it is a regression problem.

Here is the code: 


# Code: 
```
import time
import pathlib
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPool2D, Flatten, Dense, BatchNormalization, Dropout, Activation
from tensorflow.keras.optimizers import SGD, RMSprop
import sys
import json
import os
import socket
import datetime
import math
import pathlib

tf.random.set_seed(22)

hostlist=sys.argv[1].split(',')
current_host=socket.gethostname()
index=hostlist.index(current_host)

simsInFile=14406

verbose = index < 1
nodes=[]

for host in hostlist:
  nodes.append(host + ':2001')

os.environ['TF_CONFIG'] = json.dumps({
  'cluster': {
    'worker': nodes
  },
  'task': {'type': 'worker', 'index': index}
})
print(os.environ['TF_CONFIG'])

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(tf.distribute.experimental.CollectiveCommunication.AUTO)

AUTOTUNE = tf.data.experimental.AUTOTUNE

per_worker_batch_size = 512
num_workers = strategy.num_replicas_in_sync
global_batch_size = per_worker_batch_size * num_workers

num_epochs = 1

def read_csv(line):
  fields = tf.io.decode_csv(line, record_defaults=[tf.constant([], dtype=tf.float32)]*4869, field_delim="","")
  label = tf.stack(fields[4861:4869])
  label = tf.dtypes.cast(label, tf.dtypes.float32)
  label = tf.reshape(label,[8])

  features = tf.stack(fields[1:4861])
  features = tf.dtypes.cast(features, tf.dtypes.float32)
  features = tf.reshape(features,[10,486])

  return features, label

def create_train_dataset(ds_files_, batch_size=global_batch_size, nr_epochs=num_epochs, buffer_size=10000):
  dataset = ds_files_.interleave(lambda x:tf.data.TextLineDataset(x).skip(1))
  dataset = dataset.map(read_csv, num_parallel_calls=AUTOTUNE)
  dataset = dataset.cache()
  dataset = dataset.shuffle(buffer_size=buffer_size)
  dataset = dataset.repeat()
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(1)
  return dataset

learning_rate = 1e-3
total_epoch = 1
ds_files = tf.data.Dataset.list_files(xxx/*.csv')
ds_files = ds_files.shard(num_workers, index)
train_dataset = create_train_dataset(ds_files)

def build_model():
  return tf.keras.models.Sequential([
    tf.keras.layers.Conv1D(64,3,input_shape=(10, 486)),
    tf.keras.layers.Conv1D(64,3),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(100, activation='relu'),
    tf.keras.layers.Dense(8)
  ])

with strategy.scope():
  model = build_model()
  model.compile(optimizer=RMSprop(learning_rate), loss='mse')
  start_time = datetime.datetime.now()
  steps_epoch = 9651//num_workers
  history = model.fit(train_dataset,steps_per_epoch = steps_epoch,epochs = num_epochs, verbose = 1)
  print(""Training Finished!"")

os.environ.pop('TF_CONFIG', None)
```

"
47158,std::partial_sort in detection_postprocess kernel is not stable,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source): 5eac5b75b48232e93f2a96ed856131201cd2ae3c
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
std::partial_sort in detection_postprocess kernel is not stable, i.e. the order of equal elements is not guaranteed to be preserved. The problem with this is that it is hard to be bit-exact between TFL and TFLM. At least when compiling TFLM with Arm clang.

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
47157,Add the ability load specific weights with the tf.keras.Model.load_weights method,"**System information**
- **TensorFlow version (you are using):** 2.4.1
**Are you willing to contribute it (~Yes~/No):** At the moment I'm a bit short on time, I might have time to do this in the future. This feature request is a way to check if there is enough support to implement it.


**Describe the feature and the current behaviour/state.**

Currently the [tf.keras.Model.load_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) method loads all the weights that are in a checkpoint. In my project, I want the user to have the ability to load only certain layers (weights) that are in a checkpoint. The user should be able to do this by supplying CLI arguments.

Currently, I achieve this behaviour by overloading the [tf.keras.Model.load_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) method. I first store the old model state in this overloaded method before I call the `super().load_weights` method. After the weights are restored, I then overwrite the weights the user didn't want to load based on the old mode state. 

Giving Tensorflow users the ability to supply an `ignore_list`, for example, in the [CheckpointOptions](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointOptions) object would ease this process.

**Will this change the current api? How?**

It would add a extra parameter to the [CheckpointOptions](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointOptions) object and some additional code to the [tf.keras.Model.load_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) method in order to filter based on parameter names.

**Who will benefit with this feature?**

I think it will ease the loading process for people that are using transfer learning. It would stop them from relying on workarounds as the one explained above. I am not sure how much work it is to implement such an addition, and if the work outweighs the benefits. I, therefore, opened this issue to see if other people also think this is helpful."
47156,Mixed precision : Numerical instability with instance normalization layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 4.0.0
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 11.1/8.0.5
- GPU model and memory: Nvidia Quadro RTX 6000 ~ 23.8G

**Describe the current behavior**
I've developed a [WGAN-GP](http://arxiv.org/abs/1704.00028) model which uses some _Instance Normalization_ layers. The code I've written for _Instance Normalization_ is given above. Also I use mixed precision to speed up computations and save GPU memory. What I've observed by experimenting my model is that these normalization layers cause important numerical instability (gradient penalty goes up after hundreds of train steps) when their _dtype_ are left to mixed precision. Indeed, when I explicitly set the dtype to ""float32"" in the [parent constructor](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#arguments), I don't have these problems.


**Describe the expected behavior**
I don't understand why the normalization layers whose code is quite simple don't work well in mixed precision mode.

**Standalone code to reproduce the issue**
There is no simple way to reproduce the issue, I just hope to have advises, suggestions or explanations to better understand this behavior.

**Other info / logs** 
```
class InstanceNormalization(keras.layers.Layer):

    def __init__(self, epsilon=1e-3, **kwargs):
        super().__init__(**kwargs)
        self.epsilon=epsilon

    def build(self, batch_input_shape):
        self.scale = self.add_weight(
            name='scale',
            shape=batch_input_shape[-1:],
            initializer=tensorflow.random_normal_initializer(1,0.02),
            trainable=True
        )
        self.offset = self.add_weight(
            name='offset',
            shape=batch_input_shape[-1:],
            initializer=""zeros"",
            trainable=True
        )
        self.axis = range(1, len(batch_input_shape)-1)
        super().build(batch_input_shape)

    def call(self, x):
        mean = keras.backend.mean(x, axis=self.axis, keepdims=True)
        variance = keras.backend.mean(keras.backend.square(x-mean), axis=self.axis, keepdims=True)
        normalized = (x - mean) / keras.backend.sqrt(variance+self.epsilon)
        return self.scale * normalized + self.offset
```
"
47155,RuntimeError: Cannot use a constraint function on a sparse variable in google colab,"I am trying to train my model using Keras and TensorFlow 2.x, while using the model.fit() method I ran into this error

Error

```
Epoch 1/10
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)

<ipython-input-73-4871a80f91a3> in <module>()
      2 
      3 for i in range(N_epoch):
----> 4     model.fit(x=train_X,y=train_Y,batch_size=32,epochs=10,verbose=1, validation_data=(val_X,val_Y))
      5     output = model.predict_proba(val_X, batch_size=10, verbose=1)
      6     # find validation accuracy using the best threshold value t

9 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, ""ag_error_metadata""):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

RuntimeError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:498 minimize
        return self.apply_gradients(grads_and_vars, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:635 apply_gradients
        ""name"": name,
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2941 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2948 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:683 _distributed_apply  **
        var, apply_grad_to_update_var, args=(grad,), group=False))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2494 update
        return self._update(var, fn, args, kwargs, group)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3431 _update
        return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3437 _update_non_slot
        result = fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:650 apply_grad_to_update_var  **
        ""Cannot use a constraint function on a sparse variable."")

    RuntimeError: Cannot use a constraint function on a sparse variable.
```

**System information**
- Have I written custom code (Please find the code below)
- OS Platform and Distribution: Mac OSX Big Sur
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): Tensorflow 2.2
- Python version: Python 3.9.1

Code is given below. [Link](https://colab.research.google.com/drive/1nxjfPUWFWDW9jtoVK7AzKWuTT-C2o-Cb?usp=sharing) to the colab for full code. 
```

# Train data preparation
N = datasets[0].shape[0]
conv_input_width = W.shape[1]
conv_input_height = int(datasets[0].shape[1]-1)

# For each word write a word index (not vector) to X tensor
train_X = np.zeros((N, conv_input_height), dtype=np.int)
train_Y = np.zeros((N, 2), dtype=np.int)
for i in range(N):
    for j in range(conv_input_height):
        train_X[i, j] = datasets[0][i, j]
    
print ('train_X.shape = {}'.format(train_X.shape))
print ('train_Y.shape = {}'.format(train_Y.shape))

# Validation data preparation
Nv = datasets[1].shape[0]

# For each word write a word index (not vector) to X tensor
val_X = np.zeros((Nv, conv_input_height), dtype=np.int)
val_Y = np.zeros((Nv, 2), dtype=np.int)
for i in range(Nv):
    for j in range(conv_input_height):
        val_X[i, j] = datasets[1][i, j]
print('val_X.shape = {}'.format(val_X.shape))
print('val_Y.shape = {}'.format(val_Y.shape))
for i in range(Nv):
    val_Y[i,data_train.iloc[i,3]] = 1

from keras.optimizers import RMSprop
from keras import backend
backend.set_image_data_format('channels_first')
import keras


# Number of feature maps (outputs of convolutional layer)
N_fm = 200
# kernel size of convolutional layer
kernel_size = 5

model = Sequential()
# Embedding layer (lookup table of trainable word vectors)
model.add(Embedding(input_dim=W.shape[0], 
                    output_dim=W.shape[1], 
                    input_length=conv_input_height,
                    weights=[W], 
                    embeddings_constraint=UnitNorm,
                    name = 'e_l'))
# Reshape word vectors from Embedding to tensor format suitable for Convolutional layer
model.add(Reshape((1, conv_input_height, conv_input_width)))

# first convolutional layer
model.add(Convolution2D(N_fm,
                        kernel_size, 
                        conv_input_width,
                        kernel_initializer='random_uniform',
                        padding='valid',
                        kernel_regularizer=l2(0.001)))
# ReLU activation
model.add(Activation('relu'))

# aggregate data in every feature map to scalar using MAX operation
model.add(MaxPooling2D(pool_size=(conv_input_height+kernel_size+1,1), padding='same'))

model.add(Flatten())
model.add(Dropout(0.4))
model.add(Dense(128,kernel_initializer='random_uniform'))
model.add(Activation('relu'))
model.add(Dropout(0.4))
# Inner Product layer (as in regular neural network, but without non-linear activation function)
model.add(Dense(2))
# SoftMax activation; actually, Dense+SoftMax works as Multinomial Logistic Regression
model.add(Activation('softmax'))

# Custom optimizers could be used, though right now standard adadelta is employed
opt = RMSprop(lr=0.001, rho=0.9, epsilon=None)
model.compile(loss='mean_squared_error', 
              optimizer=opt,
              metrics=['accuracy'])

```

**The line that throws the error**

```
N_epoch = 3

for i in range(N_epoch):
    model.fit(x=train_X,y=train_Y,batch_size=32,epochs=10,verbose=1, validation_data=(val_X,val_Y))
    output = model.predict_proba(val_X, batch_size=10, verbose=1)
    # find validation accuracy using the best threshold value t
    vacc = np.max([np.sum((output[:,1]>t)==(val_Y[:,1]>0.5))*1.0/len(output) for t in np.arange(0.0, 1.0, 0.01)])
    # find validation AUC
    vauc = roc_auc_score(val_Y, output)
    val_acc.append(vacc)
    val_auc.append(vauc)
    print('Epoch {}: validation accuracy = {:.3%}, validation AUC = {:.3%}'.format(epoch, vacc, vauc))
    epoch += 1
    
print('{} epochs passed'.format(epoch))
print('Accuracy on validation dataset:')
print(val_acc)
print('AUC on validation dataset:')
print(val_auc)
```

**Tweaks I tried**
1. I have tried changing the UnitNorm in the embedding layer
2. Verified the embedding layer doesn't use sparse data. Instead, it uses a dense matrix to store that data.
3.  Referred this [link](https://github.com/tensorflow/tensorflow/issues/33755) but couldn't solve my error.


Please can anyone suggest a solution? Thanks"
47151,ERROR: Could not find a version that satisfies the requirement tensorflow ERROR: No matching distribution found for tensorflow,"I'm trying to install the TensorFlow lib for 3 days now & tried everything, I'm using win10 64 bit and python 3.9.1 64 bit with up to date pip

"
47150,"I tried filling in all of your ""form"" & you closed off the issue so, were going to do this EVERY DAY until you act like grown ups","Backend terminated or disconnected.Fatal Python error: Illegal instruction

Current thread 0x00007f0f5412a740 (most recent call first):
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 1101 in create_module
  File ""<frozen importlib._bootstrap>"", line 556 in module_from_spec
  File ""<frozen importlib._bootstrap>"", line 657 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 783 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1042 in _handle_fromlist
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 39 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 783 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 961 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""/home/ian/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 783 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""<pyshell>"", line 1 in <module>
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1272 in _execute_prepared_user_code
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1200 in wrapper
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1213 in wrapper
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1259 in execute_source
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 814 in _execute_source
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 444 in _cmd_execute_source
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 204 in handle_command
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 146 in mainloop
  File ""/usr/lib/python3/dist-packages/thonny/backend_launcher.py"", line 87 in <module> Use 'Stop/Restart' to restart.

"
47149,ValueError: Cannot add function '__inference_Dataset_map_<lambda>_12' because a different function with the same name already exists.,"Hi
 I am using ubuntu 16.04 , and tensorflow-gpu==1.14.0, I obtained this error 
"" ValueError: Cannot add function '__inference_Dataset_map_<lambda>_12' because a different function with the same name already exists.""

Please let me know if you have idea how I can resolve it? thank you in advance.

"
47147,GPU not detected / tf.test.is_built_with_cuda returns False,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: N/A
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: conda
-   **TensorFlow version (use command below)**: 2.3.0
-   **Python version**: 3.8.5 [Anaconda 2020.11]
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 10.1 / 7.6.4 [graphics driver 461.40]
-   **GPU model and memory**: GeForce RTX 3080 / 10GB [EVGA Black]
-   **Exact command to reproduce**: import tensorflow as tf, tf.test.is_built_with_cuda()

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
-Installed MSVS 2019 Community Edition
-Installed Cuda Toolkit 10.1 [in preparation for TensorFlow 2.3.0 from conda]. NOTE: Did custom installation, not installing the graphics or physics drivers as I already have the most up to date drivers.
-Installed / copied drivers across from cuDNN 7.6.4
-Installed Anaconda 2020.11
-Created environment and installed TensorFlow-GPU [conda create -n tf-gpu tensorflow-gpu]
-Activated tf-gpu environment [conda activate tf-gpu]
-started python [python]
-imported tensorflow [import tensorflow as tf]
-checked installation [tf.test.is_built_with_cuda()]
returns False
-checked for GPU [tf.config.list_physical_devices('GPU')
returns []

I'm not sure what I'm missing or doing wrong during the installation. I don't have the CPU only tensorflow installed. Is it because I am not installing the graphics drivers with CUDA Toolkit? Any help is much appreciated.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
47146,Python import failure,"**System information**
- OS Platform and Distribution (Linux Ubuntu 20 latest update):
- Mobile device ()Starlabs Star Lite laptop with 8Gb of RAM 113Gb of SSD space:
- TensorFlow installed from (pip install tensorflow==2.4.1):
- TensorFlow version: see above
- Python version: Python 3.8.5 (/usr/bin/python3)
- Installed using virtualenv? pip? conda?: See above
- Bazel version (if compiling from source): NA NA
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Dunno
- GPU model and memory: (description: VGA compatible controller
       product: UHD Graphics 605
       vendor: Intel Corporation
       physical id: 2
       bus info: pci@0000:00:02.0
       version: 03
       width: 64 bits
       clock: 33MHz
       capabilities: pciexpress msi pm vga_controller bus_master cap_list rom
       configuration: driver=i915 latency=0
       resources: irq:132 memory:a0000000-a0ffffff memory:90000000-9fffffff ioport:f000(size=64) memory:c0000-dffff)



**Describe the problem**
when I import tensorflow into the Thonny shell I get:

Backend terminated or disconnected.Fatal Python error: Illegal instruction

Current thread 0x00007f30949d8740 (most recent call first):
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 1101 in create_module
  File ""<frozen importlib._bootstrap>"", line 556 in module_from_spec
  File ""<frozen importlib._bootstrap>"", line 657 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 783 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1042 in _handle_fromlist
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 39 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 783 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 961 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""/home/ian/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 783 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""<pyshell>"", line 1 in <module>
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1272 in _execute_prepared_user_code
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1200 in wrapper
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1213 in wrapper
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1259 in execute_source
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 814 in _execute_source
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 444 in _cmd_execute_source
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 204 in handle_command
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 146 in mainloop
  File ""/usr/lib/python3/dist-packages/thonny/backend_launcher.py"", line 87 in <module> Use 'Stop/Restart' to restart.Backend terminated or disconnected.Fatal Python error: Illegal instruction

Current thread 0x00007f30949d8740 (most recent call first):
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 1101 in create_module
  File ""<frozen importlib._bootstrap>"", line 556 in module_from_spec
  File ""<frozen importlib._bootstrap>"", line 657 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 783 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1042 in _handle_fromlist
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""/home/ian/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 39 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 783 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 961 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""/home/ian/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 783 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 285 in _custom_import
  File ""<pyshell>"", line 1 in <module>
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1272 in _execute_prepared_user_code
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1200 in wrapper
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1213 in wrapper
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 1259 in execute_source
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 814 in _execute_source
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 444 in _cmd_execute_source
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 204 in handle_command
  File ""/usr/lib/python3/dist-packages/thonny/backend.py"", line 146 in mainloop
  File ""/usr/lib/python3/dist-packages/thonny/backend_launcher.py"", line 87 in <module> Use 'Stop/Restart' to restart.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow==2.4.1 # into the terminal
# followed by

import tensorflow # into Thonny's shell

# the error appears in the shell

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
NA
"
47145,tensorflow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
47144,Hi I am trying to compile on windows and get errors,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): trying to install from source
- TensorFlow version:2.2
- Python version:3.9
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):3.73
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.2/8.1.0.77
- GPU model and memory:RTX 2080/ 15.9GB




**Describe the problem** I followed everything explained in this tutorial https://towardsdatascience.com/how-to-compile-tensorflow-2-3-with-cuda-11-1-8cbecffcb8d3
until the command :
bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package


where I got :
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/418c218efa950245ba075b9bb3a53505b807c5df.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (419 packages loaded, 27891 targets configured).
INFO: Found 1 target...
ERROR: C:/users/korin/tensorflow/tensorflow/core/profiler/internal/gpu/BUILD:108:16: undeclared inclusion(s) in rule '//tensorflow/core/profiler/internal/gpu:cupti_wrapper':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/profiler/internal/gpu/cupti_wrapper.cc':
  'tensorflow/stream_executor/cuda/cuda_activation.h'
  'tensorflow/stream_executor/gpu/gpu_activation.h'
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1356.616s, Critical Path: 249.97s
INFO: 7667 processes: 3916 internal, 3751 local.
FAILED: Build did NOT complete successfully

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47143,EdgeTPU compilation fails for multi-head networks,"### 1. System information

- OS Platform and Distribution  Linux Ubuntu 20.04:
- TensorFlow installation pip installation 2.3.1:

### 2. Code

Model Architecture:
```
TARGET_SIZE = 224
inputs = tf.keras.layers.Input(shape=(TARGET_SIZE, TARGET_SIZE, 3))
base_model = tf.keras.applications.MobileNetV2(input_shape=(TARGET_SIZE, TARGET_SIZE, 3), alpha=1.0, include_top=False, weights='imagenet', input_tensor=inputs)
base_out = base_model.output
base_out = tf.keras.layers.Conv2D(filters=512, kernel_size=3, activation='relu')(base_out)
base_out = tf.keras.layers.Dropout(0.2)(base_out)
base_out = tf.keras.layers.GlobalAveragePooling2D()(base_out)
#base_out = tf.keras.layers.Flatten()(base_out)

# construct a fully-connected layer header to output the predicted
# bounding box coordinates
bboxHead = tf.keras.layers.Dense(256, activation=""relu"")(base_out)
bboxHead = tf.keras.layers.Dense(128, activation=""relu"")(bboxHead)
bboxHead = tf.keras.layers.Dense(64, activation=""relu"")(bboxHead)
bboxHead = tf.keras.layers.Dense(32, activation=""relu"")(bboxHead)
bboxHead = tf.keras.layers.Dense(4, activation=""sigmoid"", name=""bounding_box"")(bboxHead)

# construct a second fully-connected layer head, this one to predict
# the class label
softmaxHead = tf.keras.layers.Dense(256, activation=""relu"")(base_out)
softmaxHead = tf.keras.layers.Dense(128, activation=""relu"")(softmaxHead)
softmaxHead = tf.keras.layers.Dense(64, activation=""relu"")(softmaxHead)
softmaxHead = tf.keras.layers.Dense(8, activation=""relu"")(softmaxHead)
softmaxHead = tf.keras.layers.Dense(1, activation=""sigmoid"", name=""class_label"")(softmaxHead)

# put together our model which accept an input image and then output
# bounding box coordinates and a class label
model = tf.keras.models.Model(inputs=base_model.input, outputs=(bboxHead, softmaxHead))
```

Visualisation of the architecture : [model_plot](https://user-images.githubusercontent.com/17791005/107879987-5dc53d80-6edc-11eb-98b9-46ec369b8c89.png)

TFLite Conversion:
```
def convert_tflite(pl_model):
    converter = tf.lite.TFLiteConverter.from_keras_model(pl_model)
    # Set quantize to true
    converter.post_training_quantize = True
    converter.representative_dataset = tf.lite.RepresentativeDataset(representative_dataset_gen)
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # For EdgeTPU, no float ops allowed
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8

    tflite_model = converter.convert()
    open(""pl_od_model_edgetpu.tflite"", ""wb"").write(tflite_model)

def representative_dataset_gen():
    data_dir = os.path.join(os.path.join(ma_uav_dir, 'Vision/Detector/CNN_Classifier'), 'Data/test')
    pl_dir = os.path.join(data_dir, 'PayLoad')
    pl_imgs = [cv2.imread(os.path.join(pl_dir, image)) for image in os.listdir(pl_dir) if '.png' in image]
    not_pl_dir = os.path.join(data_dir, 'Not_PayLoad')
    not_pl_imgs = [cv2.imread(os.path.join(not_pl_dir, image)) for image in os.listdir(not_pl_dir) if '.png' in image]
    all_imgs = pl_imgs + not_pl_imgs
    random.shuffle(all_imgs)

    for i in range(len(all_imgs) - 1):
        img = all_imgs[i]
        image = resize_img(img_org=img).astype(np.float32)
        image = tf.expand_dims(image, 0)
        yield [image]

def resize_img(img_org):
    return cv2.resize(img_org, (TARGET_SIZE, TARGET_SIZE)) * (1. / 255)
```
Resulting in following Netron output: [image](https://user-images.githubusercontent.com/17791005/107880057-bac0f380-6edc-11eb-9bf9-7cfb7fff8f55.png)

### 3. Failure after conversion
The TFLite conversion is successful but the compilation to Coral EdgeTPU fails. Following command `edgetpu_compiler pl_od_model_edgetpu.tflite ` results in the error below:

EdgeTPU Compiler Output:
```
Edge TPU Compiler version 15.0.340273435
ERROR: :309 scale_diff / output_scale <= 0.02 was not true.
ERROR: Node number 79 (FULLY_CONNECTED) failed to prepare.

ERROR: :309 scale_diff / output_scale <= 0.02 was not true.
ERROR: Node number 79 (FULLY_CONNECTED) failed to prepare.

Compilation failed: Internal error

Internal compiler error. Aborting!
```

I suppose the issue is related to https://github.com/tensorflow/tensorflow/issues/41069. As I am also dealing with a model with two outputs. If the model has a single output, the compilation is successful. Could you clarify the `ERROR :309`? Is the coral able to handle a known issue regarding multi-head network quantisation? Is the coral able to handle multi-head networks?

I have to clarify that I trained the network for only 1 epoch and just wanted to use this ""dummy network"" to test if it is compatible with the Coral. 

Thanks! 
"
47142,Installing tensorflow 2 on raspberry pi 3 model B,"- Raspbian GNU/Linux 10 (buster)
- firstly trying to install via pip then second attempt trying to install from source
- TensorFlow version: 2.3.0
- Python version: 3.7.3
- gcc version 8.3.0 (Raspbian 8.3.0-6+rpi1) 

Hello, I've been trying to get tensorflow (specifically Keras) to work on a raspberry pi 3 model B, using a Raspbian GNU/Linux 10 (buster), default python version is 3.7.3 and pip version is updated to 21.0.1.
the official tensorflow site says that the installation via pip should be all the same when it comes to raspberry pi with the following details but for some reasons pip installs tensorflow 1.12.0 when I run the command line `pip3 install tensorflow`, I've also tried following a guide to install tensorflow 2.3.0 via a wheel but that failed too when pip failed to find a version of tesnorflow-estimator/tensorboard that satisfies the requirements.
I would really appreciate any help.

the exact guide I am following is this guide: https://itnext.io/installing-tensorflow-2-3-0-for-raspberry-pi3-4-debian-buster-11447cb31fc4 and when I get to the last phase of actually installing tensorflow 2.3.0 I get the following error:

` Could not find a version that satisfies the requirement tensorflow-estimator<2.4.0,>=2.3.0 (from tensorflow==2.3.0) (from versions: 1.10.0, 1.10.1, 1.10.2, 1.10.3, 1.10.4, 1.10.5, 1.10.6, 1.10.7, 1.10.8, 1.10.9, 1.10.10, 1.10.11, 1.10.12)
No matching distribution found for tensorflow-estimator<2.4.0,>=2.3.0 (from tensorflow==2.3.0)`




"
47141,Tensorflow C API Prebuilt Library for Ubuntu 20,"Hi,

Currently, published Tensorflow C API Ubuntu libraries are  built for Ubuntu 16. Is there any plan to built it for Ubuntu 20? 

When I try to link it against my Ubuntu 20 projects, I got linker errors such as:

```
.dynsym local symbol at index 857
...
/usr/bin/ld: Model.cpp:(.text+0x101): undefined reference to `TF_NewGraph'
/usr/bin/ld: Model.cpp:(.text+0x10a): undefined reference to `TF_NewSessionOptions'
/usr/bin/ld: Model.cpp:(.text+0x128): undefined reference to `TF_SetConfig'
/usr/bin/ld: Model.cpp:(.text+0x160): undefined reference to `TF_LoadSessionFromSavedModel'
```

Thanks."
47140,pip install error could not find a version that satisfies the requirement tensorflow,"I'm using python 3.9.1 64-bit, win10 and I try to install TesorFlow using either pip, pip3 or pip3.9, but I always receive the error:

ERROR: Could not find a version that satisfies the requirement tesorflow
ERROR: No matching distribution found for tesorflow

![image](https://user-images.githubusercontent.com/79036528/107870484-5896cd00-6ea1-11eb-8267-aee1f3e27e88.png)

can you please help!

"
47139,InaccessibleTensorError for tensor defined in same scope,"Hello
I am for some reason getting an error in Tensorflow 2.2 stating that I am not being able to access a tensor when created within the same scope, but using `tf.function`

```python
import tensorflow as tf
cosine_loss = tf.keras.losses.CosineSimilarity(axis=0,reduction=tf.keras.losses.Reduction.NONE)
@tf.function
def func():
    A = tf.convert_to_tensor([[1,1.,1],[1,3,1]])
    B = tf.convert_to_tensor([[1,1.,1]])

    S = []
    for b in B:
        S_n = []
        for a in A:
            S.append(-cosine_loss(a,b))
        S.append(S_n)
    
    return S
print(func())
```

```
InaccessibleTensorError: The tensor 'Tensor(""while/while/Neg:0"", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=while_while_body_55, id=140111838031568); accessed from: FuncGraph(name=func, id=140111839241488).
```

Why is this not being able to access it?
Thank you"
47138,"How to use tensor on the tensorflow lite  like tensorflow tensor operation? For example, tensor ascend dimension ,reduction dimension and tensor  slice. Thanks.","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
47137,com_github_apple_swift_swift_protobuf sha256 mismatch in tensorflow/workspace.bzl,"
**System information**

In reference to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl on master, as-written.

**Describe the current behavior**

While digging in to an unrelated buildsystem problem, I noticed that
```
    # https://github.com/apple/swift-protobuf/releases
    tf_http_archive(
        name = ""com_github_apple_swift_swift_protobuf"",
        strip_prefix = ""swift-protobuf-1.6.0/"",
        sha256 = ""4ccf6e5ea558e8287bf6331f9f6e52b3c321fca5f1d181d03680f415c32a6bba"",
        urls = [
            ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/apple/swift-protobuf/archive/1.6.0.zip"",
            ""https://github.com/apple/swift-protobuf/archive/1.6.0.zip"",
        ],
    )
```

has a `sha256` value that doesn't match what I get from either source.  Note that I'm looking at this from a ""prepare for offline builds"" perspective, I don't know if the normal build system will actually fetch this, or if ""everyone"" already has the matching version cached, or if something else is going on (like the `intel/mkl-dnn` archive-rename problem.)

**Describe the expected behavior**

The particular values in `workspace.bzl` have been unchanged for a while.  I don't know if the upstream has changed or not, intentionally, but specific releases aren't supposed to; that's one reason for the hash checks...

**Standalone code to reproduce the issue**

```
workbench$ wget https://storage.googleapis.com/mirror.tensorflow.org/github.com/apple/swift-protobuf/archive/1.6.0.zip
2021-02-13 18:49:23 (9.31 MB/s) - 1.6.0.zip saved [1918999/1918999]
workbench$ sha256sum 1.6.0.zip 
4d6d2543da84474fe50a543f70ba145d99c4d14a4aac10d7b3c7dc9f0f7ecec3  1.6.0.zip
workbench$ rm 1.6.0.zip 
workbench$ wget https://github.com/apple/swift-protobuf/archive/1.6.0.zip
2021-02-13 18:49:43 (8.42 MB/s) - 1.6.0.zip saved [1918999/1918999]
workbench$ sha256sum 1.6.0.zip 
4d6d2543da84474fe50a543f70ba145d99c4d14a4aac10d7b3c7dc9f0f7ecec3  1.6.0.zip
```
(and note that 4d6.*ec3 does not match the 4cc.*bba in the `.bzl` file.)

**Other info / logs**

Vaguely related: https://github.com/tensorflow/tensorflow/issues/12979 and https://github.com/libgit2/libgit2/issues/4343 that suggest that ""it is known"" that github archive hashes change when github itself updates; usually the `storage.googleapis.com/mirror.tensorflow.org` links are enough to cover for that but it looks like they match now.
"
47136,Incorrect console print output while printing tensor generated from tf.image.grayscale_to_rgb api in Tensorflow 1.2,"I am trying to convert a grayscale png image into rgb. I am using Tensor version 1.2. When I print the output of the converted tensor on console, it shows me zero in all 3 channels. If I visualize the output on tensor board, it shows the image just fine. 

I tried doing the same in Tensor 2.4 and it works fine there. Is there a bug in printing on 1.2 version? 

import tensorflow as tf
import os
import numpy as np
from datetime import datetime

logdir = ""logs/"" + datetime.now().strftime(""%Y%m%d-%H%M%S"")
image_list = ['<full path of IR image.png>']
            
sess = tf.InteractiveSession()
imageHandle = tf.read_file(image_list[0])

image_tensor = tf.image.decode_png(imageHandle, dtype=tf.uint8) 
image_tensor = tf.reshape(image_tensor, [1, 512, 640, 1])
image_tensor = tf.Print(image_tensor, [image_tensor], ""*** read_images_from_decode_png"", summarize=900) # **This print is correct**

image_tensor1 = tf.image.grayscale_to_rgb(image_tensor)
image_tensor1 = tf.Print(image_tensor1, [image_tensor1], ""*** read_images_from_disk_grayscale2rgb"", summarize=900) # **This print is wrong**

summary_op3 = tf.summary.image('config/config', image_tensor1)
summary_writer = tf.summary.FileWriter(logdir, sess.graph)
text = sess.run(summary_op3)
summary_writer.flush()
summary_writer.close()"
47134,[tflite] Problem reading multi-input tflite model with Java,"Hi,

with a colleague of mine, we've build a two-input model using TF2's Functional API. We want to deploy this model in Android, using `tflite` and Java.

## Model

The model has the following architecture

```{python}
# Inputs
in1 = keras.Input(shape = (24, 7), name = 'in_1')
in2 = keras.Input(shape = (2,), name = 'in_2')

# Model
x1 = keras.layers.LSTM(48, return_sequences = True)(in1)
x1 = keras.layers.LSTM(48)(x1)

x2 = keras.layers.Dense(16, activation = 'relu')(in2)
x2 = keras.layers.Dense(8, activation = 'relu')(x2)

x = keras.layers.concatenate([x1, x2], name = 'concat_00')

x = keras.layers.Dense(128, activation = 'relu', name = 'concat_dense_00')(x)
x = keras.layers.Dropout(.2)(x)
x = keras.layers.Dense(64, activation = 'relu', name = 'concat_dense_02')(x)
x = keras.layers.Dropout(.2)(x)
output = keras.layers.Dense(12)(x)

model_lstm_3 = keras.Model(inputs = [in1, in2], outputs = output)
```

## `tflite` conversion

The model had been converted to `tflite` format, using TF 2.3 and the following code:

```{python}
converter = tf.lite.TFLiteConverter.from_keras_model(model_lstm_3)
tflite_model = converter.convert()
````

## Reading the model in Java

We're trying to read in the model in Java using the following code:

```{java}
tflite = Interpreter(loadModelFile(activity, ""model_simple.tflite""))

var output = arrayOf(0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f)
val geoData = arrayOf(arrayOf(arrayOf(52.26240269140683f, 20.971026685407484f)))
val historicData = arrayOf(arrayOf(
    // arrayListOf(....),
    arrayOf(1f, 1f, 0.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 1.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 2.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 3.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 4.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 5.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 6.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 7.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 8.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 9.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 10.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 11.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 12.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 13.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 14.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 15.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 16.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 17.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 18.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 19.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 20.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 21.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 22.0f, 1f, 1f, 1f, 2020f),
    arrayOf(1f, 1f, 23.0f, 1f, 1f, 1f, 2020f),
))
var floatInput = arrayOf<Array<Array<Array<Float>>>>(geoData, historicData)

Timber.d(""Input model $floatInput"")
tflite!!.run(floatInput, output)
Timber.d(""Output model $output"")
```

## Exception

The line `tflite!!.run(floatInput, output)` generates the following exception:

```{java}
java.lang.IllegalArgumentException: DataType error: cannot resolve DataType of [[[[Ljava.lang.Float;
        at org.tensorflow.lite.Tensor.dataTypeOf(Tensor.java:352)
        at org.tensorflow.lite.Tensor.throwIfTypeIsIncompatible(Tensor.java:418)
        at org.tensorflow.lite.Tensor.getInputShapeIfDifferent(Tensor.java:287)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:146)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:360)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:319)
```

## Java libs

```
implementation('org.tensorflow:tensorflow-lite:2.4.0')
implementation('org.tensorflow:tensorflow-lite-gpu:2.4.0')
implementation('org.tensorflow:tensorflow-lite-support:0.0.0-nightly')
```

We're not sure how to solve this and we'll be grateful for any hints.

Thank you!



"
47133,Data provided by a keras.utils.sequence DataGenerator has None dimensions when used to fit Model subclass,"Code is written using Google Colab on Tensorflow Version 2.4.1

A DataGenerator subclassing keras.utils.sequence is providing data of correct shape when used to fit a sequential model. However, when used to fit a custom model subclassing keras.Model the input dimensions become (None, None, None, None) in the call method during the first training epoch.

When the custom model is fit using the DataGenerator the input dimensions to the call method should be (16, 256, 128, 3).

A simplified recreation of the issue is provided on Google Colab in the following notebook:
[Colab Notebook](https://colab.research.google.com/drive/1VqPc6Wn2tIND6myoZyFWXhGiMispiDxJ#scrollTo=KMRPZrfQE3KA)"
47132,Tensorflow 2.4.1 AUC bug,"Hi, I think there is a bug in tf 2.4.1. If I use tf 2.3.2 it works fine, if I clone the environment and update tf to 2.4.1 I get the following behaviour and auc and val_auc are always at 0.5 no matter how many epochs I run (with a simple dense model; also the loss seems to be erratic):
Epoch 1/10
585/585 [==============================] - 2s 3ms/step - loss: 0.6466 - auc: 0.5065 - val_loss: 0.5479 - val_auc: 0.5000
Epoch 2/10
585/585 [==============================] - 1s 1ms/step - loss: 0.7262 - auc: 0.5000 - val_loss: 0.4576 - val_auc: 0.5000
Epoch 3/10
585/585 [==============================] - 1s 1ms/step - loss: 0.4614 - auc: 0.5000 - val_loss: 0.4271 - val_auc: 0.5000
Epoch 4/10
585/585 [==============================] - 1s 1ms/step - loss: 0.4601 - auc: 0.5000 - val_loss: 0.5069 - val_auc: 0.5000
Epoch 5/10
585/585 [==============================] - 1s 1ms/step - loss: 0.4871 - auc: 0.5000 - val_loss: 0.4679 - val_auc: 0.5000
Epoch 6/10
585/585 [==============================] - 1s 1ms/step - loss: 0.4748 - auc: 0.5000 - val_loss: 0.4341 - val_auc: 0.5000
Epoch 7/10
585/585 [==============================] - 1s 2ms/step - loss: 0.4763 - auc: 0.5000 - val_loss: 0.4290 - val_auc: 0.5000
Epoch 8/10
585/585 [==============================] - 1s 1ms/step - loss: 0.4932 - auc: 0.5000 - val_loss: 0.4367 - val_auc: 0.5000
Epoch 9/10
585/585 [==============================] - 1s 1ms/step - loss: 0.5291 - auc: 0.5000 - val_loss: 0.4290 - val_auc: 0.5000

------------------------

I'm on a Mac Os 10.14.6 and python 3.8.5. (everything else is the same; I cloned the env and copied the notebook)
With tf 2.3.2 auc and val_auc get updated correctly and loss is much better 
Epoch 1/10
585/585 [==============================] - 1s 1ms/step - loss: 0.4133 - auc: 0.6771 - val_loss: 0.3942 - val_auc: 0.7115
Epoch 2/10
585/585 [==============================] - 1s 1ms/step - loss: 0.3959 - auc: 0.6943 - val_loss: 0.3858 - val_auc: 0.7069
Epoch 3/10
585/585 [==============================] - 1s 1ms/step - loss: 0.3889 - auc: 0.6992 - val_loss: 0.3811 - val_auc: 0.6996
"
47131,Can't set empty layer name,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):2.4
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Get variable name like 'dense/kernel:0'
**Describe the expected behavior**
Get variable name like 'kernel:0'

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```python
import tensorflow as tf
dense = tf.keras.layers.Dense(16, name='')
dense(tf.ones([16, 16]))
dense.weights[0].name
```
https://colab.research.google.com/drive/1KBzEMwJtMsgSoIfUieLBV9_5iLKmik46?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47130,Different Tensorflow Performance between Linux and Windows10,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NO
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):tensorflow 2.3.0
- Python version:3.7.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1 
- GPU model and memory:Linux:11G 1080TI  Windows:6G 1060

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I use the same python environment and tensorflow version in my pc which used windows10 trained a model, and I use model.save_weights to save the model, when i back to Linux system use load_weights and the same model arch to resume but get different performance. 
Also I tried use same code and same hyparameter to train model, they get different results.
I am very confused.
**Describe the expected behavior**
I expected the same results.
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47129,Does tf.keras.callbacks.Callback work with 3 Dimensions?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Google Colab, OS unknown.
- TensorFlow installed from (source or binary): ?
- TensorFlow version: 2.0 (tf-gpu)
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): ?
- GCC/Compiler version (if compiling from source): ?
- CUDA/cuDNN version: CUDA 11.2
- GPU model and memory: Tesla P100-PCIE w/ 16 GB



**Describe the problem**

I already documented everything here:  https://github.com/ayulockin/deepimageinpainting/issues/3
My last answer from 02/09/21 is my current state.

My plan was to inpaint on grayscale images by adapting their architecture: https://github.com/ayulockin/deepimageinpainting

(!) The current problem is indeed the grayscale instead of the 16-bit scale which I thought earlier. (!)

First I adapted their notebook ""Inpainting Partial Convolution"" with the training data from MNIST (60000,28,28)
MNIST is 8-bit.

The results from the fit_generator() was an error:

```python 
----> 7                      PredictionLogger()])
[...]                            6 frames
--> 565                            'with shape ' + str(data_shape))
ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (32, 28, 28) 
```

I tried the same with my own training dataset called VT_1000 (1000, 256, 256).
Also grayscale, but 16-bit. 
As I said before: This is not the problem due to normalization.

The result from fit_generator() was kind of the same:

```python 
----> 7                      PredictionLogger()])
[...]                            6 frames
--> 565                            'with shape ' + str(data_shape))
ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (32, 256, 256)
```




**Provide the exact sequence of commands / steps that you executed before running into the problem**




PredictionLogger is a class with tf inheritance so I assumed the problem to be here. 

Key lines:

```python

%%capture
!pip install tensorflow-gpu==2.0

!pip install wandb -q
!pip install --upgrade wandb

import wandb
from wandb.keras import WandbCallback

!wandb login ab4d648da3c296fa678ab39761b8a37e28efec62
wandb.init(entity='svsz', project=""MNIST"")


import tensorflow as tf
from tensorflow import keras

import cv2
import numpy as np




(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

maxElement_train, maxElement_test  = np.amax(x_train), np.amax(x_test)
maxElement = np.amax(np.array(maxElement_train, maxElement_test))

format = len(x_train[0][0])
formatxy = len(x_train[0][0]), len(x_train[0][0])
formatxyz = len(x_train[0][0]), len(x_train[0][0]), 1



## Ref: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly.
class createAugment(keras.utils.Sequence):
  'Generates data for Keras'
  def __init__(self, X, y, batch_size=32, dim=(formatxy), shuffle=True):
      'Initialization'
      self.batch_size = batch_size 
      self.X = X 
      self.y = y
      self.dim = dim
      self.shuffle = shuffle
      
      self.on_epoch_end()
 
  def __len__(self):
      'Denotes the number of batches per epoch'
      return int(np.floor(len(self.X) / self.batch_size))

  def __getitem__(self, index):
      'Generate one batch of data'
      # Generate indexes of the batch
      indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

      # Generate data
      X_inputs, y_output = self.__data_generation(indexes)
      return X_inputs, y_output

  def on_epoch_end(self):
      'Updates indexes after each epoch'
      self.indexes = np.arange(len(self.X))
      if self.shuffle:
          np.random.shuffle(self.indexes)

  def __data_generation(self, idxs):
    # Masked_images is a matrix of masked images used as input
    Masked_images = np.empty((self.batch_size, self.dim[0], self.dim[1])) # Masked image
    # Mask_batch is a matrix of binary masks used as input
    Mask_batch = np.empty((self.batch_size, self.dim[0], self.dim[1])) # Binary Masks
    # y_batch is a matrix of original images used for computing error from reconstructed image
    y_batch = np.empty((self.batch_size, self.dim[0], self.dim[1])) # Original image
    

    ## Iterate through random indexes
    for i, idx in enumerate(idxs):
      image_copy = self.X[idx].copy()
  
      ## Get mask associated to that image
      masked_image, mask = self.__createMask(image_copy)
      
      Masked_images[i,] = masked_image/maxElement
      Mask_batch[i,] = mask/maxElement
      y_batch[i] = self.y[idx]/maxElement

    ## Return mask as well because partial convolution require the same.
    return [Masked_images, Mask_batch], y_batch

  def __createMask(self, img):
    ## Prepare masking matrix
    mask = np.full((formatxy), maxElement, np.uint8) ## White background
    for _ in range(np.random.randint(1, 5)):
      # Get random x locations to start line
      x1, x2 = np.random.randint(1, format), np.random.randint(1, format)
      # Get random y locations to start line
      y1, y2 = np.random.randint(1, format), np.random.randint(1, format)
      # Get random thickness of the line drawn
      thickness = np.random.randint((format/28), (format/14))
      # Draw black line on the white mask
      cv2.line(mask,(x1,y1),(x2,y2),(0,0,0),thickness)

    ## Mask the image
    masked_image = img.copy()
    masked_image[mask==0] = maxElement

    return masked_image, mask



## Prepare training and testing mask-image pair generator
traingen = createAugment(x_train, x_train)
testgen = createAugment(x_test, x_test, shuffle=False)



!git clone https://github.com/ayulockin/deepimageinpainting.git
%cd deepimageinpainting/



## utils is present in the cloned repo. Visit repo for the implementation of PConv2D.
from utils.pconv_layer import PConv2D



## For more information into formulation: https://www.youtube.com/watch?v=AZr64OxshLo
## Metric
def dice_coef(y_true, y_pred):
    y_true_f = keras.backend.flatten(y_true)
    y_pred_f = keras.backend.flatten(y_pred)
    intersection = keras.backend.sum(y_true_f * y_pred_f)
    return (2. * intersection) / (keras.backend.sum(y_true_f + y_pred_f))




class InpaintingModel:
  '''
  Build UNET like model for image inpaining task.
  '''
  def prepare_model(self, input_size=(formatxyz)):
    input_image = keras.layers.Input(input_size)
    input_mask = keras.layers.Input(input_size, name='encoder_input')
  
    conv1, mask1, conv2, mask2 = self.__encoder_layer(32, input_image, input_mask, ['conv1', 'conv2'])
    conv3, mask3, conv4, mask4 = self.__encoder_layer(64, conv2, mask2, ['conv3', 'conv4'])
   # conv5, mask5, conv6, mask6 = self.__encoder_layer(128, conv4, mask4, ['conv5', 'conv6'])
   # conv7, mask7, conv8, mask8 = self.__encoder_layer(256, conv6, mask6, ['conv7', 'encoder_output'])

  #  conv9, mask9, conv10, mask10 = self.__decoder_layer(256, 128, conv8, mask8, conv7, mask7, ['conv9', 'conv10'])
  #  conv11, mask11, conv12, mask12 = self.__decoder_layer(128, 64, conv10, mask10, conv5, mask5, ['conv11', 'conv12'])
    conv13, mask13, conv14, mask14 = self.__decoder_layer(64, 32, conv4, mask4, conv3, mask3, ['conv13', 'conv14'])
    conv15, mask15, conv16, mask16 = self.__decoder_layer(32, 3, conv14, mask14, conv1, mask1, ['conv15', 'decoder_output'])

    outputs = keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(conv16)

    return keras.models.Model(inputs=[input_image, input_mask], outputs=[outputs])
    
  def __encoder_layer(self, filters, in_layer, in_mask, names):
    conv1, mask1 = PConv2D(32, (3,3), strides=1, padding='same', name=names[0])([in_layer, in_mask])
    conv1 = keras.activations.relu(conv1)

    conv2, mask2 = PConv2D(32, (3,3), strides=2, padding='same', name=names[1])([conv1, mask1])
    # conv2 = keras.layers.BatchNormalization()(conv2, training=True)gis
    conv2 = keras.activations.relu(conv2)

    return conv1, mask1, conv2, mask2

  def __decoder_layer(self, filter1, filter2, in_img, in_mask, share_img, share_mask, names):
    up_img = keras.layers.UpSampling2D(size=(2,2))(in_img)
    up_mask = keras.layers.UpSampling2D(size=(2,2))(in_mask)
    concat_img = keras.layers.Concatenate(axis=3)([share_img, up_img])
    concat_mask = keras.layers.Concatenate(axis=3)([share_mask, up_mask])

    conv1, mask1 = PConv2D(filter1, (3,3), padding='same', name=names[0])([concat_img, concat_mask])
    conv1 = keras.activations.relu(conv1)

    conv2, mask2 = PConv2D(filter2, (3,3), padding='same', name=names[1])([conv1, mask1])
    # conv2 = keras.layers.BatchNormalization()(conv2)
    conv2 = keras.activations.relu(conv2)

    return conv1, mask1, conv2, mask2




keras.backend.clear_session()
model = InpaintingModel().prepare_model()
model.summary()
model.compile(optimizer='adam', loss='mean_absolute_error', metrics=[dice_coef])



class PredictionLogger(tf.keras.callbacks.Callback):
    def __init__(self):
        super(PredictionLogger, self).__init__()

    def on_epoch_end(self, logs, epoch):
        sample_idx = 54
        [masked_images, masks], nomask = testgen[sample_idx]  
        
        m_images = []
        binary_masks = []
        predictions = []
     #  labels = []
        
        for i in range(32):
          inputs = [masked_images[i].reshape((1,)+masked_images[i].shape), masks[i].reshape((1,)+masks[i].shape)]
          impainted_image = model.predict(inputs)

          m_images.append(masked_images[i])
          binary_masks.append(masks[i])
          predictions.append(impainted_image.reshape(impainted_image.shape[1:]))
          labels.append(momasks[i])

        wandb.log({""masked_images"": [wandb.Image(m_image)
                              for m_image in m_images]})
        wandb.log({""masks"": [wandb.Image(mask)
                              for mask in binary_masks]})
        wandb.log({""predictions"": [wandb.Image(inpainted_image)
                              for inpainted_image in predictions]})
      # wandb.log({""labels"": [wandb.Image(label)
                          #   for label in labels]})




_ = model.fit_generator(traingen, validation_data=testgen, 
          epochs=20, 
          steps_per_epoch=len(traingen), 
          validation_steps=len(testgen),
          use_multiprocessing=False,
          callbacks=[WandbCallback(),
                     PredictionLogger()])

```

_______________________________________________________________________________
_______________________________________________________________________________


### My main question is: Do I need to ""reshape"" my traingen and testgen inside of _class createAugment(keras.utils.Sequence)_ to include the single-color band into an extra list to create or emulate a ""4th color-dimension"" or... is there an easy or quick way to make Callback work with 3 dimensions?


_______________________________________________________________________________
_______________________________________________________________________________


**Any other info / logs**





Expanding the 6 frames from the Error Log:


```python
Epoch 1/20
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-69-7c83ff6d9254> in <module>()
      5           use_multiprocessing=False,
      6           callbacks=[WandbCallback(),
----> 7                      PredictionLogger()])

6 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1295         shuffle=shuffle,
   1296         initial_epoch=initial_epoch,
-> 1297         steps_name='steps_per_epoch')
   1298 
   1299   def evaluate_generator(self,

/usr/local/lib/python3.6/dist-packages/wandb/integration/keras/keras.py in new_generator(*args, **kwargs)
    109             for cbk in cbks:
    110                 set_wandb_attrs(cbk, val_data)
--> 111         return old_generator(*args, **kwargs)
    112 
    113     def new_v2(*args, **kwargs):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
    263 
    264       is_deferred = not model._is_compiled
--> 265       batch_outs = batch_function(*batch_data)
    266       if not isinstance(batch_outs, list):
    267         batch_outs = [batch_outs]

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
    971       outputs = training_v2_utils.train_on_batch(
    972           self, x, y=y, sample_weight=sample_weight,
--> 973           class_weight=class_weight, reset_metrics=reset_metrics)
    974       outputs = (outputs['total_loss'] + outputs['output_losses'] +
    975                  outputs['metrics'])

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)
    251   x, y, sample_weights = model._standardize_user_data(
    252       x, y, sample_weight=sample_weight, class_weight=class_weight,
--> 253       extract_tensors_from_dataset=True)
    254   batch_size = array_ops.shape(nest.flatten(x, expand_composites=True)[0])[0]
    255   # If `model._distribution_strategy` is True, then we are in a replica context

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2470           feed_input_shapes,
   2471           check_batch_axis=False,  # Don't enforce the batch size.
-> 2472           exception_prefix='input')
   2473 
   2474     # Get typespecs for the input data and sanitize it if necessary.

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    563                            ': expected ' + names[i] + ' to have ' +
    564                            str(len(shape)) + ' dimensions, but got array '
--> 565                            'with shape ' + str(data_shape))
    566         if not check_batch_axis:
    567           data_shape = data_shape[1:]

ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (32, 256, 256)
```



"
47126,Tensorflow Custom Metric: SensitivityAtSpecificity failed during model fitting,"**System information**
- MacOS
- TensorFlow installed from conda
- TensorFlow version 2.0.0
- Python version: 3.6

**Describe the current behavior**
# Background
The metric for my machine learning task is `weight TPR = 0.4 * TPR1 + 0.3 * TPR2 + 0.3 * TPR3`. Generally, it asks for a model with higher recall rate while disturbing less negative samples. 

Some terminology:
> - TPRTrue Positive Rate, Sensitivity) : TPR = TP /TP + FN
> - FPRFalse Positive Rate, 1 - Specificity: FPR = FP /FP + TN
> - TPFNFPTN stands for True Positive, False Negative, Fasle Positive and True Negative. 
> - TPR1TPR at FPR = 0.001
> - TPR2TPR at FPR = 0.005
> - TPR3TPR at FPR = 0.01

# My attempt

Since keras does not have such metric, we need to write our own custome metric. Another word for mention, unlike in lightgbm and xgboost, custom metric in `keras` is not straight-foward because training process are on tensors instead of pandas/numpy arrays.

In lightgbm/Xgboost, I have this `wtpr` custom metric, and it works fine:
```
def tpr_weight_funtion(y_true,y_predict):
    d = pd.DataFrame()
    d['prob'] = list(y_predict)
    d['y'] = list(y_true)
    d = d.sort_values(['prob'], ascending=[0])
    y = d.y
    PosAll = pd.Series(y).value_counts()[1]
    NegAll = pd.Series(y).value_counts()[0]
    pCumsum = d['y'].cumsum()
    nCumsum = np.arange(len(y)) - pCumsum + 1
    pCumsumPer = pCumsum / PosAll
    nCumsumPer = nCumsum / NegAll
    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]
    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]
    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]
    return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3
```


In keras, I write a custom metric below. It works with regular tensor input, but it failed during **model fitting with batch Gradient descent**:
```
import keras.backend as K
def keras_wtpr_metric(y_true, y_predict):
    n = y_predict.shape[0]
    
    a = tf.dtypes.cast(y_predict, tf.float32)
    b = tf.dtypes.cast(y_true, tf.float32)
    a = tf.reshape(a,shape = [-1])
    b = tf.reshape(b,shape = [-1])
    d = tf.stack([a,b], axis = 0)
    d = tf.gather(d, tf.argsort(a,direction='DESCENDING'), axis = 1)
    PosAll = tf.math.reduce_sum(b, axis = -1) # the number of positive samples
    NegAll = tf.math.reduce_sum(1-b, axis = -1) # the number of negative samples
    pCumsum = tf.math.cumsum(d[1]) # TP
    pCumsum = tf.dtypes.cast(pCumsum,dtype = tf.float32)
    nCumsum = tf.range(0,n,dtype = tf.float32) - pCumsum + 1 # FP
    pCumsumPer = pCumsum / PosAll # tpr
    nCumsumPer = nCumsum / NegAll # fpr
    TR1 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.001))]
    TR2 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.005))]
    TR3 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.01))]
    return tf.reduce_sum(0.4*TR1+0.3*TR2+0.3*TR3)
```

My model is :
```
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(load_breast_cancer().data, load_breast_cancer().target,test_size = 0.3)

model = keras.models.Sequential([
# I have a tabular data
    keras.layers.Dense(256, activation='relu',input_shape = (x_train.shape[1],)), 
    keras.layers.Dense(64, activation = 'relu'),
    keras.layers.Dense(1, activation = 'sigmoid')
])
model.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = [keras_wtpr_metric])
# it seems can not work under batch training, I don't know why
model.fit(x=x_train, y= y_train, batch_size = 2048, epochs = 30,validation_data = [x_test,y_test]) 
```
Error message is 
``` Train on 398 samples, validate on 171 samples
Epoch 1/30
398/398 [==============================] - 1s 2ms/sample
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-639-da481d44d615> in <module>
      5 ])
      6 model.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = [keras_wtpr_metric])
----> 7 model.fit(x=x_train, y= y_train, batch_size = 2048, epochs = 30,validation_data = [x_test,y_test]) # it seems can not work under batch training, I don't know why

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87 
     88   return execution_function

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    518         # Lifting succeeded, so variables are initialized and we can run the
    519         # stateless function.
--> 520         return self._stateless_fn(*args, **kwds)
    521     else:
    522       canon_args, canon_kwds = \

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   1821     """"""Calls a graph function specialized to the inputs.""""""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 
   1825   @property

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-> 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-> 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError:  Incompatible shapes: [0] vs. [398]
	 [[node metrics/keras_wtpr_metric/sub_1 (defined at /Users/travis/opt/anaconda3/envs/envpython36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_681042]

Function call stack:
distributed_function
```
# My question 

1. How to write a weighted SensitivityAtSpecificity in keras?
2. Why my `keras_wtpr_metric` failed?

# Some Useful Sources:
1. https://keras.io/api/metrics/#creating-custom-metrics
2. https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SensitivityAtSpecificity



**Describe the expected behavior**
My custom metric should work fine.

**Standalone code to reproduce the issue**
```
# model
import numpy as np
import pandas as pd
from scipy import stats
import lightgbm as lgbm
from sklearn.model_selection import train_test_split
from sklearn.metrics import auc, roc_auc_score
import tensorflow as tf
import tensorflow.keras as keras
import keras.backend as K
def keras_wtpr_metric(y_true, y_predict):
    n = y_predict.shape[0]
    
    a = tf.dtypes.cast(y_predict, tf.float32)
    b = tf.dtypes.cast(y_true, tf.float32)
    a = tf.reshape(a,shape = [-1])
    b = tf.reshape(b,shape = [-1])
    d = tf.stack([a,b], axis = 0)
    d = tf.gather(d, tf.argsort(a,direction='DESCENDING'), axis = 1)
    PosAll = tf.math.reduce_sum(b, axis = -1) # the number of positive samples
    NegAll = tf.math.reduce_sum(1-b, axis = -1) # the number of negative samples
    pCumsum = tf.math.cumsum(d[1]) # TP
    pCumsum = tf.dtypes.cast(pCumsum,dtype = tf.float32)
    nCumsum = tf.range(0,n,dtype = tf.float32) - pCumsum + 1 # FP
    pCumsumPer = pCumsum / PosAll # tpr
    nCumsumPer = nCumsum / NegAll # fpr
    TR1 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.001))]
    TR2 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.005))]
    TR3 = pCumsumPer[tf.math.argmin(abs(nCumsumPer-0.01))]
    return tf.reduce_sum(0.4*TR1+0.3*TR2+0.3*TR3)
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(load_breast_cancer().data, load_breast_cancer().target,test_size = 0.3)

model = keras.models.Sequential([
# I have a tabular data
    keras.layers.Dense(256, activation='relu',input_shape = (x_train.shape[1],)), 
    keras.layers.Dense(64, activation = 'relu'),
    keras.layers.Dense(1, activation = 'sigmoid')
])
model.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = [keras_wtpr_metric])
# it seems can not work under batch training, I don't know why
model.fit(x=x_train, y= y_train, batch_size = 2048, epochs = 30,validation_data = [x_test,y_test]) 
```

"
47125,Model still getting updated even set the trainable flag to be False ,"I have a trained DNN `model`, and I hope to find the input `x` that maximize `model.predict(x)` through gradient descent. Here's what I did:

```python
def minimize_output(model, input_dim):
  N = input_dim
  model.trainable = False
  model.compile(optimizer='adam', loss='mean_squared_error')
  
  x = tf.Variable( np.random.uniform(-1, 1, (1, N, 1)) , name='subset', dtype=tf.float32)

  x_sigmoid = tf.sigmoid(x)
  x_target = -model(x_sigmoid)

  optimizer = tf.train.AdamOptimizer()
  train = optimizer.minimize(x_target, var_list=[subset])

  init = tf.initialize_all_variables()

  with tf.Session() as session:
      session.run(init)
      for step in range(2000):
        session.run(train)
        print(""step"", step, ""target"", x_target.eval())
      subset_pred = session.run(x_sigmoid)
  return subset_pred.reshape(-1)

min_input = minimize_output(model, input_dim=N)
```

However, it seems that value of `model.predict(min_input)` is much smaller than `-x_target.eval()`, therefore I think model is still updating during the Session. I have no idea why that happen, I already set model to be `model.trainable = False` and `var_lst=[subset]` for the optimizer.

Anyone have any idea how can I freeze the model during the training session? Thanks a lot!!!
"
47122,Memory leak loading keras model in loop,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):2.2
- Python version:3.7

**Problem Statement**
Running the following code:
```
import tensorflow as tf
def process_start():
  model = tf.keras.models.load_model(r""/large/model/file"")
while True:
  process_start()
```
Crashes my VM after some time due to OOM. Takes about 40m with a 4gb VM.
![image](https://user-images.githubusercontent.com/57200935/107836994-45c6b000-6d5c-11eb-96f3-54b4278e815b.png)

I also get these messages:
```
Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f24557770d0> and <tensorflow.python.keras.layers.core.Dropout object at 0x7f2455771750>).
WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.
```

**Describe the expected behavior**
Should be able to run this in loops as needed.

**More info**
I saw from https://github.com/tensorflow/tensorflow/issues/24695 that _Keras Layers are numbered globally within the Python process_. My guess is that every time the model is loaded it's assigning some variables that don't get properly cleaned up.

I believe this is a bug. Is there a suggested workaround or advice here?
"
47120,Flex Delegate bazel build for C api,"Hi, A bit unsure as to what category this would fall under so Iv chosen miscellaneous.
There is no documentation regarding the usage of flex delegates for the C api
I have tried the command 

bazel build --config=android_arm64 --config=monolithic --cxxopt=--std=c++11 --define=with_select_tf_ops=true -c opt //tensorflow/lite/c:libtensorflowlite_c.so //tensorflow/lite/delegates/flex:delegate

however it fails, i can provide the logs if required.

What are the ways to go about building a libtensorflowlite_c.so file that support flex delegates for android?
Thank you
"
47119,installation issue in intel core2 cpu ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 20.0.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version:version 2.5.0
- Python version:3.7.9
- Installed using virtualenv? pip? conda? usig pip
- Bazel version (if compiling from source):Bazel version 3 .7.2
- GCC/Compiler version (if compiling from source):gcc version 9.3.0
- CUDA/cuDNN version:NA
- GPU model and memory:NA



**Describe the problem**
while installing tensorflow  2. 5.0 using source  in my old cpu (intel core 2 duo) i am getting error as :- attributeError: module 'tensorflow' has no attribute '__version__'  

how to solve this error ?




**Provide the exact sequence of commands / steps that you executed before running into the problem**
/python3.7 
/import tensorflow as tf
/print(tf.__version__)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute '__version__'




**Any other info / logs**
![error](https://user-images.githubusercontent.com/77532074/107810626-023b6880-6d93-11eb-887b-ead8f971e787.jpg)

to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47117,ImportError: numpy.core._multiarray_umath failed to import ImportError: numpy.core.umath failed to import 2021-02-12 11:32:51.847320: F tensorflow/python/lib/core/bfloat16.cc:714] Check failed: PyBfloat16_Type.tp_base != nullptr  Aborted (core dumped),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):2.4.0
- Python version:3.8.5
- Bazel version (if compiling from source):4.0.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.2/8.1.0
- GPU model and memory:GTX1660 Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Tensorflow not loading after installation.
**Describe the expected behavior**
`import tensorflow as tf`
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
`import tensorflow as tf
2021-02-12 11:48:37.037515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd
RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd
ImportError: numpy.core._multiarray_umath failed to import
ImportError: numpy.core.umath failed to import
2021-02-12 11:48:37.272212: F tensorflow/python/lib/core/bfloat16.cc:714] Check failed: PyBfloat16_Type.tp_base != nullptr 
Aborted (core dumped)`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


`pip freeze
absl-py==0.11.0
anyio==2.1.0
argon2-cffi==20.1.0
astunparse==1.6.3
async-generator==1.10
attrs==20.3.0
Babel==2.9.0
backcall==0.2.0
bleach==3.3.0
cachetools==4.2.1
certifi==2020.12.5
cffi==1.14.5
chardet==4.0.0
cloudpickle==1.6.0
cycler==0.10.0
decorator==4.4.2
defusedxml==0.6.0
dm-tree==0.1.5
entrypoints==0.3
flatbuffers==1.12
gast==0.3.3
google-auth==1.26.1
google-auth-oauthlib==0.4.2
google-pasta==0.2.0
grpcio==1.32.0
h5py==2.10.0
idna==2.10
ipykernel==5.4.3
ipython==7.20.0
ipython-genutils==0.2.0
ipywidgets==7.6.3
jedi==0.18.0
Jinja2==2.11.3
joblib==1.0.1
json5==0.9.5
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.11
jupyter-console==6.2.0
jupyter-core==4.7.1
jupyter-server==1.3.0
jupyterlab==3.0.7
jupyterlab-pygments==0.1.2
jupyterlab-server==2.2.0
jupyterlab-widgets==1.0.0
Keras-Preprocessing==1.1.2
kiwisolver==1.3.1
Markdown==3.3.3
MarkupSafe==1.1.1
matplotlib==3.3.4
mistune==0.8.4
nbclassic==0.2.6
nbclient==0.5.2
nbconvert==6.0.7
nbformat==5.1.2
nest-asyncio==1.5.1
notebook==6.2.0
numpy==1.19.5
oauthlib==3.1.0
opt-einsum==3.3.0
packaging==20.9
pandas==1.2.2
pandocfilters==1.4.3
parso==0.8.1
pexpect==4.8.0
pickleshare==0.7.5
Pillow==8.1.0
prometheus-client==0.9.0
prompt-toolkit==3.0.16
protobuf==3.14.0
ptyprocess==0.7.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycparser==2.20
Pygments==2.7.4
pyparsing==2.4.7
pyrsistent==0.17.3
python-dateutil==2.8.1
pytz==2021.1
pyzmq==22.0.3
qtconsole==5.0.2
QtPy==1.9.0
requests==2.25.1
requests-oauthlib==1.3.0
rsa==4.7
scikit-learn==0.24.1
scipy==1.6.0
Send2Trash==1.5.0
six==1.15.0
sklearn==0.0
sniffio==1.2.0
tensorboard==2.4.1
tensorboard-plugin-wit==1.8.0
tensorflow==2.4.0
tensorflow-estimator==2.4.0
tensorflow-probability==0.12.1
termcolor==1.1.0
terminado==0.9.2
testpath==0.4.4
threadpoolctl==2.1.0
tornado==6.1
traitlets==5.0.5
typing-extensions==3.7.4.3
urllib3==1.26.3
wcwidth==0.2.5
webencodings==0.5.1
Werkzeug==1.0.1
widgetsnbextension==3.5.1
wrapt==1.12.1`"
47116,Custom Model Data Cardinality Check Ambiguous,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.2/2.4.1
- Python version: 3.7.8

**Describe the current behavior**

Training Keras custom model when inputs are lists fail data cardinality check if leading axis is not batch. The code below works in 2.3.2 but not 2.4.1. An example of why you would not want a leading axis as batch is a per-batch weight or dynamic parameter (see example code). 

**Describe the expected behavior**

This worked in 2.3.2 but was changed. I read the release notes and cannot seem to understand why this behavior changed. I am unsure how to fix. Either a flag to remove this check, a way to change the input spec of model call, or clarification on how to allow inputs that do not have a leading batch axis would help me fix this problem. 

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

class MyModel(tf.keras.Model):    
    def call(self, inputs, training):
        # make a model with a batched input (x) and per-batch tensor (s)
        x = inputs[0]
        s = inputs[1]
        return tf.reduce_mean(x * s, axis=1)
    
m = MyModel()
x = np.ones((10, 2))
s = np.ones(1)
y = np.ones(10)

m.compile('sgd', loss='mean_squared_error')

# works as call
m([x, s])

# fails on TF 2.4.1
# succeeds on TF 2.3.2
m.train_on_batch(x=[x, s], y=y)
```

** Error Message**
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-1e85b8f34d3e> in <module>
     21 # fails on 2.4.1
     22 # succeeds on 2.3.2
---> 23 m.train_on_batch(x=[x, s], y=y)

~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)
   1723       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,
   1724                                                     y, sample_weight,
-> 1725                                                     class_weight)
   1726       self.train_function = self.make_train_function()
   1727       logs = self.train_function(iterator)

~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in single_batch_iterator(strategy, x, y, sample_weight, class_weight)
   1511     data = (x, y, sample_weight)
   1512 
-> 1513   _check_data_cardinality(data)
   1514   dataset = dataset_ops.DatasetV2.from_tensors(data)
   1515   if class_weight:

~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in _check_data_cardinality(data)
   1527           label, "", "".join(str(i.shape[0]) for i in nest.flatten(single_data)))
   1528     msg += ""Make sure all arrays contain the same number of samples.""
-> 1529     raise ValueError(msg)
   1530 
   1531 

ValueError: Data cardinality is ambiguous:
  x sizes: 10, 1
  y sizes: 10
Make sure all arrays contain the same number of samples.
```
"
47115,tf.keras.experimental.CosineDecay() get wrong step after tf.keras.Model.load_weight(),"**System information**
-- NVIDIA RTX 2080ti 11G x2 ( with NVLink )
-- Ubuntu 20.04 LTS
-- Python 3.8.5 ( gcc / g++ 9.3.0 )
-- CUDA Toolkit 11.1, cuDNN 8.0.4, TensorRT 7.2.2, NCCL 2.8.3
-- tensorflow 2.4.1 ( build form release )

**Describe the current behavior**
Keras Model load weights saved with `tf.keras.callback.ModelCheckpoint()` makes `tf.keras.experimental.CosineDecay()` get wrong step in `Model.fit()`.
Usually, this make `CosineDecay.__call__(step)` return 0 because step>decay_steps.
**Describe the expected behavior**
The step should compute by inital_batch when call Model.fit(), but not the step in ckpt."
47114,What is the best architecture that we can converted to coreml and run on ios?,"I would like to know which architecture achieve the best resaults?
For example efficientdet or mobilenetv3.
Also, the model from this architecture must be able to convert to coreml and run on iOS."
47113,Extend tensorflow/examples for raspberry_pi,Is there any plan to extend the TensorFlow/examples for raspberry_pi? Those which are shown at https://www.tensorflow.org/lite/examples are not all for raspberry_pi though ...
47111,TF Lite benchmark NOT_EQUAL node takes up a lot of time.,"Hi,

Tensorflow used for both model and benchmark: 2.3.1
Platform: Android

I've got an internal company model (therefore can't share it, sorry). 
I've run tflite benchmark model on it and got weird results. Namely the NOT_EQUAL op apparently takes 73 ms to run, which is big part of computation time, the problem is that it does not block running other nodes (they run between the 1st ms and 38th ms of computation) therefore I'm not sure why benchmark reports that this node runs for 73ms if it doesn't actually block other nodes. 

Screen of the report:
![perf_not_equal](https://user-images.githubusercontent.com/32575801/107762540-780ce900-6d2d-11eb-8c08-b30c3812f4cb.png)

As you can see the cast, expand_dims etc. nodes don't start after 73ms but rather instantaneously which is confusing because when it comes to the total running time of model these 73ms are counted and added to the X ms of rest of the model, therefore it reports the whole model running time to be 73+x ms. 

When it comes to the node it only takes one of the inputs to the model as the input to the node, nothing else, therefore in theory it shouldn't wait for anything (I assume it waits for something if it starts but doesn't block computation).

Could you please give me some help about what's going on here? Thanks. 
"
47110,TF Lite Benchmark randomly throws errors.,"Hi,

I'm using TF Lite benchmark to benchmark a model we use internally in company (therefore I can't share it, sorry). Unfortunately when trying to run the model with given command:

`
./benchmark_model --graph=models/fp-full.v3.tflite --input_layer=""in1,in2,in3,in4"" --input_layer_shape=""1,50:1:1:1,10"" input_layer_value_range=in1,30,30:in2,0,0:in3,0,0:in4,0,0""        
`

The benchmark randomly either runs correctly or throws the given error:

`ERROR: tensorflow/lite/kernels/range.cc:45 (start > limit && delta < 0) || (start < limit && delta > 0) was not true.
ERROR: Node number 198 (RANGE) failed to invoke.`

As you see in the command I've specified the range of values to single value (I've also tried other values and it always ended the same, with randomly throwing the error) therefore there should be nothing random when running the model, but still it randomly fails. 

Could you please give me some tips what might be causing this behaviour? 

The inputs to the model are as follows (checked in Netron):
**in1**: int32[1,1]
**in2**: int32[1]
**in3**: float32[1]
**in4**: float32[1,1]

I use Tensorflow 2.3.1 for both model and benchmark.
"
47109,Make Tensorflow 2.3 available through conda on macOS,"### System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): conda
- Python version: 3.7.9

### Describe the problem

`conda install tensorflow` installs version 2.0

Please, make tensorflow 2.3 available through conda.

Other platforms support 2.3, however macOS is still stuck at 2.0:
https://anaconda.org/anaconda/tensorflow
<img width=""202"" alt=""Screen Shot 2021-02-12 at 10 15 22"" src=""https://user-images.githubusercontent.com/73581880/107749654-396e3300-6d1b-11eb-85be-02a0472dc40a.png"">"
47107,"ConverterError: <unknown>:0: error: loc(callsite(callsite(""map/TensorArrayV2_1@__inference_call_func_18494"" at ""StatefulPartitionedCall@__inference_signature_wrapper_22648"") at ""StatefulPartitionedCall"")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal <unknown>:0: note: loc(""StatefulPartitionedCall""): called from","[saved_model.zip](https://github.com/tensorflow/tensorflow/files/5970743/saved_model.zip)
I am trying to convert this model to tflite. But getting above error. 
"
47106,ValueError: Data cardinality is ambiguous:   x sizes: 60000   y sizes: 10000 Please provide data which shares the same first dimension.,"
![Screenshot (58)](https://user-images.githubusercontent.com/60438445/107751007-d0021a80-6d42-11eb-9326-ed9af5c643e3.png)
![Screenshot (59)](https://user-images.githubusercontent.com/60438445/107751014-d395a180-6d42-11eb-802f-6c3313bf0999.png)
![Screenshot (60)](https://user-images.githubusercontent.com/60438445/107750981-c37dc200-6d42-11eb-80a6-b4fffe5e445d.png)
"
47105,Data cardinality is ambiguous:   x sizes: 60000   y sizes: 10000 Please provide data which shares the same first dimension.,"complete code here  [http://localhost:8888/notebooks/Desktop/minor%20proj/Untitled-Copy1.ipynb](url) 

working on mnist data set for handwriten recognition

"
47103,Snapshot Op Reader::MakeNestedDataset didn't handle the empty shard case properly (divide by 0 error),"Hi @frankchn, I'm using snapshot op for distributed training with parameter server strategy in which one of the workers didn't get data and produces snapshot folder with empty shard. However, for the next run I got an exception _Reader::MakeNestedDataset()_ as the shard was empty. I think the existing code was written with the assumption that shard won't be empty  (start_index % shard_dirs.size()) and caused divide by 0 error. Could you please confirm.

https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/core/kernels/data/experimental/snapshot_util.cc#L590"
47101,tf.All support as TFlite buildin op,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): PIP
- TensorFlow version (or github SHA if from source): 2.4.1


**Provide the text output from tflite_convert**
```
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: All, RandomUniform
Details:
tf.All {device = """", keep_dims = false}
tf.RandomUniform {device = """", seed = 0 : i64, seed2 = 0 : i64}
```

**Standalone code to reproduce the issue** 
I have a model in [Tensorflow.](https://github.com/TensorSpeech/TensorFlowTTS/blob/master/notebooks/TensorFlowTTS_Tacotron2_with_TFLite.ipynb)
If I use this 
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS,
tf.lite.OpsSet.SELECT_TF_OPS]
model is converted into TFLite but have Flex Delegate.

Flex delegate do not worked on edge board. 
Requesting to please provide tf.All and tf.RandomUniform support as buildin op.
Please suggest some solution for now. I know, Custom op is one way, but how to do ?
Can I expect the support for Ops soon or in tf-nightly versions?"
47100,NotFoundError:  Resource localhost/_AnonymousVar29/N10tensorflow22SummaryWriterInterfaceE does not exist.,"tf 2.4.1
```
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-10-92bfedd73a83> in <module>
      2 tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, update_freq='batch')
      3 
----> 4 model.fit(x, y, initial_epoch=20, epochs=25, callbacks=[tensorboard_callback])

~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-> 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830       new_tracing_count = self.experimental_get_tracing_count()

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    853       # In this case we have created variables on the first call, so we run the
    854       # defunned version which is guaranteed to never create variables.
--> 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    856     elif self._stateful_fn is not None:
    857       # Release the lock early so that multiple threads can perform the call

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2940       (graph_function,
   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
-> 2942     return graph_function._call_flat(
   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   2944 

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1916         and executing_eagerly):
   1917       # No tape is watching; skip to running the function.
-> 1918       return self._build_call_outputs(self._inference_function.call(
   1919           ctx, args, cancellation_manager=cancellation_manager))
   1920     forward_backward = self._select_forward_and_backward_functions(

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    553       with _InterpolateFunctionError(self):
    554         if cancellation_manager is None:
--> 555           outputs = execute.execute(
    556               str(self.signature.name),
    557               num_outputs=self._num_outputs,

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57   try:
     58     ctx.ensure_initialized()
---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

NotFoundError:  Resource localhost/_AnonymousVar29/N10tensorflow22SummaryWriterInterfaceE does not exist.
	 [[{{node cond/then/_0/batch_loss}}]] [Op:__inference_train_function_9310]

Function call stack:
train_function
```"
47099,tf.keras.metrics.Metric not returning actual result value over batches,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Microsoft Windows 10 Enterprise Version 10.0.18363 Build 18363
- TensorFlow installed from (source or binary): binary (pip install tensorflow)
- TensorFlow version : v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.7.6

**Describe the current behavior**
The `result()` method of the `tf.keras.metrics.Metric` class as defined [here](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric#result) is supposed to return the calculation over state variables. The values it returns seem to be affected by some kind of decorator over it that is not allowing to return of the actual value rather a generalized version of value considering the batch number

**Describe the expected behavior**
It should return what it is calculating without any transformations 

**Standalone code to reproduce the issue**
```
class AddAllOnes(tf.keras.metrics.Metric):
  """""" A simple metric that adds all the one's in current batch and suppose to return the total ones seen at every end of batch""""""
    def __init__(self, name=""add_all_ones"", **kwargs):
        super(AddAllOnes, self).__init__(name=name, **kwargs)
        self.total = self.add_weight(name=""total"", initializer=""zeros"")

    def update_state(self, y_true, y_pred, sample_weight=None):    
        self.total.assign_add(tf.cast(tf.reduce_sum(y_true), dtype=tf.float32))
        
    def result(self):
        print('')
        print('inside result...', self.total)
        return self.total

X_train = np.random.random((512, 8))
y_train = np.random.randint(0, 2, (512, 1))

K.clear_session()
model_inputs = Input(shape=(8,))
model_unit = Dense(256, activation='linear', use_bias=False)(model_inputs)
model_unit = BatchNormalization()(model_unit)
model_unit = Activation('sigmoid')(model_unit)
model_outputs = Dense(1, activation='sigmoid')(model_unit)
optim = Adam(learning_rate=0.001)
model = Model(inputs=model_inputs, outputs=model_outputs)
model.compile(loss='binary_crossentropy', optimizer=optim, metrics=[AddAllOnes()], run_eagerly=True)
model.fit(X_train, y_train, verbose=1, batch_size=32)
```

**Other info / logs**
Do you see the difference between the printed value within the result and the returned value on the epoch logs (except the first iteration everything is different)

E.g. In the first iteration the metric saw a total of 19 ones and so returned 19.0 in epoch logs however in the 5th iteration it is supposed to return 79 but returned 49 in epoch logs! 

```
inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=19.0>
 1/16 [>.............................] - ETA: 0s - loss: 0.6744 - add_all_ones: 19.0000
inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=35.0>

inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=53.0>

inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=61.0>

inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=79.0>
 5/16 [========>.....................] - ETA: 0s - loss: 0.7072 - add_all_ones: 49.4000
inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=93.0>

inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=108.0>

inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=120.0>
 8/16 [==============>...............] - ETA: 0s - loss: 0.7084 - add_all_ones: 71.0000
inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=135.0>

inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=149.0>

inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=158.0>
11/16 [===================>..........] - ETA: 0s - loss: 0.7074 - add_all_ones: 91.8182
inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=169.0>

inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=183.0>

inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=203.0>
14/16 [=========================>....] - ETA: 0s - loss: 0.7063 - add_all_ones: 111.7857
inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=220.0>

inside result... <tf.Variable 'total:0' shape=() dtype=float32, numpy=238.0>
16/16 [==============================] - 0s 17ms/step - loss: 0.7070 - add_all_ones: 133.0000
<tensorflow.python.keras.callbacks.History at 0x1d8213676c8>
```
"
47097,post-quantizing and loading a a trained model in keras using tflite issue,"**System information**
- google colab ( intended to be used on raspberry pi)
- TensorFlow 1.14
- python 2.7 ( i know its old and not supported but i have to use it) 
-

**Provide the text output from tflite_convert**
```
/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/convert.pyc in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)

ConverterError: See console for info.
2021-02-11 22:29:16.405546: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2
2021-02-11 22:29:16.424912: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 57 operators, 100 arrays (0 quantized)
2021-02-11 22:29:16.425598: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 57 operators, 100 arrays (0 quantized)
2021-02-11 22:29:16.427571: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 25 operators, 53 arrays (0 quantized)
2021-02-11 22:29:17.452115: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 24 operators, 52 arrays (0 quantized)
2021-02-11 22:29:17.452530: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 23 operators, 50 arrays (0 quantized)
2021-02-11 22:29:17.452823: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 23 operators, 50 arrays (0 quantized)
2021-02-11 22:29:17.453000: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 23 operators, 50 arrays (0 quantized)
2021-02-11 22:29:17.453320: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 16777216 bytes, theoretical optimal value: 16777216 bytes.
2021-02-11 22:29:17.453614: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MUL, SOFTMAX. Here is a list of operators for which you will need custom implementations: AddV2.
Traceback (most recent call last):
  File ""/usr/local/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MUL, SOFTMAX. Here is a list of operators for which you will need custom implementations: AddV2.

```

**Standalone code to reproduce the issue** 

# WHOLE MODEL
import tensorflow as tf
from tensorflow import lite
from tensorflow import keras
from keras.models import load_model
import os


model=keras.models.load_model('/content/drive/MyDrive/disease classification/AGAIN/end_model_acc1.h5')
allow_custom_ops=True 
#--enable_select_tf_ops
converter = tf.lite.TFLiteConverter.from_keras_model(model)

tfmodel = converter.convert()
open (""model.tflite"" , ""wb"") .write(tfmodel)


#loading quantized model and interprete it 


interpreter = tf.lite.Interpreter(model_content=tfmodel)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details= interpreter.get_output_details()
#interpreter.resize_tensor_input(input_details[0]['index'],(10, 224, 224, 3))
#interpreter.resize_tensor_input(output_details[0]['index'], (15, 224,224,3))
interpreter.allocate_tensors()
os.chdir(""/content/drive/MyDrive/disease classification/The End/"") 
!pwd


**Any other info / logs**
the model is used for image classification of 10 classes using CNN to be performed on the raspberry pi , i would appreciate any feedback regarding the process as i am new to this. thank you. "
47095,*system info* ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
47091,Calling tf.linalg.expm causes SIGSEGV. ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home (19041.789)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0-dev20201216
- Python version: 3.7.7
- CUDA/cuDNN version: 11.0.3/8.0.4.30
- GPU model and memory: RTX 3070, 8GB

Calling `tf.linalg.expm` causes the interpreter to exit. Attaching GDB reveals a SIGSEGV in thread 1. I have attached a demangled stack trace.
Calling `tf.linalg.expm` should return a tensor with the result.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
tf.linalg.expm(tf.constant([[1.0]]))
```
Passing larger matrices or the result of `tf.random.normal` doesn't change the behavior. 

[tf_stacktrace.txt](https://github.com/tensorflow/tensorflow/files/5967082/tf_stacktrace.txt)
"
47090,Loading of shuffle buffer occurs every epoch drastically increasing training time using tf.datasets,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 4.0.2
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 8.0.2.39
- GPU model and memory: 2x RTX 3090 w/24GB memory

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

Hi all,

Firstly, apologies if this is in the wrong place but I feel this is something that would be amazing to address if possible!
I am training a model using images stored in a directory and creating datasets with the tf.keras.preprocessing.image_dataset_from_directory() method. Images are set as 50 x 50 pixels, greyscale.
My training routine is lightning fast thanks to the RTX 3090 graphics card. However, there is substantial I/O bottlenecking when filling the shuffle buffer up. Is there a way to address this and make it faster?

I can see the shuffle buffer being reloaded every single Epoch which takes significant time.

Thanks!

**Describe the expected behavior**

Loading of the shuffle buffer only once to significantly decrease training time.

**Standalone code to reproduce the issue**

```

## create data sets 

        train_ds = tf.keras.preprocessing.image_dataset_from_directory(
          data_dir,
          validation_split=validation_split,
          subset=""training"",
          seed=seed,
          image_size=(img_height, img_width),
          batch_size=batch_size,
          color_mode='grayscale'
        )

        val_ds = tf.keras.preprocessing.image_dataset_from_directory(
          data_dir,
          validation_split=validation_split,
          subset=""validation"",
          seed=seed,
          image_size=(img_height, img_width),
          batch_size=batch_size,
          color_mode='grayscale'
        )

## create and compile model 
model = get_compiled_model()

## train network (call back simply saves epoch data if epoch result is better than previous)
        history = model.fit(
          train_ds,
          validation_data=val_ds,
          epochs=epochs,
          initial_epoch=offset,
          callbacks=callbacks,
          steps_per_epoch=None,
          validation_steps=None
        )
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47089,tf.keras.metric.Metric.reset_states() fails if state contains variables with rank greater than zero,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory
- TensorFlow installed from (source or binary): Google Colaboratory
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.9

**Describe the current behavior**
When I implement my own custom keras metric that has an internal weight of rank greater than zero (meaning, its shape has length greater than zero), calling the ``reset_states()`` method on an instance of this metric fails.

See example below. This is reproducible in colab. 

**Describe the expected behavior**
I would expect ``reset_metrics()`` not to fail.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf


class MyMetric(tf.keras.metrics.Metric):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.w = self.add_weight(name='w', shape=(1,))

    def update_state(self, w):
        self.w.assign(w)

    def result(self):
        return self.w

m = MyMetric()
m.reset_states()
```

**Other info / logs** 
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-86-2dd397d265f4> in <module>()
     14 
     15 m = MyMetric()
---> 16 m.reset_states()

3 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py in reset_states(self)
    251     when a metric is evaluated during training.
    252     """"""
--> 253     K.batch_set_value([(v, 0) for v in self.variables])
    254 
    255   @abc.abstractmethod

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in batch_set_value(tuples)
   3704   if ops.executing_eagerly_outside_functions():
   3705     for x, value in tuples:
-> 3706       x.assign(np.asarray(value, dtype=dtype(x)))
   3707   else:
   3708     with get_graph().as_default():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in assign(self, value, use_locking, name, read_value)
    889             (""Cannot assign to variable%s due to variable shape %s and value ""
    890              ""shape %s are incompatible"") %
--> 891             (tensor_name, self._shape, value_tensor.shape))
    892       assign_op = gen_resource_variable_ops.assign_variable_op(
    893           self.handle, value_tensor, name=name)

ValueError: Cannot assign to variable w:0 due to variable shape (1,) and value shape () are incompatible
```"
47087,NotImplementedError: Cannot convert a symbolic Tensor to a numpy array.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pacman -S python-tensorflow)
- TensorFlow version (use command below): 2.4.1
- Python version: 3.9.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- GPU model and memory: Nvidia Geforce GTX 1060 6GB

**Describe the current behavior**
```

2021-02-10 17:51:13.037468: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-10 17:51:13.037899: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-02-10 17:51:13.038418: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Traceback (most recent call last):
  File ""/run/media/volker/DATA/configruns/load/./test.py"", line 13, in <module>
    lstm = Bidirectional(lstm_nobi, name=""layerC"")(embedding_layer)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 539, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 951, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1090, in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 822, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 863, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 652, in call
    y = self.forward_layer(forward_inputs,
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 660, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent_v2.py"", line 1157, in call
    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 859, in _process_inputs
    initial_state = self.get_initial_state(inputs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 642, in get_initial_state
    init_state = get_initial_state_fn(
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 2506, in get_initial_state
    return list(_generate_zero_filled_state_for_cell(
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 2987, in _generate_zero_filled_state_for_cell
    return _generate_zero_filled_state(batch_size, cell.state_size, dtype)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 3003, in _generate_zero_filled_state
    return nest.map_structure(create_zeros, state_size)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/util/nest.py"", line 659, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/util/nest.py"", line 659, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 3000, in create_zeros
    return array_ops.zeros(init_state_size, dtype=dtype)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 2819, in wrapped
    tensor = fun(*args, **kwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 2868, in zeros
    output = _constant_if_small(zero, shape, dtype, name)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py"", line 2804, in _constant_if_small
    if np.prod(shape) < 1000:
  File ""<__array_function__ internals>"", line 5, in prod
  File ""/home/volker/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py"", line 3030, in prod
    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
  File ""/home/volker/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py"", line 87, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
  File ""/usr/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 852, in __array__
    raise NotImplementedError(
NotImplementedError: Cannot convert a symbolic Tensor (layerC/forward_layerB/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```

**Describe the expected behavior**

No NotImplementedError. [Here](https://stackoverflow.com/questions/66141547/notimplementederror-cannot-convert-a-symbolic-tensor-to-a-numpy-array?noredirect=1#comment116947834_66141547) someone claimed that with tf version 2.3.0, python 3.7.0, and numpy 1.19.2 it works. I also remember that code working about 10 months ago.


**Standalone code to reproduce the issue**

```
import numpy as np
from keras.layers import LSTM, Embedding, Input, Bidirectional

dim = 30
max_seq_length = 40
vecs = np.random.rand(45,dim)

input_layer = Input(shape=(max_seq_length,))
embedding_layer = Embedding(len(vecs), dim, weights=[vecs], input_length=max_seq_length, trainable=False, name=""layerA"")(input_layer)
lstm_nobi = LSTM(max_seq_length, return_sequences=True, activation=""linear"", name=""layerB"")
lstm = Bidirectional(lstm_nobi, name=""layerC"")(embedding_layer)
```

"
47086, SavedModel restored in Graph mode under MirroredStrategy crashes global_variables_initializer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux: colab.research.google.com and others
- TensorFlow installed from (source or binary): pre-installed on colab
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.9

**Describe the current behavior**

1) `tf.compat.v1.global_variables_initializer()` crashes with `TypeError: Expected tf.group() expected Tensor arguments not 'None'` after restoring a SavedModel in Graph mode under MirroredStrategy as in the code below.

2) Estimator does this kind of initialization automatically when used with `RunConfig(..., train_distribute=MirroredStrategy())`, so just fixing the example below won't help. See https://github.com/tensorflow/hub/issues/704 for user-level impact.

**Describe the expected behavior**

1) `tf.compat.v1.global_variables_initializer()` succeeds in this situation.

2) An Estimator `model_fn` can use `tf.saved_model.load()` without this crash when used with `RunConfig(..., train_distribute=MirroredStrategy())`.

**Standalone code to reproduce the issue**

[Also shared with Alphabet at https://colab.research.google.com/drive/1piVum7WJb_WlYhHbW6tYO7gWSzJeT7oO ]

```python
import tensorflow as tf

class Twice(tf.train.Checkpoint):
  def __init__(self):
    self._two = tf.Variable(2.0, name=""two"")

  @tf.function(input_signature=[tf.TensorSpec((None, None), tf.float32)])
  def __call__(self, x):
    return tf.multiply(x, self._two)

export_dir = ""/tmp/twice""
tf.saved_model.save(Twice(), export_dir)

strategy = tf.distribute.MirroredStrategy()
with tf.Graph().as_default() as g:
  with strategy.scope():
    obj = tf.saved_model.load(export_dir)

  for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES):
    prefix = ""NO INIT"" if v.initializer is None else ""ok init""
    print(prefix, v)

  init_op = tf.compat.v1.global_variables_initializer()
```

```
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
NO INIT <tf.Variable 'two:0' shape=() dtype=float32>
ok init MirroredVariable:{
  0: <tf.Variable 'two:0' shape=() dtype=float32>
}
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-7fd461f5dad0> in <module>()
      8     print(prefix, v)
      9 
---> 10   init_op = tf.compat.v1.global_variables_initializer()

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in group(*inputs, **kwargs)
   2921       if not hasattr(inp, ""device""):
   2922         raise TypeError(""Expected tf.group() expected Tensor arguments not ""
-> 2923                         ""'%s' with type '%s'"" % (inp, type(inp)))
   2924       dev = inp.device
   2925       if dev in ops_on_device:

TypeError: Expected tf.group() expected Tensor arguments not 'None' with type '<class 'NoneType'>'
```
"
47085,"KerasClassifier, GridSearchCV ignore with tf.device('cpu:0')","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.5
- CUDA/cuDNN version: CUDA v11.0.2/cuDNN v8.0.4.30
- GPU model and memory: Quadro M1200

**Describe the current behavior**
Using KerasClassifier in combination with GridSearchCV ignores if I force to use CPU computing instead of GPU using `with tf.device('cpu:0')`

**Describe the expected behavior**
TF and Keras libraries should use specified hardware (CPU or GPU) if it is inside the `with tf.device(DEVICE_NAME)`.

**Standalone code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

tf.debugging.set_log_device_placement(True)
X_train = np.random.rand(1000,7)
input_dim = X_train.shape[1]
epochs = [5, 10, 15]
batch_size = [10, 20, 30, 40]
param_grid = dict(epochs=epochs, batch_size=batch_size)

def create_sequential_model(activation='relu', dropout_rate=0.2, optimizer='SGD'):
    model = Sequential()
    model.add(Dense(4, input_dim=input_dim, activation=activation))
    model.add(Dropout(dropout_rate))
    model.add(Dense(2, activation=activation))
    model.add(Dense(4, activation=activation))
    model.add(Dense(7, activation='sigmoid'))
    model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])
    return model

with tf.device('cpu:0'): # This works and runs on CPU
    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
c = tf.matmul(a, b) # This works and runs on GPU
print(c)

with tf.device('cpu:0'): # This doesn't work
    model = KerasClassifier(build_fn=create_sequential_model, verbose=1) 
    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)
    grid_result = grid.fit(X_train, X_train) 
```

**Other info / logs**
Proof it is using GPU instead of CPU
![ErrorTFCPUGPU](https://user-images.githubusercontent.com/2531157/107632092-4da42880-6c66-11eb-91a0-d10e6bf514a6.png)

Please find the error displayed below (I know this error is because the parameter `n_jobs=-1` of GridSearchCV is not supported on tensorflow if using GPU)
`TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.`
"
47083,"No gradients provided for any variable, check your graph for ops that do not support gradients, between variables. Problem with tf.cast op","I am creating a model using tf v1 and I got this problem and I dont know how to solve it, any ideas? I think is due to tf.cast op, but i need to use it so if a number is < 1, it turns into 0

Error: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables

My code:

#Ejemplo para 4 datos: Location, Email, IMEI y Device ID: 
filtracion_location = tf.placeholder(tf.float32, name='location_input')
filtracion_email = tf.placeholder(tf.float32, name='email_input')
filtracion_imei = tf.placeholder(tf.float32, name='imei_input')
filtracion_device = tf.placeholder(tf.float32, name='device_input')

#objetivo: answer: 1  0
y_ = tf.placeholder(tf.float32, name='target')

#distintos pesos para cada dato: nombre= Px
Plocation = tf.Variable(9., name='Plocation')
Pemail = tf.Variable(8., name='Pemail')
Pimei = tf.Variable(3., name='Pimei')
Pdevice = tf.Variable(2., name='Pdevice')

#umbral de decisin
umbral = tf.Variable(10., name='umbral')

#variables auxiliares: 
y_aux1 = tf.Variable(0.)
y_aux2 = tf.Variable(0.)
y_aux3 = tf.Variable(0.)
y_aux4 = tf.Variable(0.)
y_aux5 = tf.Variable(0.)
y_aux6 = tf.Variable(0.)
y_aux7 = tf.Variable(0.)

#salida del modelo
y_aux1 = tf.add(tf.multiply(filtracion_location, Plocation), 0.0)
y_aux2 = tf.add(tf.multiply(filtracion_email, Pemail), y_aux1)
y_aux3 = tf.add(tf.multiply(filtracion_imei, Pimei), y_aux2)
y_aux4 = tf.add(tf.multiply(filtracion_device, Pdevice), y_aux3)

#conseguir que la salida sea 0  1
y_aux5 = tf.divide(y_aux4, umbral)
y_aux6 = tf.cast(y_aux5, tf.int16)
y_aux7 = tf.cast(y_aux6, tf.float32)


y = tf.div_no_nan(y_aux7, y_aux7)
y = tf.identity(y, name='output')

loss = tf.reduce_mean(tf.square(y - y_))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train_op = optimizer.minimize(loss, name='train')  ---> here is the error

init = tf.global_variables_initializer()

# Creating a tf.train.Saver adds operations to the graph to save and
# restore variables from checkpoints.

saver_def = tf.train.Saver().as_saver_def()

with open('graph_Ant.pb', 'wb') as f:
  f.write(tf.get_default_graph().as_graph_def().SerializeToString())
"
47081,Multiple GPU tests run redundantly,"Using TensorFlow 2.4.1

**Describe the problem**

I noticed that some GPU tests are run multiple times redundantly with apparently no need or benefit causing test times to increase.
Example: //tensorflow/core/kernels/mlir_generated:gpu_tanh_test, //tensorflow/core/kernels/mlir_generated:gpu_tanh_test_gpu

As you can see there is a gpu_* test and a gpu_*_gpu test

This happens because `tf_cuda_cc_test` rule is used which creates a CPU and a GPU test already. However as the test is meant for GPU already and has `tags = tf_cuda_tests_tags()` set, it will create 2 tests that are both run (only) on GPU

As current master has changed considerably I just did a quick check and found e.g.:
https://github.com/tensorflow/tensorflow/blob/c820e5278288773ef2298b1796164a9826d847cf/tensorflow/core/kernels/mlir_generated/BUILD#L332-L345

This follows the same pattern and should likely be revised. I quick fix would likely be: If the passed tags already contain `gpu` then the CPU test will not be added.
"
47080,TF Lite Micro person detection example won't make?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Python pip and git clone
- TensorFlow version: 2.4.1
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I installed the whole Tensorflow package using both pip and git clone on Ubuntu 20.04. When I tried to make the person detection project as follows:
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project

It errored out with no make rule. The whole log is:

_terryl@DESKTOP-55BFST5:/mnt/c/Projects/tensorflow$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project
--2021-02-10 22:34:31--  http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip
Resolving mirror.tensorflow.org (mirror.tensorflow.org)... 216.58.195.80
Connecting to mirror.tensorflow.org (mirror.tensorflow.org)|216.58.195.80|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1760478 (1.7M) [application/zip]
Saving to: /tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip

/tmp/dca12522a9f9e37f126ab925 100%[=================================================>]   1.68M  8.23MB/s    in 0.2s

2021-02-10 22:34:32 (8.23 MB/s) - /tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip saved [1760478/1760478]

Cloning into 'tensorflow/lite/micro/tools/make/downloads/pigweed'...
remote: Sending approximately 12.05 MiB ...
remote: Counting objects: 6, done
remote: Finding sources: 100% (6/6)
remote: Total 18665 (delta 8537), reused 18663 (delta 8537)
Receiving objects: 100% (18665/18665), 11.99 MiB | 5.09 MiB/s, done.
Resolving deltas: 100% (8537/8537), done.
Updating files: 100% (1456/1456), done.
Updating files: 100% (647/647), done.
Note: switching to '47268dff45019863e20438ca3746c6c62df6ef09'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at 47268dff pw_hdlc_lite: Client I/O improvements
warning: pw_presubmit/py/pw_presubmit/format_code.py has type 100644, expected 100755
warning: pw_presubmit/py/pw_presubmit/pigweed_presubmit.py has type 100644, expected 100755
make: *** No rule to make target 'generate_person_detection_esp_project'.  Stop._

I tried multiple times clean installing tensorflow but always the same problem. Am I missing something or the example project got broken somehow? I tried making hello_world project for ESP32, it worked fine.
Please help, thanks!
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47079,model.predict infer batch size from x if x is an array/tensor,"If you use a subclassed tf keras model, and write the call method, you may want to access the shape of the `inputs` argument. While training, this is possible. The shape would be [batch_size, width, height, channels] for a batch of images using a tf data dataset. However, when calling `model.predict(images)`, where images is an array [batch_size, width, height, channels], the shape would be [None, width, height, channels]. This is confusing, though can be fixed by setting the batch_size argument of predict.

I propose that when calling model.predict on tensors/arrays (not datasets), batch_size is inferred from the shape of the inputted array. I have implemented the code myself by overriding the predict function in a class extending tf.keras.Model like shown:

```
...
def predict(self,
                x,
                batch_size=None,
                verbose=0,
                steps=None,
                callbacks=None,
                max_queue_size=10,
                workers=1,
                use_multiprocessing=False
               ):
         return super(Autoencoder, self).predict(x, (batch_size or x.shape[0]), verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
...
```

I see no reason why the shape of `inputs` is not always known, so this seems like a reasonable request to me. I have not really checked, however, if `inputs.shape` is not set when not using a dataset to fit or evaluate, it would make sense that `inputs.shape` is populated there as well. It just doesn't make sense that the shape of `inputs` should ever have a None value in it when it is known."
47074,re-open: Docker Image -e Password:password tensorflow/tensorflow:latest-gpu-jupyter,"What is the password for jupyter in this docker image (tensorflow/tensorflow:latest-gpu-jupyter)?
-e Password:password is not working, it's not allow resetting the password.

I am reopening this issue since
the use of jupyter_notebook_config.py was removed in cae5763 and the PASSWORD env variable is now ignored.
thank you.



<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47071,Micro benchmarks is excluded for cortex_m_corstone_300_makefile.inc,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
I hit this issue when working with: https://github.com/tensorflow/tensorflow/pull/46830
Micro benchmarks had to be excluded.

**Please provide the exact sequence of commands/steps when you ran into the problem**
Try to run Micro benchmark for cortex_m_corstone_300_makefile.inc

"
47069,Add support for masking to the Keras Functional API,"**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

It appears that the function API does not support masking, meaning it cannot be used to make a *layer* that propagates mask from the input to to the output. Here is an example:

Suppose that `layer` is some `tf.keras` layer that supports masking. Then we can make a model using the function API:

    inputs = tf.keras.layers.Input(shape=(None,))
    outputs = layer(inputs)
    functional_layer = tf.keras.Model(inputs=inputs, outputs=outputs)

But it does not propogate the mask: If `x_emb` is a tensor with an attached mask (so it has an `x_emb._keras_mask` attribute) then consider the following

    output = functional_layer(x_emb)
    print(output._keras_layer)

    ## AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_layer'

Obviously subclassing `tf.keras.Model` supports masking, and so does the Sequential API:

    sequential_layer = tf.keras.Sequential([
        layer,
        layer
    ])
    output2 = sequential_layer(x_emb)
    print(output2._keras_mask)

**Will this change the current api? How?**

It depends on how it is implemented. 

**Who will benefit with this feature?**

Model developers. It will allow them to use the functional API to create layers which support masking.

**Any Other info.**

I posted a [question](https://stackoverflow.com/q/66092787/1349673) on SO about this."
47068,QAT model to TFLite strict int8 quantisation - big performance gap,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0-dev20201210

### 2. Code

```python
import numpy as np

import tensorflow as tf
print(""TENSORFLOW VERSION:"", tf.__version__)
from tensorflow.python.keras.engine.input_layer import Input
import tensorflow_model_optimization as tfmo

from tensorflow.keras.layers import Input, Conv2D
from tensorflow.keras.layers import ReLU
from tensorflow.keras import Model


def calc_mse(target, pred, dtype=""float""):

    if dtype==""float"":
        scale = 255 ** 2
    elif dtype==""int"":
        scale = 1
    else:
        raise NotImplementedError(""dtype must be float or int."")

    mse = tf.reduce_mean(tf.pow(target - pred, 2)) * scale
    return mse


def validate_tflite(interpreter, dataset):
    mse = 0

    for img, img in dataset:

        # Preprocess image
        input_details = interpreter.get_input_details()[0]
        scale, zero_point = input_details['quantization']
        tflite_integer_input = img / scale + zero_point
        tflite_integer_input = tf.cast(tflite_integer_input, input_details['dtype'])
        interpreter.set_tensor(input_details['index'], tflite_integer_input)

        interpreter.invoke()

        output_details = interpreter.get_output_details()[0]
        tflite_integer_output = interpreter.get_tensor(output_details['index'])

        tflite_integer_input = tf.cast(tflite_integer_input, ""float32"")
        tflite_integer_output = tf.cast(tflite_integer_output, ""float32"")

        mse += calc_mse(tflite_integer_input, tflite_integer_output, dtype=""int"")

    return mse / len(dataset)


if __name__ == ""__main__"":

    tfmodel_old = tf.keras.models.load_model(""simpleconv"")

    tfmodel = tf.keras.Sequential([
        Input((32, 32, 3)),
        Conv2D(128, 3, padding=""same"", name=""Conv1""),
        ReLU(name=""Act1""),
        Conv2D(3, 3, padding=""same"", name=""Conv2"")])
    
    tfmodel.set_weights(tfmodel_old.get_weights())


    cifar = tf.keras.datasets.cifar10
    (train_images, _), (test_images, _) = cifar.load_data()
    train_images = train_images[:1000].astype(np.float32) / 255.0
    test_images = test_images[:1000].astype(np.float32) / 255.0

    cifar_train = tf.data.Dataset.from_tensor_slices((train_images, train_images)).map(
        lambda x, y: (tf.expand_dims(x, axis=0), tf.expand_dims(y, axis=0)))

    tfmodel.compile(optimizer=""adam"", loss=calc_mse)
    mse = tfmodel.evaluate(cifar_train)

    print(f""tf savedmodel mse: {mse}"")

    # Finetune tensorflow model with QAT
    quantise_model = tfmo.quantization.keras.quantize_model
    qa_model = quantise_model(tfmodel)
    adam = tf.keras.optimizers.Adam(learning_rate=1e-6)
    qa_model.compile(optimizer=adam, loss=calc_mse)

    # NOTE: check mse here
    qa_model.fit(cifar_train)

    # Convert to TFLite and quantise
    converter = tf.lite.TFLiteConverter.from_keras_model(tfmodel)

    # Quantise to int8 without float fallback
    converter.optimizations = [tf.lite.Optimize.DEFAULT]

    def create_repr_data_gen(data):
            
        def representative_data_gen():
            for input_arr, target in data:
                yield [input_arr]
        return representative_data_gen

    converter.representative_dataset = create_repr_data_gen(cifar_train)
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8

    quantised_qa_model = converter.convert()
    print(""TFLite conversion done!"")

    # Validate TFLite model and check mse again
    interpreter = tf.lite.Interpreter(model_content=quantised_qa_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    print(input_details, ""\n"", output_details)
    mse_tflite = validate_tflite(interpreter, cifar_train)
    print(f""TFLite mse: {mse_tflite}"")
```

### 3. Failure after conversion

I have a pretrained tensorflow model (just a simple Conv2D + ReLU + Conv2D trained with MSE) that I load, do quantisation aware training on (with the CIFAR dataset), then quantise to int8 in tensorflow lite.

MSE of the original tensorflow model is 139.7, and MSE of the QAT model is 193.9, which is reasonable given that I'm doing QAT on a 1000 images only.
However, MSE of the int8 quantised tensorflow lite model is much higher at 1620.5. The representative dataset I provide for quantisation calibration is the same, so I don't understand how performance could degrade this much.

### 5. (optional) Any other info / logs

Below is the console output of the program:
```
2021-02-10 14:17:33.598712: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:33.598730: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
TENSORFLOW VERSION: 2.5.0-dev20201210
2021-02-10 14:17:34.496241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-02-10 14:17:34.520408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-10 14:17:34.520885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 625.94GiB/s
2021-02-10 14:17:34.520962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-10 14:17:34.521347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 1 with properties: 
pciBusID: 0000:02:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s
2021-02-10 14:17:34.521474: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:34.521603: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:34.521709: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:34.524210: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-02-10 14:17:34.524500: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-02-10 14:17:34.527516: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-02-10 14:17:34.527711: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:34.527835: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:34.527845: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-02-10 14:17:34.528075: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-02-10 14:17:34.528483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-10 14:17:34.528492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
2021-02-10 14:17:35.448465: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:127] None of the MLIR optimization passes are enabled (registered 2)
2021-02-10 14:17:35.449007: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3600000000 Hz
1000/1000 [==============================] - 1s 588us/step - loss: 140.2766
tf savedmodel mse: 139.69656372070312
1000/1000 [==============================] - 3s 3ms/step - loss: 193.8917
2021-02-10 14:17:40.852032: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2021-02-10 14:17:41.087654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-10 14:17:41.087979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-10 14:17:41.088209: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2
2021-02-10 14:17:41.088302: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2021-02-10 14:17:41.321745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-10 14:17:41.322082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 625.94GiB/s
2021-02-10 14:17:41.322177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-10 14:17:41.322633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 1 with properties: 
pciBusID: 0000:02:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s
2021-02-10 14:17:41.322751: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:41.322817: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:41.322874: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:41.322891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-02-10 14:17:41.322903: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-02-10 14:17:41.322915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-02-10 14:17:41.322968: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:41.323021: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:41.323032: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-02-10 14:17:41.323050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-10 14:17:41.323058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 1 
2021-02-10 14:17:41.323065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N N 
2021-02-10 14:17:41.323070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 1:   N N 
2021-02-10 14:17:41.324051: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:933] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.005ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.

2021-02-10 14:17:41.341282: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:330] Ignored output_format.
2021-02-10 14:17:41.341306: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:333] Ignored drop_control_dependency.
2021-02-10 14:17:41.345935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-10 14:17:41.346306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 625.94GiB/s
2021-02-10 14:17:41.346397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-10 14:17:41.346725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 1 with properties: 
pciBusID: 0000:02:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s
2021-02-10 14:17:41.346839: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:41.346902: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:41.346958: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:41.346974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-02-10 14:17:41.346987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-02-10 14:17:41.346999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-02-10 14:17:41.347050: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:41.347105: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib
2021-02-10 14:17:41.347115: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-02-10 14:17:41.347135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-10 14:17:41.347141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 1 
2021-02-10 14:17:41.347147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N N 
2021-02-10 14:17:41.347152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 1:   N N 
TFLite conversion done!
[{'name': 'input_1', 'index': 7, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'shape_signature': array([-1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.003921568859368563, 0), 'quantization_parameters': {'scales': array([0.00392157], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}] 
 [{'name': 'Identity', 'index': 8, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'shape_signature': array([-1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.007350581232458353, 30), 'quantization_parameters': {'scales': array([0.00735058], dtype=float32), 'zero_points': array([30], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
TFLite mse: 1620.45458984375
```

"
47067,TF 2.4 tf.data cached shuffle 35% slower compared to TF 2.3,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4, 2.5.0a20210210
- Python version: 3.8.5

**Describe the current behavior**
When running the code example below, TF 2.4 is 35% slower in the first epoch compared to TF 2.3 when shuffling is enabled. The epoch times for the second epoch are only slightly slower, so this issue only seems to occur when the shuffle buffer is not fully filled yet.

This issue doesn't seem to happen without shuffling, so I don't think this issue is related to a problem in GCS read performance. TF nightly partially fixed this issue, but is still ~10% slower compared to TF 2.3. 

version | 1st epoch | 1st epoch no shuffling
---|---|---
`v2.2.2` | 256.2 s |
`v2.3.2` | **249.0 s** | 234.7 s
`v2.4.1` | **334.7 s** | 230.6 s
`v2.5.0a20210210` | 276.0 s | 

**Describe the expected behavior**
The time of the first epoch should not be slower compared to TF 2.3.

**Standalone code to reproduce the issue**
```python
import time
import tensorflow_datasets as tfds

dataset = tfds.load(
    ""imagenet2012"",
    decoders={""image"": tfds.decode.SkipDecoding()},
    split=""train"",
    data_dir=""gs://my_data_bucket"",
)

samples = dataset.cardinality()  # 1281167
dataset = dataset.cache().shuffle(samples).batch(256).prefetch(1)

for epoch in range(2):
    start = time.time()
    for data in dataset:
        pass
    end = time.time()
    print(f""Epoch {epoch + 1}: {(end - start):.1f} s"")
```"
47066, Request for better installation instructions for TF on Ampere GPUs!,"After 6 weeks of trial and error, I gave up and replaced our brand new 3090 cards with the old 2080 Ti. **I just could not get tensorflow on the new GPUs to work.** 

My colleagues and I have worked with TF from its early days. We have always been early adopters of the latest GPU generation. **There was never a mess like this.** In the worst case, the new TF system didn't utilize all the GPU's new resources, but we never had installation troubles like this.

I tried all installation approaches mentioned in the TF home page (python packages, docker containers, compilation from source). Either it failed completely or - after hours/days on Stack Overflow - ended in disappointing results.

The closest I got to a success came with the upgrade to TF 2.4, but I was getting a million times the warning message 
**""your cuda software stack is old. we fallback to the nvidia driver for some compilation. update your cuda version to get the best performance. the ptxas error was: ptxas fatal : value 'sm_86' is not defined for option 'gpu-name'""**
at each start of a TF script.

This is not inspiring confidence in the coupling of TF and CUDA. **This is plainly unusable.** 

As I understand it, the CUDA toolkit is too old compared to the driver, so the driver is recompiled to match the CUDA version. **But why does this happen again and again?** Once should be enough. We have Ubuntu 20.04 with driver 460 installed. It should be possible to go back to version 450, but we could not achieve that. I don't know if this would help, anyway.

With Docker containers, there seem to be the same problems because they use the driver of the host system. 

Another word about working with containers because they are recommended in the installation instructions: We work with an IDE which offers Docker support only for extra cost. No IDE, no debugger (and other important tools). And like this, Docker is not an option. 

 



"
47065,please update the release with lib file assets,"according to the releases hitory, tf 1.15 has been updated to 1.15.5,  please upload the release lib and dll file the releases page, or www.tensorflow.com

It's so difficult to compile tensorflow correct. I flow the build introduction, try many times, use multi version of bazel, python, but always get error.
i just want the new version lib files, with security bugs fixed.
And i think i am not the only person, you can see a lot of devs submit compile error issue on this project.

so please, update the latest jar and dll, any third part source is ok."
