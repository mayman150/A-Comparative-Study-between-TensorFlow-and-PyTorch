Issue Number,Issue Title,Issue Body
52640,Typo in the model.compile code examples: tf.keras.optimizers not tf.keras.optimizer,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile

## Description of issue (what needs changing):
Error in the model.compile code examples: tf.keras.optimizers not tf.keras.optimizer
The code is misspelt. See link
<img width=""705"" alt=""Screen Shot 2021-10-23 at 10 58 08 PM"" src=""https://user-images.githubusercontent.com/93064122/138578511-289edad0-d236-4b5d-8ef8-a689b66ca2c4.png"">
.
"
52639,AttributeError: 'FlatMapDataset' object has no attribute 'bucket_by_sequence_length',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur Version 11.3.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): i don't know, just did pip3 install tensorflow a few month ago
- TensorFlow version (use command below): 2.5.0
- Python version: 3.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
See attached file.
<img width=""920"" alt=""Screen Shot 2021-10-23 at 10 51 44 PM"" src=""https://user-images.githubusercontent.com/93064122/138578322-c5a2f5a1-b84b-4592-a787-226fdd197a31.png"">

**Describe the expected behavior**
See below. It should function without error as described in the official documentation.
https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length
<img width=""562"" alt=""Screen Shot 2021-10-23 at 10 52 35 PM"" src=""https://user-images.githubusercontent.com/93064122/138578345-5aac8010-96b3-43fb-a84c-76c523373d9b.png"">"
52638,ValueError: Converting a list of object of custom class (ExtensionType) to tensor error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.8.0-dev20211023
- Python version:3.8.10
- Bazel version (if compiling from source):no
- GCC/Compiler version (if compiling from source):no
- CUDA/cuDNN version: nil
- GPU model and memory: nil

i have implemented a simple class in python extending from tf.exerimental.ExtensionType which stores three tensors into it. My goal is to create a list of these objects which then can be returned from a function after converting this list into tensor. 
I have tried creating a tf.Variable object , tf.TensorArray object , tf.constant obect , passed numpy object of this list into these function too but getting an error that i cannot convert to tensor or ""classname"" is not identified as proper tf.Dtype. 

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1nea-BQ6nRsbiAAKFMnmeKSeqAeHiq563?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
```
ValueError: Attempt to convert a value (Array(values=<tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 2, 3, 4, 5], dtype=int32)>, matched=<tf.Tensor: shape=(), dtype=bool, numpy=True>, iou=<tf.Tensor: shape=(), dtype=float32, numpy=53.46>)) with an unsupported type (<class '__main__.Array'>) to a Tensor.
```
"
52637,Periodic delays of TF inference time in batch process ,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.2.0, 2.6.0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2, 11.2
- GPU model and memory: GTX 960M

**Describe the current behavior**

```
mytime = datetime.now().strftime(""%m_%d_%Y_%H_%M_%S_%f"")
logFile.write((""Prediction Time Start          :{ptime}\n"").format(ptime=mytime))

prediction_cam1 = model.predict(my_image_arr_norm_expand)

mytime = datetime.now().strftime(""%m_%d_%Y_%H_%M_%S_%f"")
logFile.write((""Prediction Time End          :{ptime}\n"").format(ptime=mytime))
```

The graph below shows batch inference time of a group of images. X-Axis is image no and Y-axis is inference time in milliseconds. Average inference time 60-70 ms is OK for my project but these periodic delays which have inference time over 150 ms are not OK. At first I thought this may be related to program priority. So I changed its priority to real time priority in Windows 10(TF:2.2.0, CUDA:10.2) but it did not changed anything. There were still these periodic delays.

Then I changed OS to Ubuntu where I can modify kernel. I repeated same steps as I did in Windows 10 first. My steps were :

- Take log of prediction code.
- Take log of prediction code with niceness value -20.
- Changed linux kernel to low latency version
- Take log of prediction code with priority 40.
- Take log of prediction code with priority 80(change multiple pid values which had same name like 'python inference.py')
- Force garbage collector at the end of each cycle with gc.collect().

After all these steps periodic delays still exist. 

![image](https://user-images.githubusercontent.com/12881237/138571880-9dcb6ce8-f242-46de-a815-b12fbbd74afa.png)

Thanks to any help in advance."
52636,"supporting ""object"" type in tensorflow dtype","**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): yes

Currently one can create tf. Variable , tf.TensorArray or tf.ragged.constant with dtype related to numbers repr (float, int) , well string too. But will it not be beneficial to allow the user to create these variables containing objects of custom class implemented in python. In NumPy, we can do this by stating the dtype = ""object"" but TensorFlow isn't compatible with dtype of NumPy. Given that the user handles this variable with proper caution, it gives more flexibility.

**Will this change the current api? How?**
Not very sure

**Who will benefit with this feature?**
Tensorflow is mainly used to create models to deal with problems involving complex mathematical calculations/approximation, but given the huge popularity of tf , while implementing some other components of the model (which can be done under numpy too) there might be a case where, some sub-components can be forced to be implemented in tf , involving custom python class objects, thus making it impossible to run, unless one removes the dependency of using those object altogether.  
One use case might be, as described in the below colab notebook , while calculating mAP metric, the postprocessing function is called via test_step function of tf.Keras.model.Model class. for abstraction purposes, I am returning a list of objects of the custom class (Bounding Box) which will be used later in the mAP class. Currently, I am able to return NumPy array of this list where dtype = ""object"" and while running in eager mode this is running, but while running in graph mode , defun function wants me to return only tensor objects and I cant do it as the list contains objects of a type not supported by tf.
If someone is wondering why would you run post-processing in tf instead of NumPy, it becomes a necessity when this function is required to run metric calc in the validation step per epoch.

**Any Other info.**
please refer to this notebook, for a possible use case of the custom class object. This might seem totally unnecessary but yet on the other side provides really good abstraction and flexibility.
https://colab.research.google.com/drive/1Ei2t9coPNEVrfmejzaRUDIs-tm05EZFD?usp=sharing"
52630,quantization aware training issue,"Hi, I am training a U2Net model and want to use quantization aware training to reduce the size of the model. According to the official Docs regarding this, We need to import the library of tensorflow_model_optimization and use the quantize model function.

Issue - When i am using this, I am getting AttributeError: 'list' object has no attribute 'dtype'
This is my model declaration code-

quantize_model = tfmot.quantization.keras.quantize_model

net_input = Input(shape=(256,256,3)) 

model_output = U2NET(net_input)

model = Model(inputs = net_input, outputs = model_output)

qa_model = quantize_model(model)
lr = 1e-3

opt = tf.keras.optimizers.Adam(learning_rate = lr)

bce = BinaryCrossentropy()

qa_model.compile(optimizer = opt, loss = loss, metrics = None)

U2net function is the model declaration part.
I am able to get the normal model.summary()

I am trying to train the model of google colab gpu

Without QAT the model is training fine.

Please help
"
52629,"in tf2.6, predict_classes has disappeared broke the backward compatibility","y_proba(shape) (3, 30)
Traceback (most recent call last):
  File ""p297.py"", line 79, in <module>
    y_pred = model.predict_classes(X_new)
AttributeError: 'Sequential' object has no attribute 'predict_classes'


Since I transitioned from tensorflow 1.x to 2.6, the above method no longer work. It appears a poor job of maintaining backward compatibility. 

model=keras.models.Sequential()
...
y_proba = model.predict(X_new) # ok
y_pred = model.predict_classes(X_new) # broken

"
52627,"TypeError: To be compatible with tf.eager.defun, Python functions must return zero or more Tensors;","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8.10
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: cuda_11.2
- GPU model and memory: nvidia geforce rtx 2060 / 6gb

I am implementing a function which is calld by tf.keras model.fit function while running the validation dataset after every epoch, hence the fucntion will be runnig in graph mode. The problem is when i am reurning parameters from this function . I am getting a typeerror stating that 'i should return 0 or more tensors' , whereas currently I am returning a list. 
*Since the list contains numpy array and each np array is collection of custom class in python. I have tried converting this numpy array in tensor variable , tensorarray (dtype is a problem ) to no avail . Hence i am unable to figure out how should i return the Box objects in form of tensor.
*As this function is running in eager mode  **prefectly fine** , i am wondering is it a strict signature constraint to return tensors and not even numpy array from a function decorated under tf.fucntion decorator.

**Standalone code to reproduce the issue**  
https://colab.research.google.com/drive/1Ei2t9coPNEVrfmejzaRUDIs-tm05EZFD?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
```
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in convert(x)
    962         try:
--> 963           x = ops.convert_to_tensor_or_composite(x)
    964         except (ValueError, TypeError):

20 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor_or_composite(value, dtype, name)
   1688   return internal_convert_to_tensor_or_composite(
-> 1689       value=value, dtype=dtype, name=name, as_ref=False)
   1690 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_composite(value, dtype, name, as_ref)
   1727         as_ref=as_ref,
-> 1728         accepted_result_types=(Tensor, composite_tensor.CompositeTensor))
   1729 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)
    162           return func(*args, **kwargs)
--> 163       return func(*args, **kwargs)
    164 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1565     if ret is None:
-> 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1567 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)
     51   del as_ref  # Unused.
---> 52   return constant_op.constant(value, dtype, name=name)
     53 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    271   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 272                         allow_broadcast=True)
    273 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    289           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--> 290           allow_broadcast=allow_broadcast))
    291   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    563         ""Element type not supported in TensorProto: %s"" % numpy_dtype.name)
--> 564   append_fn(tensor_proto, proto_values)
    565 

tensorflow/python/framework/fast_tensor_util.pyx in tensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/compat.py in as_bytes(bytes_or_text, encoding)
     86     raise TypeError('Expected binary or unicode string, got %r' %
---> 87                     (bytes_or_text,))
     88 

TypeError: Expected binary or unicode string, got <__main__.BoundingBox object at 0x7f3f3b798a50>

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-119-e9ab93aab472> in <module>()
     29 
     30 if __name__ == ""__main__"":
---> 31     test_postprocess()

<ipython-input-119-e9ab93aab472> in test_postprocess()
     24         batch_labels = [np.random.uniform(0,1 , size = [1,13,13,3,7]).astype(np.float32) , np.random.uniform(0,1 , size = [1,26,26,3,7]).astype(np.float32) , np.random.uniform(0,1 , size = [1,52,52,3,7]).astype(np.float32)]
     25         # box_objects = post_process(batch_labels , anchors)
---> 26         box_objects = post_process(batch_labels , anchors)
     27         # print(box_objects)
     28 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    883 
    884       with OptionalXlaContext(self._jit_compile):
--> 885         result = self._call(*args, **kwds)
    886 
    887       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    931       # This is the first call of __call__, so we have to initialize.
    932       initializers = []
--> 933       self._initialize(args, kwds, add_initializers_to=initializers)
    934     finally:
    935       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    758     self._concrete_stateful_fn = (
    759         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 760             *args, **kwds))
    761 
    762     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   3064       args, kwargs = None, None
   3065     with self._lock:
-> 3066       graph_function, _ = self._maybe_define_function(args, kwargs)
   3067     return graph_function
   3068 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3461 
   3462           self._function_cache.missed.add(call_context_key)
-> 3463           graph_function = self._create_graph_function(args, kwargs)
   3464           self._function_cache.primary[cache_key] = graph_function
   3465 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3306             arg_names=arg_names,
   3307             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3308             capture_by_value=self._capture_by_value),
   3309         self._function_attributes,
   3310         function_spec=self.function_spec,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)
   1010       # TensorArrays and `None`s.
   1011       func_outputs = nest.map_structure(convert, func_outputs,
-> 1012                                         expand_composites=True)
   1013 
   1014       check_mutation(func_args_before, func_args, original_func)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    867 
    868   return pack_sequence_as(
--> 869       structure[0], [func(*x) for x in entries],
    870       expand_composites=expand_composites)
    871 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
    867 
    868   return pack_sequence_as(
--> 869       structure[0], [func(*x) for x in entries],
    870       expand_composites=expand_composites)
    871 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in convert(x)
    967               ""must return zero or more Tensors; in compilation of %s, found ""
    968               ""return value of type %s, which is not a Tensor."" %
--> 969               (str(python_func), type(x)))
    970       if add_control_dependencies:
    971         x = deps_ctx.mark_as_return(x)

TypeError: To be compatible with tf.eager.defun, Python functions must return zero or more Tensors; in compilation of <function post_process at 0x7f3f368f9b00>, found return value of type <class 'numpy.ndarray'>, which is not a Tensor.
```
"
52626,Cannot load back model with no-op Concatenate layer,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.9.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When I create a simple model with a dummy Concatenate layer (i.e. the concatenation receives one single element), I am able to save it successfully, but the subsequent model loading fails.

**Describe the expected behavior**

The model loading should finish without errors.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No
- Briefly describe your candidate solution(if contributing): N/A

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf

if __name__ == ""__main__"":
    input_layer = tf.keras.Input(shape=[100])
    dense_layer = tf.keras.layers.Dense(1)(input_layer)
    concatenate_layer = tf.keras.layers.Concatenate()([dense_layer])
    model = tf.keras.Model([input_layer], [concatenate_layer])
    model.compile(optimizer=""adam"", loss=""mean_absolute_error"")
    model.save(""model.h5"")
    loaded_model = tf.keras.models.load_model(""model.h5"")
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Full traceback:

```bash
Traceback (most recent call last):
  File ""/Users/stefan/workspace/tierra/bug.py"", line 10, in <module>
    loaded_model = tf.keras.models.load_model(""model.h5"")
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/saving/save.py"", line 200, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects,
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/saving/hdf5_format.py"", line 180, in load_model_from_hdf5
    model = model_config_lib.model_from_config(model_config,
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/saving/model_config.py"", line 52, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/layers/serialization.py"", line 208, in deserialize
    return generic_utils.deserialize_keras_object(
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/utils/generic_utils.py"", line 674, in deserialize_keras_object
    deserialized_obj = cls.from_config(
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/functional.py"", line 662, in from_config
    input_tensors, output_tensors, created_layers = reconstruct_from_config(
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/functional.py"", line 1283, in reconstruct_from_config
    process_node(layer, node_data)
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/functional.py"", line 1231, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 976, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 1114, in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 848, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 886, in _infer_output_signature
    self._maybe_build(inputs)
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 2659, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/utils/tf_utils.py"", line 259, in wrapper
    output_shape = fn(instance, input_shape)
  File ""/Users/stefan/workspace/tierra/.env/lib/python3.9/site-packages/keras/layers/merge.py"", line 489, in build
    raise ValueError('A `Concatenate` layer should be called '
ValueError: A `Concatenate` layer should be called on a list of at least 1 input.
```"
52623,missing Link to nightly version of BinaryCrossentropy,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy?version=nightly

The following links throws ""404"" error
https://github.com/keras-team/keras/tree/v2.8.0/keras/losses.py#L494-L593

https://github.com/keras-team/keras/tree/v2.8.0/keras/losses.py#L145-L155

https://github.com/keras-team/keras/tree/v2.8.0/keras/losses.py#L247-L252

https://github.com/keras-team/keras/tree/v2.8.0/keras/losses.py#L106-L143



## Description of issue (what needs changing):

Nightly doc link is throwing ""404"" error


"
52621,RuntimeError: Quantization not yet supported for op: Without OP specifier,"### 1. System information

Windows 10
Tensorflow 2.3.0rc0 (also tried 2.3.1)

### 2. Code

I am trying to convert my custom yolov3-tiny (with relu instead of leakyRelu) SavedModel to 8bit quantized tflite for edge tpu support with the following:

    import tensorflow as tf
    import numpy as np

    def representative_dataset_gen():
        for _ in range(250):
            yield [np.random.uniform(0.0, 1.0, size=(1, 416, 416, 3)).astype(np.float32)]

    model = tf.keras.models.load_model('yolo_relu_model')

    batch_size = 1
    input_shape = model.inputs[0].shape.as_list()
    input_shape[0] = batch_size
    func = tf.function(model).get_concrete_function(tf.TensorSpec(input_shape, model.inputs[0].dtype))
    converter = tf.lite.TFLiteConverter.from_concrete_functions([func])

    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_dataset_gen
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    converter.experimental_new_quantizer = True

    model_lite = converter.convert()
    f = open(""yolo_relu_model.tflite"", ""wb"")
    f.write(model_lite)
    f.close()


which leads to:

```
  File ""quantize.py"", line 37, in <module>
    quantize_model(""yolo_relu"")
  File ""quantize.py"", line 31, in quantize_model
    model_lite = converter.convert()
  File ""C:\venv\lib\site-packages\tensorflow\lite\python\lite.py"", line 1076, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""C:\venv\lib\site-packages\tensorflow\lite\python\lite.py"", line 900, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File ""C:\venv\lib\site-packages\tensorflow\lite\python\lite.py"", line 638, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""C:\venv\lib\site-packages\tensorflow\lite\python\lite.py"", line 452, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File ""C:\venv\lib\site-packages\tensorflow\lite\python\optimize\calibrator.py"", line 98, in calibrate_and_quantize       
    np.dtype(activations_type.as_numpy_dtype()).num)
RuntimeError: Quantization not yet supported for op:
```


I have also started  a colab [here](https://colab.research.google.com/gist/lukqw/fcdafa1381300ca6b53804b6fdffbb16/tensorflow-lite-debugger-colab.ipynb) which runs on tensorflow 2.6 (I cant seem to change the version), which produces a different output.


you can download the zip of my model [here](https://drive.google.com/file/d/1Ovj0aCeG7yhF6_7-piQHb88LV7CTLn0z/view?usp=sharing) and upload the zip inside colab.

In colab I get this output:

```
WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named ""keras_metadata.pb"" in the SavedModel directory.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.
WARNING:absl:For model outputs containing unsupported operations which cannot be quantized, the `inference_output_type` attribute will default to the original type.
```

When converting the tflite produced by colab with edgetpu_compiler I get the following result:

```
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.

Model compiled successfully in 1270 ms.

Input model: co_yolo_relu_model.tflite
Input size: 8.57MiB
Output model: co_yolo_relu_model_edgetpu.tflite
Output size: 8.61MiB
On-chip memory used for caching model parameters: 1.95MiB
On-chip memory remaining for caching model parameters: 15.75KiB
Off-chip memory used for streaming uncached model parameters: 6.54MiB
Number of Edge TPU subgraphs: 1
Total number of operations: 62
Operation log: co_yolo_relu_model_edgetpu.log

Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 22
Number of operations that will run on CPU: 40
See the operation log file for individual operation details.
Compilation child process completed within timeout period.
Compilation succeeded!
``` 
I'd like to have all operations on the Edge TPU (obviously), but I am kinda stumped here on how to proceed.

Can someone point me in the right direction on how to convert the model correctly (with tf 2.3.1 or others)?

Why is there no specifier on which op is not supported in the error message
`RuntimeError: Quantization not yet supported for op: `
?


This is the log of edgetpu_compiler:

```
Edge TPU Compiler version 16.0.384591198
Input: co_yolo_relu_model.tflite
Output: co_yolo_relu_model_edgetpu.tflite

Operator                       Count      Status

STRIDED_SLICE                  8          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
MUL                            6          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
DEQUANTIZE                     2          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
CONCATENATION                  4          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
CONCATENATION                  1          Mapped to Edge TPU
EXP                            2          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
CONV_2D                        13         Mapped to Edge TPU
QUANTIZE                       1          Mapped to Edge TPU
QUANTIZE                       4          More than one subgraph is not supported
QUANTIZE                       4          Operation is otherwise supported, but not mapped due to some unspecified limitation
RESIZE_NEAREST_NEIGHBOR        1          Mapped to Edge TPU
LOGISTIC                       6          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
MAX_POOL_2D                    6          Mapped to Edge TPU
RESHAPE                        2          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
ADD                            2          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
```"
52620,cannot train wiht custom dataset,"hello,
i wanted to try to generate images with stylegan2. i have an dataset with 4823 images. when I run !python /content/drive/MyDrive/stylegan2/run_training.py --num-gpus=1 --data-dir=/content/drive/MyDrive/stylegan2/dataset/ --config=config-f --dataset=eye --mirror-augment=true --metric=none --total-kimg=4823 --min-h=8 --min-w=8 --res-log2=8 --result-dir=""/content/drive/My Drive/stylegan2/results"" I get this output





Local submit - run_dir: /content/drive/My Drive/stylegan2/results/00002-stylegan2-eye-1gpu-config-f
dnnlib: Running training.training_loop.training_loop() on localhost...
Streaming data using training.dataset.TFRecordDataset...
Dataset shape = [3, 2048, 2048]
Dynamic range = [0, 255]
Label size    = 0
Constructing networks...
Setting up TensorFlow plugin ""fused_bias_act.cu"": Preprocessing... Loading... Done.
Setting up TensorFlow plugin ""upfirdn_2d.cu"": Preprocessing... Loading... Done.

G                               Params    OutputShape          WeightShape     
---                             ---       ---                  ---             
latents_in                      -         (?, 512)             -               
labels_in                       -         (?, 0)               -               
lod                             -         ()                   -               
dlatent_avg                     -         (512,)               -               
G_mapping/latents_in            -         (?, 512)             -               
G_mapping/labels_in             -         (?, 0)               -               
G_mapping/Normalize             -         (?, 512)             -               
G_mapping/Dense0                262656    (?, 512)             (512, 512)      
G_mapping/Dense1                262656    (?, 512)             (512, 512)      
G_mapping/Dense2                262656    (?, 512)             (512, 512)      
G_mapping/Dense3                262656    (?, 512)             (512, 512)      
G_mapping/Dense4                262656    (?, 512)             (512, 512)      
G_mapping/Dense5                262656    (?, 512)             (512, 512)      
G_mapping/Dense6                262656    (?, 512)             (512, 512)      
G_mapping/Dense7                262656    (?, 512)             (512, 512)      
G_mapping/Broadcast             -         (?, 18, 512)         -               
G_mapping/dlatents_out          -         (?, 18, 512)         -               
Truncation/Lerp                 -         (?, 18, 512)         -               
G_synthesis/dlatents_in         -         (?, 18, 512)         -               
G_synthesis/8x8/Const           32768     (?, 512, 8, 8)       (1, 512, 8, 8)  
G_synthesis/8x8/Conv            2622465   (?, 512, 8, 8)       (3, 3, 512, 512)
G_synthesis/8x8/ToRGB           264195    (?, 3, 8, 8)         (1, 1, 512, 3)  
G_synthesis/16x16/Conv0_up      2622465   (?, 512, 16, 16)     (3, 3, 512, 512)
G_synthesis/16x16/Conv1         2622465   (?, 512, 16, 16)     (3, 3, 512, 512)
G_synthesis/16x16/Upsample      -         (?, 3, 16, 16)       -               
G_synthesis/16x16/ToRGB         264195    (?, 3, 16, 16)       (1, 1, 512, 3)  
G_synthesis/32x32/Conv0_up      2622465   (?, 512, 32, 32)     (3, 3, 512, 512)
G_synthesis/32x32/Conv1         2622465   (?, 512, 32, 32)     (3, 3, 512, 512)
G_synthesis/32x32/Upsample      -         (?, 3, 32, 32)       -               
G_synthesis/32x32/ToRGB         264195    (?, 3, 32, 32)       (1, 1, 512, 3)  
G_synthesis/64x64/Conv0_up      2622465   (?, 512, 64, 64)     (3, 3, 512, 512)
G_synthesis/64x64/Conv1         2622465   (?, 512, 64, 64)     (3, 3, 512, 512)
G_synthesis/64x64/Upsample      -         (?, 3, 64, 64)       -               
G_synthesis/64x64/ToRGB         264195    (?, 3, 64, 64)       (1, 1, 512, 3)  
G_synthesis/128x128/Conv0_up    2622465   (?, 512, 128, 128)   (3, 3, 512, 512)
G_synthesis/128x128/Conv1       2622465   (?, 512, 128, 128)   (3, 3, 512, 512)
G_synthesis/128x128/Upsample    -         (?, 3, 128, 128)     -               
G_synthesis/128x128/ToRGB       264195    (?, 3, 128, 128)     (1, 1, 512, 3)  
G_synthesis/256x256/Conv0_up    1442561   (?, 256, 256, 256)   (3, 3, 512, 256)
G_synthesis/256x256/Conv1       721409    (?, 256, 256, 256)   (3, 3, 256, 256)
G_synthesis/256x256/Upsample    -         (?, 3, 256, 256)     -               
G_synthesis/256x256/ToRGB       132099    (?, 3, 256, 256)     (1, 1, 256, 3)  
G_synthesis/512x512/Conv0_up    426369    (?, 128, 512, 512)   (3, 3, 256, 128)
G_synthesis/512x512/Conv1       213249    (?, 128, 512, 512)   (3, 3, 128, 128)
G_synthesis/512x512/Upsample    -         (?, 3, 512, 512)     -               
G_synthesis/512x512/ToRGB       66051     (?, 3, 512, 512)     (1, 1, 128, 3)  
G_synthesis/1024x1024/Conv0_up  139457    (?, 64, 1024, 1024)  (3, 3, 128, 64) 
G_synthesis/1024x1024/Conv1     69761     (?, 64, 1024, 1024)  (3, 3, 64, 64)  
G_synthesis/1024x1024/Upsample  -         (?, 3, 1024, 1024)   -               
G_synthesis/1024x1024/ToRGB     33027     (?, 3, 1024, 1024)   (1, 1, 64, 3)   
G_synthesis/2048x2048/Conv0_up  51297     (?, 32, 2048, 2048)  (3, 3, 64, 32)  
G_synthesis/2048x2048/Conv1     25665     (?, 32, 2048, 2048)  (3, 3, 32, 32)  
G_synthesis/2048x2048/Upsample  -         (?, 3, 2048, 2048)   -               
G_synthesis/2048x2048/ToRGB     16515     (?, 3, 2048, 2048)   (1, 1, 32, 3)   
G_synthesis/images_out          -         (?, 3, 2048, 2048)   -               
G_synthesis/noise0              -         (1, 1, 8, 8)         -               
G_synthesis/noise1              -         (1, 1, 16, 16)       -               
G_synthesis/noise2              -         (1, 1, 16, 16)       -               
G_synthesis/noise3              -         (1, 1, 32, 32)       -               
G_synthesis/noise4              -         (1, 1, 32, 32)       -               
G_synthesis/noise5              -         (1, 1, 64, 64)       -               
G_synthesis/noise6              -         (1, 1, 64, 64)       -               
G_synthesis/noise7              -         (1, 1, 128, 128)     -               
G_synthesis/noise8              -         (1, 1, 128, 128)     -               
G_synthesis/noise9              -         (1, 1, 256, 256)     -               
G_synthesis/noise10             -         (1, 1, 256, 256)     -               
G_synthesis/noise11             -         (1, 1, 512, 512)     -               
G_synthesis/noise12             -         (1, 1, 512, 512)     -               
G_synthesis/noise13             -         (1, 1, 1024, 1024)   -               
G_synthesis/noise14             -         (1, 1, 1024, 1024)   -               
G_synthesis/noise15             -         (1, 1, 2048, 2048)   -               
G_synthesis/noise16             -         (1, 1, 2048, 2048)   -               
images_out                      -         (?, 3, 2048, 2048)   -               
---                             ---       ---                  ---             
Total                           30394636                                       


D                     Params    OutputShape          WeightShape     
---                   ---       ---                  ---             
images_in             -         (?, 3, 2048, 2048)   -               
labels_in             -         (?, 0)               -               
2048x2048/FromRGB     128       (?, 32, 2048, 2048)  (1, 1, 3, 32)   
2048x2048/Conv0       9248      (?, 32, 2048, 2048)  (3, 3, 32, 32)  
2048x2048/Conv1_down  18496     (?, 64, 1024, 1024)  (3, 3, 32, 64)  
2048x2048/Skip        2048      (?, 64, 1024, 1024)  (1, 1, 32, 64)  
1024x1024/Conv0       36928     (?, 64, 1024, 1024)  (3, 3, 64, 64)  
1024x1024/Conv1_down  73856     (?, 128, 512, 512)   (3, 3, 64, 128) 
1024x1024/Skip        8192      (?, 128, 512, 512)   (1, 1, 64, 128) 
512x512/Conv0         147584    (?, 128, 512, 512)   (3, 3, 128, 128)
512x512/Conv1_down    295168    (?, 256, 256, 256)   (3, 3, 128, 256)
512x512/Skip          32768     (?, 256, 256, 256)   (1, 1, 128, 256)
256x256/Conv0         590080    (?, 256, 256, 256)   (3, 3, 256, 256)
256x256/Conv1_down    1180160   (?, 512, 128, 128)   (3, 3, 256, 512)
256x256/Skip          131072    (?, 512, 128, 128)   (1, 1, 256, 512)
128x128/Conv0         2359808   (?, 512, 128, 128)   (3, 3, 512, 512)
128x128/Conv1_down    2359808   (?, 512, 64, 64)     (3, 3, 512, 512)
128x128/Skip          262144    (?, 512, 64, 64)     (1, 1, 512, 512)
64x64/Conv0           2359808   (?, 512, 64, 64)     (3, 3, 512, 512)
64x64/Conv1_down      2359808   (?, 512, 32, 32)     (3, 3, 512, 512)
64x64/Skip            262144    (?, 512, 32, 32)     (1, 1, 512, 512)
32x32/Conv0           2359808   (?, 512, 32, 32)     (3, 3, 512, 512)
32x32/Conv1_down      2359808   (?, 512, 16, 16)     (3, 3, 512, 512)
32x32/Skip            262144    (?, 512, 16, 16)     (1, 1, 512, 512)
16x16/Conv0           2359808   (?, 512, 16, 16)     (3, 3, 512, 512)
16x16/Conv1_down      2359808   (?, 512, 8, 8)       (3, 3, 512, 512)
16x16/Skip            262144    (?, 512, 8, 8)       (1, 1, 512, 512)
8x8/MinibatchStddev   -         (?, 513, 8, 8)       -               
8x8/Conv              2364416   (?, 512, 8, 8)       (3, 3, 513, 512)
8x8/Dense0            16777728  (?, 512)             (32768, 512)    
Output                513       (?, 1)               (512, 1)        
scores_out            -         (?, 1)               -               
---                   ---       ---                  ---             
Total                 41595425                                       

tcmalloc: large alloc 1409286144 bytes == 0x55f1c7ba4000 @  0x7f7528fed1e7 0x7f7525ad346e 0x7f7525b23c7b 0x7f7525b2435f 0x7f7525bc6103 0x55efd81374b0 0x55efd8137240 0x55efd81ab0f3 0x55efd81a5ced 0x55efd8138bda 0x55efd81a6915 0x55efd81a5ced 0x55efd8138bda 0x55efd81a7737 0x55efd81a59ee 0x55efd8077e2b 0x55efd81a7fe4 0x55efd8138afa 0x55efd81a6915 0x55efd8138afa 0x55efd81a6c0d 0x55efd81a59ee 0x55efd8077e2b 0x55efd81a7fe4 0x55efd81a59ee 0x55efd8077e2b 0x55efd81a7fe4 0x55efd8138afa 0x55efd81a6915 0x55efd81a59ee 0x55efd81a56f3
tcmalloc: large alloc 1409286144 bytes == 0x55f232770000 @  0x7f7528fef001 0x7f7525ad354f 0x7f7525b23b58 0x7f7525b27b17 0x7f7525bc6203 0x55efd8137544 0x55efd8137240 0x55efd81ab627 0x55efd81a59ee 0x55efd8138bda 0x55efd81a6915 0x55efd81a59ee 0x55efd8138bda 0x55efd81a7737 0x55efd81a59ee 0x55efd8077e2b 0x55efd81a7fe4 0x55efd8138afa 0x55efd81a6915 0x55efd8138afa 0x55efd81a6c0d 0x55efd81a59ee 0x55efd8077e2b 0x55efd81a7fe4 0x55efd81a59ee 0x55efd8077e2b 0x55efd81a7fe4 0x55efd8138afa 0x55efd81a6915 0x55efd81a59ee 0x55efd81a56f3
tcmalloc: large alloc 1409286144 bytes == 0x55f286770000 @  0x7f7528fed1e7 0x7f7525ad346e 0x7f7525b23c7b 0x7f7525b23d18 0x7f7525bdfd79 0x7f7525be2e4c 0x7f7525d01e7f 0x7f7525d07fb5 0x7f7525d09e3d 0x7f7525d0b516 0x55efd8138720 0x55efd81382f9 0x7f7525be9e6b 0x55efd8220677 0x55efd81a7a2e 0x55efd8138afa 0x55efd81a6915 0x55efd81a59ee 0x55efd8138bda 0x55efd81a6915 0x55efd81a59ee 0x55efd8138bda 0x55efd81a7737 0x55efd81a59ee 0x55efd8077e2b 0x55efd81a7fe4 0x55efd8138afa 0x55efd81a6915 0x55efd8138afa 0x55efd81a6c0d 0x55efd81a59ee
Building TensorFlow graph...
Initializing logs...
Training for 4823 kimg...

2021-10-22 14:38:27.176309: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.00GiB (rounded to 2147483648).  Current allocation summary follows.
2021-10-22 14:38:27.178308: W tensorflow/core/common_runtime/bfc_allocator.cc:424] *****************************************_____*****_********_____________***************************
2021-10-22 14:38:27.178369: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at fused_bias_act.cu:163 : Resource exhausted: OOM when allocating tensor with shape[4,32,2048,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[4,32,2048,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node GPU0/G_loss/G/G_synthesis/2048x2048/Conv1/FusedBiasAct_1}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/content/drive/MyDrive/stylegan2/run_training.py"", line 218, in <module>
    main()
  File ""/content/drive/MyDrive/stylegan2/run_training.py"", line 213, in main
    run(**vars(args))
  File ""/content/drive/MyDrive/stylegan2/run_training.py"", line 136, in run
    dnnlib.submit_run(**kwargs)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/submission/submit.py"", line 343, in submit_run
    return farm.submit(submit_config, host_run_dir)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/submission/internal/local.py"", line 22, in submit
    return run_wrapper(submit_config)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/submission/submit.py"", line 280, in run_wrapper
    run_func_obj(**submit_config.run_func_kwargs)
  File ""/content/drive/MyDrive/stylegan2/training/training_loop.py"", line 312, in training_loop
    tflib.run(G_train_op, feed_dict_g)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/tflib/tfutil.py"", line 31, in run
    return tf.get_default_session().run(*args, **kwargs)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[4,32,2048,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node GPU0/G_loss/G/G_synthesis/2048x2048/Conv1/FusedBiasAct_1 (defined at /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py:1748) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Original stack trace for 'GPU0/G_loss/G/G_synthesis/2048x2048/Conv1/FusedBiasAct_1':
  File ""/content/drive/MyDrive/stylegan2/run_training.py"", line 218, in <module>
    main()
  File ""/content/drive/MyDrive/stylegan2/run_training.py"", line 213, in main
    run(**vars(args))
  File ""/content/drive/MyDrive/stylegan2/run_training.py"", line 136, in run
    dnnlib.submit_run(**kwargs)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/submission/submit.py"", line 343, in submit_run
    return farm.submit(submit_config, host_run_dir)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/submission/internal/local.py"", line 22, in submit
    return run_wrapper(submit_config)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/submission/submit.py"", line 280, in run_wrapper
    run_func_obj(**submit_config.run_func_kwargs)
  File ""/content/drive/MyDrive/stylegan2/training/training_loop.py"", line 231, in training_loop
    G_loss, G_reg = dnnlib.util.call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_gpu_in, **G_loss_args)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/util.py"", line 256, in call_func_by_name
    return func_obj(*args, **kwargs)
  File ""/content/drive/MyDrive/stylegan2/training/loss.py"", line 152, in G_logistic_ns_pathreg
    fake_images_out, fake_dlatents_out = G.get_output_for(latents, labels, is_training=True, return_dlatents=True)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/tflib/network.py"", line 221, in get_output_for
    out_expr = self._build_func(*final_inputs, **build_kwargs)
  File ""/content/drive/MyDrive/stylegan2/training/networks_stylegan2.py"", line 340, in G_main
    images_out = components.synthesis.get_output_for(dlatents, is_training=is_training, force_clean_graph=is_template_graph, **kwargs)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/tflib/network.py"", line 221, in get_output_for
    out_expr = self._build_func(*final_inputs, **build_kwargs)
  File ""/content/drive/MyDrive/stylegan2/training/networks_stylegan2.py"", line 647, in G_synthesis_stylegan2
    x = block(x, res)
  File ""/content/drive/MyDrive/stylegan2/training/networks_stylegan2.py"", line 593, in block
    x = layer(x, layer_idx=res*2, fmaps=nf(res+1), kernel=3)
  File ""/content/drive/MyDrive/stylegan2/training/networks_stylegan2.py"", line 572, in layer
    return apply_bias_act(x, act=act)
  File ""/content/drive/MyDrive/stylegan2/training/networks_stylegan2.py"", line 69, in apply_bias_act
    return fused_bias_act(x, b=tf.cast(b, x.dtype), act=act, alpha=alpha, gain=gain)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/tflib/ops/fused_bias_act.py"", line 68, in fused_bias_act
    return impl_dict[impl](x=x, b=b, axis=axis, act=act, alpha=alpha, gain=gain)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/tflib/ops/fused_bias_act.py"", line 193, in _fused_bias_act_cuda
    return func_zero_2nd_grad(x, b)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/custom_gradient.py"", line 168, in decorated
    return _graph_mode_decorator(f, *args, **kwargs)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/custom_gradient.py"", line 230, in _graph_mode_decorator
    result, grad_fn = f(*args)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/tflib/ops/fused_bias_act.py"", line 163, in func_zero_2nd_grad
    y = func_y(x, b)
  File ""/content/drive/MyDrive/stylegan2/dnnlib/tflib/ops/fused_bias_act.py"", line 127, in func_y
    y = cuda_kernel(x=x, b=b, ref=empty_tensor, grad=0, **cuda_kwargs)
  File ""<string>"", line 96, in fused_bias_act
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
"
52619,MirroredVariable has different values on replicas (only first device is correct),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, two lines
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.6.0 (but I also tried tf-nightly with the same result)
- Python version: 3.8
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 11.2 / 8.1.1 (same behavior also with 11.4 / 8.2.4)
- GPU model and memory: 4 x NVIDIA A40 (48GB)

**Describe the current behavior**

Minimal code example:

```python
import tensorflow as tf
    
with tf.distribute.MirroredStrategy().scope():
    print(tf.Variable(1.))
```

Output is:

```
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')
MirroredVariable:{
  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,
  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=0.0>,
  2: <tf.Variable 'Variable/replica_2:0' shape=() dtype=float32, numpy=0.0>,
  3: <tf.Variable 'Variable/replica_2:0' shape=() dtype=float32, numpy=0.0>
}
```

The problem is, as seen above, that the replicas do not contain the correct variable value, all are zero values except on the first device (the `numpy=0.0` parts). This is the same with 2 or 3 devices as well, not just with all 4.

(The same code does produce the expected behavior on a different machine with 2x Titan RTX GPUs.)

This is simply the minimal reproducing example. The real-world consequence when performing multi-gpu training is that the first forward pass succeeds, but after the first SGD update, things become NaN.

**Describe the expected behavior**

Expected output would be:

```
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')
MirroredVariable:{
  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,
  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>,
  2: <tf.Variable 'Variable/replica_2:0' shape=() dtype=float32, numpy=1.0>,
  3: <tf.Variable 'Variable/replica_2:0' shape=() dtype=float32, numpy=1.0>
}
```

(Note the `numpy=1.0` parts.)

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf
    
with tf.distribute.MirroredStrategy().scope():
    print(tf.Variable(1.))
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The server in question is a Dell PowerEdge R750xa with 4x Nvidia A40 GPUs."
52618,Offline Documentation,"Hello, TF Team.
Would you please guide me to how I can generate offline documentation for tensorflow, since I work a lot in offline settings. Or better yet, point me to where I can get a ready-made PDF version of the docs. 
Thanks."
52617,"InaccessibleTensoreorrr: tensor not acessible here , present in another or code block","

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8.10
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: cuda_11.2
- GPU model and memory: nvidia geforce rtx 2060 / 6gb 

I have implemented a post processing function for object detction model , which will also be used in mAP calcuation using tf,keras custom metric class support. Though while running the scipt in eager mode gives no porblem whatsoever,  but running the script in graph mode (by putting the main function in tf.function scope ), gives an InaccesibleTensorerror.


**Standalone code to reproduce the issue**
link to notebook : https://colab.research.google.com/drive/1Ei2t9coPNEVrfmejzaRUDIs-tm05EZFD?usp=sharing
 
**error**
InaccessibleTensorError: in user code:

    <ipython-input-30-c33ccae08316>:147 post_process  *
        keep_index = [index for index in range(len(final_probs)) if final_probs[index] > filter_threshold]
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:1817 wrapper
        return fn(x, y, *args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py:3962 greater
        ""Greater"", x=x, y=y, name=name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:597 _create_op_internal
        inp = self.capture(inp)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:647 capture
        % (tensor, tensor.graph, self))

    InaccessibleTensorError: The tensor 'Tensor(""while/while_1/cond/strided_slice_5:0"", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=while_while_1_cond_true_56189, id=140026056052816); accessed from: FuncGraph(name=while_body_55673, id=140026059019408).
"
52616,The keras.layere.reshape() is a dynamic-sized tensor ? convert CRNN model to TFLite cannot work on android GPU,"### 1. System information

- Linux Ubuntu 20.04,
- android API 25
- tf-nightly   keras=2.4.3

### 2. Code

CRNN model use rashape like this:

     x = layers.Reshape((4, 128))(x)

Convert code:

    keras_model = models.load_model(model_path, custom_objects={ 'precision': precision, 'recall': recall, 'f_score': f_score})
    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
    tflite_model = converter.convert()
    open(save_mdl_path, ""wb"").write(tflite_model)

### 3. (optional) Any other info / logs

The CRNN TFlite model  (have rashape) can work on Android CPU, but set GPU will get erro: ""java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#40 is a dynamic-sized tensor).""

The tflite model can work on android GPU when  delete the layers.rshape and GRU .

I need reshspe layer for 3D to 2D dim. how I can do this?

thanks advance
 "
52615,tensorflow lite libhexagon_nn_skel.so signed ,"About the hexagon nn deployment in tensorflow lite, the libhexagon_nn_skel.so has  already signed？
Is it possible to use hexnn_controller_request_unsigned_pd to avoid signing problem。"
52610,Golang support for tf.Variable,"**System information**
- TensorFlow version (you are using): unsure
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**

I don't see any equivalent of ""tf.Variable"" in the Go API:
https://pkg.go.dev/github.com/tensorflow/tensorflow/tensorflow/go#section-readme
and
https://pkg.go.dev/github.com/tensorflow/tensorflow/tensorflow/go/op

**Will this change the current api? How?**
It will change the Go API.

**Who will benefit with this feature?**
Golang users who want to train models.

I have a model I am creating in Golang and then sending to Python to use the optimization API. I need tf.Variable objects to pass to the minimize function.

**Any Other info.**
I mistakenly filed this FR in an unofficial repo: https://github.com/galeone/tfgo/issues/62
"
52607,MirroredStrategy throw always AttributeError at the end of execution,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version: 3.7
- CUDA/cuDNN version: 11.2/8.1.0
- GPU model and memory: 10x GTX 1080ti

**Describe the current behavior**
When using a strategy at the end of the program I got an AttributeError exception.
```
Exception ignored in: <function Pool.__del__ at 0x7f06a661ac10> 
Traceback (most recent call last): 
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 268, in del 
  File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 362, in put 
AttributeError: 'NoneType' object has no attribute 'dumps'
```
The program works as expected, but it shows always that exception.

**Describe the expected behavior**
The program should exit without this exception

**Standalone code to reproduce the issue**
```
import tensorflow as tf 
strategy = tf.distribute.MirroredStrategy()
 ```
"
52605,Model serving signature with SparseTensor input feature,"This is a bug report.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 11.5.2, GCP Linux hosted instance
-   **TensorFlow installed from (source or binary)**: TFX 1.0.0 docker image on Linux; on OSX installed binary with pip
-   **TensorFlow version (use command below)**: 2.5.0
-   **Python version**: 3.8.1 and 3.8.5 (Mac), 3.7 (Linux)
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: N/A CPU only
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**: N/A

### Describe the problem
When saving the model signature function with sparse tensor input, (generated from `get_concrete_function` or `tf.function(input_signatures=...)` decorator), the recovered model signature no longer accepts sparse tensor as inputs. Here is an example script:

```python
import tensorflow as tf

# Mock a model
input_x = tf.keras.Input(1)
output_y = tf.keras.layers.Dense(1)(input_x)
model = tf.keras.Model(input_x, output_y)

def _get_serving_signature(model):
    @tf.function
    def my_func(x, y):
        x_out = tf.cast(tf.sparse.to_indicator(x, 5), tf.int64)
        return {""x"": x_out, ""y"": y}
    
    return my_func

# Get the concrete func
concrete_func = _get_serving_signature(model).get_concrete_function(
        x=tf.SparseTensorSpec(shape=[None, None], dtype=tf.int64),
        y=tf.TensorSpec(shape=[None, 1], dtype=tf.int64),
    )

# Store this function inside the model
model.my_func = concrete_func
    
# Build the signature dict
signatures = { ""default"": concrete_func}

# save the model
model.save(""./serving_dir"", save_format=""tf"", signatures=signatures)

# Load the model back
model2 = tf.keras.models.load_model(""./serving_dir"")

# Make up some data
x = tf.ragged.constant([[1, 3], [2, 3, 1], [2]], dtype=tf.int64).to_sparse()
y = tf.expand_dims(tf.constant([1, 2, 1], dtype=tf.int64), axis=1)

# Make inference on saved my_func
out_func_attr = model2.my_func(x=x, y=y)

# Make inference using signature
out_signature = model2.signatures[""default""](x, y)
```

During saving of the model, a warning message appears:

```
WARNING:absl:Function `my_func` contains input name(s) x with unsupported characters which will be renamed to x_2 in the SavedModel.
```

Calling the saved function as a model attribute returns the correct output
```python
out_func_attr = model2.my_func(x=x, y=y)
```

```python
{'y': <tf.Tensor: shape=(3, 1), dtype=int64, numpy=
  array([[1],
        [2],
        [1]])>,
  'x': <tf.Tensor: shape=(3, 5), dtype=int64, numpy=
  array([[0, 1, 0, 1, 0],
        [0, 1, 1, 1, 0],
        [0, 0, 1, 0, 0]])>}
```

Make inference using signature 

```python
out_signature = model2.signatures[""default""](x, y)
```
returns the following error:


```python
Traceback (most recent call last):

  File ""/Users/edward/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1724, in _call_impl
return self._call_with_flat_signature(args, kwargs,

  File ""/Users/edward/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1748, in _call_with_flat_signature
raise TypeError(

TypeError: signature_wrapper(x, x_1, x_2, y) takes 0 positional arguments but 2 were given


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""/Users/edward/Desktop/sparse_serving.py"", line 61, in <module>
    out_signature = model2.signatures[""default""](x, y)

  File ""/Users/edward/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1711, in __call__
    return self._call_impl(args, kwargs)

  File ""/Users/edward/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1727, in _call_impl
    raise structured_err

  File ""/Users/edward/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1720, in _call_impl
    return self._call_with_structured_signature(args, kwargs,

  File ""/Users/edward/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1798, in _call_with_structured_signature
    self._structured_signature_check_missing_args(args, kwargs)

  File ""/Users/edward/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1817, in _structured_signature_check_missing_args
    raise TypeError(""{} missing required arguments: {}"".format(

TypeError: signature_wrapper(*, x_2, y, x_1, x) missing required arguments: x, x_1, x_2, y

```

The saved function attribute `model2.my_func` is

```python
 <ConcreteFunction my_func(x, y) at 0x7F9D824BBF10>
```

which is expected.

The signature `model2.signatures[""default""]`  is a
```python
<ConcreteFunction signature_wrapper(*, x_2, y, x_1, x) at 0x7F9D824C5A90>
```

which is incorrect.

One can also easily modify the above script and verify him/herself that, if `x` is a dense tensor, with `TensorSpec` rather than `SparseTensorSpec`, the serving signature no longer has this problem.

"
52604,"NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-  Linux Mint Ubuntu 19.2
- TensorFlow installed from pip install tensor flow
- TensorFlow version: newest
- Python version: 3.8.8
- Installed using pip:
- Want to install CUDA version
- GPU model and memory: GTX 1060 NVIDIA driver 470 installed CUDA version 11.4



**Describe the problem**
In  jupyter notebook tried running import
```
from keras.models import load_model
import cv2
import numpy as np
from moviepy.editor import VideoFileClip
import pdb
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from scipy.ndimage.measurements import label as scipyLabel 

```
got an error when trying to run `model = load_model('model.h5')` in the debug console of jupyter notebook i got a message saying 
```

2021-10-21 10:18:44.284893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-21 10:18:44.322103: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2021-10-21 10:18:44.329961: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-10-21 10:18:44.330803: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

To fix this issue I followed the instructions in the https://www.tensorflow.org/install/gpu website and ran the cod to install CUDA a

```
# Add NVIDIA package repositories
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin
sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
sudo add-apt-repository ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ /""
sudo apt-get update

wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb

sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt-get update

wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/libnvinfer7_7.1.3-1+cuda11.0_amd64.deb
sudo apt install ./libnvinfer7_7.1.3-1+cuda11.0_amd64.deb
sudo apt-get update

# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-11-0 \
    libcudnn8=8.0.4.30-1+cuda11.0  \
    libcudnn8-dev=8.0.4.30-1+cuda11.0

# Reboot. Check that GPUs are visible using the command: nvidia-smi

# Install TensorRT. Requires that libcudnn8 is installed above.
sudo apt-get install -y --no-install-recommends libnvinfer7=7.1.3-1+cuda11.0 \
    libnvinfer-dev=7.1.3-1+cuda11.0 \
    libnvinfer-plugin7=7.1.3-1+cuda11.0

```
I rebooted my machine and check that my machine does in fact see the GPU and the GPU is listed with CUDA version 11.4 
I ran the code again in jupyter notebook but no change to the output error message. 

**Any other info / logs**

Error message when ran  `model = load_model('model.h5')`
```

ValueError                                Traceback (most recent call last)
<ipython-input-5-d7950e9aea83> in <module>
----> 1 model = load_model('model.h5')

~/anaconda3/lib/python3.8/site-packages/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)
    198         if (h5py is not None and
    199             (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
--> 200           return hdf5_format.load_model_from_hdf5(filepath, custom_objects,
    201                                                   compile)
    202 

~/anaconda3/lib/python3.8/site-packages/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)
    178       model_config = model_config.decode('utf-8')
    179     model_config = json_utils.decode(model_config)
--> 180     model = model_config_lib.model_from_config(model_config,
    181                                                custom_objects=custom_objects)
    182 

~/anaconda3/lib/python3.8/site-packages/keras/saving/model_config.py in model_from_config(config, custom_objects)
     50                     '`Sequential.from_config(config)`?')
     51   from keras.layers import deserialize  # pylint: disable=g-import-not-at-top
---> 52   return deserialize(config, custom_objects=custom_objects)
     53 
     54 

~/anaconda3/lib/python3.8/site-packages/keras/layers/serialization.py in deserialize(config, custom_objects)
    206   """"""
    207   populate_deserializable_objects()
--> 208   return generic_utils.deserialize_keras_object(
    209       config,
    210       module_objects=LOCAL.ALL_OBJECTS,

~/anaconda3/lib/python3.8/site-packages/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    672 
    673       if 'custom_objects' in arg_spec.args:
--> 674         deserialized_obj = cls.from_config(
    675             cls_config,
    676             custom_objects=dict(

~/anaconda3/lib/python3.8/site-packages/keras/engine/sequential.py in from_config(cls, config, custom_objects)
    430     model = cls(name=name)
    431     for layer_config in layer_configs:
--> 432       layer = layer_module.deserialize(layer_config,
    433                                        custom_objects=custom_objects)
    434       model.add(layer)

~/anaconda3/lib/python3.8/site-packages/keras/layers/serialization.py in deserialize(config, custom_objects)
    206   """"""
    207   populate_deserializable_objects()
--> 208   return generic_utils.deserialize_keras_object(
    209       config,
    210       module_objects=LOCAL.ALL_OBJECTS,

~/anaconda3/lib/python3.8/site-packages/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    672 
    673       if 'custom_objects' in arg_spec.args:
--> 674         deserialized_obj = cls.from_config(
    675             cls_config,
    676             custom_objects=dict(

~/anaconda3/lib/python3.8/site-packages/keras/layers/core.py in from_config(cls, config, custom_objects)
   1003   def from_config(cls, config, custom_objects=None):
   1004     config = config.copy()
-> 1005     function = cls._parse_function_from_config(
   1006         config, custom_objects, 'function', 'module', 'function_type')
   1007 

~/anaconda3/lib/python3.8/site-packages/keras/layers/core.py in _parse_function_from_config(cls, config, custom_objects, func_attr_name, module_attr_name, func_type_attr_name)
   1055     elif function_type == 'lambda':
   1056       # Unsafe deserialization from bytecode
-> 1057       function = generic_utils.func_load(
   1058           config[func_attr_name], globs=globs)
   1059     elif function_type == 'raw':

~/anaconda3/lib/python3.8/site-packages/keras/utils/generic_utils.py in func_load(code, defaults, closure, globs)
    787   except (UnicodeEncodeError, binascii.Error):
    788     raw_code = code.encode('raw_unicode_escape')
--> 789   code = marshal.loads(raw_code)
    790   if globs is None:
    791     globs = globals()

ValueError: bad marshal data (unknown type code)
```
Thanks to any help in advanced"
52603,build tensorflow 2.6 on macOS Big Sur (11.6) M1 ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  macOS Big Sur v11.6 Apple M1 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source 
- TensorFlow version: 2.6.0
- Python version: 3.8.12
- Installed using virtualenv? pip? conda?: pip 21.3 (python 3.8) 
- Bazel version (if compiling from source): bazel 4.2.1-homebrew
- GCC/Compiler version (if compiling from source): llvm  (Apple clang version 13.0.0 (clang-1300.0.29.3). /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
- CUDA/cuDNN version: none
- GPU model and memory: none



**Describe the problem**
I am trying to build tensorflow v2.6.0 on macOS Big Sur using the first version of M1 chip.

1. cd ~/Downloads
2. git clone https://github.com/meteorcloudy/tensorflow.git
3. cd tensorflow
4. modify .bazelversion to current version that I have installed using brew (4.2.1).
5. unset PYTHONPATH env
6. unset PYTHON_BIN_PATH env
7. ./configure
8. bazel build --config=v2 --config=opt --cpu=darwin_arm64 //tensorflow/tools/pip_package:build_pip_package
9. ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
10. cd /tmp/tensorflow_pkg
11. pip install tensorflow-2.8.0-cp38-cp38-macosx_11_0_arm64.whl
ERROR: tensorflow-2.8.0-cp38-cp38-macosx_11_0_arm64.whl is not a supported wheel on this platform.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I got the error saying that the whl is not supported on this platform.
Is it something wrong when I use bazel? 
I have tried to use
   bazel build --config=v2 --config=opt --config=macos_arm64 //tensorflow/tools/pip_package:build_pip_package
   ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
and I got the same error.

Please kindly let me know how to fix that. 
Thanks
Kel

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
52599,Asset initialization using SavedModel C++ API,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installed from: source, commit 1dd2070b5f9
- TensorFlow version: 2.8
- Python version: 3.8.5
- Bazel version: 3.7.2
- GCC version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 11.3
- GPU model and memory: V100-SXM2 16GB

**Describe the current behavior**
I would like to infer a TF-TRT converted model using the C++ API of tensorflow. I have followed the https://github.com/bmzhao/saved-model-example. Before saving the model I have converted it with TF-TRT. See https://github.com/tfeher/tf-trt-saved-model-example for details.

While loading the SavedModel, the assets are initialized. This is confirmed by the printout from TF-TRT's op that initializes the engine from the asset file:
```
I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_resource_ops.cc:160] Loaded 1 TRT engines for op TRTEngineOp_0_0 on device /job:localhost/replica:0/task:0/device:GPU:0 from file
 /data/saved-model-example/mnist_model_trt/assets/trt-serialized-engine.TRTEngineOp_0_0

```

Afterwards, when I try to infer the model, I receive the following error:
```
Error executing session.Run() INVALID_ARGUMENT: 2 root error(s) found.
  (0) INVALID_ARGUMENT: You must feed a value for placeholder tensor 'asset_path_initializer' with dtype string
         [[{{node asset_path_initializer}}]]
         [[Func/StatefulPartitionedCall/input/_1/_67]]
  (1) INVALID_ARGUMENT: You must feed a value for placeholder tensor 'asset_path_initializer' with dtype string
         [[{{node asset_path_initializer}}]]
0 successful operations.
0 derived errors ignored.
```

I do not understand why does it want to initialize the assets again. They are already initialized. In fact, if I modify the example to add the asset_path, then TRT resource initializer throws the following error:
```
Error executing session.Run() INTERNAL: 2 root error(s) found.                                                                                                                                                    
  (0) INTERNAL: Expect engine cache to be empty, but got 1 entries.                                                                                                                                               
         [[{{function_node __inference_<lambda>_42188}}{{node InitializeTRTResource}}]]                                                                         
         [[StatefulPartitionedCall/_69]]                                                                                                                                                                          
  (1) INTERNAL: Expect engine cache to be empty, but got 1 entries.                                                                                                                                               
         [[{{function_node __inference_<lambda>_42188}}{{node InitializeTRTResource}}]]   
```

**Describe the expected behavior**
I expect that I can infer the model once it is loaded by `LoadSavedModel`, since `LoadSavedModel` runs initialization on the assets and variables.

**Standalone code to reproduce the issue**
https://github.com/tfeher/tf-trt-saved-model-example 

Tagging @bixia1 and @DEKHTIARJonathan  for visibility."
52597,"Simple SSD, face detector, training ok with GradientTape BUT loss doesn't decrease if I train an identical model with Model.fit()","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, custom
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04, and also on current Google Colab
- TensorFlow installed from (source or binary): binary, GPU version
- TensorFlow version (use command below): 2.5.1 (2.6 on Google Colab)
- Python version: 3.7
- CUDA/cuDNN version: 11.4, 8.2
- GPU model and memory: Geforce RTX 2080 Ti

**Describe the current behavior**

Hi, I found a really interesting bug when I was building a simple SSD model to find faces on images.

I have a simple, ResNet-like model, containing only convolutions and max pooling layers. I use the ""Faces in the Wild"" dataset from UMass. When I create a model and train it with using GradientTape it learns quickly, and after just 20 epochs it predicts very well face bounding boxes, and validation loss goes below 0.6. On the other hand, if I create the exactly the same model, use the same Adam optimizer, but I train the Model with Model.fit(), it just learns nothing. The validation loss starts from 1.8 and doesn't decrease at all.

I found the issue first on my machine running Tensorflow 2.5.1, but I can reproduce the exactly same behaviour on Google Colab as well, currently running 2.6.0.

**Describe the expected behavior**

The Model.fit() and the simple GradientTape training should work identical, both trained model should learn and predict similarly.

**Standalone code to reproduce the issue**

Please check out the code here in Google Colab:
https://colab.research.google.com/drive/1LvahE1CBuvvxAi7PT0gqcSrFYPDBEJmY?usp=sharing

**Other info / logs**

Please check out the ends of the verbose outputs of training loops:

Model.fit() version output:
[...]
Epoch 18/20
80/80 [==============================] - 9s 111ms/step - loss: 1.6102 - val_loss: 1.6888
Epoch 19/20
80/80 [==============================] - 9s 111ms/step - loss: 1.6102 - val_loss: 1.6888
Epoch 20/20
80/80 [==============================] - 9s 112ms/step - loss: 1.6102 - val_loss: 1.6888

GradientTape version output:
[...]
epoch: [18/20], Train: [loss: 0.4297], Test: [loss: 0.5801]
epoch: [19/20], Train: [loss: 0.4001], Test: [loss: 0.5757]
epoch: [20/20], Train: [loss: 0.3962], Test: [loss: 0.5701]

Thanks for reading it, let me know what do you think."
52596,Mixed precision training incompatible with BinaryCrossentropy label smoothing,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows Server 2012 R2
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: -
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: v2.6.0-rc2-32-g919f693420e 2.6.0
-   **Python version**: 3.9.6
-   **Bazel version (if compiling from source)**: -
-   **GCC/Compiler version (if compiling from source)**: -
-   **CUDA/cuDNN version**: 11.2.2 / 8.1
-   **GPU model and memory**: NVIDIA Titan X (Pascal), 12288 MiB
-   **Exact command to reproduce**: see example below

### Describe the problem
Reporting a possible bug. When setting float16 mixed precision policy and using label smoothing in BinaryCrossentropy, training returns a TypeError. Turning off either the mixed precision policy or label smoothing gives no errors. Passing a float16 to the label_smoothing argument does not help.

### Source code / logs

**Reproducible example**
```
import tensorflow as tf
import numpy as np

# Set mixed precision policy
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# Create some random data
inputs  = tf.random.normal((64, 256, 256, 1))
targets = tf.constant(np.random.choice(
    a       = [0, 1],
    size    = (64, 256, 256, 1),
    replace = True
))

# Create simple model
model = tf.keras.Sequential([
    tf.keras.Input(shape=(256, 256, 1)),
    tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')
])

model.compile(
    optimizer = 'adam',
    loss      = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1)
)

# Train model
model.fit(inputs, targets, batch_size=8)
```

**Traceback**
```
Traceback (most recent call last):

  File ""G:\[redacted]\src\debug1.py"", line 36, in <module>
    model.fit(inputs, targets, batch_size=8)

  File ""F:\conda_env\[redacted]\lib\site-packages\keras\engine\training.py"", line 1184, in fit
    tmp_logs = self.train_function(iterator)

  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)

  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\def_function.py"", line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)

  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\def_function.py"", line 759, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access

  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\function.py"", line 3066, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)

  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\function.py"", line 3463, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)

  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\function.py"", line 3298, in _create_graph_function
    func_graph_module.func_graph_from_py_func(

  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 1007, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)

  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\eager\def_function.py"", line 668, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)

  File ""F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 994, in wrapper
    raise e.ag_error_metadata.to_exception(e)

TypeError: in user code:

    F:\conda_env\[redacted]\lib\site-packages\keras\engine\training.py:853 train_function  *
        return step_function(self, iterator)
    F:\conda_env\[redacted]\lib\site-packages\keras\engine\training.py:842 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:1286 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:2849 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:3632 _call_for_each_replica
        return fn(*args, **kwargs)
    F:\conda_env\[redacted]\lib\site-packages\keras\engine\training.py:835 run_step  **
        outputs = model.train_step(data)
    F:\conda_env\[redacted]\lib\site-packages\keras\engine\training.py:788 train_step
        loss = self.compiled_loss(
    F:\conda_env\[redacted]\lib\site-packages\keras\engine\compile_utils.py:201 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    F:\conda_env\[redacted]\lib\site-packages\keras\losses.py:141 __call__
        losses = call_fn(y_true, y_pred)
    F:\conda_env\[redacted]\lib\site-packages\keras\losses.py:245 call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\util\dispatch.py:206 wrapper
        return target(*args, **kwargs)
    F:\conda_env\[redacted]\lib\site-packages\keras\losses.py:1805 binary_crossentropy
        y_true = tf.__internal__.smart_cond.smart_cond(label_smoothing, _smooth_labels,
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\framework\smart_cond.py:56 smart_cond
        return true_fn()
    F:\conda_env\[redacted]\lib\site-packages\keras\losses.py:1803 _smooth_labels
        return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\ops\math_ops.py:1383 binary_op_wrapper
        raise e
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\ops\math_ops.py:1367 binary_op_wrapper
        return func(x, y, name=name)
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\ops\math_ops.py:1710 _mul_dispatch
        return multiply(x, y, name=name)
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\util\dispatch.py:206 wrapper
        return target(*args, **kwargs)
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\ops\math_ops.py:530 multiply
        return gen_math_ops.mul(x, y, name)
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\ops\gen_math_ops.py:6244 mul
        _, _, _op, _outputs = _op_def_library._apply_op_helper(
    F:\conda_env\[redacted]\lib\site-packages\tensorflow\python\framework\op_def_library.py:555 _apply_op_helper
        raise TypeError(

    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
```"
52594,Failed to invoke the interpreter with error: Provided data count XXX must match the required count 3.,"### 1. System information

- OSX 11.5.2, iOS 14.7.1, CentOS 8.4.2105 
- TensorFlow v2.6.0 custom built for server (CentOS 8.4.2105)
- TensorFlow library 'TensorFlowLiteSwift', '~> 2.6'

The CentOS custom built tensorflow was built with the following optimization flags:
```
-march=native -msse4_1 -msse4_2 -mssse3 -mcx16 -mpopcnt
```

On the CentOS 8 box I successfully trained a custom model using `faster_rcnn_resnet152_v1` for it's configuration. I'm able to run the model on images and mp4 files successfully. 

I converted the model on the CentOS box for tflite. 

I copied the model into the `tensorflow/examples/tree/master/lite/examples/object_detection/ios` example project and updated the Podfile to use tflite 2.6.

### 2. Code

```
import tensorflow as tf

saved_model_dir = ""exported-models/barbell_3/saved_model""

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]

tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```


ModelDataHandler.swift
Case 1
```
  // MARK: Model parameters
  let batchSize = 1
  let inputChannels = 3
  let inputWidth = 1024
  let inputHeight = 1024
```

Case 2
```
  // MARK: Model parameters
  let batchSize = 1
  let inputChannels = 3
  let inputWidth = 1
  let inputHeight = 1
```



### 3. Failure after conversion


#### Case 1
- Interpreter can't load model

```
Failed to invoke the interpreter with error: Provided data count 3145728 must match the required count 3.
```

Note 1024 * 1024 * 3 = 3145728

Note the netron graph entry point shows the input should be `1x1x1=3` followed by a while loop whose 3rd element is (1x1024x1024x3).

Presumably, these are the correct parameters however see case 2.

#### Case 2

If I set the inputWidth and inputHeight to 1x1 the interpreter loads, the app starts and then crashes with an Out of Memory error (in a popup window) - however, nothing is logged in the XCode console and no errors are given for tensorflow lite.

```
2021-10-20 09:28:46.565241-0500 ObjectDetection[723:92493] Initialized TensorFlow Lite runtime.
INFO: Initialized TensorFlow Lite runtime.
```

Is it the case that the correct parameters are inputHeight=1 and inputWidth=1 and that my model is using too much OS memory on the phone?

If that's true are there guidelines on which model training config params are suitable for phones (both iOS and Andriod)? That is, models with better input size and hidden layer size etc?


### 4. (optional) RNN conversion support


### 5. (optional) Any other info / logs

<img width=""1394"" alt=""netron"" src=""https://user-images.githubusercontent.com/128980/138111306-28d3f186-6461-4795-a798-9777cb203496.png"">

"
52592,Will Tensorflow support Keras in the future? ,"In the [release notes](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md) 2.6 release notes, it is mentioned that keras is going to be separated from tensorflow. 
 
My query is - Does this mean that tf.keras will be in sync with the separated keras library? (I understand that tf.python.keras is deprecated and will be removed). 

So as a recommendation, should I use tf.keras or directly import and use keras in my future projects?"
52591,[mlir-hlo]The following operations cannot be legalized: tf.VariableV2 ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): No, i want to but i don't know how to

**Describe the feature and the current behavior/state.**
feature: 
lower the op tf.VariableV2/tf.VarHandleOp/tf.ReadVariableOp/tf.AssignVariableOp of tf.dialect into mlir-hlo or other dialect. 

the current behavior: 
when i trying to do the translation as follow:
tf-opt --tf-to-hlo-pipeline target-func.mlir -o target-mhlo.mlir
i get the following error message:
target-func.mlir:2:3: error: The following operations cannot be legalized: tf.Assign (count: 1); tf.AssignAdd (count: 1); tf.VariableV2 (count: 1). These legalization failure(s) may be due to missing TF to HLO lowerings and/or unsupported attributes, etc.
  func @main() attributes {tf.entry_function = {control_outputs = ""Variable/Assign,AssignAdd"", inputs = """", outputs = """"}} {

**Will this change the current api? How?**
yes.
convertion should be add to the file [legalize_hlo_patterns.td](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/transforms/legalize_hlo_patterns.td) or else

**Who will benefit with this feature?**
MLIR users, all tensorflow IR users

**Any Other info.**
is there any feature similar completed ？
"
52590,Following machine learning tutorial of Google gives wrong results,"# The case:
I have to follow this Tutorial for school and reproduce the results:
[Predicting Customer Lifetime Value with AI Platform: training the models](https://cloud.google.com/architecture/clv-prediction-with-offline-training-train)
This task should be the first step of my bachelor thesis and I have to reproduce this result very naive, so with the same parameters. I am trying on this for weeks now.

I did this already in Tensorflow 2 with newer commands, than tried the same in Tensorflow 1. Both gave the same results like in the code below. The code below was my last try so far, by loading the old packages for TF, so I can use the EXACT same commands and code structure, like they did. 
I will post the whole in detail, but I think it is structured really well (like the {}.py files of the tutorial), for you to understand. 
The data preparation I did in BigQuery and saved the train, eval and test set to [my GitHub Repo](https://github.com/timmy-ops/DNNs_for_CLVs) for importing them easily. 
The original .py files, which are named like my cells you can find in [their GitHub Repo](https://github.com/GoogleCloudPlatform/tensorflow-lifetime-value/tree/0f8c16ea70a2e7da370965e23e9e2154978364fa/clv_mle/trainer).

There are 4 cells of code in my Colab plus the module imports and below I wrote the current and expected behaviour...
```
@#title IMPORT MODULES and GitHub Repos
import pandas as pd
from datetime import datetime
import numpy as np
!pip install gast==0.2.2

%tensorflow_version 1.x
import tensorflow as tf
from tensorflow import feature_column as tfc
from six import iteritems
import shutil

#only for the hypertune path fn:
import json
import os

! git clone https://github.com/GoogleCloudPlatform/tensorflow-lifetime-value.git
! git clone https://github.com/timmy-ops/DNNs_for_CLVs
```
```
#@title DNN: context.py

class CLVFeatures(object):

  HEADERS = ['customer_id', 'monetary_dnn', 'monetary_btyd', 'frequency_dnn',
             'frequency_btyd', 'recency', 'T', 'time_between',
             'avg_basket_value', 'avg_basket_size', 'cnt_returns',
             'has_returned', 'frequency_btyd_clipped', 'monetary_btyd_clipped',
             'target_monetary_clipped', 'target_monetary']
  HEADERS_DEFAULT = [[''], [0.0], [0.0], [0],
                     [0], [0], [0], [0.0],
                     [0.0], [0.0], [0],
                     [-1], [0], [0.0],
                     [0.0], [0.0]]
  NUMERICS = {
      'monetary_dnn': [],
      'recency': [],
      'frequency_dnn': [],
      'T': [],
      'time_between': [],
      'avg_basket_value': [],
      'avg_basket_size': [],
      'cnt_returns': []}
  CATEGORICALS_W_LIST = {
      'has_returned': [0, 1]}
  CROSSED = []
  KEY = 'customer_id'
  UNUSED = [KEY, 'monetary_btyd', 'frequency_btyd', 'frequency_btyd_clipped',
            'monetary_btyd_clipped', 'target_monetary_clipped']
  TARGET_NAME = 'target_monetary'

  def __init__(self, ignore_crosses=False): #, is_dnn=None

    #if not is_dnn:
    #  return

    self.ignore_crosses = ignore_crosses
    (self.headers, self.numerics_names, self.categorical_names) = self._keep_used()
    self.continuous, self.categorical = self._make_base_features()

    if not self.ignore_crosses:
      self.crossed_for_wide, self.crossed_for_deep = self._make_crossed()
  
  def _keep_used(self):
    headers = [h for h in self.HEADERS if h not in self.UNUSED]
    numerics_names = {
        k: v for k, v in iteritems(self.NUMERICS)
        if (k not in self.UNUSED) and (k != self.TARGET_NAME)
    }
    categorical_names = {
        k: v for k, v in iteritems(self.CATEGORICALS_W_LIST)
        if k not in self.UNUSED    
    }                          
    return headers, numerics_names, categorical_names
  
  def get_key(self):
    return self.KEY
  
  def get_used_headers(self, with_key=False, with_target=False):
    used_headers = [h for h in self.headers if h != self.TARGET_NAME]

    if with_key:
      used_headers.insert(0, self.KEY)
    if with_target:
      used_headers.append(self.TARGET_NAME)

    return used_headers

  def get_defaults(self, headers_names=None, with_key=False):
    if headers_names is None:
      headers_names = self.get_used_headers(with_key)

    keep_indexes = [self.HEADERS.index(n) for n in headers_names]
    return [self.HEADERS_DEFAULT[i] for i in keep_indexes]

  def get_all_names(self):
    return self.HEADERS

  def get_all_defaults(self):
    return self.HEADERS_DEFAULT

  def get_unused(self):
    return self.UNUSED

  def get_target_name(self):
    return self.TARGET_NAME

  def _make_base_features(self):
    continuous = {
        key_name: tfc.numeric_column(key_name)
        for key_name in self.numerics_names.keys()
    }
    categorical = {
        key_name: tfc.categorical_column_with_vocabulary_list(
            key=key_name,
            vocabulary_list=voc)
        for key_name, voc in self.categorical_names.items()
    }    
    return continuous, categorical

  def get_base_features(self):
    return self.continous, self.categorical

  def _prepare_for_crossing(self, key_name, num_bck, boundaries):
    key = None
    if key_name in self.continuous.keys():
      if boundaries is not None:
        key = tfc.bucketized_column(self.continuous[key_name], boundaries)
      else:
        key = tfc.categorical_column_with_identity(key_name, num_bck)
    elif key_name in self.categorical.keys():
      key = key_name
    else:
      key = key_name
    return key

  def _make_crossed(self):
    f_crossed_for_wide = []
    f_crossed_for_deep = []
    for to_cross in self.CROSSED:
      key = []
      bck_size = 1
      for (key, bck, bnd) in to_cross:
        keys.append(self._prepare_for_crossing(key, bck, bnd))
        bck_size *= bck

      t_crossed = tfc.crossed_column(keys, min(bck_size, 10000))
      t_dimension = int(bck_size**0.25)
      f_crossed_for_wide.append(t_crossed)
      f_crossed_for_deep.append(tfc.embedding_column(t_crossed, t_dimension))
    return f_crossed_for_wide, f_crossed_for_deep

  def get_wide_features(self):
    wide_features = self.categorical.values()
    if not self.ignore_crosses:
      wide_features += self.crossed_for_wide
    return wide_features  

  def get_deep_features(self, with_continuous=True):
    deep_features = [tfc.indicator_column(f) for f in self.categorical.values()]
    if with_continuous:
      deep_features += self.continuous.values()
    if not self.ignore_crosses:
      deep_features += self.crossed_for_deep
    return deep_features         
```
```
#@title DNN: model.py

clvf = CLVFeatures(ignore_crosses=True)

def parse_csv(csv_row):
  columns = tf.decode_csv(csv_row, record_defaults = clvf.get_all_defaults())
  features = dict(zip(clvf.get_all_names(), columns))
  
  for column_name in clvf.get_unused():
    features.pop(column_name)

  target = features.pop(clvf.get_target_name())

  return features, target


def dataset_input_fn(data_folder, prefix=None, mode=None, params=None, count=None):
  shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False

  filenames = tf.matching_files('{}{}*.csv'.format(data_folder, prefix))

  dataset = tf.data.TextLineDataset(filenames).skip(1)
  dataset = dataset.map(parse_csv)
  if shuffle:
    dataset = dataset.shuffle(buffer_size=params.buffer_size)
  dataset = dataset.repeat(count=count)
  dataset = dataset.batch(params.batch_size)

  iterator = tf.compat.v1.data.make_one_shot_iterator(dataset) 
  
  features, target = iterator.get_next()

  return features, target

def read_train(data_folder, params):
  return dataset_input_fn(
      data_folder=data_folder,
      prefix='train',
      params=params,
      mode=tf.estimator.ModeKeys.TRAIN)


def read_eval(data_folder, params):
  return dataset_input_fn(data_folder=data_folder,
                          prefix='eval',
                          params=params)


def read_test(data_folder, params):
  return dataset_input_fn(data_folder=data_folder,
                          prefix='test',
                          params=params,
                          count=1)

def rmse_evaluator(labels, predictions):
  pred_values = predictions['predictions']
  return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)} 

def get_learning_rate(params):
  global_step = tf.train.get_global_step()
  learning_rate = tf.train.exponential_decay(                    
      learning_rate = params.learning_rate,
      global_step = global_step,                                                                
      decay_steps = params.checkpoint_steps,
      decay_rate = params.learning_decay_rate,
      staircase = True)
  return learning_rate

def get_optimizer(params):
  optimizer = tf.train.ProximalAdagradOptimizer(
      learning_rate = get_learning_rate(params),                                                
      l1_regularization_strength = params.l1_regularization,
      l2_regularization_strength = params.l2_regularization)
  return optimizer

def get_estimator(config, params, model_dir):   
  estimator = tf.estimator.DNNRegressor(
      feature_columns=clvf.get_deep_features(),
      hidden_units=params.hidden_units,
      config=config,
      model_dir=model_dir,
      optimizer=lambda: get_optimizer(params),
      batch_norm=True,
      dropout=params.dropout)
  
  estimator = tf.contrib.estimator.add_metrics(estimator, rmse_evaluator)       
  return estimator
```
```
#@title DNN: task.py (Hyperparameter + parser.args)

TRAIN_SIZE = 100000      #length of trainset is 883, but this param is given
NUM_EPOCHS = 70
BATCH_SIZE = 5
NUM_EVAL = 20

LEARNING_DECAY_RATE = 0.7
HIDDEN_UNITS = '128 64 32 16'
LEARNING_RATE = 0.00135
L1_REGULARIZATION = 0.0216647
L2_REGULARIZATION = 0.0673949
DROPOUT = 0.899732
SHUFFLE_BUFFER_SIZE = 10000

job_dir = '/content/model_checkpoint/'
data_src = '/content/DNNs_for_CLVs/'
ignore_crosses = False #default
learning_rate_decay = True

hypertune = False #adds numbers to ouputpath when turned on 'True'
resume = False #default (takes old savings for start if turned on 'True -> you may turn off hypertune then?)


def csv_serving_input_fn():
  clvf = CLVFeatures(ignore_crosses=True)
  used_headers = clvf.get_used_headers(with_key=True, with_target=False)
  default_values = clvf.get_defaults(used_headers)

  rows_string_tensor = tf.placeholder(dtype=tf.string, shape=[None],      #not compatible with egaer execution and tf.function?
                                      name='csv_rows')
  receiver_tensor = {'csv_rows': rows_string_tensor}

  row_columns = tf.expand_dims(rows_string_tensor, -1)
  columns = tf.decode_csv(row_columns, record_defaults=default_values)

  features = dict(zip(used_headers, columns))

  return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)
```
```
#@title DNN: task.py (main execution)

tf.logging.set_verbosity(tf.compat.v1.logging.INFO)

if hypertune:
  config = json.loads(os.environ.get('TF_CONFIG', '{}'))
  trial = config.get('task', {}).get('trial', '')
  model_dir = os.path.join(job_dir, trial)
else:
  model_dir = job_dir

data_folder = '{}'.format(data_src)

train_steps = (TRAIN_SIZE/BATCH_SIZE) * NUM_EPOCHS
checkpoint_steps = int((TRAIN_SIZE/BATCH_SIZE) * (
      NUM_EPOCHS/NUM_EVAL))

config = tf.estimator.RunConfig(
    save_checkpoints_steps=checkpoint_steps
)

hidden_units = [int(n) for n in HIDDEN_UNITS.split()]

params = tf.contrib.training.HParams(
    num_epochs = NUM_EPOCHS,
    train_steps = train_steps,
    batch_size = BATCH_SIZE,
    hidden_units = hidden_units,
    learning_rate = LEARNING_RATE,
    ignore_crosses = ignore_crosses,
    buffer_size = SHUFFLE_BUFFER_SIZE,
    learning_rate_decay = learning_rate_decay,
    learning_decay_rate = LEARNING_DECAY_RATE,
    l1_regularization = L1_REGULARIZATION,
    l2_regularization = L2_REGULARIZATION,
    dropout= DROPOUT,
    checkpoint_steps = checkpoint_steps)

estimator = None

estimator = get_estimator(config=config,
                          params=params,
                          model_dir=model_dir)

train_spec = tf.estimator.TrainSpec(
    input_fn=lambda: read_train(data_folder, params),
    max_steps=train_steps)

eval_spec = tf.estimator.EvalSpec(
    input_fn=lambda: read_eval(data_folder, params),
    exporters=[
        tf.estimator.LatestExporter(
            name='estimate',
            serving_input_receiver_fn=csv_serving_input_fn,
            exports_to_keep=1,
            as_text=True
        )
    ],
    steps=1000,
    throttle_secs=1,
    start_delay_secs=1
)

if not resume:
    print('Removing previous trained model...')
    shutil.rmtree(model_dir, ignore_errors=True)
else:
    print('Resuming training...')

tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
        
estimator.evaluate(lambda: read_test(data_folder, params), name=""Test Set"")
```


### System information:
- Have I written custom code: rather reproduced
- OS Platform: Big Sur 11.6
- TensorFlow version: 1.15.1 here, but same issue occured in TF2
- Python version: default version on Google Colab

### Describe the current behavior
Through the whole training process the RMSE is around 5341.822 and after the evaluation:
```
{'average_loss': 17710520.0,
 'global_step': 12362,
 'label/mean': 3189.7307,
 'loss': 87154410.0,
 'prediction/mean': 0.2885886,
 'rmse': 4208.3867}
```
So I am pretty new to the whole thing, but I think this model didn't learn anything.


### Describe the expected behavior
So the result I should get is given on the very below of the tutorial in this table marked as DNN (947.9). 

Model | RMSE
-- | --
DNN | 947.9
Pareto/NBD | 1558

#### by the way
It should be possible to copy the cells and execute them like they stand in this post, but you should decrease the train_size param. Because otherwise it will take one hour to learn, or... even not to learn.






"
52589,Modifying the Categorical Cross entropy for Mirrored strategy/Distributed training causing low validation accuracy,"**System information**
**Have I written custom code (yes)
**OS Platform and Distribution (Windows)
**TensorFlow installed from (binary)
**TensorFlow version (2.4.1)
**Python version(3.7.9)
**CUDA/cuDNN version(11.1/8/0):
**GPU model and memory(Titan XP, 12 GB):

**Describe the problem**
For **normal (single GPU training)** current loss is being calculated as follows:

```
def compute_loss(labels, predictions):
    loss = tf.reduce_mean(
    tf.keras.losses.categorical_crossentropy(y_true=labels, y_pred=predictions)
    )
    return loss
```

For **Mirrored strategy/Distributed training (8 GPU)**, I am computing loss as follows:


```
loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False,
              reduction=tf.keras.losses.Reduction.NONE)
def compute_loss(labels, predictions):
    per_example_loss = loss_object(labels, predictions) 
    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)
```

But in the Distributed strategy the loss is not converging as fast and getting very poor validation accuracy as compared to the original.
"
52588,Modifying the Categorical Cross entropy for Mirrored strategy/Distributed training causing low validation accuracy.,"### System information

-   **Have I written custom code (yes)
-   **OS Platform and Distribution (Windows)
-   **TensorFlow installed from (binary)
-   **TensorFlow version (2.4.1)
-   **Python version(3.7.9)
-   **CUDA/cuDNN version(11.1/8/0):
-   **GPU model and memory(Titan XP, 12 GB):

### Describe the problem
For **normal (single GPU training)** current loss is being calculated as follows:

```
def compute_loss(labels, predictions):
    loss = tf.reduce_mean(
    tf.keras.losses.categorical_crossentropy(y_true=labels, y_pred=predictions)
    )
    return loss
```

For **Mirrored strategy/Distributed training (8 GPU)**, I am computing loss as follows:

```
loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False,
              reduction=tf.keras.losses.Reduction.NONE)
def compute_loss(labels, predictions):
    per_example_loss = loss_object(labels, predictions) 
    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)
```

But in the Distributed strategy the loss is not converging as fast and getting very poor validation accuracy as compared to the original.
"
52587,tf.io.read_file loads indefinitely,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): `v2.4.0-49-g85c8b2a817f 2.4.1`
- Python version: `3.8.5`
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: - 
- GPU model and memory: - 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Splitting a Dataset  and then reading the first item of the Validation Dataset loads indefinitely.
```python
train_dataset = dataset.take(n_train_items)
val_dataset = dataset.skip(n_train_items)
```
Calling `next(iter(train_dataset .take(1)))` works calling `next(iter(val_dataset .take(1)))` loads indefinitely.


**Describe the expected behavior**
I want my Validation Dataset to just return Data as expected. 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): -
- Briefly describe your candidate solution(if contributing): -

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Notebook, that Produces the Issue: https://colab.research.google.com/drive/1w5RQDp6W3Zyu4p1kEBRlHT2Vl9zvxVOk?usp=sharing

Stackoverflow Post: https://stackoverflow.com/questions/69634758/reading-validation-dataset-takes-indefinitely-long
The Problem is withing `prep_image`, when i just return the Filename without opening the Data everything works as expected:

```python
def prep_image(filename, img_shape=(144,144), channels=3, dtype=tf.float32):
    return filename
#    image_string = tf.io.read_file(filename)
#    image = tf.image.decode_jpeg(image_string, channels=channels)
#    image = tf.image.convert_image_dtype(image, dtype)
#    image = tf.image.resize(image, img_shape)
#    return image
```
If i just return the Output of `tf.io.read_file` the first item of the Validation-Dataset already loads indefinitely:
```python
def prep_image(filename, img_shape=(144,144), channels=3, dtype=tf.float32):
    return tf.io.read_file(filename)
```

Edit:// Loading the First Train-Dataset Item Takes:
`0:00:00.063014`  `HH:MM:SS:ms`
and getting the first Validation item takes:
`0:31:09.717694`

"
52586,"ValueError ----> 1 cnn.fit(X_train, y_train, epochs=2)","Code:
cnn.fit(X_train, y_train, epochs=2)

I got an error:
ValueError Traceback (most recent call last)
in ()
----> 1 cnn.fit(X_train, y_train, epochs=2)

My Code Link in colab:
https://colab.research.google.com/drive/1dy-q0E8B7siaWNRhG9CVNWuw1NBIOTl2?usp=sharing

Total Code:
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np

(X_train, y_train), (X_test,y_test) = datasets.mnist.load_data()
X_train.shape

X_test.shape

plt.figure(figsize=(15,2))
plt.imshow(X_train[0])

y_train[:10]
y_train = y_train.reshape(-1,)
y_train[:10]

classes = [""five"",""zero"",""four"",""one"",""nine"",""two"",""one"",""three"",""one"",""four""]

def plot_sample(X, y, index):
    plt.figure(figsize = (15,3))
    plt.imshow(X[index])
    plt.xlabel(classes[y[index]])

X_train = X_train / 255.0
X_test = X_test / 255.0

cnn = models.Sequential([
layers.Conv2D(filters=28, kernel_size=(3, 3), activation='relu',
input_shape=(28, 28, 3)),
layers.MaxPooling2D((2, 2)),
layers.Conv2D(filters=56, kernel_size=(3, 3), activation='relu'),
layers.MaxPooling2D((2, 2)),
layers.Flatten(),
layers.Dense(28, activation='relu'),
layers.Dense(10, activation='softmax')
])

cnn.compile(optimizer='adam',
loss='sparse_categorical_crossentropy',
metrics=['accuracy'])
############ Error in below code############
cnn.fit(X_train, y_train, epochs=2)"
52579,Element-wise add and multiply operations are performed on CPU instead of GPU which bring about remarkable overhead,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.4.1**
- Python version: **Python 3.6.9**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **11.4**
- GPU model and memory: NVIDIA GeForce 1080Ti, 11176MiB

**Describe the current behavior**

I built a model based on tf.function and its runtime inference performance is very important for us. While using Tensorboard to profile the model inference time on GPU, I found the element-wise add and multiply operations are performed on CPU instead of GPU, which cause a remarkable overhead. Here's the codes of element-wise operations in the model:
```python
state = tf.gather(input_text, indices=indices, axis=1)
state = tf.math.add(state, byte_indices)
box = tf.gather(box, indices=state)
# more tf.gather and bit-wise operations
```
To run the model on GPU, I used:
```python
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
          profile_inference()
```
With Tensorboard, I got the profile UI as below. The state = tf.math.add(state, byte_indices) is done on CPU not GPU and it cause unexpected memory copy overhead.
![profile_with_explaination](https://user-images.githubusercontent.com/39686450/137975832-d97f04f2-cdff-4d70-9fd2-80efedd7759e.png)

Is there any way to force the element-wise operations, especially add and multiply, work on GPU?
"
52575,TF estimator train_and_evaluate: loss = None and model does not train,"Hi guys, I am working on a premade estimator model of tensorflow (DNNRegressor) in an old tensorflow version. I already found some similar issues but their solution didnt work out for me (they said it would be solved by setting max_steps to None). 

TF version: 1.15.1

```
train_spec = tf.estimator.TrainSpec(
    input_fn=lambda: read_train(data_folder, params),
    max_steps=None)
```
max_steps was 1400000 before and i tried now to set it to None, but it didnt work for me. My input pipeline looks like this:
```
clvf = CLVFeatures(ignore_crosses=True)

def parse_csv(csv_row):
  columns = tf.decode_csv(csv_row, record_defaults = clvf.get_all_defaults())
  features = dict(zip(clvf.get_all_names(), columns))
  
  for column_name in clvf.get_unused():
    features.pop(column_name)

  target = features.pop(clvf.get_target_name())

  return features, target

#@tf.function
def dataset_input_fn(data_folder, prefix=None, mode=None, params=None, count=None):
  shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False

  filenames = tf.matching_files('{}{}*.csv'.format(data_folder, prefix))
  dataset = tf.data.TextLineDataset(filenames)#skip(1)
  dataset = dataset.map(parse_csv)
  if shuffle:
    dataset = dataset.shuffle(buffer_size=params.buffer_size)
  dataset = dataset.repeat(count=count)
  dataset = dataset.batch(params.batch_size)

  iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)#tf.compat.v1.data.make_one_shot_iterator(dataset)/#tf.compat.v1.data.make_initializable_iterator(dataset)
  
  features, target = iterator.get_next()

  return features, target

def read_train(data_folder, params):
  return dataset_input_fn(
      data_folder=data_folder,
      prefix='train',
      params=params,
      mode=tf.estimator.ModeKeys.TRAIN)


def read_eval(data_folder, params):
  return dataset_input_fn(data_folder=data_folder,
                          prefix='eval',
                          params=params)


def read_test(data_folder, params):
  return dataset_input_fn(data_folder=data_folder,
                          prefix='test',
                          params=params,
                          count=1)
```

I would appreciate some help, so much. This is for school haha thank you!"
52574,No tf-nightly pip packages for MacOS available after 2021/09/22,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 11.7
- TensorFlow version: tf-nightly
- Python version: 3.9.4
- Installed using virtualenv? pip? conda?: pip

**Describe the problem**

tf-nightly pip packages seem to be missing MacOS builds after 2021/09/22. Trying `pip install --upgrade tf-nightly` installs version 2.7.0.dev20210922.

Trying to explicitly install a more recent version with `pip install --upgrade tf-nightly==2.8.0.dev20211019` fails as follows:

```
ERROR: Could not find a version that satisfies the requirement tf-nightly==2.8.0.dev20211019 (from versions: 2.6.0.dev20210611, 2.6.0.dev20210612, 2.6.0.dev20210613, 2.6.0.dev20210614, 2.6.0.dev20210615, 2.6.0.dev20210616, 2.6.0.dev20210617, 2.6.0.dev20210618, 2.6.0.dev20210619, 2.6.0.dev20210622, 2.6.0.dev20210623, 2.6.0.dev20210624, 2.6.0.dev20210625, 2.7.0.dev20210626, 2.7.0.dev20210627, 2.7.0.dev20210628, 2.7.0.dev20210629, 2.7.0.dev20210630, 2.7.0.dev20210701, 2.7.0.dev20210702, 2.7.0.dev20210703, 2.7.0.dev20210704, 2.7.0.dev20210705, 2.7.0.dev20210706, 2.7.0.dev20210707, 2.7.0.dev20210708, 2.7.0.dev20210709, 2.7.0.dev20210710, 2.7.0.dev20210711, 2.7.0.dev20210712, 2.7.0.dev20210713, 2.7.0.dev20210714, 2.7.0.dev20210715, 2.7.0.dev20210716, 2.7.0.dev20210717, 2.7.0.dev20210718, 2.7.0.dev20210719, 2.7.0.dev20210720, 2.7.0.dev20210721, 2.7.0.dev20210722, 2.7.0.dev20210723, 2.7.0.dev20210724, 2.7.0.dev20210725, 2.7.0.dev20210726, 2.7.0.dev20210727, 2.7.0.dev20210728, 2.7.0.dev20210729, 2.7.0.dev20210730, 2.7.0.dev20210731, 2.7.0.dev20210801, 2.7.0.dev20210802, 2.7.0.dev20210803, 2.7.0.dev20210804, 2.7.0.dev20210805, 2.7.0.dev20210806, 2.7.0.dev20210819, 2.7.0.dev20210820, 2.7.0.dev20210821, 2.7.0.dev20210822, 2.7.0.dev20210823, 2.7.0.dev20210824, 2.7.0.dev20210825, 2.7.0.dev20210827, 2.7.0.dev20210828, 2.7.0.dev20210829, 2.7.0.dev20210830, 2.7.0.dev20210831, 2.7.0.dev20210901, 2.7.0.dev20210902, 2.7.0.dev20210903, 2.7.0.dev20210904, 2.7.0.dev20210905, 2.7.0.dev20210906, 2.7.0.dev20210907, 2.7.0.dev20210908, 2.7.0.dev20210909, 2.7.0.dev20210910, 2.7.0.dev20210911, 2.7.0.dev20210912, 2.7.0.dev20210913, 2.7.0.dev20210914, 2.7.0.dev20210915, 2.7.0.dev20210916, 2.7.0.dev20210917, 2.7.0.dev20210918, 2.7.0.dev20210920, 2.7.0.dev20210921, 2.7.0.dev20210922)
ERROR: No matching distribution found for tf-nightly==2.8.0.dev20211019
```

**Any other info / logs**
Manually checking the [tf-nightly release files for 2021/09/22](https://pypi.org/project/tf-nightly/2.7.0.dev20210922/#files) does show a MacOSX build that is missing on the [23rd](https://pypi.org/project/tf-nightly/2.7.0.dev20210923/#files) and onwards."
52573,Crash with camera2api due to unsupported ImageFormat.YUV_420_888,"### Reproducible example
Example project from https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/android crashes after allowing camera permission due to:
```bash
org.tensorflow.lite.examples.poseestimation W/CameraDevice-JV-4: Stream configuration failed due to: createSurfaceFromGbp:1583: Camera 4: No supported stream configurations with format 0x23 defined, failed to create output stream
```

### Adbcat error log

From start to crash:
https://gist.githubusercontent.com/xRiot/f50680c94ec65dfdfae52fda609665b7/raw/aa95bf99362751f1af6d2e53fb20bfc9168c4bdc/gistfile1.txt

### Environment
This issue is specific to Android 11 on Samsung Galaxy s20+

### Source of the issue

This line causes the error, it seems like `YUV_420_888` is not supported on my particular device:
```kotlin
ImageReader.newInstance(PREVIEW_WIDTH, PREVIEW_HEIGHT, ImageFormat.YUV_420_888, 3)
```
"
52572,I Have this issue in Apple M1 ship that I can not run a training on GPU,"
I have MacBook Pro M1 ship and I face this issue when I start training with this as Benchmark 



my code : 
```
%%time
import tensorflow.compat.v2 as tf
import tensorflow_datasets as tfds

tf.enable_v2_behavior()

from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()


(ds_train, ds_test), ds_info = tfds.load(
    'mnist',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

def normalize_img(image, label):
  """"""Normalizes images: `uint8` -> `float32`.""""""
  return tf.cast(image, tf.float32) / 255., label

batch_size = 128

ds_train = ds_train.map(
    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)
ds_train = ds_train.batch(batch_size)
ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)


ds_test = ds_test.map(
    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
ds_test = ds_test.batch(batch_size)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)


model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(32, kernel_size=(3, 3),
                 activation='relu'),
  tf.keras.layers.Conv2D(64, kernel_size=(3, 3),
                 activation='relu'),
  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
#   tf.keras.layers.Dropout(0.25),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
#   tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(0.001),
    metrics=['accuracy'],
)

model.fit(
    ds_train,
    epochs=12,
    validation_data=ds_test,
)

```



Bug:

```

Metal device set to: Apple M1
WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x14a760dc0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function normalize_img at 0x14a760dc0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2021-10-19 13:11:41.757046: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2021-10-19 13:11:41.757319: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x14a760dc0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function normalize_img at 0x14a760dc0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function normalize_img at 0x14a760dc0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function normalize_img at 0x14a760dc0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2021-10-19 13:11:41.892468: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2021-10-19 13:11:41.892489: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2021-10-19 13:11:41.897826: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
2021-10-19 13:11:41.980346: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2021-10-19 13:11:41.991484: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2021-10-19 13:11:42.018471: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2021-10-19 13:11:42.032453: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2021-10-19 13:11:42.095177: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2021-10-19 13:11:42.110084: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2021-10-19 13:11:42.132235: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2021-10-19 13:11:42.149814: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2021-10-19 13:11:42.168231: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
```"
52571,Indexing into tf.shape(...) is broken when operating on KerasTensors (i.e. during Functional model building),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 (also Colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (also Colab)
- TensorFlow version (use command below): 2.6.0
- Python version: 3.7.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):  N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Calling tf.shape(...) on a tensor in a layer call, and then indexing into the shape (to get the inferred value at, e.g. the last dimension) returns None, even if the full shape is known.

**Describe the expected behavior**

We should be able to index into shapes acquired this way.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

[Colab Link](https://colab.research.google.com/drive/1GmIBQegnYN2N03BIDFu7kUe0XSiYt-4L?usp=sharing)

```
import sys
print(""Python {}"".format(sys.version))
import tensorflow as tf
print(""Tensorflow {}"".format(tf.__version__))
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Input

X = Input(shape=(4,))

x_shape = tf.shape(X)
feature_dim = tf.shape(X)[-1]
# Other ways of calculating
backend_dim = K.shape(X)[-1]
get_shape_dim = X.get_shape()[-1]
as_list_dim = X.shape.as_list()[-1]

print(x_shape)
print(feature_dim)
print(x_shape[-1])
print(backend_dim)
print(get_shape_dim)
print(as_list_dim)
```
"
52569,AttributeError: 'Tensor' object has no attribute 'values' in TensorFlow version: 2.7.0-dev20210922,"TensorFlow version: 2.7.0-dev20210922

I was testing the code from : https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough

After this line: 

train_dataset = train_dataset.map(pack_features_vector)
features, labels = next(iter(train_dataset))

I got this error: 

AttributeError: in user code:

    File ""<ipython-input-11-b2720b65aaf7>"", line 3, in pack_features_vector  *
        features = tf.stack(list(features.values()), axis=1)

    AttributeError: 'Tensor' object has no attribute 'values'

It was okay with tf 2.6.0. 

"
52562,prevent gpu memory allocation for MonitoredTrainingSession,"I am trying to restrict GPU memory allocation in a MonitoredTrainingSession.

The methods of setting tf.GPUOptions as shown here: How to prevent tensorflow from allocating the totality of a GPU memory? do not work out in the case of MonitoredTrainingSession.

I tried:
`gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=.1)
config = tf.ConfigProto(allow_soft_placement=False,
                        device_filters=filters,
                        gpu_options=gpu_options)

scaffold = tf.train.Scaffold(saver=tf.train.Saver(max_to_keep=100, keep_checkpoint_every_n_hours=.5))

with tf.train.MonitoredTrainingSession(
                server.target,
                is_chief=True,
                checkpoint_dir=log_dir,
                scaffold=scaffold,
                save_checkpoint_secs=600,
                save_summaries_secs=30,
                log_step_count_steps=int(1e7),
                config=config) as session:`
Despite using tf.GPUOptions memory consumption is 10189MiB / 11175MiB"
52561,2.7.0rc1 build error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubnutu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7.0rc1
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 4.2.1
- GCC/Compiler version (if compiling from source):  gcc 9.3.0
- CUDA/cuDNN version: 11.4/8.2
- GPU model and memory: RTX3090 GDDR6X 24GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
configure with CUDA support
bazel build --config=numa --config=nogcp //tensorflow/tools/pip_packages:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: /home/alan/.cache/bazel/_bazel_alan/166d89c2dc31e370baae784b1f517718/external/llvm-project/mlir/BUILD.bazel:2292:11: Compiling mlir/lib/Conversion/ShapeToStandard/ShapeToStandard.cpp failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/llvm-project/mlir/_objs/ShapeToStandard/ShapeToStandard.d ... (remaining 124 argument(s) skipped)
In file included from /usr/include/c++/9/bits/unique_ptr.h:37,
                 from /usr/include/c++/9/memory:80,
                 from external/llvm-project/mlir/include/mlir/Conversion/ShapeToStandard/ShapeToStandard.h:12,
                 from external/llvm-project/mlir/lib/Conversion/ShapeToStandard/ShapeToStandard.cpp:9:
/usr/include/c++/9/tuple: In instantiation of ‘constexpr std::_Tuple_impl<_Idx, _Head, _Tail ...>::_Tuple_impl(std::_Tuple_impl<_Idx, _Head, _Tail ...>&&) [with long unsigned int _Idx = 0; _Head = mlir::Value; _Tail = {mlir::Value}]’:
external/llvm-project/mlir/lib/Conversion/ShapeToStandard/ShapeToStandard.cpp:85:53:   recursively required from ‘constexpr std::tuple<_T1, _T2>::tuple(std::tuple<_T1, _T2>&&) [with _T1 = mlir::Value; _T2 = mlir::Value]’
external/llvm-project/mlir/lib/Conversion/ShapeToStandard/ShapeToStandard.cpp:85:53:   required from here
/usr/include/c++/9/tuple:227:7: internal compiler error: in lookup_template_class_1, at cp/pt.c:9466
  227 |       _Tuple_impl(_Tuple_impl&& __in)
      |       ^~~~~~~~~~~
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```"
52557,Issue created for Rollback of PR #52553: Update Readme - Test PR Notification Change,"Merged PR #52553 is rolled back in 37e47aca9c5dcbb6d27093b9550d4f0b1cca3307.
    Please follow up with the reviewer and close this issue once its resolved."
52555,Issue created for Rollback of PR #52553: Update Readme - Test PR Notification Change,"Merged PR #52553 is rolled back in e98b052c08e5d1e7906ac2f6caf95c51a1e04985.
    Please follow up with the reviewer and close this issue once its resolved."
52552,Significant difference in trained model between tf2.3 and tf>2.4 ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): osx 11.2 and google colab
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3/2.4/2.5/2.6
- Python version: 3.8.11

**Describe the current behavior**
I am working on a small deep q-network code for a course, applying it to cartpole-v0
The implementation achieves standard DQN performance level when using tf2.3.0, but learning fails (code runs but performance is extremely poor) when using tf2.4.0 and superior (tested 2.4.0, 2.5.0, 2.6.0 under osx, came accross the problem while trying to run the code on colab)

**Describe the expected behavior**
Performance should be identical on all versions

- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**
This code runs fine with tf2.3 (it shows standard DQN performance level), while learning performance totally collapses for tf2.4 and superior:
https://colab.research.google.com/drive/1xKh_Lw8gd1chPhNC8MzXmifC2zguzgU_?usp=sharing

Thanks in advance
"
52544,TensorFlow unit test failure python/kernel_tests:self_adjoint_eig_op_test,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Aarch64 Centos-8
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master
- Python version: 3.6.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: 
- GPU model and memory:

**Describe the current behavior**

Running the bazel : python/kernel_tests/self_adjoint_eig_op_test fails. 
Building the self_adjoint_eig_op_test with the following command: 
`bazel build --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=noaws --config=nogcp --config=nonccl --verbose_failures -- //tensorflow/python/kernel_tests:self_adjoint_eig_op_test`

dtypes of float32 and complex64 with a [size](https://github.com/tensorflow/tensorflow/blob/5646333e1ece8170dff7b1980905e8f9c8383881/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py#L247) of 10 all fail while the rest pass:
```
[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_3_10_10_True
[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_3_10_10_False
[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_10_10_True
[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_float32_10_10_False
[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_3_10_10_True
[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_3_10_10_False
[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_10_10_True
[  FAILED  ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex64_10_10_False

[       OK ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex128_3_10_10_False
[       OK ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_complex128_3_10_10_True
[       OK ] SelfAdjointEigGradTest.test_SelfAdjointEigGrad_float64_3_10_10_True
```

This error is occurs on Aarch64 after [discarding a random input: `_ = RandomInput()`](https://github.com/tensorflow/tensorflow/blob/5646333e1ece8170dff7b1980905e8f9c8383881/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py#L205) sample in the _GetSelfAdjointEigGradTest function. Without this input, or with when there is a second input discarded before running the tests, they pass as expected.

**Describe the expected behavior**

For python/kernel_tests/self_adjoint_eig_op_test to pass.
"
52542, Steps to create tensorflow lite c++ dynamic library through cmake.,"

I am trying to create dynamic TFLite c++ shared library.
https://www.tensorflow.org/lite/guide/build_cmake -

When I run step 5 , It creates static library.

Could you provide steps to create c++ dynamic library using cmake.
There is mentioned how to create only for c shared dynamic library but not c++.

"
52541,ModuleNotFoundError: No module named 'tensorflow.keras.model',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 

         Edition	Windows 10 Home Single Language
        Version	21H1
        Installed on	‎6/‎27/‎2020
        OS build	19043.1288
        Serial number	YN006FB0
        Experience	Windows Feature Experience Pack 120.2212.3920.0

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Applicable
- TensorFlow installed from (source or binary): Created env with Conda and then executed conda install tensorflow==2.5.0
- TensorFlow version: 2.5.0
- Python version: 3.8.12
- Installed using virtualenv? pip? conda?:  Created env with Conda and then executed conda install tensorflow==2.5.0
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 5.3.2
- CUDA/cuDNN version: 
- GPU model and memory:  GeForce MX250 2048 MB



**Describe the problem**
Unable to use keras from tensorflow module and encoutering this error:  
**ModuleNotFoundError: No module named 'tensorflow.keras.model'**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
import numpy as np
import pandas as pd
#import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error,r2_score
from tensorflow.keras.model import Sequential
from tensorflow.keras.layers import Dense,LSTM,Dropout


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
52540,TensorRT ValueError Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource,"**faild to use trt_convert to convert tensorflow .pb file to tensorrt**
**model code:**
```
inputs = keras.Input(shape=(200,), dtype=""int32"")
# Embed each integer in a 128-dimensional vector
x = layers.Embedding(max_features, 256)(inputs)
# Add 2 bidirectional LSTMs
# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
# x = layers.Bidirectional(layers.LSTM(64))(x)
x = layers.LSTM(64)(x)
# Add a classifier
outputs = layers.Dense(1, activation=""sigmoid"")(x)
model = keras.Model(inputs, outputs)
```
**convter pb to trt code**
```
model_dir = '/data/danlu/tensorrt/resnet/lstm'
opt_model_dir = '/data/danlu/tensorrt/resnet/lstm_trt'
 
precision = ""FP32""
max_workspace_size_bytes = 8000000000
conversion_params = tf_trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=precision,
                                                                  max_workspace_size_bytes=max_workspace_size_bytes,
                                                                  maximum_cached_engines=100)
converter = tf_trt.TrtGraphConverterV2(input_saved_model_dir=model_dir, conversion_params=conversion_params)
converter.convert()
 
 
def build_fn():
    Inp1 = np.random.randint(1, 200, (1, 200), dtype=np.int32)
    yield Inp1
 
 
converter.build(input_fn=build_fn)
converter.save(opt_model_dir)
```
**pb file**
[https://drive.google.com/file/d/1M4TEB3t1SDpBFj-RS7WL8AuWybZaP1YZ/view?usp=sharing](url)

**env:**
os: ubutun 16.04
python: 3.8.3
tensorflow: 2.6.0
TensorRT: 7.2.3.4
cuda: 10.2
gpu: v100:32g

**error log**
```
Traceback (most recent call last):
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 496, in _import_graph_def_internal
    results = c_api.TF_GraphImportGraphDefWithResults(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""trt_transfer.py"", line 21, in <module>
    converter.convert()
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 1087, in convert
    frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1074, in convert_variables_to_constants_v2
    return _construct_concrete_function(func, output_graph_def,
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 999, in _construct_concrete_function
    new_func = wrap_function.function_from_graph_def(output_graph_def,
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 650, in function_from_graph_def
    wrapped_import = wrap_function(_imports_graph_def, [])
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 621, in wrap_function
    func_graph.func_graph_from_py_func(
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 87, in __call__
    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 93, in wrapped
    return fn(*args, **kwargs)
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 648, in _imports_graph_def
    importer.import_graph_def(graph_def, name="""")
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 400, in import_graph_def
    return _import_graph_def_internal(
  File ""/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 501, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.
```
"
52538,AttributeError: module 'keras.engine.base_layer' has no attribute 'BaseRandomLayer',"Hello. 

Context: I'm using Colab and most of the time I'm using the  GPU runtime and High-RAM. I trained EfficientNetb7 and saved the model. When I ran tf.keras.load_model(), I got a bunch of error messages like:
```
WARNING:absl:Importing a function (__inference_block2f_activation_layer_call_and_return_conditional_losses_230237) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_sequential_6_layer_call_and_return_conditional_losses_294716) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
```
So I did some Googling and I think someone recommended installing tf-nightly. That seems to have been a mistake! I couldn't get the model to re-run after that. I uninstalled tf-nightly, and restarted my runtime. I reinstalled tf version 2.6. Now when I go to run my imports, I get the error message in the subject line. Here are some more details of the error message: 

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-25-5d853ee59f92> in <module>()
     11 import tensorflow as tf
     12 from tensorflow import keras
---> 13 from tensorflow.keras import Model
     14 from tensorflow.keras.models import Sequential
     15 from tensorflow.keras.utils import to_categorical

11 frames
/usr/local/lib/python3.7/dist-packages/keras/layers/core/dropout.py in <module>()
     24 
     25 @keras_export('keras.layers.Dropout')
---> 26 class Dropout(base_layer.BaseRandomLayer):
     27   """"""Applies Dropout to the input.
     28 

AttributeError: module 'keras.engine.base_layer' has no attribute 'BaseRandomLayer'
```
Any ideas on what is going on and how to fix it??"
52537,Ref() does not return the same ref for the element.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 34
- TensorFlow installed from (source or binary): 2.6.0
- Python version: 3.9.7

**Describe the current behavior**

When loading MINST and requesting the ref() of the first element, it is not equal to itself.

**Describe the expected behavior**

The element should have the same ref at all time.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no, I don't know why it's doing this.

**Standalone code to reproduce the issue**

```
import tensorflow_datasets as tfds

# %% Train dataset
(ds_train_original, ds_test_original), ds_info = tfds.load(
    ""mnist"",
    split=[""train"", ""test""],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

iterator = iter(ds_train_original)
el = iterator.get_next()[0]
el[0].ref() == el[0].ref()   # <- this should be True
```
"
52536,"Tensorflow serving in docker causes ""Invalid reduction dimension (1 for input with 1 dimension(s)...","I am using tensorflow-serving which runs with the build-in examples well and without issues:
cd c:\tmp\tfserving
PS C:\tmp\tfserving> C:\programme\git\bin\git clone https://github.com/tensorflow/serving
set-variable -Name ""TESTDATA"" -Value ""$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata""
docker run -t --rm -p 8501:8501 -v ""$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two"" -e MODEL_NAME=half_plus_two tensorflow/serving
curl -d ""{\""instances\"": [1.0, 2.0, 5.0]}"" -X POST http://localhost:8501/v1/models/half_plus_two:predict

However, by trying to run the following model in a recent docker container it causes:
""Invalid reduction dimension (1 for input with 1 dimension(s)\

Steps to verify:
I used the example code ...
https://keras.io/examples/structured_data/structured_data_classification_from_scratch/
and at the end saved the model 
model.save('my-model.tf')

and put it into the serving directory where it was recognized:
C:\tmp\tfserving\serving\tensorflow_serving\servables\tensorflow\testdata
cd \tmp\tfserving
set-variable -Name ""TESTDATA"" -Value ""$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata""
docker run -t --rm -p 8501:8501 -v ""$TESTDATA/my_model:/models/my_model"" -e MODEL_NAME=my_model tensorflow/serving

curl http://localhost:8501/v1/models/my_model
{  ""model_version_status"": [  { ""version"": ""1"", ""state"": ""AVAILABLE"", ""status"": { ""error_code"": ""OK"", [...]

However, trying to run a prediction 

curl -d ""{ \""instances\"": [ {\""age\"": 50,\""sex\"": 1,\""cp\"": 1,\""trestbps\"": 145,\""chol\"": 133,\""fbs\"": 1,\""restecg\"": 2,\""thalach\"": 150,\""exang\"": 0,\""oldpeak\"": 2.3,\""slope\"": 3,\""ca\"": 0,\""thal\"": \""fixed\"" } ]}"" -X POST http://localhost:8501/v1/models/my_model:predict

... returns:
>>""error"": ""Invalid reduction dimension (1 for input with 1 dimension(s)\n\t [[{{node model/integer_lookup_5/bincount/Max}}]]""


**System information**
- Running on Windows with no GPU-Support. No further changes were made on the example code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n.a.
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.8.11 (default, Aug  6 2021, 09:57:55) [MSC v.1916 64 bit (AMD64)]
- Bazel version (if compiling from source): have also tried to compile on linux with bazel but with the same result
- GCC/Compiler version (if compiling from source): n.a.
- CUDA/cuDNN version: n.a.
- GPU model and memory: n.a.


"
52535,"Cannot update variable with shape [2] using a Tensor with shape [512,1], shapes must be equal. [Op:AssignAddVariableOp]","I think this issue is more relevant to tensorflow than keras hence I'm asking it here and not in the keras repository. 
I've also gone through different versions of this problem on stack overflow but I can't seem to figure out what is going wrong.
Before starting , I'd also like to add that I've asked this question on the Tensorflow Forum  & SO, but didn't get any response.

My model takes one input and gives two outputs , I'm passing it in a dictionary:
```Python
train_dataset = tf.data.Dataset.from_tensor_slices(
    (
        {""input_1"": atr},
        {""ed"": wtr, ""sd"": wbtr},
    )
)
train_dataset = train_dataset.batch(100).repeat(3)
```
Shape of all the three arrays `atr` , `wtr` and `wbtr` is `(7838, 512, 1)`.
This is my model:
```Python
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras import Input, Model
input1 = tf.keras.layers.Input(shape=(None,1),name=""input_1"")
x = tf.keras.layers.Conv1D(filters=16, kernel_size=3, strides=1, padding=""causal"", activation=""relu"",input_shape=[None,1])(input1)
x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, activation=""tanh"", return_sequences=True))(x)
x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, activation=""tanh"", return_sequences=True))(x)
x = tf.keras.layers.Dense(128, activation=""tanh"")(x)
o1 = tf.keras.layers.Dense(1, activation=""linear"",name=""ed"")(x)
o2 = tf.keras.layers.Dense(1, activation=""sigmoid"",name=""sd"")(x)

model = Model(inputs=[input1], outputs=[o1, o2])

model.compile(loss={'ed': 'mean_squared_error', 
                    'sd': 'binary_crossentropy'},
              loss_weights={'ed':0.4,
                            'sd':0.6},
              optimizer='adam',
              metrics={'ed': tf.keras.metrics.MeanAbsoluteError(name=""mean_absolute_error"", dtype=None),
                       'sd': tfa.metrics.F1Score(name=""f1_score"",num_classes=2, threshold=0.5)})
```
Here is the model summary:
```PythonModel: ""model_14""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, None, 1)]    0                                            
__________________________________________________________________________________________________
conv1d_18 (Conv1D)              (None, None, 16)     64          input_1[0][0]                    
__________________________________________________________________________________________________
bidirectional_33 (Bidirectional (None, None, 256)    112128      conv1d_18[0][0]                  
__________________________________________________________________________________________________
bidirectional_34 (Bidirectional (None, None, 512)    789504      bidirectional_33[0][0]           
__________________________________________________________________________________________________
dense_17 (Dense)                (None, None, 128)    65664       bidirectional_34[0][0]           
__________________________________________________________________________________________________
ed(Dense)                      (None, None, 1)      129         dense_17[0][0]                   
__________________________________________________________________________________________________
sd (Dense)                      (None, None, 1)      129         dense_17[0][0]                   
==================================================================================================
Total params: 967,618
Trainable params: 967,618
Non-trainable params: 0
__________________________
```
Finally my `model.fit()` method:
```Python
history = model.fit(train_dataset,epochs=3,verbose=1,steps_per_epoch= 78)
```
Here is the error:
```Python
    InvalidArgumentError: Cannot update variable with shape [2] using a Tensor with shape [512,1], shapes must be equal. [Op:AssignAddVariableOp]
```
I don't where I'm going wrong , pls help.
"
52534,TypeError: Protocols cannot be instantiated - when iterating over a dataset,"**System information**
- OS Platform and Distribution: Ubuntu 21.04
- TensorFlow installed from binary: pip install tf-nightly
- TensorFlow version: v1.12.1-65564-gea2d617f792 2.8.0-dev20211016
- Python version: 3.9

**Describe the current behavior**
I can not iterate over a tf.data.Dataset . My guess is, that I might have a version mismatch in a dependency, but I can not figure it out.

    Traceback (most recent call last):
      File ""/home/----------/example.py"", line 6, in <module>
        for element in ds:
      File ""/home/----------/miniconda3/envs/tf-gpu-07/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 481, in __iter__
        return iterator_ops.OwnedIterator(self)
      File ""/home/----------/miniconda3/envs/tf-gpu-07/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 727, in __init__
        super(OwnedIterator, self).__init__()
      File ""/home/----------/miniconda3/envs/tf-gpu-07/lib/python3.9/typing.py"", line 1083, in _no_init
        raise TypeError('Protocols cannot be instantiated')
    TypeError: Protocols cannot be instantiated

**Describe the expected behavior**
tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)
tf.Tensor(3, shape=(), dtype=int32)
tf.Tensor(4, shape=(), dtype=int32)
tf.Tensor(5, shape=(), dtype=int32)

**Standalone code to reproduce the issue**

    import tensorflow as tf
    
    ds = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])
    
    for element in ds:
        print(element)
"
52533,Failed to build python3 package,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 21.10
- TensorFlow installed from (source or binary):source
- TensorFlow version:2.6.0
- Python version:3.9.7
- Bazel version (if compiling from source):3.7.2
- GCC/Compiler version (if compiling from source):GCC9
- CUDA/cuDNN version:11.0
- GPU model and memory:RTX2060 6GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
❯ bazel build //tensorflow/tools/pip_package:build_pip_package --config=opt --config=cuda --config=mkl
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=86
INFO: Reading rc options for 'build' from /home/zwq/workspace/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/zwq/workspace/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /home/zwq/workspace/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda
INFO: Found applicable config definition build:short_logs in file /home/zwq/workspace/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/zwq/workspace/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/zwq/workspace/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /home/zwq/workspace/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:cuda in file /home/zwq/workspace/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:mkl in file /home/zwq/workspace/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:linux in file /home/zwq/workspace/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false
INFO: Found applicable config definition build:dynamic_kernels in file /home/zwq/workspace/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /home/zwq/.cache/bazel/_bazel_zwq/004cf1a57827e926c0c6bde2a3de4f2d/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (444 packages loaded, 29466 targets configured).
INFO: Found 1 target...
Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
INFO: Elapsed time: 24.994s, Critical Path: 8.73s
INFO: 1 process: 1 internal.
INFO: Build completed successfully, 1 total action
❯ 
❯  bazel-bin/tensorflow/tools/pip_package/build_pip_package pkg
2021年 10月 17日 星期日 12:22:28 CST : === Preparing sources in dir: /tmp/tmp.FwVpMUz8nN
~/workspace/tensorflow ~/workspace/tensorflow
~/workspace/tensorflow
~/workspace/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow ~/workspace/tensorflow
~/workspace/tensorflow
/tmp/tmp.FwVpMUz8nN/tensorflow/include ~/workspace/tensorflow
~/workspace/tensorflow
2021年 10月 17日 星期日 12:22:58 CST : === Building wheel
usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]
   or: setup.py --help [cmd1 cmd2 ...]
   or: setup.py --help-commands
   or: setup.py cmd --help

error: invalid command 'bdist_wheel'
```
C++ Library has been compiled successfully."
52531,"tf.data.dataset function that only creates dataset of windows, containing tensors","It is necessary to have tf.data.dataset function that creates dataset of windows of tensor type (instead of datasets of timestamps).

**System information**
- TensorFlow version (you are using): 2.6
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

Currently, there are 2 functions that do smth similar, but not exactly:
- `tf.keras.utils.timeseries_dataset_from_array`  that also does unnecessary batching
-  `dataset.window` that creates nested Dataset. And in the case when all timestamps need to be joined together in a single Tensor - it is a problem, as Dataset API doesn't allow to join multiple dataset elements in a Tensor (or am I missing smth?)
(BTW, those 2 functions confusingly use opposite names for the arguments with a similar behavior)

Lack of a single function doing what requested above - leads to hacks [like this one, described at StackOverflow](https://stackoverflow.com/questions/64497977/tensorflow-convert-tf-dataset-to-tf-tensor)

Detailed use case:
Raw data comes has 1 sec frequency. For prediction history up to several hours is relevant. But old history is far less important (only general level), while the last minute data must be taken with the highest resolution (i.e. every second).
It is necessary to create 5 hour windows (i.e. windows size = 3600*5=18000 ) and select from them only a hundred of timestamps, that likely contain all the relevant information for the modl.

**Who will benefit with this feature?**
Those who need to do further transformations over the timeseries windows.
As a more specific example: those who need to have an uneven sampling of timesteps (.e.g  gradually increasing frequency)"
52530,Failed to initialize NVML: Driver/library version mismatch,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

Can someone help me please with this issue?
Thank you

System information

OS Platform and Distribution: Ubuntu 20.04
TensorFlow installed from (source or binary): pip install --upgrade tensorflow-gpu
TensorFlow version:
tensorflow==2.6.0
tensorflow-estimator==2.6.0
tensorflow-gpu==2.6.0
tensorflow-io-gcs-filesystem==0.21.0
tf-estimator-nightly==2.7.0.dev2021092408
tf-nightly==2.7.0.dev20210922
Python version: Python 3.7.3
Installed using virtualenv? pip? conda?: (condo env) pip install --upgrade tensorflow-gpu
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): gcc version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)
CUDA/cuDNN version: kernel version 460.91.3 does not match DSO version 470.57.2 -- cannot find working devices in this configuration
<img width=""804"" alt=""image"" src=""https://user-images.githubusercontent.com/74671619/137600054-1be66398-2738-421d-9cc3-72c150a066fb.png"">

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

After following these steps $ nvidia-smi is not working anymore

Failed to initialize NVML: Driver/library version mismatch
<img width=""1355"" alt=""image"" src=""https://user-images.githubusercontent.com/74671619/137599966-a0d39a5a-f80c-429e-b156-76447489a48e.png"">


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

$ nvidia-smi
Failed to initialize NVML: Driver/library version mismatch

2021-10-16 14:56:09.420644: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW
2021-10-16 14:56:09.420676: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Primer
2021-10-16 14:56:09.420700: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Primer
2021-10-16 14:56:09.420758: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2
2021-10-16 14:56:09.420782: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3
2021-10-16 14:56:09.420791: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.57.2 -- cannot find working devices in this configuration

$ uname -a
Linux Primer 5.4.0-88-generic #99-Ubuntu SMP Thu Sep 23 17:29:00 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux

$ nvidia-settings --help

nvidia-settings:  version 470.57.02
  The NVIDIA X Server Settings tool.

$ nvidia-smi
Failed to initialize NVML: Driver/library version mismatch

$ sudo prime-select query
nvidia

$ cat /proc/driver/nvidia/version
NVRM version: NVIDIA UNIX x86_64 Kernel Module  460.91.03  Fri Jul  2 06:04:10 UTC 2021
GCC version:  gcc version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)

$ sudo lspci -v | grep VGA
02:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1) (prog-if 00 [VGA controller])
03:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1) (prog-if 00 [VGA controller])
04:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1) (prog-if 00 [VGA controller])

$ lspci | grep -i nvidia
02:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)
02:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)
03:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)
03:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)
04:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)
04:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)

$ dpkg -l | grep -i nvidia
ii  cuda-nsight-compute-11-2                        11.2.0-1   amd64        NVIDIA Nsight Compute
ii  cuda-nsight-compute-11-4                        11.4.2-1   amd64        NVIDIA Nsight Compute
ii  cuda-nsight-systems-11-2                        11.2.0-1   amd64        NVIDIA Nsight Systems
ii  cuda-nsight-systems-11-4                        11.4.2-1   amd64        NVIDIA Nsight Systems
ii  cuda-nvtx-11-2                                  11.2.67-1   amd64        NVIDIA Tools Extension
ii  cuda-nvtx-11-4                                  11.4.120-1   amd64        NVIDIA Tools Extension
ii  libaccinj64-10.1:amd64                          10.1.243-3   amd64        NVIDIA ACCINJ Library (64-bit)
ii  libcublas10:amd64                               10.1.243-3   amd64        NVIDIA cuBLAS Library
ii  libcublaslt10:amd64                             10.1.243-3   amd64        NVIDIA cuBLASLt Library
ii  libcudart10.1:amd64                             10.1.243-3   amd64        NVIDIA CUDA Runtime Library
ii  libcufft10:amd64                                10.1.243-3   amd64        NVIDIA cuFFT Library
ii  libcufftw10:amd64                               10.1.243-3   amd64        NVIDIA cuFFTW Library
ii  libcuinj64-10.1:amd64                           10.1.243-3   amd64        NVIDIA CUINJ Library (64-bit)
ii  libcupti-dev:amd64                              10.1.243-3   amd64        NVIDIA CUDA Profiler Tools Interface development files
ii  libcupti-doc                                    10.1.243-3   all          NVIDIA CUDA Profiler Tools Interface documentation
ii  libcupti10.1:amd64                              10.1.243-3   amd64        NVIDIA CUDA Profiler Tools Interface runtime library
ii  libcurand10:amd64                               10.1.243-3   amd64        NVIDIA cuRAND Library
ii  libcusolver10:amd64                             10.1.243-3   amd64        NVIDIA cuSOLVER Library
ii  libcusolvermg10:amd64                           10.1.243-3   amd64        NVIDIA cuSOLVERmg Library
ii  libcusparse10:amd64                             10.1.243-3   amd64        NVIDIA cuSPARSE Library
ii  libnppc10:amd64                                 10.1.243-3   amd64        NVIDIA Performance Primitives core runtime library
ii  libnppial10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Arithmetic and Logic
ii  libnppicc10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Color Conversion
ii  libnppicom10:amd64                              10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Compression
ii  libnppidei10:amd64                              10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Data Exchange and Initialization
ii  libnppif10:amd64                                10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Filters
ii  libnppig10:amd64                                10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Geometry transforms
ii  libnppim10:amd64                                10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Morphological operations
ii  libnppist10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Statistics
ii  libnppisu10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Support
ii  libnppitc10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Threshold and Compare
ii  libnpps10:amd64                                 10.1.243-3   amd64        NVIDIA Performance Primitives for signal processing runtime library
ii  libnvgraph10:amd64                              10.1.243-3   amd64        NVIDIA Graph Analytics library (nvGRAPH)
ii  libnvidia-cfg1-470:amd64                        470.57.02-0ubuntu1   amd64        NVIDIA binary OpenGL/GLX configuration library
ii  libnvidia-common-460                            460.27.04-0ubuntu1   all          Shared files used by the NVIDIA libraries
ii  libnvidia-common-470                            470.57.02-0ubuntu1   all          Shared files used by the NVIDIA libraries
ii  libnvidia-compute-418:amd64                     430.50-0ubuntu3   amd64        Transitional package for libnvidia-compute-430
ii  libnvidia-compute-430:amd64                     470.57.02-0ubuntu1   amd64        Transitional package for libnvidia-compute-470
rc  libnvidia-compute-450:amd64                     450.102.04-0ubuntu0.20.04.1   amd64        NVIDIA libcompute package
rc  libnvidia-compute-460:amd64                     460.91.03-0ubuntu0.20.04.1   amd64        NVIDIA libcompute package
ii  libnvidia-compute-470:amd64                     470.57.02-0ubuntu1   amd64        NVIDIA libcompute package
ii  libnvidia-decode-470:amd64                      470.57.02-0ubuntu1   amd64        NVIDIA Video Decoding runtime libraries
ii  libnvidia-encode-470:amd64                      470.57.02-0ubuntu1   amd64        NVENC Video Encoding runtime library
ii  libnvidia-extra-470:amd64                       470.57.02-0ubuntu1   amd64        Extra libraries for the NVIDIA driver
ii  libnvidia-fbc1-470:amd64                        470.57.02-0ubuntu1   amd64        NVIDIA OpenGL-based Framebuffer Capture runtime library
ii  libnvidia-gl-470:amd64                          470.57.02-0ubuntu1   amd64        NVIDIA OpenGL/GLX/EGL/GLES GLVND libraries and Vulkan ICD
ii  libnvidia-ifr1-470:amd64                        470.57.02-0ubuntu1   amd64        NVIDIA OpenGL-based Inband Frame Readback runtime library
ii  libnvidia-ml-dev                                10.1.243-3   amd64        NVIDIA Management Library (NVML) development files
ii  libnvjpeg10:amd64                               10.1.243-3   amd64        NVIDIA JPEG library (nvJPEG)
ii  libnvrtc10.1:amd64                              10.1.243-3   amd64        CUDA Runtime Compilation (NVIDIA NVRTC Library)
ii  libnvtoolsext1:amd64                            10.1.243-3   amd64        NVIDIA Tools Extension Library
ii  libnvvm3:amd64                                  10.1.243-3   amd64        NVIDIA NVVM Library
ii  nsight-compute                                  10.1.243-3   amd64        NVIDIA Nsight Compute
ii  nsight-compute-2020.3.0                         2020.3.0.18-1   amd64        NVIDIA Nsight Compute
ii  nsight-compute-2021.2.2                         2021.2.2.1-1   amd64        NVIDIA Nsight Compute
rc  nvidia-compute-utils-450                        450.102.04-0ubuntu0.20.04.1   amd64        NVIDIA compute utilities
rc  nvidia-compute-utils-460                        460.91.03-0ubuntu0.20.04.1   amd64        NVIDIA compute utilities
ii  nvidia-compute-utils-470                        470.57.02-0ubuntu1   amd64        NVIDIA compute utilities
ii  nvidia-cuda-dev                                 10.1.243-3   amd64        NVIDIA CUDA development files
ii  nvidia-cuda-doc                                 10.1.243-3   all          NVIDIA CUDA and OpenCL documentation
ii  nvidia-cuda-gdb                                 10.1.243-3   amd64        NVIDIA CUDA Debugger (GDB)
ii  nvidia-cuda-toolkit                             10.1.243-3   amd64        NVIDIA CUDA development toolkit
rc  nvidia-dkms-450                                 450.102.04-0ubuntu0.20.04.1   amd64        NVIDIA DKMS package
rc  nvidia-dkms-460                                 460.91.03-0ubuntu0.20.04.1   amd64        NVIDIA DKMS package
ii  nvidia-dkms-470                                 470.57.02-0ubuntu1   amd64        NVIDIA DKMS package
ii  nvidia-driver-470                               470.57.02-0ubuntu1   amd64        NVIDIA driver metapackage
rc  nvidia-kernel-common-450                        450.102.04-0ubuntu0.20.04.1   amd64        Shared files used with the kernel module
rc  nvidia-kernel-common-460                        460.91.03-0ubuntu0.20.04.1   amd64        Shared files used with the kernel module
ii  nvidia-kernel-common-470                        470.57.02-0ubuntu1   amd64        Shared files used with the kernel module
ii  nvidia-kernel-source-470                        470.57.02-0ubuntu1   amd64        NVIDIA kernel source package
ii  nvidia-modprobe                                 470.57.02-0ubuntu1   amd64        Load the NVIDIA kernel driver and create device files
ii  nvidia-prime                                    0.8.14   all          Tools to enable NVIDIA's Prime
ii  nvidia-profiler                                 10.1.243-3   amd64        NVIDIA Profiler for CUDA and OpenCL
ii  nvidia-settings                                 470.57.02-0ubuntu1   amd64        Tool for configuring the NVIDIA graphics driver
ii  nvidia-utils-470                                470.57.02-0ubuntu1   amd64        NVIDIA driver support binaries
ii  nvidia-visual-profiler                          10.1.243-3   amd64        NVIDIA Visual Profiler for CUDA and OpenCL
ii  nvtop                                           1.0.0-1ubuntu2   amd64        Interactive NVIDIA GPU process monitor
ii  screen-resolution-extra                         0.18build1   all          Extension for the nvidia-settings control panel
ii  xserver-xorg-video-nvidia-470                   470.57.02-0ubuntu1   amd64        NVIDIA binary Xorg driver


"
52529,How to decrease the learning rate every 10 epochs by a factor of 0.9?,"I want to set the learning rate at 10^-3 with a decay every 10 epochs by a factor of 0.9. I am using the Adam optimizer in Tensorflow Keras. I have found this code in the official documentation:

initial_learning_rate = 0.1

```
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=100000,
    decay_rate=0.96,
    staircase=True
)
```
I do not know what is this decay_steps=100000. Actually, I want to decrease my learning rate after 10 epochs. How can I do it?"
52528,Convolution with custom operation,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.4
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Currently `tf.nn.convolution` seems to support only a convolution of the sum-product of the kernel applied to the input. Could this function (or another one with proper naming) support the declaration of a custom application function instead?

Example: I'd like to convolve a pairwise distance function between the kernel and the input vector. If the part of the input vector currently convolved on is (example with a 1-d input vector):

```
[10, 20, 30]
```

And the kernel is:

```
[9, 21, 30]
```

Then the result of this step of the convolution would be the average of the element-wise distances between input and kernel:

```
(abs(10-9) + abs(20-21) + abs(30-30)) / 3 = 0.6666
```

**Will this change the current api? How?**

It would likely need adding a new function like `tf.nn.convolution_custom`, or adding an optional argument to the existing `tf.nn.convolution`.

**Who will benefit with this feature?**

I can see a use when the network needs to learn about specific sequences or arrangements, when the element-wise values are not ordered. Example: letters. It doesn't make sense to convolve a sumproduct over an input vector if the letters are represented by numbers. However, with this feature, using the operation mentioned above as an example, the kernel could represent a sequence of letters e.g. `[a, m, s]` and it would return `0` if the current input window is also representing `[a, m, s]`, or a positive float number otherwise.

This would allow the network to spot sequences of letters such as, if again using the `ams` kernel (conceptually, and assuming a further `1 - distance` operation applied to make an exact match a `1` and a non-match a `0`):

```
t    h    e      p    o    r    t       o    f       a    m    s    t    e    r    d    a    m
     0    0      0    0    0    0       0    0       0    1    0    0    0    0    0    0
```

After this, normal convolutions could be applied in order to learn patterns.

I'm using letters as example here but I can imagine many other different applications where the sequence and its ordering matter, but the items are not ordered among themselves (e.g. categorical).

**Any Other info.**
"
52527,whether there is a timeout value can be set for bazel building download action?,"When I use bazel build tensorflow, which will download a lot of rep, so often timeout and rebuild and timeout agian...
whether thesre is a timeout value can be set for download action for bazel?"
52526,logistic error in tf.split function,"**tf nightly**

```
@tf.function
def test():
    nearest_color_neighbor = tf.constant([
     [0, 3],[0, 4],[0, 6],[0, 7],
     [1, 0],[1, 1],[1, 2],[1, 3],[1, 4],[1, 6],
     [2, 0],[2, 1],[2, 2],[2, 3],[2, 4],[2, 6],
     [3, 3],[3, 4]])

    unique, _, counts = tf.unique_with_counts(nearest_color_neighbor[:, 0])

    # ValueError: Cannot infer num from shape Tensor(""UniqueWithCounts:2"", shape=(None,), dtype=int32)
    splits = tf.split(nearest_color_neighbor[:, -1], counts)

    # TypeError: Expected int for argument 'num_split' not <tf.Tensor 'Shape:0' shape=(1,) dtype=int32>.
    splits = tf.split(nearest_color_neighbor[:, -1], counts, num=tf.shape(counts)) # error

    # output:
    # [<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 4, 6, 7], dtype=int32)>,
    #  <tf.Tensor: shape=(6,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 6], dtype=int32)>,
    #  <tf.Tensor: shape=(6,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 6], dtype=int32)>,
    #  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>]
    splits = tf.split(nearest_color_neighbor[:, -1], counts, num=4)
```

1. What's the type of `num` parameter for tf.split in https://tensorflow.google.cn/api_docs/python/tf/split?hl=en#args. Can it be of the type 'tensor' or just the 'int' type?
2. If the number of  split output can only be inferred from the context, I mean the **tf.shape(counts)** in the above example, then how to revise the code?"
52520,Document whether the GPU execution of concrete functions are asynchronous,"## URL(s) with the issue:
- https://www.tensorflow.org/guide/eager#performance
- https://www.tensorflow.org/guide/intro_to_graphs#seeing_the_speed-up
- https://www.tensorflow.org/api_docs/python/tf/experimental/async_scope

## Description of issue (what needs changing):
The comment in the example code at https://www.tensorflow.org/guide/eager#performance states:
```
# tf.matmul can return before completing the matrix multiplication
# (e.g., can return after enqueing the operation on a CUDA stream).
# The x.numpy() call below will ensure that all enqueued operations
# have completed
```

In contrast, the benchmark script at https://www.tensorflow.org/guide/intro_to_graphs#seeing_the_speed-up do not contain the `numpy()` call:

```python
def power(x, y):
  result = tf.eye(10, dtype=tf.dtypes.int32)
  for _ in range(y):
    result = tf.matmul(x, result)
  return result

print(""Eager execution:"", timeit.timeit(lambda: power(x, 100), number=1000))

power_as_graph = tf.function(power)
print(""Graph execution:"", timeit.timeit(lambda: power_as_graph(x, 100), number=1000))
```

Is the second example correct, don't we need a `numpy()` call to ensure that the computation is completed?

### Improve the documentation of graph function execution

While reading through the guides https://www.tensorflow.org/guide/intro_to_graphs#graph_execution_vs_eager_execution and https://www.tensorflow.org/guide/function, I did not find any reference to the possibly asynchronous nature of function execution.

Can we improve the guide by stating that 
```
A graph function can return before completing the computation (e.g., can return after enqueing the operation on a CUDA stream).
```
~~If this is not the case, then can we state that explicitly?~~

### async_scope

The description at https://www.tensorflow.org/api_docs/python/tf/experimental/async_scope mentions that ""function calls inside the scope can return before finishing the actual execution. "". But looking into the source code indicates that the scope controls whether remote executions are synchronous.

What is the status with a function execution on a local GPU? Is that affected by this scope? Could you expand the docstring with this information?

"
52519,TensorArray support dtype of Custom defined  class object?,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are
not verified bugs in TensorFlow, please go to
[Discourse](https://discuss.tensorflow.org/).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
52517,Error occurs during quantization usingTFLiteConvert.,"### 1. System information

- Linux Ubunut 20.04
- pip install tensorflow==2.6.0
- pip install tensorflow_model_optimization==0.7.0

### 2. Code

https://colab.research.google.com/drive/1kOgZevD3dkM0hxehTh8vNOW_1zXW4VDE?usp=sharing

### 3. Failure after conversion
error: 'tfl.max_pool_2d' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.010792325524722828> vs. !quant.uniform<i8:f32, 0.0039215686274509803:-128>
(failure on converting)

### 5. (optional) Any other info / logs

I build two very similar models, but one is successfully converted to quantized model, while other is failed.
Same with avgpool instead of maxpool

There was a  similar issue  #46754 (https://github.com/tensorflow/tensorflow/issues/46754),

but I think concat isn't problem because one of my model including concat works.

Thank you
"
52392, Compiling test case failure [TensorFlow Lite],"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v2.4.3
- Python version: 3.7
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):  gcc 7.5.0
- CUDA/cuDNN version: No
- GPU model and memory: No

1. configure

```bash
./configure
```
output

```bash
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is /media/envs/miniconda/envs/tf2.x/bin/python3]: 


Found possible Python library paths:
  /media/xx/code/AnimaX
  /media/envs/miniconda/envs/tf2.x/lib/python3.7/site-packages
Please input the desired Python library path to use.  Default is [/media/xx/code/AnimaX]

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: 
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: 
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y
Searching for NDK and SDK installations.

Please specify the home path of the Android NDK to use. [Default is /home/xx/Android/Sdk/ndk-bundle]: /home/xx/Android/Sdk/ndk/18.1.5063045


Please specify the (min) Android NDK API level to use. [Available levels: ['16', '17', '18', '19', '21', '22', '23', '24', '26', '27', '28']] [Default is 21]: 


Please specify the home path of the Android SDK to use. [Default is /home/xx/Android/Sdk]: 


Please specify the Android SDK API level to use. [Available levels: ['19', '30']] [Default is 30]: 


Please specify an Android build tools version to use. [Available versions: ['19.1.0', '30.0.2']] [Default is 30.0.2]: 


Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN support for Aarch64.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```

2. build

```bash
bash tensorflow/lite/tools/build_aar.sh
```

without any exception.

3. test

```bash
bazel build -c opt  //tensorflow/lite/kernels:reverse_test
```

output

```bash
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=146
INFO: Reading rc options for 'build' from /media/xx/libraries/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /media/xx/libraries/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /media/xx/libraries/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/media/envs/miniconda/envs/tf2.x/bin/python3 --action_env PYTHON_LIB_PATH=/media/xx/code/AnimaX --python_path=/media/envs/miniconda/envs/tf2.x/bin/python3 --action_env PYTHONPATH=/media/envs/install/pangolin/lib/python3.7/dist-packages:/media/envs/install/g2o/lib/python3.7/dist-packages:/media/envs/install/nvtop/lib/python3.7/dist-packages:/media/envs/install/ceres-solver/lib/python3.7/dist-packages:/media/envs/install/vtk/lib/python3.7/dist-packages:/media/envs/install/protobuf/lib/python3.7/dist-packages:/media/envs/install/opencv/lib/python3.7/dist-packages:/media/envs/install/cmake/lib/python3.7/dist-packages:/media/envs/install/openmpi/lib/python3.7/dist-packages:/media/envs/install/pangolin/lib/python3.7/dist-packages:/media/envs/install/g2o/lib/python3.7/dist-packages:/media/envs/install/nvtop/lib/python3.7/dist-packages:/media/envs/install/ceres-solver/lib/python3.7/dist-packages:/media/envs/install/vtk/lib/python3.7/dist-packages:/media/envs/install/protobuf/lib/python3.7/dist-packages:/media/envs/install/opencv/lib/python3.7/dist-packages:/media/envs/install/cmake/lib/python3.7/dist-packages:/media/envs/install/openmpi/lib/python3.7/dist-packages::/media/xx/code/AnimaX:/media/xx/code/AnimaX --config=xla --action_env ANDROID_NDK_HOME=/home/xx/Android/Sdk/ndk/18.1.5063045 --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=30.0.2 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/home/xx/Android/Sdk --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /media/xx/libraries/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /media/xx/libraries/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /media/xx/libraries/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:linux in file /media/xx/libraries/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /media/xx/libraries/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build options --cxxopt, --fat_apk_cpu, and --host_crosstool_top have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/lite/kernels:reverse_test (26 packages loaded, 3873 targets configured).
INFO: Found 1 target...
ERROR: /media/xx/libraries/tensorflow/tensorflow/lite/kernels/internal/BUILD:720:1: C++ compilation of rule '//tensorflow/lite/kernels/internal:kernel_utils' failed (Exit 1)
In file included from tensorflow/lite/kernels/internal/kernel_utils.cc:19:0:
./tensorflow/lite/kernels/internal/tensor_utils.h: In function 'void tflite::tensor_utils::ApplyTanhToVector(const float*, int, float*)':
./tensorflow/lite/kernels/internal/tensor_utils.h:550:39: error: 'Vector' is not a member of 'Eigen'
   using VectorMap = Eigen::Map<Eigen::Vector<float, Eigen::Dynamic>>;
                                       ^~~~~~
./tensorflow/lite/kernels/internal/tensor_utils.h:550:39: note: suggested alternative: 'VectorXd'
   using VectorMap = Eigen::Map<Eigen::Vector<float, Eigen::Dynamic>>;
                                       ^~~~~~
                                       VectorXd
./tensorflow/lite/kernels/internal/tensor_utils.h:550:39: error: 'Vector' is not a member of 'Eigen'
./tensorflow/lite/kernels/internal/tensor_utils.h:550:39: note: suggested alternative: 'VectorXd'
   using VectorMap = Eigen::Map<Eigen::Vector<float, Eigen::Dynamic>>;
                                       ^~~~~~
                                       VectorXd
./tensorflow/lite/kernels/internal/tensor_utils.h:550:67: error: template argument 1 is invalid
   using VectorMap = Eigen::Map<Eigen::Vector<float, Eigen::Dynamic>>;
                                                                   ^~
./tensorflow/lite/kernels/internal/tensor_utils.h:551:3: error: 'VectorMap' was not declared in this scope
   VectorMap input_map(const_cast<float* __restrict__>(vector), v_size);
   ^~~~~~~~~
./tensorflow/lite/kernels/internal/tensor_utils.h:551:3: note: suggested alternative: 'vector'
   VectorMap input_map(const_cast<float* __restrict__>(vector), v_size);
   ^~~~~~~~~
   vector
./tensorflow/lite/kernels/internal/tensor_utils.h:552:13: error: expected ';' before 'output_map'
   VectorMap output_map(result, v_size);
             ^~~~~~~~~~
./tensorflow/lite/kernels/internal/tensor_utils.h:553:3: error: 'output_map' was not declared in this scope
   output_map.array() = input_map.array().tanh();
   ^~~~~~~~~~
./tensorflow/lite/kernels/internal/tensor_utils.h:553:24: error: 'input_map' was not declared in this scope
   output_map.array() = input_map.array().tanh();
                        ^~~~~~~~~
./tensorflow/lite/kernels/internal/tensor_utils.h: In function 'void tflite::tensor_utils::ApplySigmoidToVector(const float*, int, float*)':
./tensorflow/lite/kernels/internal/tensor_utils.h:567:39: error: 'Vector' is not a member of 'Eigen'
   using VectorMap = Eigen::Map<Eigen::Vector<float, Eigen::Dynamic>>;
                                       ^~~~~~
./tensorflow/lite/kernels/internal/tensor_utils.h:567:39: note: suggested alternative: 'VectorXd'
   using VectorMap = Eigen::Map<Eigen::Vector<float, Eigen::Dynamic>>;
                                       ^~~~~~
                                       VectorXd
./tensorflow/lite/kernels/internal/tensor_utils.h:567:39: error: 'Vector' is not a member of 'Eigen'
./tensorflow/lite/kernels/internal/tensor_utils.h:567:39: note: suggested alternative: 'VectorXd'
   using VectorMap = Eigen::Map<Eigen::Vector<float, Eigen::Dynamic>>;
                                       ^~~~~~
                                       VectorXd
./tensorflow/lite/kernels/internal/tensor_utils.h:567:67: error: template argument 1 is invalid
   using VectorMap = Eigen::Map<Eigen::Vector<float, Eigen::Dynamic>>;
                                                                   ^~
./tensorflow/lite/kernels/internal/tensor_utils.h:568:3: error: 'VectorMap' was not declared in this scope
   VectorMap input_map(const_cast<float* __restrict__>(vector), v_size);
   ^~~~~~~~~
./tensorflow/lite/kernels/internal/tensor_utils.h:568:3: note: suggested alternative: 'vector'
   VectorMap input_map(const_cast<float* __restrict__>(vector), v_size);
   ^~~~~~~~~
   vector
./tensorflow/lite/kernels/internal/tensor_utils.h:569:13: error: expected ';' before 'output_map'
   VectorMap output_map(result, v_size);
             ^~~~~~~~~~
./tensorflow/lite/kernels/internal/tensor_utils.h:570:3: error: 'output_map' was not declared in this scope
   output_map.array() = input_map.array().logistic();
   ^~~~~~~~~~
./tensorflow/lite/kernels/internal/tensor_utils.h:570:24: error: 'input_map' was not declared in this scope
   output_map.array() = input_map.array().logistic();
                        ^~~~~~~~~
Target //tensorflow/lite/kernels:reverse_test failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 4.225s, Critical Path: 1.62s
INFO: 27 processes: 27 local.
FAILED: Build did NOT complete successfully
```"
52391,Training Object Detection model on Cloud VM error with TPU RET_CHECK failure,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud TPU VM default
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.6
- Python version: default on TPU VM (3.8.10)
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: TPU


**Describe the current behavior**
I am running the EfficentdetD0 OD model installed from tensorflow/models. The model training fails with an error 'tensorflow.python.framework.errors_impl.InternalError: RET_CHECK failure' although is saving a checkpoint. 


The same model configuration runs OK on my local machine
The same model configuration trains OK when run using the cpu on the cloud VM (but slow) i.e. --use_tpu=false
I have tried this using accelerator-type=v3-8 and 2-8 and in different regions
For some reason I cant create older version of the TPU/tf. the only version I can create is 2.6.0 or alpha, so have only tested against these.
Python env: created via pip3 install -r /usr/share/tpu/models/official/requirements.txt

**Describe the expected behavior**
Model trains correctly using TPU

**Standalone code to reproduce the issue**
TPU VM created: gcloud alpha compute tpus tpu-vm  create efficientd0-2 --zone=us-central1-a --accelerator-type=v3-8 --version=tpu-vm-tf-2.6.0
OD installed: git clone https://github.com/tensorflow/models.git
udo protoc object_detection/protos/*.proto --python_out=.
Install TensorFlow Object Detection API.
cp object_detection/packages/tf2/setup.py .
pip3 install --use-feature=2020-resolver .

I need to run these for some reason:
sudo apt-get update
apt install libgl1-mesa-glx

The command I am running is: python3 object_detection/model_main_tf2.py --pipeline_config_path=gs://cloudtest1bx/data/pipeline.config --model_dir=gs://cloudtest1bx/model --use_tpu=true --tpu_name=local --alsologtostderr

Stack trace
2021-10-14 20:43:30.134769: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:263] Subgraph fingerprint:7734062378992412431
2021-10-14 20:43:30.604562: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:263] Subgraph fingerprint:16822654659801201346
2021-10-14 20:43:35.926086: E tensorflow/compiler/xla/status_macros.cc:56] Internal: RET_CHECK failure (tensorflow/core/tpu/graph_rewrite/distributed_tpu_rewrite_pass.cc:1777) arg_shape.handle_type != DT_INVALID  input edge: [id=19474 Func/while/body/_1/input/_2460:0 -> cluster_while_body_128945:1265]
*** Begin stack trace ***
        tensorflow::CurrentStackTrace[abi:cxx11]()

        xla::status_macros::MakeErrorStream::Impl::GetStatus()
        tensorflow::DistributedTPURewritePass::GetArgAndRetvalShapes(std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<tensorflow::InferredShape, std::allocator<tensorflow::InferredShape> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<tensorflow::InferredShape, std::allocator<tensorflow::InferredShape> > > > > const&, tensorflow::Node const&, tensorflow::DistributedTPURewritePass::ParameterInfo const&, std::vector<tensorflow::InferredShape, std::allocator<tensorflow::InferredShape> >*, std::vector<tensorflow::InferredShape, std::allocator<tensorflow::InferredShape> >*)
        tensorflow::DistributedTPURewritePass::RewriteTPUReplicateNode(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::DeviceSet const&, tensorflow::Node*, tensorflow::FunctionLibraryDefinition*, tensorflow::FunctionLibraryRuntime*, tensorflow::Node*, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<tensorflow::Node*, std::allocator<tensorflow::Node*> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<tensorflow::Node*, std::allocator<tensorflow::Node*> > > > > const&, std::vector<tensorflow::Node*, std::allocator<tensorflow::Node*> > const&, absl::lts_20210324::node_hash_map<tensorflow::Node*, std::vector<tensorflow::Node*, std::allocator<tensorflow::Node*> >, absl::lts_20210324::container_internal::HashEq<tensorflow::Node*, void>::Hash, absl::lts_20210324::container_internal::HashEq<tensorflow::Node*, void>::Eq, std::allocator<std::pair<tensorflow::Node* const, std::vector<tensorflow::Node*, std::allocator<tensorflow::Node*> > > > >*, tensorflow::Graph*, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<tensorflow::InferredShape, std::allocator<tensorflow::InferredShape> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<tensorflow::InferredShape, std::allocator<tensorflow::InferredShape> > > > > const&, absl::lts_20210324::flat_hash_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::allocator<std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > >, absl::lts_20210324::container_internal::StringHash, absl::lts_20210324::container_internal::StringHashEq::Eq, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::allocator<std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > >*, long)
        tensorflow::DistributedTPURewritePass::Run(tensorflow::GraphOptimizationPassOptions const&)
        tensorflow::OptimizationPassRegistry::RunGrouping(tensorflow::OptimizationPassRegistry::Grouping, tensorflow::GraphOptimizationPassOptions const&)
        tensorflow::ProcessFunctionLibraryRuntime::InstantiateMultiDevice(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::AttrSlice, tensorflow::FunctionLibraryRuntime::InstantiateOptions const&, unsigned long*)
        tensorflow::ProcessFunctionLibraryRuntime::Instantiate(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::AttrSlice, tensorflow::FunctionLibraryRuntime::InstantiateOptions const&, unsigned long*)
        tensorflow::KernelAndDeviceFunc::InstantiateFunc(bool, tensorflow::NodeDef const&, tensorflow::GraphCollector*)
        tensorflow::KernelAndDeviceFunc::Init(bool, tensorflow::NodeDef const&, tensorflow::GraphCollector*)


        tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*)
        tensorflow::EagerOperation::Execute(absl::lts_20210324::Span<tensorflow::AbstractTensorHandle*>, int*)
        tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*)
        TFE_Execute
        TFE_Py_ExecuteCancelable(TFE_Context*, char const*, char const*, absl::lts_20210324::InlinedVector<TFE_TensorHandle*, 4ul, std::allocator<TFE_TensorHandle*> >*, _object*, TFE_CancellationManager*, absl::lts_20210324::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*)



        PyCFunction_Call
        _PyObject_MakeTpCall
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName

        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName

        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyFunction_Vectorcall
        _PyObject_FastCallDict
        _PyObject_Call_Prepend

        PyObject_Call
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyFunction_Vectorcall

        PyObject_Call
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyObject_FastCallDict
        _PyObject_Call_Prepend

        _PyObject_MakeTpCall
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault

        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        PyEval_EvalCode



        PyRun_SimpleFileExFlags
        Py_RunMain
        Py_BytesMain
        __libc_start_main
        _start
*** End stack trace ***

Traceback (most recent call last):
  File ""object_detection/model_main_tf2.py"", line 115, in <module>
    tf.compat.v1.app.run()
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/danbu/.local/lib/python3.8/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/home/danbu/.local/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""object_detection/model_main_tf2.py"", line 106, in main
    model_lib_v2.train_loop(
  File ""/home/danbu/.local/lib/python3.8/site-packages/object_detection/model_lib_v2.py"", line 678, in train_loop
    losses_dict = _dist_train_step(train_input_iter)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py"", line 950, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 3039, in __call__
    return graph_function._call_flat(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 591, in call
    outputs = execute.execute(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: RET_CHECK failure (tensorflow/core/tpu/graph_rewrite/distributed_tpu_rewrite_pass.cc:1777) arg_shape.handle_type != DT_INVALID  input edge: [id=19474 Func/while/body/_1/input/_2460:0 -> cluster_while_body_128945:1265] [Op:__inference__dist_train_step_200662]
"
52388,tf.image.non_max_suppression_padded returns dynamic-sized tensors: fails on TFlite GPU Delegate,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- Mobile device: Snapdragon 845 (Pixel 3, Galaxy S9)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.6.0
- Python version: 3.7

**Describe the current behavior**
Exporting `tf.image.non_max_suppression_padded` to tflite yields a model that fails to run on the TFlite GPU delegate:
```
$ ./android_aarch64_benchmark_model --use_gpu=true --graph=nms (3).tflite                                                                       
STARTING!
Log parameter values verbosely: [0]
Graph: [nms (3).tflite]
Use gpu: [1]
Loaded model nms (3).tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
Failed to apply GPU delegate.
Benchmarking failed.
```

**Describe the expected behavior**
Compared to `tf.image.non_max_suppression`, `tf.image.non_max_suppression_padded` should not return dynamic-sized tensors, and should therefore be able to run on the GPU delegate.

**Standalone code to reproduce the issue**
Collab to reproduce model:
https://colab.research.google.com/drive/14KIEfiNIHB9q2erUgHs3uwEY4mxvgGeU?usp=sharing

Model:
[nms (3).tflite.zip](https://github.com/tensorflow/tensorflow/files/7355110/nms.3.tflite.zip)


Benchmarking tool:
[android_aarch64_benchmark_model.zip](https://github.com/tensorflow/tensorflow/files/7349358/android_aarch64_benchmark_model.zip)
(source: https://www.tensorflow.org/lite/performance/measurement#download_or_build_the_binary )

![image](https://user-images.githubusercontent.com/20094729/137530204-f1bd75d0-44b2-40e5-bfa2-d4d944f91f31.png)



"
52387,Multilingual Universal Sentence Encoder  Quantization-aware training error,"### 1. System information

- OS Platform and Distribution: Ubuntu 20.04.2
- TensorFlow installation : 2.6.0

### 2. Code
```
def create_model():        
       hub_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder-multilingual/3"", input_shape=[], 
                           dtype=tf.string, trainable=False)
        model = tf.keras.Sequential()
        model.add(hub_layer)
        model.add(tf.keras.layers.Dense(64, activation='relu'))
        model.add(tf.keras.layers.Dense(1, activation='softmax'))
        optimizer = SGD(lr=0.001, momentum=0.6, decay=1e-6)
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        model.summary()
        return model

custom_objects={'KerasLayer':hub.KerasLayer}        
base_model = create_model()
with tfmot.quantization.keras.quantize_scope(custom_objects):
    q_model = tfmot.quantization.keras.quantize_model(base_model)
q_model.compile(loss=self.model_param.loss, optimizer=optimizer, metrics=self.model_param.metrics)

history = q_model.fit()
q_model.save(model_file_name)

converter = tf.lite.TFLiteConverter.from_keras_model(q_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
    ]
quantized_tflite_model = converter.convert()
```


### 3. Failure after conversion
Conversion failed at this part :
```
with tfmot.quantization.keras.quantize_scope(custom_objects):
    q_model = tfmot.quantization.keras.quantize_model(base_model)
```
Below is the error message:

```
Traceback (most recent call last):
  File ""/home/user/quant_model_multilingual.py"", line 125, in qat
    **q_model = tfmot.quantization.keras.quantize_model(base_model)**
  File ""/home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py"", line 138, in quantize_model
    **return quantize_apply(annotated_model)**
  File ""/home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py"", line 71, in inner
    raise error
  File ""/home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py"", line 66, in inner
    results = func(*args, **kwargs)
  File ""/home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py"", line 436, in quantize_apply
    transformed_model, layer_quantize_map = quantize_transform.apply(
  File ""/home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_layout_transform.py"", line 71, in apply
    return model_transformer.ModelTransformer(
  File ""/home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/graph_transformations/model_transformer.py"", line 622, in transform
    transformed_model = keras.Sequential.from_config(self._config,
  File ""/home/user/py38/lib/python3.8/site-packages/keras/engine/sequential.py"", line 434, in from_config
    model.add(layer)
  File ""/home/user/py38/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py"", line 530, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/user/py38/lib/python3.8/site-packages/keras/engine/sequential.py"", line 217, in add
    output_tensor = layer(self.outputs[0])
  File ""/home/user/py38/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 976, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/home/user/py38/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 1114, in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
  File ""/home/user/py38/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 848, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""/home/user/py38/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 888, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/home/user/py38/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 695, in wrapper
    raise e.ag_error_metadata.to_exception(e)

    /home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_layer.py:69 quantizer_fn  *
        inputs, train_var,
    /home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/keras/utils.py:54 smart_cond  *
        pred, true_fn=true_fn, false_fn=false_fn, name=name)
    /home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantizers.py:341 __call__  *
        weights['max_var'],
    /home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quant_ops.py:79 AllValuesQuantize  *
        max_var,
    /home/user/py38/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quant_ops.py:340 _FakeQuantWithMinMaxVars  *
        inputs, min_var, max_var, num_bits=num_bits, narrow_range=narrow_range)
    /home/user/py38/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:2890 fake_quant_with_min_max_vars  **
        _, _, _op, _outputs = _op_def_library._apply_op_helper(
    /home/user/py38/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:544 _apply_op_helper
        raise TypeError(""%s expected type of %s."" %

    TypeError: Input 'inputs' of 'FakeQuantWithMinMaxVars' Op has type string that does not match expected type of float32.
```


It seems that the error is caused because ""multilingual universal sentence encoder"" has input type as tf.string, and the output to the dense layer is float32.


"
52384,Tensorflow 2.5 and above causes crash on load on Ubuntu 20.04,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installed from (source or binary): `sudo pip3 install tensorflow`
- TensorFlow version: 2.5+
- Python version: 3.8.10
- Installed using virtualenv? pip? conda?: pip `sudo pip3 install tensorflow`
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): Not relevant, but `gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0`
- CUDA/cuDNN version: Not installed
- GPU model and memory: GeForce GTX 750 Ti [nouveau drivers installed, CUDA is NOT installed, not currently using the GPU]



**Describe the problem**

If I install Tensorflow 2.5 or above and then attempt to import it in a simple Python program, it causes an instant and highly reproducible crash.

I don't care about GPU support. I just want to be able to use Tensorflow at all....


**Provide the exact sequence of commands / steps that you executed before running into the problem**

First, install Tensorflow:

```bash
sudo pip3 install tensorflow==2.5
```

OR

```bash
sudo pip3 install tensorflow==2.6
```

OR

```bash
sudo pip3 install tensorflow
```

Then, create a simple test program:

```python
#/usr/bin/env python3

import tensorflow as tf

print(tf.__version__)
```

Finally, chmod and run the test program:

```bash
chmod +x path/to/test.py
path/to/test.py
```

See crash.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Example crash from tensorflow 2.5:

```
2021-10-14 16:46:53.912576: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-10-14 16:46:53.912599: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""./src/image_classifier.py"", line 10, in <module>
    import tensorflow as tf
  File ""/home/bryan-smithl/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 436, in <module>
    _ll.load_library(_main_dir)
  File ""/home/bryan-smithl/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 153, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeEN4absl14lts_2020_09_2311string_viewEOSt6vectorINS_10StackFrameESaIS7_EE
````

Example crash from Tensorflow 2.6:

```
2021-10-14 16:40:43.830370: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-10-14 16:40:43.830389: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""./src/image_classifier.py"", line 10, in <module>
    import tensorflow as tf
  File ""/home/bryan-smithl/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 436, in <module>
    _ll.load_library(_main_dir)
  File ""/home/bryan-smithl/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 153, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE
````

Tensorflow 2.4 and below work fine:

```
2021-10-14 16:53:07.554556: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-10-14 16:53:07.554582: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2.4.1
```"
52383,"Inputs to eager execution function cannot be Keras symbolic tensors, Custom loss function, Tensorflow 2.3.0, Gradient Tape","I am trying to use a custom loss function in my `Keras` model (TensorFlow 2.3.0). This custom loss (ideally) will calculate the data loss plus the residual of a physical equation (say, diffusion equation, Navier Stokes, etc.). This residual error is based on the model output derivative wrt its inputs and I want to use `GradientTape`.

In this MWE, I removed the data loss term and other equation losses, and just used the derivative of the output wrt its first input. The dataset can be found [here][1]. 

    from numpy import loadtxt
    from keras.models import Sequential
    from keras.layers import Dense
    import tensorflow as tf #tf.__version__ = '2.3.0'
    # tf.compat.v1.disable_eager_execution()
    
    # load the dataset
    dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')
    # split into input (X) and output (y) variables
    X = dataset[:,0:8] #X.shape = (768, 8)
    y = dataset[:,8]
    
    def customLoss(yTrue,yPred):
        x_tensor = tf.convert_to_tensor(model.input, dtype=tf.float32)
        x_tensor = tf.cast(x_tensor, tf.float32)
        with tf.GradientTape() as t:
            t.watch(x_tensor)
            output = model(model.input)
        DyDX = t.gradient(output, model.input)    
        dy_t = DyDX[:, 1:2][0]
        R_pred=dy_t
        # loss_data = tf.reduce_mean(tf.square(yTrue - yPred), axis=-1)
        loss_PDE = tf.reduce_mean(tf.square(R_pred))
        return loss_PDE
    
    model = Sequential()
    model.add(Dense(12, input_dim=8, activation='relu'))
    model.add(Dense(12, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    
    model.compile(loss=customLoss, optimizer='adam', metrics=['accuracy'])
    
    model.fit(X, y, epochs=15, batch_size=10)

After execution, I get this `_SymbolicException`:

    _SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'dense_6_input:0' shape=(None, 8) dtype=float32>]

When I uncomment `tf.compat.v1.disable_eager_execution()` (the fifth line), the issue seems to vanish and the model starts training. I was wondering why I am getting this `_SymbolicException` and how can I work it out without disabling eager execution. Any ideas? (By the way, I know questions similar to this have been posted, but they used `gradients `instead.)

  [1]: https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
52382,AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'to_tensor',"**System information**
Tensorflow: 2.6.0
The problem arises running the example code provided in Hugging face in Keras/Tensorflow: https://huggingface.co/transformers/training.html
Platform: Google Colab (Public and Pro) (in all GPU, TPU and CPU)
Python version: 3.7.12

**Describe the current behavior**
Getting this error when running the model:
![image](https://user-images.githubusercontent.com/70186411/137340499-de614365-78db-4b52-b4ae-c533a87c505e.png)

**Describe the expected behavior**
Same model runs locally in Apple M1 system without errors.
Two days ago was running fine in Google Colab too. 
Maybe an update in the system?
"
52381,How to enable multi thread computing for tf.data,"System: Ubuntu 20.04
GPU/CUDA: RTX3090 CUDA10.1(must be upgraded)
Before Dataloader the data doesn't goes to the GPU. And i have read about, that df.data runs on cpu. And i have tried:

```
tf.config.threading.set_intra_op_parallelism_threads(64)
tf.config.threading.set_inter_op_parallelism_threads(64)
```
before the code:
`train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_lbl))`
Since i ran the line, it always come to such warning:
`W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 7957118976 exceeds 10% of free system memory.`
Is that because always max. 2 threads are used. (I have 64 threads cpu). Thanks for the help!"
52380,Meshgrid does not work with tf.function,"The following code fails at runtime:

```
import tensorflow as tf


def f(x, y):
    return tf.meshgrid(x, y)


@tf.function
def g(x, y):
    return tf.meshgrid(x, y)


def main():
    print(f""tensorflow version: {tf.version.VERSION}"")
    all_values = tf.range(0.0, 1.0, .1)
    x = y = tf.expand_dims(all_values, -1)

    print(f(x, y))  # This works
    print(g(x, y)) # This fails


if __name__ == '__main__':
    main()
```

The output:

```
2021-10-14 12:51:02.388948: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-10-14 12:51:03.952753: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-10-14 12:51:04.028174: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-10-14 12:51:04.028234: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (removed): /proc/driver/nvidia/version does not exist
2021-10-14 12:51:04.028962: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
tensorflow version: 2.5.1
[<tf.Tensor: shape=(10, 10), dtype=float32, numpy=
array([[0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],
       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],
       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],
       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],
       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],
       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],
       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],
       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],
       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ],
       [0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
        0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ]],
      dtype=float32)>, <tf.Tensor: shape=(10, 10), dtype=float32, numpy=
array([[0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ],
       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,
        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],
       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ,
        0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],
       [0.3       , 0.3       , 0.3       , 0.3       , 0.3       ,
        0.3       , 0.3       , 0.3       , 0.3       , 0.3       ],
       [0.4       , 0.4       , 0.4       , 0.4       , 0.4       ,
        0.4       , 0.4       , 0.4       , 0.4       , 0.4       ],
       [0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,
        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ],
       [0.6       , 0.6       , 0.6       , 0.6       , 0.6       ,
        0.6       , 0.6       , 0.6       , 0.6       , 0.6       ],
       [0.70000005, 0.70000005, 0.70000005, 0.70000005, 0.70000005,
        0.70000005, 0.70000005, 0.70000005, 0.70000005, 0.70000005],
       [0.8000001 , 0.8000001 , 0.8000001 , 0.8000001 , 0.8000001 ,
        0.8000001 , 0.8000001 , 0.8000001 , 0.8000001 , 0.8000001 ],
       [0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,
        0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ]],
      dtype=float32)>]
Traceback (most recent call last):
  File ""/mnt/workspace/tmp/pycharm_project_951/python_gamma/meshgrid_bug.py"", line 23, in <module>
    main()
  File ""/mnt/workspace/tmp/pycharm_project_951/python_gamma/meshgrid_bug.py"", line 19, in main
    print(g(x, y))
  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 889, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 764, in _initialize
    *args, **kwds))
  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3050, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3444, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3289, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 999, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 672, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/local/lib64/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in wrapper
    raise e.ag_error_metadata.to_exception(e)
NotImplementedError: in user code:

    /mnt/workspace/tmp/pycharm_project_951/python_gamma/meshgrid_bug.py:11 g  *
        return tf.meshgrid(x, y)
    /usr/local/lib64/python3.7/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **
        return target(*args, **kwargs)
    /usr/local/lib64/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3644 meshgrid
        mult_fact = ones(shapes, output_dtype)
    /usr/local/lib64/python3.7/site-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /usr/local/lib64/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3212 ones
        output = _constant_if_small(one, shape, dtype, name)
    /usr/local/lib64/python3.7/site-packages/tensorflow/python/ops/array_ops.py:2896 _constant_if_small
        if np.prod(shape) < 1000:
    <__array_function__ internals>:6 prod
        
    /mnt/workspace/python/numpy/core/fromnumeric.py:3052 prod
        keepdims=keepdims, initial=initial, where=where)
    /mnt/workspace/python/numpy/core/fromnumeric.py:86 _wrapreduction
        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
    /usr/local/lib64/python3.7/site-packages/tensorflow/python/framework/ops.py:870 __array__
        "" a NumPy call, which is not supported"".format(self.name))

    NotImplementedError: Cannot convert a symbolic Tensor (meshgrid/Size_1:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```

This is running the AWS DLAMI for TF 2.5.1 using the following AMI: ami-09ddcd88a97c092e5. The EC2 instance type is a t3.small. "
52379,ctc_loss_calculator.h:499] No valid path found,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOs Big Sur 11.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.9.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

The issue is addressed in multiple SO questions and none provides a clear solution if any answers at all

 - [CTC loss Tensorflow, No valid path found](https://stackoverflow.com/questions/47461331/ctc-loss-tensorflow-no-valid-path-found)
 - [ctc_loss error ""No valid path found.""](https://stackoverflow.com/questions/45130184/ctc-loss-error-no-valid-path-found)
 - [CTC Loss bug: no valid path found? OCR difficulties in Tf.keras](https://stackoverflow.com/questions/64676073/ctc-loss-bug-no-valid-path-found-ocr-difficulties-in-tf-keras)
 - [Tensorflow ctc_loss_calculator: No valid path found](https://stackoverflow.com/questions/44195801/tensorflow-ctc-loss-calculator-no-valid-path-found)

Here's the code I run ...

    from itertools import groupby
    from pathlib import Path
    
    import numpy as np
    import tensorflow as tf
    from matplotlib import pyplot as plt
    from tensorflow.keras import Model
    from tensorflow.keras.callbacks import EarlyStopping
    from tensorflow.keras.layers import (LSTM, BatchNormalization, Bidirectional,
                                         Conv2D, Dense, Input, Lambda, Layer,
                                         MaxPool2D)
    from tensorflow.keras.layers.experimental.preprocessing import StringLookup
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    
    
    class CTCLayer(Layer):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.loss_fn = tf.keras.backend.ctc_batch_cost
    
        def call(self, y_true, *args, **kwargs):
            y_pred = args[0]
            batch_len = tf.cast(tf.shape(y_true)[0], dtype='int64')
            input_length = tf.cast(tf.shape(y_pred)[1], dtype='int64')
            label_length = tf.cast(tf.shape(y_true)[1], dtype='int64')
            input_length = input_length * tf.ones(shape=(batch_len, 1), dtype='int64')
            label_length = label_length * tf.ones(shape=(batch_len, 1), dtype='int64')
            loss = self.loss_fn(y_true, y_pred, input_length, label_length)
            self.add_loss(loss)
            return y_pred
    
    
    class TrainingManager:
        def __init__(
            self, images, labels, batch_size=256, validation_size=0.1, resize=(32, 128)
        ):
            self.images = images
            self.labels = labels
            self.batch_size = batch_size
            self.validation_size = validation_size
            self.resize = resize
            self.vocabulary = sorted(set(''.join(self.labels)))
            self.max_label_length = len(max(self.labels, key=len))
            self.char_to_num = StringLookup(
                vocabulary=self.vocabulary, num_oov_indices=0, mask_token=None
            )
            self.num_to_char = StringLookup(
                vocabulary=self.char_to_num.get_vocabulary(), mask_token=None, invert=True
            )
    
        def process_example(self, img_path, label):
            img = tf.io.read_file(img_path)
            img = tf.io.decode_png(img, channels=1)
            img = tf.image.convert_image_dtype(img, tf.float32)
            img = tf.image.resize(img, self.resize)
            return {'image': img, 'label': label}
    
        def preview_dataset(self, dataset, n_rows, n_cols, fig_size=(15, 10)):
            fig, ax = plt.subplots(n_rows, n_cols, figsize=fig_size)
            for batch in dataset.take(1):
                images = batch['image']
                labels = batch['label']
                for i in range(n_rows * n_cols):
                    img = (images[i] * 255).numpy().astype('uint8')
                    label = (
                        tf.strings.reduce_join(self.num_to_char(labels[i] + 1))
                        .numpy()
                        .decode('utf-8')
                        .replace('[UNK]', '')
                    )
                    ax[i // n_rows, i % n_cols].imshow(img[:, :, 0], cmap='gray')
                    ax[i // n_rows, i % n_cols].set_title(label)
                    ax[i // n_rows, i % n_cols].axis('off')
    
        def decode_predictions(self, predictions):
            text_list = []
            prediction_indices = np.argmax(predictions, axis=2)
            for i in range(prediction_indices.shape[0]):
                text = ''
                for p, _ in groupby(prediction_indices[i]):
                    if p != len(self.vocabulary):
                        text += self.vocabulary[p]
                text_list.append(text)
            return text_list
    
        def create_dataset(self, x, y):
            dataset = tf.data.Dataset.from_tensor_slices((x, y))
            return (
                dataset.map(
                    self.process_example, num_parallel_calls=tf.data.experimental.AUTOTUNE
                )
                .batch(self.batch_size)
                .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
            )
    
        def split_data(self):
            separation_idx = int(len(self.images) * (self.validation_size - 1))
            train_images = self.images[:separation_idx]
            valid_images = self.images[separation_idx:]
            labels = [
                self.char_to_num(tf.strings.unicode_split(label, input_encoding='UTF-8'))
                for label in self.labels
            ]
            labels = pad_sequences(labels, self.max_label_length, padding='post')
            train_labels = labels[:separation_idx]
            valid_labels = labels[separation_idx:]
            return train_images, valid_images, train_labels, valid_labels
    
        def create_datasets(self):
            train_images, valid_images, train_labels, valid_labels = self.split_data()
            train_dataset = self.create_dataset(train_images, train_labels)
            valid_dataset = self.create_dataset(valid_images, valid_labels)
            return train_dataset, valid_dataset
    
        def create_model(self, training=True):
            x0 = Input(shape=(32, 128, 1), name='image')
            x = Conv2D(32, (3, 3), activation='selu', padding='same')(x0)
            x = MaxPool2D(pool_size=(2, 2))(x)
            x = Conv2D(64, (3, 3), activation='selu', padding='same')(x)
            x = MaxPool2D(pool_size=(2, 2))(x)
            x = Conv2D(128, (3, 3), activation='selu', padding='same')(x)
            x = Conv2D(128, (3, 3), activation='selu', padding='same')(x)
            x = MaxPool2D(pool_size=(2, 1))(x)
            x = Conv2D(256, (3, 3), activation='selu', padding='same')(x)
            x = BatchNormalization()(x)
            x = Conv2D(256, (3, 3), activation='selu', padding='same')(x)
            x = BatchNormalization()(x)
            x = MaxPool2D(pool_size=(2, 1))(x)
            x = Conv2D(64, (2, 2), activation='selu')(x)
            x = Lambda(lambda i: tf.squeeze(i, 1))(x)
            x = Bidirectional(LSTM(128, return_sequences=True))(x)
            x = Bidirectional(LSTM(128, return_sequences=True))(x)
            output = Dense(len(self.vocabulary) + 1, activation='softmax', name='dense')(x)
            if not training:
                return Model(x0, output)
            labels = Input(name='label', shape=(None,), dtype='float32')
            output = CTCLayer(name='ctc_loss')(labels, output)
            return Model(inputs=[x0, labels], outputs=output)
    
    
    if __name__ == '__main__':
        photos, texts = [], []
        for line in open('labels.txt'):
            photo_path, photo_text = line.split(',')
            photos.append((Path('examples') / photo_path).as_posix())
            texts.append(photo_text.strip())
        manager = TrainingManager(photos, texts, batch_size=32)
        optimizer = Adam()
        model = manager.create_model()
        model.compile(optimizer=optimizer, metrics=[tf.keras.metrics.Accuracy()])
        model.summary()
        tds, vds = manager.create_datasets()
        manager.preview_dataset(tds, 2, 2)
        plt.show()
        history = model.fit(
            tds,
            epochs=100,
            validation_data=vds,
            verbose=1,
            callbacks=[EarlyStopping(verbose=1, patience=3, restore_best_weights=True)],
            shuffle=True,
        )

**`examples` + `labels.txt` (inside examples folder)**

[examples.tar.gz](https://drive.google.com/file/d/1hBVhUNvlpwhENgLf5Ui_AS7eiSb_L39B/view?usp=sharing)

**Note:** The code works perfectly fine for labels that are 15 characters long or shorter. The labels and respective photos in the example above have 1-20 characters each. What exactly do I need to modify, to make it work, given a label of length n? 


**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

2021-10-14 05:22:31.610 Python[18731:595755] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/hr/61r_7jcx2r3cnklwrr2zwbqw0000gn/T/org.python.python.savedState
    2021-10-14 05:22:31.746089: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
    Epoch 1/100
    2021-10-14 05:22:41.480510: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:41.480551: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:41.480614: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:41.480785: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    1/1 [==============================] - ETA: 0s - loss: inf - accuracy: 0.0000e+002021-10-14 05:22:44.004554: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.004595: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.004646: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.004705: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.004822: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.004859: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.004890: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.004907: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.005056: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.005073: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.005204: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.005233: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.005292: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.005322: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.160657: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.160745: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.160787: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.160862: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.160886: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.160959: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161019: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161058: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161081: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161108: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161236: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161306: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161352: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161394: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161416: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161439: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161504: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.161650: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.315489: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.315528: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.315676: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.316045: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.316060: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.316151: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.316276: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.316282: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.467841: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.467882: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.467911: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.468036: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.468267: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.468388: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.468427: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.468476: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.468526: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.468723: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.468737: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.510596: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    1/1 [==============================] - 9s 9s/step - loss: inf - accuracy: 0.0000e+00 - val_loss: inf - val_accuracy: 0.0000e+00
    Epoch 2/100
    2021-10-14 05:22:44.622235: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.622277: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.622401: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:44.622617: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    1/1 [==============================] - ETA: 0s - loss: inf - accuracy: 0.0000e+002021-10-14 05:22:45.075456: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.075495: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.075544: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.075600: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.075660: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.075716: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.075740: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.075760: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.075806: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.075877: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.075995: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.076016: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.076178: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.076210: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234284: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234392: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234438: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234483: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234530: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234544: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234596: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234639: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234768: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234837: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234882: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234913: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234959: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.234991: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.235021: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.235153: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.235235: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.235299: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.391722: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.391763: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.391838: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.391867: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.391914: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.392013: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.392028: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.392276: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.545771: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.545837: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.545860: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.546005: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.546136: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.546205: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.546389: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.546450: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.546475: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.546489: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.546529: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.587576: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    1/1 [==============================] - 1s 1s/step - loss: inf - accuracy: 0.0000e+00 - val_loss: inf - val_accuracy: 0.0000e+00
    Epoch 3/100
    2021-10-14 05:22:45.698230: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.698437: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.698533: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:45.698647: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    1/1 [==============================] - ETA: 0s - loss: inf - accuracy: 0.0000e+002021-10-14 05:22:46.127968: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128016: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128060: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128121: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128177: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128236: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128244: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128368: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128391: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128524: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128631: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128675: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128696: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.128926: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290187: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290274: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290309: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290390: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290411: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290485: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290545: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290574: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290605: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290663: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290756: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290822: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290884: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290895: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.290993: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.291033: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.291057: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.291095: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.448088: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.448374: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.448476: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.448486: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.448636: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.448658: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.448730: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.448954: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.604924: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.604977: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.604992: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.605256: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.605280: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.605321: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.605429: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.605523: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.605583: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.605591: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.605646: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    2021-10-14 05:22:46.646805: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
    1/1 [==============================] - 1s 1s/step - loss: inf - accuracy: 0.0000e+00 - val_loss: inf - val_accuracy: 0.0000e+00
    Restoring model weights from the end of the best epoch.
    Epoch 00003: early stopping
"
52378,GPU not found on Tensorflow 1.4.1,"**System information**
- Linux Ubuntu 20.04
- tensorflow-gpu            1.4.1                    pypi_0    pypi
- Python version: 2.7.15
- Installed using virtualenv? pip? conda?:
- CUDA/cuDNN version: CUDA version 8.0.61 with cuDNN version 6.0.21
- 1x GTX 1080TI

**The problem**
I cannot run any scripts from GPU using tensorflow. When I run:
```
import os
import tensorflow as tf
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""
print(tf.test.gpu_device_name())
```
I get:
```
2021-10-14 11:35:56.443831: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2021-10-14 11:35:57.277446: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_INVALID_DEVICE
2021-10-14 11:35:57.277535: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: erwin-SYS
2021-10-14 11:35:57.277557: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: erwin-SYS
2021-10-14 11:35:57.277638: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 470.74.0
2021-10-14 11:35:57.277692: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  470.74  Mon Sep 13 23:09:15 UTC 2021
GCC version:  gcc version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)
""""""
2021-10-14 11:35:57.277742: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 470.74.0
2021-10-14 11:35:57.277762: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 470.74.0
''

```


**Other info / logs**
When I run nvidia-smi it does find the gpu:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.74       Driver Version: 470.74       CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |
| 27%   26C    P8     6W / 180W |     14MiB /  8118MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+


```
"
52377,Build libtensorflow-lite.a failed on centos 7 platform,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux CentOS 7):
- TensorFlow installed from (source or binary):source 
- TensorFlow version:current master branch and r2.6
- Python version:N/A
- Installed using virtualenv? pip? conda?: compile from source code
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A



**Describe the problem**
cmake ../tensorflow_src/tensorflow/lite return fail

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build
3. cd tflite_build
4. cmake ../tensorflow_src/tensorflow/lite  

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

er@RC802802663 tflite_build]$ cmake ../tensorflow_src/tensorflow/lite
-- Setting build type to Release, for debug builds use'-DCMAKE_BUILD_TYPE=Debug'.
[ 11%] Performing download step (git clone) for 'eigen-populate'
Cloning into 'eigen'...
error: RPC failed; result=22, HTTP code = 422
fatal: the remote end hung up unexpectedly
Cloning into 'eigen'...
error: RPC failed; result=22, HTTP code = 422
fatal: the remote end hung up unexpectedly
Cloning into 'eigen'...
error: RPC failed; result=22, HTTP code = 422
fatal: the remote end hung up unexpectedly
-- Had to git clone more than once:
          3 times.
CMake Error at tmp/eigen-populate-gitclone.cmake:31 (message):
  Failed to clone repository: 'https://gitlab.com/libeigen/eigen'


gmake[2]: *** [/home/80280266user/work/ubuntu/mylite/tflite_build/src/eigen-populate-stamp/eigen-populate-download] Error 1
gmake[1]: *** [CMakeFiles/eigen-populate.dir/all] Error 2
gmake: *** [all] Error 2

CMake Error at /usr/local/share/cmake-3.18/Modules/FetchContent.cmake:987 (message):
  Build step for eigen failed: 2
Call Stack (most recent call first):
  /usr/local/share/cmake-3.18/Modules/FetchContent.cmake:1082:EVAL:2 (__FetchContent_directPopulate)
  /usr/local/share/cmake-3.18/Modules/FetchContent.cmake:1082 (cmake_language)
  tools/cmake/modules/OverridableFetchContent.cmake:531 (FetchContent_Populate)
  tools/cmake/modules/eigen.cmake:39 (OverridableFetchContent_Populate)
  tools/cmake/modules/Findeigen.cmake:18 (include)
  CMakeLists.txt:128 (find_package)


-- Configuring incomplete, errors occurred!
See also ""/home/80280266user/work/ubuntu/mylite/tflite_build/CMakeFiles/CMakeOutput.log"".
See also ""/home/80280266user/work/ubuntu/mylite/tflite_build/CMakeFiles/CMakeError.log"".
[80280266user@RC802802663 tflite_build]$ 

solution:
er@RC802802663 tensorflow_src]$ git diff
diff --git a/tensorflow/lite/tools/cmake/modules/eigen.cmake b/tensorflow/lite/tools/cmake/modules/eigen.cmake
index 0b9080a476e..8879364bac2 100644
--- a/tensorflow/lite/tools/cmake/modules/eigen.cmake
+++ b/tensorflow/lite/tools/cmake/modules/eigen.cmake
@@ -21,7 +21,7 @@ include(OverridableFetchContent)
 
 OverridableFetchContent_Declare(
   eigen
-  GIT_REPOSITORY https://gitlab.com/libeigen/eigen
+  GIT_REPOSITORY https://gitlab.com/libeigen/eigen.git
   # Sync with tensorflow/third_party/eigen3/workspace.bzl
   GIT_TAG 7b35638ddb99a0298c5d3450de506a8e8e0203d3
   # It's not currently (cmake 3.17) possible to shallow clone with a GIT TAG
[80280266user@RC802802663 tensorflow_src]$ 


"
52376,Run lite/tools/benchmark: setting use_hexagon to be true does not works.,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung, cpuinfo: ""Qualcomm Technologies, Inc KONA""
![image](https://user-images.githubusercontent.com/39039715/137239991-47fa8809-e78f-4955-99d3-ad3ff7ac0c6f.png)

- TensorFlow installed from (source or binary): source 
- TensorFlow version: commit id: 0088a1ada5a45c383e35c31ba7a552894936f998
![image](https://user-images.githubusercontent.com/39039715/137240102-942e1b22-9a43-4734-b024-67ed8e7d324b.png)

- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): bazel 3.7.2
- GCC/Compiler version (if compiling from source): gcc (GCC) 7.3.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Run lite/tools/benchmark: setting use_hexagon to be true does not works.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Build from source:
```
bazel build -c opt --config=android_arm --distdir=/usr1/tf_downloads --repository_cache=/usr1/tf_downloads tensorflow/lite/tools/benchmark:benchmark_model
```
2. push `benchmark_model` and `libhexagon_interface.so` to the phone
3. Run the model:
```
./benchmark_model --graph=video_infer.tflite --use_hexagon=true
```

```
WARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
```

![Snipaste_2021-10-14_10-14-07](https://user-images.githubusercontent.com/39039715/137240424-19c1b665-b485-4e38-b6bf-422dda385663.png)

> `libhexagon_nn_skel.so` is in the path `/vendor/lib/rfsa/adsp`, which is read-only file system. 

4. To solve the problem in step3, I download `libhexagon_nn_skel.so` from the [document](https://www.tensorflow.org/lite/performance/hexagon_delegate). I download hexagon_nn_skel.run of v1.21. `/vendor/lib/rfsa/adsp`  is read-only file system, so I push the `libhexagon_nn_skel.so` to path  `/data/local/tmp/tf` 

```
LD_PRELOAD=/data/local/tmp/tf/libhexagon_nn_skel.so ./benchmark_model --graph=video_infer.tflite --use_hexagon=true
```

```
CANNOT LINK EXECUTABLE ""./benchmark_model"": ""/data/local/tmp/tf/libhexagon_nn_skel.so"" is for EM_??? (164) instead of EM_ARM (40)
```

![image](https://user-images.githubusercontent.com/39039715/137242448-14680126-c881-4c38-b9e8-91f329764a78.png)


How can I run  `benchmark_model` with `use_hexagon=true`?

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
52375,meet error when i restore my model ,"**when i restore my model i meet issues:
Not found: Key g_/g_f_h0_lin/bias not found in checkpoint
Not found: Key g_/g_f_h0_lin/Matrix not found in checkpoint
Not found: Key g_/g_f_h0_lin/bias/Adam_1 not found in checkpoint**

**I restore my model:**
saver = tf.train.Saver()
coord = tf.train.Coordinator()

sess = tf.Session(config=config)
init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
sess.run(init_op)
saver.restore(sess, os.path.join(checkpoint_dir,params.checkpoint))
threads = tf.train.start_queue_runners(sess=sess, coord=coord)

**but  these param is not in my model**
**here are my generator param:**
('g_/BatchNorm/beta', [64])
('g_/BatchNorm/beta/Adam', [64])
('g_/BatchNorm/beta/Adam_1', [64])
('g_/BatchNorm/moving_mean', [64])
('g_/BatchNorm/moving_variance', [64])
('g_/enc_bn2/beta', [128])
('g_/enc_bn2/beta/Adam', [128])
('g_/enc_bn2/beta/Adam_1', [128])
('g_/enc_bn2/moving_mean', [128])
('g_/enc_bn2/moving_variance', [128])
('g_/enc_bn3/beta', [256])
('g_/enc_bn3/beta/Adam', [256])
('g_/enc_bn3/beta/Adam_1', [256])
('g_/enc_bn3/moving_mean', [256])
('g_/enc_bn3/moving_variance', [256])
('g_/enc_bn4/beta', [512])
('g_/enc_bn4/beta/Adam', [512])
('g_/enc_bn4/beta/Adam_1', [512])
('g_/enc_bn4/moving_mean', [512])
('g_/enc_bn4/moving_variance', [512])
('g_/enc_conv1/biases', [64])
('g_/enc_conv1/biases/Adam', [64])
('g_/enc_conv1/biases/Adam_1', [64])
('g_/enc_conv1/filters', [4, 4, 4, 3, 64])
('g_/enc_conv1/filters/Adam', [4, 4, 4, 3, 64])
('g_/enc_conv1/filters/Adam_1', [4, 4, 4, 3, 64])
('g_/enc_conv1/filters_init', [4, 4, 4, 3, 64])
('g_/enc_conv2/biases', [128])
('g_/enc_conv2/biases/Adam', [128])
('g_/enc_conv2/biases/Adam_1', [128])
('g_/enc_conv2/filters', [4, 4, 4, 64, 128])
('g_/enc_conv2/filters/Adam', [4, 4, 4, 64, 128])
('g_/enc_conv2/filters/Adam_1', [4, 4, 4, 64, 128])
('g_/enc_conv2/filters_init', [4, 4, 4, 64, 128])
('g_/enc_conv3/biases', [256])
('g_/enc_conv3/biases/Adam', [256])
('g_/enc_conv3/biases/Adam_1', [256])
('g_/enc_conv3/filters', [4, 4, 4, 128, 256])
('g_/enc_conv3/filters/Adam', [4, 4, 4, 128, 256])
('g_/enc_conv3/filters/Adam_1', [4, 4, 4, 128, 256])
('g_/enc_conv3/filters_init', [4, 4, 4, 128, 256])
('g_/enc_conv4/biases', [512])
('g_/enc_conv4/biases/Adam', [512])
('g_/enc_conv4/biases/Adam_1', [512])
('g_/enc_conv4/filters', [4, 4, 4, 256, 512])
('g_/enc_conv4/filters/Adam', [4, 4, 4, 256, 512])
('g_/enc_conv4/filters/Adam_1', [4, 4, 4, 256, 512])
('g_/enc_conv4/filters_init', [4, 4, 4, 256, 512])
('g_/g_f_bn0/beta', [512])
('g_/g_f_bn0/beta/Adam', [512])
('g_/g_f_bn0/beta/Adam_1', [512])
('g_/g_f_bn0/moving_mean', [512])
('g_/g_f_bn0/moving_variance', [512])
('g_/g_f_bn1/beta', [256])
('g_/g_f_bn1/beta/Adam', [256])
('g_/g_f_bn1/beta/Adam_1', [256])
('g_/g_f_bn1/moving_mean', [256])
('g_/g_f_bn1/moving_variance', [256])
('g_/g_f_bn2/beta', [128])
('g_/g_f_bn2/beta/Adam', [128])
('g_/g_f_bn2/beta/Adam_1', [128])
('g_/g_f_bn2/moving_mean', [128])
('g_/g_f_bn2/moving_variance', [128])
('g_/g_f_bn3/beta', [64])
('g_/g_f_bn3/beta/Adam', [64])
('g_/g_f_bn3/beta/Adam_1', [64])
('g_/g_f_bn3/moving_mean', [64])
('g_/g_f_bn3/moving_variance', [64])
('g_/g_f_h1/biases', [256])
('g_/g_f_h1/biases/Adam', [256])
('g_/g_f_h1/biases/Adam_1', [256])
('g_/g_f_h1/filter_init', [4, 4, 4, 256, 512])
('g_/g_f_h1/filters', [4, 4, 4, 256, 512])
('g_/g_f_h1/filters/Adam', [4, 4, 4, 256, 512])
('g_/g_f_h1/filters/Adam_1', [4, 4, 4, 256, 512])
('g_/g_f_h2/biases', [128])
('g_/g_f_h2/biases/Adam', [128])
('g_/g_f_h2/biases/Adam_1', [128])
('g_/g_f_h2/filter_init', [4, 4, 4, 128, 256])
('g_/g_f_h2/filters', [4, 4, 4, 128, 256])
('g_/g_f_h2/filters/Adam', [4, 4, 4, 128, 256])
('g_/g_f_h2/filters/Adam_1', [4, 4, 4, 128, 256])
('g_/g_f_h3/biases', [64])
('g_/g_f_h3/biases/Adam', [64])
('g_/g_f_h3/biases/Adam_1', [64])
('g_/g_f_h3/filter_init', [4, 4, 4, 64, 128])
('g_/g_f_h3/filters', [4, 4, 4, 64, 128])
('g_/g_f_h3/filters/Adam', [4, 4, 4, 64, 128])
('g_/g_f_h3/filters/Adam_1', [4, 4, 4, 64, 128])
('g_/g_f_h4/biases', [3])
('g_/g_f_h4/biases/Adam', [3])
('g_/g_f_h4/biases/Adam_1', [3])
('g_/g_f_h4/filter_init', [4, 4, 4, 3, 64])
('g_/g_f_h4/filters', [4, 4, 4, 3, 64])
('g_/g_f_h4/filters/Adam', [4, 4, 4, 3, 64])
('g_/g_f_h4/filters/Adam_1', [4, 4, 4, 3, 64])


"
52373,TensorFlow Lite Build and Linker issues in 2.4 -> 2.8 versions,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4 -> 2.8
- Python version: 3.x
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: AMD Radeon series

I pulled a number of versions starting with 2.4 to 2.8 or so and couldn't seem to get the tflite minimal example to build on my own.  I am linking to the libtensorflow-lite.a fine but it is producing strange linker errors with ruy library, even though I am explicitly building it with -DTFLITE_ENABLE_RUY=Off.  I cleaned a million times and it just includes ruy references without being compiled for it.

cmake ../tensorflow/lite -DTFLITE_ENABLE_RUY=OFF -DTFLITE_ENABLE_NNAPI=OFF -DTFLITE_ENABLE_GPU=OFF -DTFLITE_ENABLE_XNNPACK=OFF -DTFLITE_ENABLE_MMAP=OFF

make

It seems to build ruy right away here:
$ make
[  0%] Building CXX object _deps/ruy-build/CMakeFiles/ruy.dir/__/__/ruy/ruy/allocator.cc.o
[  0%] Building CXX object _deps/ruy-build/CMakeFiles/ruy.dir/__/__/ruy/ruy/apply_multiplier.cc.o
[  0%] Building CXX object _deps/ruy-build/CMakeFiles/ruy.dir/__/__/ruy/ruy/block_map.cc.o
[  0%] Building CXX object _deps/ruy-build/CMakeFiles/ruy.dir/__/__/ruy/ruy/blocking_counter.cc.o
[  0%] Building CXX object _deps/ruy-build/CMakeFiles/ruy.dir/__/__/ruy/ruy/cont
...

And when I link against the libtensorflow-lite.a library I get these:
Undefined symbol: ruy::ThreadPool::ExecuteImpl(int, int, ruy::Task*)
Undefined symbol: ruy::Kernel8bitAvx(ruy::KernelParams8bit<8, 8> const&)
Undefined symbol: ruy::Kernel8bitAvx2(ruy::KernelParams8bit<8, 8> const&)
Undefined symbol: ruy::KernelFloatAvx(ruy::KernelParamsFloat<8, 8> const&)
Undefined symbol: ruy::KernelFloatAvx2(ruy::KernelParamsFloat<8, 8> const&)
Undefined symbol: ruy::Kernel8bitAvx512(ruy::KernelParams8bit<16, 16> const&)
..."
52370,SavedModel Loading Issue - '_UserObject' object has no attribute 'add_slot',"Some models that used to be working in TensorFlow 2.5 are now impossible to load with TF 2.6 and onward.

Google Colab to reproduce the issue: https://colab.research.google.com/drive/1n_FY1YNBDaWd43SJI8oxjHPEsA32FPEV?usp=sharing

Script:
```python
import tensorflow as tf
from tensorflow.python.saved_model import tag_constants

print(""TF Version:"", tf.__version__)

saved_model_loaded = tf.saved_model.load(
    ""resnet_v1.5_50_tfv2"", tags=[tag_constants.SERVING]
)

print(""Success !"")
```

Error:
```python
AttributeError                            Traceback (most recent call last)

<ipython-input-2-714dfc7741a0> in <module>()
      5 
      6 saved_model_loaded = tf.saved_model.load(
----> 7     ""resnet_v1.5_50_tfv2"", tags=[tag_constants.SERVING]
      8 )
      9 

4 frames

/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py in _load_nodes(self)
    446         optimized_variable = nodes[
    447             slot_variable_proto.original_variable_node_id]
--> 448         slot_variable = optimizer_object.add_slot(
    449             var=optimized_variable,
    450             slot_name=slot_variable_proto.slot_name)

AttributeError: '_UserObject' object has no attribute 'add_slot'
```"
52368,keras.models.load_model resets the optimizer's state,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `yes, mostly based on the example from https://www.tensorflow.org/guide/keras/save_and_serialize`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab (`Linux 59a52e5448f6 5.4.104+ #1 SMP Sat Jun 5 09:50:34 PDT 2021 x86_64 x86_64 x86_64 GNU/Linux`)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `no`
- TensorFlow installed from (source or binary): `google colab version`
- TensorFlow version (use command below): `v2.6.0-0-g919f693420e 2.6.0`
- Python version: `3.7.12 (default, Sep 10 2021, 00:21:48)  [GCC 7.5.0]`
- Bazel version (if compiling from source): `no`
- GCC/Compiler version (if compiling from source): `no`
- CUDA/cuDNN version: `11.2`
- GPU model and memory: `Tesla K80, 11441MiB`

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When restoring a keras model with `keras.models.load_model`, the returned model's optimizer is in the reset state (e.g. its `weights` attribute is empty).

**Describe the expected behavior**

The original call:
```python
reconstructed_model = tf.keras.models.load_model(""my_model"")
```
should have restored and kept the optimizer's weights.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): `no`
- Briefly describe your candidate solution(if contributing): `-`

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
import numpy as np

def get_model():
    # Create a simple model.
    inputs = tf.keras.Input(shape=(32,))
    outputs = tf.keras.layers.Dense(1)(inputs)
    model = tf.keras.Model(inputs, outputs)
    model.compile(optimizer=""adam"", loss=""mean_squared_error"")
    return model


model = get_model()

# Train the model.
test_input = np.random.random((128, 32))
test_target = np.random.random((128, 1))
model.fit(test_input, test_target)

# Calling `save('my_model')` creates a SavedModel folder `my_model`.
model.save(""my_model"")

# It can be used to reconstruct the model identically.
reconstructed_model = tf.keras.models.load_model(""my_model"")

print(reconstructed_model.optimizer.weights)
```
output:
> 4/4 [==============================] - 1s 4ms/step - loss: 0.1829
INFO:tensorflow:Assets written to: my_model/assets
[]

If we additionally provide a `compile=False` argument, the optimizer's weights are restored:
```python
reconstructed_model = tf.keras.models.load_model(""my_model"", compile=False)
for w in reconstructed_model.optimizer.weights:
    print(w.shape)
```
output:
>(32, 1)
(1,)
(32, 1)
(1,)

However, trying to use the restored optimizer fails with an exception:

```python
reconstructed_model.compile(reconstructed_model.optimizer, loss=""mean_squared_error"")
reconstructed_model.fit(test_input, test_target)
```
output:

```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-3-22a4ff24818b> in <module>()
      1 reconstructed_model.compile(reconstructed_model.optimizer, loss=""mean_squared_error"")
----> 2 reconstructed_model.fit(test_input, test_target)

9 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    992           except Exception as e:  # pylint:disable=broad-except
    993             if hasattr(e, ""ag_error_metadata""):
--> 994               raise e.ag_error_metadata.to_exception(e)
    995             else:
    996               raise

NotImplementedError: in user code:

    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:835 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:791 train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:522 minimize
        return self.apply_gradients(grads_and_vars, name=name)
    /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:660 apply_gradients
        apply_state)
    /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:707 _distributed_apply
        var, apply_grad_to_update_var, args=(grad,), group=False)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2595 update
        var, fn, args=args, kwargs=kwargs, group=group)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2473 _replica_ctx_update
        return replica_context.merge_call(merge_fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3064 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3071 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2471 merge_fn  **
        return self.update(var, fn, merged_args, merged_kwargs, group=group)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2592 update
        return self._update(var, fn, args, kwargs, group)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3646 _update
        return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3652 _update_non_slot
        result = fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:689 apply_grad_to_update_var  **
        update_op = self._resource_apply_dense(grad, var, **apply_kwargs)
    /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:1241 _resource_apply_dense
        raise NotImplementedError(""Must be implemented in subclasses."")

    NotImplementedError: Must be implemented in subclasses.
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52366,tf failed to allocate memory on GPU ,"I have a similar problem. 
I tried batch size 64 or 32,16,8 all failed. 
I also downgrade to tensorflow and tensorflow-gpu to 2.5 and still getting the same error. 

tensorflow.python.framework.errors_impl.ResourceExhaustedError: failed to allocate memory [Op:Mul]

Here is the link to the error: https://pastebin.com/raw/TMSCH9Va

I ran the same code yesterday and it was fine with batch size 64. But I needed to stop to change something and after that I keep getting this error. 

<img width=""731"" alt=""image"" src=""https://user-images.githubusercontent.com/74671619/137164238-6f39cb77-740d-40ff-9040-31eaed1307de.png"">

There is no running process and still 

Sum Total of in-use chunks: 7.17GiB
2021-10-13 11:14:03.092854: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] total_region_allocated_bytes_: 7773224960 memory_limit_: 7773224960 available bytes: 0 curr_region_allocation_bytes_: 15546449920
2021-10-13 11:14:03.092863: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Stats:
Limit:                      7773224960
InUse:                      7702514176
MaxInUse:                   7702514688
NumAllocs:                        2285
MaxAllocSize:               1761195520
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2021-10-13 11:14:03.092920: W tensorflow/core/common_runtime/bfc_allocator.cc:467] ***************************************************************************************************x

$pip freeze
absl-py==0.14.1
alembic==1.4.3
argon2-cffi==20.1.0
asn1crypto==0.24.0
astunparse==1.6.3
async-generator==1.10
attrs==20.3.0
backcall==0.2.0
backports.entry-points-selectable==1.1.0
bleach==3.2.1
cached-property==1.5.2
cachetools==4.2.4
certifi==2020.12.5
certipy==0.1.3
cffi==1.12.3
chardet==3.0.4
clang==5.0
conda==4.8.1
conda-package-handling==1.3.11
cryptography==2.7
cycler==0.10.0
decorator==4.4.2
defusedxml==0.6.0
distlib==0.3.3
entrypoints==0.3
filelock==3.3.0
flatbuffers==1.12
future==0.18.2
gast==0.4.0
google-auth==1.35.0
google-auth-oauthlib==0.4.6
google-pasta==0.2.0
grpcio==1.34.1
h5py==3.1.0
idna==2.8
importlib-metadata==3.4.0
ipykernel==5.4.3
ipython==7.19.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
jedi==0.18.0
Jinja2==2.11.2
joblib==1.1.0
json5==0.9.5
jsonschema==3.2.0
jupyter-client==6.1.11
jupyter-core==4.7.0
jupyter-telemetry==0.1.0
jupyterhub==1.2.2
jupyterlab==2.2.9
jupyterlab-pygments==0.1.2
jupyterlab-server==1.2.0
keras==2.6.0
keras-nightly==2.5.0.dev2021032900
Keras-Preprocessing==1.1.2
kiwisolver==1.3.2
libarchive-c==2.8
libclang==11.1.0
llvmlite==0.37.0
Mako==1.1.4
Markdown==3.3.4
MarkupSafe==1.1.1
matplotlib==3.4.3
mistune==0.8.4
mtcnn==0.1.1
nbclient==0.5.1
nbconvert==6.0.7
nbformat==5.1.1
nbgitpuller==0.9.0
nbresuse==0.3.6
nest-asyncio==1.4.3
notebook==6.1.5
nteract-on-jupyter==2.1.3
numba==0.54.1
numpy==1.19.5
oauthlib==3.1.0
opencv-python==4.5.3.56
opt-einsum==3.3.0
packaging==20.8
pamela==1.0.0
pandas==1.3.3
pandocfilters==1.4.3
parso==0.8.1
pexpect==4.8.0
pickleshare==0.7.5
Pillow==8.3.2
platformdirs==2.4.0
prometheus-client==0.9.0
prompt-toolkit==3.0.10
protobuf==3.18.1
psutil==5.8.0
ptyprocess==0.7.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycosat==0.6.3
pycparser==2.19
Pygments==2.7.4
pyOpenSSL==19.0.0
pyparsing==2.4.7
pyrsistent==0.17.3
PySocks==1.7.0
python-dateutil==2.8.1
python-editor==1.0.4
python-json-logger==2.0.1
pytz==2021.3
pyzmq==21.0.0
requests==2.22.0
requests-oauthlib==1.3.0
rsa==4.7.2
ruamel.yaml.clib==0.2.2
ruamel_yaml==0.15.46
scikit-learn==1.0
Send2Trash==1.5.0
six==1.15.0
SQLAlchemy==1.3.22
tb-nightly==2.7.0a20211010
tensorboard==2.6.0
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.0
tensorflow==2.5.0
tensorflow-estimator==2.5.0
tensorflow-gpu==2.5.0
tensorflow-io-gcs-filesystem==0.21.0
termcolor==1.1.0
terminado==0.9.2
testpath==0.4.4
threadpoolctl==3.0.0
torch==1.6.0
tornado==5.1.1
tqdm==4.32.1
traitlets==5.0.5
typing-extensions==3.7.4.3
urllib3==1.24.2
virtualenv==20.8.1
virtualenv-clone==0.5.7
wcwidth==0.2.5
webencodings==0.5.1
Werkzeug==2.0.2
widgetsnbextension==3.5.1
wrapt==1.12.1
zipp==3.4.0

_Originally posted by @maryamxasghari in https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-942425438_"
52365,'tf.Selu' op is neither a custom op nor a flex op,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS: Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.8
- CUDA/cuDNN version: 11.4
- GPU model and memory: GTX 1060

**Describe the current behavior**
Error when quantizing a model with selu activation.

**Describe the expected behavior**
Quantize selu activation.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no (sorry)

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import pathlib

def get_model():
    input_layer = tf.keras.Input(shape=[128], dtype=tf.float32)
    dense = tf.keras.layers.Dense(128, kernel_initializer='lecun_normal', activation='selu')(input_layer)
    return tf.keras.models.Model(inputs=input_layer, outputs=dense)

def quantize(model):
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_types = [tf.float16]
    tflite_model = converter.convert()

    tflite_models_dir = pathlib.Path('.')
    tflite_models_dir.mkdir(exist_ok=True, parents=True)
    tflite_model_file = tflite_models_dir/'unquantizable.tflite'
    tflite_model_file.write_bytes(tflite_model)
    pass

if __name__ == '__main__':
    model = get_model()
    quantize(model)
    pass
```

**Other info / logs**

```
error: 'tf.Selu' op is neither a custom op nor a flex op
error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: Selu
Details:
        tf.Selu(tensor<?x128xf32>) -> (tensor<?x128xf32>) : {device = """"}
```"
52364,Performance of a model loading the same weights changes depending on the version of tensorflow ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu20.04 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.3 & 2.6.0
- Python version: 3.6.13

**Describe the current behavior**
I received weights of 3 keras models (ResNet50V2, EfficientNetB0, and VGG16) that had been trained with a version of tensorflow 2.x (I don't know and can't find out). I do know the weights were created with the keras model checkpoint callback. I created a new venv and pip installed version 2.6.0 and loaded the weights and tested out the performance on the task. It was completely different to expected performance and terrible. It occurred for all three networks. I spent a while trying to trouble shoot and eventually thought I would try with an older version (2.4.3), and performance was as expected.

**Describe the expected behavior**
Performance should not be different for models loading the same weights under 2.4.3 and 2.6.0.
"
52363,Input name order breaks input alignment during training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, see bellow.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur (11.5.2)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.8.2
- Bazel version (if compiling from source): --
- GCC/Compiler version (if compiling from source): --
- CUDA/cuDNN version: --
- GPU model and memory: --

**Describe the current behavior**

I am training a model based on tabular/ structured data. The model itself is a combination of 2 encoders and a simple layer (model) to compute the losses (See example bellow).

Depending on the input name it breaks the ""data alignment""  during training. For instance, when the `INPUT_BX_NAME` is equal to `b0`, the model trains without any issue, but if `INPUT_BX_NAME` is set to `bx`, then it raises the follow exception:

```
Traceback (most recent call last):
  File ""bug-example.py"", line 45, in <module>
    model.fit([x_a, x_b], y)
  File ""/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/keras/engine/training.py"", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File ""/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 950, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3039, in __call__
    return graph_function._call_flat(
  File ""/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 591, in call
    outputs = execute.execute(
  File ""/Users/tlewin/.virtualenvs/tf-data/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnimplementedError:  Cast string to float is not supported
	 [[node model_2/Cast_3 (defined at bug-example.py:45) ]] [Op:__inference_train_function_970]

Function call stack:
train_function
```

I guess the same issue happens when the input's data types are the same, but probably it's a silent bug. 

**Describe the expected behavior**
The training should consistently works regardless of the name of the inputs.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf


INPUT_BX_NAME = ""bx""


# NOTE: Encoder A
input_a0 = tf.keras.Input(shape=(1,), name=""a0"", dtype=""float32"")
input_a1 = tf.keras.Input(shape=(1,), name=""a1"", dtype=""float32"")
all_feature_a = tf.keras.layers.concatenate([input_a0, input_a1])
output_a = tf.keras.layers.Dense(4, activation=""relu"")(all_feature_a)
encoder_a = tf.keras.Model(inputs=[input_a0, input_a1], outputs=output_a)

# NOTE: Encoder B
input_b1 = tf.keras.Input(shape=(1,), name=""b1"", dtype=""float32"")
input_bx = tf.keras.Input(shape=(1,), name=INPUT_BX_NAME, dtype=""string"")
encoded_bx = tf.keras.layers.StringLookup(
    vocabulary=(""a"", ""b"", ""c""), output_mode=""one_hot""
)(input_bx)
all_feature_b = tf.keras.layers.concatenate([encoded_bx, input_b1])
output_b = tf.keras.layers.Dense(4, activation=""relu"")(all_feature_b)
encoder_b = tf.keras.Model(inputs=[input_bx, input_b1], outputs=output_b)

# NODE: Final model
a_inputs = encoder_a.inputs
b_inputs = encoder_b.inputs
encoded_a = encoder_a(a_inputs)
encoded_b = encoder_b(b_inputs)
prob = tf.keras.layers.Dense(1, activation=""sigmoid"")(tf.abs(encoded_a - encoded_b))
model = tf.keras.Model(inputs=[a_inputs, b_inputs], outputs=prob)

x_a = dict(
    a0=tf.constant([0, 1, 0, 1, 0, 1]),
    a1=tf.constant([1, 1, 0, 0, 0, 1]),
)
x_b = {
    ""b1"": tf.constant([1, 0, 0, 1, 0, 0]),
    INPUT_BX_NAME: tf.constant([""a"", ""b"", ""c"", ""a"", ""b"", ""a""]),
}
y = tf.constant([1, 0, 1, 0, 1, 0])

model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.binary_crossentropy,
    metrics=[""acc""],
)
model.fit([x_a, x_b], y)
```"
52362,the usage of -tf-input-shapes,"Hi,
I tried to use `tf-mlir-translate` to convert tensorflow graph to mlir file.
I am confused about the usage of `-tf-input-shapes`
Here is my code.
![圖片](https://user-images.githubusercontent.com/40784675/137134088-059a686d-cb4c-4805-b13c-4e40812f0c82.png)

And here is the command I used.
![圖片](https://user-images.githubusercontent.com/40784675/137134042-a662d1ad-12d4-430c-88d9-75481a297a6f.png)

Thanks!
"
52361,build_pip_package broken by removal of keras/api directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

build_pip_pacakge fails with error output
cp: cannot stat /tmp/tmp.enXUtfEE9u/tensorflow/python/keras/api/_v2/keras/: No such file or directory

**Provide the exact sequence of commands / steps that you executed before running into the problem**

$ bazel build --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures
$ mkdir tensorflow-pkg
$ bazel-bin/tensorflow/tools/pip_package/build_pip_package --cpu --project_name tensorflow_aarch64 ./tensorflow-pkg

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Seems to be a result of the commit 
https://github.com/tensorflow/tensorflow/commit/61754492129f651f23f05b5aed889f376ec35bb8#diff-cebea885ef40f295a311dfe2879c002bba33695e437f859f7ce111645a99c0ec"
52360,Custom trainable function in tf.scan within keras model,"I need to employ [tf.scan](https://www.tensorflow.org/api_docs/python/tf/scan) with a custom function (**fn**) in my Keras-based constructed model. The function contains a set of layers with some trainable weights; however, tf outputs some warnings which can not be ignored simply and need corrections. Please find below a very simplified code sample to reproduce the warnings.


------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes, a sample code is provided in the next.
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.6
-   **Python version**: Python 3.8.10
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 11.1
-   **GPU model and memory**: 2080 ti
-   **Exact command to reproduce**: a sample code is provided in the next.

------------------------

### Sample code:

```

import tensorflow as tf

class fn(tf.keras.Model):
# class fn(tf.keras.layers.Layer):
  def __init__(self, units:int=512):
    super(fn, self).__init__()
    self.units = units
    self.dense1 = tf.keras.layers.Dense(self.units, name=""dense1"")
    self.dense2 = tf.keras.layers.Dense(self.units, name=""dense2"")
  def call(self, a, x):
    out = self.dense1(a) + self.dense2(x)
    return out
  


class solution:
  
  def __init__(self, a, b, units):
    self.a = a
    self.b = b
    self.units = units
    self.dense0 = tf.keras.layers.Dense(self.units, name=""dense1"")
    self.fn = fn(units=self.units)  
    self.make()
  
  def make(self):
    inputs = tf.keras.layers.Input(shape=[self.a, self.b],name=""inputs"")
    outputs = self.dense0(inputs)
    outputs = tf.scan(self.fn, outputs)
    # outputs = tf.scan(lambda a, x: self.fn(a,x), outputs)
    self.model = tf.keras.Model(inputs=inputs, outputs=outputs) 

model = solution(a=301, b=256, units=256).model

```

------------------------


### Model summary:
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
inputs (InputLayer)          [(None, 301, 256)]        0         
_________________________________________________________________
dense1 (Dense)               (None, 301, 256)          65792     
_________________________________________________________________
tf.scan_1 (TFOpLambda)       (None, 301, 256)          0         
=================================================================
Total params: 65,792
Trainable params: 65,792
Non-trainable params: 0
_________________________________________________________________
```

------------------------


### Warnings:

```
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (tf.scan_1), but
are not present in its tracked objects:
  <tf.Variable 'tf.compat.v1.scan_1/scan/while/fn_1/dense1/kernel:0' shape=(256, 256) dtype=float32>
  <tf.Variable 'tf.compat.v1.scan_1/scan/while/fn_1/dense1/bias:0' shape=(256,) dtype=float32>
  <tf.Variable 'tf.compat.v1.scan_1/scan/while/fn_1/dense2/kernel:0' shape=(256, 256) dtype=float32>
  <tf.Variable 'tf.compat.v1.scan_1/scan/while/fn_1/dense2/bias:0' shape=(256,) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.

```


------------------------

### Other links:

I also followed some other links, but not helpful: [link1](https://stackoverflow.com/questions/66873192/subclassing-a-keras-layer-the-following-variables-were-used-a-lambda-layers-ca) [link2](https://programming.vip/docs/day-6-tensorflow2-model-subclassing-api.html) [link3](https://stackoverflow.com/questions/67724151/keras-layers-multiheadattention-gives-warning-the-following-variables-were-used)

------------------------



### Stackoverflow link: 
(unfortunately, no feedback yet)

[Link](https://stackoverflow.com/questions/69546769/custome-trainable-function-in-tf-scan-variables-used-a-lambda-layers-call-are)

------------------------

**Deeply appreciate any help.**
"
52357,different output value in pytorch->onnx->tflite(int8 quantization),"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): tensorflow:2.5.0-gpu docker
- TensorFlow version (use command below): 2.5.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2/ 8.1.0
- GPU model and memory: RTX 3090

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I convert resnet50 pytorch -> onnx -> tflite with int8 quantization.
output value validation between pytorch <-> onnx, pytorch <-> pb, pytorch <-> tflite, pb <-> tflite
input is same image with size 256, check output value ""np.testing.assert_allclose(output1, output2, rtol=1e-3, atol=1e-05)""
(using tflite interpreter only when i inference tflite ""https://www.tensorflow.org/lite/guide/python?hl=ko"")

Max absolute difference: 0.00076199 in pytorch <-> onnx
Max absolute difference: 0.00112534 in pytorch <-> pb
Max absolute difference: 13.387602 in pytorch <-> tflite(quantized)
Max absolute difference: 13.387438 in pb <-> tflite(quantized)
it's same max absolute difference between tflite(no quantized) and something(pytorch, onnx, pb)
ex) 0.0076~ in pytorch <-> tflite(no quant), 0.0011~ in pytorch <-> tflite(no quant)
i don't know why occur this difference


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.



**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
**pb to tflite log**
2021-10-13 09:18:56.162936: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-10-13 09:18:57.485452: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-10-13 09:18:57.511230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:57.511916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.86GHz coreCount: 82 deviceMemorySize: 23.68GiB deviceMemoryBandwidth: 871.81GiB/s
2021-10-13 09:18:57.511955: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-10-13 09:18:57.513717: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-10-13 09:18:57.513767: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2021-10-13 09:18:57.514354: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2021-10-13 09:18:57.514537: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2021-10-13 09:18:57.515198: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2021-10-13 09:18:57.515720: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2021-10-13 09:18:57.515866: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-10-13 09:18:57.515918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:57.516398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:57.516976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-10-13 09:18:57.517199: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-10-13 09:18:57.517766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:57.518224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.86GHz coreCount: 82 deviceMemorySize: 23.68GiB deviceMemoryBandwidth: 871.81GiB/s
2021-10-13 09:18:57.518272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:57.518814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:57.519243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-10-13 09:18:57.519268: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-10-13 09:18:57.810376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-10-13 09:18:57.810410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-10-13 09:18:57.810420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-10-13 09:18:57.810591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:57.811162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:57.811684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:57.812186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21512 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
2021-10-13 09:18:58.498192: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format.
2021-10-13 09:18:58.498225: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency.
2021-10-13 09:18:58.498234: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored change_concat_input_ranges.
2021-10-13 09:18:58.498881: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: backbone_saved_model/
2021-10-13 09:18:58.515289: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }
2021-10-13 09:18:58.515331: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: backbone_saved_model/
2021-10-13 09:18:58.515383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-10-13 09:18:58.515393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      
2021-10-13 09:18:58.527926: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.
2021-10-13 09:18:58.546224: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3699850000 Hz
2021-10-13 09:18:58.563849: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: backbone_saved_model/
2021-10-13 09:18:58.577967: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 79088 microseconds.
2021-10-13 09:18:58.657933: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2021-10-13 09:18:58.675431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:58.675985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.86GHz coreCount: 82 deviceMemorySize: 23.68GiB deviceMemoryBandwidth: 871.81GiB/s
2021-10-13 09:18:58.676068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:58.676635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:58.677107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-10-13 09:18:58.677148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-10-13 09:18:58.677157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-10-13 09:18:58.677165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-10-13 09:18:58.677253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:58.677779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-13 09:18:58.678280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21512 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0
"
52356,Tensorflow on Apple M1,"Hi,

I would need to get a **tensorflow-gpu<2,>=1.15.2** version for a python library I would need to use on Apple M1. I successfully installed tensorflow on my mac, but the version I have is now 2.6.0. Is there a way to get one <2, or not at all (as it is my understanding at now)?

Thank you,
Silvia"
52355,Use vector version of the logo in README file,"<img src=""https://user-images.githubusercontent.com/29678011/137022160-44006611-08af-4c5f-9ddb-f1d387b999ee.png"" width=""256"" />

Please replace the raster logo in the repository [README.md](https://github.com/tensorflow/tensorflow/blob/master/README.md) file with its vector (SVG) alternative available in the [brand assets](https://www.tensorflow.org/extras/tensorflow-brand-assets.zip).

Also, I think the horizontal lockup is marked as the preferred one in the [brand guidelines](https://www.tensorflow.org/extras/tensorflow_brand_guidelines.pdf).


"
52348,Documentation problem for gradient clipping parameter,"This issue reflect documentation of this: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer

As stated in the above documentation (relevant section c and pasted below), there is no difference between clipnorm and global_clipnorm.  However, looking at the code shows these parameters do 2 different things.  Recommend to update documentation.


clipnorm | float or None. If set, clips gradients to a maximum norm.
clipvalue | float or None. If set, clips gradients to a maximum value.
global_clipnorm | float or None. If set, clips gradients to a maximum norm.
iterations
"
52346,Restore from checkpoint loads optimizer incorrectly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.8
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 11.4
- GPU model and memory: 1080TI 11Gb

**Describe the current behavior**

1. After checkpoint restoration optimizer weights are different from optimizer weights before saving checkpoint.
2. As `assert_consumed` notifies checkpoint file has unresolved optimizer slots (variables). 
```
Unresolved object in checkpoint (root).optimizer.iter: attributes {
  name: ""VARIABLE_VALUE""
  full_name: ""Adam/iter""
  checkpoint_key: ""optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE""
}
```

**Describe the expected behavior**

1. The optimizer weights should be the same before save and after load.
2. The `assert_consumed` should not warn about anything.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): - 

**Standalone code to reproduce the issue**
The reproducible code example is presented in [colab](https://colab.research.google.com/drive/1R01obneq7_jSfBRdUabTAMQIYxuzShqe?usp=sharing).


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52345,ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory. I am using servers,"When I run a tensorflow model then I am getting the error - ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory. 

The missing files are present at the specified location. I have install cuda 11.4 and cuda 9.0. 

Exporting the files from terminal or adding them on .bashrc file does not work.

export LD_LIBRARY_PATH=/usr/local/cuda/lib64\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}

System Configurations :
Total RAM - 125.65 GB
Used Ram - 2.87 GB"
52344,Migration script inserts loss_reduction argument,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): using code from OSS repo mentioned below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): PyPI binary
- TensorFlow version (use command below): 1.15.3
- Python version: 3.7.11
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0.130 / 7.6.5
- GPU model and memory: Tesla T4 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

After applying the [migration script](https://www.tensorflow.org/guide/migrate/upgrade) `tf_upgrade_v2 ` to [the recommenders repo](https://github.com/microsoft/recommenders/tree/main/recommenders), tests on [this code](https://github.com/microsoft/recommenders/blob/main/recommenders/models/wide_deep/wide_deep_utils.py) 
fail with the error
```
E       AttributeError: module 'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction'
```
The cause of the error is that the script has inserted an additional argument 
`loss_reduction=tf.keras.losses.Reduction.SUM`
in lines https://github.com/microsoft/recommenders/blob/27709229cdc4aa7d39ab715789f093a2d21d2661/recommenders/models/wide_deep/wide_deep_utils.py#L176
https://github.com/microsoft/recommenders/blob/27709229cdc4aa7d39ab715789f093a2d21d2661/recommenders/models/wide_deep/wide_deep_utils.py#L186
https://github.com/microsoft/recommenders/blob/27709229cdc4aa7d39ab715789f093a2d21d2661/recommenders/models/wide_deep/wide_deep_utils.py#L200

**Describe the expected behavior**

It seems that the migration script should not insert the `loss_reduction` argument in the code. Because after removing this argument the tests pass successfully. 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):N/A

**Standalone code to reproduce the issue**

From a conda environment with `python=3.7 cudatoolkit=10.0 ""cudnn>=7"" tensorflow==1.15` do
```
git clone https://github.com/microsoft/recommenders.git
cd recommenders
tf_upgrade_v2 --intree recommenders --outtree recommenders_v2/recommenders --reportfile recommenders_report.txt
tf_upgrade_v2 --intree tests --outtree recommenders_v2/tests --reportfile tests_report.txt
mv recommenders_v2/recommenders recommenders
mv recommenders_v2/tests tests
```
Deactivate this env and then do
```
conda create -n tf1_15 python=3.7 cudatoolkit=10.0 ""cudnn>=7""
conda activate tf1_15
pip install --upgrade pip setuptools
pip install .[gpu,dev]
pytest tests/unit/recommenders/models/test_wide_deep_utils.py::test_wide_model
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
[log.txt](https://github.com/tensorflow/tensorflow/files/7329622/log.txt)
"
52343,Pixel OpenGL only not working,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.6 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel3a
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below):2.4
- Python version: Python 3.6.9
- Bazel version (if compiling from source): bazel 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I have an [application](https://github.com/terryky/android_tflite/tree/master/tflite_dbface) running face detection with c++ on android, and when using default Tensorflow GPU backend (which seems to be OpenCL) everything works fine, but when adding `.experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_GL_ONLY,`  because I have an android device which support only OpenGL drivers (db845c) I get no errors but nothing is displayed on screen and the detection results are 0.
**Describe the expected behavior**
I would expect at least having same error logs when it cannot use OpenGL only as if OpenCL is not supported, GPU Delegate will automatically fall back to OpenGL and run in this problem.

**Standalone code to reproduce the issue**
Code using to run with OpenGL only: 
![Screenshot from 2021-10-12 11-21-18](https://user-images.githubusercontent.com/80002509/136929039-3d5eef9b-2547-410e-af00-0dde7d4a6a7b.png)
The only difference between apps below is removing `.experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_GL_ONLY,` 
I have prebuild the same app in both modes so you can test it easier. Details are in the name of the application. 
[face_detection_gpu.zip](https://github.com/tensorflow/tensorflow/files/7328587/face_detection_gpu.zip)


**Other info / logs** 
This are logs that I get from Pixel3a when running model with OpenGL only.
[pixel3a.txt](https://github.com/tensorflow/tensorflow/files/7328564/pixel3a.txt)
Model used in this app: 
[dbface_keras_480x640_float32_nhwc.zip](https://github.com/tensorflow/tensorflow/files/7328651/dbface_keras_480x640_float32_nhwc.zip)
"
52336,Unspecified data type in image_dataset_from_directory,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue: https://keras.io/api/preprocessing/image/#imagedatasetfromdirectory-function

There are issues with the follwoing entries: 

**labels:** Either ""inferred"" (labels are generated from the directory structure), None (no labels), or a list/tuple of integer labels of the same size as the number of image files found in the directory. Labels should be sorted according to the alphanumeric order of the image file paths (obtained via os.walk(directory) in Python).
**label_mode:** - 'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss). - 'categorical' means that the labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss). - 'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1 (e.g. for binary_crossentropy). - None (no labels).

## Description of issue (what needs changing):

The data types for input are not clearly described and there are no examples of what would be an acceptable input if its is not feasible to supply a data directory organized into files by class labels.  Currently when i try this approach the fucntion does not even recognize that there are files in the directory I am giving it, but when I move up in the directory and supply the file containing the images instead I get a Dataset object for training a single class. 

The errors are not useful and confusing.  It is not clear why the function does not recognize that there are image files in the folder in one case and not another.  Examples would make implementing this usecase much easier.

"
52335,tensoflow.keras.preprocessing.image_dataset_from_directory doesn't recognize there are files in a directory when labels are supplied as a list/tupple,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

https://keras.io/api/preprocessing/image/#imagedatasetfromdirectory-function

My code:

```
img_width = 224
img_hieght = 224
batch_size = 100
train_labels = train_metadat_df.sort_values(by='filename')['sirna'].tolist()

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  ""../input/recursion-cellular-image-classification-224-jpg/train/train"",
  validation_split=0.2,
  subset=""training"",
  labels= train_labels,
  label_mode= ""categorical"",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
```

This function normally takes a pointer for a directory with images organized by lab into directories ( i.e. all flowers in a file called flowers, all animals in a file called animals) and then outputs a dataset object.  But there is also an option to provide a list of 
When providing a list of file labels instead but when this happens the following error is produced:

`ValueError: Expected the lengths of `labels` to match the number of files in the target directory. len(labels) is 73030 while we found 0 files in ../input/recursion-cellular-image-classification-224-jpg/train/train.`

There are ~73000 .jpeg files in that directory and when I run it on ../input/recursion-cellular-image-classification-224-jpg/train it finds ~54000 of them and trains them all on one class (because they are all stored in one file).

**System information**
- using Tensorflow in a kaggle notebook
- currently no GPU is turned on
- tensorflow version 2.4.1

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52332,Compiling TF 2.6 in debug mode on Windows env. (unresolved externals),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): r2.6.0
- Python version: 3.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.3/8
- GPU model and memory: GTX 1060

**Describe the current behavior**
I try to compile my example program that utilises TF:
// tensorflow/cc/example/example.cc

#include ""tensorflow/cc/client/client_session.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/tensor.h""

int main() {
  using namespace tensorflow;
  using namespace tensorflow::ops;
  Scope root = Scope::NewRootScope();
  // Matrix A = [3 2; -1 0]
  auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });
  // Vector b = [3 5]
  auto b = Const(root, { {3.f, 5.f} });
  // v = Ab^T
  auto v = MatMul(root.WithOpName(""v""), A, b, MatMul::TransposeB(true));
  std::vector<Tensor> outputs;
  ClientSession session(root);
  // Run and fetch v
  TF_CHECK_OK(session.Run({v}, &outputs));
  // Expect outputs[0] == [19; -3]
  LOG(INFO) << outputs[0].matrix<float>();
  return 0;
}

After patching few files to allow debug compilation (https://github.com/tensorflow/tensorflow/issues/51799#issuecomment-912567685) all tensorflow libs compile, but the example linkage still fails.

bazel build --jobs 1 --local_ram_resources=HOST_RAM*.7 -c dbg --config=opt --config=windows --config=cuda --copt=/FS --copt=-nvcc_options=disable-warnings --linkopt=/DEBUG:NONE --strip=never --define=no_tensorflow_py_deps=true -s  --verbose_explanations --subcommands=pretty_print //tensorflow/cc/example:example

ERROR: E:/tensorflow_r2.6_cpp_debug_gpu/tensorflow/cc/example/BUILD:3:13: Linking of rule '//tensorflow/cc/example:example' failed (Exit 1120): link.exe failed: error executing command
  cd E:/tensorflow_r2.6_cpp_debug_gpu/output/fqonm2l5/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3
    SET CUDNN_INSTALL_PATH=C:/Program Files/cudnn-11.3-windows-x64-v8.2.0.53
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30037\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30037\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30037\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30037\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30037\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\\MSBuild\Current\Bin;C:\windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\andrey\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.0,7.5,8.0,8.6
    SET TF_CUDA_VERSION=11.3
    SET TF_CUDNN_VERSION=8
    SET TMP=C:\Users\andrey\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC/14.29.30037/bin/HostX64/x64/link.exe @bazel-out/x64_windows-dbg/bin/tensorflow/cc/example/example.exe-2.params
Execution platform: @local_execution_config_platform//:platform
   Creating library bazel-out\x64_windows-dbg\bin\tensorflow\cc\example\example.lib and object bazel-out\x64_windows-dbg\bin\tensorflow\cc\example\example.exp
LINK : warning LNK4286: symbol '?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'utils.lib(utils.obj)'
LINK : warning LNK4286: symbol '?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'captured_function.lib(captured_function.obj)'
LINK : warning LNK4286: symbol '?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'arithmetic_optimizer.lib(arithmetic_optimizer.obj)'
LINK : warning LNK4286: symbol '?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'memory_optimizer.lib(memory_optimizer.obj)'
LINK : warning LNK4286: symbol '?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'pin_to_host_optimizer.lib(pin_to_host_optimizer.obj)'
LINK : warning LNK4217: symbol '?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'batch_kernels.lo.lib(batch_kernels.obj)' in function '""void __cdecl tensorflow::`dynamic initializer for 'register_kernel_0''(void)"" (??__Eregister_kernel_0@tensorflow@@YAXXZ)'
LINK : warning LNK4286: symbol '?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'window_dataset.lib(window_dataset.obj)'
LINK : warning LNK4286: symbol '?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'ragged_tensor_variant.lib(ragged_tensor_variant.obj)'
LINK : warning LNK4286: symbol '?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'snapshot_utils.lib(snapshot_utils.obj)'
LINK : warning LNK4286: symbol '?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'utils.lib(utils.obj)'
LINK : warning LNK4286: symbol '?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'gpu_id_impl.lib(gpu_id_manager.obj)'
LINK : warning LNK4217: symbol '?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'batch_kernels.lo.lib(batch_kernels.obj)' in function '""void __cdecl tensorflow::`dynamic initializer for 'register_kernel_1''(void)"" (??__Eregister_kernel_1@tensorflow@@YAXXZ)'
LINK : warning LNK4286: symbol '?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'arithmetic_optimizer.lib(arithmetic_optimizer.obj)'
LINK : warning LNK4286: symbol '?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'memory_optimizer.lib(memory_optimizer.obj)'
LINK : warning LNK4286: symbol '?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU)' defined in 'tensor.lo.lib(types.obj)' is imported by 'pin_to_host_optimizer.lib(pin_to_host_optimizer.obj)'
LINK : warning LNK4286: symbol '?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level)' defined in 'traceme_recorder_impl.lo.lib(traceme_recorder.obj)' is imported by 'bfc_allocator.lib(bfc_allocator.obj)'
LINK : warning LNK4217: symbol '?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level)' defined in 'traceme_recorder_impl.lo.lib(traceme_recorder.obj)' is imported by 'batch_kernels.lo.lib(batch_kernels.obj)' in function '""public: static bool __cdecl tensorflow::profiler::TraceMeRecorder::Active(int)"" (?Active@TraceMeRecorder@profiler@tensorflow@@SA_NH@Z)'
LINK : warning LNK4286: symbol '?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level)' defined in 'traceme_recorder_impl.lo.lib(traceme_recorder.obj)' is imported by 'batch_resource_base.lib(batch_resource_base.obj)'
LINK : warning LNK4286: symbol '?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level)' defined in 'traceme_recorder_impl.lo.lib(traceme_recorder.obj)' is imported by 'snapshot_utils.lib(snapshot_utils.obj)'
LINK : warning LNK4286: symbol '?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level)' defined in 'traceme_recorder_impl.lo.lib(traceme_recorder.obj)' is imported by 'captured_function.lib(captured_function.obj)'
LINK : warning LNK4217: symbol 'TF_DataTypeSize' defined in 'tf_datatype.lo.lib(tf_datatype.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4286: symbol 'TF_DataTypeSize' defined in 'tf_datatype.lo.lib(tf_datatype.obj)' is imported by 'tf_tensor.lib(tf_tensor.obj)'
LINK : warning LNK4217: symbol 'TF_NewStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl RegisterBitcastOp(void)"" (?RegisterBitcastOp@@YAXXZ)'
LINK : warning LNK4286: symbol 'TF_NewStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_NewStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_NewStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_DeleteStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl RegisterBitcastOp(void)"" (?RegisterBitcastOp@@YAXXZ)'
LINK : warning LNK4286: symbol 'TF_DeleteStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_DeleteStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_DeleteStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4286: symbol 'TF_SetStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'tf_tensor.lib(tf_tensor.obj)'
LINK : warning LNK4217: symbol 'TF_SetStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4286: symbol 'TF_SetStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_SetStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_SetStatus' defined in 'tf_status.lib(tf_status.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_GetCode' defined in 'tf_status.lib(tf_status.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4286: symbol 'TF_GetCode' defined in 'tf_status.lib(tf_status.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_GetCode' defined in 'tf_status.lib(tf_status.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_GetCode' defined in 'tf_status.lib(tf_status.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_Message' defined in 'tf_status.lib(tf_status.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl RegisterBitcastOp(void)"" (?RegisterBitcastOp@@YAXXZ)'
LINK : warning LNK4286: symbol 'TF_Message' defined in 'tf_status.lib(tf_status.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_Message' defined in 'tf_status.lib(tf_status.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_Message' defined in 'tf_status.lib(tf_status.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_NewOpDefinitionBuilder' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl RegisterBitcastOp(void)"" (?RegisterBitcastOp@@YAXXZ)'
LINK : warning LNK4286: symbol 'TF_NewOpDefinitionBuilder' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_NewOpDefinitionBuilder' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_NewOpDefinitionBuilder' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_RegisterOpDefinition' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl RegisterBitcastOp(void)"" (?RegisterBitcastOp@@YAXXZ)'
LINK : warning LNK4286: symbol 'TF_RegisterOpDefinition' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_RegisterOpDefinition' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_RegisterOpDefinition' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_OpDefinitionBuilderAddAttr' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl RegisterBitcastOp(void)"" (?RegisterBitcastOp@@YAXXZ)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderAddAttr' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderAddAttr' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderAddAttr' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_OpDefinitionBuilderAddInput' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl RegisterBitcastOp(void)"" (?RegisterBitcastOp@@YAXXZ)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderAddInput' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderAddInput' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderAddInput' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_OpDefinitionBuilderAddOutput' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl RegisterBitcastOp(void)"" (?RegisterBitcastOp@@YAXXZ)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderAddOutput' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderAddOutput' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderAddOutput' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_OpDefinitionBuilderSetShapeInferenceFunction' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl RegisterBitcastOp(void)"" (?RegisterBitcastOp@@YAXXZ)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderSetShapeInferenceFunction' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderSetShapeInferenceFunction' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_OpDefinitionBuilderSetShapeInferenceFunction' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_NewShapeHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextGetInput' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextSetOutput' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4286: symbol 'TF_ShapeInferenceContextSetOutput' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_ShapeInferenceContextSetOutput' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_ShapeInferenceContextSetOutput' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextVectorFromSize' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_NewDimensionHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContext_GetAttrType' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextRankKnown' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextWithRankAtLeast' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextDim' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextSubshape' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextSetUnknownShape' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_DimensionHandleValueKnown' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_DimensionHandleValue' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextConcatenateShapes' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_DeleteShapeHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4286: symbol 'TF_DeleteShapeHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)'
LINK : warning LNK4286: symbol 'TF_DeleteShapeHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_DeleteShapeHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_DeleteDimensionHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextScalar' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)' in function '""void __cdecl histogram_summary_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?histogram_summary_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextScalar' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)' in function '""void __cdecl merge_summary_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?merge_summary_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextScalar' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)' in function '""void __cdecl scalar_summary_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?scalar_summary_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_NumDims' defined in 'tf_tensor.lib(tf_tensor.obj)' is imported by 'tensor_shape_utils.lib(tensor_shape_utils.obj)' in function '""class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::ShapeDebugString(struct TF_Tensor *)"" (?ShapeDebugString@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAUTF_Tensor@@@Z)'
LINK : warning LNK4217: symbol 'TF_Dim' defined in 'tf_tensor.lib(tf_tensor.obj)' is imported by 'tensor_shape_utils.lib(tensor_shape_utils.obj)' in function '""class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::ShapeDebugString(struct TF_Tensor *)"" (?ShapeDebugString@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAUTF_Tensor@@@Z)'
concat_op.lo.lib(concat_op.obj) : error LNK2019: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt32>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQInt32@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function ""public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,struct Eigen::QInt32,1>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@UQInt32@2@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
pack_op.lo.lib(pack_op.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt32>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQInt32@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
tensor_array_ops.lo.lib(tensor_array_ops.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt32>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQInt32@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
list_kernels.lo.lib(list_kernels.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt32>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQInt32@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
concat_op.lo.lib(concat_op.obj) : error LNK2019: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt16>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQInt16@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function ""public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,struct Eigen::QInt16,1>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@UQInt16@2@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
list_kernels.lo.lib(list_kernels.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt16>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQInt16@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
concat_op.lo.lib(concat_op.obj) : error LNK2019: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt16>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQUInt16@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function ""public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,struct Eigen::QUInt16,1>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@UQUInt16@2@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
list_kernels.lo.lib(list_kernels.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt16>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQUInt16@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
concat_op.lo.lib(concat_op.obj) : error LNK2019: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function ""public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,struct Eigen::QInt8,1>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@UQInt8@2@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
pack_op.lo.lib(pack_op.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
tensor_array_ops.lo.lib(tensor_array_ops.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
list_kernels.lo.lib(list_kernels.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
concat_op.lo.lib(concat_op.obj) : error LNK2019: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQUInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function ""public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,struct Eigen::QUInt8,1>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@UQUInt8@2@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
pack_op.lo.lib(pack_op.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQUInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
tensor_array_ops.lo.lib(tensor_array_ops.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQUInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
list_kernels.lo.lib(list_kernels.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@UQUInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
concat_op.lo.lib(concat_op.obj) : error LNK2019: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<class tensorflow::tstring>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@Vtstring@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function ""public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring,1>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
pack_op.lo.lib(pack_op.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<class tensorflow::tstring>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@Vtstring@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
tensor_array_ops.lo.lib(tensor_array_ops.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<class tensorflow::tstring>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@Vtstring@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
list_kernels.lo.lib(list_kernels.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<class tensorflow::tstring>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@Vtstring@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,struct Eigen::QInt8,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@UQInt8@2@$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::GpuDevice,struct Eigen::QInt8>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UGpuDevice@Eigen@@UQInt8@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@H$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned char,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@E$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBE$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@E$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@E@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::Variant,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::tstring,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,bool,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@_N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<double>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double>,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@N@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class std::complex<double> >::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@V?$complex@N@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<float>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float>,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@M@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class std::complex<float> >::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@V?$complex@M@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,double,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<double,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBN$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,double>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,float,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<float,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@M$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBM$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@M$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,float>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@M@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@Ubfloat16@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUbfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Ubfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Ubfloat16@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::half,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@Uhalf@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Uhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,struct Eigen::half>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Uhalf@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,signed char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<signed char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<signed char,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@C$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBC$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@C$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<short,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@F$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBF$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@F$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,short>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@F@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@G$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBG$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@G$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned short>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@G@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned int,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@I$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBI$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@I$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned int>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@I@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,__int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<__int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<__int64,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@_J$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,__int64>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_J@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned __int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@_K$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned __int64>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_K@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,struct Eigen::QInt8,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,5,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,5,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@UQInt8@2@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$04$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$04$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::GpuDevice,struct Eigen::QInt8>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UGpuDevice@Eigen@@UQInt8@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,struct Eigen::QInt8,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,5,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,5,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@UQInt8@2@$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$04$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$04$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::GpuDevice,struct Eigen::QInt8>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UGpuDevice@Eigen@@UQInt8@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@H$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::Variant,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::tstring,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,bool,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@_N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<double>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double>,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@N@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class std::complex<double> >::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@V?$complex@N@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<float>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float>,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@M@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class std::complex<float> >::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@V?$complex@M@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,double,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<double,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBN$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,double>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,float,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<float,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@M$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBM$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@M$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,float>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@M@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@Ubfloat16@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUbfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Ubfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Ubfloat16@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::half,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@Uhalf@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Uhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,struct Eigen::half>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Uhalf@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,signed char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<signed char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<signed char,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@C$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBC$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@C$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned char,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@E$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBE$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@E$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@E@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<short,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@F$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBF$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@F$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,short>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@F@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@G$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBG$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@G$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned short>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@G@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned int,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@I$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBI$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@I$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned int>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@I@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,__int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<__int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<__int64,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@_J$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,__int64>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_J@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned __int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@_K$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned __int64>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_K@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
pack_op.lo.lib(pack_op.obj) : error LNK2019: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<class tensorflow::Variant>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@VVariant@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function ""public: virtual void __cdecl tensorflow::PackOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$PackOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
tensor_array_ops.lo.lib(tensor_array_ops.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<class tensorflow::Variant>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@VVariant@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
list_kernels.lo.lib(list_kernels.obj) : error LNK2001: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<class tensorflow::Variant>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@VVariant@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)
pack_op.lo.lib(pack_op.obj) : error LNK2019: unresolved external symbol ""void __cdecl tensorflow::ConcatGPU<class tensorflow::ResourceHandle>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,2,1,__int64>,16,struct Eigen::MakePointer> *)"" (??$ConcatGPU@VResourceHandle@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function ""public: virtual void __cdecl tensorflow::PackOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$PackOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
resource_variable_ops.lo.lib(resource_variable_ops.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DenseUpdate<struct Eigen::GpuDevice,class tensorflow::Variant,2>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,1,1,__int64>,16,struct Eigen::MakePointer>,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,1,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DenseUpdate@UGpuDevice@Eigen@@VVariant@tensorflow@@$01@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@VVariant@tensorflow@@$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""class tensorflow::Status __cdecl tensorflow::EnsureSparseVariableAccess<struct Eigen::GpuDevice,class tensorflow::Variant>(class tensorflow::OpKernelContext *,class tensorflow::Var *)"" (??$EnsureSparseVariableAccess@UGpuDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@YA?AVStatus@0@PEAVOpKernelContext@0@PEAVVar@0@@Z)
self_adjoint_eig_v2_op.lo.lib(self_adjoint_eig_v2_op_gpu.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::UnaryFunctor<struct Eigen::GpuDevice,struct tensorflow::functor::conj<double> >::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double,1,1,__int64>,16,struct Eigen::MakePointer>,class Eigen::TensorMap<class Eigen::Tensor<double const ,1,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$UnaryFunctor@UGpuDevice@Eigen@@U?$conj@N@functor@tensorflow@@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@N$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@V?$TensorMap@V?$Tensor@$$CBN$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SelfAdjointEigV2OpGpu<double>::ComputeAsync(class tensorflow::OpKernelContext *,class std::function<void __cdecl(void)>)"" (?ComputeAsync@?$SelfAdjointEigV2OpGpu@N@tensorflow@@UEAAXPEAVOpKernelContext@2@V?$function@$$A6AXXZ@std@@@Z)
self_adjoint_eig_v2_op.lo.lib(self_adjoint_eig_v2_op_gpu.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::UnaryFunctor<struct Eigen::GpuDevice,struct tensorflow::functor::conj<float> >::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float,1,1,__int64>,16,struct Eigen::MakePointer>,class Eigen::TensorMap<class Eigen::Tensor<float const ,1,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$UnaryFunctor@UGpuDevice@Eigen@@U?$conj@M@functor@tensorflow@@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@M$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@V?$TensorMap@V?$Tensor@$$CBM$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SelfAdjointEigV2OpGpu<float>::ComputeAsync(class tensorflow::OpKernelContext *,class std::function<void __cdecl(void)>)"" (?ComputeAsync@?$SelfAdjointEigV2OpGpu@M@tensorflow@@UEAAXPEAVOpKernelContext@2@V?$function@$$A6AXXZ@std@@@Z)
resize_bilinear_op.lo.lib(resize_bilinear_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::ResizeBilinearGrad<struct Eigen::GpuDevice,struct Eigen::half>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float const ,4,1,__int64>,16,struct Eigen::MakePointer>,float,float,bool,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$ResizeBilinearGrad@UGpuDevice@Eigen@@Uhalf@2@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBM$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@MM_NV?$TensorMap@V?$Tensor@Uhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::ResizeBilinearOpGrad<struct Eigen::GpuDevice,struct Eigen::half>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$ResizeBilinearOpGrad@UGpuDevice@Eigen@@Uhalf@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
bazel-out\x64_windows-dbg\bin\tensorflow\cc\example\example.exe : fatal error LNK1120: 51 unresolved externals
Target //tensorflow/cc/example:example failed to build
INFO: Elapsed time: 67285.637s, Critical Path: 2170.18s
INFO: 12571 processes: 3669 internal, 8902 local.
FAILED: Build did NOT complete successfully"
52331,How to use to use TFLite model in Flask ?,"I have a tflite model which accepts float32 input. Works fine if I load model each time on prediction. But for faster processing, I want to do as below. Then it's throwing an error of -

****RuntimeError: There is at least 1 reference to internal data in the interpreter in the form of a NumPy array or slice. Be sure to only hold the function returned from tensor() if you are using raw data access.****

```
[physical_devices = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)  
    
app = Flask(__name__,template_folder='template', static_url_path = ""/static"")
CORS(app)

interpreter = tf.lite.Interpreter(model_path=""quant_model.tflite"")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

def generate_frames(frame):

    img_face = cv2.resize(frame,(256,256))
    img_face = cv2.cvtColor(img_face, cv2.COLOR_BGR2RGBA)

    #converting into float32
    img_face_f = (img_face/255.0).astype(np.float32)

    #prediction
    img_face_f = run_inference(np.expand_dims(img_face_f[:,:,:3], axis=0)) # <<< problem happens here

            
    final_result = (img_face_f*255).astype(np.uint8)
            
    ret,buffer=cv2.imencode('.jpg',final_result)

    frame=buffer.tobytes()

    return frame



def run_inference(image):
    # perform inference and parse the outputs
    interpreter.set_tensor(input_details[0]['index'], image)
    interpreter.invoke()
    outputs = interpreter.get_tensor(output_details[0]['index'])[0]
    return outputs
    
    
if __name__ == '__main__':
    app.run(debug=True)](url)
```

Why TFLite doesn’t support calling like above?"
52330,Problems to convert a TF model with mutable variables even if I use converter.experimental_enable_resource_variables = True,"### System information

- OS Platform and Distribution: Linux Ubuntu 20.10
- TensorFlow installation: pip package
- Tensorflow version: 2.6.0
-  Python version: 3.8

I'm trying to convert a TF model which uses a custom layer where there are some tf.Variable (instead of numpy arrays for partial computations) and for loops statements (not tf.while_loop statements). The model works fine without the custom layer. 
In order to convert the model I use these lines of code:

    tmpdir = tempfile.mkdtemp()
    model_save_path = os.path.join(tmpdir, ""model/1/"")
    tf.saved_model.save(model, model_save_path) 
    
    converter = tf.lite.TFLiteConverter.from_saved_model(model_save_path)
    converter.target_spec.supported_ops = [
       tf.lite.OpsSet.TFLITE_BUILTINS, 
       tf.lite.OpsSet.SELECT_TF_OPS 
    ]
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()
    with open('model.tflite', 'wb') as f:
       f.write(tflite_model)

However I get this error:
    
    WARNING:absl:Found untraced functions such as mylayer_layer_call_and_return_conditional_losses, mylayer_layer_call_fn,     
    mylayer_layer_call_fn, mylayer_layer_call_and_return_conditional_losses, mylayer_layer_call_and_return_conditional_losses    
    while saving (showing 5 of 5). These functions will not be directly callable after loading.
    2021-10-11 19:37:04.586845: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.
    2021-10-11 19:37:04.586883: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored     
    drop_control_dependency.
    2021-10-11 19:37:04.586892: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored   
    change_concat_input_ranges.
    2021-10-11 19:37:04.587804: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /tmp/tmp332hc4f9  
    /model/1/
    2021-10-11 19:37:04.647880: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }
    2021-10-11 19:37:04.647960: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from:     
    /tmp/tmp332hc4f9/model/1/
    2021-10-11 19:37:04.990385: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.
    2021-10-11 19:37:05.508553: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at   
    path: /tmp/tmp332hc4f9/model/1/
    2021-10-11 19:37:05.985453: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: 
    OK. Took 1397650 microseconds.
    2021-10-11 19:37:07.818608: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash 
    reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
    loc(""module_wrapper/mylayer/Variable""): error: is not immutable, try removing mutable variables in your model since mutable 
    variables are currently not supported through this converter
    Traceback (most recent call last):
    File ""model.py"", line 231, in <module>
    tflite_model = converter.convert()
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 729, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 715, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 994, in convert
    result = _convert_saved_model(**converter_kwargs)
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 215, in wrapper
    raise converter_error from None  # Re-throws the exception.
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 208, in wrapper
    return func(*args, **kwargs)
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert.py"", line 821, in convert_saved_model
    data = toco_convert_protos(
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert.py"", line 313, in toco_convert_protos
    raise converter_error
    tensorflow.lite.python.convert_phase.ConverterError: <unknown>:0: error: loc(""module_wrapper/mylayer/Variable""): is not   
    immutable, try removing mutable variables in your model since mutable variables are currently not supported through this    
    converter

Instead, if I add this line:
  
    converter.experimental_enable_resource_variables =True

I get this:

    WARNING:absl:Found untraced functions such as mylayer_layer_call_and_return_conditional_losses, mylayer_layer_call_fn,     
    mylayer_layer_call_fn, mylayer_layer_call_and_return_conditional_losses, mylayer_layer_call_and_return_conditional_losses    
    while saving (showing 5 of 5). These functions will not be directly callable after loading.
    2021-10-11 19:37:04.586845: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.
    2021-10-11 19:37:04.586883: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored     
    drop_control_dependency.
    2021-10-11 19:37:04.586892: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored   
    change_concat_input_ranges.
    2021-10-11 19:37:04.587804: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /tmp/tmp332hc4f9  
    /model/1/
    2021-10-11 19:37:04.647880: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }
    2021-10-11 19:37:04.647960: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from:     
    /tmp/tmp332hc4f9/model/1/
    2021-10-11 19:37:04.990385: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.
    2021-10-11 19:37:05.508553: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at   
    path: /tmp/tmp332hc4f9/model/1/
    2021-10-11 19:37:05.985453: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: 
    OK. Took 1397650 microseconds.
    2021-10-11 19:37:07.818608: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash 
    reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.

    segmentation fault (core dumped)

With these lines:

    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.target_spec.supported_ops = [
       tf.lite.OpsSet.TFLITE_BUILTINS, 
       tf.lite.OpsSet.SELECT_TF_OPS 
    ]
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()
    with open('model.tflite', 'wb') as f:
       f.write(tflite_model)

I have this error:
   
    WARNING:absl:Found untraced functions such as mylayer_layer_call_fn, mylayer_layer_call_and_return_conditional_losses, 
    mylayer_layer_call_fn, mylayer_layer_call_and_return_conditional_losses, mylayer_layer_call_and_return_conditional_losses 
    while saving (showing 5 of 5). These functions will not be directly callable after loading.
    2021-10-11 19:57:58.096866: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute    
    capability >= 0.0): 0
    2021-10-11 19:57:58.097108: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
    2021-10-11 19:58:00.117971: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler   
    item: graph_to_optimize
    function_optimizer: Graph size after: 41 nodes (24), 46 edges (27), time = 273.451ms.
    function_optimizer: Graph size after: 41 nodes (0), 46 edges (0), time = 471.521ms.
    Optimization results for grappler item: while_body_119
    function_optimizer: function_optimizer did nothing. time = 0.019ms.
    function_optimizer: function_optimizer did nothing. time = 0.002ms.
    Optimization results for grappler item: while_cond_118
    function_optimizer: function_optimizer did nothing. time = 0.015ms.
    function_optimizer: function_optimizer did nothing. time = 0.002ms.

    Traceback (most recent call last):
    File ""model.py"", line 231, in <module>
    tflite_model = converter.convert()
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 729, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 715, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1123, in convert
    self._freeze_keras_model())
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 218, in wrapper
    raise error from None  # Re-throws the exception.
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 208, in wrapper
    return func(*args, **kwargs)
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1079, in _freeze_keras_model
    _convert_to_constants.convert_variables_to_constants_v2_as_graph(
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1234, in    
    convert_variables_to_constants_v2_as_graph
    frozen_func = _construct_concrete_function(func, output_graph_def,
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1078, in    
    _construct_concrete_function
    new_func = wrap_function.function_from_graph_def(output_graph_def,
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 650, in    
    function_from_graph_def
    wrapped_import = wrap_function(_imports_graph_def, [])
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 621, in wrap_function
    func_graph.func_graph_from_py_func(
    File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1007, in   
    func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
     File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 87, in __call__
     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
     File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 93, in wrapped
     return fn(*args, **kwargs)
     File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 648, in _imports_graph_def
     importer.import_graph_def(graph_def, name="""")
     File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 549, in new_func
     return func(*args, **kwargs)
     File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 400, in import_graph_def
     return _import_graph_def_internal(
     File ""/home/es/venv/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 501, in     
      _import_graph_def_internal
      raise ValueError(str(e))

     ValueError: Input 0 of node sequential/module_wrapper/mylayer/StatefulPartitionedCall/AssignVariableOp was passed int32     
     from Func/sequential/module_wrapper/mylayer/StatefulPartitionedCall/input/_1:0 incompatible with expected resource.













"
52329,Invalid argument error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10

- TensorFlow installed from (source or binary):
pip install tensorflow

- TensorFlow version (use command below):
2.6.0

- Python version:
3.7

- CUDA/cuDNN version:
11.3

- GPU model and memory:
NVIDIA 2060 SUPER, compute capability: 7.5
6010MB


I am trying to create a model that takes two images (one taken right after the other) and train it so that it can predict how much movement has occurred. I use a smaller model that processes one image at a time, then concatenate the two outputs in a larger model.

I tried testing it and the model compiles just fine, but it crashes when I call fit() and gives me an invalid argument error.

```
2021-10-11 10:46:28.321065: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2021-10-11 10:46:29.319124: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at transpose_op.cc:143 : Invalid argument: transpose expects a vector of size 5. But input(1) is a vTraceback (most recent call last):
2021-10-11 10:46:29.319416: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at transpose_op.cc:143 : Invalid argument: transpose expects a vector of size 5. But input(1) is a vTraceback (most recent call last):
  File ""d:/.../Deep Sight/deep_sight.py"", line 155, in <module>
    main()
  File ""d:/.../Deep Sight/deep_sight.py"", line 150, in main
    final_model.fit(train_data, epochs=5)
  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\engine\training.py"", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"", line 950, in _call
    return self._stateless_fn(*args, **kwds)
  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"", line 3040, in __call__
    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"", line 1964, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"", line 596, in call
    ctx=ctx)
  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  transpose expects a vector of size 5. But input(1) is a vector of size 4
         [[{{node gradient_tape/model_1/model/conv2d/Conv2D/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]
         [[Func/mean_squared_error/cond/then/_0/input/_29/_48]]
  (1) Invalid argument:  transpose expects a vector of size 5. But input(1) is a vector of size 4
         [[{{node gradient_tape/model_1/model/conv2d/Conv2D/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_3095]

Function call stack:
train_function -> train_function
```

**Standalone code to reproduce the issue**
My batch size is 32 as defined in my dataset. I believe it has something to do with the squeeze method because in an earlier version of this model I did not use the squeeze or split methods and it ran perfectly fine. However, because my dataset is much larger now, I shifted to using tf.Datasets to input the data. This required me to input the 2 images together in a single tensor as the input.

The shape of my dataset is: (batch, features, labels) with the label being a scalar, and each feature being of shape: (2, 256, 256, 3)

```python
def main():
    # Create model

    # Start with smaller model that processes the two images in the same way.
    single_image_input = keras.Input(shape=(256,256,3))

    image = layers.Conv2D(64, (3,3))(single_image_input)
    image = layers.LeakyReLU()(image)
    image = layers.BatchNormalization()(image)
    # Run through MaxPool2D to help the algorithm identify features in different areas of the image.
    # Has the effect of downsampling and cutting the dimensions in half.
    image = layers.MaxPool2D()(image)

    image = layers.Conv2D(128, (3, 3))(image)
    image = layers.LeakyReLU()(image)
    image = layers.BatchNormalization()(image)
    image = layers.Dropout(.3)(image)

    image_model = keras.Model(single_image_input, image)
    
    # Create larger model
    image_inputs = keras.Input(shape=(2,256,256,3))

    first_image, second_image = tf.split(image_inputs, num_or_size_splits=2, axis=0)
    first_image, second_image = tf.squeeze(first_image), tf.squeeze(second_image)

    image_outputs = [image_model(first_image), image_model(second_image)]
    model = layers.Concatenate()(image_outputs)

    model = layers.Flatten()(model)

    model = layers.Dense(128)(model)
    model = layers.LeakyReLU()(model)
    model = layers.BatchNormalization()(model)
    model = layers.Dropout(.3)(model)

    # Output is change in y-position of drone
    out_layer = layers.Dense(1, activation='linear')(model)

    final_model = keras.Model(image_inputs, out_layer)
    final_model.compile(loss=""mse"", optimizer=optimizers.Adam(lr=0.0003, beta_1=0.7))

    image_model.summary()

    final_model.summary()


    #Preprocess data
    print(""Loading and processing data..."")
    train_data = tf_load_data()

    #Train model
    final_model.fit(train_data, epochs=5)
```
"
52325,keras.layers.IntegerLookup fails to deserialize vocubulary,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6
- Python version: 3.7

**Describe the current behavior**
When creating a keras.layers.IntegerLookup, we provide a vocabulary.  This is saved off via serialization.  However, upon deserialization, the vocabulary is not loaded correctly --- the layer continues to work, however, one cannot serialize again.

**Describe the expected behavior**
We should be able to serialize and deserialize IntegerLookups any number of times.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

To reproduce, see https://colab.research.google.com/drive/1tpXdEsfYKyax5QhYLR7KAV4P2vQHIT8D?usp=sharing .  It is interesting to note that I cannot reproduce by just serializing to JSON --- I have to serialize to SavedModel.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

From looking at the code, https://github.com/keras-team/keras/blob/v2.6.0/keras/layers/preprocessing/index_lookup.py#L200 , it appears that we are setting `vocabulary`, but not `input_vocabulary`.  Since `vocabulary` is set, the layer works fine.  But, upon the next serialization, we do serialize an empty vocab: https://github.com/keras-team/keras/blob/v2.6.0/keras/layers/preprocessing/index_lookup.py#L333 .

It is not clear to me the difference between `input_vocabulary` and `vocabulary`.

This is also related to https://github.com/tensorflow/tensorflow/issues/43834
"
52324,Add support for python 3.10 please.,
52323,TFlite model is too slow when Output Buffer is Direct().,"**System information**
- tensorflow-lite-gpu:2.7.0-rc0
- Are you willing to contribute it- Yes



**Describe the feature and the current behavior/state.**
I'm trying to use output buffer as```output =  ByteBuffer.allocateDirect(4 * LARGE_DIMENSIONS)
                .let {
                    it.order(ByteOrder.nativeOrder())
                    it.asFloatBuffer()
                }```
               with a buffer of large dimensions. The problem is that using direct allocation make the model run half of the velocity. I believe this because current tflite api is only able to use `copyTo`  instead of what happens with direct memory allocation in `setTo`.
Can you confirm that this is the expected behavior? 
"
52322,tensorflow_addons.losses.metric_learning.pairwise_distance. Cannot convert a symbolic Tensor (2nd_target:0) to a numpy array,"Whent I try to use the pairwise_distance function is the the following error ""Cannot convert a symbolic Tensor (2nd_target:0) to a numpy array"".

I found that is because **line 67 in the pairwise_distance function** is written as:

**tf.ones([num_data])**

Instead of: 

**tf.ones((num_data))**

Please check if this is correct and fix it.

The function works perfectly after I changed that line of code.
"
52321,Tutorial link is missing,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/lite/tutorials/pose_classification

## Description of issue (what needs changing):

![2021-10-11-16-41-12](https://user-images.githubusercontent.com/22192610/136760032-46631c11-5db9-46f4-9833-2ae4f2e4618a.png)
Tutorial link is missing:
![2021-10-11-16-42-33](https://user-images.githubusercontent.com/22192610/136760173-e6fcbaa5-db92-4f74-8ffb-3fe1870f4ff4.png)
![2021-10-11-16-43-19](https://user-images.githubusercontent.com/22192610/136760314-73b2970d-d283-4cfe-b319-918b0549b848.png)



### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
52320,CropAndResizeGradImage yields different results for each run,"**System information**
- Have I written custom code, see attached script at the end
- Linux Ubuntu 18.04, kernel 4.15
- TensorFlow 2.6 installed via pip
- Python version 3.6.9

**Describe the current behavior**
running the attached script fails due to results mismatch between runs

**Describe the expected behavior**
all runs of CropAndResizeGradImage should produce the same exact results

Note that changing channels to smaller value makes the test passing.

Code:
```python
import numpy as np
import tensorflow as tf

out_dtype = tf.float32
img_size = 1
C = 40 # stable results
C = 41 # unstable results
N = 1  # batch size
H = img_size
W = img_size
B = 2  # qnt of boxes
boxes_shape = (B, 4)
indices_shape = (B,)
cropsize = [14, 14]
image_size = [N, H, W, C]
grads_shape = (B, cropsize[0], cropsize[1], C)
np.random.seed(1000)
grads = np.random.uniform(-1.0, 1.0, grads_shape)
boxes = np.zeros(boxes_shape)
indices = np.zeros(indices_shape)

def run_graph():
    return tf.raw_ops.CropAndResizeGradImage(
            grads=grads,
            boxes=boxes,
            box_ind=indices,
            image_size=image_size,
            method='bilinear',
            T=out_dtype)

first_result = run_graph()

for i in range(10):
    np.testing.assert_array_equal(run_graph(), first_result)
```"
52319,Build the proto files using protoc seperately,"Hi Tensorflow team,

I want to separately compile the proto files which are present here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/framework and generate the .pb.h and .pb.h.cc files for each proto file.
So, what is the way to do that? Can anyone help me with the same? Would be a great help from yourside.

**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow version: 2.3
- Python version: 3.6
- Bazel version (if compiling from source): 3.9.1
- GCC/Compiler version (if compiling from source): 7.5.0

Looking forward to your reply.

Thanks and Regards



"
52318,tf.lite.Interpreter set_tensor failing to properly recognize uint8 input tensors,"**System information**
- Ubuntu 20.0.04
- Intel Atom
- Binary installation installed from build without SSE3 instructions
- TensorFlow 2.3.0
- Python 3.6
- No CUDA/GPU/TPU


**Describe the current behavior**

I have a working `.tflite` model (which takes 180x180 float greyscale image) as input, and returns 6 float sigmoid outputs. All works as-expected yielding expected results with test images.

I am trying to quantize `.tflite` model to `uint8`. Notice I am setting the input and output types to `unit8` (edited for brevity):

```model = tf.keras.models.load_model(""bkgmodel.h5"")
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
```
When I try to run inference on it with the following code (brevity):

``` img_array= np.array(np.expand_dims(img_array/1.0,0), dtype=np.uint8)
  print (""IMAGE_ARRAY type"",img_array.dtype)
  interpreter = tf.lite.Interpreter(model_path=""bkgmodel_quant.tflite"")
  interpreter.resize_tensor_input(0, [1, 180, 180, 1])
  interpreter.allocate_tensors()
  print (""INPUT TENSOR"",interpreter.get_input_details())
  input = interpreter.tensor(interpreter.get_input_details()[0][""index""])
  output = interpreter.tensor(interpreter.get_output_details()[0][""index""])
  interpreter.set_tensor(0, img_array)
```
When I run I get the following error:

`ValueError: Cannot set tensor: Got value of type UINT8 but expected type INT8 for input 0, name: input_2_int8`

When I look at the types of the image data and input tensors, they inded _are_ `uint8`:

```
IMAGE_ARRAY type uint8
INPUT TENSOR [{'name': 'input_2', 'index': 22, 'shape': array([  1, 180, 180,   1], dtype=int32), 'shape_signature': array([ -1, 180, 180,   1], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (1.0, 0), 'quantization_parameters': {'scales': array([1.], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
```
(look only at `dtype` in the input tensor, I assume).

So - indeed, the input tensor _should_ be a uint8, and the image _is_ a uint8 - yet I get this error.

As an experiment - I tried changing _only_ my inference code to set the image data to `int8`. i.e:

```img_array= np.array(np.expand_dims(img_array/1.0,0), dtype=np.int8)
```

When I do, the error goes away and the inference runs (as expected) - no error, *however* my models predictions are all wrong.

(It is notable that the output values all add up to _almost_ 256. I am assuming this means that the model is working correctly and yielding valid data - and I am also assuming that 8-bit sigmoids are expected to have a bit of a roundoff error where they don't add up to exactly 256??)

If I am doing something wrong, or is this a bug in Tensorflow-lite??

**Describe the expected behavior**

I would expect that when I leave it as:
`img_array= np.array(np.expand_dims(img_array/1.0,0), dtype=np.uint8)`
...that it runs with no error, and yields expected inference results against these known files  - which give expected results running all the same code (except changing the `uint8` and `int8`s above to floats) with a non-quantitized, float `.tflite` model.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
See `liteinfer.py` and `convert.py` (quantize) at:
https://github.com/bkgoodman/espcam_training_tools


"
52314,The loss is negative number and the accuracy is always 0 per epoch,"I downloaded csv data of boston house price to practice;
The csv data have 13 columns and the No.13 column is price;
But the loss is negative number and became bigger and bigger, the accuracy is always 0 per epoch when training.
Is there any wrong with my code when process the data or build the model?
```
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

f = open('bostonHouse.csv')

df = pd.read_csv(f)
data = np.array(df)
plt.figure()
plt.plot(data)
plt.show()

normalize_data = (data - np.mean(data)) / np.std(data)
normalize_data = normalize_data[:, np.newaxis]

train_x, train_y = [], []
for i in range(len(normalize_data)):
    x = normalize_data[i][0][:12]
    y = normalize_data[i][0][12]
    train_x.append(x.tolist())
    train_y.append(y.tolist())

# print(""train_x data:{}"".format(train_x[:1]))
# print(""train_y data:{}"".format(train_y[:1]))
# print('train x len', len(train_x[0]))
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(12,)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    loss=""binary_crossentropy"",
    optimizer='adam',
    metrics=['accuracy'],
)

model.summary()
model.fit(x=train_x, y=train_y, epochs=20)
```
Training result:
```
Epoch 15/20
16/16 [==============================] - 0s 2ms/step - loss: -57761.8555 - accuracy: 0.0000e+00
Epoch 16/20
16/16 [==============================] - 0s 2ms/step - loss: -78732.5547 - accuracy: 0.0000e+00
Epoch 17/20
16/16 [==============================] - 0s 2ms/step - loss: -105031.8750 - accuracy: 0.0000e+00
Epoch 18/20
16/16 [==============================] - 0s 2ms/step - loss: -137373.6094 - accuracy: 0.0000e+00
Epoch 19/20
16/16 [==============================] - 0s 2ms/step - loss: -177102.2344 - accuracy: 0.0000e+00
Epoch 20/20
16/16 [==============================] - 0s 2ms/step - loss: -224581.0312 - accuracy: 0.0000e+00
```
my csv data:
```
a | b | c | d | e | f | g | h | i | j | k | l | m
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | 296 | 15.3 | 396.9 | 4.98
0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.9 | 9.14
0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03
0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94
0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.9 | 5.33
0.02985 | 0 | 2.18 | 0 | 0.458 | 6.43 | 58.7 | 6.0622 | 3 | 222 | 18.7 | 394.12 | 5.21
0.08829 | 12.5 | 7.87 | 0 | 0.524 | 6.012 | 66.6 | 5.5605 | 5 | 311 | 15.2 | 395.6 | 12.43
...
```

"
52312,TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.3
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.6.0
Keras version (use command below): 2.6.0
Python version: 3.8.12
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Describe the current behavior

When building simple Sequential model with Keras, strange errors occur. I'v tried to update Conda, re-create a new py38 virtualenv to establish another environment but still encountered the same issue. I'v searched similiar issues before, like re-install protobuf, add pywrap_tensorflow etc. but did not work for me. It should be compatible issue. The traceback is:
-----------------------------------------
TypeError                                 Traceback (most recent call last)
/var/folders/94/dvh9t1_x3w9bnp_ghskg3rzr0000gn/T/ipykernel_17083/2882519647.py in <module>
      1 # define the model
----> 2 model = Sequential([
      3     Dense(units=16,input_shape=(1,),activation='relu'),
      4     Dense(units=32,activation='relu'),
      5     Dense(units=2,activation='softmax')

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    528     self._self_setattr_tracking = False  # pylint: disable=protected-access
    529     try:
--> 530       result = method(self, *args, **kwargs)
    531     finally:
    532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)
    112     """"""
    113     # Skip the init in FunctionalModel since model doesn't have input/output yet
--> 114     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call
    115         name=name, autocast=False)
    116     base_layer.keras_api_gauge.get_cell('Sequential').set(True)

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    528     self._self_setattr_tracking = False  # pylint: disable=protected-access
    529     try:
--> 530       result = method(self, *args, **kwargs)
    531     finally:
    532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)
    316     self._steps_per_execution = None
    317 
--> 318     self._init_batch_counters()
    319     self._base_model_initialized = True
    320 

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    528     self._self_setattr_tracking = False  # pylint: disable=protected-access
    529     try:
--> 530       result = method(self, *args, **kwargs)
    531     finally:
    532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _init_batch_counters(self)
    324     # `evaluate`, and `predict`.
    325     agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA
--> 326     self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)
    327     self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)
    328     self._predict_counter = variables.Variable(

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    266       return cls._variable_v1_call(*args, **kwargs)
    267     elif cls is Variable:
--> 268       return cls._variable_v2_call(*args, **kwargs)
    269     else:
    270       return super(VariableMetaclass, cls).__call__(*args, **kwargs)

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)
    248     if aggregation is None:
    249       aggregation = VariableAggregation.NONE
--> 250     return previous_getter(
    251         initial_value=initial_value,
    252         trainable=trainable,

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)
    241                         shape=None):
    242     """"""Call on Variable class. Useful to force the signature.""""""
--> 243     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
    244     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    245       previous_getter = _make_getter(getter, previous_getter)

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)
   2660   shape = kwargs.get(""shape"", None)
   2661 
-> 2662   return resource_variable_ops.ResourceVariable(
   2663       initial_value=initial_value,
   2664       trainable=trainable,

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    268       return cls._variable_v2_call(*args, **kwargs)
    269     else:
--> 270       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    271 
    272 

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
   1601       self._init_from_proto(variable_def, import_scope=import_scope)
   1602     else:
-> 1603       self._init_from_args(
   1604           initial_value=initial_value,
   1605           trainable=trainable,

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
   1755           else:
   1756             shape = initial_value.shape
-> 1757           handle = eager_safe_variable_handle(
   1758               initial_value=initial_value,
   1759               shape=shape,

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in eager_safe_variable_handle(initial_value, shape, shared_name, name, graph_mode)
    237   """"""
    238   dtype = initial_value.dtype.base_dtype
--> 239   return _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name,
    240                                                graph_mode, initial_value)
    241 

~/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name, graph_mode, initial_value)
    177     handle_data.is_set = True
    178     handle_data.shape_and_type.append(
--> 179         cpp_shape_inference_pb2.CppShapeInferenceResult.HandleShapeAndType(
    180             shape=shape.as_proto(), dtype=dtype.as_datatype_enum))
    181 

TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.
"
52311,"""zoom_range"" argument of ""ImageDataGenerator"" class show distortions around edges","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are
not verified bugs in TensorFlow, please go to
[Discourse](https://discuss.tensorflow.org/).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org

================================================================================

""zoom_range"" argument of ""ImageDataGenerator"" class shows distortions/noise on the augmented images towards the edges. It turns out, new augmentations are carried out on previously augmented image instead of original image. Please have a look at image #2 and image #4 from left in below images. The legs of the dog are distorted for a zoom_range=0.5

![image](https://user-images.githubusercontent.com/20167138/136653991-92ecdc9b-a895-4dc9-8b94-fc00493c9b06.png)
"
52310,copy_to_device too slow,"now the order of my dataset:
```TFRecordDataset->batch->map->copy_to_device```
map will output lots of small tensor, small tensor memcpyH2D is too slow. data by nsys:
![image](https://user-images.githubusercontent.com/33950866/136652365-9639e001-c0b2-4ce0-9d3f-e02e73139d2a.png)

```TFRecordDataset->batch->map->merge->copy_to_device->split``` maybe have a good performance.

I can contribute this part of the code if necessary


"
52309,wrong cupti dll name,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):  source
- TensorFlow version (use command below): 2.6.0
- Python version: 3.9.7
- Bazel version (if compiling from source): 3.9.2
- GCC/Compiler version (if compiling from source): Visual Studio 2019
- CUDA/cuDNN version: 11.4/8.2.4
- GPU model and memory: RTX3090 GDDR6x 24GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
wrong cupti name for loading

**Describe the expected behavior**
cupti loaded

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):  change filename to  cupti64*.dll

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=""logs"")
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=""logs"")
```"
52308,shared_embedding can't be used with tf.keras.layers.DenseFeatures,"Hi, i'm using tf.keras.layers.DenseFeatures with feature_columns to build my model. But it seems i can't use shared_embedding in feature_columns.
code:
         input_layer = tf.keras.layers.DenseFeatures(all_feature_columns)

and in all_feature_columns, there exists a _shared_embedding:

`_SharedEmbeddingColumn(categorical_column=HashedCategoricalColumn(key='tag_1_int', hash_bucket_size=1000, dtype=tf.string), dimension=100, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7fec1bc30d60>, shared_embedding_collection_name='tag', ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True)`

and it causes:
`ValueError: Items of feature_columns must be a FeatureColumn. Given (type <class 'tensorflow.python.feature_column.feature_column._SharedEmbeddingColumn'>): _SharedEmbeddingColumn(categorical_column=HashedCategoricalColumn(key='tag_1_int', hash_bucket_size=1000, dtype=tf.string), dimension=100, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7fd488b86dc0>, shared_embedding_collection_name='tag', ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True)`

when i change the _SharedEmbeddingColumn to EmbeddingColumn, it works.

Same question with tf.keras.experimental.SequenceFeatures([seq_feature_column])

To summary, that's DenseFeatures and SequenceFeatures can't accept _SharedEmbeddingColumn

**System information**
- TensorFlow version 2.6.0
- Python 3.8


"
52307,map_on_gpu,"Does this function work?
Where is the manual for this function?
```tensorflow/python/data/experimental/ops/prefetching_ops.py: 264``` ```def map_on_gpu(map_func):```"
52303,Argmin failures for N-D tensors,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
tensor = tf.constant([[1.5, 3.2], 
                      [1.7, 0.9]])
argmin = tf.argmin(a_total)
```
I would expect argmin to be the global index, i.e. [1,1]. however, it returns [0,1]

**Describe the expected behavior**
I would expect argmin to be the global index, i.e. [1,1]. however, it returns [0,1]

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
tensor = tf.constant([[1.5, 3.2], 
                      [1.7, 0.9]])
argmin = tf.argmin(a_total)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52301,NotAbleToGenerate : Inference time and model size ,"I have tested a model, I have also run the epochs but I am not able to generate inference time and model size.
Can you tell me a way in which I can print these two? I search it in the TensorFlow library but it shows an error.
Please also share the necessary links.
Thanks!"
52300,Segmentation fault when running inference using the TensorFlow Lite C++ API,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): Compiled against TensorFlow 919f693420e35d00c8d0a42100837ae3718f7927, however we've also seen the issue in [the `tflite` crate](https://github.com/boncheolgu/tflite-rs) which is pinned to 3db52be
- Python version: N/A
- Bazel version (if compiling from source): We have seen this when using both the Bazel and CMake builds
- GCC/Compiler version (if compiling from source): Various - seen using recent Clang and GCC
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

While using the TensorFlow Lite C++ API inside a Rust application, doing inference on certain models will trigger a segfault deep inside the `gemmlowp` library used by TensorFlow Lite. This only seems to occur on MacOS machines and not for all models.

Steps to reproduce:

```shell
# Install the Rust toolchain from rustup
$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Download a version of the Rune binary
$ wget https://github.com/hotg-ai/rune/releases/download/v0.8.0/rune.x86_64-apple-darwin.zip
$ unzip rune.x86_64-apple-darwin.zip
$ chmod +x ./rune

# Run the Rune
$ ./rune run examples/person_detection/person_detection.rune --image examples/person_detection/image_grayscale.png
zsh: segmentation fault
```

**Describe the expected behavior**

No segfault.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No
- Briefly describe your candidate solution(if contributing): N/A

**Standalone code to reproduce the issue**


**Other info / logs**

We managed to get a backtrace using LLDB. 

```
(lldb) thread backtrace
* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x3110)
  * frame #0: 0x000000010095f664 rune`void gemmlowp::DispatchGemmShape<unsigned char, short, gemmlowp::BitDepthParams<gemmlowp::OperandRange<1, 255>, gemmlowp::OperandRange<0, 255> >, (gemmlowp::MapOrder)1, (gemmlowp::MapOrder)0, (gemmlowp::MapOrder)0, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)0>, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)1>, std::__1::tuple<gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToInt16>, gemmlowp::GemmContext>(gemmlowp::GemmContext*, gemmlowp::MatrixMap<unsigned char const, (gemmlowp::MapOrder)1> const&, gemmlowp::MatrixMap<unsigned char const, (gemmlowp::MapOrder)0> const&, gemmlowp::MatrixMap<short, (gemmlowp::MapOrder)0>*, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)0> const&, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)1> const&, std::__1::tuple<gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToInt16> const&) + 20
    frame #1: 0x000000028456d9c0
    frame #2: 0x000000010095b3e8 rune`gemmlowp::ComputeImpl<gemmlowp::PackedSideBlock<gemmlowp::KernelSideFormat<gemmlowp::CellFormat<4, 2, (gemmlowp::CellOrder)1>, 3> >, gemmlowp::PackedSideBlock<gemmlowp::KernelSideFormat<gemmlowp::CellFormat<4, 2, (gemmlowp::CellOrder)1>, 1> >, gemmlowp::PackedResult>::ComputeRun(int, int, int, int) + 216
    frame #3: 0x00000001008cd436 rune`void gemmlowp::SingleThreadGemm<gemmlowp::KernelFormat<gemmlowp::KernelSideFormat<gemmlowp::CellFormat<4, 2, (gemmlowp::CellOrder)1>, 3>, gemmlowp::KernelSideFormat<gemmlowp::CellFormat<4, 2, (gemmlowp::CellOrder)1>, 1> >, unsigned char, unsigned char, gemmlowp::BitDepthParams<gemmlowp::OperandRange<1, 255>, gemmlowp::OperandRange<0, 255> >, (gemmlowp::MapOrder)1, (gemmlowp::MapOrder)0, (gemmlowp::MapOrder)1, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)1>, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)0>, std::__1::tuple<gemmlowp::OutputStageBiasAddition<gemmlowp::VectorMap<int const, (gemmlowp::VectorShape)1> >, gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToUint8> >(gemmlowp::SingleThreadGemmContext*, gemmlowp::KernelBase const&, gemmlowp::MatrixMap<unsigned char const, (gemmlowp::MapOrder)1> const&, gemmlowp::MatrixMap<unsigned char const, (gemmlowp::MapOrder)0> const&, gemmlowp::MatrixMap<unsigned char, (gemmlowp::MapOrder)1>*, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)1> const&, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)0> const&, std::__1::tuple<gemmlowp::OutputStageBiasAddition<gemmlowp::VectorMap<int const, (gemmlowp::VectorShape)1> >, gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToUint8> const&) + 1398
    frame #4: 0x00000001008cc28f rune`void gemmlowp::DispatchGemmShape<unsigned char, unsigned char, gemmlowp::BitDepthParams<gemmlowp::OperandRange<1, 255>, gemmlowp::OperandRange<0, 255> >, (gemmlowp::MapOrder)1, (gemmlowp::MapOrder)0, (gemmlowp::MapOrder)1, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)1>, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)0>, std::__1::tuple<gemmlowp::OutputStageBiasAddition<gemmlowp::VectorMap<int const, (gemmlowp::VectorShape)1> >, gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToUint8>, gemmlowp::GemmContext>(gemmlowp::GemmContext*, gemmlowp::MatrixMap<unsigned char const, (gemmlowp::MapOrder)1> const&, gemmlowp::MatrixMap<unsigned char const, (gemmlowp::MapOrder)0> const&, gemmlowp::MatrixMap<unsigned char, (gemmlowp::MapOrder)1>*, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)1> const&, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)0> const&, std::__1::tuple<gemmlowp::OutputStageBiasAddition<gemmlowp::VectorMap<int const, (gemmlowp::VectorShape)1> >, gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToUint8> const&) + 287
    frame #5: 0x00000001008cc13b rune`void gemmlowp::DispatchGemmShape<unsigned char, unsigned char, gemmlowp::BitDepthParams<gemmlowp::OperandRange<1, 255>, gemmlowp::OperandRange<0, 255> >, (gemmlowp::MapOrder)1, (gemmlowp::MapOrder)0, (gemmlowp::MapOrder)0, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)0>, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)1>, std::__1::tuple<gemmlowp::OutputStageBiasAddition<gemmlowp::VectorMap<int const, (gemmlowp::VectorShape)0> >, gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToUint8>, gemmlowp::GemmContext>(gemmlowp::GemmContext*, gemmlowp::MatrixMap<unsigned char const, (gemmlowp::MapOrder)1> const&, gemmlowp::MatrixMap<unsigned char const, (gemmlowp::MapOrder)0> const&, gemmlowp::MatrixMap<unsigned char, (gemmlowp::MapOrder)0>*, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)0> const&, gemmlowp::VectorDup<int const, (gemmlowp::VectorShape)1> const&, std::__1::tuple<gemmlowp::OutputStageBiasAddition<gemmlowp::VectorMap<int const, (gemmlowp::VectorShape)0> >, gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToUint8> const&) + 251
    frame #6: 0x00000001008cbfcc rune`tflite::cpu_backend_gemm::detail::GemmImplUsingGemmlowp<unsigned char, unsigned char, int, unsigned char, (tflite::cpu_backend_gemm::QuantizationFlavor)1>::Run(tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char*, tflite::cpu_backend_gemm::GemmParams<int, unsigned char, (tflite::cpu_backend_gemm::QuantizationFlavor)1> const&, tflite::CpuBackendContext*) + 220
    frame #7: 0x00000001008c93e7 rune`void tflite::cpu_backend_gemm::Gemm<unsigned char, unsigned char, int, unsigned char, (tflite::cpu_backend_gemm::QuantizationFlavor)1>(tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char*, tflite::cpu_backend_gemm::GemmParams<int, unsigned char, (tflite::cpu_backend_gemm::QuantizationFlavor)1> const&, tflite::CpuBackendContext*) + 279
    frame #8: 0x00000001008c92ba rune`tflite::optimized_ops::Conv(tflite::ConvParams const&, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, unsigned char*, tflite::RuntimeShape const&, unsigned char*, tflite::CpuBackendContext*) + 618
    frame #9: 0x00000001008df171 rune`void tflite::ops::builtin::conv::EvalQuantized<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*) + 1249
    frame #10: 0x00000001008dcb1a rune`TfLiteStatus tflite::ops::builtin::conv::EvalImpl<(tflite::ops::builtin::conv::KernelType)2, (TfLiteType)3>(TfLiteContext*, TfLiteNode*) + 698
    frame #11: 0x00000001008a5780 rune`TfLiteStatus tflite::ops::builtin::conv::Eval<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*) + 128
    frame #12: 0x0000000100a757be rune`tflite::Subgraph::Invoke() + 1150
    frame #13: 0x0000000100a79579 rune`tflite::Interpreter::Invoke() + 89
    frame #14: 0x000000010081fe5b rune`infer + 139
    frame #15: 0x00000001005dac7b rune`hotg_runecoral::context::InferenceContext::infer::h9edb39c38c81e707 + 763
    frame #16: 0x000000010015a4b8 rune`_$LT$hotg_runicos_base_runtime..image..runecoral..RuneCoralModel$u20$as$u20$hotg_runicos_base_runtime..image..Model$GT$::infer::hc87b3f895994a40f + 312
    frame #17: 0x0000000100167a63 rune`hotg_runicos_base_runtime::image::wasmer_impl::rune_model_infer::he7a5c51e5c2920d7 + 1091
    frame #18: 0x00000001001551eb rune`_$LT$Func$u20$as$u20$wasmer..externals..function..inner..HostFunction$LT$$LP$A1$C$A2$C$A3$RP$$C$Rets$C$wasmer..externals..function..inner..WithEnv$C$Env$GT$$GT$::function_body_ptr::func_wrapper::h490ab2c575aacc33 + 11
    frame #19: 0x00000001017c7dd8
    frame #20: 0x00000001017cb7bc
    frame #21: 0x00000001017cc8de
    frame #22: 0x00000001017db3f2
    frame #23: 0x000000010057b409 rune`wasmer_register_setjmp + 73
    frame #24: 0x0000000100177780 rune`wasmer_vm::trap::traphandlers::tls::set::h18404a427f1d465b + 128
    frame #25: 0x000000010017372a rune`wasmer_vm::trap::traphandlers::wasmer_call_trampoline::hb18b59c24dca92f7 + 138
    frame #26: 0x0000000100176e3d rune`wasmer::native::NativeFunc$LT$$LP$A1$C$A2$C$A3$RP$$C$Rets$GT$::call::h2d748b56879248d3 + 525
    frame #27: 0x0000000100172c1b rune`hotg_rune_wasmer_runtime::Runtime::call::h0cd6d828d9f429eb + 331
    frame #28: 0x0000000100034ac5 rune`hotg_rune_cli::run::command::Run::execute::hb9bfa1ab91f6d41b + 4053
    frame #29: 0x00000001000073bc rune`rune::main::h823e153ddfeaa780 + 4220
    frame #30: 0x000000010000aff6 rune`std::sys_common::backtrace::__rust_begin_short_backtrace::h96a6a8ebc29b41a3 + 6
    frame #31: 0x0000000100002ee1 rune`std::rt::lang_start::_$u7b$$u7b$closure$u7d$$u7d$::h1b2d1c373cb0f18b + 17
    frame #32: 0x00000001007f6819 rune`std::rt::lang_start_internal::h89c9a0e84eca73a6 [inlined] core::ops::function::impls::_$LT$impl$u20$core..ops..function..FnOnce$LT$A$GT$$u20$for$u20$$RF$F$GT$::call_once::hdf4f188556f78df7 at function.rs:259:13 [opt]
    frame #33: 0x00000001007f680c rune`std::rt::lang_start_internal::h89c9a0e84eca73a6 [inlined] std::panicking::try::do_call::hb7b1a93f0946c677 at panicking.rs:403 [opt]
    frame #34: 0x00000001007f680c rune`std::rt::lang_start_internal::h89c9a0e84eca73a6 [inlined] std::panicking::try::hc49e70695c7e0cac at panicking.rs:367 [opt]
    frame #35: 0x00000001007f680c rune`std::rt::lang_start_internal::h89c9a0e84eca73a6 [inlined] std::panic::catch_unwind::h3df4d361649a5b95 at panic.rs:129 [opt]
    frame #36: 0x00000001007f680c rune`std::rt::lang_start_internal::h89c9a0e84eca73a6 [inlined] std::rt::lang_start_internal::_$u7b$$u7b$closure$u7d$$u7d$::hdba2bd3fb6f7733e at rt.rs:45 [opt]
    frame #37: 0x00000001007f680c rune`std::rt::lang_start_internal::h89c9a0e84eca73a6 [inlined] std::panicking::try::do_call::h90a258d8350a7861 at panicking.rs:403 [opt]
    frame #38: 0x00000001007f680c rune`std::rt::lang_start_internal::h89c9a0e84eca73a6 [inlined] std::panicking::try::h32056c8bc9923070 at panicking.rs:367 [opt]
    frame #39: 0x00000001007f680c rune`std::rt::lang_start_internal::h89c9a0e84eca73a6 [inlined] std::panic::catch_unwind::h5db5f082733be7e8 at panic.rs:129 [opt]
    frame #40: 0x00000001007f680c rune`std::rt::lang_start_internal::h89c9a0e84eca73a6 at rt.rs:45 [opt]
    frame #41: 0x0000000100008609 rune`main + 41
    frame #42: 0x00007fff6b928cc9 libdyld.dylib`start + 1
    frame #43: 0x00007fff6b928cc9 libdyld.dylib`start + 1
```

See also https://github.com/hotg-ai/rune/issues/131."
52298,ModifyGraphWithDelegate crashes when calling with XNNPack delegate,"Hello, 

I want to run TF an a arm64 device with XNNPack enabled. I am using the C++ API. When calling `ModifyGraphWithDelegate` it segfaults.

This is how I load the interpreter and delegate:

```
    model_ = (tflite::FlatBufferModel::BuildFromBuffer(buffer_.get(), bufSize));
    
    if (!model_) {
        std::cout << ""Could not load model."";
        return false;
    }
    
    tflite::ops::builtin::BuiltinOpResolver resolver;
    
    TfLiteStatus status = tflite::InterpreterBuilder(*model_, resolver)(&tfinterpreter_);

    TfLiteXNNPackDelegateOptions options = TfLiteXNNPackDelegateOptionsDefault();
    tflite::Interpreter::TfLiteDelegatePtr delegate(
      TfLiteXNNPackDelegateCreate(&options),
         [](TfLiteDelegate* delegate) { TfLiteXNNPackDelegateDelete(delegate); });

    
    if (tfinterpreter_->ModifyGraphWithDelegate(std:move(delegate)) != kTfLiteOk) {
        std::cout << ""XNNPACK Loading failed"";
        return false;
    }
```

`ModifyGraphWithDelegate` never returns. The crashlog does not tell me much:

```
10-08 11:46:31.049 17616 17616 F libc    : Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 17616 (server), pid 17616 (server)
10-08 11:46:31.106 17619 17619 E crash_dump64: unknown process state: t
10-08 11:46:31.120 17619 17619 I crash_dump64: obtaining output fd from tombstoned, type: kDebuggerdTombstone
10-08 11:46:31.122  1040  1040 I /system/bin/tombstoned: received crash request for pid 17616
10-08 11:46:31.124 17619 17619 I crash_dump64: performing dump of process 17616 (target tid = 17616)
10-08 11:46:31.126  2950  2950 D io_stats: !@   8,0 r 773089 29031960 w 2641492 116829328 d 176910 191244240 f 718246 1038251 iot 3371860 2240840 th 51200 51200 39664 pt 3530 inp 0 0 638962.480
10-08 11:46:31.136 17619 17619 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
10-08 11:46:31.136 17619 17619 F DEBUG   : Build fingerprint: 'samsung/a70qeea/a70q:10/QP1A.190711.020/A705FNXXU5BTF1:user/release-keys'
10-08 11:46:31.136 17619 17619 F DEBUG   : Revision: '14'
10-08 11:46:31.136 17619 17619 F DEBUG   : ABI: 'arm64'
10-08 11:46:31.138 17619 17619 F DEBUG   : Timestamp: 2021-10-08 11:46:31+0200
10-08 11:46:31.138 17619 17619 F DEBUG   : pid: 17616, tid: 17616, name: server  >>> /data/local/tmp/erver/server <<<
10-08 11:46:31.138 17619 17619 F DEBUG   : uid: 2000
10-08 11:46:31.138 17619 17619 F DEBUG   : signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0
10-08 11:46:31.139 17619 17619 F DEBUG   : Cause: null pointer dereference
10-08 11:46:31.139 17619 17619 F DEBUG   :     x0  0000000000000001  x1  0000007f1aa58ca8  x2  0000000000000002  x3  0000000000000300
10-08 11:46:31.139 17619 17619 F DEBUG   :     x4  000000000000000f  x5  0000007fc6afa0e8  x6  0000007fc6af9fc0  x7  0000000000000001
10-08 11:46:31.139 17619 17619 F DEBUG   :     x8  ffffffffffffffff  x9  0000000000000040  x10 0000007f1ab1e5f8  x11 0000007f1ab19990
10-08 11:46:31.139 17619 17619 F DEBUG   :     x12 0000007f19480a00  x13 0000000000000000  x14 0000000000000000  x15 0000007f9da1316c
10-08 11:46:31.139 17619 17619 F DEBUG   :     x16 0000000000000000  x17 0000007f1abbe8f8  x18 0000000000000080  x19 000000000000000f
10-08 11:46:31.139 17619 17619 F DEBUG   :     x20 0000000000000000  x21 0000000000000000  x22 0000007fc6af9fc0  x23 000000000000007e
10-08 11:46:31.139 17619 17619 F DEBUG   :     x24 0000000000000010  x25 0000007f1a54a238  x26 0000007fc6afa0e8  x27 0000007fa24de020
10-08 11:46:31.139 17619 17619 F DEBUG   :     x28 000000000000000f  x29 0000007fc6af9ee0
10-08 11:46:31.139 17619 17619 F DEBUG   :     sp  0000007fc6af9db0  lr  0000007f9d7499a0  pc  0000007f9d74da94
10-08 11:46:31.143 17619 17619 F DEBUG   : 
10-08 11:46:31.143 17619 17619 F DEBUG   : backtrace:
10-08 11:46:31.143 17619 17619 F DEBUG   :     NOTE: Function names and BuildId information is missing for some frames due
10-08 11:46:31.144 17619 17619 F DEBUG   :     NOTE: to unreadable libraries. For unwinds of apps, only shared libraries
10-08 11:46:31.144 17619 17619 F DEBUG   :     NOTE: found under the lib/ directory are readable.
10-08 11:46:31.144 17619 17619 F DEBUG   :       #00 pc 000000000149fa94  /data/local/tmp/al_server/libserver.so
```


I compiled TF 2.4.3 with cmake:

```
cmake -DCMAKE_TOOLCHAIN_FILE=~/android-ndk-r19c/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a -DTFLITE_ENABLE_XNNPACK=ON -DCMAKE_ANDROID_ARM_NEON=ON -DANDROID_ARM_NEON=ON ../tensorflow/lite
```


Any help would be much appreciated,
Thanks"
52297,TypeError: unhashable type: 'DictWrapper'   【self.model.add_metric】,"I add  my custom metrics  
the code are as follows:
        def get_accu1(args):
            y_true, y_pred, mask = args
            y_true = K.cast(K.round(y_true), 'int32')
            y_pred = K.cast(K.round(y_pred), 'int32')
            accu = K.cast(K.equal(y_true, y_pred), 'float32')  # 预测位置正确
            return K.sum(accu * mask) / K.sum(mask)
        accu1 = Lambda(get_accu1)([y_true, seq, loss_mask])
        self.model.add_metric(value=accu1,name='accu1')

but it is wrong under TF2.3.1, but has no problem under TF1.15.1. BUT I have to use the TF2.3.1
![image](https://user-images.githubusercontent.com/74778093/136537299-a04acf00-7377-4a76-9d73-1903ee06301a.png)
could you like to give me some  suggestions?"
52296,loss function bombs,"def my_loss(y_true, y_pred):
    epsilon=1e-7
    tloss=0.0
    for i in range(len(pos_weights)):
# for each class, add average weighted loss for that class 
            tloss +=-(pos_weights[i]*y_true[i]*K.log(y_pred[i]+epsilon)+neg_weights[i]*(1-y_true[i])*K.log(1-(y_pred[i])+epsilon))
    return K.mean(tloss)
opt = keras.optimizers.Adam(learning_rate=0.0005)
model.compile(optimizer=opt,
              loss=my_loss,
              metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.CategoricalAccuracy(),
                       tf.keras.metrics.FalseNegatives(),
                      tf.keras.metrics.FalsePositives()])
model.summary() ( model summary outtput received)
history = model.fit(train_generator, steps_per_epoch=10,
	validation_data=valid_generator, validation_steps=10, epochs=20, verbose=2)
Epoch 1/20
WARNING:tensorflow:AutoGraph could not transform <function my_loss at 0x000001D9006A63A0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function my_loss at 0x000001D9006A63A0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-14-5bd50e19a974> in <module>
----> 1 history = model.fit(train_generator, steps_per_epoch=10,
      2 	validation_data=valid_generator, validation_steps=10, epochs=20, verbose=2)

~\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

~\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-> 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

~\anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

~\anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--> 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

~\anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py in _initialize(self, args, kwds, add_initializers_to)
    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    695     self._concrete_stateful_fn = (
--> 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    697             *args, **kwds))
    698 

~\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

~\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

~\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3063     arg_names = base_arg_names + missing_arg_names
   3064     graph_function = ConcreteFunction(
-> 3065         func_graph_module.func_graph_from_py_func(
   3066             self._name,
   3067             self._python_function,

~\anaconda3\lib\site-packages\tensorflow\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~\anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

~\anaconda3\lib\site-packages\tensorflow\python\framework\func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, ""ag_error_metadata""):
--> 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

TypeError: in user code:

    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:806 train_function  *
        return step_function(self, iterator)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:796 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:1211 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:2585 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:2945 _call_for_each_replica
        return fn(*args, **kwargs)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:789 run_step  **
        outputs = model.train_step(data)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:748 train_step
        loss = self.compiled_loss(
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\keras\engine\compile_utils.py:204 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\keras\losses.py:149 __call__
        losses = ag_call(y_true, y_pred)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\keras\losses.py:253 call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    <ipython-input-11-595b5a4de413>:6 my_loss  **
        tloss +=-(pos_weights[i]*y_true[i]*K.log(y_pred[i]+epsilon)+neg_weights[i]*(1-y_true[i])*K.log(1-(y_pred[i])+epsilon))
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:1141 binary_op_wrapper
        raise e
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:1125 binary_op_wrapper
        return func(x, y, name=name)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:1457 _mul_dispatch
        return multiply(x, y, name=name)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py:201 wrapper
        return target(*args, **kwargs)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:509 multiply
        return gen_math_ops.mul(x, y, name)
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\ops\gen_math_ops.py:6174 mul
        _, _, _op, _outputs = _op_def_library._apply_op_helper(
    C:\Users\owner\anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:503 _apply_op_helper
        raise TypeError(

    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'."
52294,Using the same seed kwarg returns different values between GlorotUniform and GlorotUniformV2,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Pip (binary)
- TensorFlow version (use command below): v2.6.0-0-g919f693420e 2.6.0
- Python version: 3.7.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Setting the `operation` seed returns different tensors when using GlorotUniform when imported from `tf.compat.v1` and `tensorflow.keras.initializers`

**Describe the expected behavior**
Both tensors must be equal

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
[Colab link](https://colab.research.google.com/drive/13V8v7a7fCQQMroaLz1PGvgrQ5Wf81IRp?usp=sharing)

In the notebook, I share reproducible code about the current behavior and the expected behavior, as well as *why* this might be happening. 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

"
52293,Cannot create interpreter:  Op builtin_code out of range: 142. Are you using old TFLite binary with newer model?,"Hi TensorFlow, I am trying to create a tflite that is trainable on device using your article 
https://www.tensorflow.org/lite/examples/on_device_training/overview

 have been facing this issue  

**Cannot create interpreter:  Op builtin_code out of range: 142. Are you using old TFLite binary with newer model?**

While trying to create an interpreter for my tflite in Java
 Interpreter interpreter = new Interpreter(loadModelFile(MainActivity.this))

to hopefully run the function interpreter.runSignature(inputs, outputs, ""train"");

Exactly Similar to the issue found in https://github.com/tensorflow/tensorflow/issues/51849 

... However I am using java and the remedy of removing jcenter() doesn't seem to be fixing my problem.
Here are the dependencies I am using for android

implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'

mavenCentral()
maven { // Only for snapshot artifacts
 name 'ossrh-snapshot'
 url 'https://oss.sonatype.org/content/repositories/snapshots'
}

If possible can you please send me the working code for android implementation of on Device Training, as discussed in this article 
https://www.tensorflow.org/lite/examples/on_device_training/overview 
or please help me resolve this issue."
52290,"Issue with conversion of dilated Convolutions  #29509 reappears in versions 2.5.0, 2.5.1 and 2.6.0","### 1. System information

- OS Platform and Distribution Ubuntu 20.04 
- TensorFlow installation (pip package or built from source): pip + official docker images
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0, 2.5.1, 2.6.0

### 2. Code 

Minimal model to reproduce issue:

```python
import pathlib

import tensorflow as tf
from tensorflow import keras

input = keras.Input([128, 128, 3])
x = keras.layers.Conv2D(8, 5, dilation_rate=2, padding=""same"", use_bias=False)(input)
x = keras.layers.BatchNormalization()(x)
output = keras.layers.ReLU()(x)

m = keras.Model(inputs=input, outputs=output)



def representative_data_gen():
  for input_value in range(10):
    # Model has only one input so each data point has one element.
    yield [tf.random.uniform((1,128,128,3), dtype=tf.float32)]

converter = tf.lite.TFLiteConverter.from_keras_model(m)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen

tflite_model = converter.convert()
output_file = ""./test.tflite""
with open(output_file, 'wb') as f:
  f.write(tflite_model)

print(f""Converted model was written to {output_file}"")
```



### 3. Failure after conversion

- Model produces correct results, but it is slower than expected.

Mapping to EdgeTPU fails. 
This is the same as #29509. Issue was solved but appears again on newer releases.


```bash
Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 2
Number of operations that will run on CPU: 5

Operator                       Count      Status

DEQUANTIZE                     1          Operation is working on an unsupported data type
SPACE_TO_BATCH_ND              1          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
MUL                            1          Mapped to Edge TPU
QUANTIZE                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation
CONV_2D                        1          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
BATCH_TO_SPACE_ND              1          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
ADD                            1          Mapped to Edge TPU
Compilation child process completed within timeout period.
Compilation succeeded! 
(base) waterview@waterview-w

```

With TF 2.4.3 used to be:

```bash
Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 1
Number of operations that will run on CPU: 2

Operator                       Count      Status

DEQUANTIZE                     1          Operation is working on an unsupported data type
CONV_2D                        1          Mapped to Edge TPU
QUANTIZE                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation
Compilation child process completed within timeout period.
Compilation succeeded! 

```


"
52289,Latest TensorFlow fails to install due to yesterday's Keras requirement change ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: latest as of https://github.com/tensorflow/tensorflow/commit/ca0fa154128d793df5e6809cb86336ac1fed3466 
- Python version: python3.8
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source):  3.7.2
- GCC/Compiler version (if compiling from source): GCC9.3
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**

**Build was successful, but installation of the wheel failed with the following Error message
09:34:20  ERROR: No matching distribution found for keras<2.8,>=2.7 (from tensorflow==2.8.0)**


**Provide the exact sequence of commands / steps that you executed before running into the problem**

pip install /tmp/pip3/tensorflow-2.8.0-cp38-cp38-linux_x86_64.whl

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

You can take a detailed look here: https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/941/console
"
52288,save_model() still fails with custom layer and SavedModel format,"Basically same issue that was closed before:
https://github.com/tensorflow/tensorflow/issues/40912

With TF 2.6 I'm experiencing exactly the same problem.
Also with tf-nightly the issue is still there."
52287,[TF-TRT] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson AGX Xavier Jetpack Version 4.5.1 ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): nvidia container install
- TensorFlow version (use command below): 7.1.3
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.2 cuDNN 8.0 
- GPU model and memory: Jetson AGX Xavier, 16GB

<br>

**Describe the current behavior**

Hi, while using TF-TRT, always I faced on an error `2021-10-04 00:54:11.478005: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.` when I called `converter.build(input_fn=my_input_fn)` method. (either the error raised at runtime when **not** call build() method for pre-build engine.)

(image : trials, parameter tuning `minimum_segment_size` and so on.)
![image](https://user-images.githubusercontent.com/46595649/136313733-2ab4dea4-7847-494e-a466-20bfc7701b1e.png)

But any of them couldn't remove that error message.

My entire source code wasn't different with any other example source codes.

```python3
# memory growing mode (already tried fixed gpu memory mode either.)
gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)

#https://github.com/tensorflow/tensorrt/issues/195
#https://github.com/tensorflow/tensorrt/issues/200
env = os.environ.get('TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE')
new_env = os.environ['TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE'] = ""1""
print(f'Setting environment variable... TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE={new_env}... (before {env})')

# Conversion Parameters 
conversion_params = trt.TrtConversionParams(
    precision_mode=QUANTIZATION_MODE, # 'INT8' or 'FP16'
    max_workspace_size_bytes=MAX_GPU_TRTENGINE_SIZE_MB<<20, # 2048 ... etc.
    minimum_segment_size=MINIMUM_SEGMENT_SIZE, # 10 ... etc.
    maximum_cached_engines=MAXIMUM_CACHED_ENGINE, # 20 ... etc.
    use_calibration=True # This argument is ignored if precision_mode is not INT8.
    )

converter = trt.TrtGraphConverterV2(
    input_saved_model_dir=SAVED_MODEL_DIR,
    input_saved_model_tags=SAVED_MODEL_TAG_SET,
    input_saved_model_signature_key=SAVED_MODEL_SIGNATURE_DEF,
    conversion_params=conversion_params
    )

if QUANTIZATION_MODE.lower() == 'int8':
    print('Convert with calibration')
    converter.convert(
        calibration_input_fn=lambda:input_fn(args) # args : my command line input
        # input_fn returns numpy array [1, 320, 320, 3] shaped.
        ) # Maybe converter.convert() works fine...?
else:
    print('Convert without calibration')
    converter.convert()

print('[Info] Convert complete.')
print('[Info] Now prebuild trt-engine.')
converter.build(input_fn=lambda:input_fn(args)) # here raises error message, but process doesn't shut down.

optimized_saved_model_fulldir = parse_tftrt_path(args)

# Save the model to the disk
converter.save(optimized_saved_model_fulldir)
print('[Info] writing savedmodel complete.')
```

I found that optimized model does not improving model inference speed, so I guess the error is not just a warning. Maybe It's a bug.

Thanks!

<br>

**Describe the expected behavior**

- no warning
- inference speed improvement

<br>

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

COLAB's TF-TRT is unstable.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


model signature
```
==============================
2021-10-07 12:58:34.474165: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2
The given SavedModel contains the following tag-sets:
'serve'

2021-10-07 12:59:08.535907: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2
The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:
SignatureDef key: ""__saved_model_init_op""
SignatureDef key: ""serving_default""

2021-10-07 12:59:41.622920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2
The given SavedModel SignatureDef contains the following input(s):
  inputs['input_1'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, -1, -1, 3)
      name: serving_default_input_1:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['softmax'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, -1, -1, 9)
      name: StatefulPartitionedCall:0
Method name is: tensorflow/serving/predict
==============================
```

Entire program log description below
```
2021-10-07 12:58:32.487612: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-10-07 12:58:32.513042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 12:58:32.513711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: 
pciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2
coreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s
2021-10-07 12:58:32.514587: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2
2021-10-07 12:58:32.515218: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.10
2021-10-07 12:58:32.515752: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.10
2021-10-07 12:58:32.516480: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2021-10-07 12:58:32.522466: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2021-10-07 12:58:32.535133: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10
2021-10-07 12:58:32.543363: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.10
2021-10-07 12:58:32.546243: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-10-07 12:58:32.546781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 12:58:32.547194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 12:58:32.547471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1872] Adding visible gpu devices: 0

Setting environment variable... TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE=1... (before None)
2021-10-07 13:00:15.205272: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libnvinfer.so.7
Convert with calibration
2021-10-07 13:01:31.023349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:01:31.023722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: 
pciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2
coreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s
2021-10-07 13:01:31.024083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:01:31.024338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:01:31.024470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1872] Adding visible gpu devices: 0
2021-10-07 13:01:31.025038: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2
2021-10-07 13:01:36.768804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-10-07 13:01:36.768996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-10-07 13:01:36.769069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-10-07 13:01:36.769398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:01:36.769747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:01:36.769976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:01:36.770180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 18830 MB memory) -> physical GPU (device: 0, name: Xavier, pci bus id: 0000:00:00.0, compute capability: 7.2)
2021-10-07 13:06:42.865817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:06:42.866173: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2021-10-07 13:06:42.866754: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2021-10-07 13:06:42.870918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:06:42.871261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: 
pciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2
coreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s
2021-10-07 13:06:42.871593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:06:42.871875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:06:42.872000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1872] Adding visible gpu devices: 0
2021-10-07 13:06:42.872134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-10-07 13:06:42.872198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-10-07 13:06:42.872245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-10-07 13:06:42.872483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:06:42.872836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:06:42.873158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 18830 MB memory) -> physical GPU (device: 0, name: Xavier, pci bus id: 0000:00:00.0, compute capability: 7.2)
2021-10-07 13:06:42.874915: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 31250000 Hz
2021-10-07 13:06:47.236124: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1171] Optimization results for grappler item: graph_to_optimize
  function_optimizer: Graph size after: 1216 nodes (901), 1824 edges (1509), time = 176.228ms.
  function_optimizer: function_optimizer did nothing. time = 2.022ms.

2021-10-07 13:07:39.201871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:07:39.202299: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2021-10-07 13:07:39.202699: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2021-10-07 13:07:39.210304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:07:39.210650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: 
pciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2
coreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s
2021-10-07 13:07:39.211093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:07:39.211425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:07:39.211551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1872] Adding visible gpu devices: 0
2021-10-07 13:07:39.211696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-10-07 13:07:39.211766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-10-07 13:07:39.211813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-10-07 13:07:39.212127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:07:39.212488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-10-07 13:07:39.212722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 18830 MB memory) -> physical GPU (device: 0, name: Xavier, pci bus id: 0000:00:00.0, compute capability: 7.2)
2021-10-07 13:07:45.495807: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:790] There are 30 ops of 8 different types in the graph that are not converted to TensorRT: Identity, ResizeNearestNeighbor, Placeholder, NoOp, Mul, Shape, DataFormatVecPermute, StridedSlice, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).
2021-10-07 13:07:46.085000: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:759] Number of TensorRT candidate segments: 6
2021-10-07 13:07:46.227369: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 0 consisting of 424 nodes by TRTEngineOp_0_0.
2021-10-07 13:07:46.232840: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 1 consisting of 22 nodes by StatefulPartitionedCall/model/TRTEngineOp_0_1.
2021-10-07 13:07:46.234022: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 2 consisting of 22 nodes by StatefulPartitionedCall/model/TRTEngineOp_0_2.
2021-10-07 13:07:46.234935: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 3 consisting of 22 nodes by StatefulPartitionedCall/model/TRTEngineOp_0_3.
2021-10-07 13:07:46.236717: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 4 consisting of 22 nodes by StatefulPartitionedCall/model/TRTEngineOp_0_4.
2021-10-07 13:07:46.237427: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 5 consisting of 27 nodes by TRTEngineOp_0_5.
2021-10-07 13:07:47.073017: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1171] Optimization results for grappler item: tf_graph
  constant_folding: Graph size after: 558 nodes (-624), 1200 edges (-624), time = 554.18ms.
  layout: Graph size after: 595 nodes (37), 1237 edges (37), time = 328.333ms.
  constant_folding: Graph size after: 587 nodes (-8), 1229 edges (-8), time = 230.304ms.
  TensorRTOptimizer: Graph size after: 54 nodes (-533), 62 edges (-1167), time = 975.276ms.
  constant_folding: Graph size after: 54 nodes (0), 62 edges (0), time = 21.947ms.
Optimization results for grappler item: StatefulPartitionedCall/model/TRTEngineOp_0_1_native_segment
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 26.112ms.
  layout: Graph size after: 26 nodes (0), 25 edges (0), time = 42.376ms.
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 31.746ms.
  TensorRTOptimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 5.574ms.
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 22.56ms.
Optimization results for grappler item: StatefulPartitionedCall/model/TRTEngineOp_0_2_native_segment
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 6.176ms.
  layout: Graph size after: 26 nodes (0), 25 edges (0), time = 9.286ms.
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 8.636ms.
  TensorRTOptimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.877ms.
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 6.713ms.
Optimization results for grappler item: StatefulPartitionedCall/model/TRTEngineOp_0_4_native_segment
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.232ms.
  layout: Graph size after: 26 nodes (0), 25 edges (0), time = 4.541ms.
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.265ms.
  TensorRTOptimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.405ms.
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.41ms.
Optimization results for grappler item: TRTEngineOp_0_5_native_segment
  constant_folding: Graph size after: 29 nodes (0), 40 edges (0), time = 4.414ms.
  layout: Graph size after: 29 nodes (0), 40 edges (0), time = 4.662ms.
  constant_folding: Graph size after: 29 nodes (0), 40 edges (0), time = 4.751ms.
  TensorRTOptimizer: Graph size after: 29 nodes (0), 40 edges (0), time = 0.367ms.
  constant_folding: Graph size after: 29 nodes (0), 40 edges (0), time = 4.544ms.
Optimization results for grappler item: StatefulPartitionedCall/model/TRTEngineOp_0_3_native_segment
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.687ms.
  layout: Graph size after: 26 nodes (0), 25 edges (0), time = 7.218ms.
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.93ms.
  TensorRTOptimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.535ms.
  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.577ms.
Optimization results for grappler item: TRTEngineOp_0_0_native_segment
  constant_folding: Graph size after: 431 nodes (0), 440 edges (0), time = 57.452ms.
  layout: Graph size after: 431 nodes (0), 440 edges (0), time = 88.399ms.
  constant_folding: Graph size after: 431 nodes (0), 440 edges (0), time = 70.943ms.
  TensorRTOptimizer: Graph size after: 431 nodes (0), 440 edges (0), time = 8.772ms.
  constant_folding: Graph size after: 431 nodes (0), 440 edges (0), time = 66.01ms.

Found 30 files belonging to 1 classes.
2021-10-07 13:40:34.221298: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
step 1/30, image shape : (1, 320, 320, 3)
2021-10-07 13:40:35.773429: I tensorflow/compiler/tf2tensorrt/common/utils.cc:58] Linked TensorRT version: 7.1.3
2021-10-07 13:40:35.773914: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libnvinfer.so.7
2021-10-07 13:40:35.773981: I tensorflow/compiler/tf2tensorrt/common/utils.cc:60] Loaded TensorRT version: 7.1.3
2021-10-07 13:40:35.785002: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libnvinfer_plugin.so.7
2021-10-07 13:41:56.956338: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-10-07 13:41:56.964140: I tensorflow/stream_executor/cuda/cuda_dnn.cc:380] Loaded cuDNN version 8000
2021-10-07 13:42:03.715476: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.10
step 2/30, image shape : (1, 320, 320, 3)
step 3/30, image shape : (1, 320, 320, 3)
step 4/30, image shape : (1, 320, 320, 3)
step 5/30, image shape : (1, 320, 320, 3)
step 6/30, image shape : (1, 320, 320, 3)
step 7/30, image shape : (1, 320, 320, 3)
step 8/30, image shape : (1, 320, 320, 3)
step 9/30, image shape : (1, 320, 320, 3)
step 10/30, image shape : (1, 320, 320, 3)
step 11/30, image shape : (1, 320, 320, 3)
step 12/30, image shape : (1, 320, 320, 3)
step 13/30, image shape : (1, 320, 320, 3)
step 14/30, image shape : (1, 320, 320, 3)
step 15/30, image shape : (1, 320, 320, 3)
step 16/30, image shape : (1, 320, 320, 3)
step 17/30, image shape : (1, 320, 320, 3)
step 18/30, image shape : (1, 320, 320, 3)
step 19/30, image shape : (1, 320, 320, 3)
step 20/30, image shape : (1, 320, 320, 3)
step 21/30, image shape : (1, 320, 320, 3)
step 22/30, image shape : (1, 320, 320, 3)
step 23/30, image shape : (1, 320, 320, 3)
step 24/30, image shape : (1, 320, 320, 3)
step 25/30, image shape : (1, 320, 320, 3)
step 26/30, image shape : (1, 320, 320, 3)
step 27/30, image shape : (1, 320, 320, 3)
step 28/30, image shape : (1, 320, 320, 3)
step 29/30, image shape : (1, 320, 320, 3)
step 30/30, image shape : (1, 320, 320, 3)
[Info] Convert complete.
[Info] Now prebuild trt-engine.
Found 30 files belonging to 1 classes.
step 1/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.501273: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.518895: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.520264: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.521836: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.523049: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.525338: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 2/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.553969: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.555540: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.556665: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.558038: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.559088: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.559983: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 3/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.594004: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.597923: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.599552: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.601033: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.602305: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.604312: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 4/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.634213: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.638674: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.640178: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.642291: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.643825: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.649495: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 5/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.668430: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.671339: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.672554: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.678498: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.679625: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.681399: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 6/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.699913: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.703150: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.705529: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.708076: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.711813: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.713090: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 7/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.748968: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.750285: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.751345: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.752453: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.753522: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.754501: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 8/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.773218: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.774877: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.776059: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.777162: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.778034: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.779149: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 9/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.794973: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.796168: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.797143: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.797970: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.798687: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.799433: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 10/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.848015: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.849429: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.850419: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.851397: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.852383: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.853300: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 11/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.890036: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.892058: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.905496: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.906838: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.907802: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.908979: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 12/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.931153: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.933730: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.935208: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.936988: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.938206: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.945270: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 13/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:54.996327: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.997990: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:54.999318: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.000395: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.001443: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.002548: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 14/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.026953: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.028467: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.029830: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.030897: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.033084: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.034140: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 15/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.049957: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.051390: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.052484: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.053757: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.054732: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.055674: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 16/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.072601: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.075016: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.077017: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.078947: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.080625: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.081977: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 17/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.122032: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.123231: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.124173: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.125291: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.126088: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.126915: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 18/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.142622: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.144997: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.146100: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.147124: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.147956: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.148818: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 19/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.163239: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.165280: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.167056: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.170588: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.173496: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.174742: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 20/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.191675: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.193147: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.195249: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.196687: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.197986: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.198907: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 21/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.223086: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.224427: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.225536: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.226596: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.227501: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.228680: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 22/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.243688: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.245667: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.247469: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.253290: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.255746: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.258155: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 23/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.272156: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.273568: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.274988: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.276136: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.277656: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.278708: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 24/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.299802: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.301459: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.303199: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.304482: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.305795: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.306835: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 25/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.322027: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.324808: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.326183: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.327298: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.328234: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.329430: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 26/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.347076: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.348356: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.351511: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.354776: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.357402: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.358476: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 27/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.378031: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.383370: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.386487: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.387798: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.393018: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.395249: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 28/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.426818: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.431675: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.434095: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.436594: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.437868: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.438853: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 29/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.454118: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.455327: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.456274: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.460399: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.461846: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.462862: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
step 30/30, image shape : (1, 320, 320, 3)
2021-10-07 14:21:55.475968: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.477193: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.478811: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.479840: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.480687: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
2021-10-07 14:21:55.481721: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.
[Info] writing savedmodel complete.
```"
52286,Not able to merge results,"I am working on a project in which I am using different models of machine learning.
![1](https://user-images.githubusercontent.com/26819449/136387688-78e5b8b2-39c9-4acb-bd0e-b643a88a24d3.JPG)
I am able to generate results for a particular model. But not able to generate loss for all models in the same graph so I could compare well.
I am doing a project in google collab and different models are running in different files of colab.
Please can you tell me a way how could I solve this or some alternative?
Thanks! 
"
52285,Duplicate PID in multiple GPU,"Tensorflow creates a duplicate PID on all the GPUs. I am running on a system with 2 GPUs and everytime I execute a training, it allocates memory from both the GPU's with the same PID, although one GPU is used to the computations


![image](https://user-images.githubusercontent.com/50949120/136385342-31389d3d-ec38-42ad-816f-1705dd140ab5.png)
"
52284,Segmentation fault (core dumped) while getting the node attributes from the TF Graphdef in C++,"Dear Tensorflow Team,

I am trying to load the Tensorflow model(frozen graph) and get the attributes of each Node in C++.  Having said that, I am able to get the deserialized object from the model correctly using the below code:

```
    FILE* fd = fopen(graphFile, ""rb"");
    google::protobuf::io::FileInputStream inStream(fileno(fd));
    google::protobuf::io::CodedInputStream coded_stream(&inStream);
    coded_stream.SetTotalBytesLimit(1024LL << 20, 512LL << 20);
    bool success = graphDef.ParseFromCodedStream(&coded_stream);

```
And I am able to print the Graphdef content and all seems to be right. I am able to get the correct content from the graphDef using the below code.

```
    for (int i = 0; i < graphDef.node_size(); i++)
    {
            graphDef.node(i).PrintDebugString();

    }

```
but while I am trying to get the node attributes from the graphDef, I am getting the Segmentation fault (core dumped).

```
	const tensorflow::NodeDef node = graphDef.node(2);
        auto attr = node.attr(); //getting the error here

```

I am attaching trace below:

```
0x00007ffff5f372ea in google::protobuf::hash<char const*>::operator() (this=0x7fffffffcea7, str=0x5de58948f7894855 <error: Cannot access memory at address 0x5de58948f7894855>)
    at /home/darshan/project/sw/external/protobuf/src/google/protobuf/stubs/hash.h:66
66	    for (; *str != '\0'; str++) {
(gdb) bt
#0  0x00007ffff5f372ea in google::protobuf::hash<char const*>::operator() (this=0x7fffffffcea7, str=0x5de58948f7894855 <error: Cannot access memory at address 0x5de58948f7894855>)
    at /home/darshan/project/sw/external/protobuf/src/google/protobuf/src/google/protobuf/stubs/hash.h:66
#1  0x00007ffff5f3735b in google::protobuf::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::operator() (this=0x5555557c5450, 
    key=<error: Cannot access memory at address 0x5de58948f7894855>) at /home/darshan/project/sw/external/protobuf/src/google/protobuf/stubs/hash.h:83
#2  0x00007ffff5f44e49 in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::InnerMap::BucketNumber (this=0x5555557c5450, 
    k=<error: Cannot access memory at address 0x5de58948f7894855>) at /home/darshan/project/sw/external/protobuf/src/google/protobuf/map.h:888
#3  0x00007ffff5f4321e in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::InnerMap::FindHelper (this=0x5555557c5450, 
    k=<error: Cannot access memory at address 0x5de58948f7894855>, it=0x0) at /home/darshan/project/sw/external/protobuf/src/google/protobuf/map.h:647
#4  0x00007ffff5f40376 in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::InnerMap::FindHelper (this=0x5555557c5450, 
    k=<error: Cannot access memory at address 0x5de58948f7894855>) at/home/darshan/project/sw/external/protobuf/src/google/protobuf/map.h:643
#5  0x00007ffff5f3c782 in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::InnerMap::find (this=0x5555557c5450, 
    k=<error: Cannot access memory at address 0x5de58948f7894855>) at /home/darshan/project/sw/external/protobuf/src/google/protobuf/map.h:558
#6  0x00007ffff5f3c9d0 in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::find (this=0x7fffffffd210, 
    key=<error: Cannot access memory at address 0x5de58948f7894855>) at /home/darshan/project/sw/external/protobuf/src/google/protobuf/map.h:1078
#7  0x00007ffff5f3a623 in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::insert<google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::const_iterator> (this=0x7fffffffd210, first=..., last=...)
    at /home/darshan/project/sw/external/protobuf/src/google/protobuf/src/google/protobuf/map.h:1112
#8  0x00007ffff5f38a5a in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::Map (this=0x7fffffffd210, other=...)
    at /home/darshan/project/sw/external/protobuf/src/google/protobuf/src/google/protobuf/map.h:149
#9  0x00007ffff5e1c7c9 in parse ()
    at tensorflow/TFParser.cpp:824
#10 0x0000555555563632 in parseTFNetwork() ()
#11 0x0000555555563e34 in parseAndCompile() ()
#12 0x000055555555e2cb in launchTest() ()
#13 0x000055555555f44d in main ()

```
I am not sure what exactly is the issue here. Whether it is a deserialization issue, or the way of accessing is wrong, or any other?

Can you please help me to resolve the issue? It would be a great help from your side.


**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: Source
- TensorFlow version : 2.3
- Python version: 3.6
- Bazel version : 3.1.0
- Protobuf version: 3.9.2


Best Regards,
Darshan C G"
52283,Build r2.2.0 fails when build ppc64le machine,"when i Compiling tensorflow-gpu from source,I do not know how to solve the following error

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:v2.2.0
- Python version:3.6
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):2.0.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:CUDA 10.1/cuDNN 7
- GPU model and memory:k80

./configure
You have bazel 2.0.0- (@non-git) installed.
Please specify the location of python. [Default is /home/owner/anaconda3/envs/tensorflow/bin/python]: /home/owner/anaconda3/envs/tensorflow/bin/python3.6


Found possible Python library paths:
  /home/owner/anaconda3/envs/tensorflow/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/owner/anaconda3/envs/tensorflow/lib/python3.6/site-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include
Found cuDNN 7 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -mcpu=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished


when build blow error is happen and abend build...

ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 1210
		_create_local_cuda_repository(<1 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 934, in _create_local_cuda_repository
		_find_libs(repository_ctx, <2 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 577, in _find_libs
		_check_cuda_libs(repository_ctx, <2 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 479, in _check_cuda_libs
		execute(repository_ctx, <1 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/remote_config/common.bzl"", line 208, in execute
		fail(<1 more arguments>)
Repository command failed
Traceback (most recent call last):
  File ""script.py"", line 88, in <module>
    main()
  File ""script.py"", line 77, in main
    check_cuda_lib(path, check_soname=args[i + 1] == ""True"")
  File ""script.py"", line 62, in check_cuda_lib
    output = subprocess.check_output([objdump, ""-p"", path]).decode(""ascii"")
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 40: ordinal not in range(128)
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 1210
		_create_local_cuda_repository(<1 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 934, in _create_local_cuda_repository
		_find_libs(repository_ctx, <2 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 577, in _find_libs
		_check_cuda_libs(repository_ctx, <2 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 479, in _check_cuda_libs
		execute(repository_ctx, <1 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/remote_config/common.bzl"", line 208, in execute
		fail(<1 more arguments>)
Repository command failed
Traceback (most recent call last):
  File ""script.py"", line 88, in <module>
    main()
  File ""script.py"", line 77, in main
    check_cuda_lib(path, check_soname=args[i + 1] == ""True"")
  File ""script.py"", line 62, in check_cuda_lib
    output = subprocess.check_output([objdump, ""-p"", path]).decode(""ascii"")
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 40: ordinal not in range(128)
WARNING: Target pattern parsing failed.
ERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 1210
		_create_local_cuda_repository(<1 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 934, in _create_local_cuda_repository
		_find_libs(repository_ctx, <2 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 577, in _find_libs
		_check_cuda_libs(repository_ctx, <2 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl"", line 479, in _check_cuda_libs
		execute(repository_ctx, <1 more arguments>)
	File ""/home/owner/Downloads/tensorflow-2.2.0/third_party/remote_config/common.bzl"", line 208, in execute
		fail(<1 more arguments>)
Repository command failed
Traceback (most recent call last):
  File ""script.py"", line 88, in <module>
    main()
  File ""script.py"", line 77, in main
    check_cuda_lib(path, check_soname=args[i + 1] == ""True"")
  File ""script.py"", line 62, in check_cuda_lib
    output = subprocess.check_output([objdump, ""-p"", path]).decode(""ascii"")
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 40: ordinal not in range(128)
INFO: Elapsed time: 0.404s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
"
52282,SyntaxError: invalid syntax && ImportError: cannot import name pywrap_tensorflow,"**System information**
- Linux Ubuntu 16.04
- TensorFlow version: tensorflow-gpu 1.5
- Python version: 2.7
- Installed using virtualenv: pip
- CUDA/cuDNN version: CUDA9.0 & CUDNN7.0

**Describe the problem**
    Using anaconda to create a virtual environment named tf.
```
(tf) ➜  ~ python                         
Python 2.7.18 |Anaconda, Inc.| (default, Jun  4 2021, 14:47:46) 
[GCC 7.3.0] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/google/protobuf/descriptor.py"", line 113
    class DescriptorBase(metaclass=DescriptorMetaclass):
                                  ^
SyntaxError: invalid syntax
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
ImportError: cannot import name pywrap_tensorflow
```"
52281,RuntimeError: Encountered unresolved custom op: ReorderAxes. See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 284 (ReorderAxes) failed to prepare.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): Colab 
- TensorFlow version (or github SHA if from source): convert model in TF1.x / interpreter in  latest tf-nightly.

I convert a model to tflite using following codes:

    co = tf.compat.v1.lite.TFLiteConverter.from_saved_model(modelName)
    co.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
    co.allow_custom_ops=True

There are no error in converting.But when I try to interpreter the tfliter model, I encoder error:
--> 916     self._interpreter.Invoke()
    917 
    918   def reset_all_variables(self):

**RuntimeError: Encountered unresolved custom op: ReorderAxes.**
See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 284 (ReorderAxes) failed to prepare.
Could you please tell me how to solve this?
"
52280,"Add Metadata Text Fields Before Model Trained, Enabling Clear Line of IP Ownership.","Feature Request: Add Metadata Text Fields to TF API, Just Before Training Model, Enabling Clear Line of IP Ownership.

TF API could contain metadata text fields: 
1) Model Name
2) Model Version
3) Author Name(s)
4) Author email Contact
5) Content Source(s) Used to Train Model
6) Source Content Copyright Author(s), Owner(s), Holder(s), Copyright Registration Numbers from copyright.gov
7) 256 Character Free Form Text Field
8) QR code

BENEFITS
1) Clear Line of IP ownership, authorship. Independent inventor(s), engineer(s) and corporations need to easily demonstrate clear line of IP ownership from original dataset copyrights to machine learning model. 

2) API example tf.model.metadata, tf.model.info, tf.model.qrcode. Specific metadata: Exact model name, model version, creator name, source data used, copyright registration numbers, patent registration numbers of source data and potentially a 256 character free form field. Enables consistent, legitimate line of IP ownership and registrations if model used in a larger project, larger software product or license.

3) Training to create model only occurs if metadata filled out before model trained. If metadata not filled out meaningfully or correctly by developer, machine learning model users will know and can choose to use model or not. Adds to Author, content source(s) credibiliity. Also enables model users to know exactly or at least more closely what content was used to train model.

4) Metadata queries with one or more: command line, API, mobile app, TF utility or operating system file properties to easily view Properties/metadata of model. Once model trained, metadata needs to be read only, embedded in model as to not interfere with model and not be altered in model.

5) IP more defensible in contracts, disputes, sale/purchase of IP, partnering, gaining potential investment in software projects, valuations.

DRAWBACKS
1) Will add miniscule amount of text to model somewhere- at end of model? 
2) Formal specification needed, project owner, developer time.
3) Need to ensure data cannot be erased, altered or truncated once model is trained, created.
"
52279,support TensorRT 8.2,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.7.0rc0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
TensorRT Optimization of tensorflow model,  currently TensorRT is not supported
**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Anyone who use TensorRT

**Any Other info.**
"
52278,W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found 2021-10-06 23:46:08.198654: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
52277,`tf.random.shuffle` is deterministic when XLA is used,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary (I assume that's what `pip install` does, at least)
- TensorFlow version (use command below): `v2.6.0-rc2-32-g919f693420e 2.6.0`
- Python version: Python 3.6.9
- CUDA/cuDNN version: N/A, program runs on CPU
- GPU model and memory: N/A, program runs on CPU

**Describe the current behavior**

If I call `tf.random.shuffle` on a 1D tensor of float-32 values of size 10 (for example) inside a JIT-compiled `tf.function`, the shuffled tensor is the same every time I run the program.

**Describe the expected behavior**

The shuffled tensor should usually be different between runs.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

@tf.function(jit_compile=True)
def shuffle_me(some_tensor):
    output = tf.random.shuffle(some_tensor)
    return output

input_ = [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]
input_ = tf.constant(input_, dtype=tf.float32)

shuffled = shuffle_me(input_)
print(shuffled)
```

I ran this program 20 times, and the output was always `tf.Tensor([0. 2. 5. 9. 3. 4. 8. 7. 6. 1.], shape=(10,), dtype=float32)`. The probability of this happening at random is 1 in (10!)^19, or about 1 in 4.3 x 10^124. If you comment out the line `@tf.function(jit_compile=True)`, the outputs are random.

If you are running on Linux, you may find it easiest to do something like:

```bash
for N in $(seq 1 20); do python test_shuffle_xla.py; done 2> /dev/null
```

This will filter out all of the warnings and just show each of the shuffled tensors.

**Other info / logs**

I do not have any other information or logs to share at this time, but I would be happy to generate any logs that the TensorFlow team might find useful.

Thank you for your time!"
52275,RuntimeError: Encountered unresolved custom op: BoostedTreesEnsembleResourceHandleOp. Node number 0 (BoostedTreesEnsembleResourceHandleOp) failed to prepare.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.8.0-dev20211004
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

Converter:
 
        # Convert the model
        saved_model_obj = tf.saved_model.load(self.pb_model_path)
        concrete_func = saved_model_obj.signatures['predict']
        converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])

        converter.experimental_new_converter = True

        converter.target_spec.supported_ops = [
            tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
            tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
        ]
        converter.allow_custom_ops = True

        converter.optimizations = [tf.lite.Optimize.DEFAULT]

        tflite_model = converter.convert()

Inference:

        test_data = np.expand_dims(data, axis=0).astype(np.float32)

        interpreter.resize_tensor_input(input_index, [test_data.shape[0], test_data.shape[1]])

        interpreter.allocate_tensors()

        interpreter.set_tensor(input_index, test_data)

        # Run inference.
        interpreter.invoke()

        # Post-processing: remove batch dimension and find the digit with highest probability.
        output = interpreter.get_tensor(output_index)


But, 

The output from the converter invocation:

> Traceback (most recent call last):
> File ""/home/aiteam/daeho/PomaUpdrs/runTensorFlowModels/PomaGBT/poma_GBT_estimator_ver_tflite_inference.py"", line 122, in <module>
>     print(tf_lite_inference(model_path='/home/aiteam/daeho/PomaUpdrs/TFLite_models/GBT_TFLite_models/poma_GBT_TFLite_Model/poma_gbt_tflite_20211006_160120.tflite',
>   File ""/home/aiteam/daeho/PomaUpdrs/runTensorFlowModels/PomaGBT/poma_GBT_estimator_ver_tflite_inference.py"", line 69, in tf_lite_inference
>     interpreter.allocate_tensors()
>   File ""/home/aiteam/.conda/envs/daehoPython38/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py"", line 514, in allocate_tensors
>     return self._interpreter.AllocateTensors()
> RuntimeError: Encountered unresolved custom op: BoostedTreesEnsembleResourceHandleOp.
> See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 0 (BoostedTreesEnsembleResourceHandleOp) failed to prepare.



I'm wondering why the flex delegate couldn't prepare the BoostedTreesEnsembleResourceHandleOp function, even though I have the SELECT_TF_OPS / experimental_new_converter /  allow_custom_ops flag enabled?"
52272,Build r2.6 fails when build ppc64le machine,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):**source build**
- TensorFlow version: **2.6(with gpu)**
- Python version: **python3.6**
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source):**bazel 3.7.2- (@non-git)**
- GCC/Compiler version (if compiling from source):gcc 5.5.0(but gcc7 is also the same err is happened)
- CUDA/cuDNN version: cuda 10.2 cuDNN7.6
- GPU model and memory: RTX 2070 super(8gb gpu memory)

when build blow error is happen and abend build...

ERROR: /home/sueoka/gitrepo/tensorflow/tensorflow/core/platform/default/BUILD:147:11: C++ compilation of rule '//tensorflow/core/platform/default:env_time' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/ppc-opt/bin/tensorflow/core/platform/default/_objs/env_time/env_time.d ... (remaining 43 argument(s) skipped)
In file included from external/eigen_archive/Eigen/Core:210:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/core/platform/bfloat16.h:21,
                 from ./tensorflow/core/platform/types.h:21,
                 from ./tensorflow/core/platform/env_time.h:20,
                 from tensorflow/core/platform/default/env_time.cc:19:
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmul(const Packet&, const Packet&) [with Packet = __vector(8) short int]':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:801:122: error: invalid parameter combination for AltiVec intrinsic
 template<> EIGEN_STRONG_INLINE Packet8s   pmul<Packet8s>  (const Packet8s&   a, const Packet8s&   b) { return vec_mul(a,b); }
                                                                                                                          ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmul(const Packet&, const Packet&) [with Packet = __vector(8) short unsigned int]':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:802:122: error: invalid parameter combination for AltiVec intrinsic
 template<> EIGEN_STRONG_INLINE Packet8us  pmul<Packet8us> (const Packet8us&  a, const Packet8us&  b) { return vec_mul(a,b); }
                                                                                                                          ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmul(const Packet&, const Packet&) [with Packet = __vector(16) signed char]':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:803:122: error: invalid parameter combination for AltiVec intrinsic
 template<> EIGEN_STRONG_INLINE Packet16c  pmul<Packet16c> (const Packet16c&  a, const Packet16c&  b) { return vec_mul(a,b); }
                                                                                                                          ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmul(const Packet&, const Packet&) [with Packet = __vector(16) unsigned char]':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:804:122: error: invalid parameter combination for AltiVec intrinsic
 template<> EIGEN_STRONG_INLINE Packet16uc pmul<Packet16uc>(const Packet16uc& a, const Packet16uc& b) { return vec_mul(a,b); }
                                                                                                                          ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmadd(const Packet&, const Packet&, const Packet&) [with Packet = __vector(8) short int]':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:833:127: error: invalid parameter combination for AltiVec intrinsic
 template<> EIGEN_STRONG_INLINE Packet8s pmadd(const Packet8s& a, const Packet8s& b, const Packet8s& c) { return vec_madd(a,b,c); }
                                                                                                                               ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmadd(const Packet&, const Packet&, const Packet&) [with Packet = __vector(8) short unsigned int]':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:834:131: error: invalid parameter combination for AltiVec intrinsic
 template<> EIGEN_STRONG_INLINE Packet8us pmadd(const Packet8us& a, const Packet8us& b, const Packet8us& c) { return vec_madd(a,b,c); }
                                                                                                                                   ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Eigen::internal::Packet8bf Eigen::internal::F32ToBf16(Eigen::internal::Packet4f)':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1267:89: error: 'vec_cmpne' was not declared in this scope
   Packet4bi is_mant_not_zero = vec_cmpne(mantissa, reinterpret_cast<Packet4ui>(p4i_ZERO));
                                                                                         ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux_mul(const Packet&) [with Packet = __vector(8) short int; typename Eigen::internal::unpacket_traits<T>::type = short int]':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1546:37: error: invalid parameter combination for AltiVec intrinsic
   pair = vec_mul(a, vec_sld(a, a, 8));
                                     ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1547:46: error: invalid parameter combination for AltiVec intrinsic
   quad = vec_mul(pair, vec_sld(pair, pair, 4));
                                              ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1548:46: error: invalid parameter combination for AltiVec intrinsic
   octo = vec_mul(quad, vec_sld(quad, quad, 2));
                                              ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux_mul(const Packet&) [with Packet = __vector(8) short unsigned int; typename Eigen::internal::unpacket_traits<T>::type = short unsigned int]':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1557:37: error: invalid parameter combination for AltiVec intrinsic
   pair = vec_mul(a, vec_sld(a, a, 8));
                                     ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1558:46: error: invalid parameter combination for AltiVec intrinsic
   quad = vec_mul(pair, vec_sld(pair, pair, 4));
                                              ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1559:46: error: invalid parameter combination for AltiVec intrinsic
   octo = vec_mul(quad, vec_sld(quad, quad, 2));
                                              ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux_mul(const Packet&) [with Packet = __vector(16) signed char; typename Eigen::internal::unpacket_traits<T>::type = signed char]':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1577:37: error: invalid parameter combination for AltiVec intrinsic
   pair = vec_mul(a, vec_sld(a, a, 8));
                                     ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1578:46: error: invalid parameter combination for AltiVec intrinsic
   quad = vec_mul(pair, vec_sld(pair, pair, 4));
                                              ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1579:46: error: invalid parameter combination for AltiVec intrinsic
   octo = vec_mul(quad, vec_sld(quad, quad, 2));
                                              ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1580:48: error: invalid parameter combination for AltiVec intrinsic
   result = vec_mul(octo, vec_sld(octo, octo, 1));
                                                ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux_mul(const Packet&) [with Packet = __vector(16) unsigned char; typename Eigen::internal::unpacket_traits<T>::type = unsigned char]':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1589:37: error: invalid parameter combination for AltiVec intrinsic
   pair = vec_mul(a, vec_sld(a, a, 8));
                                     ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1590:46: error: invalid parameter combination for AltiVec intrinsic
   quad = vec_mul(pair, vec_sld(pair, pair, 4));
                                              ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1591:46: error: invalid parameter combination for AltiVec intrinsic
   octo = vec_mul(quad, vec_sld(quad, quad, 2));
                                              ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1592:48: error: invalid parameter combination for AltiVec intrinsic
   result = vec_mul(octo, vec_sld(octo, octo, 1));
                                                ^
In file included from external/eigen_archive/Eigen/Core:212:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/core/platform/bfloat16.h:21,
                 from ./tensorflow/core/platform/types.h:21,
                 from ./tensorflow/core/platform/env_time.h:20,
                 from tensorflow/core/platform/default/env_time.cc:19:
external/eigen_archive/Eigen/src/Core/arch/AltiVec/Complex.h: In member function 'Eigen::internal::Packet2cf Eigen::internal::Packet2cf::operator-() const':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/Complex.h:77:31: error: 'vec_neg' was not declared in this scope
     return Packet2cf(vec_neg(v));
                               ^
external/eigen_archive/Eigen/src/Core/arch/AltiVec/Complex.h: In member function 'Eigen::internal::Packet1cd Eigen::internal::Packet1cd::operator-() const':
external/eigen_archive/Eigen/src/Core/arch/AltiVec/Complex.h:330:31: error: 'vec_neg' was not declared in this scope
     return Packet1cd(vec_neg(v));
                               ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 76.801s, Critical Path: 11.73s
INFO: 221 processes: 7 internal, 214 local.
FAILED: Build did NOT complete successfully



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
52266,TFLite: How to mix floating-point kernels with quantized kernels for different parts of the graph.,"## URL(s) with the issue:
https://www.tensorflow.org/lite/performance/post_training_quant

## Description of issue (what needs changing):
On this page it says:

> TFLite supports on the fly quantization and dequantization of activations to allow for:
>
> 1. Using quantized kernels for faster implementation when available.
> **2. Mixing of floating-point kernels with quantized kernels for different parts of the graph.**

Where is it documented how this can be done?

### Clear description

I would like to create a TFLite model where the first and last layer use floating-point kernels and the intermediate layers use use quantized kernels. From the bold section above it seems possible.

The TFLite version of my model has high accuracy using float32. But it has really poor accuracy with int8. I've narrowed the issue down to my convolution layer which is used to pre and post process my audio signal. These layers have a very low parameter count compared to the rest of my model.

If I could quantized the only the big, middle portion of my model then I could get a high accuracy model with much faster accuracy and latency.

If there's another solution to this I'd love to hear about it.

### Correct links

Yes

### Parameters defined

Yes

### Returns defined

Yes

### Raises listed and defined

Yes

### Usage example

Is there a usage example?

No

### Request visuals, if applicable

No

### Submit a pull request?

No"
52265,TensorFlowLite with XNNPack error with dynamic shapes (while loop in model appeared after TFLite conversion),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device if the issue happens on mobile device: ARM64
- TensorFlow installed from (source or binary): from source
- TensorFlow version (use command below): nightly 2.8.0-dev20211004  and 2.6.0
- Python version: python 3.8.10
- Bazel version (if compiling from source): bazel 3.7.2., installed using bazelisk
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: no cuda
- GPU model and memory: no

**Describe the current behavior**
1. I've converted my model from the ONNX file to TFLite.
2. I've built TensorFlow Lite for ARM64 device using XNNpack with `bazel build --config=elinux_aarch64 --define tflite_with_xnnpack=true -c opt //tensorflow/lite:libtensorflowlite.so`. (used TF [guide](https://www.tensorflow.org/lite/guide/build_arm))
When I run it I see:
```
ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
Failed to modify graph!
```
Everything works correctly without XNNpack, but I need it so desperately.
Also, I've tried using the TensorFlow delegation mechanism for the [ARM NN](https://github.com/ARM-software/armnn/tree/branches/armnn_20_11/delegate) library, but it has the same issue.

**Describe the expected behavior**
I expect it to process my model. 
I've been exploring my model after conversion. All tensors shapes are defined, no `None` s are present. But it seems to me that the conversion process created some `while` loop and it can be the cause of the issue. Please correct me if I'm wrong. I also don't know why it's created. Maybe it is possible just to find a workaround in the converter.

**Standalone code to reproduce the issue**
I'm adding here my ONNX model and TFLite model archived. Please put them on the `some_path `variable. [model.zip](https://github.com/tensorflow/tensorflow/files/7288640/model.zip)
Converter code:
```
onnx_model = onnx.load(some_path)   # load onnx model
tf_rep = prepare(onnx_model, 'CPU')   # ONNX -> TensorFlowRep representation (computational graph)
tf_rep.export_graph(some_path) 
converter = tf.lite.TFLiteConverter.from_saved_model(some_path)
converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()
with open(os.path.join(some_path,'model.tflite'), 'wb') as f:
  f.write(tflite_model)    
```   

**Other info / logs**
INFO: Created TensorFlow Lite delegate for select TF ops.
INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 32 nodes with 1 partitions.
INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 4 nodes with 1 partitions.
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.
INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 29 nodes with 1 partitions.
INFO: TfLiteArmnnDelegate: Created TfLite ArmNN delegate.
ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
Failed to modify graph!

"
52263,Pruned TFLite model has invalid model identifier after post training quantisation,"### 1. System information

- Linux Ubuntu 20.04
- Pip TensorFlow package version 2.6.0

### 2. Code
Reproducable code in this [gist](https://colab.research.google.com/gist/niciBume/2fb8d9eb36d5e13d8faaea9e1273e980/tensorflow-lite-debugger-colab.ipynb#scrollTo=RD0CEfccGN2C)

### 3. Description
After pruning a keras model according to [tf colab](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras), the post training quantized tflite can't be invoked (the quantisation works for the un-pruned model).

`interpreter` = tf.lite.Interpreter(model_path=MODEL_TF_PRUNE)`

Leads to the error:
```
ValueError                                Traceback (most recent call last)

<ipython-input-56-edee2625144a> in <module>()
----> 1 interpreter = tf.lite.Interpreter(model_path=MODEL_TF_PRUNE)

/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)
    365           _interpreter_wrapper.CreateWrapperFromFile(
    366               model_path, op_resolver_id, custom_op_registerers_by_name,
--> 367               custom_op_registerers_by_func, experimental_preserve_all_tensors))
    368       if not self._interpreter:
    369         raise ValueError('Failed to open {}'.format(model_path))

ValueError: Model provided has model identifier '

', should be 'TFL3'
```
"
52262,ERROR: Using member tf.contrib.layers,"Hi, I converted my code from v1.x to v2.x and now I have the following erros:

ERROR: Using member tf.contrib.layers.l1_l2_regularizer in deprecated module tf.contrib. tf.contrib.layers.l1_l2_regularizer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.

and

ERROR: Using member tf.contrib.layers.apply_regularization in deprecated module tf.contrib. tf.contrib.layers.apply_regularization cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.

for the following lines of code:

 ` l1_l2_regularization = tf.contrib.layers.l1_l2_regularizer(scale_l1=self.h['l1_const'], scale_l2=self.h['l2_const'], scope=None)`
` vars_ = tf.compat.v1.trainable_variables()`
` regularization_penalty = tf.contrib.layers.apply_regularization(l1_l2_regularization, vars_)`

Can someone help me how to convert the above lines of code to accommodate v2.x?

Any help
Thanks"
52260,No gradient is provided to any variable. This occurs only when I want to use lambda layer for keras.argmax function and keep a model non trainable,"`def define_gan(g_model, d_model,timestep):
  
    discriminator.trainable = False
    generator.trainable= True
    
    a = Input(shape=(timestep,))
    b = g_model(a)
    c=K.argmax(b,axis=-1)
    output_d = d_model(c)
    model = Model(inputs=a,outputs=output_d)
    
    # compile model
    opt = Adam(lr=0.0002, beta_1=0.5)
    model.compile(loss=['binary_crossentropy'], optimizer=opt,metrics=['accuracy'])
    model.summary()
    return model`"
52259,The difference in the return order of object detection results in tf2.5-cpu and tf2.6-gpu,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/lite/tutorials/model_maker_object_detection#optional_test_the_tflite_model_on_your_image


## Description of issue (what needs changing):
the return order in tf2.5-cpu in google colab is 
```
  boxes = get_output_tensor(interpreter, 0)
  classes = get_output_tensor(interpreter, 1)
  scores = get_output_tensor(interpreter, 2)
  count = int(get_output_tensor(interpreter, 3))
```

but in tf2.6-gpu the return order is
```
    scores = get_output_tensor(interpreter, 0)
    print(scores)
    boxes = get_output_tensor(interpreter, 1)
    print(boxes)
    count = int(get_output_tensor(interpreter, 2))
    print(count)
    classes = get_output_tensor(interpreter, 3)
    print(classes)
```

the link to my script is: https://github.com/MaoXianXin/Tensorflow_tutorial/blob/tf_hub/TFLite_prac/object_detection_lite.py
the detect result is in the Clear description

### Clear description
![2021-10-05-19-58-43](https://user-images.githubusercontent.com/22192610/136018975-a943ff2b-3698-44c7-bd32-d5ab4d156f33.png)
the following is my result:
![result](https://user-images.githubusercontent.com/22192610/136019965-c69c2071-149b-49fa-8cee-b41318e41012.png)

if i use the code in docs the error is result:
![2021-10-05-20-13-48](https://user-images.githubusercontent.com/22192610/136020174-4ff542e1-37a3-4adb-96b7-0894d43f6e49.png)

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
52258,Building tflite with xnnpack on windows 10 ,"hello team,
 
i wanted to use xnnpack with tflite on my windows 10 machine. found out that we need to build tf from source with tflite_with_xnnpack=true flag. i was not able to build successfully ,and I am not sure I am following the correct steps or not as I didn't get proper installation guide for building tf with xnnpack on windows. It would be helpful if I get any documentation or resource to build and use tflite with XNN pack"
52255,Model Maker Object Detection Tutorial Bug,"I ran the Model Maker Object Detection Tutorial via Colab.
(https://colab.research.google.com/drive/1DhxMGuQ9ep9mrfDBrFBx47zmOeEOn9_W#scrollTo=qhl8lqVamEty)

However, a problem occurred in
 `model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)`.

```
Epoch 1/50
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
<ipython-input-5-187f39c1697e> in <module>()
----> 1 model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)

9 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

UnknownError: 2 root error(s) found.
  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]
  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]
	 [[Func/cond/then/_3378/input/_6828/_56]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_96849]

Function call stack:
train_function -> train_function
```

Please solve this problem."
52254,issue with tf.saved_model.save,"https://github.com/tensorflow/tensorflow/issues/51587

I cannot understand why this serious issue is not paid any attention for such a  long time"
52253,Incorrect Links under API Reference ,"## URL(s) with the issue:

Please provide a link to the documentation entry : 
https://www.tensorflow.org/community/contribute/docs#api_reference


## Description of issue (what needs changing): 

### Clear description

Under the description of  [""API Reference"",](https://www.tensorflow.org/community/contribute/docs#api_reference) there is a link to a google doc for [Tensorflow 2 API Docs Advice](https://docs.google.com/document/d/1e20k9CuaZ_-hp25-sSd8E8qldxKPKQR-SkwojYr_r-U/preview#). This doc is having some links which are not working and may create confusion for any beginner who wants to start contributing to docs. 
Links like - 
1. link of [tensorflow docs task tracker](https://docs.google.com/spreadsheets/d/1p3vqbocbKmcZQGrlxk9m2jHuleu8yr-XRbfKum0IDD8/edit?usp=sharing) - this doc is not currently under consideration so this (along with its related info) should be removed to avoid any confusion. 
2.  [docstring links](https://www.tensorflow.org/community/documentation) - this link is not working (needs to be correctly linked)
3.  links given for [examples , guide & tutorials](https://github.com/tensorflow/docs/tree/master/site/en/r2) - incorrect link
4. link for [python API documentation](https://www.tensorflow.org/community/documentation#generating_python_api_documentation) - incorrect link

So, overall from a beginner's point of view, I can say that this FAQ doc for API is not much useful & helpful So I'll suggest either update it or remove it! 
"
52252,"tf.data.service worker occasionally fails ""to send worker update"" and will be unrecoverable","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Mostly congruent to https://github.com/tensorflow/ecosystem/blob/master/data_service/tf_std_data_server.py 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Image based on `https://gcr.io/deeplearning-platform-release/tf2-gpu.2-5`
- TensorFlow installed from (source or binary): see image
- TensorFlow version (use command below): 2.5
- Python version: 3.7 (see image)

**Describe the current behavior**
Occasionally, a third of my 10 workers will all report something like this about 30 times

```
2021-10-05 02:19:10.033421: W tensorflow/core/data/service/worker_impl.cc:311] Failed to send task updates to dispatcher: Not found: Failed to send worker update: Task 4003 not found
```

and then not serve any data.  This is confirmed by checking network egress metrics for the workers.  And throughput on the training node will collapse.

This seems to come from https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/data/service/worker_impl.cc;l=308-312;bpv=0;bpt=1

This happens maybe every 3 times I use the service.  It does not seem to recover by itself.  I have to manually restart each worker that is experiencing this to recover throughput.

It is okay that this fails, but it does not seem to exit code as a failure and just sits there.

None of these workers have high mem pressure or cpu usage.

NOTE: I am not trying to consume multiple datasets.  there is only 1 job and 1 dataset but multiple consumers (using `strategy.distribute_dataset`)

**Describe the expected behavior**
Either workers report this an error and exit code properly or retry until success.

"
52250,Compiling TF 2.6 in debug mode on Windows env. (hude pdb file),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): r2.6.0
- Python version: 3.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.3/8
- GPU model and memory: GTX 1060

**Describe the current behavior**
I try to compile my example program that utilises TF:
// tensorflow/cc/example/example.cc

#include ""tensorflow/cc/client/client_session.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/tensor.h""

int main() {
  using namespace tensorflow;
  using namespace tensorflow::ops;
  Scope root = Scope::NewRootScope();
  // Matrix A = [3 2; -1 0]
  auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });
  // Vector b = [3 5]
  auto b = Const(root, { {3.f, 5.f} });
  // v = Ab^T
  auto v = MatMul(root.WithOpName(""v""), A, b, MatMul::TransposeB(true));
  std::vector<Tensor> outputs;
  ClientSession session(root);
  // Run and fetch v
  TF_CHECK_OK(session.Run({v}, &outputs));
  // Expect outputs[0] == [19; -3]
  LOG(INFO) << outputs[0].matrix<float>();
  return 0;
}

After patching few files to allow debug compilation (https://github.com/tensorflow/tensorflow/issues/51799#issuecomment-912567685) all tensorflow libs compile, but the example linkage still fails.

bazel build --local_ram_resources=HOST_RAM*.7 --config=dbg --config=windows --copt=/FS --copt=-nvcc_options=disable-warnings --linkopt=/DEBUG:FULL --strip=never --define=no_tensorflow_py_deps=true -s --verbose_explanations --subcommands=pretty_print //tensorflow/cc/example:example

The compilation fails with exception:
LINK : fatal error LNK1201: error writing to program database 'E:\tensorflow_r2.6_cpp_debug_cpu\output\xpduztnu\execroot\org_tensorflow\bazel-out\x64_windows-dbg\bin\tensorflow\cc\example\example.pdb'; check for insufficient disk space, invalid path, or insufficient privilege
Target //tensorflow/cc/example:example failed to build

example.pdb has size of 4.41 GB (4,742,363,136 bytes)"
52249,Error compilation NVCC for TF 2.5 from source using Clang-11 for Debian 11,"Hi all!

**System information**
- OS Platform and Distribution: Linux Debian 11, kernel 5.10.0-8-amd64 
- Mobile device : No
- TensorFlow installed from: try compile from source 
- TensorFlow version: 2.5.1
- Python version: 3.8.12
- Installed using: virtualenv
- Bazel version: 3.7.2
- Compiler version: Debian Clang version 11.1.0-++20210804031632+1fdec59bffc1-1~exp1~20210804132251.11 installed from https://apt.llvm.org/
- CUDA version:  Driver Version: 460.32.03  /  CUDA Version: 11.2 **installed from run**
- cuDNN version: Cudnn_v8.1.1.33
- GPU model and memory: GeForce GTX 1050 2GB RAM single
- NCCL:  2.8.4, for CUDA 11.2, February 03,2021
- RAM:  16GB


**Problem description**
Permanently  when I trying  compile TF 2.5.1 from source using Clang-11 raising  error compilation.
**Before compile TF 2.5.1 with Clang-11 I successful compiled  TF 2.5.1  for verification  from source using standard  Debian gcc compiler version 10.2.1-6 - it was compiled without problems** but with a **permanenty  warning** in the IPython console 
`: I tensorflow / compiler / mlir / mlir_graph_optimization_pass.cc: 176] None of the MLIR Optimization Passes are enabled
`



**1. Configure TF 2.5.1  screen  before compilation:**
```
tf251) mvg@debian:~/tensorflow$ ./configure
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is /home/mvg/tf251/bin/python3]: 

Found possible Python library paths:
  /home/mvg/tf251/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/home/mvg/tf251/lib/python3.8/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.2 in:
    /usr/local/cuda-11.2/targets/x86_64-linux/lib
    /usr/local/cuda-11.2/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/local/cuda-11.2/targets/x86_64-linux/lib
    /usr/local/cuda-11.2/targets/x86_64-linux/include

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: 

Do you want to use clang as CUDA compiler? [y/N]: y
Clang will be used as CUDA compiler.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify which clang should be used as device and host compiler. [Default is /usr/lib/llvm-11/bin/clang]: 

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```


**2. Run compilation command and their console output:**
```
(tf251) mvg@debian:~/tensorflow$ bazel build --config=opt --copt=-march=native --copt=-Wno-sign-compare \
--copt=-mavx --copt=-mfma  --copt=-mfma4  \
--copt=-msse4.1 --copt=-msse4.2 --jobs=4 //tensorflow/tools/pip_package:build_pip_package --verbose_failures
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=80
INFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/mvg/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/mvg/tf251/bin/python3 --action_env PYTHON_LIB_PATH=/home/mvg/tf251/lib/python3.8/site-packages --python_path=/home/mvg/tf251/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-11/bin/clang --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /home/mvg/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/mvg/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /home/mvg/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /home/mvg/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/mvg/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build option --action_env has changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (426 packages loaded, 36834 targets configured).
INFO: Found 1 target...
```

**3. Error log:**
```
ERROR: /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/nccl_archive/BUILD.bazel:54:17: C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command 
  (cd /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/mvg/tf251/bin:/usr/lib/llvm-11/bin:/usr/local/nccl_2.8.4/lib:/usr/local/nccl_2.8.4/lib/pkgconfig:/usr/local/cuda-11.2/nsight-systems-2020.4.3/bin/:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \
    PWD=/proc/self/cwd \
  /usr/lib/llvm-11/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.o' -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c bazel-out/host/bin/external/nccl_archive/src/collectives/device/min_f16_reduce_scatter.cu.cc -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.o)
Execution platform: @local_execution_config_platform//:platform
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:432:
In file included from /usr/include/strings.h:144:
/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration
__NTH (bcopy (const void *__src, void *__dest, size_t __len))
       ^
/usr/include/strings.h:38:13: note: previous declaration is here
extern void bcopy (const void *__src, void *__dest, size_t __n)
            ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:432:
In file included from /usr/include/strings.h:144:
/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration
__NTH (bzero (void *__dest, size_t __len))
       ^
/usr/include/strings.h:42:13: note: previous declaration is here
extern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));
            ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration
__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,
       ^
/usr/include/string.h:43:14: note: previous declaration is here
extern void *memcpy (void *__restrict __dest, const void *__restrict __src,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration
__NTH (memmove (void *__dest, const void *__src, size_t __len))
       ^
/usr/include/string.h:47:14: note: previous declaration is here
extern void *memmove (void *__dest, const void *__src, size_t __n)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration
__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,
       ^
/usr/include/string.h:378:14: note: previous declaration is here
extern void *mempcpy (void *__restrict __dest,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration
__NTH (memset (void *__dest, int __ch, size_t __len))
       ^
/usr/include/string.h:61:14: note: previous declaration is here
extern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration
__NTH (explicit_bzero (void *__dest, size_t __len))
       ^
/usr/include/string.h:436:13: note: previous declaration is here
extern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1));
            ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration
__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))
       ^
/usr/include/string.h:122:14: note: previous declaration is here
extern char *strcpy (char *__restrict __dest, const char *__restrict __src)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration
__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))
       ^
/usr/include/string.h:452:14: note: previous declaration is here
extern char *stpcpy (char *__restrict __dest, const char *__restrict __src)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration
__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,
       ^
/usr/include/string.h:125:14: note: previous declaration is here
extern char *strncpy (char *__restrict __dest,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:116:8: error: exception specification in declaration does not match previous declaration
__NTH (stpncpy (char *__dest, const char *__src, size_t __n))
       ^
/usr/include/string.h:460:14: note: previous declaration is here
extern char *stpncpy (char *__restrict __dest,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:126:8: error: exception specification in declaration does not match previous declaration
__NTH (strcat (char *__restrict __dest, const char *__restrict __src))
       ^
/usr/include/string.h:130:14: note: previous declaration is here
extern char *strcat (char *__restrict __dest, const char *__restrict __src)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-11/lib/clang/11.1.0/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:495:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:133:8: error: exception specification in declaration does not match previous declaration
__NTH (strncat (char *__restrict __dest, const char *__restrict __src,
       ^
/usr/include/string.h:133:14: note: previous declaration is here
extern char *strncat (char *__restrict __dest, const char *__restrict __src,
             ^
13 errors generated when compiling for sm_61.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/mvg/tensorflow/tensorflow/lite/python/BUILD:59:10 C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1): clang failed: error executing command 
  (cd /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/mvg/tf251/bin:/usr/lib/llvm-11/bin:/usr/local/nccl_2.8.4/lib:/usr/local/nccl_2.8.4/lib/pkgconfig:/usr/local/cuda-11.2/nsight-systems-2020.4.3/bin/:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \
    PWD=/proc/self/cwd \
  /usr/lib/llvm-11/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.o' -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.2' -g0 -w -Wno-sign-compare -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -O3 -fcuda-rdc -Xcuda-ptxas '-maxrregcount=96' -c bazel-out/host/bin/external/nccl_archive/src/collectives/device/min_f16_reduce_scatter.cu.cc -o bazel-out/host/bin/external/nccl_archive/_objs/device_lib/min_f16_reduce_scatter.cu.o)
Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 870.023s, Critical Path: 58.66s
INFO: 11462 processes: 7400 internal, 4062 local.
FAILED: Build did NOT complete successfully
```

**4. About compability CUDA  v11.2.2 and Clang-11**
1.[ Official documentation](https://docs.nvidia.com/cuda/archive/11.2.2/cuda-installation-guide-linux/index.html) in the partition `Installation Guide Linux` Clang-11 support  declared.
2.  file in the CUDA directory  '/usr/local/cuda-11-2/include/crt/host_config.h` explicitly next stated:
```
#if defined(__clang__) && !defined(__ibmxl_vrm__) && !defined(__ICC) && !defined(__HORIZON__) && !defined(__APPLE__)
#if (__clang_major__ >= 12) || (__clang_major__ < 3) || ((__clang_major__ == 3) &&  (__clang_minor__ < 3))
#error -- unsupported clang version! clang version must be less than 12 and greater than 3.2 . The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.
```
3. File ` /home/mvg/tensorflow/.bazelrc` contains pointer for in the next tensofrlow distibution in the partiotion `Remote build execution options (only configured to work with TF team projects for now.)` : 
`build:rbe_linux_cuda_clang_base --crosstool_top=""@ubuntu18.04-clang_manylinux2010-cuda11.2-cudnn8.1-tensorrt7.2_config_cuda//crosstool:toolchain"" `
I have installed Clang-11 - hence following the infomation above I should compile normally TF 2.5.1 from source with Clang-11.


**5. The appointment of a responsible specialist for this ticket**
I ask for the solution of this problem to appoint responsible [Artem Belevich Artem-B](https://github.com/Artem-B) and not to football me like balls for various person who haven't knoweledge in the subject and turn the solution into an eternal groundhog day - since every time I retell a human problem, after a while it disappears, a new person appears , I will explain again, the opat disappeared and so on ad infinitum.

Best regards, Vadim Maklakov.

P.S. Information for Artem-B - I think that   problem is somehow related to nccl. It doesn't matter if nccl is installed or not - it is constantly mentioned in the compilation error log.
P.P.S `/home/mvg/tf251/bin:` - path for Python 3.8.12 virtual environment whe installef all required modules how in [this ](https://www.tensorflow.org/install/source)instruction.


"
52245,model.fit() showing value error. I don't know where i am doing wrong.,"Epoch 1/5
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-24-3875a67a7083> in <module>()
     50 model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'] )
     51 
---> 52 history = model.fit(train_x, train_y, epochs=5)
     53 

9 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    992           except Exception as e:  # pylint:disable=broad-except
    993             if hasattr(e, ""ag_error_metadata""):
--> 994               raise e.ag_error_metadata.to_exception(e)
    995             else:
    996               raise

ValueError: in user code:

    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *
        return step_function(self, iterator)
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:835 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:789 train_step
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py:201 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    /usr/local/lib/python3.7/dist-packages/keras/losses.py:141 __call__
        losses = call_fn(y_true, y_pred)
    /usr/local/lib/python3.7/dist-packages/keras/losses.py:245 call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/keras/losses.py:1666 categorical_crossentropy
        y_true, y_pred, from_logits=from_logits, axis=axis)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/keras/backend.py:4839 categorical_crossentropy
        target.shape.assert_is_compatible_with(output.shape)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:1161 assert_is_compatible_with
        raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
ValueError: Shapes (None, 1) and (None, 10) are incompatible"
52244,"XLA dumps are not generated after setting XLA_FLAGS=""--xla_dump_to=/some/path""","I was running the unit test ```//tensorflow/compiler/tests:categorical_op_test_gpu``` and wanted to see the XLA dumps for this unit test. I set the --xla_dump_to to a correct path, but dump files were never generated (the setup is correct as for other tests, It can generate XLA dumps). 

I tried to investigate this issue  further and I found that [```detailed_logging```](https://github.com/tensorflow/tensorflow/blob/70ba657dcc0aed3593562931df16090218ab616b/tensorflow/compiler/jit/xla_compile_on_demand_op.cc#L138) has been hard coded to false in this location. This value is being used later on in [here](https://github.com/tensorflow/tensorflow/blob/70ba657dcc0aed3593562931df16090218ab616b/tensorflow/compiler/xla/service/dump.cc#L76) to reset the ```dump_to``` path. 

With this empty value for ```dump_to```, it will never generate the files even though ```xla_dump_to``` has been set. This behavior is contradictive to the XLA documentation described [here](https://www.tensorflow.org/xla)"
52240,Op type not registered 'NormalizeUTF8' in binary running when reload model #51080,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): 2.6
- TensorFlow version (use command below):
- Python version: Python 3.7.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: GPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Actually , I am trying to convert the saved_model to tensorflowlite model and while using this code
import tensorflow as tf

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model('./translator') # path to the SavedModel directory
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)

I am getting this error
![image](https://user-images.githubusercontent.com/21074002/135761699-070a32cf-7387-48bf-a60a-cad371eb855a.png)
"
52239,Shaky sidebar on the docs,"## URL(s) with the issue: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate

## Description of issue (what needs changing):

When I scroll down in the docs, the left side of pages shakes very aggressively.

https://user-images.githubusercontent.com/35577566/135758613-fbdf9612-e663-4dda-af53-aaae4d26b97a.mov


"
52237,"kerasTensor not behaving as expected, functional api incorrectly skipped","

**System information**
Tf version:
v2.6.0-rc2-32-g919f693420e 2.6.0

**Problem**
Using `tf.keras.Input(4, 1)` doesn't work with all layers. I am using an op which does not support dispatching, which will break the use of `KerasTensor`s. Adding dispatching fixes this problem, and digging into the dispatching code I see it uses `tensorflow.python.keras.layers.KerasOpDispatcher` which just wraps my op in a `TFOpLambda` layer. Rather than add the dispatching I figure I can just use the TFOpLambda layer directly. But using the `TFOpLambda` layer directly does not work, because the check for whether the input to the layer is a keras tensor erroneously returns false, which means the layer is not built through the functional api.

**Minimal example**
```python
import tensorflow as tf
import tensorflow.keras.layers as layers

from tensorflow.python.ops.gen_cudnn_rnn_ops import cudnn_rnn
from tensorflow.python.util.dispatch import add_dispatch_support
from tensorflow.python.keras.layers.core import TFOpLambda

# explicitly adding dispatching works!
# the call to cudnn_rnn_dispatch calls a tfoplayer, which is built CORRECTLY with the
# functional API as the check _in_functional_construction_mode returns true this check
# is done in keras.engine.base_layer and it returns true because we have a tensor t such that:
# t is an instance of <class 'keras.engine.keras_tensor.KerasTensor'> 
# is_instance(t, keras_tensor.KerasTensor) -> True
x = layers.Input([1, 4])
cudnn_rnn_dispatch = add_dispatch_support(cudnn_rnn)
out = cudnn_rnn_dispatch(x, x, 0, tf.zeros(128), rnn_mode='gru')
y = out[0]

# turning into a layer does not work!
# it fails because in the base_layer now the _in_functional_construction_mode check
# erroneuously returns False!
# the _in_functional_construction_mode check now takes place in
# tensorflow.python.keras.engine.base_layer
# and we have the confusing:
# t is an instance of <class 'keras.engine.keras_tensor.KerasTensor'> 
# is_instance(t, keras_tensor.KerasTensor) -> False
x = layers.Input([1, 4])
cudnn_layer = TFOpLambda(cudnn_rnn)
try:
    out = cudnn_layer(x, x, 0, tf.zeros(128), rnn_mode='gru')
    y = out[0]
    print(y)
except TypeError:
    print('IM BROOOOOOOOOOOOOOOKEN')

```"
52236,mixed_precision returns gradient zeros when the model input size is large,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Noe
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.9
- CUDA/cuDNN version: 11.1/8.1.1
- GPU model and memory: RTX 3090/24GB

**Describe the current behavior**
When using mixed_precision policy described in [https://www.tensorflow.org/guide/mixed_precision](url) with large model input size, for example `(256, 368, 368,)`, the returned gradient are constantly ZEROS. However, if remove the mixed_precision policy, the returned gradient is normal with non-zeros numbers.

Moreover, if we use small model input size, let's say `(16, 16, 16)`, the returned gradient is normal no matter the mixed_precision is allowed or not. 

My model is a typical U-net like model.

**Describe the expected behavior**
With large model input size like `(256, 368, 368)` used above, the returned gradient should be at least non-zeros. Otherwise the model won't be trained. 


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.


The code below set the input shape to `[1, 256, 368, 368, 1]` and allows `mixed_precision.Policy('mixed_float16')`. It will return zeros gradients in the end (hence, no training at all). Setting `tf16_flag=False` will returns normal gradient behavior.

Also, by change `shape = [1, 16, 16, 16, 1]`, the gradient behaves normally no matter allows `mixed_precision.Policy('mixed_float16')` or not

```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import mixed_precision
import numpy as np
from tqdm import tqdm

gpus = tf.config.experimental.list_physical_devices('GPU')

for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

tf16_flag = True

if tf16_flag:
    policy = mixed_precision.Policy('mixed_float16')
    mixed_precision.set_global_policy(policy)


shape = [1, 256, 368, 368, 1]
# shape = [1, 16, 16, 16, 1]


def forward_conv(x, filters, kernels, name='forward', padding='same'):
    i = 0
    for flt, kernel in zip(filters, kernels):
        x = layers.Conv3D(flt, kernel, activation='relu', padding=padding, dilation_rate=(1, 1, 1),
                          use_bias=False, name=str(i) + '_' + name)(x)
        x = layers.BatchNormalization(name=str(i) + '_bn_' + name)(x)
        i += 1
    return x


def part_one(ipt):
    l1 = forward_conv(ipt, (4, 4), (3, 3), name='enc1')
    d2 = layers.MaxPool3D(pool_size=(2, 2, 2))(l1)
    l2 = forward_conv(d2, (4, 4), (3, 3), name='enc2')
    return l1, l2


def part_inner(ipt1, ipt2):
    l1 = forward_conv(ipt1, (4, 4), (3, 3), name='enc1')
    l2 = forward_conv(ipt2, (4, 4), (3, 3), name='enc2')
    return l1, l2


def part_two(ipt1, ipt2):
    l2 = forward_conv(ipt2, (4, 4), (3, 3), name='dec2')
    u1 = layers.UpSampling3D(size=(2, 2, 2))(l2)
    r1 = forward_conv(ipt1 + u1, (4, 4), (3, 3), name='dec1')
    return r1

initial = tf.ones(shape, dtype=tf.float16) if tf16_flag \
    else tf.ones(shape, dtype=tf.float32)

tf.random.set_seed(1)

with tf.GradientTape() as g:
    g.watch(initial)
    l1_, l2_ = part_one(initial)
    for _ in range(2):
        l1_, l2_ = part_inner(l1_, l2_)
    opt_ = part_two(l1_, l2_)
    loss = tf.reduce_mean(l1_) + tf.reduce_mean(opt_)
    gd = g.gradient(loss, initial)
    print('-' * 100)
    print(f'loss is {loss} and grad is {np.sum(gd)} with input shape {shape}')

```
"
52234,Normalization layer masking,"**System information**
- TensorFlow version (you are using): nightly (on colab) (also on my machine, but behind a few weeks (built upstream from source for ROCm support))
- Are you willing to contribute it (Yes/No): Yes, once maintainers evaluate proposal

**Describe the feature and the current behavior/state.**
tf.keras.layers.Normalization() does not ignore masked values.

**Will this change the current api? How?**
Only additional parameters, should not contain any non-backwards compatible changes.

**Who will benefit with this feature?**
People working with ragged data who want to include a preprocessing layer to normalize in model.

**Any Other info.**

Proposing:
- add parameter to `.adapt()` method (mask_value=0.0 and/or boolean mask)
- add `call(..., mask=None)` and tensor multiply to allow masking during model fitting/evaluation

[Colab gist current behavior](https://colab.research.google.com/drive/1aKGeS0lJy-QEQx0jARyOAqAMFBEFRe--?usp=sharing)

An attempt at masked normalization (does **NOT** generalize to any input shape) 
```
class MaskedNormalization(tf.keras.layers.Layer):
    """"""custom masked normalization layer""""""

    def __init__(
        self,
        mask_value: int = 999,
        input_dims: tuple = (51, 3),
        name: str = ""masked_normalizer"",
    ):
        super(MaskedNormalization, self).__init__()

        self.mask_value = mask_value
        self.supports_masking = True
 
        self.normalization = tf.keras.layers.experimental.preprocessing.Normalization(axis=2)


    def adapt(self, inputs):
        """"""redefined adapt to ignore masked values""""""
        if not self.normalization.built:
            raise RuntimeError(""layer must be \""built\"" prior to calling `.adapt()`, see `model.build_graph()`"")

        mask = inputs != self.mask_value

        float_mask = tf.cast(mask, dtype=tf.dtypes.float32)

        mean = tf.reduce_sum(inputs * float_mask, axis=[0, 1]) / tf.reduce_sum(
            float_mask, axis=[0, 1]
        )
        # sample variance (divide by: n-1)
        var = tf.reduce_sum(((inputs - mean) ** 2) * float_mask, axis=[0, 1]) / (
            tf.reduce_sum(float_mask, axis=[0, 1])
            - tf.constant(1, dtype=tf.dtypes.float32, shape=(3,))
        )
        count = tf.experimental.numpy.count_nonzero(
            float_mask
        )

        self.normalization.adapt_mean.assign(mean)
        self.normalization.adapt_variance.assign(var)
        self.normalization.count.assign(count)

        self.normalization.finalize_state()
        self.normalization._is_adapted = True

    def call(self, inputs, mask=None):

        # if mask provided mask output values (multiply by zero)
        if mask is not None:
                reshape_mask = tf.transpose(
                    tf.expand_dims(tf.cast(mask, dtype=tf.dtypes.float32), axis=0),
                    perm=[1, 2, 0],
                )
            return self.normalization(inputs) * reshape_mask

        return self.normalization(inputs)

    def compute_mask(self, inputs, mask=None):
        """"""return the input_mask directly""""""
        return mask
```
"
52232,TF2 suggests adding report_tensor_allocations_upon_oom to RunOptions while the option is not available,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.18.0-193.60.2.el8_2.x86_64
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8.0
- GPU model and memory: Tesla V100-SXM2-32GB

**Describe the current behavior**
When OOM, the following message is displayed:
`Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.`
However, the suggested option is not available in tf2.

This (only!) works in tf1, e.g. as described in [this SO answer](https://stackoverflow.com/a/49675283/11611246).
tf2 doesn't have `RunOptions`

`run_opts = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom=True)`
results in:
`TypeError: Invalid keyword argument(s) in 'compile': {'options'}`

**Describe the expected behavior**

1. Preferably; Implementation of the option `report_tensor_allocations_upon_oom` in tf2
2. Removal of the misleading ""hint"" in tf2
"
52231,Addition of Resnet18 Model in Application,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.** Addition of a new pre-trained model: Resnet18 in the application section.

**Will this change the current api? How?** Yes. Addition of a new Resnet18 pre-trained model in the application section.

**Who will benefit with this feature?** Researchers as well as individuals who want to fine-tune or train Resnet18 in Tensorflow without switching to PyTorch

**Any Other info.** This will be a helpful implementation to Tensorflow as currently Resnet18 is present in PyTorch but becomes difficult for individuals to implement who are not familiar with Pytorch and have knowledge of Tensorflow.
"
52230,Cant dynamically change the weights of a multi-output model loss functions during training,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):2.4
- Are you willing to contribute it (Yes/No):yes

**Describe the feature and the current behavior/state.**
I have a multi output model using the sub classing API where each output have a different loss function, and i want to change the loss functions weights dynamically in the training  process ( as shown : https://arxiv.org/pdf/1711.02257.pdf  and more papers). 
It is reasonable to have the option to alter these weights using a callback
 ( very much like the learning rate scheduler https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler)  

here is the call() method that my sub-class model uses: 
NOTE : the ges_out is a classifier output 
```
    def call(self, inputs, training=False, **kwargs):

        # intro block1
        x = self.conv1(inputs)
        x = self.spatial_dropout1(x, training=training)
        x = self.conv2(x)
        x = self.spatial_dropout2(x, training=training)
        # x = self.avgpool1(x)
        x = self.conv3(x)
        intro_out = self.spatial_dropout3(x, training=training)
        # intro_out = self.avgpool2(x)

        # classification block

        g = self.conv_ges1(intro_out)
        g = self.spatial_dropout_ges1(g)
        g = self.maxpool_ges1(g)
        g = self.conv_ges2(g)
        g = self.spatial_dropout_ges2(g)
        g = self.maxpool_ges2(g)
        g = self.flatten_ges(g)
        g = self.fc_ges1(g)
        g = self.dropout_ges1(g, training=training)
        g = self.fc_ges2(g)
        g = self.dropout_ges2(g, training=training)
        ges_out = self.fc_ges_out(g)

        # segmentation block
        s = self.conv_seg1(intro_out)
        s = self.spatial_dropout_seg1(s)
        s = self.avgpool_seg1(s)
        s = self.conv_seg2(s)
        s = self.spatial_dropout_seg2(s)
        s = self.avgpool_seg2(s)
        s = self.flatten_seg(s)
        s = self.fc_seg1(s)
        s = self.dropout_seg1(s, training=training)
        seg_out = self.fc_seg_out(s)

        return {""gesture_out"": ges_out, ""segmentation_out"": seg_out}
```

and here is how i use model_compile(...):
  ```
  def model_compile_and_summary():
        metrics = {""gesture_out"": [""accuracy""], ""segmentation_out"": [""mean_squared_error""]}
        loss_functions = {""gesture_out"": keras.losses.CategoricalCrossentropy(from_logits=True),
                              ""segmentation_out"": keras.losses.MeanAbsoluteError()}
        loss_weights = {""gesture_out"": 1.0, ""segmentation_out"": 0.0001}
        optimizer_name = ""Adam""
        optimizer = get_optimizer_from_name(optimizer_name)
        MyModel.model.compile(optimizer=optimizer, loss=loss_functions, metrics=metrics, loss_weights=loss_weights)
```
  
**Will this change the current API? How?**
the current API have the option to pass constant loss functions weights in the model.compile(...) method.  
i assigned each output to each loss using a dictionary   
**Who will benefit with this feature?**
 i think all the researchers/engineers  that currently deal with multi-task learning. 
"
52225,This build was never aapproved..,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
52223,OP_REQUIRES failed at conv_ops.cc:1276 : Not found: No algorithm worked!,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4 / 8.2.4.15
- GPU model and memory: NVIDIA GeForce RTX 2070 

**Describe the current behavior**

I get the following error on GPU:
```
2021-10-01 23:05:27.951528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6173 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id:0000:09:00.0, compute capability: 7.5
2021-10-01 23:05:28.331213: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204
2021-10-01 23:05:28.866860: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at conv_ops.cc:1276 : Not found: No algorithm worked!
Traceback (most recent call last):
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1375, in _do_call
    return fn(*args)
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.
  (0) Not found: No algorithm worked!
         [[{{node convolution}}]]
  (1) Not found: No algorithm worked!
         [[{{node convolution}}]]
         [[convolution/_5]]
0 successful operations.
0 derived errors ignored.
```

**Describe the expected behavior**

It is expected that I get some error here. But it should be about some wrong filter shape. The exception I get is very misleading.

On CPU, the exception looks much different, and is more like what I would expect.

**Standalone code to reproduce the issue**

```
import tensorflow as tf
import numpy

tf.compat.v1.disable_eager_execution()


with tf.Graph().as_default() as graph:
    with tf.compat.v1.Session(graph=graph) as session:
        x = tf.compat.v1.placeholder(tf.float32, (None, None, 1, 40))  # [B,T,1,40]
        filters = tf.compat.v1.placeholder(tf.float32, (3, 3, None, 32))
        y = tf.compat.v1.nn.convolution(x, filter=filters, padding=""SAME"")

        session.run(
            y,
            feed_dict={
                x: numpy.zeros((3, 4, 1, 40)),
                filters: numpy.zeros((3, 3, 1, 32)),
                })
```

**Other info / logs**

This problem was originally reported here: https://github.com/rwth-i6/returnn/issues/703

There are a couple of the same error also reported here:
* https://github.com/tensorflow/tensorflow/issues/43174
* https://github.com/tensorflow/tensorflow/issues/45044
* https://github.com/tensorflow/tensorflow/issues/48117

In many of these cases, it seems to be caused by too less GPU memory. However, not in the case here. So probably these are not duplicates. Although it's not totally clear.

I stumbled upon this problem due to a wrong model checkpoint, and the model checkpoint loading ignored the different shape of the filter, which is another bug (https://github.com/tensorflow/tensorflow/issues/52220).
"
52222,Blas xGEMV launch failed error when running on GPU. ,"I've ran into a really strange error that is incredibly difficult to track down.  I have a version of Tensorflow (2.6.0-rc1) using Cuda 11.3 that I compiled from source.  When I run it using my GPU for evaluating a pretty normal feed forward model I constantly get this crash on this one particular line. 

```
    h = tf.matmul(h, self.output_weight)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py"", line 3654, in matmul
    return gen_math_ops.mat_mul(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 5696, in mat_mul
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6941, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Blas xGEMV launch failed : a.shape=[1,2728550,5], b.shape=[1,5,1], m=2728550, n=1, k=5 [Op:MatMul]

```
When I look up this error it usually indicates a problem in the CUDA install or other similar problems, but in this particular case CUDA is working just fine. I am able to run another script which uses Keras from the same Tensorflow install and use the GPU with zero problem. In fact in this same exact code 2-3 TF.MatMul calls go through with zero problem right before it crashes on this line.  

``
h = tf.matmul(h, self.output_weight)
``
Where self.output_weight is a tensorflow Variable class and h is as well.  Both data types float32.  The thing is the h variable also was computed by multiple sequential calls that run on the GPU just fine.  When I run in CPU only mode it runs just fine with zero problems.  The model only uses matmul opperations and standard tensorflow library functions. What's even stranger is this same exact code works on the GPU for a different set of input!  

I'm running this on Linux Mint 20.2 with Kernel version 5.4.0-88-generic.  Cuda version 11.3 with the corresponding compatible version of TensorRT, the NVidia drivers, etc.  This is being done on a RTX 3090 graphics card.   It's an incredibly strange error since it only pops up on this one particular line. I also tested out my Cuda install via the samples folder in the Cuda install directory.  My other Cuda based GPU applications work just fine as well.  Nvidia-smi also shows Python3 taking up most of the card's memory as it usually does. 

"
52221,TensorFlow Object Detection : Error while executing python model_main_tf2.py,"Environment:

1. TensorFlow:2.6
2. Python : 3.9.6
3. Transfer learning for Object Detection using TensorFlow
4. Dataset : Digital Meter
5. Example Link: https://medium.com/analytics-vidhya/digit-recognition-of-digital-meters-using-tensorflow-2-object-detection-api-48364cd678a9 
6. Pre-trained Model : ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8
7. Generated Tf Record successfully( train.record and test.record )
8. Python command:  python model_main_tf2.py --model_dir=models/my_ssd_mobilenet_
9. v1_fpn --pipeline_config_path=models/my_ssd_mobilenet_v1_fpn/pipeline.config

**Error:** 
**2021-10-02 01:05:11.264437: W tensorflow/core/framework/dataset.cc:679] Input of GeneratorDatasetOp::Dataset will not be op
timized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\keras\backend.py:401: UserWarning: `tf.keras.backend
.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to th
e `training` argument of the `__call__` method of your layer or model.
  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '
Traceback (most recent call last):
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\Workspace\model_main_tf2.py"", line 115, in <module>
    tf.compat.v1.app.run()
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\platform\app.py"", line 40,
 in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\absl\app.py"", line 303, in run
    _run_main(main, args)
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\Workspace\model_main_tf2.py"", line 106, in main
    model_lib_v2.train_loop(
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\object_detection\model_lib_v2.py"", line 618,
 in train_loop
    ckpt.restore(latest_checkpoint)
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\training\tracking\util.py""
, line 2335, in restore
    status = self.read(save_path, options=options)
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\training\tracking\util.py""
, line 2220, in read
    result = self._saver.restore(save_path=save_path, options=options)
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\training\tracking\util.py""
, line 1382, in restore
    base.CheckpointPosition(
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\training\tracking\base.py""
, line 254, in restore
    restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\training\tracking\base.py""
, line 980, in _restore_from_checkpoint_position
    current_position.checkpoint.restore_saveables(
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\training\tracking\util.py""
, line 351, in restore_saveables
    new_restore_ops = functional_saver.MultiDeviceSaver(
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\training\saving\functional
_saver.py"", line 339, in restore
    restore_ops = restore_fn()
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\training\saving\functional
_saver.py"", line 323, in restore_fn
    restore_ops.update(saver.restore(file_prefix, options))
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\training\saving\functional
_saver.py"", line 115, in restore
    restore_ops[saveable.name] = saveable.restore(
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\distribute\values.py"", lin
e 1079, in restore
    return values_util.get_on_write_restore_ops(self._mirrored_variable,
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\distribute\values_util.py""
, line 94, in get_on_write_restore_ops
    tuple(
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\distribute\values_util.py""
, line 95, in <genexpr>
    assign_on_device(v.device, v, tensor)
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\distribute\values_util.py""
, line 302, in assign_on_device
    return variable.assign(tensor)
  File ""C:\Users\shilpa.m\PycharmProjects\TensorFlow_OD\venv\lib\site-packages\tensorflow\python\ops\resource_variable_ops.
py"", line 899, in assign
    raise ValueError(
ValueError: Cannot assign to variable WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredict
or/bias:0 due to variable shape (66,) and value shape (72,) are incompatible**
"
52220,Model checkpoint load ignores wrong shape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 11.6 + Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below):  v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.9.6 + 3.8
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**

Saving a variable of shape (3,3,40,32) into a checkpoint, and then loading the variable with shape (3,3,1,32) from this checkpoint does not cause an error. Instead, it loads fine. The variable shape still claims to be (3,3,1,32), but when evaluated, one can see that it is indeed (3,3,40,32).

So there does not seem to be any validation of the shape during checkpoint loading.

**Describe the expected behavior**

In general, if a tensor or variable claims to be of some static shape, I think it should never be possible that its actual shape when evaluated is different.

**Standalone code to reproduce the issue**
```

import tensorflow as tf

print(""TF:"", tf.__version__)
tf.compat.v1.disable_eager_execution()

filename = ""test-ckpt-diff-shape.model""


with tf.Graph().as_default() as graph:
    with tf.compat.v1.Session(graph=graph) as session:
        shape1 = (3,3,40,32)
        v = tf.compat.v1.get_variable(name=""W"", shape=shape1)
        print(v)
        saver = tf.compat.v1.train.Saver(var_list=[v])
        session.run(tf.compat.v1.global_variables_initializer())
        saver.save(sess=session, save_path=filename)


with tf.Graph().as_default() as graph:
    with tf.compat.v1.Session(graph=graph) as session:
        shape2 = (3,3,1,32)
        v = tf.compat.v1.get_variable(name=""W"", shape=shape2)
        print(v)
        saver = tf.compat.v1.train.Saver(var_list=[v])
        saver.restore(sess=session, save_path=filename)
        v_raw = session.run(v)
        print(v)
        print(v_raw.shape)
        assert v.shape.as_list() == list(shape2)
        assert v.shape.as_list() == list(v_raw.shape)
```

**Other info / logs**

I stumbled upon this problem because it causes a seemingly unrelated error for some following 2D convolution where this variable is used as a kernel:
```
2021-09-30 12:52:25.768174: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops.cc:1
115 : Not found: No algorithm worked!
TensorFlow exception: 2 root error(s) found.
  (0) Not found: No algorithm worked!
         [[node conv0/convolution (defined at u/schmitt/src/returnn/returnn/tf/layers/basic.py:4061) ]]
         [[output/rec/while/Switch_14/_553]]
  (1) Not found: No algorithm worked!
         [[node conv0/convolution (defined at u/schmitt/src/returnn/returnn/tf/layers/basic.py:4061) ]]
0 successful operations.
```
This was reported here: https://github.com/rwth-i6/returnn/issues/703

There are a couple of these errors also reported here:
* https://github.com/tensorflow/tensorflow/issues/43174
* https://github.com/tensorflow/tensorflow/issues/45044
* https://github.com/tensorflow/tensorflow/issues/48117

I wonder if some of them have a similar source.

But anyway, this issue is not really about the convolution error, but about the checkpoint loading behavior, and the variable shape.
"
52219,Could not run the task evaluation!-tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:tf2.6.0
- Python version:3.7.4
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):3.7.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
When I use **thetensorflow/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/** link tool to build and evaluate my tflite model, it shows that the evaluation task cannot be run. I have confirmed that the file path of all transmission parameters is correct. Hope to get your help!
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

$ bazel run -c opt \
>   -- \
>   //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval \
>   --model_file=mobilenet_224_1_0_INT8.tflite \
>   --ground_truth_images_path=./test/ \
>   --ground_truth_labels=validation_ground_truth.txt \
>   --model_output_labels=mobilenet_labels.txt \
>   --output_file_path=/tmp/accuracy_output.txt \
>   --num_images=0 # Run on all images.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=80
INFO: Reading rc options for 'run' from /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'run' from /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/a21c557955c6ea5cd02b9a145ad6469c608446c7.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/ad9981d394f5ccc784d6273e51aa41c38b7cf727.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /home/xdd/project/mobilenet/tf/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/xdd/project/mobilenet/tf/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  /home/xdd/.cache/bazel/_bazel_xdd/238513a408dfab846cdfe0f15ca64c47/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  /home/xdd/.cache/bazel/_bazel_xdd/238513a408dfab846cdfe0f15ca64c47/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/694d2524757f9040e65a02c374e152a462fe57eb.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
Target //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval up-to-date:
  bazel-bin/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/run_eval
INFO: Elapsed time: 0.089s, Critical Path: 0.00s
INFO: 1 process: 1 internal.
INFO: Build completed successfully, 1 total action
INFO: Running command line: bazel-bin/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/run_eval '--model_file=mobilenet_224_1_0_INT8.tflite' '--ground_truth_images_path=./test/' '--ground_truth_labels=validation_ground_truth.txt' '--model_output_labels=mobilenet_labels.txt' '--output_file_path=/tmp/INFO: Build completed successfully, 1 total action
Could not run the task evaluation!

"
52218,Converted tf.gfile.GFile to tf.io.gfile.GFile ,"**System information**
- OS Platform and Distribution (Google Colab):
- TensorFlow version (tensorflow 2.6.0):

I was working with the tensorflow's object detection api and as soon as i ran the following code:

PATH_TO_LABELS = '/content/models/research/object_detection/data/mscoco_label_map.pbtxt'
category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)

It showed the following error.
AttributeError                            Traceback (most recent call last)
<ipython-input-82-651c9b9bcbff> in <module>()
      1 PATH_TO_LABELS = '/content/models/research/object_detection/data/mscoco_label_map.pbtxt'
----> 2 category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)

2 frames
/usr/local/lib/python3.7/dist-packages/object_detection/utils/label_map_util.py in load_labelmap(path)
    130   Returns:
    131     a StringIntLabelMapProto
--> 132   """"""
    133   with tf.io.gfile.GFile(path, 'r') as fid:
    134     label_map_string = fid.read()

AttributeError: module 'tensorflow' has no attribute 'gfile'

I tried to change the load_labelmap file too but it showed no result. Viewing this on stack overflow suggested that I should downgrade my tensorflow version to 1.x but for the particular program, I need to have the tensorflow version above 2.5

What can be the possible solution to this problem."
52216,"Mac m1 tf2.5.0: No layer for IntegerLookup, Normalization, StringLookup","**System information**
- OS Platform and Distribution: macOS Big Sur Version 11.3.1 with mac m1 chip
- TensorFlow installed from: https://developer.apple.com/metal/tensorflow-plugin/
- TensorFlow version (use command below): tf 2,5
- Python version: 3.8

Debug output:
```
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION)
```
unknown 2.5.0

**Describe the current behavior**
```
from tensorflow.keras.layers import IntegerLookup
from tensorflow.keras.layers import Normalization
from tensorflow.keras.layers import StringLookup
```
ImportError                               Traceback (most recent call last)
/var/folders/1z/77jbzk11477gc69_s8n2y0r00000gn/T/ipykernel_27569/1759035919.py in <module>
----> 1 from tensorflow.keras.layers import IntegerLookup

ImportError: cannot import name 'IntegerLookup' from 'tensorflow.keras.layers' (/Users/username/miniforge3/envs/env/lib/python3.8/site-packages/tensorflow/keras/layers/__init__.py)


**Standalone code to reproduce the issue**
Follow link https://developer.apple.com/metal/tensorflow-plugin/ to install tf2.5 on mac and then run the import
"
52215,Creating dataset from large numpy arrays via from_tensor_slices crashes without any error message or warning,"I use tf 2.6 and when I try to create datasets from larger numpy arrays (>10GB) via from_tensor_slices the code breaks when I try to train via ""fit"" or even just attempt to iterate over the dataset. The code just breaks and exists, no warning, no error message, nothing. I have not found any similar issue mentioned anywhere else. What is the actual limitation here? 

I have over 128GB ram and due to the code breaking already on the dataset iteration part it is surely unrelated to my GPU and its memory (24GB). The numpy arrays load without issues into memory but once the iterator causes the execution of ""from_tensor_slices"", the code breaks shortly after. 

What are workable solutions here? DataGenerators? Creating TFRecord files? I try to avoid going the route of TFRecords because it appears very poorly documented how to create such binary files. 

As no warning or error message is output there is no log to show at all. 

The problem can be reproduced with a large numpy array (random data works) and the following code:

#obtain training data
    print(""loading training data..."")
    train_data = np.load(os.path.join(os.getcwd(), ""source_datasets"", f""{train_data_id}_features.npy""))
    train_targets = np.load(os.path.join(os.getcwd(), ""source_datasets"", f""{train_data_id}_targets.npy""))

    print(""training dataset construction..."")
    train_ds = tf.data.Dataset.from_tensor_slices((train_data, train_targets))

    for x,y in train_ds:
        do_something"
52214,A Problem that Only One Inference Can Be Made when I Loaded the Saved Model on Tensorflow-gpu 2.5.0 and latest.,"**My System Information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: 
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.5.0 and latest.
Python version: 3.7.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA 11.2 / cuDNN 8.1
GPU model and memory: RTX3060 / 12gb
Exact command to reproduce:

------------------------------------------------------------------------

**Describe the problem**

I want to load the saved model using ```saved_model.load()``` and make inference using the ```serving_default``` signature key.

However, there is a problem that inference only occurs once.

When there is a single image, inference is made normally, but when there are multiple images or a video, inference is not made at all from the second image (or frame).

Continuous inference is possible by reloading the model before every inference, but it is very slow.

This problem does not occur in previous versions (2.3.0) of tensorflow.
This problem only occurs in version 2.5.0 or later. I tested the 2.5.0 / 2.6.0 / 2.8.0 (nightly) version, but none of them work.


**Source code / logs**

The code below is a summary.

 ```
#model load
saved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING])
infer = saved_model_loaded.signatures['serving_default']

# begin video capture
try:
    vid = cv2.VideoCapture(int(video_path))
except:
    vid = cv2.VideoCapture(video_path)

while True:
    return_value, frame = vid.read()
    if return_value:
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image = Image.fromarray(frame)
    else:
        print('Video has ended or failed, try a different video format!')
        break
    
    frame_size = frame.shape[:2]
    image_data = cv2.resize(frame, (input_size, input_size))
    image_data = image_data / 255.
    image_data = image_data[np.newaxis, ...].astype(np.float32)
    start_time = time.time()

    #prediction
    batch_data = tf.constant(image_data)
    pred_bbox = infer(batch_data)
```

The original code is [detect_video.py](https://github.com/theAIGuysCode/tensorflow-yolov4-tflite/blob/master/detect_video.py) of [theAIGuysCode/tensorflow-yolov4-tflite](https://github.com/theAIGuysCode/tensorflow-yolov4-tflite).

If you run the above code and do `print(pred_bbox)`, it is inferred normally in the first frame, but nothing is printed from the second.

e.g. 1) First Frame
```
{'tf.concat_16': <tf.Tensor: shape=(1, 21, 84), dtype=float32, numpy=
array([[[5.54353952e-01, 2.06503779e-01, 8.69302511e-01, ...,
         1.45839895e-05, 2.10430017e-06, 8.75245519e-07],
        [5.53583145e-01, 2.05075234e-01, 8.70401978e-01, ...,
         1.43441921e-05, 2.20338029e-06, 7.08290202e-07],
        [5.51974833e-01, 2.04461098e-01, 8.72702539e-01, ...,
         1.21745070e-05, 2.31040190e-06, 7.29801911e-07],
        ...,
        [5.51477075e-01, 2.00088605e-01, 8.68865132e-01, ...,
         8.14086491e-07, 2.59839396e-07, 1.50752749e-06],
        [5.49633265e-01, 1.98086709e-01, 8.72833788e-01, ...,
         1.83860755e-06, 4.25420922e-07, 3.30964349e-06],
        [8.79046202e-01, 6.94515137e-03, 1.00040674e+00, ...,
         1.45153881e-05, 5.73599527e-06, 6.34236858e-06]]], dtype=float32)>}
```

e.g. 2) Second+ Frame
```
{'tf.concat_16': <tf.Tensor: shape=(1, 0, 84), dtype=float32, numpy=array([], shape=(1, 0, 84), dtype=float32)>}
```"
52210,Monitor TPU-VM utilization,"Hi Tensorflow Team,

I'm trying to use the monitoring function: [`tf.profiler.experimental.client.monitor`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/profiler/profiler_client.py#L135), but getting the following error:
```
return _pywrap_profiler.monitor(
tensorflow.python.framework.errors_impl.UnimplementedError: unimplemented.
```

My setup is the following: I `ssh` into a host tpu-vm and run everything locally on the host vm.
(using the following `gcloud` command: `gcloud alpha compute tpus tpu-vm  ssh my-tpu ...`)

Code looks something like this:
```
import tensorflow as tf
from tf.python.profiler import profiler_client

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=""local"")
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

class Monitor(object):
    def init(self, service_addr, duration_ms):
        self.service_addr = service_addr
        self.duration_ms = duration_ms
        self._stop = True
        self.client = profiler_client

    def _loop(self):
       while not self._stop:
            time.sleep(0.5)
            try:
                self.client.monitor(self.service_addr, duration_ms=self.duration_ms, level=1)
            except Exception as e:
                print(e)
                time.sleep(1)

    def start(self):
        if self._stop:
            self._thread = threading.Thread(target=self._loop, daemon=True)
            self._stop = False
            self._thread.start()

    def stop(self):
        if not self._stop:
            self._stop = True
            self._thread.join()

tf.profiler.experimental.server.start(8466)
tpu_monitor = Monitor(""grpc://localhost:8466"", 2000)
tpu_monitor.start()
train_model()
tpu_monitor.stop()
```
Could you please advice what am i doing wrong?
Is monitoring not supported when running locally on the tpu-host vm?

Also alternatively tried doing this: (following this [tutorial](https://cloud.google.com/tpu/docs/cloud-tpu-tools#monitor_job))
`pip3 install --upgrade ""cloud-tpu-profiler>=2.3.0""`
running my training scrip in one shell and in another shell running (on the host tpu-vm): `capture_tpu_profile --service_addr localhost:8466 --monitoring_level 1 --num_queries 1000`, however looking at the code for the `capture_tpu_profile` script it uses the same `profiler_client.monitor(service_addr, duration_ms, monitoring_level)` function (so probably not surprising that i get the same error)"
52208,"NVlink for tensorflow2.5, linux, 3090 GPU","OS: Linux Ubuntu18.04
TF building method: TF docker with Nvidia Docker,  CUDA version: 11.1
Version: Tensorflow 2.5.0
GPUs: RTX3090 x2

Hi there.

I noticed that the NVlink for 3090 is published. I got a workstation with 2 3090 GPU cards. I wish to know is it necessary to got a NVlink to accelerate data transfer between these 2 cards?

For tensorflow building via docker, should i install some extra toolkit? And when I train model with MirrowedStrategy, what should I do to activate the NVlink? 
"
52207,"When using .fit() to train a binary classification model, the metrics (i.e. loss, accuracy) shown in the history log of .fit() for training set is different from the result from .evaluate()","I'm using the tf.keras.Model.fit() to train a binary classification model for images. I found that the metrics like accuracy shown in the final epoch's log of .fit() can be 1.0, while the accuracy shown in model.evaluate(ds_train) for training set can be only 0.71. The loss and any other metrics are also different. However, the results for validation set are perfectly consistent for .fit and .evaluate. And also, for multiclass classification training, this issue never happens.

The issue has happened multiple times for different projects. Please let me know if anything wrong happened.

Here is a sample code for my binary classification training:

def load_image(img_path,size = (32,32)):
    label = tf.constant([1],tf.int8) if tf.strings.regex_full_match(img_path,"".*automobile.*"") \
            else tf.constant([0],tf.int8)
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img) 
    img = tf.image.resize(img,size)/255.0
    return(img,label)

ds_train = tf.data.Dataset.list_files(""./data/cifar2/train/*/*.jpg"") \
           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \
           .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
           .prefetch(tf.data.experimental.AUTOTUNE)  

ds_test = tf.data.Dataset.list_files(""./data/cifar2/test/*/*.jpg"") \
           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \
           .batch(BATCH_SIZE) \
           .prefetch(tf.data.experimental.AUTOTUNE)  
tf.keras.backend.clear_session() 

inputs = layers.Input(shape=(32,32,3))
x = layers.Conv2D(32,kernel_size=(3,3))(inputs)
x = layers.MaxPool2D()(x)
x = layers.Conv2D(64,kernel_size=(5,5))(x)
x = layers.MaxPool2D()(x)
x = layers.Dropout(rate=0.1)(x)
x = layers.Flatten()(x)
x = layers.Dense(32,activation='relu')(x)
outputs = layers.Dense(1,activation = 'sigmoid')(x)

model = models.Model(inputs = inputs,outputs = outputs)

model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss=tf.keras.losses.binary_crossentropy,
        metrics=[""accuracy""]
    )

history = model.fit(ds_train,epochs= 5,validation_data=ds_test)

*******************************************************************
after running for .fit():
Epoch 5/5
100/100 [==============================] - 1s 7ms/step - loss: 0.2098 - accuracy: 0.9154 - val_loss: 0.2171 - val_accuracy: 0.9080

for .evaluate() after training:
model.evaluate(ds_train)

100/100 [==============================] - 0s 5ms/step - loss: 0.1968 - accuracy: 0.9195

The accuracy can be more different for some other projects."
52206,RuntimeError: hashtable need to be initialized before usingNode number 74 (HASHTABLE_FIND) failed to invoke.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): PIP
- TensorFlow library (version, if pip package or github SHA, if built from source):  tensorflow-gpu==2.3.0

NOTE: Used tensorflow-gpu==2.3.0 to build and train the model and USED tf-nightly-gpu==2.8.0 to convert the model to tflite.

### 2. Code
```
@tf.function
def preprocessing_space_tf(text):

    def remove_special_characters(text, remove_digits=False):
        pattern = r'[^a-zA-z0-9\s]' if not remove_digits else r'[^a-zA-z\s]'
        text = tf.strings.regex_replace(text, pattern, '')
        return text

    def replace(text):
        text = tf.strings.lower(text)
        text = tf.strings.regex_replace(text, '_', ' ')
        text = tf.strings.regex_replace(text, '-', ' ')
        text = tf.strings.regex_replace(text, ':', ' ')
        text = tf.strings.regex_replace(text, '/', ' ')
        return text

    text = replace(text)
    text = remove_special_characters(text)

    return text


class USE_CNN(tf.keras.Model):
    def __init__(self, num_classes, **kwargs):
        super(USE_CNN, self).__init__(name='USE_CNN', **kwargs)
        self.num_classes = num_classes
        self.preprocess = preprocessing_space_tf
        self.embedding = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4', trainable=True)
        self.dense_1024 = tf.keras.layers.Dense(1024, activation='relu')
        self.dropout = tf.keras.layers.Dropout(0.35)
        self.dense_out = tf.keras.layers.Dense(self.num_classes, activation='softmax')

    def call(self, text):
        x = self.preprocess(text)
        x = self.embedding(tf.squeeze(x, axis=1))
        x = self.dense_1024(x)
        x = self.dropout(x)
        x = self.dense_out(x)
        return x


nlp_model = USE_CNN(num_classes=3266)

nlp_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=tf.keras.metrics.SparseCategoricalAccuracy())

nlp_model.fit(x_train, y_train, epochs=50, batch_size=100)

nlp_model.save('/use_cnn/', save_format='tf')

# convert to tflite
converter = tf.lite.TFLiteConverter.from_saved_model('/use_cnn/')  # path to the SavedModel directory
converter.experimental_enable_resource_variables = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

with open('model.tflite', 'wb') as f:
    f.write(tflite_model)

# Inference using tflite
interpreter = tf.lite.Interpreter(model_path='model.tflite')
interpreter.allocate_tensors()
input_index = interpreter.get_input_details()[0][""index""]
interpreter.set_tensor(input_index, np.expand_dims(['Rear Air Conditioning'], 0))
interpreter.invoke()
output_details = interpreter.get_output_details()

```

### 3. Failure after conversion
Exception occurs while invoking the interpreter.
**RuntimeError: hashtable need to be initialized before usingNode number 74 (HASHTABLE_FIND) failed to invoke.**

"
52203,macOS debug build issue,"**System information**
- OS Platform and Distribution: macOS Big Sur 11.6
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): Apple clang via Xcode 12.5.1.12E507
- CUDA/cuDNN version: CPU only
- GPU model and memory: CPU only

**Describe the problem**

Reincarnation of #46566 (`truncated or malformed object (LC_SEGMENT_64 command 0 fileoff field plus filesize field extends past the end of the file)` during linkage of debug build) but with latest versions (tensorflow 2.6, bazel 3.7.2, Xcode 12.5.1, macOS 11.6). I'm also fine with re-opening #46566 (I can't, permission denied).

I even tried bazel 4.2.1 (in case this was fixed there but not backported to v3.x) by increasing the max-allowed-version beyond 3.99.0 in tensorflow/configure.py. However this didn't change anything.

Seems that the size of the debug symbols grew beyond some maximum supported threshold."
52200,No registered 'Const' OpKernel for GPU devices with constant folding,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4 / 8.2.4.15
- GPU model and memory: NVIDIA GeForce RTX 2070 

**Describe the current behavior**

The code below fails with an exception.
This is the full output:
```
TF: 2.6.0
2021-09-30 15:52:24.159169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.162278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.162637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.163155: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-30 15:52:24.163754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.164103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.164431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.456691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.457036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.457342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.457640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5732 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:09:00.0, compute capability: 7.5
2021-09-30 15:52:24.466132: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'Const' OpKernel for GPU devices compatible with node {{node ConstantFolding/Const_enter}}
         (OpKernel was found, but attributes didn't match) Requested Attributes: dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=""/job:localhost/replica:0/task:0/device:GPU:0""
        .  Registered:  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_VARIANT]
  device='DEFAULT'; dtype in [DT_BOOL]
  device='DEFAULT'; dtype in [DT_QUINT16]
  device='DEFAULT'; dtype in [DT_QINT16]
  device='DEFAULT'; dtype in [DT_QINT32]
  device='DEFAULT'; dtype in [DT_QUINT8]
  device='DEFAULT'; dtype in [DT_QINT8]
  device='DEFAULT'; dtype in [DT_COMPLEX128]
  device='DEFAULT'; dtype in [DT_COMPLEX64]
  device='DEFAULT'; dtype in [DT_INT8]
  device='DEFAULT'; dtype in [DT_UINT8]
  device='DEFAULT'; dtype in [DT_INT16]
  device='DEFAULT'; dtype in [DT_UINT16]
  device='DEFAULT'; dtype in [DT_UINT32]
  device='DEFAULT'; dtype in [DT_INT64]
  device='DEFAULT'; dtype in [DT_UINT64]
  device='DEFAULT'; dtype in [DT_DOUBLE]
  device='DEFAULT'; dtype in [DT_FLOAT]
  device='DEFAULT'; dtype in [DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_HALF]
  device='DEFAULT'; dtype in [DT_INT32]
  device='CPU'
  device='TPU_SYSTEM'
  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_UINT64]
  device='GPU'; dtype in [DT_INT64]
  device='GPU'; dtype in [DT_QINT32]
  device='GPU'; dtype in [DT_UINT32]
  device='GPU'; dtype in [DT_QUINT16]
  device='GPU'; dtype in [DT_QINT16]
  device='GPU'; dtype in [DT_INT16]
  device='GPU'; dtype in [DT_UINT16]
  device='GPU'; dtype in [DT_QINT8]
  device='GPU'; dtype in [DT_INT8]
  device='GPU'; dtype in [DT_UINT8]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_BFLOAT16]
  device='GPU'; dtype in [DT_HALF]

Traceback (most recent call last):
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1375, in _do_call
    return fn(*args)
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Const' OpKernel for 'GPU' devices compatible with node {{node ConstantFolding/Const_enter}}
         (OpKernel was found, but attributes didn't match) Requested Attributes: _XlaHasReferenceVars=false, dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=""/job:localhost/replica:0/task:0/device:GPU:0""
        .  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]
  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]
  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_VARIANT]
  device='DEFAULT'; dtype in [DT_BOOL]
  device='DEFAULT'; dtype in [DT_QUINT16]
  device='DEFAULT'; dtype in [DT_QINT16]
  device='DEFAULT'; dtype in [DT_QINT32]
  device='DEFAULT'; dtype in [DT_QUINT8]
  device='DEFAULT'; dtype in [DT_QINT8]
  device='DEFAULT'; dtype in [DT_COMPLEX128]
  device='DEFAULT'; dtype in [DT_COMPLEX64]
  device='DEFAULT'; dtype in [DT_INT8]
  device='DEFAULT'; dtype in [DT_UINT8]
  device='DEFAULT'; dtype in [DT_INT16]
  device='DEFAULT'; dtype in [DT_UINT16]
  device='DEFAULT'; dtype in [DT_UINT32]
  device='DEFAULT'; dtype in [DT_INT64]
  device='DEFAULT'; dtype in [DT_UINT64]
  device='DEFAULT'; dtype in [DT_DOUBLE]
  device='DEFAULT'; dtype in [DT_FLOAT]
  device='DEFAULT'; dtype in [DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_HALF]
  device='DEFAULT'; dtype in [DT_INT32]
  device='CPU'
  device='TPU_SYSTEM'
  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_UINT64]
  device='GPU'; dtype in [DT_INT64]
  device='GPU'; dtype in [DT_QINT32]
  device='GPU'; dtype in [DT_UINT32]
  device='GPU'; dtype in [DT_QUINT16]
  device='GPU'; dtype in [DT_QINT16]
  device='GPU'; dtype in [DT_INT16]
  device='GPU'; dtype in [DT_UINT16]
  device='GPU'; dtype in [DT_QINT8]
  device='GPU'; dtype in [DT_INT8]
  device='GPU'; dtype in [DT_UINT8]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_BFLOAT16]
  device='GPU'; dtype in [DT_HALF]

         [[ConstantFolding/Const_enter]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tf-const-gpu.py"", line 18, in <module>
    session.run(n)
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1190, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1368, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1394, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Const' OpKernel for 'GPU' devices compatible with node {{node ConstantFolding/Const_enter}}
         (OpKernel was found, but attributes didn't match) Requested Attributes: _XlaHasReferenceVars=false, dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=""/job:localhost/replica:0/task:0/device:GPU:0""
        .  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]
  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]
  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_VARIANT]
  device='DEFAULT'; dtype in [DT_BOOL]
  device='DEFAULT'; dtype in [DT_QUINT16]
  device='DEFAULT'; dtype in [DT_QINT16]
  device='DEFAULT'; dtype in [DT_QINT32]
  device='DEFAULT'; dtype in [DT_QUINT8]
  device='DEFAULT'; dtype in [DT_QINT8]
  device='DEFAULT'; dtype in [DT_COMPLEX128]
  device='DEFAULT'; dtype in [DT_COMPLEX64]
  device='DEFAULT'; dtype in [DT_INT8]
  device='DEFAULT'; dtype in [DT_UINT8]
  device='DEFAULT'; dtype in [DT_INT16]
  device='DEFAULT'; dtype in [DT_UINT16]
  device='DEFAULT'; dtype in [DT_UINT32]
  device='DEFAULT'; dtype in [DT_INT64]
  device='DEFAULT'; dtype in [DT_UINT64]
  device='DEFAULT'; dtype in [DT_DOUBLE]
  device='DEFAULT'; dtype in [DT_FLOAT]
  device='DEFAULT'; dtype in [DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_HALF]
  device='DEFAULT'; dtype in [DT_INT32]
  device='CPU'
  device='TPU_SYSTEM'
  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_UINT64]
  device='GPU'; dtype in [DT_INT64]
  device='GPU'; dtype in [DT_QINT32]
  device='GPU'; dtype in [DT_UINT32]
  device='GPU'; dtype in [DT_QUINT16]
  device='GPU'; dtype in [DT_QINT16]
  device='GPU'; dtype in [DT_INT16]
  device='GPU'; dtype in [DT_UINT16]
  device='GPU'; dtype in [DT_QINT8]
  device='GPU'; dtype in [DT_INT8]
  device='GPU'; dtype in [DT_UINT8]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_BFLOAT16]
  device='GPU'; dtype in [DT_HALF]

         [[ConstantFolding/Const_enter]]
```

**Describe the expected behavior**

The code below should work without error on a GPU.

**Standalone code to reproduce the issue**


```
import tensorflow as tf


print(""TF:"", tf.__version__)
tf.compat.v1.disable_eager_execution()
tf.compat.v1.disable_control_flow_v2()


with tf.compat.v1.Session() as session:
  x = tf.constant(""foo"")

  def body(i):
    with tf.control_dependencies([tf.print(x)]):
      return i + 1

  n = tf.while_loop(cond=lambda i: tf.less(i, 1), body=body, loop_vars=[0])
  session.run(n)
```
"
52199,ipykernel_launcher.py: error: unrecognized arguments,"
![1](https://user-images.githubusercontent.com/26819449/135461638-7520ab41-8310-4c5f-8016-2d9b4609e205.JPG)
 The error pops out ipykernel_launcher.py: error: unrecognized arguments.
Can you just tell me why I am getting this error. I can't share the code with you.
I can fix it on my own just tell me why I am getting this Error.
Thank You.
"
52198,Tensorflow models predictions not working on multithreading ,"I am facing this issue while testing FasterRCNN and SSD on multithreading on **AWS Deep Learning AMI Ubuntu**.

However, the testing is going fine without multithreading.

**Also, the same code is working fine **Colab** but not on **Kaggle****.

- TensorFlow installed from (source or binary): 2.4.2
- Python version: 3.7.10
- AWS deep learning AMI env - for TensorFlow 2.4 with Python3.7 (CUDA + and Intel MKL-DNN) ________________________ source activate tensorflow2_latest_p37
- CUDA/cuDNN version: 11.0
- GPU model and memory: Tesla T4 16 GB

Error - 

2.4.2
{4: 'gun_not_inhand', 2: 'curlinary_knife', 3: 'gun_inhand', 1: 'bloody', 5: 'knife_in_hand', 6: 'knife_not_in_hand'}
Traceback (most recent call last):
  File ""ssd_img.py"", line 132, in <module>
    for _ in executor.map(call,listF):
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/concurrent/futures/_base.py"", line 598, in result_iterator
    yield fs.pop().result()
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/concurrent/futures/_base.py"", line 435, in result
    return self.__get_result()
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/concurrent/futures/_base.py"", line 384, in __get_result
    raise self._exception
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/concurrent/futures/thread.py"", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""ssd_img.py"", line 87, in call
    res = detector_prediction(i)
  File ""ssd_img.py"", line 38, in detector_prediction
    output = detection_model(img_array) #get list of tensors discussed above as output
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1669, in __call__
    return self._call_impl(args, kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 247, in _call_impl
    args, kwargs, cancellation_manager)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1687, in _call_impl
    return self._call_with_flat_signature(args, kwargs, cancellation_manager)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1736, in _call_with_flat_signature
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1919, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 560, in call
    ctx=ctx)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.
  (0) Not found:  Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysPreprocessor/map/TensorArray_43)
         [[node Preprocessor/map/while/TensorArrayReadV3 (defined at ssd_img.py:18) ]]
         [[SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape_1/_296]]
  (1) Not found:  Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysPreprocessor/map/TensorArray_43)
         [[node Preprocessor/map/while/TensorArrayReadV3 (defined at ssd_img.py:18) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_pruned_7040]

Function call stack:
pruned -> pruned

(tensorflow2_latest_p37) shiva.baghel@ip-10-0-0-175:~/Data/shiva.baghel/SSD$ python ssd_img.py
2.4.2
{4: 'gun_not_inhand', 2: 'curlinary_knife', 3: 'gun_inhand', 1: 'bloody', 5: 'knife_in_hand', 6: 'knife_not_in_hand'}
Exception in thread Thread-8:
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""ssd_img.py"", line 87, in call
    res = detector_prediction(i)
  File ""ssd_img.py"", line 38, in detector_prediction
    output = detection_model(img_array) #get list of tensors discussed above as output
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1669, in __call__
    return self._call_impl(args, kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 247, in _call_impl
    args, kwargs, cancellation_manager)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1687, in _call_impl
    return self._call_with_flat_signature(args, kwargs, cancellation_manager)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1736, in _call_with_flat_signature
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1919, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 560, in call
    ctx=ctx)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.
  (0) Not found:  No algorithm worked!
         [[node FirstStageFeatureExtractor/resnet_v1_50/resnet_v1_50/conv1/Conv2D (defined at ssd_img.py:18) ]]
  (1) Not found:  No algorithm worked!
         [[node FirstStageFeatureExtractor/resnet_v1_50/resnet_v1_50/conv1/Conv2D (defined at ssd_img.py:18) ]]
         [[map/while/LoopCond/_454]]
0 successful operations.
0 derived errors ignored. [Op:__inference_pruned_7079]

Function call stack:
pruned -> pruned


Exception in thread Thread-15:
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""ssd_img.py"", line 87, in call
    res = detector_prediction(i)
  File ""ssd_img.py"", line 38, in detector_prediction
    output = detection_model(img_array) #get list of tensors discussed above as output
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1669, in __call__
    return self._call_impl(args, kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 247, in _call_impl
    args, kwargs, cancellation_manager)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1687, in _call_impl
    return self._call_with_flat_signature(args, kwargs, cancellation_manager)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1736, in _call_with_flat_signature
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1919, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 560, in call
    ctx=ctx)
  File ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.
  (0) Not found:  No algorithm worked!
         [[node FirstStageFeatureExtractor/resnet_v1_50/resnet_v1_50/conv1/Conv2D (defined at ssd_img.py:18) ]]
  (1) Not found:  No algorithm worked!
         [[node FirstStageFeatureExtractor/resnet_v1_50/resnet_v1_50/conv1/Conv2D (defined at ssd_img.py:18) ]]
         [[map/while/LoopCond/_454]]
0 successful operations.
0 derived errors ignored. [Op:__inference_pruned_7079]

Function call stack:
pruned -> pruned
"
52197,tf.distribute.MultiWorkerStrategy - Don't mirror models on workers.,"I am exploring multi-worker training with Tensorflow and I'm at the point where I would need a `tf.distribute.MultiWorkerStrategy`. Compare `tf.distribute.Strategy` with the `tf.distribute.MirroredStrategy`.

I do not want the model to get mirrored on each GPU of each worker, instead I want the model to get mirrored on each worker.

Is there a way I can to that? Would be possible to implement tf.distribute.MultiWorkerStrategy` by myself (without too much effort)?"
52196,TensorRT conversion via docker image 2.6.0-gpu - getInferLibVersion symbol not found,"Trying to convert a tf.keras model to tftrt.


- CUDA/cuDNN version: 11.2
- GPU model and memory: GeForce RTX 2070
v2.6.0-rc2-32-g919f693420e 2.6.0

**Describe the current behavior**
```
 from tensorflow.python.compiler.tensorrt import trt_convert as trt
    precision = 'FP16'
    params = tf.experimental.tensorrt.ConversionParams(precision_mode=precision, use_calibration=False, minimum_segment_size=3)
    converter = trt.TrtGraphConverterV2(input_saved_model_dir=model_path, conversion_params=params)
    calibration_input_fn_arg = None
    converter.convert(calibration_input_fn_arg)
    converter.save(trt_save_path)
```
**Describe the expected behavior**
2021-09-30 09:41:30.493064: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-09-30 09:41:30.493081: F tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:49] getInferLibVersion symbol not found.
Aborted (core dumped)
"
52195,Support torch.nn.fold in tensorflow?,"This function is popular in Vision Models. 

I don't find any api function in tf that is equivalent with [torch.nn.fold](https://pytorch.org/docs/stable/generated/torch.nn.Fold.html?highlight=fold#).

So does Tensowflow already have such a function or is there a plan to do so?
"
52194,Object Detection Android App Crash,"I am trying to convert my custom `MobileNet Single Shot Detector (v2)`  TF 1.x to `tflite` using [Roboflow tutorial colab](https://colab.research.google.com/drive/1qXn9q6m5ug7EWJsJov6mHaotHhCUY-wG?usp=sharing). After Conversion I deploy it on [Tensorflow android app demo ](https://github.com/tensorflow/examples.git). When I run the app it always crashes after lunch immediately showing the following error:
'''
E/AndroidRuntime: FATAL EXCEPTION: main
Process: org.tensorflow.lite.examples.detection, PID: 10743
java.lang.AssertionError: Error occurred when initializing ObjectDetector: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.
at org.tensorflow.lite.task.vision.detector.ObjectDetector.initJniWithByteBuffer(Native Method)
at org.tensorflow.lite.task.vision.detector.ObjectDetector.access$100(ObjectDetector.java:86)
at org.tensorflow.lite.task.vision.detector.ObjectDetector$3.createHandle(ObjectDetector.java:211)
at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:91)
at org.tensorflow.lite.task.vision.detector.ObjectDetector.createFromBufferAndOptions(ObjectDetector.java:207)
at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.(TFLiteObjectDetectionAPIModel.java:87)
at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:81)
at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)
at org.tensorflow.lite.examples.detection.CameraActivity$7.onPreviewSizeChosen(CameraActivity.java:446)
at org.tensorflow.lite.examples.detection.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:357)
at org.tensorflow.lite.examples.detection.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:362)
at org.tensorflow.lite.examples.detection.CameraConnectionFragment.access$300(CameraConnectionFragment.java:66)
at org.tensorflow.lite.examples.detection.CameraConnectionFragment$3.onSurfaceTextureAvailable(CameraConnectionFragment.java:171)
at android.view.TextureView.getTextureLayer(TextureView.java:415)
at android.view.TextureView.draw(TextureView.java:360)
at android.view.View.updateDisplayListIfDirty(View.java:21389)
at android.view.View.draw(View.java:22254)
at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
at android.view.View.updateDisplayListIfDirty(View.java:21380)
at android.view.View.draw(View.java:22254)
at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
at android.view.View.updateDisplayListIfDirty(View.java:21380)
at android.view.View.draw(View.java:22254)
at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
at android.view.View.draw(View.java:22538)
at android.view.View.updateDisplayListIfDirty(View.java:21389)
at android.view.View.draw(View.java:22254)
at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
at androidx.coordinatorlayout.widget.CoordinatorLayout.drawChild(CoordinatorLayout.java:1277)
at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
at android.view.View.draw(View.java:22538)
at android.view.View.updateDisplayListIfDirty(View.java:21389)
at android.view.View.draw(View.java:22254)
at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
at android.view.View.updateDisplayListIfDirty(View.java:21380)
at android.view.View.draw(View.java:22254)
at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
at android.view.View.updateDisplayListIfDirty(View.java:21380)
at android.view.View.draw(View.java:22254)
at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
at android.view.View.updateDisplayListIfDirty(View.java:21380)
at android.view.View.draw(View.java:22254)
at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
at android.view.View.updateDisplayListIfDirty(View.java:21380)
at android.view.View.draw(View.java:22254)
at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
at android.view.View.draw(View.java:22538)
at com.android.internal.policy.DecorView.draw(DecorView.java:848)
at android.view.View.updateDisplayListIfDirty(View.java:21389)
at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:559)
at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:565)
at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:647)
at android.view.ViewRootImpl.draw(ViewRootImpl.java:4417)
at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:4144)
at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:3391)
at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:2182)
at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:8730)
at android.view.Choreographer$CallbackRecord.run(Choreographer.java:1352)
at android.view.Choreographer.doCallbacks(Choreographer.java:1149)
at android.view.Choreographer.doFrame(Choreographer.java:1049)
at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:1333)
at android.os.Handler.handleCallback(Handler.java:938)
at android.os.Handler.dispatchMessage(Handler.java:99)
at android.os.Looper.loop(Looper.java:233)
at android.app.ActivityThread.main(ActivityThread.java:8010)
at java.lang.reflect.Method.invoke(Native Method)
at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:631)
at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:978)
I/Process: Sending signal. PID: 10743 SIG: 9
'''"
52193,API to explicitly label python input arguments in ``tf.function``,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tf 2.5.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

In Jax, ``tf.function``'s equivalent ``jax.jit`` has an argument ``static_argnums=(1, )``, which explicitly tells the compiler that the second(1) argument of the function to be jitted is a python object instead of a tensor, and the function should be retraced for each different argument in ``static_argnums``.

It seems to me that, ``tf.function`` has no corresponding API. Though, in general case, the function wrapped by ``tf.function`` can automatically determine whether the input is a tensor or a python object. Such automatic control fails when the function jitted is a function with ``tf.custom_gradient``. Please see the demo below.

```python
import tensorflow as tf
import numpy as np

class TensorWrapper:
    def __init__(self, tensor):
        self._tensor = tensor  # suppose we have no way to directly access the tensor

    def plus(self, other):
        # only public API
        return other + self._tensor

    def __str__(self):
        return ""TensorWrapper("" + self._tensor.__str__() + "")""

    __repr__ = __str__


TW_list_tf = list(map(TensorWrapper, [tf.zeros([2, 2]), tf.ones([2, 2])]))

print(TW_list_tf)


def u_tf(a, b):
    return tf.reduce_sum(TW_list_tf[b].plus(a))

print(""u_tf "", u_tf(tf.zeros([2, 2]), 1))

u_tf_jit = tf.function(u_tf)

print(""u_tf_jit"", u_tf_jit(tf.zeros([2, 2]), 1))

a = tf.zeros([2, 2])
with tf.GradientTape() as tape:
    tape.watch(a)
    loss = u_tf(a, 1)
print(""u_tf grad "", tape.gradient(loss, a))

a = tf.zeros([2, 2])
with tf.GradientTape() as tape:
    tape.watch(a)
    loss = u_tf_jit(a, 1)
print(""u_tf_jit grad "", loss, tape.gradient(loss, a))

@tf.custom_gradient
def u_tf_grad_v2(a, b):
    r = u_tf(a, b)

    def grad(dr):
        return 2.0 * dr * tf.ones_like(a), tf.zeros_like(b)

    return r, grad

a = tf.zeros([2, 2])
with tf.GradientTape() as tape:
    tape.watch(a)
    loss = u_tf_grad_v2(a, 1)
print(""u_tf_grad_v2 grad "", tape.gradient(loss, a))

u_tf_grad_v2_jit = tf.function(u_tf_grad_v2)

try:
    a = tf.zeros([2, 2])
    with tf.GradientTape() as tape:
        tape.watch(a)
        loss = u_tf_grad_v2_jit(a, 1)
    print(loss, tape.gradient(loss, a))
except Exception as e:
    print(""u_tf_grad_v2_jit grad:"", e)
# list indices must be integers or slices, not Tensor since int b is compiled to tensor
```

There are two ways to solve the final exception. Add ``nondiff_argnums`` arguments in ``tf.custom_gradient`` API, or better add ``static_argnums`` arguments in ``tf.function`` API. Or is there currently any workaround for this ``tf.custom_gradient`` + ``tf.function`` + non tensor input issue?


**Will this change the current api? How?**
Yes, a new arguments ``static_argnums=`` should be added in ``tf.function()`` API.

**Who will benefit with this feature?**
Everyone wants better control on the jit behavior in tf and enjoys writting highly customized code with tf.

**Any Other info.**
"
52192,TF not using GPU for models,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Latest Windows 10
- TensorFlow installed from (source or binary): PIP
- TensorFlow version (use command below): 2.6.0
- Python version: 3.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4 CUDA, 8.1.0
- GPU model and memory: 3060 12gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Tensorflow uses the cpu when training models and not the gpu. I have installed Cuda and cudnn properly, and TensorFlow confirms the loading of cudnn lib. When I train models, the gpu fan doesn't spin, nor does it heat up, and the cpu shows heavy usage in task manager
**Describe the expected behavior**
TF uses gpu and not cpu
**Standalone code to reproduce the issue**
this is the can notebook I am using:
https://www.tensorflow.org/tutorials/images/cnn

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52190,tf.constant's dtype causes exception after model load in tf.math.pow and other math_ops,"I ran into this issue using tensorflow 2.5.1, and now tensorflow 2.6.0 after just updating.  Here's the abridged code:

```
def test_model():
   inputs = tf.keras.layers.Input(shape=(10, 10, 1), name=""input_1"") 

   a = tf.cast(2, dtype=tf.int64)
   b = tf.cast(inputs, dtype=tf.int64)

   outputs = tf.math.pow(a, b)

   model = tf.keras.models.Model(inputs, outputs)
   return model

if __name__ == '__main__':
    model =  test_model()
    save_path = ""./test_model""
    tf.keras.models.save_model(model, save_path, save_format=""tf"")
    model = tf.keras.models.load_model(save_path)
```

Which produces the following exception:s
`ValueError: Tensor conversion requested dtype int32 for Tensor with dtype int64: <tf.Tensor 'Placeholder:0' shape=(None, 6, 6, 1) dtype=int64>`
`TypeError: Input 'y' of 'Pow' Op has type int64 that does not match type int32 of argument 'x'.
`

These exceptions are thrown only after loading the model, the model works completely fine if not saved. Specifying the dtype of the Input layer, rather than casting `inputs`, does not seem to make a difference either. 
I've tried changing the dtypes of the both `a` and `b`, but every combination appears to throw this same exception. Can anyone recreate this issue?

I assume there generally isn't an issue with using math ops in a functional model in this way? Is the only way to do this currently by subclassing tf.Module? 

"
52184,AttributeError: 'Conv2D' object has no attribute 'shape' and ValueError: You are trying to load a weight file containing 1 layers into a model with 19 layers.,"TENSORFLOW VERSION: 2.1.0
Hey there i am trying to load a model like the following - 
Below is my model architecture - 
```py
def down_block(x, filters, kernel_size=(3, 3), padding=""same"", strides=1, input = False):
    if input == False:
        c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
    else:
        c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"", input_shape=(HEIGHT, WIDTH, 3))
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(c)
    p = MaxPool2D((2, 2), (2, 2))(c)
    return c, p

def up_block(x, skip, filters, kernel_size=(3, 3), padding=""same"", strides=1):
    us = UpSampling2D((2, 2))(x)
    concat = Concatenate()([us, skip])
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(concat)
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(c)
    return c

def bottleneck(x, filters, kernel_size=(3, 3), padding=""same"", strides=1):
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(c)
    return c

def UNet():
    f = [16, 32, 64, 128, 256]
 
    

    c1, p1 = down_block(None, f[0], input = True) #128 -> 64
    c2, p2 = down_block(p1, f[1]) #64 -> 32
    c3, p3 = down_block(p2, f[2]) #32 -> 16
    c4, p4 = down_block(p3, f[3]) #16->8
    
    bn = bottleneck(p4, f[4])
    
    u1 = up_block(bn, c4, f[3]) #8 -> 16
    u2 = up_block(u1, c3, f[2]) #16 -> 32
    u3 = up_block(u2, c2, f[1]) #32 -> 64
    u4 = up_block(u3, c1, f[0]) #64 -> 128
    
    outputs = Conv2D(13, (1, 1), padding=""same"", activation=""sigmoid"")(u4)
    model = Model(inputs, outputs)
    return model
```
Then i make a variable named unet and pass in the model architecture - 
```py
unet = UNet()
unet.compile(optimizer=""adam"", loss=""categorical_crossentropy"", metrics=[""acc""])
unet.summary()
```

I get the following error - 
```py
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-26-33ba33cd9519> in <module>
----> 1 unet = UNet()
      2 unet.compile(optimizer=""adam"", loss=""categorical_crossentropy"", metrics=[""acc""])
      3 unet.summary()

<ipython-input-25-6aea0c9181eb> in UNet()
     25 
     26 
---> 27     c1, p1 = down_block(None, f[0], input = True) #128 -> 64
     28     c2, p2 = down_block(p1, f[1]) #64 -> 32
     29     c3, p3 = down_block(p2, f[2]) #32 -> 16

AttributeError: 'Conv2D' object has no attribute 'shape'
```

Full traceback below -
https://pastebin.com/mkTSg4bS

by the way originally i started out with the following code the thing is the actual code was -
```py
def down_block(x, filters, kernel_size=(3, 3), padding=""same"", strides=1):
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(c)
    p = MaxPool2D((2, 2), (2, 2))(c)
    return c, p

def up_block(x, skip, filters, kernel_size=(3, 3), padding=""same"", strides=1):
    us = UpSampling2D((2, 2))(x)
    concat = Concatenate()([us, skip])
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(concat)
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(c)
    return c

def bottleneck(x, filters, kernel_size=(3, 3), padding=""same"", strides=1):
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(c)
    return c

def UNet():
    f = [16, 32, 64, 128, 256]
    inputs = Input((HEIGHT,WIDTH,3))
    
    p0 = inputs
    c1, p1 = down_block(p0, f[0]) #128 -> 64
    c2, p2 = down_block(p1, f[1]) #64 -> 32
    c3, p3 = down_block(p2, f[2]) #32 -> 16
    c4, p4 = down_block(p3, f[3]) #16->8
    
    bn = bottleneck(p4, f[4])
    
    u1 = up_block(bn, c4, f[3]) #8 -> 16
    u2 = up_block(u1, c3, f[2]) #16 -> 32
    u3 = up_block(u2, c2, f[1]) #32 -> 64
    u4 = up_block(u3, c1, f[0]) #64 -> 128
    
    outputs = Conv2D(13, (1, 1), padding=""same"", activation=""sigmoid"")(u4)
    model = Model(inputs, outputs)
    return model
```
When i would try to load my model using this code- 
```py
unet = UNet()
unet.compile(optimizer=""adam"", loss=""categorical_crossentropy"", metrics=[""acc""])
unet.summary()

# unet.load_weights('unet1m.h5')
unet.load_weights('unet.h5')
```
 i would get the following  errro -
```
ValueError: You are trying to load a weight file containing 1 layers into a model with 19 layers.
```
Then i found this issue on github [ISSUE](https://github.com/keras-team/keras/issues/10417#issuecomment-415905336).
 It suggested not to use an input layer instead use input_shape in conv2d 
so i edited the code as you see in the original post
i did not want to modify my whole code so came up with a messy solution
i added an argument named input
then in the function i wrote that if input = False just do the bussiness it was doing in the original code
if  it was input = true we add that extra input_shape argument in Conv2D 
Now for not to remove the x parameter i just simply passed x as none in the input block
Then i found this issue on github. It suggested not to use an input layer instead use input_shape in conv2d 
so i edited the code as you see at the top

**FOR EXPERIMENTING WITH THE CODE **
The model -
[MODEL](https://drive.google.com/file/d/1RaRn9eI40ZDXAX-ajnCT0oWJclR8w9RF/view)
The script - 
[SCRIPT](https://gist.github.com/Space-Fighter/53d7a25dd50278c9106103dc6bb5afbd)
The training script - 
[TRAINING SCRIPT](https://github.com/srihari-humbarwadi/cityscapes-segmentation-with-Unet/blob/master/batch_training.py)
The repositry from where i have taken the model and code - 
[REPOSITRY](https://github.com/srihari-humbarwadi/cityscapes-segmentation-with-Unet)"
52183,Object Detection Android App Crash ,"I am trying to convert my custom  `MobileNet Single Shot Detector (v2)` to `tflite` using Roboflow tutorial colab (https://colab.research.google.com/drive/1qXn9q6m5ug7EWJsJov6mHaotHhCUY-wG?usp=sharing). After Conversion I deploy it on Tensorflow android app demo (https://github.com/tensorflow/examples.git). When I run the app it always crashes after lunch immediately showing the following error:

E/AndroidRuntime: FATAL EXCEPTION: main
    Process: org.tensorflow.lite.examples.detection, PID: 10743
    java.lang.AssertionError: Error occurred when initializing ObjectDetector: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.
        at org.tensorflow.lite.task.vision.detector.ObjectDetector.initJniWithByteBuffer(Native Method)
        at org.tensorflow.lite.task.vision.detector.ObjectDetector.access$100(ObjectDetector.java:86)
        at org.tensorflow.lite.task.vision.detector.ObjectDetector$3.createHandle(ObjectDetector.java:211)
        at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:91)
        at org.tensorflow.lite.task.vision.detector.ObjectDetector.createFromBufferAndOptions(ObjectDetector.java:207)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.<init>(TFLiteObjectDetectionAPIModel.java:87)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:81)
        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)
        at org.tensorflow.lite.examples.detection.CameraActivity$7.onPreviewSizeChosen(CameraActivity.java:446)
        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:357)
        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:362)
        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.access$300(CameraConnectionFragment.java:66)
        at org.tensorflow.lite.examples.detection.CameraConnectionFragment$3.onSurfaceTextureAvailable(CameraConnectionFragment.java:171)
        at android.view.TextureView.getTextureLayer(TextureView.java:415)
        at android.view.TextureView.draw(TextureView.java:360)
        at android.view.View.updateDisplayListIfDirty(View.java:21389)
        at android.view.View.draw(View.java:22254)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
        at android.view.View.updateDisplayListIfDirty(View.java:21380)
        at android.view.View.draw(View.java:22254)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
        at android.view.View.updateDisplayListIfDirty(View.java:21380)
        at android.view.View.draw(View.java:22254)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
        at android.view.View.draw(View.java:22538)
        at android.view.View.updateDisplayListIfDirty(View.java:21389)
        at android.view.View.draw(View.java:22254)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
        at androidx.coordinatorlayout.widget.CoordinatorLayout.drawChild(CoordinatorLayout.java:1277)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
        at android.view.View.draw(View.java:22538)
        at android.view.View.updateDisplayListIfDirty(View.java:21389)
        at android.view.View.draw(View.java:22254)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
        at android.view.View.updateDisplayListIfDirty(View.java:21380)
        at android.view.View.draw(View.java:22254)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
        at android.view.View.updateDisplayListIfDirty(View.java:21380)
        at android.view.View.draw(View.java:22254)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
        at android.view.View.updateDisplayListIfDirty(View.java:21380)
        at android.view.View.draw(View.java:22254)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
        at android.view.View.updateDisplayListIfDirty(View.java:21380)
        at android.view.View.draw(View.java:22254)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)
        at android.view.View.draw(View.java:22538)
        at com.android.internal.policy.DecorView.draw(DecorView.java:848)
        at android.view.View.updateDisplayListIfDirty(View.java:21389)
        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:559)
        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:565)
        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:647)
        at android.view.ViewRootImpl.draw(ViewRootImpl.java:4417)
        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:4144)
        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:3391)
        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:2182)
        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:8730)
        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:1352)
        at android.view.Choreographer.doCallbacks(Choreographer.java:1149)
        at android.view.Choreographer.doFrame(Choreographer.java:1049)
        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:1333)
        at android.os.Handler.handleCallback(Handler.java:938)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:233)
        at android.app.ActivityThread.main(ActivityThread.java:8010)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:631)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:978)
I/Process: Sending signal. PID: 10743 SIG: 9"
52182,Didn't find op for builtin opcode 'QUANTIZE' version '2',"### 1. System information

- TensorFlow installation (pip package or built from source): 2.5.0

#### 2. Description 
**I have tried to import a TensorFlow lite model using int8 quantization into a Nucleo l496zg and I received the following error.**  


### 3. Error
""Didn't find op for builtin opcode 'QUANTIZE' version '2'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?""

Failed to get registration from op code QUANTIZE"
52180,Tensorflow GPU 2.4 from source - fatal error in compilation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): W10 Pro 21H1
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4
- Python version: 3.7.8 64-bits
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0/8
- GPU model and memory: Nvidia Quadro K4200


**Describe the problem**

I try to compile Tensorflow 2.4 from source for GPU compute capability 3.0 compatibility. I run into a weird fail (log below) I don't understand and for wich I don't find any ressource online. It seems to be related to Cython maybe.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

From https://www.tensorflow.org/install/source_windows

```git clone https://github.com/tensorflow/tensorflow.git
git checkout r2.4
.\configure.py
bazel build --config=opt --copt=-nvcc_options=disable-warnings --copt=-DTF_EXTRA_CUDA_CAPABILITIES=3.0 --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**

.configure.py log
```
PS C:\Users\*username*\tensorflow> C:\Users\*username*\AppData\Local\Programs\Python\Python37\python.exe .\configure.py
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is C:\Users\*username*\AppData\Local\Programs\Python\Python37\python.exe]:


Found possible Python library paths:
  C:\Users\*username*\AppData\Local\Programs\Python\Python37\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\*username*\AppData\Local\Programs\Python\Python37\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 11.0 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include
Found cuDNN 8 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.0


WARNING: XLA does not support CUDA compute capabilities lower than 3.5. Disable XLA when running on older GPUs.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
```

Build log with the error
```
PS C:\Users\*username*\tensorflow> bazel build --config=opt --copt=-nvcc_options=disable-warnings --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
WARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=209
INFO: Reading rc options for 'build' from c:\users\*username*\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/*username*/AppData/Local/Programs/Python/Python39/python.exe
INFO: Reading rc options for 'build' from c:\users\*username*\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from c:\users\*username*\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/*username*/AppData/Local/Programs/Python/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Users/*username*/AppData/Local/Programs/Python/Python37/lib/site-packages --python_path=C:/Users/*username*/AppData/Local/Programs/Python/Python37/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file c:\users\*username*\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\*username*\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file c:\users\*username*\tensorflow\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file c:\users\*username*\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\users\*username*\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:opt in file c:\users\*username*\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX --define with_default_optimizations=true
INFO: Found applicable config definition build:cuda in file c:\users\*username*\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\users\*username*\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:windows in file c:\users\*username*\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\*username*\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Build option --copt has changed, discarding analysis cache.
INFO: Repository cython instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule tf_http_archive defined at:
  C:/users/*username*/tensorflow/third_party/repo.bzl:131:19: in <toplevel>
INFO: Repository 'cython' used the following cache hits instead of downloading the corresponding file.
 * Hash 'bccc9aa050ea02595b2440188813b936eaf345e85fb9692790cecfe095cf91aa' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/cython/cython/archive/0.28.4.tar.gz
If the definition of 'cython' was updated, verify that the hashes were also updated.
ERROR: An error occurred during the fetch of repository 'cython':
   Traceback (most recent call last):
        File ""C:/users/*username*/tensorflow/third_party/repo.bzl"", line 105
                _apply_delete(ctx, <1 more arguments>)
        File ""C:/users/*username*/tensorflow/third_party/repo.bzl"", line 74, in _apply_delete
                _execute_and_check_ret_code(ctx, <1 more arguments>)
        File ""C:/users/*username*/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(<1 more arguments>)
Non-zero return code(256) when executing 'C:\msys64\usr\bin\bash.exe -l -c ""rm"" ""-rf"" ""C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/cython/BUILD.bazel""':
Stdout:
Stderr:       1 [main] bash (14200) C:\msys64\usr\bin\bash.exe: *** fatal error - add_item (""\??\C:\msys64"", ""/"", ...) failed, errno 1
Stack trace:
Frame        Function    Args
000FFFF8630  00180063085 (00180297142, 00180272E41, 0000000003F, 000FFFF8B10)
000FFFF8B60  001800488F2 (000FFFFFFFF, 00180020010, 000FFFFABCA, 000FFFF9BB0)
000FFFF9B70  00180048931 (000FFFF9BB0, 00000000001, 0000000003F, 00000000001)
000FFFF9C00  001800E658D (00000000000, 00140000024, 00000000000, 000FFFFCC50)
000FFFFCC70  00180136D95 (00000000000, 00000000000, 00000000000, 00000000000)
000FFFFCCE0  00180048F75 (00000000000, 00000000000, 00000000000, 00000000000)
000FFFFCDA0  0018004794A (00000000000, 00000000000, 00000000000, 00000000000)
000FFFFCE50  00180047A0C (00000000000, 00000000000, 00000000000, 00000000000)
End of stack trace
ERROR: C:/users/*username*/tensorflow/tensorflow/python/BUILD:7748:1: //tensorflow/python:framework/fast_tensor_util.pyx_cython_translation depends on @cython//:cython_binary in repository @cython which failed to fetch. no such package '@cython//': Traceback (most recent call last):
        File ""C:/users/*username*/tensorflow/third_party/repo.bzl"", line 105
                _apply_delete(ctx, <1 more arguments>)
        File ""C:/users/*username*/tensorflow/third_party/repo.bzl"", line 74, in _apply_delete
                _execute_and_check_ret_code(ctx, <1 more arguments>)
        File ""C:/users/*username*/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(<1 more arguments>)
Non-zero return code(256) when executing 'C:\msys64\usr\bin\bash.exe -l -c ""rm"" ""-rf"" ""C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/cython/BUILD.bazel""':
Stdout:
Stderr:       1 [main] bash (14200) C:\msys64\usr\bin\bash.exe: *** fatal error - add_item (""\??\C:\msys64"", ""/"", ...) failed, errno 1
Stack trace:
Frame        Function    Args
000FFFF8630  00180063085 (00180297142, 00180272E41, 0000000003F, 000FFFF8B10)
000FFFF8B60  001800488F2 (000FFFFFFFF, 00180020010, 000FFFFABCA, 000FFFF9BB0)
000FFFF9B70  00180048931 (000FFFF9BB0, 00000000001, 0000000003F, 00000000001)
000FFFF9C00  001800E658D (00000000000, 00140000024, 00000000000, 000FFFFCC50)
000FFFFCC70  00180136D95 (00000000000, 00000000000, 00000000000, 00000000000)
000FFFFCCE0  00180048F75 (00000000000, 00000000000, 00000000000, 00000000000)
000FFFFCDA0  0018004794A (00000000000, 00000000000, 00000000000, 00000000000)
000FFFFCE50  00180047A0C (00000000000, 00000000000, 00000000000, 00000000000)
End of stack trace
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@cython//': Traceback (most recent call last):
        File ""C:/users/*username*/tensorflow/third_party/repo.bzl"", line 105
                _apply_delete(ctx, <1 more arguments>)
        File ""C:/users/*username*/tensorflow/third_party/repo.bzl"", line 74, in _apply_delete
                _execute_and_check_ret_code(ctx, <1 more arguments>)
        File ""C:/users/*username*/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(<1 more arguments>)
Non-zero return code(256) when executing 'C:\msys64\usr\bin\bash.exe -l -c ""rm"" ""-rf"" ""C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/cython/BUILD.bazel""':
Stdout:
Stderr:       1 [main] bash (14200) C:\msys64\usr\bin\bash.exe: *** fatal error - add_item (""\??\C:\msys64"", ""/"", ...) failed, errno 1
Stack trace:
Frame        Function    Args
000FFFF8630  00180063085 (00180297142, 00180272E41, 0000000003F, 000FFFF8B10)
000FFFF8B60  001800488F2 (000FFFFFFFF, 00180020010, 000FFFFABCA, 000FFFF9BB0)
000FFFF9B70  00180048931 (000FFFF9BB0, 00000000001, 0000000003F, 00000000001)
000FFFF9C00  001800E658D (00000000000, 00140000024, 00000000000, 000FFFFCC50)
000FFFFCC70  00180136D95 (00000000000, 00000000000, 00000000000, 00000000000)
000FFFFCCE0  00180048F75 (00000000000, 00000000000, 00000000000, 00000000000)
000FFFFCDA0  0018004794A (00000000000, 00000000000, 00000000000, 00000000000)
000FFFFCE50  00180047A0C (00000000000, 00000000000, 00000000000, 00000000000)
End of stack trace
INFO: Elapsed time: 26.147s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (2 packages loaded, 15684 targets configured)
    Fetching @llvm-project; fetching 24s
    Fetching @local_config_git; fetching 24s
    Fetching ...me/*username*/_bazel_*username*/feynteyc/external/llvm-project; Extracting C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/llvm-project/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz 22s
    Fetching @aws; fetching 22s
    Fetching C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/aws; Extracting C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/aws/1.7.336.tar.gz 22s
``"
52179,FloorMod on gpu,"```
^f41959ccb2 (Manjunath Kudlur       2015-11-06 16:27:58 -0800 20) namespace tensorflow {
^f41959ccb2 (Manjunath Kudlur       2015-11-06 16:27:58 -0800 21) namespace functor {
bdae9c62caa (Andrew Selle           2016-10-18 16:08:51 -0800 22) // TODO(b/32239807) No GPU ops for mod yet.
^f41959ccb2 (Manjunath Kudlur       2015-11-06 16:27:58 -0800 23) }  // namespace functor
^f41959ccb2 (Manjunath Kudlur       2015-11-06 16:27:58 -0800 24) }  // namespace tensorflow
^f41959ccb2 (Manjunath Kudlur       2015-11-06 16:27:58 -0800 25)
```
TODO(b/32239807) on **2016-10-18**, and now **2021-9-23**
When will we develop this feature?
"
52178,Different prediction on GPU between `tf.keras.models.load_model(..)` and `tf.saved_model.load(..)`,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.2
- CUDA/cuDNN version: 11.0
- GPU model and memory: GeForce GTX 1050, 4 GB

**Describe the current behavior**

I have a CNN based regression model. Surprisingly, the model is predicting different outputs when loaded with `tf.keras.models.load_model(..)` and `tf.saved_model.load(..)`. However, this only occurs when I am using a GPU and also not always but ~5% of inference times. On CPU, they both produce the same outputs always. The difference is rather small, happens after `1e-7` but still big for my use case.

**Describe the expected behavior**
Irrespective of the loading method and whether GPU is used for inference or not, predicted values should always be the same.

**Standalone code to reproduce the issue**
The link to colab: `https://colab.research.google.com/drive/1JwXNx-MbVqB7HDXF4z9lqa91oWsfvpA0?usp=sharing`

Colab uses different TF and python versions but the issue still exists.

**Other info / logs** 
`AssertionError: [[0.12652352]] and [[0.12652355]]`"
52176,"Running with multiple GPUs, the model has been successfully loaded into the GPUs, but the program is stuck and there is no error message","**The issue:**
Running with multiple GPUs, the model has been successfully loaded into the GPUs, but the program is stuck and there is no error message.

**Program running status:**
[Program running status](https://drive.google.com/file/d/1i0e1ONe3WQ1qb5Ol_0b9CZWvf2qRqW9k/view?usp=sharing)

**Execution environment:**
OS: Ubuntu 18.04.5 LTS
GPU: NVIDIA RTX A6000 * 2
TF Version:  tensorflow-gpu  2.4.0
Python Version: python 3.8.8
CUDA Version: V11.1.74
CuDNN Version: 8.1.0 (the 8.0.4 is also tried)

**Program for running:**
```
import tensorflow as tf
#from tensorflow.keras.utils import multi_gpu_model
from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model
tf.debugging.set_log_device_placement(True)
mirrored_strategy = tf.distribute.MirroredStrategy(devices=[""GPU:0"", ""GPU:1""])

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

with mirrored_strategy.scope():
    model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.BatchNormalization(renorm=False),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(10)
    ])

    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)


    model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)

```

The program is stuck and there is no progress. However, GPUs resources (memory) have been used by the program.
The same situation also appears in the jax/flax pmap program, but pytorch (nn.DataParallel) can be executed correctly.
The problem is as above, Thanks!


**Running log:**
[Running log](https://drive.google.com/file/d/1wNXoyNjrpv6LFV3_WRtHYREnKJ_RbTSh/view?usp=sharing)
"
52175,Gradients are zero ,"I have an ANN to classify food, however it doesn't work, i kept debugging till i found that the problem is with the weights aren't updated, so i printed the grads to find out that they are always zero except for the first time

Here is the code
   `def forward(x):
  
        return tf.matmul(x,W) + b`

   `def activate(x):
        return tf.nn.softmax(forward(x))`

   `def model(x):
   
        x = flatten(x)
    
        return activate(x)`

    `def cross_entropy(y_label, y_pred):
  
         return (-tf.reduce_sum(y_label * tf.math.log(y_pred + 1.e-10)))`

    `optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)`

```
  `def train_step(x, y ):
             with tf.GradientTape() as tape:
             #compute loss function
             current_loss = cross_entropy( y, model(x))
             print(current_loss)
             # compute gradient of loss 
            #(This is automatic! Even with specialized funcctions!)
            grads = tape.gradient( current_loss , [W,b])
            # Apply SGD step to our Variables W and b
            print(grads)
        
            optimizer.apply_gradients( zip( grads , [W,b] ) )   
        
           return current_loss.numpy()`
```

    `W = tf.Variable(tf.zeros([196608, 3],tf.float32))
      #Bias tensor
       b = tf.Variable(tf.zeros([3],tf.float32))

       loss_values=[]
       accuracies = []
       epochs = 10
       x_train = np.empty([50,256,256,3])
       y_train = np.empty([50,1,3])

    for i in range(epochs):
         j=0
         k = 0
        # each batch has 50 examples
        for x_train_batch, y_train_batch in train_ds:
                    if j == 0 and i == 0 :
                        for k in range(50):
                             x_train[k] = x_train_batch[k]
                             y_train[k] = y_train_batch[k]
                        
            
        
                    j+=1
                    current_loss = train_step(x_train_batch/255.0, y_train_batch)
                     if j%500 == 0: #reporting intermittent batch statistics
                         print(""epoch "", str(i), ""batch"", str(j), ""loss:"", str(current_loss) ) 
            
        x_train = tf.convert_to_tensor(x_train)
        y_train = tf.convert_to_tensor(y_train)`

I also checked for the current lose (if you notice it's the first output just before the gradient) and it isn't zero as you see
OUTPUT:
`tf.Tensor(54.930614, shape=(), dtype=float32)
[<tf.Tensor: shape=(196608, 3), dtype=float32, numpy=
array([[ 2.7033854,  1.9847847, -4.6881704],
       [ 2.6230955,  1.6556438, -4.278739 ],
       [ 2.289314 ,  1.2310266, -3.5203404],
       ...,
       [ 1.6812016,  1.4880723, -3.1692739],
       [ 1.6665692,  1.2746813, -2.9412503],
       [ 1.73799  ,  0.5771449, -2.3151352]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 2.6666658,  6.6666684, -9.333334 ], dtype=float32)>]
tf.Tensor(483.5429, shape=(), dtype=float32)
[<tf.Tensor: shape=(196608, 3), dtype=float32, numpy=
array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       ...,
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]
tf.Tensor(713.80145, shape=(), dtype=float32)
[<tf.Tensor: shape=(196608, 3), dtype=float32, numpy=
array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       ...,
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]
tf.Tensor(483.5429, shape=(), dtype=float32)
[<tf.Tensor: shape=(196608, 3), dtype=float32, numpy=
array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       ...,
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]
tf.Tensor(483.5429, shape=(), dtype=float32)
[<tf.Tensor: shape=(196608, 3), dtype=float32, numpy=
array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       ...,
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]`"
52174,Different results between model.fit() and model.predict(),"I get the problem when I try to fine-tuned my model.
Here is my `model.fit` code and training progress bar.

![2021-09-29 10-42-31 的螢幕擷圖](https://user-images.githubusercontent.com/42731603/135195990-85ea0501-fd37-4ac9-a575-48a719872c89.png)

![2021-09-29 10-42-46 的螢幕擷圖](https://user-images.githubusercontent.com/42731603/135196092-1e32fdae-5149-445c-a17c-0647512b970d.png)

I found the `keras.losses.CategoricalCrossentropy()` and `keras.metrics.CategoricalCrossentropy()` return different results when training epoch.

Moreover, I use the same validation data in the `model.predict` function, and check the AUC-score, **the socre in the final epoch** and **the score which compute manually** not the same. 

![2021-09-29 10-42-59 的螢幕擷圖](https://user-images.githubusercontent.com/42731603/135196679-a9245642-9a94-4227-94ea-61a3500a9c51.png)

And I research the similar [issue](https://github.com/keras-team/keras/issues/5140]) before, but I didn't found the solution.

Does anyone get the same problem?

OS and environment information:
- Ubuntu 20.04.3 LTS
- Conda virtual environment
- Cuda 11.2
- Nvidia-driver 460
- Tensorflow 2.5.0 (use Keras inside tensorflow)

I fix it, sorry.
The problem is some function using keras, another is tensorflow.keras.
"
52170,Document description error,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/lite/guide/python

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/

## Description of issue (what needs changing):
To convert other TensorFlow models to TensorFlow Lite, read about **the the** TensorFlow Lite Converter.
There's an extra word the

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links
This is my PR: https://github.com/tensorflow/tensorflow/pull/52169

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
52168,Tensorflow lite conversion error when converting the bidirectional LSTM,"### System information

-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.3.0
-   **Python version**: 3.8.10

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I am trying to convert a tensorflow model into the tensorflow lite format. It prompts that it failed to duplicate values for the stateful op when the conversion goes to the bidirectional lstm layer. Here below is the network structure.
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   

conv1d (Conv1D)              (None, 1558, 32)          5024      

max_pooling1d (MaxPooling1D) (None, 389, 32)           0       
  
batch_normalization (BatchNo (None, 389, 32)           128       

conv1d_1 (Conv1D)            (None, 386, 64)           8256      

max_pooling1d_1 (MaxPooling1 (None, 96, 64)            0         

batch_normalization_1 (Batch (None, 96, 64)            256       

bidirectional (Bidirectional (None, 50)                18000     

dense (Dense)                (None, 32)                1632      

dense_1 (Dense)              (None, 25)                825       

Total params: 34,121
Trainable params: 33,929
Non-trainable params: 192
__________________________

### Source code / logs
Here is the code to raised this error.
`converter = tf.lite.TFLiteConverter.from_saved_model(save_model_dir)`
`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                           tf.lite.OpsSet.SELECT_TF_OPS]`
` tflite_model = converter.convert()`
 `with open('converted_model.tflite', 'wb') as f:`
`        f.write(tflite_model)`

**Here below are the logs:**
```
2021-09-28 16:44:25.817579: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2021-09-28 16:44:25.817782: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
2021-09-28 16:44:25.819144: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: D:\VoiceRecognition\trained_model
2021-09-28 16:44:25.884916: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2021-09-28 16:44:25.885104: I tensorflow/cc/saved_model/loader.cc:234] Reading SavedModel debug info (if present) from: D:\VoiceRecognition\trained_model
2021-09-28 16:44:26.135034: I tensorflow/cc/saved_model/loader.cc:199] Restoring SavedModel bundle.
2021-09-28 16:44:26.476369: I tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: D:\VoiceRecognition\trained_model
2021-09-28 16:44:26.761966: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 942808 microseconds.
loc(callsite(callsite(callsite(unknown at ""sequential/bidirectional/backward_lstm/PartitionedCall@__inference__wrapped_model_44427"") at ""StatefulPartitionedCall@__inference_signature_wrapper_52710"") at ""StatefulPartitionedCall"")): error: We cannot duplicate the value since it's not constant.

error: Failed to duplicate values for the stateful op

Traceback (most recent call last):
  File ""C:\Users\lizai\anaconda3\envs\SpectrumAnalysis\lib\site-packages\tensorflow\lite\python\convert.py"", line 196, in toco_convert_protos
    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,
  File ""C:\Users\lizai\anaconda3\envs\SpectrumAnalysis\lib\site-packages\tensorflow\lite\python\wrap_toco.py"", line 32, in wrapped_toco_convert
    return _pywrap_toco_api.TocoConvert(
Exception: <unknown>:0: error: loc(callsite(callsite(callsite(unknown at ""sequential/bidirectional/backward_lstm/PartitionedCall@__inference__wrapped_model_44427"") at ""StatefulPartitionedCall@__inference_signature_wrapper_52710"") at ""StatefulPartitionedCall"")): We cannot duplicate the value since it's not constant.

<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(callsite(unknown at ""sequential/bidirectional/backward_lstm/PartitionedCall@__inference__wrapped_model_44427"") at ""StatefulPartitionedCall@__inference_signature_wrapper_52710"") at ""StatefulPartitionedCall"")): see current operation: %23 = ""tfl.unidirectional_sequence_lstm""(%22, %cst_20, %cst_21, %cst_22, %cst_23, %cst_12, %cst_13, %cst_14, %cst_15, %cst_7, %cst_7, %cst_7, %cst_16, %cst_17, %cst_18, %cst_19, %cst_7, %cst_7, %21, %21, %cst_7, %cst_7, %cst_7, %cst_7) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<?x96x64xf32>, tensor<25x64xf32>, tensor<25x64xf32>, tensor<25x64xf32>, tensor<25x64xf32>, tensor<25x25xf32>, tensor<25x25xf32>, tensor<25x25xf32>, tensor<25x25xf32>, none, none, none, tensor<25xf32>, tensor<25xf32>, tensor<25xf32>, tensor<25xf32>, none, none, tensor<?x25xf32>, tensor<?x25xf32>, none, none, none, none) -> tensor<?x96x25xf32>
<unknown>:0: error: Failed to duplicate values for the stateful op

```"
52167,tf.image.resize() behaves differently when wrapped in a tf.data.Dataset.map(),"`tf.image.resize()` gives different results if used as is (plain) or if wrapped in a `tf.data.Dataset.map()`, however, I except the output to be the same in both cases. The behaviour is shown in the output of the snipped below.

Specifically, this issue seems related to the specific interpolation method `tf.image.ResizeMethod.BILINEAR` (default) with uses `tensorflow.python.ops.gen_image_ops.resize_bilinear` under-the-hood. Changing the interp method (e.g. to `tf.image.ResizeMethod.BICUBIC`, or setting bilinear + `antialias=True`, which uses a different method) no longer show any differences.

Environment:
- Tensorflow 2.6.0
- Python 3.8.6
- Linux 20.04
- Driver Version: 460.32.03
- CUDA Version: 11.2
- cuDNN 8.1.0
- NVIDIA T4 GPU (16GB)

Reproducible script:
```

import tensorflow as tf
import numpy as np
from PIL import Image

print(tf.__version__)

@tf.function
def tf_read_and_resize_image(filename):
    image_string = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(image_string, channels=3)
    image=tf.cast(image, tf.float32)
    image = tf.image.resize(image, (224, 224), method=tf.image.ResizeMethod.BILINEAR)
    return image

# create random uint8 image
np.random.seed(42)
im = np.random.randint(low=0, high=255, size=(100, 100, 3), dtype=np.uint8)
im_filename = ""/tmp/test_image.jpg""
im = Image.fromarray(im).save(im_filename)

# plain function
plain_tensor = tf_read_and_resize_image(im_filename)

# wrapped in tf.data map()
ds = tf.data.Dataset.from_tensor_slices([im_filename])
ds = ds.map(tf_read_and_resize_image)
ds_tensor = next(ds.as_numpy_iterator())

assert plain_tensor.dtype == ds_tensor.dtype == ""float32""
assert plain_tensor.shape == ds_tensor.shape == (224, 224, 3)
np.testing.assert_array_equal(plain_tensor, ds_tensor) # fails with ~11% diff.
```

Output:
```
(tf_issue) root@f1f0235af85c:/srv/2D3Dreg/deeplearning# python test.py 
2.6.0
2021-09-28 14:10:08.987749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-28 14:10:08.997055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-28 14:10:08.997667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-28 14:10:08.998427: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-28 14:10:08.999018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-28 14:10:08.999644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-28 14:10:09.000204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-28 14:10:09.584861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-28 14:10:09.585487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-28 14:10:09.586072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-28 14:10:09.586636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13666 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5
2021-09-28 14:10:09.596141: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Traceback (most recent call last):
  File ""test.py"", line 30, in <module>
    np.testing.assert_array_equal(input_tensor_read, ds_read) # fails with ~11% diff.
  File ""/srv/2D3Dreg/deeplearning/tf_issue/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 930, in assert_array_equal
    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
  File ""/srv/2D3Dreg/deeplearning/tf_issue/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 840, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Arrays are not equal

Mismatched elements: 16381 / 150528 (10.9%)
Max absolute difference: 0.00051498
Max relative difference: 1.0005011e-05
 x: array([[[164.      , 208.      , 131.      ],
        [154.83928 , 198.83928 , 124.55357 ],
        [130.73215 , 174.73215 , 107.58929 ],...
 y: array([[[164.      , 208.      , 131.      ],
        [154.83928 , 198.83928 , 124.55357 ],
        [130.73215 , 174.73215 , 107.58929 ],...
```"
52166,Model evaluate accuracy drops following Save + Load,"After training EfficientNetB0 on a custom dataset, i want to ensure that the model i save give the same accuracy after i load it:

```
# Evaluate model
print(""After training: "")
model.evaluate(dataset)

# Save model
model.save('my_model.hdf5')

# Reload the saved model
new_model = tf.keras.models.load_model('my_model.hdf5')

# Evaluate again
print(""After saving and reloading: "")
new_model.evaluate(dataset)
```

Which gives the output :

```
After training : 
1000/1000 [==============================] - 20s 19ms/step - loss: 0.4617 - accuracy: 0.8139
After saving and reloading : 
1000/1000 [==============================] - 20s 18ms/step - loss: 0.5586 - accuracy: 0.7688
```

Batch size 32. The model got much worse after save + reloading

I tried EfficientNetB1, B2, B3 as well, all same issue

Then I tried simply switching to MobileNetV3 and Xception, they both work perfectly fine! The accuracy after Load is identical to that measured before Save...

Whats going on? Using tf 2.6 with python 3.8 and cudnn 8.2"
52165,BoostedTreesClassifier Not Progressing Past Step 0,"OS: macOS Catalina 10.15.7
TF 2.6.0 from binary
Python 3.9.6
Running inside Jupyter Notebook

**Describe the current behavior**
Training a BoostedTreesClassifier based on a transformation of TFRecordsDataset does not progress past step 0, and thus has no trees.  Evaluation results come back as if no prediction is being made and running feature importance afterwords raises an error that says the model must be trained first.

**Describe the expected behavior**
Should train a model with trees, make predictions, and be able to analyze the trees with say experimental_feature_importances

- Do you want to contribute a PR? (yes/no): No

**Code**
```
def parse(record):
    attributes = {feat: tf.io.FixedLenFeature([], tf.float32) for feat in feature_names}
    attributes[""label.label_xf""] = tf.io.FixedLenFeature([], tf.int64)

    parsed = tf.io.parse_single_example(record, attributes)
    features = {feat: tf.convert_to_tensor(parsed[feat]) for feat in feature_names}

    return features, tf.convert_to_tensor(parsed[""label.label_xf""])

def train_fn():
    train_examples = tf.data.TFRecordDataset(TRAIN_PATH, compression_type=""GZIP"")
    
    return train_examples.map(parse).batch(train_size)

def eval_fn():
    eval_examples = tf.data.TFRecordDataset(EVAL_PATH, compression_type=""GZIP"")
    
    return eval_examples.map(parse).batch(eval_size)

btc_est = tf.estimator.BoostedTreesClassifier(feature_columns, 1)

btc_est.train(train_fn, max_steps=10)
```
does not progress beyond step 0, and
`btc_est.evaluate(eval_fn)`
gives results
```
{'accuracy': 0.5158002,
 'accuracy_baseline': 0.51580024,
 'auc': 0.5,
 'auc_precision_recall': 0.7420999,
 'average_loss': 0.69314647,
 'label/mean': 0.4841998,
 'loss': 0.69314647,
 'precision': 0.0,
 'prediction/mean': 0.5,
 'recall': 0.0,
 'global_step': 0}
```
and
```btc_est.experimental_feature_importances()```
raises the error
```ValueError: Found empty serialized string for TreeEnsemble.You should only call this method after training.```
where as a LinearClassifier trained and evaluated in the same way
```
lin_est = tf.estimator.LinearClassifier(feature_columns)

lin_est.train(train_fn)
lin_est.evaluate(eval_fn)
```
yields sensible results that indicate a model has been successfully trained
```
{'accuracy': 0.5897044,
 'accuracy_baseline': 0.5158002,
 'auc': 0.59089816,
 'auc_precision_recall': 0.59708935,
 'average_loss': 0.6869413,
 'label/mean': 0.4841998,
 'loss': 0.6869413,
 'precision': 0.6975477,
 'prediction/mean': 0.49952453,
 'recall': 0.26947367,
 'global_step': 1}
```
"
52164,unit test failure kernels:sparse_matmul_op_test on AARCH64,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git HEAD
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Test fails

**Describe the expected behavior**

Test passes

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

bazel test //tensorflow/core/kernels:sparse_matmul_op_test

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

[ RUN      ] SparseMatmulOpTest.BroadcastPacketTest
[0.170094 0.170094 0.170094 0.170094] != [  0.170094    0.14922 -0.0823886   0.026985], differences: [         0 -0.0208738  -0.252482  -0.143109]
tensorflow/core/kernels/sparse_matmul_op_test.cc:329: Failure
Value of: areApprox(ref, data2, PacketSize)
Actual: false
Expected: true
[  FAILED  ] SparseMatmulOpTest.BroadcastPacketTest (0 ms)"
52163,bazel build tensorflow:tensorflow_cc is not working,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 64-bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6.0
- Python version: 3.9
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): bazel 3.7.2
- GCC/Compiler version (if compiling from source): 8.1.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

I am using conda environment which does not have tensorflow installed. I am using tensorflow 2.6 from github and building it using bazel.


**Describe the problem**
I am trying to use tensorflow c++ api to simulate tensorflow from python into c++ to simulate tensorflow models using c++. 
I am facing a problem while using bazel to build tensorflow and get .cc and .h files from tensorflow to use in c++ api.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout v2.6

bazel build tensorflow:tensorflow_cc

Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
   Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from d:\tensorflow\tensorflow-master\.bazelrc:
   Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=D:/Anaconda3/envs/tfcpp/python.exe                                                     
INFO: Reading rc options for 'build' from d:\tensorflow\tensorflow-master\.bazelrc:                                       
    'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils    

INFO: Found applicable config definition build:short_logs in file d:\tensorflow\tensorflow-master\.bazelrc: --output_filter=DONT_MATCH_ANYTHING                                                                                                 
INFO: Found applicable config definition build:v2 in file d:\tensorflow\tensorflow-master\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1                                                                                       
INFO: Found applicable config definition build:windows in file d:\tensorflow\tensorflow-master\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false                                                                                                   INFO: Found applicable config definition build:monolithic in file d:\tensorflow\tensorflow-master\.bazelrc: --define framework_shared_object=false                                                                                             

WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/8d3c2c75e02d3333df81807ff8f6c64f55353766.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found                                                                                                 

WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/efb284c07e97776e01933f470afb5215a561db3e.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found                                                                          

INFO: Repository local_config_python instantiated at:                                                                    
  D:/tensorflow/tensorflow-master/WORKSPACE:15:14: in <toplevel>                                                          
  D:/tensorflow/tensorflow-master/tensorflow/workspace2.bzl:1080:19: in workspace                                         
  D:/tensorflow/tensorflow-master/tensorflow/workspace2.bzl:99:21: in _tf_toolchains                                    
Repository rule python_configure defined at:                                                                              
  D:/tensorflow/tensorflow-master/third_party/py/python_configure.bzl:294:35: in <toplevel>                             

ERROR: An error occurred during the fetch of repository 'local_config_python':                                             
   Traceback (most recent call last):                                                                                          
       File ""D:/tensorflow/tensorflow-master/third_party/py/python_configure.bzl"", line 267, column 40, in _python_autoconf_impl                                                                                                                               _create_local_python_repository(repository_ctx)                                                                 
       File ""D:/tensorflow/tensorflow-master/third_party/py/python_configure.bzl"", line 209, column 22, in 
               _create_local_python_repository                                                                                                                     
                  _check_python_bin(repository_ctx, python_bin)                                                                   
       File ""D:/tensorflow/tensorflow-master/third_party/py/python_configure.bzl"", line 143, column 52, in _check_python_bin                                                                                                                                   
                result = raw_exec(repository_ctx, [get_bash_bin(repository_ctx), ""-c"", cmd])                                    
       File ""D:/tensorflow/tensorflow-master/third_party/remote_config/common.bzl"", line 88, column 26, in get_bash_bin                
                bash_bin_path = which(repository_ctx, ""bash"")                                                                   
       File ""D:/tensorflow/tensorflow-master/third_party/remote_config/common.bzl"", line 27, column 22, in which                       
                out = execute(                                                                                                  
       File ""D:/tensorflow/tensorflow-master/third_party/remote_config/common.bzl"", line 230, column 13, in execute                    
                fail(                                                                                                   

Error in fail: Repository command failed                                                                                
INFO: Could not find files for the given pattern(s).                                                                    
INFO: Repository go_sdk instantiated at:                                                                                  
   D:/tensorflow/tensorflow-master/WORKSPACE:23:14: in <toplevel>                                                          
   D:/tensorflow/tensorflow-master/tensorflow/workspace0.bzl:120:20: in workspace                                          
   D:/_bazel/lshsjacg/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps                    
   D:/_bazel/lshsjacg/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains             
   D:/_bazel/lshsjacg/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk                            
Repository rule _go_download_sdk defined at:                                                                              
   D:/_bazel/lshsjacg/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>                                 
ERROR: Analysis of target '//tensorflow:tensorflow_cc' failed; build aborted: Repository command failed                 
INFO: Could not find files for the given pattern(s).                                                                    
INFO: Elapsed time: 288.274s                                                                                            
INFO: 0 processes.                                                                                                      
FAILED: Build did NOT complete successfully (11 packages loaded, 15 targets configured)                                     
    Fetching @local_execution_config_python; fetching                                                                       
    Fetching ...docker; Cloning 9bfcd7dbf0294ed9d11a99da6363fc28df904502 of https://github.com/bazelbuild/rules_docker\ .git 

The warning that ""download from....Exception GET returned 404 Not Found"",  occurs twice in above trace log. And it occurs everytime I run the same commands. 
"
52161,Unable to use ImageDataGenerator on M1 mac ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS BigSur 11.6
- TensorFlow installed from (source or binary): https://developer.apple.com/metal/tensorflow-plugin/
- TensorFlow version: 2.5.0
- Python version: 3.8.11
- GPU model and memory: MacBook Pro M1, 16GB



**Describe the problem**
I'm creating a Neural Network to classify rock-paper-scissors images. I'm doing it by using an ImageDataGenerator in the script. 
Everything works fine, but when I try to train the model the error ""ImportError: Image transformations require SciPy. Install SciPy"" appears. The error appears when calling `model.fit` or `model.fit_generator`
The scipy library is already installed, version is 1.7.0. I tried to uninstall and re-install it, but nothing changed.
Does anybody know a solution to this?

Here is my code:

```
import os
import zipfile

local_zip = './rps.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('tmp/rps-train')
zip_ref.close()

local_zip = './rps-test-set.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('tmp/rps-test')
zip_ref.close()

base_dir = 'tmp/rps-train/rps'

rock_dir = os.path.join(base_dir, 'rock')
paper_dir = os.path.join(base_dir, 'paper')
scissors_dir = os.path.join(base_dir, 'scissors')

rock_files = os.listdir(rock_dir)
paper_files = os.listdir(paper_dir)
scissors_files = os.listdir(scissors_dir)

import tensorflow as tf
import keras_preprocessing
from keras_preprocessing import image
from keras_preprocessing.image import ImageDataGenerator

TRAINING_DIR = 'tmp/rps-train/rps'
training_datagen = ImageDataGenerator(rescale=1./255,
                                      rotation_range=40, 
                                      width_shift_range=0.2,
                                      height_shift_range=0.2,
                                      horizontal_flip=False
)

VALIDATION_DIR = 'tmp/rps-test/rps-test-set'
validation_datagen = ImageDataGenerator(rescale=1./255)

train_generator = training_datagen.flow_from_directory(
    TRAINING_DIR,
    target_size=(150,150),
    class_mode='categorical',
    batch_size=126
)

validation_generator = validation_datagen.flow_from_directory(
    VALIDATION_DIR,
    target_size=(150,150),
    class_mode='categorical',
    batch_size=126
)

model = tf.keras.models.Sequential([
    # This is the first convolution
    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    # The second convolution
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The third convolution
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The fourth convolution
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # Flatten the results to feed into a DNN
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.5),
    # 512 neuron hidden layer
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.summary()

model.compile(optimizer=tf.optimizers.RMSprop(learning_rate=0.001),
              loss=tf.metrics.categorical_crossentropy,
              metrics=['accuracy'])

```


After running the following line the error occurs:

```
model.fit_generator(train_generator,
                    epochs=3)

ImportError                               Traceback (most recent call last)
/var/folders/j4/flhsd8lj4z7g689p_y84tfjh0000gn/T/ipykernel_89314/1416594977.py in <module>
      1 import scipy
      2 
----> 3 model.fit_generator(train_generator,
      4                     epochs=3)

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1941                   'will be removed in a future version. '
   1942                   'Please use `Model.fit`, which supports generators.')
-> 1943     return self.fit(
   1944         generator,
   1945         steps_per_epoch=steps_per_epoch,

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1131          training_utils.RespectCompiledTrainableState(self):
   1132       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.
-> 1133       data_handler = data_adapter.get_data_handler(
   1134           x=x,
   1135           y=y,

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in get_data_handler(*args, **kwargs)
   1362   if getattr(kwargs[""model""], ""_cluster_coordinator"", None):
   1363     return _ClusterCoordinatorDataHandler(*args, **kwargs)
-> 1364   return DataHandler(*args, **kwargs)
   1365 
   1366 

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1152     adapter_cls = select_data_adapter(x, y)
   1153     self._verify_data_adapter_compatibility(adapter_cls)
-> 1154     self._adapter = adapter_cls(
   1155         x,
   1156         y,

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    930     self._keras_sequence = x
    931     self._enqueuer = None
--> 932     super(KerasSequenceAdapter, self).__init__(
    933         x,
    934         shuffle=False,  # Shuffle is handed in the _make_callable override.

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    807     # Since we have to know the dtype of the python generator when we build the
    808     # dataset, we have to look at a batch to infer the structure.
--> 809     peek, x = self._peek_and_restore(x)
    810     peek = self._standardize_batch(peek)
    811     peek = _process_tensorlike(peek)

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in _peek_and_restore(x)
    941   @staticmethod
    942   def _peek_and_restore(x):
--> 943     return x[0], x
    944 
    945   def _handle_multiprocessing(self, x, workers, use_multiprocessing,

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/keras_preprocessing/image/iterator.py in __getitem__(self, idx)
     63         index_array = self.index_array[self.batch_size * idx:
     64                                        self.batch_size * (idx + 1)]
---> 65         return self._get_batches_of_transformed_samples(index_array)
     66 
     67     def __len__(self):

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/keras_preprocessing/image/iterator.py in _get_batches_of_transformed_samples(self, index_array)
    236             if self.image_data_generator:
    237                 params = self.image_data_generator.get_random_transform(x.shape)
--> 238                 x = self.image_data_generator.apply_transform(x, params)
    239                 x = self.image_data_generator.standardize(x)
    240             batch_x[i] = x

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/keras_preprocessing/image/image_data_generator.py in apply_transform(self, x, transform_parameters)
    861         img_channel_axis = self.channel_axis - 1
    862 
--> 863         x = apply_affine_transform(x, transform_parameters.get('theta', 0),
    864                                    transform_parameters.get('tx', 0),
    865                                    transform_parameters.get('ty', 0),

~/miniforge3/envs/python3.8/lib/python3.8/site-packages/keras_preprocessing/image/affine_transformations.py in apply_affine_transform(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)
    279     """"""
    280     if scipy is None:
--> 281         raise ImportError('Image transformations require SciPy. '
    282                           'Install SciPy.')
    283     transform_matrix = None

ImportError: Image transformations require SciPy. Install SciPy.
```

Here are the packages installed in the virtual environment I am using:


<img width=""352"" alt=""Screenshot 2021-09-28 at 8 53 00 AM"" src=""https://user-images.githubusercontent.com/91497431/135038153-199fe46b-9269-48e7-a01d-4c06a519cdb2.png"">
<img width=""255"" alt=""Screenshot 2021-09-28 at 8 53 15 AM"" src=""https://user-images.githubusercontent.com/91497431/135038191-c0a4038d-0d67-45d5-a7d1-1fccdf0c2682.png"">
<img width=""282"" alt=""Screenshot 2021-09-28 at 8 53 34 AM"" src=""https://user-images.githubusercontent.com/91497431/135038209-9a9a302f-4560-4c82-8223-208d0259b56b.png"">


"
52160,Make libTensorFlow C++ library work for the latest M1 Mac,"The M1 Mac with it's own non-Intel processor was released ten months ago but is still not supported by the libTensorFlow C++ library **https://www.tensorflow.org/install/lang_c**.  It would be good to get that done so that we can release apps for the Mac.  As far as I know TensorFlow does work for the M1 Mac in Python.

You can test the library file compatibility by running xcrun as follows:-
Run the XCode command: xcrun lipo -info ""libtensorflow.dylib"" and it returns ""libtensorflow.dylib is architecture x86_64"".  

For it to support the Intel Mac and the M1 Mac it should return ""libtensorflow.dylib are: arm64 x86_64"" (ie. both formats so that the compiler can build the app for Intel, Arm or both (the default))."
52159,"Why does tensorflow only use one GPU, but it takes up two GPUs? ","
|=============================================================================|
|    0   N/A  N/A     27330      C   ...envs/p3.8_t2.2/bin/python      249MiB |
|    1   N/A  N/A     27330      C   ...envs/p3.8_t2.2/bin/python     7517MiB |
+-----------------------------------------------------------------------------+

I have two GPUs on my machine. I run the GPU training code and found that the process occupies 2 GUPs. The actual use is the No. 1 GPU, but the No. 0 GUP occupies 249 MiB of memory but is not used. I don’t understand why if only one GPU is used, the other should be occupied? 

**Reproduce the code：**
```python
import tensorflow as tf
import os


mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train[..., tf.newaxis].astype(""float32"")
x_test = x_test[..., tf.newaxis].astype(""float32"")

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(10, kernel_size=(3, 3),input_shape=(28, 28, 1)),
  tf.keras.layers.Conv2D(20, kernel_size=(3,3)),
  tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.summary()
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=32, epochs=1)
```
"
52158,"TensorFlow Lite C library built with CMake, ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **from Source**
- TensorFlow version: 
- Python version: 
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): **GNU 7.5.0**
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
The project I am working on requires the **TensorFlow Lite C API** which I built with **CMake** following this instructions https://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library (shared library).

I have built and run pretrained models successfully before, however, I am currently trying to integrate a model that takes 2 images as input such as **HITNet** from google-research group (https://github.com/google-research/google-research/tree/master/hitnet). The model mentioned in the paper is in protobuf format here https://storage.googleapis.com/tensorflow-graphics/models/hitnet/default_models/middlebury_d400.pb, and I found in this repo https://github.com/PINTO0309/PINTO_model_zoo/tree/main/142_HITNET the model converted to tflite model https://drive.google.com/uc?export=download&id=1cqxZ-hCQagdwYQee4U8LsaHAJA3y-Go3

The following error arises when creating the model from file with `TfLiteModelCreateFromFile`:
```
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
ERROR: Node number 271 (FlexStridedSlice) failed to prepare.
```

_**How to add support of Select TensorFlow op(s)/Flex delegate to TensorFlow Lite C API with CMake?**_
I didn't find any suggestion for C API in https://www.tensorflow.org/lite/guide/ops_select


**Provide the exact sequence of commands / steps that you executed before running into the problem**
`TfLiteModel* model = TfLiteModelCreateFromFile((test_data_dir() + ""/tflite_hitnet_model/hitnet_model_float32_480x640.tflite"").c_str());`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
52156,Keras model saved with user-defined signature works with TensorFlow Serving but not Python,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): TensorFlow 2.6 and Nightly
- Python version: Python 3.7
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When manually specifying the signature in [tf.keras.Model.save](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save) a correct model that works with TensorFlow Serving is saved. However, when reloading the same model in Python, TensorFlow tries to execute the incorrect concrete function.

In the example below, I create a model that accepts a `[None, 4]` input but when saving, change it to a `[None, 2]` input that I simply concatenate along `axis=1` to turn it into a `[None, 4]` input. TensorFlow Serving can run this model without problems, but reloading the model with `restored_model = tf.keras.model.load_model(...)` and calling the restored model as a function incorrectly tries to run a function that expects a `[None, 4]` input instead of my user-specified one that expects a `[None, 2]` input.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf


input_name = ""abc""

#####################################################################################################
# model that accepts [None, 4] input
#####################################################################################################
class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.x = tf.keras.layers.Dense(1)

    def call(self, inputs, training=None):
        return self.x(inputs[input_name])


inputs = tf.data.Dataset.from_tensor_slices(({input_name: [[1, 2, 3, 4]]}, [1])).batch(1)
model = MyModel()
model.compile(loss=""binary_crossentropy"", optimizer=""sgd"")
model.fit(inputs)
model.save(""4d_model"")
#####################################################################################################


#####################################################################################################
# override the signature to accept [None, 2] and simply concatenate it into [None, 4] for the model
#####################################################################################################
@tf.function(input_signature=[tf.TensorSpec([None, 2], dtype=tf.int32, name=input_name)])
def serve(x):
    return model({input_name: tf.concat((x, x), axis=1)})

model.save(""2d_model"", signatures={""serving_default"": serve})
#####################################################################################################

restored_model = tf.keras.models.load_model(""2d_model"")
x = next(iter(tf.data.Dataset.from_tensor_slices({input_name: [[1, 2]]}).batch(1)))
outputs = restored_model(x)  # incorrectly requires [None, 4] instead of [None, 2]
```

```python
>>> print(restored_model.signatures[""serving_default""].structured_input_signature)
((), {'abc': TensorSpec(shape=(None, 2), dtype=tf.int32, name='abc')})
```


**Other info / logs** Include any logs or source code that would be helpful to
```
ValueError: Exception encountered when calling layer ""my_model"" (type MyModel).

Could not find matching concrete function to call loaded from the SavedModel. Got:
  Positional arguments (2 total):
    * {'abc': <tf.Tensor 'inputs:0' shape=(1, 2) dtype=int32>}
    * False
  Keyword arguments: {}

 Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (2 total):
    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='inputs/abc')}
    * False
  Keyword arguments: {}

Option 2:
  Positional arguments (2 total):
    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='inputs/abc')}
    * True
  Keyword arguments: {}

Option 3:
  Positional arguments (2 total):
    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='abc')}
    * False
  Keyword arguments: {}

Option 4:
  Positional arguments (2 total):
    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='abc')}
    * True
  Keyword arguments: {}

Call arguments received:
  • args=({'abc': 'tf.Tensor(shape=(1, 2), dtype=int32)'},)
  • kwargs={'training': 'None'}
```
"
52155,Building failure of libtensorflowlite_gpu_delegate.so,"

**System information**
- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6.0
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 9.3.0


**Describe the problem**
Building libtensorflowlite_gpu_delegate.so fails complaining about `absl::Status`.
However building libtensorflowlite_c.so works well (using `bazel build -c opt tensorflow/lite/c:libtensorflowlite_c.so`)

Using lastest or v2.5.0 also fails.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
git clone https://github.com/tensorflow/tensorflow -b v2.6.0   
cd tensorflow  
bazel build -c opt tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so
```

**Any other info / logs**

```
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=201
INFO: Reading rc options for 'build' from /home/michel/gin-tflite/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/michel/gin-tflite/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Found applicable config definition build:short_logs in file /home/michel/gin-tflite/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/michel/gin-tflite/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/michel/gin-tflite/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false
INFO: Found applicable config definition build:dynamic_kernels in file /home/michel/gin-tflite/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /home/michel/.cache/bazel/_bazel_michel/2729c94059558bc79129827a71924973/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /home/michel/gin-tflite/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/michel/gin-tflite/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  /home/michel/.cache/bazel/_bazel_michel/2729c94059558bc79129827a71924973/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  /home/michel/.cache/bazel/_bazel_michel/2729c94059558bc79129827a71924973/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/michel/gin-tflite/tensorflow/tensorflow/lite/delegates/gpu/gl/BUILD:48:11: C++ compilation of rule '//tensorflow/lite/delegates/gpu/gl:api2' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 43 argument(s) skipped)
In file included from external/opencl_headers/CL/cl.h:32,
                 from ./tensorflow/lite/delegates/gpu/api.h:42,
                 from ./tensorflow/lite/delegates/gpu/gl/api2.h:23,
                 from tensorflow/lite/delegates/gpu/gl/api2.cc:16:
external/opencl_headers/CL/cl_version.h:34:104: note: #pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)
   34 | #pragma message(""cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)"")
      |                                                                                                        ^
In file included from /usr/include/EGL/eglplatform.h:128,
                 from /usr/include/EGL/egl.h:39,
                 from ./tensorflow/lite/delegates/gpu/gl/portable_gl31.h:21,
                 from ./tensorflow/lite/delegates/gpu/api.h:50,
                 from ./tensorflow/lite/delegates/gpu/gl/api2.h:23,
                 from tensorflow/lite/delegates/gpu/gl/api2.cc:16:
./tensorflow/lite/delegates/gpu/api.h:261:17: error: expected unqualified-id before 'int'
  261 |   virtual absl::Status SetInputShape(int index,
      |                 ^~~~~~
./tensorflow/lite/delegates/gpu/api.h:271:17: error: expected unqualified-id before 'int'
  271 |   virtual absl::Status SetInputObjectDef(int index, ObjectDef def) = 0;
      |                 ^~~~~~
./tensorflow/lite/delegates/gpu/api.h:272:17: error: expected unqualified-id before 'int'
  272 |   virtual absl::Status SetOutputObjectDef(int index, ObjectDef def) = 0;
      |                 ^~~~~~
./tensorflow/lite/delegates/gpu/api.h:273:17: error: expected unqualified-id before 'int'
  273 |   virtual absl::Status SetAllInputObjectDefsTo(ObjectDef def) {
      |                 ^~~~~~
```
"
52154,Tensorflow fails to build on Fedora ppc64le,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 34 ppcle64
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.3
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: source
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): gcc 11.2.1
- GPU model and memory:



**Describe the problem**
I installed all the dependencies that I knew about (eigen3, protobuf*, pip, bazel, etc.) and I get a lengthy build error

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`bazel build --config=opt tensorflow:libtensorflow.so`

**Any other info / logs**
The build error:
https://paste.centos.org/view/11a395f2

```
  exec env - \
    PATH=/home/bkeys/Devel/Software/bazel/output/:/home/bkeys/.local/bin:/home/bkeys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
  /bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/ppc-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_single_threaded_conv2d/runtime_single_threaded_conv2d.pic.d '-frandom-seed=bazel-out/ppc-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_single_threaded_conv2d/runtime_single_threaded_conv2d.pic.o' -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -iquote . -iquote bazel-out/ppc-opt/bin -iquote external/com_google_absl -iquote bazel-out/ppc-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/ppc-opt/bin/external/eigen_archive -isystem external/eigen_archive -isystem bazel-out/ppc-opt/bin/external/eigen_archive -w -DAUTOLOAD_DYNAMIC_KERNELS '-mcpu=native' '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc -o bazel-out/ppc-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_single_threaded_conv2d/runtime_single_threaded_conv2d.pic.o)
Execution platform: @local_execution_config_platform//:platform
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h: In instantiation of 'void Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>; LhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>; RhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>; Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:890:25:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*, Eigen::TensorContractionEvaluatorBase<Derived>::Index, Eigen::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = true; int Alignment = 0; bool use_output_kernel = true; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float; Eigen::TensorContractionEvaluatorBase<Derived>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:783:52:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemm(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = true; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:720:66:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalProductSequential(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = true; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:1015:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::evalProduct(Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar*) const [with int Alignment = 0; Indices = const Eigen::array<Eigen::IndexPair<long long int>, 1>; LeftArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >; OutputKernelType = const Eigen::NoOpOutputKernel; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:699:70:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:180:39:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const Eigen::DSizes<long long int, 4>; ArgType = const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:152:44:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:131:61:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true, Eigen::internal::Off>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> > >]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; ExpressionType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; DeviceType = Eigen::DefaultDevice]'
./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:79:25:   required from 'void tensorflow::xla::EigenConvImpl(const EigenDevice&, ScalarType*, ScalarType*, ScalarType*, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index) [with EigenDevice = Eigen::DefaultDevice; ScalarType = float; Eigen::Index = long int]'
tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:57:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: ambiguous template instantiation for 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>, 4, 0, false, false>'
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:319,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/products/GeneralBlockPanelKernel.h:2551:8: note: candidates are: 'template<class Scalar, class Index, class DataMapper, int nr, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_rhs<Scalar, Index, DataMapper, nr, 0, Conjugate, PanelMode> [with Scalar = float; Index = long int; DataMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>; int nr = 4; bool Conjugate = false; bool PanelMode = false]'
 2551 | struct gemm_pack_rhs<Scalar, Index, DataMapper, nr, ColMajor, Conjugate, PanelMode>
      |        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:339,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:2691:8: note:                 'template<class Index, class DataMapper, int nr, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_rhs<float, Index, DataMapper, nr, 0, Conjugate, PanelMode> [with Index = long int; DataMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>; int nr = 4; bool Conjugate = false; bool PanelMode = false]'
 2691 | struct gemm_pack_rhs<float, Index, DataMapper, nr, ColMajor, Conjugate, PanelMode>
      |        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from ./tensorflow/core/kernels/eigen_spatial_convolutions.h:23,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
./tensorflow/core/kernels/eigen_spatial_convolutions-inl.h:1044:8: note:                 'template<class NewDimension, long int Rows, long int Cols, class ArgType, class Device, class Scalar, class Index, class nocontract_t, class contract_t, int packet_size, bool inner_dim_contiguous, bool inner_dim_reordered, int Alignment, int nr> struct Eigen::internal::gemm_pack_rhs<Scalar, Index, Eigen::internal::TensorContractionSubMapper<Scalar, Index, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimension, const Eigen::TensorImagePatchOp<Rows, Cols, ArgType> >, Device>, nocontract_t, contract_t, packet_size, inner_dim_contiguous, inner_dim_reordered, Alignment>, nr, 0, false, false> [with NewDimension = const Eigen::DSizes<long long int, 2>; long int Rows = -1; long int Cols = -1; ArgType = const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Scalar = float; Index = long int; nocontract_t = Eigen::array<long int, 1>; contract_t = Eigen::array<long int, 1>; int packet_size = 4; bool inner_dim_contiguous = true; bool inner_dim_reordered = true; int Alignment = 0; int nr = 4]'
 1044 | struct gemm_pack_rhs<
      |        ^~~~~~~~~~~~~~
 1045 |     Scalar, Index,
      |     ~~~~~~~~~~~~~~
 1046 |     TensorContractionSubMapper<
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1047 |         Scalar, Index, Rhs,
      |         ~~~~~~~~~~~~~~~~~~~
 1048 |         TensorEvaluator<
      |         ~~~~~~~~~~~~~~~~
 1049 |             const TensorReshapingOp<
      |             ~~~~~~~~~~~~~~~~~~~~~~~~
 1050 |                 NewDimension, const TensorImagePatchOp<Rows, Cols, ArgType> >,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1051 |             Device>,
      |             ~~~~~~~~
 1052 |         nocontract_t, contract_t, packet_size, inner_dim_contiguous,
      |         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1053 |         inner_dim_reordered, Alignment>,
      |         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1054 |     nr, ColMajor, false, false> {
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: invalid use of incomplete type 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>, 4, 0, false, false>'}
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:275,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:25:8: note: declaration of 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>, 4, 0, false, false>'}
   25 | struct gemm_pack_rhs;
      |        ^~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h: In instantiation of 'void Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>; LhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>; RhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>; Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:890:25:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*, Eigen::TensorContractionEvaluatorBase<Derived>::Index, Eigen::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = false; int Alignment = 0; bool use_output_kernel = true; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float; Eigen::TensorContractionEvaluatorBase<Derived>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:783:52:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemm(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = false; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:720:66:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalProductSequential(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = false; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:1015:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::evalProduct(Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar*) const [with int Alignment = 0; Indices = const Eigen::array<Eigen::IndexPair<long long int>, 1>; LeftArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >; OutputKernelType = const Eigen::NoOpOutputKernel; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:699:70:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:180:39:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const Eigen::DSizes<long long int, 4>; ArgType = const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:152:44:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:131:61:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true, Eigen::internal::Off>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> > >]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; ExpressionType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; DeviceType = Eigen::DefaultDevice]'
./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:79:25:   required from 'void tensorflow::xla::EigenConvImpl(const EigenDevice&, ScalarType*, ScalarType*, ScalarType*, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index) [with EigenDevice = Eigen::DefaultDevice; ScalarType = float; Eigen::Index = long int]'
tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:57:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: ambiguous template instantiation for 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>'
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:319,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/products/GeneralBlockPanelKernel.h:2551:8: note: candidates are: 'template<class Scalar, class Index, class DataMapper, int nr, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_rhs<Scalar, Index, DataMapper, nr, 0, Conjugate, PanelMode> [with Scalar = float; Index = long int; DataMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>; int nr = 4; bool Conjugate = false; bool PanelMode = false]'
 2551 | struct gemm_pack_rhs<Scalar, Index, DataMapper, nr, ColMajor, Conjugate, PanelMode>
      |        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:339,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:2691:8: note:                 'template<class Index, class DataMapper, int nr, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_rhs<float, Index, DataMapper, nr, 0, Conjugate, PanelMode> [with Index = long int; DataMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>; int nr = 4; bool Conjugate = false; bool PanelMode = false]'
 2691 | struct gemm_pack_rhs<float, Index, DataMapper, nr, ColMajor, Conjugate, PanelMode>
      |        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from ./tensorflow/core/kernels/eigen_spatial_convolutions.h:23,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
./tensorflow/core/kernels/eigen_spatial_convolutions-inl.h:1044:8: note:                 'template<class NewDimension, long int Rows, long int Cols, class ArgType, class Device, class Scalar, class Index, class nocontract_t, class contract_t, int packet_size, bool inner_dim_contiguous, bool inner_dim_reordered, int Alignment, int nr> struct Eigen::internal::gemm_pack_rhs<Scalar, Index, Eigen::internal::TensorContractionSubMapper<Scalar, Index, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimension, const Eigen::TensorImagePatchOp<Rows, Cols, ArgType> >, Device>, nocontract_t, contract_t, packet_size, inner_dim_contiguous, inner_dim_reordered, Alignment>, nr, 0, false, false> [with NewDimension = const Eigen::DSizes<long long int, 2>; long int Rows = -1; long int Cols = -1; ArgType = const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Scalar = float; Index = long int; nocontract_t = Eigen::array<long int, 1>; contract_t = Eigen::array<long int, 1>; int packet_size = 4; bool inner_dim_contiguous = true; bool inner_dim_reordered = false; int Alignment = 0; int nr = 4]'
 1044 | struct gemm_pack_rhs<
      |        ^~~~~~~~~~~~~~
 1045 |     Scalar, Index,
      |     ~~~~~~~~~~~~~~
 1046 |     TensorContractionSubMapper<
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1047 |         Scalar, Index, Rhs,
      |         ~~~~~~~~~~~~~~~~~~~
 1048 |         TensorEvaluator<
      |         ~~~~~~~~~~~~~~~~
 1049 |             const TensorReshapingOp<
      |             ~~~~~~~~~~~~~~~~~~~~~~~~
 1050 |                 NewDimension, const TensorImagePatchOp<Rows, Cols, ArgType> >,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1051 |             Device>,
      |             ~~~~~~~~
 1052 |         nocontract_t, contract_t, packet_size, inner_dim_contiguous,
      |         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1053 |         inner_dim_reordered, Alignment>,
      |         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1054 |     nr, ColMajor, false, false> {
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: invalid use of incomplete type 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>'}
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:275,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:25:8: note: declaration of 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>'}
   25 | struct gemm_pack_rhs;
      |        ^~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h: In instantiation of 'void Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>; LhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>; RhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>; Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:890:25:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*, Eigen::TensorContractionEvaluatorBase<Derived>::Index, Eigen::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = true; int Alignment = 0; bool use_output_kernel = true; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float; Eigen::TensorContractionEvaluatorBase<Derived>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:783:52:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemm(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = true; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:720:66:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalProductSequential(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = true; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:1015:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::evalProduct(Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar*) const [with int Alignment = 0; Indices = const Eigen::array<Eigen::IndexPair<long long int>, 1>; LeftArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >; OutputKernelType = const Eigen::NoOpOutputKernel; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:699:70:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:180:39:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const Eigen::DSizes<long long int, 4>; ArgType = const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:152:44:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:131:61:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true, Eigen::internal::Off>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> > >]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; ExpressionType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; DeviceType = Eigen::DefaultDevice]'
./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:79:25:   required from 'void tensorflow::xla::EigenConvImpl(const EigenDevice&, ScalarType*, ScalarType*, ScalarType*, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index) [with EigenDevice = Eigen::DefaultDevice; ScalarType = float; Eigen::Index = long int]'
tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:57:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: ambiguous template instantiation for 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>, 4, 0, false, false>'
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:319,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/products/GeneralBlockPanelKernel.h:2551:8: note: candidates are: 'template<class Scalar, class Index, class DataMapper, int nr, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_rhs<Scalar, Index, DataMapper, nr, 0, Conjugate, PanelMode> [with Scalar = float; Index = long int; DataMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>; int nr = 4; bool Conjugate = false; bool PanelMode = false]'
 2551 | struct gemm_pack_rhs<Scalar, Index, DataMapper, nr, ColMajor, Conjugate, PanelMode>
      |        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:339,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:2691:8: note:                 'template<class Index, class DataMapper, int nr, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_rhs<float, Index, DataMapper, nr, 0, Conjugate, PanelMode> [with Index = long int; DataMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>; int nr = 4; bool Conjugate = false; bool PanelMode = false]'
 2691 | struct gemm_pack_rhs<float, Index, DataMapper, nr, ColMajor, Conjugate, PanelMode>
      |        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from ./tensorflow/core/kernels/eigen_spatial_convolutions.h:23,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
./tensorflow/core/kernels/eigen_spatial_convolutions-inl.h:1044:8: note:                 'template<class NewDimension, long int Rows, long int Cols, class ArgType, class Device, class Scalar, class Index, class nocontract_t, class contract_t, int packet_size, bool inner_dim_contiguous, bool inner_dim_reordered, int Alignment, int nr> struct Eigen::internal::gemm_pack_rhs<Scalar, Index, Eigen::internal::TensorContractionSubMapper<Scalar, Index, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimension, const Eigen::TensorImagePatchOp<Rows, Cols, ArgType> >, Device>, nocontract_t, contract_t, packet_size, inner_dim_contiguous, inner_dim_reordered, Alignment>, nr, 0, false, false> [with NewDimension = const Eigen::DSizes<long long int, 2>; long int Rows = -1; long int Cols = -1; ArgType = const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Scalar = float; Index = long int; nocontract_t = Eigen::array<long int, 1>; contract_t = Eigen::array<long int, 1>; int packet_size = 4; bool inner_dim_contiguous = false; bool inner_dim_reordered = true; int Alignment = 0; int nr = 4]'
 1044 | struct gemm_pack_rhs<
      |        ^~~~~~~~~~~~~~
 1045 |     Scalar, Index,
      |     ~~~~~~~~~~~~~~
 1046 |     TensorContractionSubMapper<
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1047 |         Scalar, Index, Rhs,
      |         ~~~~~~~~~~~~~~~~~~~
 1048 |         TensorEvaluator<
      |         ~~~~~~~~~~~~~~~~
 1049 |             const TensorReshapingOp<
      |             ~~~~~~~~~~~~~~~~~~~~~~~~
 1050 |                 NewDimension, const TensorImagePatchOp<Rows, Cols, ArgType> >,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1051 |             Device>,
      |             ~~~~~~~~
 1052 |         nocontract_t, contract_t, packet_size, inner_dim_contiguous,
      |         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1053 |         inner_dim_reordered, Alignment>,
      |         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1054 |     nr, ColMajor, false, false> {
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: invalid use of incomplete type 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>, 4, 0, false, false>'}
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:275,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:25:8: note: declaration of 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>, 4, 0, false, false>'}
   25 | struct gemm_pack_rhs;
      |        ^~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h: In instantiation of 'void Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>; LhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>; RhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>; Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:890:25:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*, Eigen::TensorContractionEvaluatorBase<Derived>::Index, Eigen::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = false; int Alignment = 0; bool use_output_kernel = true; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float; Eigen::TensorContractionEvaluatorBase<Derived>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:783:52:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemm(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = false; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:720:66:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalProductSequential(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = false; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:1015:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::evalProduct(Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar*) const [with int Alignment = 0; Indices = const Eigen::array<Eigen::IndexPair<long long int>, 1>; LeftArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >; OutputKernelType = const Eigen::NoOpOutputKernel; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:699:70:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:180:39:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const Eigen::DSizes<long long int, 4>; ArgType = const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:152:44:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:131:61:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true, Eigen::internal::Off>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> > >]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; ExpressionType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; DeviceType = Eigen::DefaultDevice]'
./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:79:25:   required from 'void tensorflow::xla::EigenConvImpl(const EigenDevice&, ScalarType*, ScalarType*, ScalarType*, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index) [with EigenDevice = Eigen::DefaultDevice; ScalarType = float; Eigen::Index = long int]'
tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:57:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: ambiguous template instantiation for 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, 4, 0, false, false>'
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:319,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/products/GeneralBlockPanelKernel.h:2551:8: note: candidates are: 'template<class Scalar, class Index, class DataMapper, int nr, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_rhs<Scalar, Index, DataMapper, nr, 0, Conjugate, PanelMode> [with Scalar = float; Index = long int; DataMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>; int nr = 4; bool Conjugate = false; bool PanelMode = false]'
 2551 | struct gemm_pack_rhs<Scalar, Index, DataMapper, nr, ColMajor, Conjugate, PanelMode>
      |        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:339,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:2691:8: note:                 'template<class Index, class DataMapper, int nr, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_rhs<float, Index, DataMapper, nr, 0, Conjugate, PanelMode> [with Index = long int; DataMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>; int nr = 4; bool Conjugate = false; bool PanelMode = false]'
 2691 | struct gemm_pack_rhs<float, Index, DataMapper, nr, ColMajor, Conjugate, PanelMode>
      |        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from ./tensorflow/core/kernels/eigen_spatial_convolutions.h:23,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
./tensorflow/core/kernels/eigen_spatial_convolutions-inl.h:1044:8: note:                 'template<class NewDimension, long int Rows, long int Cols, class ArgType, class Device, class Scalar, class Index, class nocontract_t, class contract_t, int packet_size, bool inner_dim_contiguous, bool inner_dim_reordered, int Alignment, int nr> struct Eigen::internal::gemm_pack_rhs<Scalar, Index, Eigen::internal::TensorContractionSubMapper<Scalar, Index, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimension, const Eigen::TensorImagePatchOp<Rows, Cols, ArgType> >, Device>, nocontract_t, contract_t, packet_size, inner_dim_contiguous, inner_dim_reordered, Alignment>, nr, 0, false, false> [with NewDimension = const Eigen::DSizes<long long int, 2>; long int Rows = -1; long int Cols = -1; ArgType = const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Scalar = float; Index = long int; nocontract_t = Eigen::array<long int, 1>; contract_t = Eigen::array<long int, 1>; int packet_size = 4; bool inner_dim_contiguous = false; bool inner_dim_reordered = false; int Alignment = 0; int nr = 4]'
 1044 | struct gemm_pack_rhs<
      |        ^~~~~~~~~~~~~~
 1045 |     Scalar, Index,
      |     ~~~~~~~~~~~~~~
 1046 |     TensorContractionSubMapper<
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1047 |         Scalar, Index, Rhs,
      |         ~~~~~~~~~~~~~~~~~~~
 1048 |         TensorEvaluator<
      |         ~~~~~~~~~~~~~~~~
 1049 |             const TensorReshapingOp<
      |             ~~~~~~~~~~~~~~~~~~~~~~~~
 1050 |                 NewDimension, const TensorImagePatchOp<Rows, Cols, ArgType> >,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1051 |             Device>,
      |             ~~~~~~~~
 1052 |         nocontract_t, contract_t, packet_size, inner_dim_contiguous,
      |         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1053 |         inner_dim_reordered, Alignment>,
      |         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1054 |     nr, ColMajor, false, false> {
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: invalid use of incomplete type 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, 4, 0, false, false>'}
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:275,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:25:8: note: declaration of 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, 4, 0, false, false>'}
   25 | struct gemm_pack_rhs;
      |        ^~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h: In instantiation of 'void Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>; LhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>; RhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>; Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:890:25:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*, Eigen::TensorContractionEvaluatorBase<Derived>::Index, Eigen::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = true; int Alignment = 0; bool use_output_kernel = true; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float; Eigen::TensorContractionEvaluatorBase<Derived>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:783:52:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemm(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = true; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:720:66:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalProductSequential(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = true; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:1015:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::evalProduct(Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar*) const [with int Alignment = 0; Indices = const Eigen::array<Eigen::IndexPair<long long int>, 1>; LeftArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >; OutputKernelType = const Eigen::NoOpOutputKernel; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:699:70:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:180:39:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const Eigen::DSizes<long long int, 4>; ArgType = const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:152:44:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:131:61:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true, Eigen::internal::Off>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> > >]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; ExpressionType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; DeviceType = Eigen::DefaultDevice]'
./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:79:25:   required from 'void tensorflow::xla::EigenConvImpl(const EigenDevice&, ScalarType*, ScalarType*, ScalarType*, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index) [with EigenDevice = Eigen::DefaultDevice; ScalarType = float; Eigen::Index = long int]'
tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:57:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: invalid use of incomplete type 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>, 4, 0, false, false>'}
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:275,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:25:8: note: declaration of 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, true, 0, Eigen::MakePointer>, 4, 0, false, false>'}
   25 | struct gemm_pack_rhs;
      |        ^~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h: In instantiation of 'void Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>; LhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>; RhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>; Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:890:25:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*, Eigen::TensorContractionEvaluatorBase<Derived>::Index, Eigen::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = false; int Alignment = 0; bool use_output_kernel = true; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float; Eigen::TensorContractionEvaluatorBase<Derived>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:783:52:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemm(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = false; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:720:66:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalProductSequential(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = true; bool rhs_inner_dim_reordered = false; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:1015:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::evalProduct(Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar*) const [with int Alignment = 0; Indices = const Eigen::array<Eigen::IndexPair<long long int>, 1>; LeftArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >; OutputKernelType = const Eigen::NoOpOutputKernel; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:699:70:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:180:39:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const Eigen::DSizes<long long int, 4>; ArgType = const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:152:44:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:131:61:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true, Eigen::internal::Off>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> > >]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; ExpressionType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; DeviceType = Eigen::DefaultDevice]'
./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:79:25:   required from 'void tensorflow::xla::EigenConvImpl(const EigenDevice&, ScalarType*, ScalarType*, ScalarType*, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index) [with EigenDevice = Eigen::DefaultDevice; ScalarType = float; Eigen::Index = long int]'
tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:57:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: invalid use of incomplete type 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>'}
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:275,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:25:8: note: declaration of 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>'}
   25 | struct gemm_pack_rhs;
      |        ^~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h: In instantiation of 'void Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>; LhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>; RhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>; Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:890:25:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*, Eigen::TensorContractionEvaluatorBase<Derived>::Index, Eigen::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = true; int Alignment = 0; bool use_output_kernel = true; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float; Eigen::TensorContractionEvaluatorBase<Derived>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:783:52:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemm(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = true; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:720:66:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalProductSequential(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = true; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:1015:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::evalProduct(Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar*) const [with int Alignment = 0; Indices = const Eigen::array<Eigen::IndexPair<long long int>, 1>; LeftArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >; OutputKernelType = const Eigen::NoOpOutputKernel; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:699:70:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:180:39:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const Eigen::DSizes<long long int, 4>; ArgType = const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:152:44:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:131:61:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true, Eigen::internal::Off>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> > >]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; ExpressionType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; DeviceType = Eigen::DefaultDevice]'
./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:79:25:   required from 'void tensorflow::xla::EigenConvImpl(const EigenDevice&, ScalarType*, ScalarType*, ScalarType*, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index) [with EigenDevice = Eigen::DefaultDevice; ScalarType = float; Eigen::Index = long int]'
tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:57:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: invalid use of incomplete type 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>, 4, 0, false, false>'}
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:275,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:25:8: note: declaration of 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, true, 0, Eigen::MakePointer>, 4, 0, false, false>'}
   25 | struct gemm_pack_rhs;
      |        ^~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:110,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h: In instantiation of 'void Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>; LhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>; RhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>; Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:890:25:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*, Eigen::TensorContractionEvaluatorBase<Derived>::Index, Eigen::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = false; int Alignment = 0; bool use_output_kernel = true; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float; Eigen::TensorContractionEvaluatorBase<Derived>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:783:52:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemm(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = false; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:720:66:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalProductSequential(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = false; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:1015:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::evalProduct(Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar*) const [with int Alignment = 0; Indices = const Eigen::array<Eigen::IndexPair<long long int>, 1>; LeftArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >; OutputKernelType = const Eigen::NoOpOutputKernel; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:699:70:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:180:39:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const Eigen::DSizes<long long int, 4>; ArgType = const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:152:44:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:131:61:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true, Eigen::internal::Off>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> > >]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::NoOpOutputKernel> >; ExpressionType = Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer>; DeviceType = Eigen::DefaultDevice]'
./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:79:25:   required from 'void tensorflow::xla::EigenConvImpl(const EigenDevice&, ScalarType*, ScalarType*, ScalarType*, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index) [with EigenDevice = Eigen::DefaultDevice; ScalarType = float; Eigen::Index = long int]'
tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:57:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:248:5: error: invalid use of incomplete type 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, 4, 0, false, false>'}
  248 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:275,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:25:8: note: declaration of 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 4, false, false, 0, Eigen::MakePointer>, 4, 0, false, false>'}
   25 | struct gemm_pack_rhs;
      |        ^~~~~~~~~~~~~
Target //tensorflow:libtensorflow.so failed to build
INFO: Elapsed time: 1163.559s, Critical Path: 148.84s
INFO: 4754 processes: 4754 local.
FAILED: Build did NOT complete successfully
```"
52153,Linking of rule '@com_github_grpc_grpc//src/compiler:grpc_cpp_plugin' failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: linux Centos7.7.1908
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: 3.8.10
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 9.3.1
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
hi,

I am trying to build TensorFlow 2.2.0's cpp dependency library on centos7.7, but bazel will always prompt me 
```
Linking of rule'@com_github_grpc_grpc//src/compiler:grpc_cpp_plugin' failed
```

I used the same method and everything went well when I built TensorFlow on Ubuntu system. I don’t know if it’s the problem of the system or my method.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
sk103@super5:/home/sk103/tool/tensorflow/tensorflow-2.2.0>./configure 
You have bazel 2.0.0 installed.
Please specify the location of python. [Default is /usr/bin/python]: 


Found possible Python library paths:
  /usr/local/python3/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/usr/local/python3/lib/python3.8/site-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
sk103@super5:/home/sk103/tool/tensorflow/tensorflow-2.2.0>
```

```
sk103@super5:/home/sk103/tool/tensorflow/tensorflow-2.2.0>bazel build //tensorflow:libtensorflow_cc.so
...

ERROR: /home/sk103/.cache/bazel/_bazel_sk103/46f66c8afcc04e6816b6d553e9e7afff/external/com_github_grpc_grpc/src/compiler/BUILD:80:1: Linking of rule '@com_github_grpc_grpc//src/compiler:grpc_cpp_plugin' failed (Exit 1)
bazel-out/host/bin/external/com_github_grpc_grpc/src/compiler/_objs/grpc_cpp_plugin/cpp_plugin.o:cpp_plugin.cc:function ProtoBufMethod::~ProtoBufMethod(): error: undefined reference to 'operator delete(void*, unsigned long)'
bazel-out/host/bin/external/com_github_grpc_grpc/src/compiler/_objs/grpc_cpp_plugin/cpp_plugin.o:cpp_plugin.cc:function ProtoBufService::~ProtoBufService(): error: undefined reference to 'operator delete(void*, unsigned long)'
bazel-out/host/bin/external/com_github_grpc_grpc/src/compiler/_objs/grpc_cpp_plugin/cpp_plugin.o:cpp_plugin.cc:function ProtoBufFile::~ProtoBufFile(): error: undefined reference to 'operator delete(void*, unsigned long)'
bazel-out/host/bin/external/com_github_grpc_grpc/src/compiler/_objs/grpc_cpp_plugin/cpp_plugin.o:cpp_plugin.cc:function google::protobuf::io::StringOutputStream::~StringOutputStream(): error: undefined reference to 'operator delete(void*, unsigned long)'
bazel-out/host/bin/external/com_github_grpc_grpc/src/compiler/_objs/grpc_cpp_plugin/cpp_plugin.o:cpp_plugin.cc:function grpc_cpp_generator::ClassName(google::protobuf::Descriptor const*, bool): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'
bazel-out/host/bin/external/com_github_grpc_grpc/src/compiler/_objs/grpc_cpp_plugin/cpp_plugin.o:cpp_plugin.cc:function ProtoBufFile::package_parts() const: error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'
bazel-out/host/bin/external/com_github_grpc_grpc/src/compiler/_objs/grpc_cpp_plugin/cpp_plugin.o:cpp_plugin.cc:function CppGrpcGenerator::Generate(google::protobuf::FileDescriptor const*, std::string const&, google::protobuf::compiler::GeneratorContext*, std::string*) const: error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'
bazel-out/host/bin/external/com_github_grpc_grpc/src/compiler/_objs/grpc_cpp_plugin/cpp_plugin.o:cpp_plugin.cc:function CppGrpcGenerator::Generate(google::protobuf::FileDescriptor const*, std::string const&, google::protobuf::compiler::GeneratorContext*, std::string*) const: error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'
bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/dynamic_message.o:dynamic_message.cc:function google::protobuf::DynamicMessageFactory::GetPrototypeNoLock(google::protobuf::Descriptor const*) [clone .cold]: error: undefined reference to '__cxa_throw_bad_array_new_length'
bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/struct.pb.o:struct.pb.cc:function google::protobuf::Struct::SerializeWithCachedSizes(google::protobuf::io::CodedOutputStream*) const [clone .cold]: error: undefined reference to '__cxa_throw_bad_array_new_length'
bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/extension_set.o:extension_set.cc:function google::protobuf::internal::ExtensionSet::~ExtensionSet(): error: undefined reference to 'operator delete[](void*, unsigned long)'
bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/extension_set.o:extension_set.cc:function google::protobuf::internal::ExtensionSet::GrowCapacity(unsigned long): error: undefined reference to 'operator delete[](void*, unsigned long)'
collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/sk103/tool/tensorflow/tensorflow-2.2.0/tensorflow/core/BUILD:1763:1 Linking of rule '@com_github_grpc_grpc//src/compiler:grpc_cpp_plugin' failed (Exit 1)
INFO: Elapsed time: 94.025s, Critical Path: 28.32s
INFO: 347 processes: 347 local.
FAILED: Build did NOT complete successfully

```"
52152,TensorFlow unit test failure kernels:requantize_op_test on AARCH64,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git HEAD
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Test fails

**Describe the expected behavior**

Test passes

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

bazel test //tensorflow/core/kernels:requantize_op_test

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

[ RUN      ] RequantizeTest.HandCraftedRequantize
tensorflow/core/framework/tensor_testutil.cc:128: Failure
Value of: IsEqual(Tx[i], Ty[i], t)
Actual: false (128 not equal to 127)
Expected: true
i = 1
[  FAILED  ] RequantizeTest.HandCraftedRequantize (37 ms)
"
52151,mkl_fused_batch_norm_op_test failing ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.0 / 2.6.0
- Python version: 3.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3

**Describe the current behavior**

According to https://groups.google.com/a/tensorflow.org/g/build/c/RZhgZst-fgQ we build without `--config=mkl` and  when running the tests of the build the test //tensorflow/core/kernels/mkl:mkl_fused_batch_norm_op_test fails:

```
[==========] 5 tests from 1 test suite ran. (197 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 5 tests, listed below:
[  FAILED  ] Test/FusedBatchNormOpTest/0.Training, where TypeParam = float
[  FAILED  ] Test/FusedBatchNormOpTest/0.TrainingRunningMean, where TypeParam = float
[  FAILED  ] Test/FusedBatchNormOpTest/0.Inference, where TypeParam = float
[  FAILED  ] Test/FusedBatchNormOpTest/0.InferenceIgnoreAvgFactor, where TypeParam = float
[  FAILED  ] Test/FusedBatchNormOpTest/0.FusedBatchNormGradV3, where TypeParam = float
```

The last one (`FusedBatchNormGradV3`) seemingly succeeds on TF 2.6 while the other 4 fail on 2.5 and 2.6.
The tests succeed when using `--config=mkl` and on other systems. It seems the AMD Epyc CPUs are affected, another Intel node works fine. So that might be related although the Intel CPUs are a bit older (broadwell) and we use `-march=native`.

**Other info / logs**

Test log: [test.log](https://github.com/tensorflow/tensorflow/files/7236226/test.log)

Command used: `CC_OPT_FLAGS=""-O3 -march=native -fno-math-errno -fPIC"" bazel  test --config=noaws --config=nogcp --config=nohdfs --compilation_mode=opt --config=opt --subcommands --verbose_failures --jobs=64 --copt=""-fPIC"" --action_env=PYTHONPATH --action_env=EBPYTHONPREFIXES --action_env=PYTHONNOUSERSITE=1 --distinct_host_configuration=false --test_output=errors --build_tests_only --local_test_jobs=64 --test_env=CUDA_VISIBLE_DEVICES='-1' --test_timeout=3600 -- //tensorflow/core/kernels/mkl:mkl_fused_batch_norm_op_test`"
52150,Add full parameter to SSIM,"**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Currently, `tf.image.ssim` and also `tf.image.ssim_multiscale` return the mean ssim value of the images (across height and width dimensions), but returning the full ssim image would be highly useful for multiple applications (reconstruction autoencoders, etc).

**Will this change the current api? How?**

Yes, both functions (`tf.image.ssim` and `tf.image.ssim_multiscale`) will include a new parameter `full` and will return a tensor containing the full ssim image if `full` is True. In fact, this ressembles the api of [scikit-image structural_similarity](https://scikit-image.org/docs/dev/api/skimage.metrics.html?highlight=structural_similarity#skimage.metrics.structural_similarity).

**Any Other info.**

The implementation for `tf.image.ssim` is straightforward.  It consists of removing the `reduce_mean` operation if `full` is True, or conversely `reduce_mean` for no axes (`axes=[]`).

https://github.com/tensorflow/tensorflow/blob/bdc6a138403e8257841e8dff6d6b9322bb65053a/tensorflow/python/ops/image_ops_impl.py#L4227

I am not sure about the implementation for `tf.image.ssim_multiscale`, but it should be similar (or even the same)."
52149,TPU not connecting in virtualenv in Cloud TPU-VM,"TF failed to connect to TPU when TF installed in virtualenv.
getting the following error

```
Tensorflow version 2.6.0
2021-09-27 08:27:32.379081: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-27 08:27:32.420848: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Traceback (most recent call last):
  File ""/home/raj/env/lib/python3.8/site-packages/tensorflow/python/tpu/tpu_strategy_util.py"", line 107, in initialize_tpu_system
    output = _tpu_init_fn()
  File ""/home/raj/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3039, in __call__
    return graph_function._call_flat(
  File ""/home/raj/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/home/raj/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 591, in call
    outputs = execute.execute(
  File ""/home/raj/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_embedding_config="""", is_global_init=false, embedding_config="""", enable_whole_mesh_compilations=false]
Registered devices: [CPU]
Registered kernels:
  <no registered kernels>

         [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tpu-test.py"", line 10, in <module>
    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
  File ""/home/raj/env/lib/python3.8/site-packages/tensorflow/python/tpu/tpu_strategy_util.py"", line 110, in initialize_tpu_system
    raise errors.NotFoundError(
tensorflow.python.framework.errors_impl.NotFoundError: TPUs not found in the cluster. Failed in initialization: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_embedding_config="""", is_global_init=false, embedding_config="""", enable_whole_mesh_compilations=false]
Registered devices: [CPU]
Registered kernels:
  <no registered kernels>

         [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]
```
code runs successfully with pre-installed TensorFlow. Fails when installed in a virtualenv
pre-installed TF version `tf-nightly==2.6.0`


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
TPU-VM create
```bash
gcloud alpha compute tpus tpu-vm create deberta --accelerator-type=v3-8 --version=v2-alpha --zone europe-west4-a
```
```bash
virtualenv env
source env/bin/activate
```
```python
import tensorflow as tf
print(""Tensorflow version "" + tf.__version__)

@tf.function
def add_fn(x,y):
  z = x + y
  return z

cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')
tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
strategy = tf.distribute.TPUStrategy(cluster_resolver)

x = tf.constant(1.)
y = tf.constant(1.)
z = strategy.run(add_fn, args=(x,y))
print(z)
```
```
python tpu-test.py
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52148,tf.matmul returns wrong results if called within tf.vectorized_map,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- macOS Big Sure, Version 11.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.8.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
`tf.matmul(a,b, transpose_a=False, transpose_b=True)` returns wrong result for `a,b` of complex
dtypes if wrapped with `tf.vectorized_map`. In fact, `tf.matmul(a,b, transpose_a=False, transpose_b=True)` returns the result expected from the call `tf.matmul(a,b, adjoint_a=False, adjoint_b=True)`, and vice versa.
**Describe the expected behavior**
`tf.matmul(a,b, transpose_a=False, transpose_b=True)` should return `a@b.T` if wrapped in `tf.vectorized_map`

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**
```python 
import tensorflow as tf
import numpy as np
def matmul_adjoint(args):
    t1, t2 = args
    return tf.matmul(t1, t2, adjoint_a=False, adjoint_b=True)


def matmul_transpose(args):
    t1, t2 = args
    return tf.matmul(t1, t2, transpose_a=False, transpose_b=True)

def test_vectorized_matmul(dtype):
    rdtype= np.ones(0, dtype).real.dtype
    if dtype in (np.complex64, np.complex128):
        a = np.random.rand(2,4,4).astype(rdtype) + 1j * np.random.rand(2,4,4).astype(rdtype)
        b = np.random.rand(2,4,4).astype(rdtype) + 1j * np.random.rand(2,4,4).astype(rdtype)
    else:
        a = np.random.rand(2,4,4).astype(rdtype) 
        b = np.random.rand(2,4,4).astype(rdtype)
    A = tf.convert_to_tensor(a, dtype=dtype)
    B = tf.convert_to_tensor(b, dtype=dtype)
    result_adjoint = tf.vectorized_map(matmul_adjoint,(A,B))
    result_transpose = tf.vectorized_map(matmul_transpose,(A,B))
    expected_transpose = []
    for n in range(2):
        expected_transpose.append(a[n] @ b[n].T)
    expected_transpose = np.stack(expected_transpose)
    expected_adjoint = []
    for n in range(2):
        expected_adjoint.append(a[n] @ (b[n].T.conj()))
    expected_adjoint = np.stack(expected_adjoint)
    eps = np.finfo(rdtype).eps
    try:
        np.testing.assert_allclose(result_adjoint, expected_adjoint, 
                                   atol=10*eps, rtol=10*eps)
    except AssertionError as err:
        print(''.join(['#']*60))
        print(f""matmul adjoint test failed failed for dtype {dtype} with error {err}"")
    try:
        np.testing.assert_allclose(result_transpose, expected_transpose, atol=10*eps, rtol=10*eps)
    except AssertionError as err:
        print(''.join(['#']*60))
        print(f""matmul_transpose failed for dtype {dtype} with error {err}"")
      
    # the following passes, so it seems that the transpose and adjoint arguments
    # to tf.matmul need to be swapped
    np.testing.assert_allclose(result_adjoint, expected_transpose, 
                               atol=10*eps, rtol=10*eps)
    
    np.testing.assert_allclose(result_transpose, expected_adjoint, 
                               atol=10*eps, rtol=10*eps)
    
test_vectorized_matmul(np.float32)
test_vectorized_matmul(np.complex64)

```
"
52147,tensorFlow.compat.v1 no module named compat,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are
not verified bugs in TensorFlow, please go to
[Discourse](https://discuss.tensorflow.org/).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
52144,returning NUMA node zero?,"**System information**
- Fedora 34
- TensorFlow version: 2.6.0
- Python version: 3.9.7
- Installed Tensorflow using pip
- CUDA/cuDNN version: 11.4
- GPU model and memory: NVIDIA T600 / 4GB

**Problem description:**
```python
import tensorflow as tf
tf.config.list_physical_devices('GPU')
```

> 2021-09-26 15:29:11.893611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-09-26 15:29:11.898701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-09-26 15:29:11.898993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

Should I solve it? or ignore it?"
52143,Errors,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04 and 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung galaxy m12
- TensorFlow installed from (source or binary): I used the venv code
- TensorFlow version (use command below): none: read below
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: mediatek and 6gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
`(venv) om@localhost:~/twitchtubemain$ python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
> import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)
>`


**Describe the current behavior**
(venv) om@localhost:~/twitchtubemain$ pip install --upgrade tensorflow
Collecting tensorflow
  Cache entry deserialization failed, entry ignored
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
(venv) om@localhost:~/twitchtubemain$
**Describe the expected behavior**
Tensor flow getting installed using pip

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
(venv) om@localhost:~/twitchtubemain$ pip install --upgrade tensorflow
Collecting tensorflow
  Cache entry deserialization failed, entry ignored
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
(venv) om@localhost:~/twitchtubemain$
"
52142,Cannot convert concrete functions with no input arguments,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): from pip package tf-nightly
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.7.0-dev20210922

### 2. Code

Current TFLite conversion code seems to assume that all signatures take at least 1 input parameter. If you try to export a signature with no input parameters, you get the following error:

`tensorflow.lite.python.convert_phase.ConverterError: input array size mismatch: got 1, expected: 0`

Here's an example code reproducing the issue. If an input argument is added to the `test` function (even if ignored), and accordingly to `get_concrete_function`, the error disappears.

```python
import tensorflow as tf                                                        
                                                                               
class TestModel(tf.keras.models.Model):
  @tf.function
  def test(self):
    return 123

test_model = TestModel()
signatures = [test_model.test.get_concrete_function()]

converter = tf.lite.TFLiteConverter.from_concrete_functions(signatures, test_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
```

### 3. Other info

In another code also trying to export a function without arguments I got this other variation of the error.

`ValueError: Failed to parse the model: NULL SignatureDef inputs for exported method testFailed to construct interpreter.`

However, I couldn't provide a simple repro code for this exact one. Sharing just in case it gives some kind of additional clue."
52141,Is it even possible to build tensorflow on Windows? ,"**System information**
- Windows 10 Pro x64 - 19043.1237
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4
- Python version: 3.8
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 3.7.1
- GCC/Compiler version (if compiling from source): VC 14.29.30133
- CUDA/cuDNN version: 11.0/8.0
- GPU model and memory: Nvidia GTX 1080


I checked the the build from source page and followed the instructions. I have the repository cloned, I checked out 2.4 and I configured it with default settings except for 'cuda support' which I enabled. 

I go to bazel, try to build it, and all hell breaks loose: 

`bazel build -c opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow:tensorflow.dll --verbose_failures.` 

> ERROR: Config value 'cuda' is not defined in any .rc file

Even if I give up on cuda option, it still fails: 

`bazel build -c opt --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow:tensorflow.so --verbose_failures`

> ERROR: no such target '//tensorflow:tensorflow.dll': target 'tensorflow.dll' not declared in package 'tensorflow'

Then I tried a suggestion to use libtensorflow_cc.so: 

`bazel build -c opt --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow:libtensorflow_cc.so --verbose_failures`

> ERROR: no such target '//tensorflow:libtensorflow_cc.so': target 'libtensorflow_cc.so' not declared in package 'tensorflow'

In other words, nothing is even remotely working. 

I don't understand anything about anything at this point. Is it not supposed to work? Was support for Windows removed? "
52140,More than 20% of video memory missing both Linux and Windows [RTX 3080],"### **System information**
**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**
This is the same for my custom code and for simple scripts while setting GPU in TensorFlow. 
**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**
This was tested on Windows 10 and Linux Ubuntu 20.04
**- TensorFlow installed from (source or binary):**
TensorFlow installed from binary.
**- TensorFlow version (use command below):**
This was tested on 2.5.0, 2.5.1 and 2.6.0
**- Python version:**
Python 3.8.10
**- CUDA/cuDNN version:**
Tested on 
Cuda Toolkit: 11.2, 11.4
CuDNN: 8.1.0, 8.1.1, 8.2.4
**- GPU model and memory:**
MSI RTX 3080 10GB memory

### **Describe the current behavior**
I have a 10GB 3080RTX GPU, NVidia-smi reports 10014MiB memory, Tensorflow reports:
> Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7591 MB memory

After initial research I was convinced that this is related to Windows 10 OS limitations, so I installed Ubuntu 20.04 in dual boot. It didn't change anything, I tried various versions of Tensorflow, Cuda, Cudnn.
I tried using:

```
physical_devices = tf.config.list_physical_devices('GPU')
for gpu_instance in physical_devices:
    tf.config.experimental.set_memory_growth(gpu_instance, True)

```

It didn't fix the problem. Also, I tried :

```
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 1.0
session = InteractiveSession(config=config)

```
And indeed, TensorFlow started to report proper full 10GB of memory in 'Created device' message, so tf should see the memory properly. With this method I was able to push memory to almost 8GB and it even allowed me to run slightly higher batch size. But, if I specify fraction of more than 0.8 (it may slightly vary from run-to-run) than i have:
> 2021-09-26 12:48:26.691479: F tensorflow/core/util/cuda_solvers.cc:115] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.

One important thing to note, while TensorFlow is reporting a device with ~7.5GB, in nvidia-smi it is reporting more than 9GB by /usr/bin/python3! I am not running any other Python script in parallel. 

![Screenshot from 2021-09-26 12-53-00](https://user-images.githubusercontent.com/13711526/134804804-3dced212-188e-4e49-881e-d19e7e6c8d90.png)

So, the memory usage in reality is reaching its limits while I am able to use only 7.5GB, which is even less than known 81% limitation for Windows 10 users! Why am I being allocated almost extra 2GB on top which I can't even use for training?

I was trying to fix it for a long time and really don't have any idea what to do now. Other people's problems with missing tf memory that I found on Internet were related to Windows OS, mine is not. Am I missing something? I would really appreciate any idea on what is going on.
Thank you in advance.

"
52139,When does tensorflow 2.x support crf？,"In view of the fact that the crf implemented by tensorflow addons or implemented by ourselves are integrated into the tensorflow 2.x framework is not friendly, so, when will tensorflow 2.x support crf?"
52138,(Apple Silicon aka M1) import tensorflow killed python in terminal,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.6 (Big Sur)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.4, 2.5, 2.6
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: conda (Miniforge 3 ARM64), pip 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: M1

**Describe the problem**
Hi, there. 
I have a problem with importing tensorflow related modules (tensorflow, keras, etc.). 
Others e.g., pandas, numpy, pillow, etc. worked without issue.
Obviously, I installed tensorflow supporting arm64 NOT Intel x86x64, and other modules such as PyTorch installed as ARM worked well without Rosseta.

After executing 'import tensorflow' in the terminal, it instantaneously kills python. (zsh: killed python)
I tried to change the python version to 3.8 and tensorflow version to 2.4 or 2.5, but results were the same. 
Reinstalling miniforge 3 ARM64 also did not work well.
Actually, importing tensorflow worked very well about just a week ago, but suddenly it didn't work :(

However, one strange thing is when I open the terminal with Rosetta, there is no issue for importing tensorflow and it uses GPU, too when I execute some toy example codes (MNIST).
Thanks!

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. tensorflow install command
conda install -c apple tensorflow-deps
conda install tensorflow
pip instal ltensorflow-metal

2. import tensorflow in terminal (M1 native)

Python 3.9.7 | packaged by conda-forge | (default, Sep 23 2021, 07:30:24) 
[Clang 11.1.0 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
'>>> import tensorflow as tf
zsh: killed     python

3. import tensorflow in terminal with Rosetta

Python 3.9.7 | packaged by conda-forge | (default, Sep 23 2021, 07:30:24)
[Clang 11.1.0 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

'>>> import tensorflow as tf
Init Plugin
Init Graph Optimizer
Init Kernel

'>>> tf.test.is_gpu_available()
WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
Metal device set to: Apple M1

systemMemory: 16.00 GB
maxCacheSize: 5.33 GB

2021-09-26 12:25:10.098855: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2021-09-26 12:25:10.099073: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
True

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

**(tmp) jh@JHs-M1-MacBookAir ~ % conda install -c apple tensorflow-deps**
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: done

Package Plan

  environment location: /opt/homebrew/Caskroom/miniforge/base/envs/tmp

  added / updated specs:
    - tensorflow-deps


The following NEW packages will be INSTALLED:

  absl-py            conda-forge/noarch::absl-py-0.10.0-pyhd8ed1ab_1
  aiohttp            conda-forge/osx-arm64::aiohttp-3.7.4.post0-py39h5161555_0
  astunparse         conda-forge/noarch::astunparse-1.6.3-pyhd8ed1ab_0
  async-timeout      conda-forge/noarch::async-timeout-3.0.1-py_1000
  attrs              conda-forge/noarch::attrs-21.2.0-pyhd8ed1ab_0
  blinker            conda-forge/noarch::blinker-1.4-py_1
  brotlipy           conda-forge/osx-arm64::brotlipy-0.7.0-py39h5161555_1001
  c-ares             conda-forge/osx-arm64::c-ares-1.17.2-h3422bc3_0
  ca-certificates    conda-forge/osx-arm64::ca-certificates-2021.5.30-h4653dfc_0
  cached-property    conda-forge/noarch::cached-property-1.5.2-hd8ed1ab_1
  cached_property    conda-forge/noarch::cached_property-1.5.2-pyha770c72_1
  cachetools         conda-forge/noarch::cachetools-4.2.2-pyhd8ed1ab_0
  certifi            conda-forge/osx-arm64::certifi-2021.5.30-py39h2804cbe_0
  cffi               conda-forge/osx-arm64::cffi-1.14.6-py39h52b1de0_1
  chardet            conda-forge/osx-arm64::chardet-4.0.0-py39h2804cbe_1
  charset-normalizer conda-forge/noarch::charset-normalizer-2.0.0-pyhd8ed1ab_0
  click              conda-forge/osx-arm64::click-8.0.1-py39h2804cbe_0
  cryptography       conda-forge/osx-arm64::cryptography-3.4.7-py39h73257c9_0
  dataclasses        conda-forge/noarch::dataclasses-0.8-pyhc8e2a94_3
  flatbuffers        conda-forge/osx-arm64::flatbuffers-2.0.0-hbdafb3b_0
  gast               conda-forge/noarch::gast-0.4.0-pyh9f0ad1d_0
  google-auth        conda-forge/noarch::google-auth-1.35.0-pyh6c4a22f_0
  google-auth-oauth~ conda-forge/noarch::google-auth-oauthlib-0.4.6-pyhd8ed1ab_0
  google-pasta       conda-forge/noarch::google-pasta-0.2.0-pyh8c360ce_0
  grpcio             conda-forge/osx-arm64::grpcio-1.38.1-py39h9e1b6db_0
  h5py               conda-forge/osx-arm64::h5py-3.1.0-nompi_py39h99babb8_100
  hdf5               conda-forge/osx-arm64::hdf5-1.10.6-nompi_h0fc092c_1114
  idna               conda-forge/noarch::idna-3.1-pyhd3deb0d_0
  importlib-metadata conda-forge/osx-arm64::importlib-metadata-4.8.1-py39h2804cbe_0
  keras              conda-forge/noarch::keras-2.6.0-pyhd8ed1ab_0
  keras-preprocessi~ conda-forge/noarch::keras-preprocessing-1.1.2-pyhd8ed1ab_0
  krb5               conda-forge/osx-arm64::krb5-1.19.2-hd92b7a7_0
  libblas            conda-forge/osx-arm64::libblas-3.9.0-11_osxarm64_openblas
  libcblas           conda-forge/osx-arm64::libcblas-3.9.0-11_osxarm64_openblas
  libclang           conda-forge/osx-arm64::libclang-11.1.0-default_h0fdd720_1
  libcurl            conda-forge/osx-arm64::libcurl-7.79.1-h8fe1914_0
  libcxx             conda-forge/osx-arm64::libcxx-12.0.1-h168391b_0
  libedit            conda-forge/osx-arm64::libedit-3.1.20191231-hc8eb9b7_2
  libev              conda-forge/osx-arm64::libev-4.33-h642e427_1
  libffi             conda-forge/osx-arm64::libffi-3.4.2-hbdafb3b_4
  libgfortran        conda-forge/osx-arm64::libgfortran-5.0.0.dev0-11_0_1_hf114ba7_23
  libgfortran5       conda-forge/osx-arm64::libgfortran5-11.0.1.dev0-hf114ba7_23
  liblapack          conda-forge/osx-arm64::liblapack-3.9.0-11_osxarm64_openblas
  libllvm11          conda-forge/osx-arm64::libllvm11-11.1.0-h93073aa_2
  libnghttp2         conda-forge/osx-arm64::libnghttp2-1.43.0-hf3018f0_0
  libopenblas        conda-forge/osx-arm64::libopenblas-0.3.17-openmp_h5dd58f0_1
  libprotobuf        conda-forge/osx-arm64::libprotobuf-3.18.0-hccf11d3_0
  libssh2            conda-forge/osx-arm64::libssh2-1.10.0-hb80f160_1
  llvm-openmp        conda-forge/osx-arm64::llvm-openmp-12.0.1-hf3c4609_1
  markdown           conda-forge/noarch::markdown-3.3.4-pyhd8ed1ab_0
  multidict          conda-forge/osx-arm64::multidict-5.1.0-py39h5161555_1
  ncurses            conda-forge/osx-arm64::ncurses-6.2-h9aa5885_4
  numpy              conda-forge/osx-arm64::numpy-1.19.5-py39h1f3b974_2
  oauthlib           conda-forge/noarch::oauthlib-3.1.1-pyhd8ed1ab_0
  openssl            conda-forge/osx-arm64::openssl-1.1.1l-h3422bc3_0
  opt_einsum         conda-forge/noarch::opt_einsum-3.3.0-pyhd8ed1ab_1
  pip                conda-forge/noarch::pip-21.2.4-pyhd8ed1ab_0
  protobuf           conda-forge/osx-arm64::protobuf-3.18.0-py39hfb83b0d_0
  pyasn1             conda-forge/noarch::pyasn1-0.4.8-py_0
  pyasn1-modules     conda-forge/noarch::pyasn1-modules-0.2.7-py_0
  pycparser          conda-forge/noarch::pycparser-2.20-pyh9f0ad1d_2
  pyjwt              conda-forge/noarch::pyjwt-2.1.0-pyhd8ed1ab_0
  pyopenssl          conda-forge/noarch::pyopenssl-20.0.1-pyhd8ed1ab_0
  pysocks            conda-forge/osx-arm64::pysocks-1.7.1-py39h2804cbe_3
  python             conda-forge/osx-arm64::python-3.9.7-h54d631c_2_cpython
  python_abi         conda-forge/osx-arm64::python_abi-3.9-2_cp39
  pyu2f              conda-forge/noarch::pyu2f-0.1.5-pyhd8ed1ab_0
  readline           conda-forge/osx-arm64::readline-8.1-hedafd6a_0
  requests           conda-forge/noarch::requests-2.26.0-pyhd8ed1ab_0
  requests-oauthlib  conda-forge/noarch::requests-oauthlib-1.3.0-pyh9f0ad1d_0
  rsa                conda-forge/noarch::rsa-4.7.2-pyh44b312d_0
  scipy              conda-forge/osx-arm64::scipy-1.7.0-py39h5060c3b_0
  setuptools         conda-forge/osx-arm64::setuptools-58.0.4-py39h2804cbe_2
  six                conda-forge/noarch::six-1.15.0-pyh9f0ad1d_0
  sqlite             conda-forge/osx-arm64::sqlite-3.36.0-h72a2b83_2
  tensorboard        conda-forge/noarch::tensorboard-2.6.0-pyhd8ed1ab_1
  tensorboard-data-~ conda-forge/osx-arm64::tensorboard-data-server-0.6.0-py39hfb8cd70_0
  tensorboard-plugi~ conda-forge/noarch::tensorboard-plugin-wit-1.8.0-pyh44b312d_0
  tensorflow-deps    apple/osx-arm64::tensorflow-deps-2.6.0-0
  termcolor          conda-forge/noarch::termcolor-1.1.0-py_2
  tk                 conda-forge/osx-arm64::tk-8.6.11-he1e0b03_1
  typing-extensions  conda-forge/noarch::typing-extensions-3.7.4.3-0
  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0
  tzdata             conda-forge/noarch::tzdata-2021a-he74cb21_1
  urllib3            conda-forge/noarch::urllib3-1.26.7-pyhd8ed1ab_0
  werkzeug           conda-forge/noarch::werkzeug-2.0.1-pyhd8ed1ab_0
  wheel              conda-forge/noarch::wheel-0.35.1-pyh9f0ad1d_0
  wrapt              conda-forge/osx-arm64::wrapt-1.12.1-py39h5161555_3
  xz                 conda-forge/osx-arm64::xz-5.2.5-h642e427_1
  yarl               conda-forge/osx-arm64::yarl-1.6.3-py39h5161555_2
  zipp               conda-forge/noarch::zipp-3.5.0-pyhd8ed1ab_0
  zlib               conda-forge/osx-arm64::zlib-1.2.11-h31e879b_1009


Proceed ([y]/n)? y

Preparing transaction: done
Verifying transaction: done
Executing transaction: done

**(tmp) jh@JHs-M1-MacBookAir ~ % conda install tensorflow**
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: done

Package Plan

  environment location: /opt/homebrew/Caskroom/miniforge/base/envs/tmp

  added / updated specs:
    - tensorflow


The following NEW packages will be INSTALLED:

  abseil-cpp         conda-forge/osx-arm64::abseil-cpp-20210324.2-hbdafb3b_0
  astor              conda-forge/noarch::astor-0.8.1-pyh9f0ad1d_0
  giflib             conda-forge/osx-arm64::giflib-5.2.1-h27ca646_2
  grpc-cpp           conda-forge/osx-arm64::grpc-cpp-1.37.1-h538f867_3
  icu                conda-forge/osx-arm64::icu-68.1-h17758a7_0
  jpeg               conda-forge/osx-arm64::jpeg-9d-h27ca646_0
  libpng             conda-forge/osx-arm64::libpng-1.6.37-hf7e6567_2
  python-flatbuffers conda-forge/noarch::python-flatbuffers-1.12-pyhd8ed1ab_1
  re2                conda-forge/osx-arm64::re2-2021.08.01-hbdafb3b_0
  snappy             conda-forge/osx-arm64::snappy-1.1.8-hc88da5d_3
  tensorflow         conda-forge/osx-arm64::tensorflow-2.6.0-py39hdf13c20_0
  tensorflow-base    conda-forge/osx-arm64::tensorflow-base-2.6.0-py39h34a2d98_0
  tensorflow-estima~ conda-forge/osx-arm64::tensorflow-estimator-2.6.0-py39h4ec10df_0

The following packages will be DOWNGRADED:

  grpcio                              1.38.1-py39h9e1b6db_0 --> 1.37.1-py39h9e1b6db_0
  libprotobuf                             3.18.0-hccf11d3_0 --> 3.15.8-hccf11d3_1
  protobuf                            3.18.0-py39hfb83b0d_0 --> 3.15.8-py39hfb83b0d_0


Proceed ([y]/n)? y

Preparing transaction: done
Verifying transaction: done
Executing transaction: done

**(tmp) jh@JHs-M1-MacBookAir ~ % pip install tensorflow-metal**
Collecting tensorflow-metal
  Using cached tensorflow_metal-0.1.2-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)
Requirement already satisfied: wheel~=0.35 in /opt/homebrew/Caskroom/miniforge/base/envs/tmp/lib/python3.9/site-packages (from tensorflow-metal) (0.35.1)
Requirement already satisfied: six~=1.15.0 in /opt/homebrew/Caskroom/miniforge/base/envs/tmp/lib/python3.9/site-packages (from tensorflow-metal) (1.15.0)
Installing collected packages: tensorflow-metal
Successfully installed tensorflow-metal-0.1.2
"
52135,Restart kernel (Ipython Konsole in Spyder 5.1.5) with os._exit(0) does not work when using tensorflow.distribute.MirroredStrategy,"Hi together,

I have installed

****System information**
- OS Platform and Distribution: Windows Server 2016
- TensorFlow installed from: binary? (conda install)
- TensorFlow version: 2.6.0
- Python version: 3.9.7
- Installed using: pip & conda
- Bazel version: ?
- GCC/Compiler version:  4.7.0 20111220
- CUDA/cuDNN version: 8.1.0.77 & 11.2.2
- GPU model and memory:  2xTesla M60, 2x8gb**


**When I use the code:

```
import tensorflow
import os

os._exit(0)
```

in Spyder 5.1.5 the kernel (IPython-Konsole) gets restarted without problems.

When I use the code:

```
import tensorflow
import os

strategy = tensorflow.distribute.MirroredStrategy(cross_device_ops=tensorflow.distribute.HierarchicalCopyAllReduce())

os._exit(0)
```

in Spyder 5.1.5 the kernel (IPython-Konsole) could not be restarted properly.

The error message which is displayed is:

```
**Beim Starten des Kernels ist ein Fehler aufgetreten** (GERMAN)

2021...15:44:01.699142: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance‑critical operations: AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021...15:44:02.898151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6938 MB memory: ‑> device: 0, name: Tesla M60, pci bus id: 0000:04:00.0, compute capability: 5.2
2021...15:44:02.903256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6939 MB memory: ‑> device: 1, name: Tesla M60, pci bus id: 0000:05:00.0, compute capability: 5.2
```
**

I want to quit and restart a kernel session when a condition in the training arises to start the current script from new. (GPU memory etc. gets released with a restart)

Why is this behaviour and how can I get around this, thanks?
"
52134,Function call stack: train_function ,"
![2](https://user-images.githubusercontent.com/26819449/134773664-3244a63e-aab1-4259-8c1a-27a0fb2b91df.JPG)

Why I am getting this error? any solution? 

"
52133,ValueError: unknown url type: '/content/Mango',"![1](https://user-images.githubusercontent.com/26819449/134773285-e34a8aaf-2cbe-43be-9507-94cf0d47283d.JPG)
 I want to run my Mango dataset. Can you please tell me what I am doing wrong here.?
Thank You."
52132,TF 2.6 OOM Error not happening in TF 2.3,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.9.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2.2/8.2.1
- GPU model and memory: NVIDIA GEForce GTX 1060

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Code crashes at the end of an epoch when training 
[tf-2-6.log](https://github.com/tensorflow/tensorflow/files/7229788/tf-2-6.log)
with a larger size. In TF2.3, warning was generated but code used to run. Also, I have tried using as below
'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
When I set this env variable and run, the code simple exits after sometime and as far as i can see, the fit function is not called. 
**Describe the expected behavior**
Code is expected to run without crash. In TF 2.3, there is no problem. In tf 2.6, there is a crash.
**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Code is same as the keras example below
https://keras.io/examples/vision/image_classification_from_scratch/
Problem occurs at batch size 32. For this batch size, though i get the initial warning about memory,the code does not crash.
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52131,build failed with bazel 4.2.1 ( windows 10 ),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7.0rc0
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 4.2.1
- GCC/Compiler version (if compiling from source): Visual Studio 2019
- CUDA/cuDNN version: 11.4 / 8.2.6
- GPU model and memory: RTX3090 GDDR6X 24GB



**Describe the problem**

build error (permission)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel build --copt=-nvcc_options=disable-warnings --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
INFO: Repository llvm-project instantiated at:
  D:/repo/tensorflow/WORKSPACE:15:14: in <toplevel>
  D:/repo/tensorflow/tensorflow/workspace2.bzl:1090:21: in workspace
  D:/repo/tensorflow/tensorflow/workspace2.bzl:654:15: in _tf_repositories
  D:/repo/tensorflow/third_party/llvm/setup.bzl:10:19: in llvm_setup
Repository rule llvm_configure defined at:
  C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/configure.bzl:83:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'llvm-project':
   Traceback (most recent call last):
        File ""C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/configure.bzl"", line 73, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/configure.bzl"", line 62, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'D:/anaconda3/python.exe C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/overlay_directories.py --src C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw --overlay C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""C:\users\alanp\_bazel_alanp\ibqopsat\external\llvm-raw\utils\bazel\overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""C:\users\alanp\_bazel_alanp\ibqopsat\external\llvm-raw\utils\bazel\overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""C:\users\alanp\_bazel_alanp\ibqopsat\external\llvm-raw\utils\bazel\overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] A required privilege is not held by the client: 'C:\\users\\alanp\\_bazel_alanp\\ibqopsat\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'C:\\users\\alanp\\_bazel_alanp\\ibqopsat\\external\\llvm-project\\.bazelignore'

ERROR: Error fetching repository: Traceback (most recent call last):
        File ""C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/configure.bzl"", line 73, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/configure.bzl"", line 62, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'D:/anaconda3/python.exe C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/overlay_directories.py --src C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw --overlay C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""C:\users\alanp\_bazel_alanp\ibqopsat\external\llvm-raw\utils\bazel\overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""C:\users\alanp\_bazel_alanp\ibqopsat\external\llvm-raw\utils\bazel\overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""C:\users\alanp\_bazel_alanp\ibqopsat\external\llvm-raw\utils\bazel\overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] A required privilege is not held by the client: 'C:\\users\\alanp\\_bazel_alanp\\ibqopsat\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'C:\\users\\alanp\\_bazel_alanp\\ibqopsat\\external\\llvm-project\\.bazelignore'

ERROR: D:/repo/tensorflow/tensorflow/tools/pip_package/BUILD:278:10: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/compiler/mlir/tensorflow:gen_mlir_passthrough_op_py in repository @ which failed to fetch. no such package '@llvm-project//mlir': Failed to execute overlay script: 'D:/anaconda3/python.exe C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/overlay_directories.py --src C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw --overlay C:/users/alanp/_bazel_alanp/ibqopsat/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""C:\users\alanp\_bazel_alanp\ibqopsat\external\llvm-raw\utils\bazel\overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""C:\users\alanp\_bazel_alanp\ibqopsat\external\llvm-raw\utils\bazel\overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""C:\users\alanp\_bazel_alanp\ibqopsat\external\llvm-raw\utils\bazel\overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] A required privilege is not held by the client: 'C:\\users\\alanp\\_bazel_alanp\\ibqopsat\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'C:\\users\\alanp\\_bazel_alanp\\ibqopsat\\external\\llvm-project\\.bazelignore'

ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 110.401s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (53 packages loaded, 14 targets configured)
    currently loading: tensorflow/lite/python ... (2 packages)
    Fetching @flatbuffers; fetching
    Fetching ...uffers; Extracting C:/users/alanp/_bazel_alanp/ibqopsat/external/flatbuffers/temp10953078659403648945/\
v1.12.0.tar.gz
```"
52130,Tensorflow build debug failed,"

**System information**
- OS Platform and Distribution (e.g., Linux xxx-ROG 5.12.19-1-MANJARO #1 SMP PREEMPT Tue Jul 20 20:57:37 UTC 2021 x86_64 GNU/Linux):
- TensorFlow installed from (source or binary):source
- TensorFlow version: master,lastest commit:225be1b5fb5 Update TFRT dependency to use revision
- Python version:3.9.7
- Installed using virtualenv? pip? conda?:None,just use system python and pip
- Bazel version (if compiling from source): bazel 3.7.2
- GCC/Compiler version (if compiling from source):/usr/bin/gcc-10
- CUDA/cuDNN version:release 11.4, V11.4.100/cudnn8.2
- GPU model and memory:RTX3090 32G GDDR6,memory 64G 3200Mhz



**Describe the problem**
- command:
  -  bazel build --config=mkl --copt=-mavx2 --copt=-O3 --copt=-DINTEL_MKL_QUANTIZED -s -c dbg --verbose_failures //tensorflow/tools/pip_package:build_pip_package
  -  bazel build -c dbg --config=cuda  //tensorflow/tools/pip_package:build_pip_package
  - bazel build -c opt --copt=""-g"" --cxxopt=""-g"" //tensorflow/tools/pip_package:build_pip_package
**I had used all method,same error like this:**

/home/xxx/tensorflow/tensorflow/python/BUILD:3170:24: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-dbg/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x6087e): relocation truncated to fit: R_X86_64_32 against `.debug_info'
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x60c9c): relocation truncated to fit: R_X86_64_32 against `.debug_info'
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x60df0): relocation truncated to fit: R_X86_64_32 against `.debug_info'
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x60e1a): relocation truncated to fit: R_X86_64_32 against `.debug_info'
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x60ef7): relocation truncated to fit: R_X86_64_32 against `.debug_info'
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x60f0f): relocation truncated to fit: R_X86_64_32 against `.debug_info'
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x60f27): relocation truncated to fit: R_X86_64_32 against `.debug_info'
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x60f3f): relocation truncated to fit: R_X86_64_32 against `.debug_info'
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x60f57): relocation truncated to fit: R_X86_64_32 against `.debug_info'
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x60fbf): relocation truncated to fit: R_X86_64_32 against `.debug_info'
bazel-out/k8-dbg/bin/external/llvm-project/mlir/libGPUToNVVMTransforms.a(LowerGpuOpsToNVVMOps.pic.o):(.debug_loc+0x60fd7): additional relocation overflows omitted from the output
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.

"
52129,Tensorflow site language switching,"![tf-site](https://user-images.githubusercontent.com/5677691/134763303-c4bb3386-de5f-41e6-9ac7-6c5ef0f58e9e.png)
Switching from Russian to English doesn't work.
English -> Russian is ok
Russian -> French is ok
Russian -> Deutsch is ok
I've tried this: https://www.tensorflow.org/ and this
https://www.tensorflow.org/?hl=en
Also I've deleted all other languages except English from my Google Account."
52128,Differences between tf.saved_model.save and model.save,"## URL(s) with the issue:

- https://www.tensorflow.org/api_docs/python/tf/saved_model/save
- https://www.tensorflow.org/api_docs/python/tf/keras/Model#save

## Description of issue (what needs changing):

### Clear description

I was recently trying to save a subclassed model with `model.save()` (I had always thought `tf.saved_model.save` and `model.save` pretty much work the same way) which led me to receive:

> ValueError: Model <__main__. ... object at ...> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs).

whereas I had already set shapes, I was later able to easily save the model with `tf.saved_model.save` after seeing #31057 .

I believe adding to the docs how these two work differently would be a really useful addition.

### Submit a pull request?

> Are you planning to also submit a pull request to fix the issue?

I would love to create a PR after first discussing this issue."
52125,'pip install tensorflow' does not work on FreeBSD,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): FreeBSD 12.2
- TensorFlow installed from (source or binary): trying to install binary using `pip`
- TensorFlow version: any
- Python version: 3.8

**Describe the problem**

`pip install tensorflow` does not work on FreeBSD. Apparently there is no precompiled package for FreeBSD. Why?

```
FreeBSD% ./spleeter/bin/pip install tensorflow     
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```

**Any other info / logs**
Was pointed to this issue tracker from https://github.com/pypa/packaging-problems/issues/547."
52124,Mysterious bunch of meta_optimizer.cc:801 errors,"- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow compiled from source master, via Docker
- TensorFlow version 2.7.0
- Python version: 3.9.7
- Bazel version: 3.7.2
- CUDA/cuDNN version: 11.4/8.2
- GPU model and memory: NVIDIA 1080 Ti

I'm getting the following error messages which I don't understand if I have to pay attention to or disregard:

```
2021-09-24 18:59:43.243685: E tensorflow/core/framework/resource_handle.cc:39] A ref-counted ResourceHandle cannot be serialized losslesslyDeserializing the result is a failure: ShuffleDatasetV3/SeedGenerator_2
2021-09-24 18:59:53.258130: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2799 of 10000
2021-09-24 19:00:03.260611: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 5660 of 10000
2021-09-24 19:00:13.259760: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 8506 of 10000
2021-09-24 19:00:18.494994: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.
2021-09-24 19:00:20.879185: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: INVALID_ARGUMENT: Node 'model_lstm_partitionedcall_24_RetVal': Connecting to invalid output 27 of source node model/lstm/PartitionedCall which has 27 outputs.
2021-09-24 19:00:20.901732: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] layout failed: OUT_OF_RANGE: src_output = 27, but num_outputs is only 27
2021-09-24 19:00:20.929210: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: INVALID_ARGUMENT: Node 'model_lstm_partitionedcall_24_RetVal': Connecting to invalid output 27 of source node model/lstm/PartitionedCall which has 27 outputs.
2021-09-24 19:00:20.970470: W tensorflow/core/common_runtime/process_function_library_runtime.cc:859] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 0 of node model_lstm_partitionedcall_2_RetVal was passed bool from model/lstm/PartitionedCall:5 incompatible with expected int32.
2021-09-24 19:00:21.214614: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: INVALID_ARGUMENT: Input 5 of node gradients/decoder/lstm_1/PartitionedCall_grad/PartitionedCall was passed int32 from gradients_decoder_lstm_1_partitionedcall_grad_decoder_lstm_1_partitionedcall:0 incompatible with expected bool.
2021-09-24 19:00:21.275349: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: INVALID_ARGUMENT: Input 5 of node gradients/decoder/lstm_1/PartitionedCall_grad/PartitionedCall was passed int32 from gradients_decoder_lstm_1_partitionedcall_grad_decoder_lstm_1_partitionedcall:0 incompatible with expected bool.
2021-09-24 19:00:21.331375: W tensorflow/core/common_runtime/process_function_library_runtime.cc:859] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 5 of node gradients/decoder/lstm_1/PartitionedCall_grad/PartitionedCall was passed int32 from gradients_decoder_lstm_1_partitionedcall_grad_decoder_lstm_1_partitionedcall:0 incompatible with expected bool.
```

This happens while running custom training loop:

```
num_epochs = 20
optimizer = Adam(learning_rate=0.001)
train_loss_result = []
val_loss_result = []


@tf.function
def ger_preproc(ger):
    inputs = ger[:, :-1]
    outputs = ger[:, 1:]
    return inputs, outputs


@tf.function
def masked_loss(true_german, predicted_german):
    loss = SparseCategoricalCrossentropy(from_logits=True, reduction='none')(true_german, predicted_german)
    mask = tf.cast(true_german != 0, tf.float32)
    loss *= mask
    return tf.reduce_mean(loss)


@tf.function
def forward_pass(eng_inputs, ger_inputs):
    g_in, g_out = ger_preproc(ger_inputs)
    hidden_state, cell_state = encoder(eng_inputs)
    predicted_german, _, _ = decoder(g_in, hidden_state, cell_state)
    current_loss = masked_loss(g_out, predicted_german)
    return current_loss


for epoch in range(num_epochs):
    train_ds, val_ds = make_dss()
    train_loss = Mean()
    val_loss = Mean()

    # train
    for eng_inputs, ger_inputs in train_ds:
        with tf.GradientTape() as t:
            current_loss = forward_pass(eng_inputs, ger_inputs)
        trainable_vars = encoder.trainable_variables + decoder.trainable_variables
        grads = t.gradient(current_loss, trainable_vars)
        optimizer.apply_gradients(zip(grads, trainable_vars))
        train_loss(current_loss)

    # validate
    for eng_inputs, ger_inputs in val_ds:
        current_loss = forward_pass(eng_inputs, ger_inputs)
        val_loss(current_loss)
```

The funny thing [here](https://www.tensorflow.org/text/tutorials/nmt_with_attention#test_the_training_step) they have similar 801 errors:

```
2021-08-31 11:08:27.919851: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Input 6 of node gradient_tape/while/while_grad/body/_531/gradient_tape/while/gradients/while/decoder_1/gru_3/PartitionedCall_grad/PartitionedCall was passed variant from gradient_tape/while/while_grad/body/_531/gradient_tape/while/gradients/while/decoder_1/gru_3/PartitionedCall_grad/TensorListPopBack_2:1 incompatible with expected float.
2021-08-31 11:08:28.004195: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] shape_optimizer failed: Out of range: src_output = 25, but num_outputs is only 25
2021-08-31 11:08:28.044145: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] layout failed: Out of range: src_output = 25, but num_outputs is only 25
2021-08-31 11:08:28.169643: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Input 6 of node gradient_tape/while/while_grad/body/_531/gradient_tape/while/gradients/while/decoder_1/gru_3/PartitionedCall_grad/PartitionedCall was passed variant from gradient_tape/while/while_grad/body/_531/gradient_tape/while/gradients/while/decoder_1/gru_3/PartitionedCall_grad/TensorListPopBack_2:1 incompatible with expected float.
2021-08-31 11:08:28.227653: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] shape_optimizer failed: Out of range: src_output = 25, but num_outputs is only 25
2021-08-31 11:08:28.301920: W tensorflow/core/common_runtime/process_function_library_runtime.cc:841] Ignoring multi-device function optimization failure: Invalid argument: Input 1 of node while/body/_1/while/TensorListPushBack_56 was passed float from while/body/_1/while/decoder_1/gru_3/PartitionedCall:6 incompatible with expected variant.
{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.0628138>}
```
which they explain will disappear after couple of training loops.

Interestingly enough, when I run the same code on Google Colab I don't have any errors

**Question:**

1. What these errors mean and should I pay attention to them?
2. Does not having these errors on Colab mean there is a problem with locally compiled TF?"
52123,GPU Out of Memory for different training-data sizes but same training/model configurations (tested on two installations of python),"Hi together,

I have installed

****System information**
- OS Platform and Distribution: Windows Server 2016
- TensorFlow installed from: binary? (conda install)
- TensorFlow version: 2.6.0
- Python version: 3.9.7
- Installed using: pip & conda
- Bazel version: ?
- GCC/Compiler version:  4.7.0 20111220
- CUDA/cuDNN version: 8.1.0.77 & 11.2.2
- GPU model and memory:  2xTesla M60, 2x8gb**

**I train a model with tensorflow.keras on two GPUs, Tesla M60.

The model gets trained successfully when I limit the amount of training-data below a certain value (around 700). When i go above that value I get an out of memory error for the GPU (overall possible training-data size is 2196).** 

**I use the same setting for the model- and training configuration, but feed different size of training-data:

```
learning_rate = 0.001
epochs = 200
batch_size = 80
buffer = 1000;

x_train = io.loadmat(...)
x_train = x_train[0:700,:,:] # optional
y_train = io.loadmat(...)
y_train = x_train[0:700,:,:] # optional
x_test = io.loadmat(...)
y_test = io.loadmat(...)

strategy = tensorflow.distribute.MirroredStrategy(cross_device_ops=tensorflow.distribute.HierarchicalCopyAllReduce())

with strategy.scope():
    
     autoencoder = tensorflow.keras.Model(...)
    
     loss = tensorflow.keras.losses.MeanSquaredError(name='MeanSquaredError')

     autoencoder.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss])

train_data = tensorflow.data.Dataset.from_tensor_slices((x_train, y_train))
val_data = tensorflow.data.Dataset.from_tensor_slices((x_test, y_test))

train_data = train_data.shuffle(buffer).batch(batch_size)
val_data = val_data.shuffle(buffer).batch(batch_size)

options = tensorflow.data.Options()
options.experimental_distribute.auto_shard_policy = tensorflow.data.experimental.AutoShardPolicy.DATA

train_data = train_data.with_options(options)
val_data = val_data.with_options(options)

autoencoder.fit(train_data,
          validation_data=val_data,
          epochs=epochs,
          callbacks=[livelossplot.PlotLossesKeras(outputs=[livelossplot.outputs.MatplotlibPlot(figpath=save_string_plot)]),tensorflow.keras.callbacks.TensorBoard(log_dir=""model\\"" + string_name_model, histogram_freq = True, write_grads=True)],
          verbose=0)
```

When I do not use tensorflow.data it works for a size until 1100 and then crashes (with the warning: AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset):

```
autoencoder.fit(x_train, y_train,
          validation_data=(x_test, y_test),
          epochs=epochs,
          batch_size= batch_size,
          shuffle = 1,
          callbacks=[livelossplot.PlotLossesKeras(outputs=[livelossplot.outputs.MatplotlibPlot(figpath=save_string_plot)]),tensorflow.keras.callbacks.TensorBoard(log_dir=""model\\"" + string_name_model, histogram_freq = True, write_grads=True)],
          verbose=0)
```

Output of the console:

```
2021-09-24 16:37:15.026869: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
Number of GPU devices: 2
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
...
=================================================================
Total params: 5,449,621
Trainable params: 5,449,621
Non-trainable params: 0
_________________________________________________________________
WARNING: tensorflow:""write_grads"" will be ignored in TensorFlow 2.0 for the ""TensorBoard"" Callback.

2021-09-24 16:37:15.026869: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-24 16:37:16.869314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7999 MB memory:  -> device: 0, name: Tesla M60, pci bus id: 0000:04:00.0, compute capability: 5.2
2021-09-24 16:37:16.886082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7999 MB memory:  -> device: 1, name: Tesla M60, pci bus id: 0000:05:00.0, compute capability: 5.2
2021-09-24 16:37:16.940255: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:37:17.130269: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:37:34.157788: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.
2021-09-24 16:37:34.158409: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.
2021-09-24 16:37:34.158977: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 2 GPUs
2021-09-24 16:37:34.651065: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.
2021-09-24 16:37:34.656233: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed
INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = hierarchical_copy, num_packs = 1
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = hierarchical_copy, num_packs = 1

2021-09-24 16:37:15.026869: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-24 16:37:16.869314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7999 MB memory:  -> device: 0, name: Tesla M60, pci bus id: 0000:04:00.0, compute capability: 5.2
2021-09-24 16:37:16.886082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7999 MB memory:  -> device: 1, name: Tesla M60, pci bus id: 0000:05:00.0, compute capability: 5.2
2021-09-24 16:37:16.940255: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:37:17.130269: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:37:34.157788: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.
2021-09-24 16:37:34.158409: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.
2021-09-24 16:37:34.158977: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 2 GPUs
2021-09-24 16:37:34.651065: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.
2021-09-24 16:37:34.656233: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed
2021-09-24 16:37:35.497874: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2021-09-24 16:37:43.261958: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
2021-09-24 16:37:44.298596: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
2021-09-24 16:37:56.952727: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.44GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:38:00.057046: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:38:00.067119: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2021-09-24 16:37:15.026869: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-24 16:37:16.869314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7999 MB memory:  -> device: 0, name: Tesla M60, pci bus id: 0000:04:00.0, compute capability: 5.2
2021-09-24 16:37:16.886082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7999 MB memory:  -> device: 1, name: Tesla M60, pci bus id: 0000:05:00.0, compute capability: 5.2
2021-09-24 16:37:16.940255: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:37:17.130269: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:37:34.157788: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.
2021-09-24 16:37:34.158409: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.
2021-09-24 16:37:34.158977: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 2 GPUs
2021-09-24 16:37:34.651065: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.
2021-09-24 16:37:34.656233: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed
2021-09-24 16:37:35.497874: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2021-09-24 16:37:43.261958: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
2021-09-24 16:37:44.298596: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
2021-09-24 16:37:56.952727: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.44GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:38:00.057046: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:38:00.067119: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:38:05.247935: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_1_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:38:05.633715: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:38:05.635113: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_1_bfc) ran out of memory trying to allocate 592.64MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:38:05.637273: W tensorflow/core/kernels/gpu_utils.cc:49] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
2021-09-24 16:38:05.676313: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_1_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:38:09.847595: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_1_bfc) ran out of memory trying to allocate 2.56GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:38:10.081564: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:38:10.094857: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:38:10.096256: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 576.64MiB (rounded to 604651520)requested by op model/up_sampling2d_2/resize/ResizeNearestNeighbor
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2021-09-24 16:38:10.100036: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc
2021-09-24 16:38:10.101103: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256): 	Total Chunks: 53, Chunks in use: 51. 13.3KiB allocated for chunks. 12.8KiB in use in bin. 464B client-requested in use in bin.
2021-09-24 16:38:10.103078: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512): 	Total Chunks: 6, Chunks in use: 6. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 3.0KiB client-requested in use in bin.
2021-09-24 16:38:10.104925: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024): 	Total Chunks: 7, Chunks in use: 7. 7.5KiB allocated for chunks. 7.5KiB in use in bin. 7.0KiB client-requested in use in bin.
2021-09-24 16:38:10.106369: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048): 	Total Chunks: 6, Chunks in use: 6. 12.0KiB allocated for chunks. 12.0KiB in use in bin. 12.0KiB client-requested in use in bin.
2021-09-24 16:38:10.107841: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-24 16:38:10.109201: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192): 	Total Chunks: 3, Chunks in use: 3. 24.0KiB allocated for chunks. 24.0KiB in use in bin. 24.0KiB client-requested in use in bin.
2021-09-24 16:38:10.110729: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-24 16:38:10.112922: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768): 	Total Chunks: 2, Chunks in use: 2. 80.0KiB allocated for chunks. 80.0KiB in use in bin. 80.0KiB client-requested in use in bin.
2021-09-24 16:38:10.114261: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536): 	Total Chunks: 3, Chunks in use: 3. 295.0KiB allocated for chunks. 295.0KiB in use in bin. 265.0KiB client-requested in use in bin.
2021-09-24 16:38:10.115402: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072): 	Total Chunks: 1, Chunks in use: 1. 221.3KiB allocated for chunks. 221.3KiB in use in bin. 112.5KiB client-requested in use in bin.
2021-09-24 16:38:10.116460: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-24 16:38:10.117438: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288): 	Total Chunks: 3, Chunks in use: 3. 2.17MiB allocated for chunks. 2.17MiB in use in bin. 1.88MiB client-requested in use in bin.
2021-09-24 16:38:10.118483: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576): 	Total Chunks: 3, Chunks in use: 1. 4.57MiB allocated for chunks. 1.51MiB in use in bin. 1.51MiB client-requested in use in bin.
2021-09-24 16:38:10.119711: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152): 	Total Chunks: 6, Chunks in use: 6. 12.01MiB allocated for chunks. 12.01MiB in use in bin. 12.00MiB client-requested in use in bin.
2021-09-24 16:38:10.120816: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304): 	Total Chunks: 2, Chunks in use: 2. 9.01MiB allocated for chunks. 9.01MiB in use in bin. 9.01MiB client-requested in use in bin.
2021-09-24 16:38:10.121871: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608): 	Total Chunks: 8, Chunks in use: 8. 74.46MiB allocated for chunks. 74.46MiB in use in bin. 66.02MiB client-requested in use in bin.
2021-09-24 16:38:10.122955: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-24 16:38:10.124753: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432): 	Total Chunks: 5, Chunks in use: 3. 191.93MiB allocated for chunks. 116.25MiB in use in bin. 116.25MiB client-requested in use in bin.
2021-09-24 16:38:10.126652: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864): 	Total Chunks: 5, Chunks in use: 4. 417.20MiB allocated for chunks. 300.95MiB in use in bin. 267.62MiB client-requested in use in bin.
2021-09-24 16:38:10.127752: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728): 	Total Chunks: 11, Chunks in use: 10. 1.69GiB allocated for chunks. 1.46GiB in use in bin. 1.46GiB client-requested in use in bin.
2021-09-24 16:38:10.128826: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456): 	Total Chunks: 10, Chunks in use: 9. 4.64GiB allocated for chunks. 4.38GiB in use in bin. 4.38GiB client-requested in use in bin.
2021-09-24 16:38:10.129892: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 576.64MiB was 256.00MiB, Chunk State: 
2021-09-24 16:38:10.130516: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 268.79MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 144.16MiB | Requested Size: 144.16MiB | in_use: 1 | bin_num: -1
2021-09-24 16:38:10.131899: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 7548803072
2021-09-24 16:38:10.134235: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308600000 of size 256 next 1
2021-09-24 16:38:10.135090: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308600100 of size 1280 next 2
2021-09-24 16:38:10.136035: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308600600 of size 256 next 3
2021-09-24 16:38:10.136988: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308600700 of size 256 next 4
2021-09-24 16:38:10.137955: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308600800 of size 256 next 5
2021-09-24 16:38:10.138876: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308600900 of size 512 next 6
2021-09-24 16:38:10.139927: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308600b00 of size 256 next 9
2021-09-24 16:38:10.140919: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308600c00 of size 256 next 10
2021-09-24 16:38:10.141902: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308600d00 of size 1024 next 11
2021-09-24 16:38:10.142889: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308601100 of size 256 next 14
2021-09-24 16:38:10.143898: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308601200 of size 256 next 15
2021-09-24 16:38:10.144874: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308601300 of size 2048 next 16
2021-09-24 16:38:10.145839: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308601b00 of size 256 next 19
2021-09-24 16:38:10.146883: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308601c00 of size 256 next 20
2021-09-24 16:38:10.147809: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308601d00 of size 256 next 21
2021-09-24 16:38:10.148730: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308601e00 of size 256 next 24
2021-09-24 16:38:10.149641: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308601f00 of size 256 next 25
2021-09-24 16:38:10.150520: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308602000 of size 2048 next 26
2021-09-24 16:38:10.151408: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308602800 of size 1024 next 29
2021-09-24 16:38:10.152308: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308602c00 of size 512 next 33
2021-09-24 16:38:10.153202: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308602e00 of size 256 next 34
2021-09-24 16:38:10.154137: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308602f00 of size 256 next 35
2021-09-24 16:38:10.155069: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603000 of size 256 next 36
2021-09-24 16:38:10.155968: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603100 of size 256 next 39
2021-09-24 16:38:10.156904: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603200 of size 256 next 40
2021-09-24 16:38:10.157817: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603300 of size 256 next 41
2021-09-24 16:38:10.158709: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603400 of size 256 next 42
2021-09-24 16:38:10.159620: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603500 of size 256 next 43
2021-09-24 16:38:10.160283: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603600 of size 256 next 44
2021-09-24 16:38:10.160887: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603700 of size 256 next 49
2021-09-24 16:38:10.161529: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603800 of size 256 next 50
2021-09-24 16:38:10.162132: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603900 of size 256 next 51
2021-09-24 16:38:10.162729: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603a00 of size 256 next 52
2021-09-24 16:38:10.163341: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603b00 of size 256 next 53
2021-09-24 16:38:10.163931: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603c00 of size 256 next 54
2021-09-24 16:38:10.164527: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603d00 of size 256 next 55
2021-09-24 16:38:10.165123: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603e00 of size 256 next 56
2021-09-24 16:38:10.165723: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308603f00 of size 256 next 57
2021-09-24 16:38:10.166320: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308604000 of size 256 next 58
2021-09-24 16:38:10.166919: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308604100 of size 256 next 59
2021-09-24 16:38:10.167585: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308604200 of size 512 next 61
2021-09-24 16:38:10.168191: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308604400 of size 1280 next 7
2021-09-24 16:38:10.168783: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308604900 of size 8192 next 8
2021-09-24 16:38:10.169474: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308606900 of size 8192 next 60
2021-09-24 16:38:10.170079: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308608900 of size 2048 next 62
2021-09-24 16:38:10.170778: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308609100 of size 71680 next 23
2021-09-24 16:38:10.171890: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 430861a900 of size 40960 next 22
2021-09-24 16:38:10.173425: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308624900 of size 256 next 63
2021-09-24 16:38:10.174511: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308624a00 of size 2048 next 64
2021-09-24 16:38:10.175245: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308625200 of size 1024 next 66
2021-09-24 16:38:10.175960: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308625600 of size 512 next 67
2021-09-24 16:38:10.176683: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308625800 of size 226560 next 38
2021-09-24 16:38:10.177507: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 430865cd00 of size 115200 next 37
2021-09-24 16:38:10.178267: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308678f00 of size 965120 next 28
2021-09-24 16:38:10.179019: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308764900 of size 655360 next 27
2021-09-24 16:38:10.179931: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308804900 of size 2105344 next 13
2021-09-24 16:38:10.180725: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308a06900 of size 2097152 next 12
2021-09-24 16:38:10.181481: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308c06900 of size 2097152 next 32
2021-09-24 16:38:10.182494: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4308e06900 of size 2097152 next 31
2021-09-24 16:38:10.183930: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4309006900 of size 12582912 next 18
2021-09-24 16:38:10.186139: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4309c06900 of size 8388608 next 17
2021-09-24 16:38:10.187408: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 430a406900 of size 8388608 next 30
2021-09-24 16:38:10.188021: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 430ac06900 of size 165334528 next 45
2021-09-24 16:38:10.188640: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 43149b3700 of size 165334528 next 46
2021-09-24 16:38:10.189259: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 431e760500 of size 828325376 next 47
2021-09-24 16:38:10.189878: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 434fd54300 of size 828325376 next 48
2021-09-24 16:38:10.190495: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4381348100 of size 8388608 next 65
2021-09-24 16:38:10.191114: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4381b48100 of size 256 next 68
2021-09-24 16:38:10.191702: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4381b48200 of size 8192 next 69
2021-09-24 16:38:10.192302: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4381b4a200 of size 512 next 70
2021-09-24 16:38:10.192923: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4381b4a400 of size 2097152 next 71
2021-09-24 16:38:10.193532: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4381d4a400 of size 1024 next 72
2021-09-24 16:38:10.194126: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4381d4a800 of size 8388608 next 73
2021-09-24 16:38:10.194731: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 438254a800 of size 2048 next 74
2021-09-24 16:38:10.195326: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 438254b000 of size 40960 next 75
2021-09-24 16:38:10.195924: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4382555000 of size 256 next 76
2021-09-24 16:38:10.196512: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4382555100 of size 655360 next 77
2021-09-24 16:38:10.197162: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 43825f5100 of size 2048 next 78
2021-09-24 16:38:10.197935: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 43825f5900 of size 8388608 next 79
2021-09-24 16:38:10.198566: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4382df5900 of size 1024 next 80
2021-09-24 16:38:10.199171: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4382df5d00 of size 2097152 next 81
2021-09-24 16:38:10.199774: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4382ff5d00 of size 512 next 82
2021-09-24 16:38:10.200370: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4382ff5f00 of size 115200 next 83
2021-09-24 16:38:10.200972: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012100 of size 256 next 84
2021-09-24 16:38:10.201567: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012200 of size 256 next 85
2021-09-24 16:38:10.202154: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012300 of size 256 next 86
2021-09-24 16:38:10.202746: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012400 of size 256 next 87
2021-09-24 16:38:10.203336: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012500 of size 256 next 88
2021-09-24 16:38:10.203925: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012600 of size 256 next 89
2021-09-24 16:38:10.204516: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012700 of size 256 next 90
2021-09-24 16:38:10.205110: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012800 of size 256 next 91
2021-09-24 16:38:10.205700: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012900 of size 256 next 92
2021-09-24 16:38:10.206305: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012a00 of size 256 next 93
2021-09-24 16:38:10.206890: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012b00 of size 256 next 94
2021-09-24 16:38:10.207480: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012c00 of size 256 next 95
2021-09-24 16:38:10.208083: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 4383012d00 of size 256 next 96
2021-09-24 16:38:10.208672: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383012e00 of size 256 next 97
2021-09-24 16:38:10.209263: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 4383012f00 of size 256 next 98
2021-09-24 16:38:10.209855: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383013000 of size 256 next 99
2021-09-24 16:38:10.210453: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 4383013100 of size 1587200 next 121
2021-09-24 16:38:10.211148: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4383196900 of size 1587200 next 122
2021-09-24 16:38:10.212069: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 438331a100 of size 1616384 next 107
2021-09-24 16:38:10.213013: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 43834a4b00 of size 256 next 103
2021-09-24 16:38:10.213903: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 43834a4c00 of size 14105088 next 104
2021-09-24 16:38:10.214862: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4384218600 of size 4723968 next 102
2021-09-24 16:38:10.215837: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4384699b00 of size 4723968 next 105
2021-09-24 16:38:10.216802: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4384b1b000 of size 604651520 next 100
2021-09-24 16:38:10.217784: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 43a8bbf000 of size 9447680 next 108
2021-09-24 16:38:10.218759: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 43a94c1900 of size 604651520 next 101
2021-09-24 16:38:10.219791: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 43cd565900 of size 151162880 next 110
2021-09-24 16:38:10.220784: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 43d658e900 of size 604651520 next 106
2021-09-24 16:38:10.221783: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 43fa632900 of size 302325760 next 109
2021-09-24 16:38:10.222803: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 440c684900 of size 302325760 next 111
2021-09-24 16:38:10.223807: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 441e6d6900 of size 75581440 next 114
2021-09-24 16:38:10.224553: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4422eeb100 of size 302325760 next 112
2021-09-24 16:38:10.225302: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4434f3d100 of size 77455360 next 113
2021-09-24 16:38:10.225931: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 443991b100 of size 154910720 next 115
2021-09-24 16:38:10.226570: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4442cd7100 of size 154910720 next 116
2021-09-24 16:38:10.227203: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 444c093100 of size 38727680 next 117
2021-09-24 16:38:10.227826: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 444e582100 of size 154910720 next 118
2021-09-24 16:38:10.228460: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 445793e100 of size 40632320 next 119
2021-09-24 16:38:10.229099: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 4459ffe100 of size 40632320 next 120
2021-09-24 16:38:10.229716: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 445c6be100 of size 40632320 next 123
2021-09-24 16:38:10.230343: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 445ed7e100 of size 40632320 next 124
2021-09-24 16:38:10.230960: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 446143e100 of size 81264640 next 127
2021-09-24 16:38:10.231581: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 44661be100 of size 121896960 next 125
2021-09-24 16:38:10.232205: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 446d5fe100 of size 162529280 next 126
2021-09-24 16:38:10.232828: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 44770fe100 of size 81264640 next 128
2021-09-24 16:38:10.233609: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 447be7e100 of size 151162880 next 130
2021-09-24 16:38:10.234502: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 4484ea7100 of size 255160320 next 129
2021-09-24 16:38:10.235133: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 44941fe100 of size 325058560 next 131
2021-09-24 16:38:10.235754: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 44a77fe100 of size 151162880 next 132
2021-09-24 16:38:10.236381: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 44b0827100 of size 151162880 next 133
2021-09-24 16:38:10.237279: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 44b9850100 of size 281843456 next 18446744073709551615
2021-09-24 16:38:10.238327: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size: 
2021-09-24 16:38:10.239150: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 51 Chunks of size 256 totalling 12.8KiB
2021-09-24 16:38:10.239952: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 6 Chunks of size 512 totalling 3.0KiB
2021-09-24 16:38:10.240768: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 5 Chunks of size 1024 totalling 5.0KiB
2021-09-24 16:38:10.241629: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 1280 totalling 2.5KiB
2021-09-24 16:38:10.242496: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 6 Chunks of size 2048 totalling 12.0KiB
2021-09-24 16:38:10.243374: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 3 Chunks of size 8192 totalling 24.0KiB
2021-09-24 16:38:10.244263: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 40960 totalling 80.0KiB
2021-09-24 16:38:10.245130: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 71680 totalling 70.0KiB
2021-09-24 16:38:10.246015: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 115200 totalling 225.0KiB
2021-09-24 16:38:10.247038: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 226560 totalling 221.3KiB
2021-09-24 16:38:10.248025: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 655360 totalling 1.25MiB
2021-09-24 16:38:10.248993: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 965120 totalling 942.5KiB
2021-09-24 16:38:10.249967: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1587200 totalling 1.51MiB
2021-09-24 16:38:10.250796: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 5 Chunks of size 2097152 totalling 10.00MiB
2021-09-24 16:38:10.251468: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 2105344 totalling 2.01MiB
2021-09-24 16:38:10.252107: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 4723968 totalling 9.01MiB
2021-09-24 16:38:10.252738: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 5 Chunks of size 8388608 totalling 40.00MiB
2021-09-24 16:38:10.253379: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 9447680 totalling 9.01MiB
2021-09-24 16:38:10.254007: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 12582912 totalling 12.00MiB
2021-09-24 16:38:10.254649: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 14105088 totalling 13.45MiB
2021-09-24 16:38:10.255296: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 3 Chunks of size 40632320 totalling 116.25MiB
2021-09-24 16:38:10.255945: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 75581440 totalling 72.08MiB
2021-09-24 16:38:10.256603: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 77455360 totalling 73.87MiB
2021-09-24 16:38:10.257252: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 81264640 totalling 155.00MiB
2021-09-24 16:38:10.257897: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 4 Chunks of size 151162880 totalling 576.64MiB
2021-09-24 16:38:10.258555: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 3 Chunks of size 154910720 totalling 443.20MiB
2021-09-24 16:38:10.259204: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 162529280 totalling 155.00MiB
2021-09-24 16:38:10.259850: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 165334528 totalling 315.35MiB
2021-09-24 16:38:10.260496: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 3 Chunks of size 302325760 totalling 864.96MiB
2021-09-24 16:38:10.261148: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 325058560 totalling 310.00MiB
2021-09-24 16:38:10.261806: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 3 Chunks of size 604651520 totalling 1.69GiB
2021-09-24 16:38:10.262449: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 828325376 totalling 1.54GiB
2021-09-24 16:38:10.263077: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 6.34GiB
2021-09-24 16:38:10.263676: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 7548803072 memory_limit_: 8387559424 available bytes: 838756352 curr_region_allocation_bytes_: 16775118848
2021-09-24 16:38:10.264780: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: 
Limit:                      8387559424
InUse:                      6807338240
MaxInUse:                   7269322496
NumAllocs:                         237
MaxAllocSize:               2722111488
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2021-09-24 16:38:10.266805: W tensorflow/core/common_runtime/bfc_allocator.cc:468] ******************************************************************************_******__**********___
2021-09-24 16:38:10.267767: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at image_resizer_state.h:150 : Resource exhausted: OOM when allocating tensor with shape[40,122,242,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""U:\Roman Foell\EAC\PT Autostart\autoencoder3.py"", line 368, in <module>
    autoencoder.fit(train_data,
  File ""C:\Users\FLO9FE\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py"", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File ""C:\Users\FLO9FE\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\eager\def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\FLO9FE\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\eager\def_function.py"", line 950, in _call
    return self._stateless_fn(*args, **kwds)
  File ""C:\Users\FLO9FE\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\eager\function.py"", line 3039, in __call__
    return graph_function._call_flat(
  File ""C:\Users\FLO9FE\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\eager\function.py"", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""C:\Users\FLO9FE\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\eager\function.py"", line 591, in call
    outputs = execute.execute(
  File ""C:\Users\FLO9FE\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
ResourceExhaustedError: 3 root error(s) found.
  (0) Resource exhausted:  OOM when allocating tensor with shape[40,122,242,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node model/up_sampling2d_2/resize/ResizeNearestNeighbor (defined at \Anaconda3\envs\keras\lib\threading.py:973) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
	 [[div_no_nan/ReadVariableOp/_82]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
  (1) Resource exhausted:  OOM when allocating tensor with shape[40,122,242,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node model/up_sampling2d_2/resize/ResizeNearestNeighbor (defined at \Anaconda3\envs\keras\lib\threading.py:973) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
  (2) Resource exhausted:  OOM when allocating tensor with shape[40,122,242,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node model/up_sampling2d_2/resize/ResizeNearestNeighbor (defined at \Anaconda3\envs\keras\lib\threading.py:973) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
	 [[update_0/AssignAddVariableOp/_115]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_3609]
Function call stack:
train_function -> train_function -> train_function
```

This confuses me, as I think the GPU memory usage is mainly dependent on the batch size, not the amount of training-data and for smaller training-sizes it works with the same batch size.

Can anyone tell me, what is wrong in my thinking?

I tested the exact same code for another configuration of python installation:

****System information**
- OS Platform and Distribution: Windows Server 2016
- TensorFlow installed from: binary? (conda install)
- TensorFlow version: 2.1.0
- Python version: 3.6.12
- Installed using: pip & conda
- Bazel version: ?
- GCC/Compiler version:  4.7.0 20111220
- CUDA/cuDNN version: 7.6.5 & 10.1.243
- GPU model and memory:  2xTesla M60, 2x8gb**

And now the amount of training-data does not affect the GPU memory, so no out of memory any more (each case with tensorflow.data and without).


Output of the console:
```
2021-09-24 16:20:59.870861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:10.628594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-09-24 16:22:10.799285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:10.804613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:10.806588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:10.816111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:10.822623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-09-24 16:22:10.826388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-09-24 16:22:10.837615: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-09-24 16:22:10.844618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-09-24 16:22:10.866320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:10.872124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
2021-09-24 16:22:10.874800: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2021-09-24 16:22:11.008743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:11.010790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:11.012167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:11.012877: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:11.013581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-09-24 16:22:11.014257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-09-24 16:22:11.014944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-09-24 16:22:11.015645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-09-24 16:22:11.016341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:11.019457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
Number of GPU devices: 2
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
...
=================================================================
Total params: 5,449,621
Trainable params: 5,449,621
Non-trainable params: 0
_________________________________________________________________
WARNING: tensorflow: ""write_grads"" will be ignored in TensorFlow 2.0 for the ""TensorBoard"" Callback.
Using TensorFlow backend.
INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = hierarchical_copy, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = hierarchical_copy, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

2021-09-24 16:20:59.870861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:10.628594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-09-24 16:22:10.799285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:10.804613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:10.806588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:10.816111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:10.822623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-09-24 16:22:10.826388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-09-24 16:22:10.837615: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-09-24 16:22:10.844618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-09-24 16:22:10.866320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:10.872124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
2021-09-24 16:22:10.874800: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2021-09-24 16:22:11.008743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:11.010790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:11.012167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:11.012877: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:11.013581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-09-24 16:22:11.014257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-09-24 16:22:11.014944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-09-24 16:22:11.015645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-09-24 16:22:11.016341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:11.019457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
2021-09-24 16:22:12.903330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-09-24 16:22:12.904505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 
2021-09-24 16:22:12.905254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N Y 
2021-09-24 16:22:12.906147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   Y N 
2021-09-24 16:22:12.910666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7999 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0000:04:00.0, compute capability: 5.2)
2021-09-24 16:22:12.921192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7999 MB memory) -> physical GPU (device: 1, name: Tesla M60, pci bus id: 0000:05:00.0, compute capability: 5.2)
2021-09-24 16:22:13.020179: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:22:13.390727: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:22:47.092329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:48.266275: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:49.534717: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. This message will be only logged once.
2021-09-24 16:22:56.271936: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:22:56.526591: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:22:56.528823: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 592.64MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:22:56.532185: W tensorflow/core/kernels/gpu_utils.cc:48] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.

2021-09-24 16:20:59.870861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:10.628594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-09-24 16:22:10.799285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:10.804613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:10.806588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:10.816111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:10.822623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-09-24 16:22:10.826388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-09-24 16:22:10.837615: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-09-24 16:22:10.844618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-09-24 16:22:10.866320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:10.872124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
2021-09-24 16:22:10.874800: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2021-09-24 16:22:11.008743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:11.010790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:11.012167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:11.012877: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:11.013581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-09-24 16:22:11.014257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-09-24 16:22:11.014944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-09-24 16:22:11.015645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-09-24 16:22:11.016341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:11.019457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
2021-09-24 16:22:12.903330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-09-24 16:22:12.904505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 
2021-09-24 16:22:12.905254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N Y 
2021-09-24 16:22:12.906147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   Y N 
2021-09-24 16:22:12.910666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7999 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0000:04:00.0, compute capability: 5.2)
2021-09-24 16:22:12.921192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7999 MB memory) -> physical GPU (device: 1, name: Tesla M60, pci bus id: 0000:05:00.0, compute capability: 5.2)
2021-09-24 16:22:13.020179: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:22:13.390727: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:22:47.092329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:48.266275: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:49.534717: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. This message will be only logged once.
2021-09-24 16:22:56.271936: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:22:56.526591: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:22:56.528823: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 592.64MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:22:56.532185: W tensorflow/core/kernels/gpu_utils.cc:48] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
2021-09-24 16:22:59.342139: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:23:02.347060: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.56GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:23:06.302744: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_1_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:23:06.604781: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:23:06.606373: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_1_bfc) ran out of memory trying to allocate 592.64MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.

2021-09-24 16:20:59.870861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:10.628594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-09-24 16:22:10.799285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:10.804613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:10.806588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:10.816111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:10.822623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-09-24 16:22:10.826388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-09-24 16:22:10.837615: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-09-24 16:22:10.844618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-09-24 16:22:10.866320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:10.872124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
2021-09-24 16:22:10.874800: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2021-09-24 16:22:11.008743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:11.010790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.44GiB deviceMemoryBandwidth: 149.31GiB/s
2021-09-24 16:22:11.012167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-09-24 16:22:11.012877: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:11.013581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-09-24 16:22:11.014257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-09-24 16:22:11.014944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-09-24 16:22:11.015645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-09-24 16:22:11.016341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:11.019457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
2021-09-24 16:22:12.903330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-09-24 16:22:12.904505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 
2021-09-24 16:22:12.905254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N Y 
2021-09-24 16:22:12.906147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   Y N 
2021-09-24 16:22:12.910666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7999 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0000:04:00.0, compute capability: 5.2)
2021-09-24 16:22:12.921192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7999 MB memory) -> physical GPU (device: 1, name: Tesla M60, pci bus id: 0000:05:00.0, compute capability: 5.2)
2021-09-24 16:22:13.020179: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:22:13.390727: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 7.81G (8387559424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:22:47.092329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-09-24 16:22:48.266275: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-09-24 16:22:49.534717: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. This message will be only logged once.
2021-09-24 16:22:56.271936: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:22:56.526591: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
locator (GPU_0_bfc) ran out of memory trying to allocate 592.64MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:22:56.532185: W tensorflow/core/kernels/gpu_utils.cc:48] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
2021-09-24 16:22:59.342139: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:23:02.347060: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.56GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:23:06.302744: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_1_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:23:06.604781: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 799.90M (838756352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-24 16:23:06.606373: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_1_bfc) ran out of memory trying to allocate 592.64MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-24 16:23:09.801961: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_1_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
```

I wanted to use the latest version because of some additional features. So can anyone tell me still, what might went wrong?
**
"
52122,TFLite dynamic size does not work for GRUs,"TensorFlow and TensorFlowLite allow to use dynamic sizes for some given dimensions, basicaly setting them to None.
This works perfectly with a convolutional layer, however does not seem to work in TensorFlowLite for GRU (most probably all RNNs) layers. The following code and output gives an example of the error:

```
RuntimeError: tensorflow/lite/kernels/concatenation.cc:80 t->dims->data[d] != t0->dims->data[d] (2 != 1)Node number 24 (CONCATENATION) failed to prepare.
Node number 10 (WHILE) failed to invoke.
```

```
###
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, GRU, Conv1D

###
def get_tf_net(input_shp, outsize_size=10, layer_name='RNN'):

    if layer_name=='RNN':
        layer = GRU(outsize_size, return_sequences=True, use_bias=True)
    elif layer_name=='Conv':
        layer = Conv1D(outsize_size, kernel_size=1, padding='same')
        
    inputs = Input(shape=input_shp)
    outputs = layer(inputs)
    model = Model(inputs, outputs)
    
    return model
def convert2TFLite(path2Model, path2Save):

    if path2Model.split('.')[-1]=='h5':
        model = tf.keras.models.load_model(path2Model)
        model = tf.lite.TFLiteConverter.from_keras_model(model)
    elif path2Model.split('.')[-1]=='pb':
        model = tf.lite.TFLiteConverter.from_saved_model(path2Model)

    model = model.convert()

    with open(path2Save, 'wb') as f:
        f.write(model)

    return

print(f""TensorFlow Version = {tf.__version__}"")

def main(layer_name):
    
    ###
    tensor_shape = (1, 3, 5)
    outsize_size = 7
    path_to_save = ""model""

    ###
    input_shp = list(tensor_shape)
    input_shp[0] = None
    input_shp[1] = None
    input_shp = input_shp[1:]

    model = get_tf_net(input_shp=input_shp, outsize_size=outsize_size, layer_name=layer_name)
    model.compile()

    model.summary()

    model.save('{}.h5'.format(path_to_save))

    path2Model = '{}.h5'.format(path_to_save)
    path2Save = '{}-from-h5.tflite'.format(path_to_save)
    convert2TFLite(path2Model, path2Save)

    model = tf.lite.Interpreter(model_path=path2Save)

    input_details = model.get_input_details()
    output_details = model.get_output_details()

    model.allocate_tensors()

    ###
    tests = []

    x = np.random.randn(*tensor_shape).astype(np.float32)
    out_shape = list(x.shape)
    out_shape[-1] = outsize_size
    y = np.random.randn(*out_shape).astype(np.float32)
    tests += [(x,y)]

    tmp_tensor_shape = list(tensor_shape)
    tmp_tensor_shape[1] = 2*tmp_tensor_shape[1]
    x = np.random.randn(*tmp_tensor_shape).astype(np.float32)
    out_shape = list(x.shape)
    out_shape[-1] = outsize_size
    y = np.random.randn(*out_shape).astype(np.float32)
    tests += [(x,y)]

    tmp_tensor_shape = list(tensor_shape)
    tmp_tensor_shape[0] = 2*tmp_tensor_shape[0]
    x = np.random.randn(*tmp_tensor_shape).astype(np.float32)
    out_shape = list(x.shape)
    out_shape[-1] = outsize_size
    y = np.random.randn(*out_shape).astype(np.float32)
    tests += [(x,y)]

    for x,y in tests:
        print(""Testing with:"",x.shape, y.shape)
        model.resize_tensor_input(input_details[0]['index'], x.shape)
        model.resize_tensor_input(output_details[0]['index'], y.shape)

        input_details = model.get_input_details()
        output_details = model.get_output_details()

        model.allocate_tensors()

        model.set_tensor(input_details[0]['index'], x)
        model.invoke()
        y = model.get_tensor(output_details[0]['index'])
        
    return

main(layer_name='Conv')
main(layer_name='RNN')
```

```
TensorFlow Version = 2.6.0
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, None, 5)]         0         
_________________________________________________________________
conv1d (Conv1D)              (None, None, 7)           42        
=================================================================
Total params: 42
Trainable params: 42
Non-trainable params: 0
_________________________________________________________________
INFO:tensorflow:Assets written to: /tmp/tmpv4iu1kbp/assets
Testing with: (1, 3, 5) (1, 3, 7)
Testing with: (1, 6, 5) (1, 6, 7)
Testing with: (2, 3, 5) (2, 3, 7)
Model: ""model_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, None, 5)]         0         
_________________________________________________________________
gru (GRU)                    (None, None, 7)           294       
=================================================================
Total params: 294
Trainable params: 294
Non-trainable params: 0
_________________________________________________________________
WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: /tmp/tmpmafb70jj/assets
INFO:tensorflow:Assets written to: /tmp/tmpmafb70jj/assets
Testing with: (1, 3, 5) (1, 3, 7)
Testing with: (1, 6, 5) (1, 6, 7)
Testing with: (2, 3, 5) (2, 3, 7)

RuntimeErrorTraceback (most recent call last)
<ipython-input-1-1cc2a142f111> in <module>
    108 
    109 main(layer_name='Conv')
--> 110 main(layer_name='RNN')

<ipython-input-1-1cc2a142f111> in main(layer_name)
    102 
    103         model.set_tensor(input_details[0]['index'], x)
--> 104         model.invoke()
    105         y = model.get_tensor(output_details[0]['index'])
    106 

~/.local/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py in invoke(self)
    873     """"""
    874     self._ensure_safe()
--> 875     self._interpreter.Invoke()
    876 
    877   def reset_all_variables(self):

RuntimeError: tensorflow/lite/kernels/concatenation.cc:80 t->dims->data[d] != t0->dims->data[d] (2 != 1)Node number 24 (CONCATENATION) failed to prepare.
Node number 10 (WHILE) failed to invoke.
```"
52121,"Saving customized RNN model, got error of ""Tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError""","**System information**
- I have written the customized simple RNN model based on TF layers:
- OS Platform and Distribution: Windows 10 
- TensorFlow installed from (source or binary): TF2.6 
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: Python 3.7.11
- CUDA/cuDNN version: 11.4
- GPU model and memory: NVIDIA TITAN RTX 24 G


**Describe the current behavior**
v2.6.0-rc2-32-g919f693420e 2.6.0

**Describe the expected behavior**
I have implemented a simple RNN model by using customized layers. The model is running ok with the eager execution and graph execution in the stage of the model building. However, while I tried to save the generated dummy model, I got the following error: 

 ""tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature."" 


**Standalone code to reproduce the issue:** 

import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Layer
from tensorflow.python.framework.ops import disable_eager_execution


##### ------- (1)  check if the eager execution is used -----

non_eager_execution = False
if non_eager_execution:
    tf.compat.v1.disable_eager_execution()
else:
    tf.config.run_functions_eagerly(True)

print(""Eager execution is: "" + str(tf.executing_eagerly()))


##### ------- (2)   define the customized RNN layer -----

class RNN_simple_layer(Layer):

    def __init__(self, neurons_numbers, return_sequences=False,**kwargs):
        super(RNN_simple_layer, self).__init__()
        self.neurons_numbers  = neurons_numbers
        self.return_sequences = return_sequences

    def build(self, input_shape):
        self.kernel_input = self.add_weight('kernel_input', shape=[int(input_shape[-1]), self.neurons_numbers], initializer=""random_normal"",trainable=True)
        self.kernel_state = self.add_weight('kernel_state', shape=[self.neurons_numbers,self.neurons_numbers], initializer=""random_normal"",trainable=True)
        self.bias = self.add_weight('bias', shape=[self.neurons_numbers,],initializer=""zeros"",trainable=True)

    def call(self, inputs):

        sequence_length = inputs.shape[-2]

        # outputs = []
        outputs = tf.TensorArray(tf.float32, size=sequence_length)
        states  = tf.TensorArray(tf.float32, size=sequence_length)
        inital_states  = tf.zeros(shape=[tf.shape(inputs)[0],self.neurons_numbers])

        states = inital_states
        for i_step in tf.range(sequence_length):
            output_i_step = tf.tanh(tf.matmul(inputs[:,i_step,:],self.kernel_input) + tf.matmul(states, self.kernel_state) + self.bias)
            states        = output_i_step
            outputs = outputs.write(i_step,output_i_step)

        # output the RNN resuts
        if self.return_sequences == True:
            output = tf.transpose(outputs.stack(), [1,0,2])
        else:
            output = outputs.stack()[-1]
        return output

    def get_config(self):
        config = super().get_config()
        config.update({
            'neurons_numbers': self.neurons_numbers,
            'return_sequences': self.return_sequences
        })
        return config



##### ------- (3)   Test the customized simple RNN layer-----

RNN_layer    = RNN_simple_layer(4,return_sequences=True)
input_array  = tf.ones((32,10,8))
output_array = RNN_layer(input_array)

print('RNN input tensor shape:' + str(input_array.shape))
print('RNN input tensor has batch size:' + str(input_array.shape[0]))
print('RNN input tensor has time samples:' + str(input_array.shape[-2]))
print('RNN input tensor has features of each sample:' + str(input_array.shape[-1]))
print('RNN output tensor shape:' + str(output_array.shape))


##### ------- (4)   build a dummy Tensorflow model-----

inputs  = tf.keras.Input(shape=(10,20))
x       = tf.keras.layers.Conv1D(10,3)(inputs)
x       = RNN_simple_layer(4,return_sequences=True)(x)
outputs = tf.keras.layers.Dense(30, activation=""relu"")(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs, name=""test_model"")
model.summary()



##### ------- (5)   save the customized dummy model-----
model.save('Model_simple_RNN')


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Eager execution is: True
RNN input tensor shape:(32, 10, 8)
RNN input tensor has batch size:32
RNN input tensor has time samples:10
RNN input tensor has features of each sample:8
RNN output tensor shape:(32, 10, 4)
Model: ""test_model""

#####  _________________________________________________________________
#####  Layer (type)                 Output Shape              Param #   
#####  =================================================================
#####   input_7 (InputLayer)         [(None, 10, 20)]          0         
#####  _________________________________________________________________
#####  conv1d_6 (Conv1D)            (None, 8, 10)             610       
#####  _________________________________________________________________
#####  rnn_simple_layer_13 (RNN_sim (None, 8, 4)              60        
#####  _________________________________________________________________
#####  dense_6 (Dense)              (None, 8, 30)             150       
#####  =================================================================
Total params: 820
Trainable params: 820
Non-trainable params: 0
_________________________________________________________________


model.save('Model_simple_RNN')
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
Traceback (most recent call last):
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\IPython\core\interactiveshell.py"", line 3441, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-9-6d9b393b5457>"", line 1, in <module>
    model.save('Model_simple_RNN')
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\engine\training.py"", line 2146, in save
    signatures, options, save_traces)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\save.py"", line 150, in save_model
    signatures, options, save_traces)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save.py"", line 91, in save
    model, filepath, signatures, options)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1228, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def))
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1399, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1336, in _build_meta_graph_impl
    checkpoint_graph_view)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\saved_model\signature_serialization.py"", line 99, in find_function_to_export
    functions = saveable_view.list_functions(saveable_view.root)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\saved_model\save.py"", line 164, in list_functions
    self._serialization_cache)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\engine\training.py"", line 2813, in _list_functions_for_serialization
    Model, self)._list_functions_for_serialization(serialization_cache)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\engine\base_layer.py"", line 3086, in _list_functions_for_serialization
    .list_functions_for_serialization(serialization_cache))
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\base_serialization.py"", line 93, in list_functions_for_serialization
    fns = self.functions_to_serialize(serialization_cache)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\layer_serialization.py"", line 74, in functions_to_serialize
    serialization_cache).functions_to_serialize)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\layer_serialization.py"", line 90, in _get_serialized_attributes
    serialization_cache)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\model_serialization.py"", line 57, in _get_serialized_attributes_internal
    serialization_cache))
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\layer_serialization.py"", line 99, in _get_serialized_attributes_internal
    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save_impl.py"", line 197, in wrap_layer_functions
    fn.get_concrete_function()
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\contextlib.py"", line 119, in __exit__
    next(self.gen)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save_impl.py"", line 359, in tracing_scope
    fn.get_concrete_function(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\eager\def_function.py"", line 1233, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\eager\def_function.py"", line 1213, in _get_concrete_function_garbage_collected
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\eager\def_function.py"", line 760, in _initialize
    *args, **kwds))
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\eager\function.py"", line 3066, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\eager\function.py"", line 3463, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\eager\function.py"", line 3308, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 1007, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\eager\def_function.py"", line 668, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save_impl.py"", line 572, in wrapper
    ret = method(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\utils.py"", line 166, in wrap_with_training_arg
    lambda: replace_training_and_call(False))
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\utils\control_flow_util.py"", line 106, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\framework\smart_cond.py"", line 58, in smart_cond
    return false_fn()
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\utils.py"", line 166, in <lambda>
    lambda: replace_training_and_call(False))
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\utils.py"", line 162, in replace_training_and_call
    return wrapped_call(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save_impl.py"", line 651, in call
    return call_and_return_conditional_losses(inputs, *args, **kwargs)[0]
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save_impl.py"", line 609, in __call__
    return self.wrapped_call(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\eager\def_function.py"", line 862, in __call__
    return self._python_function(*args, **kwds)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save_impl.py"", line 572, in wrapper
    ret = method(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\utils.py"", line 166, in wrap_with_training_arg
    lambda: replace_training_and_call(False))
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\utils\control_flow_util.py"", line 106, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\framework\smart_cond.py"", line 58, in smart_cond
    return false_fn()
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\utils.py"", line 166, in <lambda>
    lambda: replace_training_and_call(False))
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\utils.py"", line 162, in replace_training_and_call
    return wrapped_call(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save_impl.py"", line 633, in call_and_return_conditional_losses
    call_output = layer_call(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\engine\functional.py"", line 415, in call
    inputs, training=training, mask=mask)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\engine\functional.py"", line 550, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\engine\base_layer.py"", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\utils.py"", line 68, in return_outputs_and_add_losses
    outputs, losses = fn(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save_impl.py"", line 609, in __call__
    return self.wrapped_call(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\eager\def_function.py"", line 862, in __call__
    return self._python_function(*args, **kwds)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save_impl.py"", line 572, in wrapper
    ret = method(*args, **kwargs)
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\keras\saving\saved_model\save_impl.py"", line 633, in call_and_return_conditional_losses
    call_output = layer_call(*args, **kwargs)
  File ""<ipython-input-8-4f4d66e68e0a>"", line 45, in call
    for i_step in tf.range(sequence_length):
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\framework\ops.py"", line 520, in __iter__
    self._disallow_iteration()
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\framework\ops.py"", line 513, in _disallow_iteration
    self._disallow_when_autograph_enabled(""iterating over `tf.Tensor`"")
  File ""C:\Users\zhaoh\anaconda3\envs\tensorflowV26\lib\site-packages\tensorflow\python\framework\ops.py"", line 491, in _disallow_when_autograph_enabled
    "" indicate you are trying to use an unsupported feature."".format(task))
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.


"
52118,Different inference result when use models exported by the same ckpt.,"I save tf model multi times using the same ckpt as following:

```
builder = tf.saved_model.builder.SavedModelBuilder(export_path)
builder.add_meta_graph_and_variables(
                    sess,
                    tags=[tf.saved_model.tag_constants.SERVING],
                    signature_def_map=signature_def_map,
                    main_op=tf.tables_initializer(),
                    saver=saver,
                )
builder.save()
```

But when i load model with saved pb and variables(generated by multi times export as above) and infer with the same input. The results are slightly different. Is it normal?

Thanks very much!"
52117,the link 404,"examples/lite/examples/pose_estimation/raspberry_pi/setup.sh
line9-10
# Install Python dependencies
python3 -m pip install -r requirements.txt

the link in the file requirements.txt --extra-index-url https://google-coral.github.io/py-repo/
argparse cannot found"
52115,Inconsistent broadcast behavior when using model.fit vs custom training loop,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linunx ? Repro on colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6
- Python version: 3.7 ? 
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/a

**Describe the current behavior**

broadcast behavior different between `keras.Model.fit` v/s custom training loop

**Describe the expected behavior**

broadcast behavior is consistent.

**[Contributing](https://www.tensorflow.org/community/contribute)**
N/A

- Do you want to contribute a PR? (yes/no): no

- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
https://gist.github.com/pavanky/95d6ae0a1e875d2cd9e60e24e9d3e1b6
"
52114," Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, float32, float32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.","### 1. System information

- OS Platform and Distribution : Ubuntu 20.4
- TensorFlow installation : Tensorflow 2.6


### 2. Code

   # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]
    # Coordinates are normalized.
    detections = tf.concat([
        tf.gather(refined_rois, keep),
       # tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],
        tf.cast(tf.gather(class_ids, keep), dtype=tf.float32)[..., tf.newaxis],
        tf.gather(class_scores, keep),[..., tf.newaxis]
        ], axis=1)

### 5. (optional) Any other info / logs

WARNING:tensorflow:From /home/nika/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Use fn_output_signature instead

 Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, float32, float32, <NOT CONVERTIBLE TO TENSOR>] that don't all match."
52113,libtensorflowlite with Select Ops linking failure,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): from source
- TensorFlow version: 2.6.0
- Python version: python 3.8.10
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): bazel 3.7.2
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: no cuda
- GPU model and memory: no

**Describe the problem**

I'm trying to build from source Tensorflow Lite with Select Tensorflow Operation for Ubuntu 20.04 using Bazel build system and I get the compilation error.

```
ERROR: /home/anastasiia/tensorflow/tensorflow/lite/BUILD:1033:24: Linking of rule '//tensorflow/lite:libtensorflowlite.so' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/lite/libtensorflowlite.so-2.params
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates::GetDelegates(int) const'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::GetDelegates(int) const'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here
```

For Select TF operations I used tutorial: [ops_select](https://www.tensorflow.org/lite/guide/ops_select)
Without Select TF ops everything is ok. (without dependency `//tensorflow/lite/delegates/flex:delegate` )
Without buildin ops everything is ok. (without dependency `//tensorflow/lite/kernels:builtin_ops_all_linked`)

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. git clone https://github.com/tensorflow/tensorflow.git
2. Add the TensorFlow ops delegate library dependency to the build dependencies. In the file tensorflow/lite/BUILD for target libtensorflowlite add dependency : `tensorflow/lite/delegates/flex:delegate`.
3. bazel build --config=monolithic -c opt //tensorflow/lite:libtensorflowlite.so

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: /home/anastasiia/tensorflow/tensorflow/lite/BUILD:1033:24: Linking of rule '//tensorflow/lite:libtensorflowlite.so' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/lite/libtensorflowlite.so-2.params
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates::GetDelegates(int) const'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::GetDelegates(int) const'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here
collect2: error: ld returned 1 exit status
Target //tensorflow/lite:libtensorflowlite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.240s, Critical Path: 6.93s
INFO: 6 processes: 3 internal, 3 local.
FAILED: Build did NOT complete successfully

```"
52112,tensorflow 2.6 does not detect ptxas.exe,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.6
- Python version: 3.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4
- GPU model and memory: GTX 2060 6GB



**Describe the problem**

I'm currently having problems with Tensorflow not detecting the ptxas.exe inside the cuda folders as seen here:

`2021-09-23 23:36:12.332220: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204
2021-09-23 23:36:12.941230: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2
2021-09-23 23:36:12.944774: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2
2021-09-23 23:36:12.947274: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas.exe --version
2021-09-23 23:36:12.951640: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2
2021-09-23 23:36:12.953523: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.`

But I can confirm that the ptxas.exe does exist.

I'm currently new to this kind of stuff, so if possible elaborate more so I could understand better, thank you.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

``import os
from pickle import TRUE
os.add_dll_directory(""C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.4\\bin"")
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
import numpy as np
import tensorflow as tf

batch_size = 128
num_classes = 10
epochs = 12

img_rows, img_cols = 28, 28

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(60000,28,28,1)
x_test = x_test.reshape(10000,28,28,1)

print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=(28,28,1)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=tf.keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])``

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is the complete output:
`x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
2021-09-23 23:36:10.447932: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-23 23:36:10.895610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3967 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.52021-09-23 23:36:11.489872: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/12
2021-09-23 23:36:12.332220: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204
2021-09-23 23:36:12.941230: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2
2021-09-23 23:36:12.944774: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2
2021-09-23 23:36:12.947274: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas.exe --version
2021-09-23 23:36:12.951640: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2
2021-09-23 23:36:12.953523: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
469/469 [==============================] - 7s 9ms/step - loss: 29.2319 - accuracy: 0.1548 - val_loss: 3.7147 - val_accuracy: 0.4812
Epoch 2/12
469/469 [==============================] - 4s 9ms/step - loss: 12.0754 - accuracy: 0.2700 - val_loss: 1.5613 - val_accuracy: 0.6339
Epoch 3/12
469/469 [==============================] - 4s 9ms/step - loss: 6.1234 - accuracy: 0.3334 - val_loss: 1.2056 - val_accuracy: 0.6057
Epoch 4/12
469/469 [==============================] - 4s 9ms/step - loss: 3.6699 - accuracy: 0.3449 - val_loss: 1.4396 - val_accuracy: 0.5644
Epoch 5/12
Epoch 6/12
469/469 [==============================] - 4s 9ms/step - loss: 2.2604 - accuracy: 0.3577 - val_loss: 1.6565 - val_accuracy: 0.5153
Epoch 7/12
469/469 [==============================] - 4s 9ms/step - loss: 2.0842 - accuracy: 0.3743 - val_loss: 1.6186 - val_accuracy: 0.5310
Epoch 8/12
469/469 [==============================] - 4s 9ms/step - loss: 1.9903 - accuracy: 0.3897 - val_loss: 1.5678 - val_accuracy: 0.5573
Epoch 9/12
469/469 [==============================] - 4s 9ms/step - loss: 1.9153 - accuracy: 0.4106 - val_loss: 1.4787 - val_accuracy: 0.5954
Epoch 10/12
469/469 [==============================] - 4s 9ms/step - loss: 1.8446 - accuracy: 0.4291 - val_loss: 1.4024 - val_accuracy: 0.6281
Epoch 11/12
469/469 [==============================] - 4s 9ms/step - loss: 1.7916 - accuracy: 0.4465 - val_loss: 1.3342 - val_accuracy: 0.6566
Epoch 12/12
469/469 [==============================] - 4s 10ms/step - loss: 1.7438 - accuracy: 0.4593 - val_loss: 1.2915 - val_accuracy: 0.6798
Test loss: 1.2914707660675049
Test accuracy: 0.6797999739646912`"
52109,A bug for tf.keras.layers.TextVectorization when built from saved configs and weights,"I have tried writing a python program to save tf.keras.layers.TextVectorization to disk and load it with the answer of https://stackoverflow.com/questions/65103526/how-to-save-textvectorization-to-disk-in-tensorflow.
The TextVectorization layer built from saved configs outputs a vector with wrong length when the arg output_sequence_length is not None and output_mode='int'.
For example, if I set output_sequence_length= 10, and output_mode='int', it is expected that given a text, TextVectorization should output a vector with length of 10, see vectorizer and new_v2 in the code below.
However, if TextVectorization's arg output_mode='int' is set from saved configs, it doesn't output a vector with length of 10(actually it is 9, the real length of the sentence. It seems like output_sequence_length is not set successfully). See the object new_v1 in the code below.
The interesting thing is, I have compared from_disk['config']['output_mode'] and 'int', they equal to each other.
```
import tensorflow as tf
from tensorflow.keras.models import load_model
import pickle
# In[]
max_len = 10  # Sequence length to pad the outputs to.
text_dataset = tf.data.Dataset.from_tensor_slices([
                                                   ""I like natural language processing"",
                                                   ""You like computer vision"",
                                                   ""I like computer games and computer science""])
# Fit a TextVectorization layer
VOCAB_SIZE = 10  # Maximum vocab size.
vectorizer = tf.keras.layers.TextVectorization(
        max_tokens=None,
        standardize=""lower_and_strip_punctuation"",
        split=""whitespace"",
        output_mode='int',
        output_sequence_length=max_len
        )
vectorizer.adapt(text_dataset.batch(64))
# In[]
#print(vectorizer.get_vocabulary())
#print(vectorizer.get_config())
#print(vectorizer.get_weights())
# In[]


# Pickle the config and weights
pickle.dump({'config': vectorizer.get_config(),
             'weights': vectorizer.get_weights()}
            , open(""./models/tv_layer.pkl"", ""wb""))


# Later you can unpickle and use
# `config` to create object and
# `weights` to load the trained weights.

from_disk = pickle.load(open(""./models/tv_layer.pkl"", ""rb""))

new_v1 = tf.keras.layers.TextVectorization(
        max_tokens=None,
        standardize=""lower_and_strip_punctuation"",
        split=""whitespace"",
        output_mode=from_disk['config']['output_mode'],
        output_sequence_length=from_disk['config']['output_sequence_length'],
        )
# You have to call `adapt` with some dummy data (BUG in Keras)
new_v1.adapt(tf.data.Dataset.from_tensor_slices([""xyz""]))
new_v1.set_weights(from_disk['weights'])
new_v2 = tf.keras.layers.TextVectorization(
        max_tokens=None,
        standardize=""lower_and_strip_punctuation"",
        split=""whitespace"",
        output_mode='int',
        output_sequence_length=from_disk['config']['output_sequence_length'],
        )

# You have to call `adapt` with some dummy data (BUG in Keras)
new_v2.adapt(tf.data.Dataset.from_tensor_slices([""xyz""]))
new_v2.set_weights(from_disk['weights'])
print (""*""*10)
# In[]
test_sentence=""Jack likes computer scinece, computer games, and foreign language""

print(vectorizer(test_sentence))
print (new_v1(test_sentence))
print (new_v2(test_sentence))
print(from_disk['config']['output_mode']=='int')
```
Here are the print() outputs:
```
**********
tf.Tensor([ 1  1  3  1  3 11 12  1 10  0], shape=(10,), dtype=int64)
tf.Tensor([ 1  1  3  1  3 11 12  1 10], shape=(9,), dtype=int64)
tf.Tensor([ 1  1  3  1  3 11 12  1 10  0], shape=(10,), dtype=int64)
True
```
Does anyone know why?
I have also raised a same issue as this in the repo of Keras https://github.com/keras-team/keras/issues/15382 
"
52108,Error using xla_sharding split on XLA_GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Unknown
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2/8.2
- GPU model and memory: 8x NVIDIA A100 (40 GB)

**Describe the current behavior**
I get an error when trying to use xla_sharding on an XLA_GPU device. The error is:

InvalidArgumentError: Trying to access resource _AnonymousVar584 (defined @ <ipython-input-27-90261c699d9c>:18) located in device /job:localhost/replica:0/task:0/device:XLA_GPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_test_xla_sharding_split_3074]

**Describe the expected behavior**
A tensor split into 8 parts across the 0 dimension.

**Standalone code to reproduce the issue**

```
import os
os.environ[""XLA_FLAGS""] = ""--xla_gpu_cuda_data_dir=/usr/local/cuda""
os.environ[""TF_XLA_FLAGS""] = ""--tf_xla_enable_xla_devices""

from tensorflow.compiler.xla.experimental.xla_sharding import xla_sharding
import numpy as np
import tensorflow as tf

tf.config.optimizer.set_jit(True)
print(""Tensorflow version "" + tf.__version__)

dim = 0
num_parts = 8

mirrored_strategy = tf.distribute.MirroredStrategy(
    devices=[""device:XLA_GPU:0"", ""device:XLA_GPU:1"", ""device:XLA_GPU:2"", ""device:XLA_GPU:3"", 
             ""device:XLA_GPU:4"", ""device:XLA_GPU:5"", ""device:XLA_GPU:6"", ""device:XLA_GPU:7""
])

with mirrored_strategy.scope():
    var0 = tf.Variable([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0])
    var0_split = tf.Variable([10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0])

@tf.function(jit_compile=True)
def test_xla_sharding_split(x, y):
    with mirrored_strategy.scope():
        y.assign(xla_sharding.split(x, dim, num_parts, use_sharding_op=True))

test_xla_sharding_split(var0, var0_split)
```


**Other info / logs** Include any logs or source code that would be helpful to
attached tf_env.txt file
"
52107,"ValueError: Layer conv2d_18 expects 1 inputs, but it received 2 input tensors. ","```

def unet_model(input_size=(96, 128, 3), n_filters=32, n_classes=23):
    """"""
    Unet model
    
    Arguments:
        input_size -- Input shape 
        n_filters -- Number of filters for the convolutional layers
        n_classes -- Number of output classes
    Returns: 
        model -- tf.keras.Model
    """"""
    
    inputs = Input(input_size)
    
    # Contracting Path (encoding)
    # Add a conv_block with the inputs of the unet_ model and n_filters
    ### START CODE HERE
    
    cblock1 = conv_block(inputs,n_filters)
    
    # Chain the first element of the output of each block to be the input of the next conv_block. 
    # Double the number of filters at each new step
    cblock2 = conv_block(inputs,max_pooling=True) #inputs=None, n_filters=32, dropout_prob=0, max_pooling=True
    cblock3 = conv_block(cblock2,max_pooling=True)
    cblock4 = conv_block(cblock3,dropout=0.3,max_pooling=True) # Include a dropout_prob of 0.3 for this layer
    # Include a dropout_prob of 0.3 for this layer, and avoid the max_pooling layer
    cblock5 = conv_block(cblock4,128, dropout=0.3, max_pooling=False) 
    ### END CODE HERE
    
    # Expanding Path (decoding)
    # Add the first upsampling_block.
    # Use the cblock5[0] as expansive_input and cblock4[1] as contractive_input and n_filters * 8
    ### START CODE HERE
    ublock6 = upsampling_block(expansive_input ,contractive_input, n_filters * 8) #upsampling_block(expansive_input, contractive_input, n_filters=32)
    
    # Chain the output of the previous block as expansive_input and the corresponding contractive block output.
    # Note that you must use the second element of the contractive block i.e before the maxpooling layer. 
    # At each step, use half the number of filters of the previous block 
    
    ublock7 = upsampling_block(ublock6,contractive_input, n_filter*4)
    ublock8 = upsampling_block(ublock5,contractive_input, n_filter*2)
    ublock9 = upsampling_block(ublock9,contractive_input,  n_filter*1)
    
    ### END CODE HERE

    conv9 = Conv2D(n_filters,
                 3,
                 activation='relu',
                 padding='same',
                 kernel_initializer='he_normal')(ublock9)

    # Add a Conv2D layer with n_classes filter, kernel size of 1 and a 'same' padding
    ### START CODE HERE
    conv10 = Conv2D(n_classes,1, padding='same')(conv9)
    ### END CODE HERE
    
    model = tf.keras.Model(inputs=inputs, outputs=conv10)

    return model



```

![2](https://user-images.githubusercontent.com/26819449/134488571-b8c36108-bd89-437d-a63c-ab4d71a03c93.JPG)

![1](https://user-images.githubusercontent.com/26819449/134488612-c3db09a3-94c7-4759-a7e0-44f844e500a9.JPG)

Can you tell me why I am getting this error I am trying to Implement U-net respectively?
Thank You.


"
52106,Pricing details for Tensorflow ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


Hi Team,
I am interested in knowing the pricing details and the storage availability as I want to use this Tensorflow software in my project. I also would want to know any new and important feature the latest version has come up with. Appreciating the response at the latest.
Thanking you.

Regards,
Aditi G"
52105,Very slow inference on TF Lite>=2.4.0 converted model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.4 and Ubuntu 18.04 LTS on Colab machine
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.3.0 and 2.6.0


### 2. Code

This notebook demonstrates the bug with the simplest example I came up with.

https://colab.research.google.com/gist/ebraraktas/30d0a24ce2fb391a22e68a30e2f9cdc1

### 3. Failure after conversion

Inference takes too much time on models converted by `tensorflow>=2.4.0` (See `Inference Logs`). I have attached the models; but as it is described in the colab, you can create them as well. 

As you can see from the logs below, quantized `CONV2D` seems to updated to `version '5'` and this **runs approximately 27x slower** on x86_64 machine. However, it runs slightly faster on ARM.

#### Inference Logs:

##### x86_64 - Colab
```
TF Runtime Version: 2.6.0
Model path: model_2.3.0.tflite
Test Duration: 0.5242369174957275 s
= = = = = = = = = = = = = = = = = = = = 
TF Runtime Version: 2.6.0
Model path: model_2.3.0_quant.tflite
Test Duration: 0.7532312870025635 s
= = = = = = = = = = = = = = = = = = = = 
TF Runtime Version: 2.6.0
Model path: model_2.6.0.tflite
Test Duration: 0.532163143157959 s
= = = = = = = = = = = = = = = = = = = = 
TF Runtime Version: 2.6.0
Model path: model_2.6.0_quant.tflite
Test Duration: 18.914307594299316 s
= = = = = = = = = = = = = = = = = = = = 
= = = = = = = = = = = = = = = = = = = = 
TF Runtime Version: 2.3.0
Model path: model_2.3.0.tflite
Test Duration: 0.5360305309295654 s
= = = = = = = = = = = = = = = = = = = = 
TF Runtime Version: 2.3.0
Model path: model_2.3.0_quant.tflite
Test Duration: 0.5518338680267334 s
= = = = = = = = = = = = = = = = = = = = 
TF Runtime Version: 2.3.0
Model path: model_2.6.0.tflite
Test Duration: 0.5399584770202637 s
= = = = = = = = = = = = = = = = = = = = 
TF Runtime Version: 2.3.0
Model path: model_2.6.0_quant.tflite
Cannot create Interpreter! Exception:
Didn't find op for builtin opcode 'CONV_2D' version '5'
Registration failed.
```

##### ARM - iPhone 12 - TensorFlowLite 2.6.0

```
| model_2.3.0.tflite | 199.0520 ms | 
| model_2.6.0.tflite | 183.2420 ms | 
| model_2.3.0_quant.tflite |  71.3949 ms | 
| model_2.6.0_quant.tflite |  63.4819 ms | 
```

[models.zip](https://github.com/tensorflow/tensorflow/files/7216038/models.zip)
"
52103,libtensorflowlite.so building problem tf2.3,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.3
- Python version: 3.7
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 5.5
- CUDA/cuDNN version: 10.2
- GPU model and memory: 1080ti


**Describe the problem**
I am going to build **libtensorflowlite.so** for raspberry pi 3b using `bazel build --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so `.
However,  I get the error as shown bellow, I dont know how to fix that proble, it seems like like cross-compile toolchains errors `Internal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_embedded_arm//:aarch64_toolchain_config BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false' (requested by nodes '@local_config_embedded_arm//:cc-compiler-aarch64 BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false'),` I have tried several ways to download ruy btw,  but still cannot get the bazel build works
**Provide the exact sequence of commands / steps that you executed before running into the problem**
1.  I put the following into **tensorflow/lite/BUILD**

```
cc_binary(
    name=""libtensorflowLite.so"",
    linkopts=[
        ""-shared"",
        ""-Wl,-soname=libtensorflowLite.so""
    ],
    linkshared = 1,
    copts = tflite_copts(),
    deps=[
        "":framework"",
        ""//tensorflow/lite/kernels:builtin_ops""
    ]
)
```

2.  `bazel build --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so
`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from /home/yckj1497/tf_23/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/yckj1497/tf_23/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from /home/yckj1497/tf_23/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/yckj2211/anaconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/yckj2211/anaconda3/lib/python3.6/site-packages --python_path=/home/yckj2211/anaconda3/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:elinux_armhf in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --config=elinux --cpu=armhf
INFO: Found applicable config definition build:elinux in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
INFO: Found applicable config definition build:linux in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/yckj1497/tf_23/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Repository ruy instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule third_party_http_archive defined at:
  /home/yckj1497/tf_23/tensorflow/third_party/repo.bzl:223:28: in <toplevel>
ERROR: An error occurred during the fetch of repository 'ruy':
   java.io.IOException: thread interrupted
Internal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_embedded_arm//:aarch64_toolchain_config BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false' (requested by nodes '@local_config_embedded_arm//:cc-compiler-aarch64 BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false')
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:515)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)
	at java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1386)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.execLocalTasks(ForkJoinPool.java:1040)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1058)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.lang.IllegalStateException: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\y'
	at com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:983)
	at com.google.devtools.build.lib.rules.cpp.CcModule.ccToolchainConfigInfoFromSkylark(CcModule.java:902)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:130)
	at com.google.devtools.build.lib.syntax.BuiltinCallable.fastcall(BuiltinCallable.java:73)
	at com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)
	at com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:588)
	at com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)
	at com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:674)
	at com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)
	at com.google.devtools.build.lib.syntax.Eval.execReturn(Eval.java:215)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:259)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:231)
	at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:53)
	at com.google.devtools.build.lib.syntax.Eval.execFunctionBody(Eval.java:37)
	at com.google.devtools.build.lib.syntax.StarlarkFunction.fastcall(StarlarkFunction.java:115)
	at com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)
	at com.google.devtools.build.lib.syntax.Starlark.call(Starlark.java:334)
	at com.google.devtools.build.lib.analysis.skylark.SkylarkRuleConfiguredTargetUtil.buildRule(SkylarkRuleConfiguredTargetUtil.java:136)
	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:468)
	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:190)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:888)
	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:899)
	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:338)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:438)
	... 7 more
Caused by: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\y'
	at com.google.protobuf.TextFormat$Tokenizer.parseException(TextFormat.java:1238)
	at com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1228)
	at com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1200)
	at com.google.protobuf.TextFormat$Tokenizer.consumeString(TextFormat.java:1181)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1987)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1823)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1689)
	at com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1675)
	at com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1566)
	at com.google.protobuf.TextFormat.merge(TextFormat.java:1370)
	at com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getFeature(CppActionConfigs.java:1594)
	at com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:413)
	... 34 more

INFO: Elapsed time: 24.924s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (41 packages loaded, 251 targets configured)
Internal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_embedded_arm//:aarch64_toolchain_config BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false' (requested by nodes '@local_config_embedded_arm//:cc-compiler-aarch64 BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false')
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:515)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)
	at java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1386)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.execLocalTasks(ForkJoinPool.java:1040)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1058)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.lang.IllegalStateException: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\y'
	at com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:983)
	at com.google.devtools.build.lib.rules.cpp.CcModule.ccToolchainConfigInfoFromSkylark(CcModule.java:902)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:130)
	at com.google.devtools.build.lib.syntax.BuiltinCallable.fastcall(BuiltinCallable.java:73)
	at com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)
	at com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:588)
	at com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)
	at com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:674)
	at com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)
	at com.google.devtools.build.lib.syntax.Eval.execReturn(Eval.java:215)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:259)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:231)
	at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:53)
	at com.google.devtools.build.lib.syntax.Eval.execFunctionBody(Eval.java:37)
	at com.google.devtools.build.lib.syntax.StarlarkFunction.fastcall(StarlarkFunction.java:115)
	at com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)
	at com.google.devtools.build.lib.syntax.Starlark.call(Starlark.java:334)
	at com.google.devtools.build.lib.analysis.skylark.SkylarkRuleConfiguredTargetUtil.buildRule(SkylarkRuleConfiguredTargetUtil.java:136)
	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:468)
	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:190)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:888)
	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:899)
	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:338)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:438)
	... 7 more
Caused by: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\y'
	at com.google.protobuf.TextFormat$Tokenizer.parseException(TextFormat.java:1238)
	at com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1228)
	at com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1200)
	at com.google.protobuf.TextFormat$Tokenizer.consumeString(TextFormat.java:1181)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1987)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1823)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1689)
	at com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1675)
	at com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1566)
	at com.google.protobuf.TextFormat.merge(TextFormat.java:1370)
	at com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getFeature(CppActionConfigs.java:1594)
	at com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:413)
	... 34 more
java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_embedded_arm//:aarch64_toolchain_config BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false' (requested by nodes '@local_config_embedded_arm//:cc-compiler-aarch64 BuildConfigurationValue.Key[6b681d7708c478d1cd948cd10629a4e4fb7eb97326825b5498e94f895f792b01] false')
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:515)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)
	at java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1386)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.execLocalTasks(ForkJoinPool.java:1040)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1058)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.lang.IllegalStateException: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\y'
	at com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:983)
	at com.google.devtools.build.lib.rules.cpp.CcModule.ccToolchainConfigInfoFromSkylark(CcModule.java:902)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:130)
	at com.google.devtools.build.lib.syntax.BuiltinCallable.fastcall(BuiltinCallable.java:73)
	at com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)
	at com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:588)
	at com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)
	at com.google.devtools.build.lib.syntax.Eval.doEval(Eval.java:674)
	at com.google.devtools.build.lib.syntax.Eval.eval(Eval.java:423)
	at com.google.devtools.build.lib.syntax.Eval.execReturn(Eval.java:215)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:259)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:231)
	at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:53)
	at com.google.devtools.build.lib.syntax.Eval.execFunctionBody(Eval.java:37)
	at com.google.devtools.build.lib.syntax.StarlarkFunction.fastcall(StarlarkFunction.java:115)
	at com.google.devtools.build.lib.syntax.Starlark.fastcall(Starlark.java:359)
	at com.google.devtools.build.lib.syntax.Starlark.call(Starlark.java:334)
	at com.google.devtools.build.lib.analysis.skylark.SkylarkRuleConfiguredTargetUtil.buildRule(SkylarkRuleConfiguredTargetUtil.java:136)
	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:468)
	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:190)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:888)
	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:899)
	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:338)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:438)
	... 7 more
Caused by: com.google.protobuf.TextFormat$ParseException: 10:13: Invalid escape sequence: '\y'
	at com.google.protobuf.TextFormat$Tokenizer.parseException(TextFormat.java:1238)
	at com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1228)
	at com.google.protobuf.TextFormat$Tokenizer.consumeByteString(TextFormat.java:1200)
	at com.google.protobuf.TextFormat$Tokenizer.consumeString(TextFormat.java:1181)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1987)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1823)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValue(TextFormat.java:1944)
	at com.google.protobuf.TextFormat$Parser.consumeFieldValues(TextFormat.java:1877)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1812)
	at com.google.protobuf.TextFormat$Parser.mergeField(TextFormat.java:1689)
	at com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1675)
	at com.google.protobuf.TextFormat$Parser.merge(TextFormat.java:1566)
	at com.google.protobuf.TextFormat.merge(TextFormat.java:1370)
	at com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getFeature(CppActionConfigs.java:1594)
	at com.google.devtools.build.lib.rules.cpp.CppActionConfigs.getLegacyFeatures(CppActionConfigs.java:413)
FAILED: Build did NOT complete successfully (41 packages loaded, 251 targets configured)

```"
52101,Error compilation NVCC for   TF 2.5 from source using Clang-12 ,"Hi all!
**System information**
- OS  -  Linux Debian 10 debian 4.19.0-17-amd64
- TensorFlow: try install from source
- TensorFlow version:  TF 2.5.1 nightly
- Python version: 3.8.12 build from source Clang-12
- Try compiled in the separate venv 3.8.12 
- Bazel version: 3.7.2 
- GCC/Compiler version:  gcc/g++ -  Debian 8.3.0-6
- Clang compiler: complete  bundle of  LLVM-12 from https://apt.llvm.org/  
- CUDA/cuDNN version:  Cuda_11.2.2_460.32.03 installed from `run` file / CUDNN 8.1.1.3
- GPU model and memory:  GT1050 2GB RAM single

**Describe the problem**
I tried several times to compile from source TF 2.5 from sources using clang as CUDA compiler.
Before I successful   compiled TF 2.5.1 from source using  system GCC 8 with warning about unsporting  MLIR.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
First step: run configure.py with separate venv

```
(tf25) mvg@debian:~/tensorflow$ ./configure
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is /home/mvg/tf25/bin/python3]: 
Found possible Python library paths:
  /home/mvg/tf25/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/home/mvg/tf25/lib/python3.8/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.2 in:
    /usr/local/cuda-11.2/targets/x86_64-linux/lib
    /usr/local/cuda-11.2/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/local/cuda-11.2/targets/x86_64-linux/lib
    /usr/local/cuda-11.2/targets/x86_64-linux/include

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: 

Do you want to use clang as CUDA compiler? [y/N]: y
Clang will be used as CUDA compiler.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify which clang should be used as device and host compiler. [Default is /usr/lib/llvm-12/bin/clang]: 

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```

Step 2: compilation log
```
(tf251) mvg@debian:~/tensorflow$ bazel build -c opt --copt=-msse3  --copt=-msse4.1 --copt=-msse4.2  --copt=-mavx --copt=-mfma --copt=-mfma4 --config=cuda --jobs=5 //tensorflow/tools/pip_package:build_pip_package
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=80
INFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/mvg/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/mvg/tf251/bin/python3 --action_env PYTHON_LIB_PATH=/home/mvg/tf251/lib/python3.8/site-packages --python_path=/home/mvg/tf251/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=/home/mvg/python3.8.12/lib/ --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-12/bin/clang --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /home/mvg/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/mvg/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/mvg/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/mvg/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /home/mvg/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/mvg/tensorflow/tensorflow/workspace0.bzl:105:34: in workspace
  /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (246 packages loaded, 33167 targets configured).
INFO: Found 1 target...
ERROR: /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/nccl_archive/BUILD.bazel:77:13: C++ compilation of rule '@nccl_archive//:nccl' failed (Exit 1): clang failed: error executing command /usr/lib/llvm-12/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/nccl/bootstrap.pic.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/nccl/bootstrap.pic.o' -iquote ... (remaining 53 argument(s) skipped)
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:431:
In file included from /usr/include/strings.h:144:
/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration
__NTH (bcopy (const void *__src, void *__dest, size_t __len))
       ^
/usr/include/strings.h:38:13: note: previous declaration is here
extern void bcopy (const void *__src, void *__dest, size_t __n)
            ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:431:
In file included from /usr/include/strings.h:144:
/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration
__NTH (bzero (void *__dest, size_t __len))
       ^
/usr/include/strings.h:42:13: note: previous declaration is here
extern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));
            ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration
__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,
       ^
/usr/include/string.h:42:14: note: previous declaration is here
extern void *memcpy (void *__restrict __dest, const void *__restrict __src,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration
__NTH (memmove (void *__dest, const void *__src, size_t __len))
       ^
/usr/include/string.h:46:14: note: previous declaration is here
extern void *memmove (void *__dest, const void *__src, size_t __n)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration
__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,
       ^
/usr/include/string.h:377:14: note: previous declaration is here
extern void *mempcpy (void *__restrict __dest,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration
__NTH (memset (void *__dest, int __ch, size_t __len))
       ^
/usr/include/string.h:60:14: note: previous declaration is here
extern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration
__NTH (explicit_bzero (void *__dest, size_t __len))
       ^
/usr/include/string.h:435:13: note: previous declaration is here
extern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1));
            ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration
__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))
       ^
/usr/include/string.h:121:14: note: previous declaration is here
extern char *strcpy (char *__restrict __dest, const char *__restrict __src)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration
__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))
       ^
/usr/include/string.h:451:14: note: previous declaration is here
extern char *stpcpy (char *__restrict __dest, const char *__restrict __src)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration
__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,
       ^
/usr/include/string.h:124:14: note: previous declaration is here
extern char *strncpy (char *__restrict __dest,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:116:8: error: exception specification in declaration does not match previous declaration
__NTH (stpncpy (char *__dest, const char *__src, size_t __n))
       ^
/usr/include/string.h:459:14: note: previous declaration is here
extern char *stpncpy (char *__restrict __dest,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:126:8: error: exception specification in declaration does not match previous declaration
__NTH (strcat (char *__restrict __dest, const char *__restrict __src))
       ^
/usr/include/string.h:129:14: note: previous declaration is here
extern char *strcat (char *__restrict __dest, const char *__restrict __src)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:
In file included from /usr/include/string.h:494:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:133:8: error: exception specification in declaration does not match previous declaration
__NTH (strncat (char *__restrict __dest, const char *__restrict __src,
       ^
/usr/include/string.h:132:14: note: previous declaration is here
extern char *strncat (char *__restrict __dest, const char *__restrict __src,
             ^
In file included from external/nccl_archive/src/bootstrap.cc:12:
bazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:117:7: error: call to 'memcpy' is ambiguous
      memcpy(addrs+found, interface->ifa_addr, salen);
      ^~~~~~
/usr/include/string.h:42:14: note: candidate function
extern void *memcpy (void *__restrict __dest, const void *__restrict __src,
             ^
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: note: candidate function
__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,
       ^
In file included from external/nccl_archive/src/bootstrap.cc:12:
bazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:192:5: error: call to 'memcpy' is ambiguous
    memcpy(localAddrs+found, interface->ifa_addr, salen);
    ^~~~~~
/usr/include/string.h:42:14: note: candidate function
extern void *memcpy (void *__restrict __dest, const void *__restrict __src,
             ^
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: note: candidate function
__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,
       ^
In file included from external/nccl_archive/src/bootstrap.cc:12:
bazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:227:5: error: call to 'memset' is ambiguous
    memset(&hints, 0, sizeof(hints));
    ^~~~~~
/usr/include/string.h:60:14: note: candidate function
extern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));
             ^
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: note: candidate function
__NTH (memset (void *__dest, int __ch, size_t __len))
       ^
In file included from external/nccl_archive/src/bootstrap.cc:12:
bazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:239:7: error: call to 'memcpy' is ambiguous
      memcpy(&sin, p->ai_addr, sizeof(struct sockaddr_in));
      ^~~~~~
/usr/include/string.h:42:14: note: candidate function
extern void *memcpy (void *__restrict __dest, const void *__restrict __src,
             ^
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: note: candidate function
__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,
       ^
In file included from external/nccl_archive/src/bootstrap.cc:12:
bazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:245:7: error: call to 'memcpy' is ambiguous
      memcpy(&sin6, p->ai_addr, sizeof(struct sockaddr_in6));
      ^~~~~~
/usr/include/string.h:42:14: note: candidate function
extern void *memcpy (void *__restrict __dest, const void *__restrict __src,
             ^
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: note: candidate function
__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,
       ^
external/nccl_archive/src/bootstrap.cc:135:5: error: call to 'memcpy' is ambiguous
    memcpy(rankAddressesRoot+info.rank, &info.extAddressListenRoot, sizeof(union socketAddress));
    ^~~~~~
/usr/include/string.h:42:14: note: candidate function
extern void *memcpy (void *__restrict __dest, const void *__restrict __src,
             ^
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: note: candidate function
__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,
       ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated when compiling for sm_61.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/mvg/tensorflow/tensorflow/lite/python/BUILD:59:10 C++ compilation of rule '@nccl_archive//:nccl' failed (Exit 1): clang failed: error executing command /usr/lib/llvm-12/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/nccl/bootstrap.pic.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/nccl/bootstrap.pic.o' -iquote ... (remaining 53 argument(s) skipped)
INFO: Elapsed time: 8973.744s, Critical Path: 178.02s
INFO: 16630 processes: 4349 internal, 12281 local.
FAILED: Build did NOT complete successfully
```
as see above compilation error rising when compiling section NCCL which turn off by configure script.
`.bazelrc` has link for Ubuntu repo NCCL
```
434: build:rbe_linux_cuda11.2_nvcc_base --repo_env=TF_NCCL_CONFIG_REPO=""@ubuntu18.04-gcc7_manylinux2010-cuda11.2-cudnn8.1-tensorrt7.2_config_nccl""
460: build:rbe_linux_cuda_clang_base --repo_env=TF_NCCL_CONFIG_REPO=""@ubuntu18.04-clang_manylinux2010-cuda11.2-cudnn8.1-tensorrt7.2_config_nccl""
```
May be problem is here?

"
52098,Build libtensorflow_cc.dll x86 error on Window,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.6.0
- Python version: Python 3.9.7
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): MSVC 2019
- CUDA/cuDNN version:  11.2
- GPU model and memory: NVIDIA GeForce GTX 1650


As title, I got an error when try to create libtensorflow_cc.lib and libtensorflow_cc.dll which can be used with MSVC2019_32bit compiler.

I used below command:
`bazel build --cpu=x86 --config=opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --local_ram_resources=2048 tensorflow:tensorflow_cc`
But I got below error:
` The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=225
INFO: Reading rc options for 'build' from c:\users\1111\desktop\work\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/1111/AppData/Local/Programs/Python/Python39/python.exe
INFO: Reading rc options for 'build' from c:\users\1111\desktop\work\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from c:\users\1111\desktop\work\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/1111/AppData/Local/Programs/Python/Python39/python.exe --action_env PYTHON_LIB_PATH=C:/Users/1111/AppData/Local/Programs/Python/Python39/lib/site-packages --python_path=C:/Users/1111/AppData/Local/Programs/Python/Python39/python.exe --action_env TF_CUDA_VERSION=11.2 --action_env TF_CUDNN_VERSION=8 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file c:\users\1111\desktop\work\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\1111\desktop\work\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file c:\users\1111\desktop\work\tensorflow\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file c:\users\1111\desktop\work\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:cuda in file c:\users\1111\desktop\work\tensorflow\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:windows in file c:\users\1111\desktop\work\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\1111\desktop\work\tensorflow\.bazelrc: --define framework_shared_object=false
DEBUG: C:/users/1111/_bazel_1111/ejl7ywpf/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
ERROR: C:/users/1111/_bazel_1111/ejl7ywpf/external/local_config_cuda/crosstool/BUILD:24:19: in cc_toolchain_suite rule @local_config_cuda//crosstool:toolchain: cc_toolchain_suite '@local_config_cuda//crosstool:toolchain' does not contain a toolchain for cpu 'x86'
ERROR: Analysis of target '//tensorflow:tensorflow_cc' failed; build aborted: Analysis of target '@local_config_cuda//crosstool:toolchain' failed
INFO: Elapsed time: 1.352s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (35 packages loaded, 175 targets configured)`

Is there any way to fix this?
If yes, please share with me.
Thanks"
52096,training with model_main_tf2.py to mobilenetv2 quantized_300x300 cause error,"Hello 
I've been training my model with 
ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8
but it's very slow with raspberry pi, 
so I think I should try with new pretrained model : ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03
with new model, 
when I start training, it cause error 
`
ValueError: ssd_mobilenet_v2 is not supported. See `model_builder.py` for features extractors compatible with different versions of Tensorflow`

I saw #46970 and  
saidRaiss recommend to change type :""ssd_mobilenet_v2"" to ssd_mobilenet_v2_keras
it cause other error saying 
```

2021-09-22 20:51:48.499444: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101
Traceback (most recent call last):
  File ""C:\Users\pedro\TFODCourse\Tensorflow\models\research\object_detection\model_main_tf2.py"", line 115, in <module>
    tf.compat.v1.app.run()
  File ""C:\Users\pedro\anaconda3\envs\tensorflow39\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\pedro\anaconda3\envs\tensorflow39\lib\site-packages\absl\app.py"", line 312, in run
    _run_main(main, args)
  File ""C:\Users\pedro\anaconda3\envs\tensorflow39\lib\site-packages\absl\app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""C:\Users\pedro\TFODCourse\Tensorflow\models\research\object_detection\model_main_tf2.py"", line 106, in main
    model_lib_v2.train_loop(
  File ""C:\Users\pedro\anaconda3\envs\tensorflow39\lib\site-packages\object_detection-0.1-py3.9.egg\object_detection\model_lib_v2.py"", line 599, in train_loop
    load_fine_tune_checkpoint(
  File ""C:\Users\pedro\anaconda3\envs\tensorflow39\lib\site-packages\object_detection-0.1-py3.9.egg\object_detection\model_lib_v2.py"", line 389, in load_fine_tune_checkpoint
    raise IOError('Checkpoint is expected to be an object-based checkpoint.')
OSError: Checkpoint is expected to be an object-based checkpoint.
```

my pre-trainedmodel pipeline.config is as below

```

model {
  ssd {
    num_classes: 90
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    feature_extractor {
      type: ""ssd_mobilenet_v2_keras""
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.99999989895e-05
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.0299999993294
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.999700009823
          center: true
          scale: true
          epsilon: 0.0010000000475
          train: true
        }
      }
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.99999989895e-05
            }
          }
          initializer {
            truncated_normal_initializer {
              mean: 0.0
              stddev: 0.0299999993294
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.999700009823
            center: true
            scale: true
            epsilon: 0.0010000000475
            train: true
          }
        }
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.800000011921
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.20000000298
        max_scale: 0.949999988079
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.333299994469
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 9.99999993923e-09
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.990000009537
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 3
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
  }
}
train_config {
  batch_size: 24
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
  optimizer {
    rms_prop_optimizer {
      learning_rate {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.00400000018999
          decay_steps: 800720
          decay_factor: 0.949999988079
        }
      }
      momentum_optimizer_value: 0.899999976158
      decay: 0.899999976158
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""PATH_TO_BE_CONFIGURED/model.ckpt""
  from_detection_checkpoint: true
  num_steps: 20000000
}
train_input_reader {
  label_map_path: ""PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt""
  tf_record_input_reader {
    input_path: ""PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100""
  }
}
eval_config {
  num_examples: 8000
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: true
  include_metrics_per_category: true
}
eval_input_reader {
  label_map_path: ""PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010""
  }
}
graph_rewriter {
  quantization {
    delay: 48000
    weight_bits: 8
    activation_bits: 8
  }
}

```"
52094,Couldn't import 'tensorflow.tools.graph_transforms' in tensorflow 2.5.0,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary pip install
- TensorFlow version (use command below): 2.5.0
- Python version: 3.9.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

```
from tensorflow.tools.graph_transforms import TransformGraph
```

Error message:

```
ModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'
```

Tensorflow version:
```
> pip show tensorflow

Name: tensorflow
Version: 2.5.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: ..\appdata\local\programs\python\python39\lib\site-packages
Requires: tensorboard, wheel, keras-nightly, grpcio, wrapt, six, astunparse, gast, protobuf, typing-extensions, google-pasta, numpy, tensorflow-estimator, h5py, flatbuffers, opt-einsum, absl-py, keras-preprocessing, termcolor
Required-by:
```

**Describe the expected behavior**
Successfully import the graph_transforms module.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): N/A

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
from tensorflow.tools.graph_transforms import TransformGraph
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Source code: https://replit.com/@Amit-KumarKuma4/objectdetection"
52092,Undeclared identifier when building from source v2.3.2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.3.2
- Python version: 3.8
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): msvc v142
- CUDA/cuDNN version: CUDA 10.2 / cuDNN 7.6
- GPU model and memory: Nvidia GeForce RTX 2060 / 14 GB



**Describe the problem**
Undeclared identifier error when building from source v2.3.2. 
**Provide the exact sequence of commands / steps that you executed before running into the problem**
**Configuration**
> $ python configure.py
> You have bazel 3.1.0 installed.
> Please specify the location of python. [Default is C:\python\python.exe]:
> 
> 
> Found possible Python library paths:
>   C:\python\lib\site-packages
> Please input the desired Python library path to use.  Default is [C:\python\lib\site-packages]
> 
> Do you wish to build TensorFlow with ROCm support? [y/N]: n
> No ROCm support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with CUDA support? [y/N]: y
> CUDA support will be enabled for TensorFlow.
> 
> Found CUDA 10.2 in:
>     C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64
>     C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include
> Found cuDNN 7 in:
>     C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64
>     C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include
> 
> 
> Please specify a list of comma-separated CUDA compute capabilities you want to build with.
> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
> Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:
> 
> 
> Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:
> 
> 
> Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: n
> Not overriding eigen strong inline, some compilations could take more than 20 mins.
> 
> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
> Not configuring the WORKSPACE for Android builds.
> 
> Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
>         --config=mkl            # Build with MKL support.
>         --config=monolithic     # Config for mostly static monolithic build.
>         --config=ngraph         # Build with Intel nGraph support.
>         --config=numa           # Build with NUMA support.
>         --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
>         --config=v2             # Build TensorFlow 2.x instead of 1.x.
> Preconfigured Bazel build configs to DISABLE default on features:
>         --config=noaws          # Disable AWS S3 filesystem support.
>         --config=nogcp          # Disable GCP support.
>         --config=nohdfs         # Disable HDFS support.
>         --config=nonccl         # Disable NVIDIA NCCL support.
> 
**Build command:**
`bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package` 

**Any other info / logs**
`external/com_google_absl\absl/time/clock.h(70): error C2065: 'Duration': undeclared identifier
external/com_google_absl\absl/time/clock.h(70): error C2146: syntax error: missing ')' before identifier 'duration'
external/com_google_absl\absl/time/clock.h(70): error C2143: syntax error: missing ';' before '{'
external/com_google_absl\absl/time/clock.h(70): error C2447: '{': missing function header (old-style formal list?)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 899.084s, Critical Path: 504.33s
INFO: 1352 processes: 1352 local.
FAILED: Build did NOT complete successfully` 

The issue has been reported already [#38316](https://github.com/tensorflow/tensorflow/issues/38316). But the solution is not proper.
"
52091,Cannot load saved tf model (AttributeError: '_UserObject' object has no attribute 'add_slot'),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.9
- CUDA/cuDNN version: 11.3/8.2.1
- GPU model and memory: RTX 8000, 48 GB

**Describe the current behavior**
I'm loading our upsample model and saving it as a saved model:
```py
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow_addons.layers import InstanceNormalization, SpectralNormalization

model = load_model('upsample---[_20]---[______3171]---[_____63420].h5', custom_objects={'InstanceNormalization': InstanceNormalization, 'SpectralNormalization': SpectralNormalization})

tf.saved_model.save(model, 'upsample_saved_model')
```
Then, when I'm trying to load it (do not use the same runtime if running in Collab):
```py
import tensorflow as tf

model = tf.saved_model.load('upsample_saved_model')
```
It errors with:
```
Traceback (most recent call last):
  File ""/home/daniel/trt/2_find_layer_names.py"", line 4, in <module>
    model = tf.saved_model.load('upsample_saved_model')
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py"", line 864, in load
    result = load_internal(export_dir, tags, options)[""root""]
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py"", line 902, in load_internal
    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py"", line 162, in __init__
    self._load_all()
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py"", line 259, in _load_all
    self._load_nodes()
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py"", line 448, in _load_nodes
    slot_variable = optimizer_object.add_slot(
AttributeError: '_UserObject' object has no attribute 'add_slot'
```

The same code works just fine in TensorFlow 2.5.

If you need the model to test: https://github.com/Sentdex/GANTheftAuto/tree/main/trained_models (it's the upsample one).

**Describe the expected behavior**
This should not error and load the model like in TensorFlow 2.5

**Standalone code to reproduce the issue**
As described above.

Additionally, I mentioned this issue here: https://github.com/tensorflow/tensorflow/issues/52013#issuecomment-920475505 and 
mohantym created a notebook and ran it in Collab: https://colab.research.google.com/gist/mohantym/187d7a2dc387998237211d7c590c6b1a/github_52013_sentdex.ipynb - you can replicate the issue in this notebook too, but after downloading/installing all the dependencies, then loading the model and saving it as the saved model, use Runtime -> Restart runtime before attempting to load saved model, otherwise this bug is not going to be triggered."
52090,"Unable to run the exe that has wheel generated by Rosetta terminal on MacOs Mojave, error: Symbol not found: ___darwin_check_fd_set_overflow","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs Mojave 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not relevant
- TensorFlow installed from (source or binary): private wheel
- TensorFlow version: 1.13.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): not relevant 
- CUDA/cuDNN version: not relevant
- GPU model and memory: not relevant



**Describe the problem**
Following the instructions in forum: https://github.com/tensorflow/tensorflow/issues/46044
I compiled tensorflow 1.13.1 using Bazel 0.21.1 under Bigsur using rosetta terminal in order to run this wheel under Rosettna emulator over arm/intel cpu, the generated wheel works fine on Bigsur and Catalina but it didn't work on Mojave I am getting error appeared in the logs section.
However the public wheel works fine on Mojave 10.14.6.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Use Big Sir with arm processor m1 since it has Terminal for Rosetta

2. open Terminal Get Info and enable ""Open using Rosetta"" (duplicate Terminal app for this and open Rosetta terminal)

3. git clone https://github.com/tensorflow/tensorflow.git

4. cd tensorflow

5. git checkout branch_name (for us: v1.13.1)

6. install bazel version 0.21.0

7. generate virtual environment of python3.6 that has in the requirements tensorflow==1.13.1 and other packages related to our project

8. ./configure (say N to everything except setting the python virtual environment created in 7 here)

9. bazel build //tensorflow/tools/pip_package:build_pip_package

10. bazel-bin/tensorflow/tools/pip_package/build_pip_package/tmp/tensorflow_pkg

11. rename .whl file to disable OS restrictions (us: tensorflow-1.13.1-py3-none-any.whl)

12. Take this wheel that support rosetta and set it in the virtual environment (instead of downloading public wheel), in requirements: ./wheels/tensorflow-1.13.1-py3-none-any.whl 

13. Build the exe using pyinstaller version 3.4 with this virtual environment that has private tensorflow wheel.

14. Run the exe from 13 on Moajve and you will get the error attached in logs 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

2021-09-20 14:10:58,992 INFO Traceback (most recent call last):
  File ""tensorflow/python/pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
  File ""imp.py"", line 297, in find_module
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
  File ""/p4client/ProAudio/dev_mainV12/ProAudio/XPlatform/Apps/WavesPluginServer/pythonVirtualEnv/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py"", line 627, in exec_module
  File ""tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
  File ""tensorflow/python/pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
ImportError: dlopen(/Library/Application Support/Waves/WavesPluginServer/WavesPluginServerV13.1.bundle/Contents/MacOS/_pywrap_tensorflow_internal.so, 6): Symbol not found: ___darwin_check_fd_set_overflow
  Referenced from: /Library/Application Support/Waves/WavesPluginServer/WavesPluginServerV13.1.bundle/Contents/MacOS/_pywrap_tensorflow_internal.so (which was built for Mac OS X 11.0)
  Expected in: /usr/lib/libSystem.B.dylib
 in /Library/Application Support/Waves/WavesPluginServer/WavesPluginServerV13.1.bundle/Contents/MacOS/_pywrap_tensorflow_internal.so


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
52089,tensor flow resource exhausted error on Kaggle notebooks,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0
- Python version: 3.9
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): NIL
- CUDA/cuDNN version: NIL
- GPU model and memory: kaggle notebook

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**this is the error i have been displayed**:
Resource exhausted: OOM when allocating tensor with
 shape[800000,32,30,62] and type float on
 /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc    
 [[{{node conv2d_1/convolution}}]]

**Describe the expected behavior**:

the expected behavior was that it runs the code and outputs the desired result. 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

none

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

none
"
52088,How to reduce libtensorflowlite.so size with C++ api,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.5.1
- Python version: 3.8
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 12.0.5
- CUDA/cuDNN version: None
- GPU model and memory: android GPU



**Describe the problem**
Hello , I want to build libtensorflowlite.so for android with c++ api, I want to build the selective so with a tflite model  .I follow the guide in https://github.com/tensorflow/tensorflow/issues/50946
I add tflite_custom_cc_library like this:

load(""//tensorflow/lite:build_def.bzl"", ""tflite_cc_shared_object"", ""tflite_copts"", ""tflite_copts_warnings"", ""tflite_custom_cc_library"")
tflite_custom_cc_library(
    name = ""custom_tflite_lib"",
    models = [
        ""test.tflite"",
    ],
)
and the trained tflite model for selective build. And the the command is :
bazel build -c opt --config android_arm64 tensorflow/lite:custom_tflite_lib

It built successfully.And the  so size reduced from 3.7M to 380K. But when I link the so, I found that it only contains the kernels and so on ,it doesn't contain the flatbuffer and other functions such as ""TfLiteDelegateCreate"" and so on.I have also read the issue https://github.com/tensorflow/tensorflow/issues/50946 
but the so built is only for c api, I want to use the android gpu to inference with opencl, so how should I reduce the libtensorflowlite.so for c++ api with other necessary parts?

Thanks.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build -c opt --config android_arm64 tensorflow/lite:custom_tflite_lib

"
52087,Potential dangling-pointer bug in function `GetPyArrayDescrForTensor` by holding a reference in a list after all its references released (a static analyzer report),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- TensorFlow installed from (source or binary): source
- TensorFlow version (commit): faad219
- Python version: 3.8.5

**Static analysis results, no POC.**
This static analysis report has been manually reviewed to verify its validity.

**Describe the current behavior**

The path provided by the static analyzer is as follows.

1. A new reference is returned from `PyTuple_New` and pointed to by `field`.
https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L341

2. A reference is stolen by function `PyList_SetItem`.
https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L350

3. Refcnt decrement in macro `Py_CLEAR` will make the reference held in the list a dangling pointer.
https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L352

**Contributing**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

A potential correct fix can be removing the following line.
https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/lib/core/ndarray_tensor.cc#L352"
52086, _EagerConst: Dst tensor is not initialized,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 21.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0-dev20210921 / 2.6
- Python version: 3.9.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4
- GPU model and memory: NVIDIA TITAN RTX 24220MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When running TF 2.7 and 2.6 with eager execution gives  the following error: ""in order to run _EagerConst: Dst tensor is not initialized."" 

It works with tf.compat.v1.disable_eager_execution() for TF 2.7/2.6 and TF 2.5.


**Describe the expected behavior**



LearningRate of 1.000000e-04
Epoch 1/100
2021-09-22 12:01:03.300192: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2021-09-22 12:01:04.275126: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
225/225 [==============================] - ETA: 0s - loss: 114.7353 - mae: 0.3077 - mse: 0.1493 - r2: 0.9997 - lr: 1.0000e-04
2021-09-22 12:04:35.644414: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.90GiB (rounded to 5259396608)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2021-09-22 12:04:35.644512: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc
2021-09-22 12:04:35.644553: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): 	Total Chunks: 82, Chunks in use: 73. 20.5KiB allocated for chunks. 18.2KiB in use in bin. 3.8KiB client-requested in use in bin.
2021-09-22 12:04:35.644571: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): 	Total Chunks: 2, Chunks in use: 0. 1.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-22 12:04:35.644587: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): 	Total Chunks: 2, Chunks in use: 1. 2.8KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2021-09-22 12:04:35.644602: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-22 12:04:35.644620: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): 	Total Chunks: 3, Chunks in use: 2. 16.2KiB allocated for chunks. 8.5KiB in use in bin. 8.0KiB client-requested in use in bin.
2021-09-22 12:04:35.644637: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): 	Total Chunks: 4, Chunks in use: 2. 47.0KiB allocated for chunks. 19.8KiB in use in bin. 16.0KiB client-requested in use in bin.
2021-09-22 12:04:35.644653: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): 	Total Chunks: 1, Chunks in use: 1. 28.2KiB allocated for chunks. 28.2KiB in use in bin. 28.1KiB client-requested in use in bin.
2021-09-22 12:04:35.644667: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-22 12:04:35.644679: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-22 12:04:35.644694: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): 	Total Chunks: 7, Chunks in use: 6. 1.42MiB allocated for chunks. 1.24MiB in use in bin. 1.15MiB client-requested in use in bin.
2021-09-22 12:04:35.644717: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): 	Total Chunks: 3, Chunks in use: 2. 1.07MiB allocated for chunks. 641.0KiB in use in bin. 463.5KiB client-requested in use in bin.
2021-09-22 12:04:35.644732: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): 	Total Chunks: 1, Chunks in use: 0. 840.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-22 12:04:35.644747: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): 	Total Chunks: 2, Chunks in use: 2. 2.85MiB allocated for chunks. 2.85MiB in use in bin. 2.85MiB client-requested in use in bin.
2021-09-22 12:04:35.644762: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): 	Total Chunks: 3, Chunks in use: 2. 7.27MiB allocated for chunks. 4.69MiB in use in bin. 4.69MiB client-requested in use in bin.
2021-09-22 12:04:35.644788: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-22 12:04:35.644795: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-22 12:04:35.644801: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): 	Total Chunks: 2, Chunks in use: 2. 53.47MiB allocated for chunks. 53.47MiB in use in bin. 53.47MiB client-requested in use in bin.
2021-09-22 12:04:35.644807: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-22 12:04:35.644813: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): 	Total Chunks: 1, Chunks in use: 0. 80.21MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-22 12:04:35.644821: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-22 12:04:35.644828: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): 	Total Chunks: 3, Chunks in use: 1. 22.13GiB allocated for chunks. 17.62GiB in use in bin. 17.62GiB client-requested in use in bin.
2021-09-22 12:04:35.644837: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 4.90GiB was 256.00MiB, Chunk State: 
2021-09-22 12:04:35.644863: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 748.21MiB | Requested Size: 80.21MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.42MiB | Requested Size: 1.42MiB | in_use: 1 | bin_num: -1, next:   Size: 1.42MiB | Requested Size: 1.42MiB | in_use: 1 | bin_num: -1
2021-09-22 12:04:35.644874: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 3.78GiB | Requested Size: 121.76MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.42MiB | Requested Size: 1.42MiB | in_use: 1 | bin_num: -1
2021-09-22 12:04:35.644879: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 23913299968
2021-09-22 12:04:35.644892: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8016000000 of size 18924364800 next 41
2021-09-22 12:04:35.644897: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f847dfae400 of size 28036096 next 79
2021-09-22 12:04:35.644902: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f847fa6b000 of size 28036096 next 80
2021-09-22 12:04:35.644908: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8481527c00 of size 2461184 next 81
2021-09-22 12:04:35.644913: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8481780a00 of size 2461184 next 82
2021-09-22 12:04:35.644919: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84819d9800 of size 256 next 86
2021-09-22 12:04:35.644924: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84819d9900 of size 256 next 87
2021-09-22 12:04:35.644929: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f84819d9a00 of size 84108288 next 60
2021-09-22 12:04:35.644943: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f8486a0fe00 of size 1492992 next 149
2021-09-22 12:04:35.644950: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f8486b7c600 of size 784551936 next 127
2021-09-22 12:04:35.644955: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f84b57b1600 of size 1492992 next 144
2021-09-22 12:04:35.644960: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f84b591de00 of size 4056293888 next 18446744073709551615
2021-09-22 12:04:35.644964: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2097152
2021-09-22 12:04:35.644969: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00000 of size 256 next 1
2021-09-22 12:04:35.644974: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00100 of size 1280 next 2
2021-09-22 12:04:35.644978: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00600 of size 256 next 3
2021-09-22 12:04:35.644982: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446a00700 of size 256 next 4
2021-09-22 12:04:35.644987: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00800 of size 256 next 5
2021-09-22 12:04:35.644991: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00900 of size 256 next 6
2021-09-22 12:04:35.644995: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00a00 of size 256 next 7
2021-09-22 12:04:35.645000: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00b00 of size 256 next 8
2021-09-22 12:04:35.645008: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00c00 of size 256 next 9
2021-09-22 12:04:35.645013: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00d00 of size 256 next 140
2021-09-22 12:04:35.645018: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446a00e00 of size 256 next 154
2021-09-22 12:04:35.645024: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a00f00 of size 256 next 14
2021-09-22 12:04:35.645029: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a01000 of size 256 next 15
2021-09-22 12:04:35.645033: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446a01100 of size 256 next 24
2021-09-22 12:04:35.645038: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a01200 of size 256 next 161
2021-09-22 12:04:35.645042: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a01300 of size 256 next 20
2021-09-22 12:04:35.645047: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a01400 of size 256 next 21
2021-09-22 12:04:35.645052: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a01500 of size 221184 next 115
2021-09-22 12:04:35.645056: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446a37500 of size 256 next 135
2021-09-22 12:04:35.645061: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446a37600 of size 860672 next 162
2021-09-22 12:04:35.645067: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446b09800 of size 4608 next 150
2021-09-22 12:04:35.645073: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446b0aa00 of size 464896 next 110
2021-09-22 12:04:35.645078: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446b7c200 of size 8192 next 157
2021-09-22 12:04:35.645083: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446b7e200 of size 256 next 12
2021-09-22 12:04:35.645087: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446b7e300 of size 12032 next 102
2021-09-22 12:04:35.645092: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446b81200 of size 330240 next 143
2021-09-22 12:04:35.645096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446bd1c00 of size 256 next 145
2021-09-22 12:04:35.645101: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446bd1d00 of size 256 next 95
2021-09-22 12:04:35.645105: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446bd1e00 of size 256 next 105
2021-09-22 12:04:35.645110: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446bd1f00 of size 188672 next 18446744073709551615
2021-09-22 12:04:35.645123: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4194304
2021-09-22 12:04:35.645129: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c00000 of size 256 next 23
2021-09-22 12:04:35.645133: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00100 of size 256 next 131
2021-09-22 12:04:35.645138: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00200 of size 256 next 26
2021-09-22 12:04:35.645144: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00300 of size 256 next 27
2021-09-22 12:04:35.645149: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00400 of size 256 next 28
2021-09-22 12:04:35.645153: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00500 of size 256 next 31
2021-09-22 12:04:35.645158: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c00600 of size 256 next 32
2021-09-22 12:04:35.645162: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00700 of size 256 next 34
2021-09-22 12:04:35.645166: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00800 of size 256 next 35
2021-09-22 12:04:35.645172: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00900 of size 256 next 36
2021-09-22 12:04:35.645178: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00a00 of size 256 next 37
2021-09-22 12:04:35.645183: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00b00 of size 256 next 38
2021-09-22 12:04:35.645187: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00c00 of size 256 next 39
2021-09-22 12:04:35.645192: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00d00 of size 256 next 43
2021-09-22 12:04:35.645196: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00e00 of size 256 next 44
2021-09-22 12:04:35.645201: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c00f00 of size 256 next 45
2021-09-22 12:04:35.645205: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01000 of size 256 next 46
2021-09-22 12:04:35.645209: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01100 of size 256 next 47
2021-09-22 12:04:35.645214: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01200 of size 256 next 48
2021-09-22 12:04:35.645218: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01300 of size 256 next 49
2021-09-22 12:04:35.645223: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01400 of size 256 next 148
2021-09-22 12:04:35.645230: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01500 of size 256 next 13
2021-09-22 12:04:35.645234: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01600 of size 256 next 57
2021-09-22 12:04:35.645239: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01700 of size 256 next 51
2021-09-22 12:04:35.645243: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01800 of size 256 next 58
2021-09-22 12:04:35.645247: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01900 of size 256 next 98
2021-09-22 12:04:35.645252: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01a00 of size 256 next 106
2021-09-22 12:04:35.645256: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c01b00 of size 256 next 50
2021-09-22 12:04:35.645260: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01c00 of size 256 next 107
2021-09-22 12:04:35.645265: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01d00 of size 256 next 124
2021-09-22 12:04:35.645270: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01e00 of size 256 next 138
2021-09-22 12:04:35.645274: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c01f00 of size 256 next 158
2021-09-22 12:04:35.645278: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02000 of size 256 next 112
2021-09-22 12:04:35.645282: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02100 of size 256 next 141
2021-09-22 12:04:35.645288: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02200 of size 256 next 66
2021-09-22 12:04:35.645293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02300 of size 256 next 67
2021-09-22 12:04:35.645297: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02400 of size 256 next 68
2021-09-22 12:04:35.645301: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02500 of size 256 next 33
2021-09-22 12:04:35.645306: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c02600 of size 768 next 92
2021-09-22 12:04:35.645310: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02900 of size 256 next 101
2021-09-22 12:04:35.645314: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02a00 of size 256 next 62
2021-09-22 12:04:35.645321: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02b00 of size 256 next 103
2021-09-22 12:04:35.645326: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02c00 of size 256 next 54
2021-09-22 12:04:35.645330: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c02d00 of size 256 next 137
2021-09-22 12:04:35.645337: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c02e00 of size 256 next 97
2021-09-22 12:04:35.645341: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c02f00 of size 13568 next 30
2021-09-22 12:04:35.645347: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c06400 of size 28928 next 42
2021-09-22 12:04:35.645351: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c0d500 of size 14336 next 120
2021-09-22 12:04:35.645356: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c10d00 of size 256 next 117
2021-09-22 12:04:35.645361: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c10e00 of size 1536 next 146
2021-09-22 12:04:35.645366: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c11400 of size 256 next 10
2021-09-22 12:04:35.645370: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c11500 of size 256 next 152
2021-09-22 12:04:35.645375: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c11600 of size 140288 next 91
2021-09-22 12:04:35.645380: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c33a00 of size 512 next 25
2021-09-22 12:04:35.645384: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c33c00 of size 256 next 53
2021-09-22 12:04:35.645389: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c33d00 of size 256 next 130
2021-09-22 12:04:35.645394: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446c33e00 of size 7936 next 22
2021-09-22 12:04:35.645398: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c35d00 of size 4096 next 56
2021-09-22 12:04:35.645403: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c36d00 of size 230144 next 63
2021-09-22 12:04:35.645408: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f000 of size 256 next 69
2021-09-22 12:04:35.645415: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f100 of size 256 next 70
2021-09-22 12:04:35.645420: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f200 of size 256 next 71
2021-09-22 12:04:35.645427: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f300 of size 256 next 72
2021-09-22 12:04:35.645432: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f400 of size 256 next 73
2021-09-22 12:04:35.645437: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f500 of size 256 next 74
2021-09-22 12:04:35.645441: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f600 of size 256 next 75
2021-09-22 12:04:35.645446: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f700 of size 256 next 76
2021-09-22 12:04:35.645451: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f800 of size 256 next 77
2021-09-22 12:04:35.645455: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6f900 of size 256 next 78
2021-09-22 12:04:35.645460: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446c6fa00 of size 228096 next 83
2021-09-22 12:04:35.645465: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446ca7500 of size 228096 next 84
2021-09-22 12:04:35.645469: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446cdf000 of size 253440 next 85
2021-09-22 12:04:35.645474: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f9446d1ce00 of size 326144 next 17
2021-09-22 12:04:35.645481: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f9446d6c800 of size 2701312 next 18446744073709551615
2021-09-22 12:04:35.645487: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: 
2021-09-22 12:04:35.645495: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 73 Chunks of size 256 totalling 18.2KiB
2021-09-22 12:04:35.645500: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB
2021-09-22 12:04:35.645505: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4096 totalling 4.0KiB
2021-09-22 12:04:35.645510: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4608 totalling 4.5KiB
2021-09-22 12:04:35.645515: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 8192 totalling 8.0KiB
2021-09-22 12:04:35.645520: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 12032 totalling 11.8KiB
2021-09-22 12:04:35.645524: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 28928 totalling 28.2KiB
2021-09-22 12:04:35.645532: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 140288 totalling 137.0KiB
2021-09-22 12:04:35.645536: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 221184 totalling 216.0KiB
2021-09-22 12:04:35.645541: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 228096 totalling 445.5KiB
2021-09-22 12:04:35.645546: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 230144 totalling 224.8KiB
2021-09-22 12:04:35.645551: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 253440 totalling 247.5KiB
2021-09-22 12:04:35.645555: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 326144 totalling 318.5KiB
2021-09-22 12:04:35.645560: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 330240 totalling 322.5KiB
2021-09-22 12:04:35.645564: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1492992 totalling 2.85MiB
2021-09-22 12:04:35.645569: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 2461184 totalling 4.69MiB
2021-09-22 12:04:35.645574: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 28036096 totalling 53.47MiB
2021-09-22 12:04:35.645579: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 18924364800 totalling 17.62GiB
2021-09-22 12:04:35.645584: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 17.69GiB
2021-09-22 12:04:35.645588: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 23919591424 memory_limit_: 23919591424 available bytes: 0 curr_region_allocation_bytes_: 34359738368
2021-09-22 12:04:35.645598: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: 
Limit:                     23919591424
InUse:                     18990380800
MaxInUse:                  21956931328
NumAllocs:                       82937
MaxAllocSize:              18924364800
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2021-09-22 12:04:35.645614: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ********************************************************************************___*_______________*
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
/tmp/ipykernel_24845/1230787370.py in <module>
---> 16 early_history = model.fit(X_train,y_train,validation_data=(X_test,y_test),
     17                     epochs=EPOCHS,initial_epoch=start_step, verbose=1,batch_size=32,

~/data/python_tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~/data/python_tensorflow_env/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
    104       dtype = dtypes.as_dtype(dtype).as_datatype_enum
    105   ctx.ensure_initialized()
--> 106   return ops.EagerTensor(value, ctx.device_name, dtype)
    107 
    108 

InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
"
52085,Potential use-after-free bug in function `EagerTensor_shape_tuple` by using an object after all its references released (a static analyzer report),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): faad219f
- Python version: 3.8.5

**Static analysis results, no POC available.**
This static analysis report has been manually reviewed to verify its validity.

**Describe the current behavior**

The path provided by the static analyzer is as follows.

1. A new reference is returned from `PyLong_FromLongLong` and pointed to by `dim`.
https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/eager/pywrap_tensor.cc#L605

2. A reference is stolen by function `PyTuple_SetItem`. Assume error occurs, take the true branch (line 608~609).
https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/eager/pywrap_tensor.cc#L609

3. Use after free in macro `Py_DECREF`.
https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/eager/pywrap_tensor.cc#L618

Python API function `PyTuple_SetItem` will always steal a reference for the item set, and it will decrease the refcnt on error. Therefore, the refcnt will be first decreased in this API function, then decreased again on line 618.

**Contributing**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

A potential correct fix can be removing the following line.
https://github.com/tensorflow/tensorflow/blob/faad219fc46032a0ae9576ccc3076612cc1f5f72/tensorflow/python/eager/pywrap_tensor.cc#L618
"
52083,Bug,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


"
52079,make error in Tensorflow Lite Micro,"**System information**
- Linux Ubuntu 20.04
- Python version: 3.8.10


**The problem**
I'm trying to generate the example projects from the tensorflow C++ library with Make and I run in this error:

/bin/sh: 1: python: not found
make: *** [tensorflow/lite/micro/examples/network_tester/Makefile.inc:52 tensorflow/lite/micro/tools/make/gen/linux_x86_64default/prj/network_tester_test/keil/keil_project.uvprojx] Error 127

**Sequence of commands**
git clone https://github.com/tensorflow/tflite-micro
cd tflite-micro
make -f tensorflow/lite/micro/tools/make/Makefile generate_projects
"
52077,[PluggableDevice] C-API Symbols not exposed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pypi)
- TensorFlow version: 2.6.0.dev20210612 
- Python version: 3.7.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

Hello,

I’m getting started with building a plugin using the new PluggableDeviceAPI, and am hitting a few snags. Using [this example](https://github.com/jzhoulon/community/tree/plugin_tutorial/rfcs/20200624-pluggable-device-for-tensorflow/sample) as my starting point, my current goal is to have my own custom optimize callback function that walks the NodeDef's and prints the shapes of each of the Input/Output Tensor(s).

Version of tensorflow I’m using:
tf-nightly == 2.6.0.dev20210612

This problem reproduced on the latest tf-nightly for me too.

The c_api.cc symbols and the grappler.cc symbols are not exposed in the monolithic libtensorflow_framework.so. The header files are part of the tensorflow wheel distribution, but we get linker errors for things calling functions like TF_NewBuffer when using @local_config_tf//:tf_header_lib as a dep. When running nm -gDC libtensorflow_framework.so.2 it’s clear that these symbols aren’t exposed. Are there any known workarounds for this?

Also, what is the recommended way to use the C-API _with_ `libtensorflow_framework.so`? When using the `@local_config_tf` provided in the BUILD files at the example linked above, I get duplicate registries / static global's when trying to link `libtensorflow.so` _with_ `libtensorflow_framework.so` at runtime. 

Cross post from [the Tensorflow forum](https://discuss.tensorflow.org/t/pluggabledeviceapi-c-api-symbols-not-exposed/3662) due to inactivity.

Thanks in advance!
"
52076,Python: XNNPack delegate performance with num_threads is slower than normal tflite without any delagte,"Hi,

For the python-based yolov4 (tflite fp32 model) detection pipeline observed the performance is slower with xnn pack delegate (with num threads options )compared to tflite without any delegate

**System Information**

- Ubuntu 20.04
- Intel i5 8th gen
- Installed TF from source
- TF version 2.6
- Python version 3.9
- Bazel version 3.7.2
- GCC 9.3.0 

With reference to the previous issue #42277 I tried to use setNumThreads, but for python, I couldn't find the function. I have also set num threads in `tflite.interpreter (model, num_threads)`  API. With that num threads, I didn't observe performance improvement. I could also see only one thread being utilized in my system

So to use all the threads in my machine I hardcoded the thread value in the ""tflite_with_xnnpack_optional.cc"" file

`  opts.num_threads = 8;//num_threads > 1 ? num_threads : 0;  (hard coded it to 8)`

With this change for my python pipeline, I could see a 30% improvement in performance 

Kindly redirect me to the API to enable the num threads option for python ( similar to c++)

The command I used to compile TF from source
`bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both -k --define tflite_with_xnnpack=true //tensorflow/tools/pip_package:build_pip_package 
`

I have also ensured that my tflite model has delegate kernels support by running in benchmark tool (13 delegate kernels)
"
52073,Can't install tensorflow 1.8 on windows 10," python --version --> `Python 3.8.8`
 pip --version --> `pip 21.0.1 from C:\Python\anaconda3\lib\site-packages\pip (python 3.8)`

I typed `pip install tensorflow==1.8`
but it's giving me an error

```
ERROR: Could not find a version that satisfies the requirement tensorflow==1.8
ERROR: No matching distribution found for tensorflow==1.8
```

there is no more tensorflow 1.8 ?
How can I install tensorflow 1.8 ?"
52072,Oks,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52071,Tensor flow,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52070,AttributeError: module 'tensorflow_datasets' has no attribute 'deprecated',"I'm using tensorflow Version: 2.3.0
and python 3.8.5"
52069,bazel test //tensorflow/tools/docs:tf_doctest fails on aarch64,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git HEAD
- Python version: 3.8.10
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Test fails

**Describe the expected behavior**

Test passes

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): I think the test needs to be relaxed slightly to accept the values produced by AARCH64 CPUs.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

bazel test //tensorflow/tools/docs:tf_doctest

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

======================================================================
FAIL: Tanh (tensorflow.python.ops.gen_math_ops)
Doctest: tensorflow.python.ops.gen_math_ops.Tanh
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/python3.8/doctest.py"", line 2204, in runTest
    raise self.failureException(self.format_failure(new.getvalue()))
AssertionError: Failed doctest test for tensorflow.python.ops.gen_math_ops.Tanh
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py"", line 408, in Tanh

----------------------------------------------------------------------
File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py"", line 416, in tensorflow.python.ops.gen_math_ops.Tanh
Failed example:
    tf.math.tanh(x)
Expected:
    <tf.Tensor: shape=(8,), dtype=float32, numpy=
    array([-1.        , -0.99990916, -0.46211717,  0.7615942 ,  0.8336547 ,
            0.9640276 ,  0.9950547 ,  1.        ], dtype=float32)>
Got:
    <tf.Tensor: shape=(8,), dtype=float32, numpy=
    array([-0.99999976, -0.99990916, -0.46211717,  0.7615942 ,  0.8336546 ,
            0.9640276 ,  0.9950547 ,  0.99999976], dtype=float32)>



    #############################################################
    Check the documentation
    (https://www.tensorflow.org/community/contribute/docs_ref) on how to write testable docstrings.
    #############################################################

======================================================================
FAIL: tanh (tensorflow.python.ops.gen_math_ops)
Doctest: tensorflow.python.ops.gen_math_ops.tanh
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/python3.8/doctest.py"", line 2204, in runTest
    raise self.failureException(self.format_failure(new.getvalue()))
AssertionError: Failed doctest test for tensorflow.python.ops.gen_math_ops.tanh
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py"", line 11336, in tanh

----------------------------------------------------------------------
File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py"", line 11344, in tensorflow.python.ops.gen_math_ops.tanh
Failed example:
    tf.math.tanh(x)
Expected:
    <tf.Tensor: shape=(8,), dtype=float32, numpy=
    array([-1.        , -0.99990916, -0.46211717,  0.7615942 ,  0.8336547 ,
            0.9640276 ,  0.9950547 ,  1.        ], dtype=float32)>
Got:
    <tf.Tensor: shape=(8,), dtype=float32, numpy=
    array([-0.99999976, -0.99990916, -0.46211717,  0.7615942 ,  0.8336546 ,
            0.9640276 ,  0.9950547 ,  0.99999976], dtype=float32)>



    #############################################################
    Check the documentation
    (https://www.tensorflow.org/community/contribute/docs_ref) on how to write testable docstrings.
    #############################################################

======================================================================
FAIL: sigmoid (tensorflow.python.ops.math_ops)
Doctest: tensorflow.python.ops.math_ops.sigmoid
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/python3.8/doctest.py"", line 2204, in runTest
    raise self.failureException(self.format_failure(new.getvalue()))
AssertionError: Failed doctest test for tensorflow.python.ops.math_ops.sigmoid
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/math_ops.py"", line 174, in sigmoid

----------------------------------------------------------------------
File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/math_ops.py"", line 187, in tensorflow.python.ops.math_ops.sigmoid
Failed example:
    tf.math.sigmoid(x)
Expected:
    <tf.Tensor: shape=(4,), dtype=float32,
    numpy=array([0.5      , 0.7310586, 1.       , 1.       ], dtype=float32)>
Got:
    <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.5      , 0.7310586, 0.9999998, 0.9999998], dtype=float32)>

"
52068,Loss output gives Nan in Linux but gives normal values in Windows ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux & Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): GCC 7.5.0
- CUDA/cuDNN version:10.1
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I defined a model (two inputs and two outputs) that trains well on my local PC (Windows 10), but only produce Nan in Linux
**Describe the expected behavior**
The model should be trained properly in both Windows and Linux

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
[slurm-8183898.txt](https://github.com/tensorflow/tensorflow/files/7195723/slurm-8183898.txt)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52067,Memory Consumption | Boosting model,"<em> This is an issue related to performance of [TensorFlow.BoostedTree](https://www.tensorflow.org/tutorials/estimator/boosted_trees)</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): '2.4.1'
- Python version:  3.6.5

**Describe the model**

I am training the Boosted Tree of the TensorFlow to review the memory consumption of the model by using the tracemalloc.
This model is based on the Gradient Boosting and XGBoost idea.

**Describe the current behavior**

The trend of the model is not growing linearly and has too much noise in terms of memory consumption.

**Describe the expected behavior**

As the XGboost demonstrates, it should grow linearly.

**Possible cause**
I think there is something inside tf.estimator that causes this behavior, if it is so, would you please indicate the exact reason?

**Standalone code to reproduce the issue**
I provided a notebook over a UCI Dataset in this [link](https://github.com/samanemami/TFBoostedTree/blob/main/examples/memory_consumption_tensorflow.ipynb).
Which shows the memory consumption of the model.
"
52066,SyntaxError: positional argument follows keyword argument,"```
 base_model = tf.keras.applications.MobileNetV2(input_shape=( 160, 160, 3),
                                                   include_top=False, # <== Important!!!!
                                                   weights='imagenet') # From imageNet
    
    # freeze the base model by making it non trainable
    base_model.trainable = False 

    # create the input layer (Same as the imageNetv2 input size)
    inputs = tf.keras.Input(shape=160,160,3) 
    
    # apply data augmentation to the inputs
    x = data_augmentation(inputs)
    
    # data preprocessing using the same weights the model was trained on
    x = preprocess_input('imagenet') 
    
    # set training to False to avoid keeping track of statistics in the batch norm layer
    x = base_model(x, training=False) 
    
    # add the new Binary classification layers
    # use global avg pooling to summarize the info in each channel
    x = keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Dropout(0.2)(x) 
    # include dropout with probability of 0.2 to avoid overfitting
    
        
    # use a prediction layer with one neuron (as a binary classifier only needs one)
    outputs = keras.layers.Dense(1)(x)

```
![4 1](https://user-images.githubusercontent.com/26819449/134000933-2fcf9fdb-cdd6-4663-a5cb-b76f5691ba04.JPG)
![4 2](https://user-images.githubusercontent.com/26819449/134000944-17c2f7be-99b9-492c-ae3e-e55bea84537f.JPG)

This is from documentation:
![image](https://user-images.githubusercontent.com/26819449/134001112-2940bf65-c4fb-4546-a737-2e70986faacf.png)
 Is this is a bug ??

Thanks!


"
52065,Issue with weight clustering API -Error : Tried to expand dim index 4 for tensor with 3 dimensions. [Op:ExpandDims],"This template is for miscellaneous issues not covered by the other issue categories.

https://colab.research.google.com/drive/1ID38NAluH0FA0d4G2kSaKZo4C5IinHlh#scrollTo=PzHWGU72831C 

Hi , Please use the above link for the reference code.
I did some experiments with weight clustering method . I could able to do with some layers like Dense, Conv2D. 
But , my model consists of Conv1d layers and one use defined custom layer . In this case is it possible to do weight clustering method for this model ?
I got the below mentioned error , while I tried some sample model consists are conv1d layer. How to resolve this error.

InvalidArgumentError: Tried to expand dim index 4 for tensor with 3 dimensions. [Op:ExpandDims]

Few more questions I'm having : 
1)Does weight clustering supported by tf.keras.layers.Conv1D layer or not ?
if , by default it is not supported , then How can I do it .
2)  (I model consists conv1d layers and Conv1DTranspose layer) Shall I use weight clustering method for my model , what are the blocking things to do it . 
Can you anyone give some ideas to optimize this kind of model ."
52064,TF Lite `Conv1D` conversion bug,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.4 and Ubuntu 18.04 LTS on Colab machine
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.3.0 and 2.6.0

### 2. Code

This notebook demonstrates the bug with the simplest example I came up with.

https://colab.research.google.com/gist/ebraraktas/69995d036a35a8d7f744c845a163863e

You can set `use_tf_2_3 = True` to see that it runs on tensorflow `2.3.0` with no error.

### 3. Failure after conversion

As you can see in the Colab gist, model can be converted to TF Lite model successfully, but different tensorflow versions creates different graphs (see [Netron](https://github.com/lutzroeder/netron) outputs below). And tensorflow lite versions >= 2.4.0 fail to reshape and allocate tensors. See runtime error below:

```
RuntimeError: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (25000 != 0)Node number 2 (RESHAPE) failed to prepare.
```

| TensorFlow 2.3.0 | TensorFlow 2.6.0 |
| --- | --- |
| ![model_2 3 0](https://user-images.githubusercontent.com/62459770/133984264-8e2149f3-df82-4b3b-9cfe-1a5b4b2dabf5.png) | ![model_2 6 0](https://user-images.githubusercontent.com/62459770/133984266-4aac1db5-99df-4d7c-b5cf-7bde923971a8.png) |
 "
52063,tf.math.unsorted_segment_sum silently output wrong result on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0 / 2.7.0
- Python version: 3.6.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
When `segment_ids` exceeds the range `[0, num_segments)`, `tf.math.unsorted_segment_sum` did not throw any exception, and outputs a zero-dimension tensor on GPU. `tf.math.unsorted_segment_sum` can throw an Exception in this case on CPU.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
data = tf.random.uniform([3, 80], dtype=tf.float32)
segment_ids = [1, 0, 1]
num_segments = 0

try:
  with tf.device('/GPU:0'):
    res_GPU = tf.math.unsorted_segment_sum(data=data,segment_ids=segment_ids,num_segments=num_segments,)
    print(""res_GPU: "", res_GPU)
except Exception as e_gpu:
  print('error on gpu', e_gpu)
    
try:
  with tf.device('/CPU'):
    res_CPU = tf.math.unsorted_segment_sum(data=data,segment_ids=segment_ids,num_segments=num_segments,)
    print(""res_CPU: "", res_CPU)
except Exception as e_cpu:
  print('error on cpu:', e_cpu)

```
outputs:
```
res_GPU:  tf.Tensor([], shape=(0, 80), dtype=float32)
error on cpu: segment_ids[0] = 1 is out of range [0, 0) [Op:UnsortedSegmentSum]
```
"
52062,NotFoundError: NewRandomAccessFile failed to Create/Open: images/labelmap.pbtxt : The system cannot find the file specified; No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Install-TensorFlow-Python 3.7 file from github repository given by the tutorial below.
- TensorFlow version: 2.3
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4
- GPU model and memory: 



**Describe the problem**
OK, so I followed this tutorial to the T: https://www.youtube.com/watch?v=a1br6gW-8Ss&t=157s. And then I get this random error. Everything was fine, I used labelImg, I generated the tfrecords, I downloaded all the software, I organized the files and extracted all the files. I'm using an up to date version of tensorflow. And the images/labelmap.pbtxt exists in the images folder (maniacal laughter). Why can't it find this file? I added a path, that still doesn't work.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
python model_main_tf2.py --pipeline_config_path=training/ssd_efficientdet_d0_512x512_coco17_tpu-8.config --model_dir=training --alsologtostderr.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

![Screen Shot 2021-09-18 at 8 18 19 PM](https://user-images.githubusercontent.com/69585784/133936283-762c2bfa-dc92-4cc7-a8ea-b86b42942b95.png)
![Screen Shot 2021-09-19 at 1 07 55 PM](https://user-images.githubusercontent.com/69585784/133936400-d9c7b782-9a8f-4000-bdf0-e01096105dd6.png)

"
52061,GPU delegate problems in tensorflow-lite in android studio,"hello everyone! I'm new in the community so please bear with me. I would like to get some help with my problem that I've been stuck in for a while now. I get an error when I run my app in android studio and through my mobile phone. I am following ElectroCode's tutorial videos in making sign language recognition app on youtube.

This is my error log:

		E/AndroidRuntime: FATAL EXCEPTION: Thread-4
		    Process: com.example.imagepro, PID: 7161
		    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Next operations are not supported by GPU delegate:
		    CUSTOM TFLite_Detection_PostProcess: Operation is not supported.
		    First 98 operations will run on the GPU, and the remaining 1 on the CPU.
		    OpenCL library not loaded - dlopen failed: library ""libOpenCL-pixel.so"" not found
		    Falling back to OpenGL
		    TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size.
		       Node number 99 (TfLiteGpuDelegateV2) failed to invoke.
    
	        	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
	        	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:163)
	        	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:360)
	        	at com.example.imagepro.objectDetectorClass.recognizeImage(objectDetectorClass.java:159)
	        	at com.example.imagepro.CameraActivity.onCameraFrame(CameraActivity.java:130)
		        at org.opencv.android.CameraBridgeViewBase.deliverAndDrawFrame(CameraBridgeViewBase.java:392)
		        at org.opencv.android.JavaCameraView$CameraWorker.run(JavaCameraView.java:373)
	 	       at java.lang.Thread.run(Thread.java:929)
		D/CameraBridge: call checkCurrentState
"
52060,"ValueError: Shapes (None, 8) and (None, 7) are incompatible","ValueError: in user code:

    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:835 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:789 train_step
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py:201 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    /usr/local/lib/python3.7/dist-packages/keras/losses.py:141 __call__
        losses = call_fn(y_true, y_pred)
    /usr/local/lib/python3.7/dist-packages/keras/losses.py:245 call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/keras/losses.py:1666 categorical_crossentropy
        y_true, y_pred, from_logits=from_logits, axis=axis)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/keras/backend.py:4839 categorical_crossentropy
        target.shape.assert_is_compatible_with(output.shape)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:1161 assert_is_compatible_with
        raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))

    ValueError: Shapes (None, 8) and (None, 7) are incompatible

Please check the code in 
[link](https://colab.research.google.com/drive/1WHp2T_jOjtXUHeBhiYR3zaeumtXtSDwn#scrollTo=1rsivhQNf36Y&uniqifier=3)"
52059,keras load_model not working,"1. Tensorflow Version 2.4.0
2. Ubuntu 20.04
3. Python 3.8.5

I have two projects. Use the same training code. One is for one gpu training (Project A), the other one is for  two gpus training (Project B). And these two projects in different paths.

When I use Project A get trained model   C   in Project B path for prediction, Most of  the results is simillar . But When use  model C from  Project A  in Project A, It works fine. I'm Sure that the model file is the same by md5 code.

Project A save model code is about like this:

```
model = create_mdoel()
......
training task
......
model.save(""c.h5"")
```
and the results is :



Project B load model code is about like this:
```
from tensorflow.keras.models import load_model
model = load_model(""c.h5"")
```

I use different images ,but got the simillar prediction.
```
[[1.01394113e-02 2.92072829e-04 4.19789851e-02 1.22826849e-03
  9.46218133e-01 1.30874541e-04 1.23131176e-05]

 [1.06976507e-02 3.13456461e-04 4.33462970e-02 1.27648050e-03
  9.44213808e-01 1.38900723e-04 1.34185420e-05]

 [1.01642376e-02 2.94585945e-04 4.19760570e-02 1.23549451e-03
  9.46185470e-01 1.31785186e-04 1.24300368e-05]

 [1.05263414e-02 3.09864059e-04 4.28950936e-02 1.27590902e-03
  9.44842041e-01 1.37657087e-04 1.31396355e-05]

 [1.04539478e-02 3.05163179e-04 4.27655652e-02 1.25962391e-03
  9.45066750e-01 1.35915790e-04 1.30421986e-05]

 [1.04103265e-02 3.03133012e-04 4.26876061e-02 1.25538185e-03
  9.45195496e-01 1.35144786e-04 1.29277996e-05]

 [1.06208669e-02 3.10785865e-04 4.33004946e-02 1.27685803e-03
  9.44339037e-01 1.38675998e-04 1.33183985e-05]

 [1.04063889e-02 3.03731475e-04 4.25971374e-02 1.25621166e-03
  9.45288241e-01 1.35323397e-04 1.29246901e-05]]
```


What's more, When I changed checkpoint directory name in the project A, The model trained by project A is not working in Project A.
If I reset checkpoint directory name to before , The checkpoint weights works fine again in project A ! 





"
52058,Issue with tensorflow lite interpreter ,"`import tensorflow as tf
import tensorflow_datasets as tfds
(ds_train, ds_test), ds_info = tfds.load( 'mnist',
split=['train', 'test'],
shuffle_files=True,
as_supervised=True,
with_info=True,
)
def normalize_img(image, label):
return tf.cast(image, tf.float32) / 255., label

ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)
ds_train = ds_train.batch(128)
ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)
ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)

ds_test = ds_test.batch(128)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)

#model
model = tf.keras.models.Sequential([
tf.keras.layers.Flatten(input_shape=(28, 28)),
tf.keras.layers.Reshape((1,784)),
tf.keras.layers.Conv1D( 16, 1,use_bias=False,activation = 'relu' ),
tf.keras.layers.Conv1D(16, 1,use_bias=False ,activation = 'relu' ),
tf.keras.layers.Conv1D( 16, kernel_size=3, strides=1, padding= ""causal"", dilation_rate=2**0, groups=16,use_bias=False,activation ='relu'),
tf.keras.layers.Conv1D( 16, 1 ,use_bias=False) ,
tf.keras.layers.Flatten(),
tf.keras.layers.Dense(10)
])
model.compile(
optimizer=tf.keras.optimizers.Adam(0.001),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

model.fit(
ds_train,
epochs=1,
validation_data=ds_test,
)
model.summary()
model.save(""./model.h5"")

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open('model.tflite', 'wb') as f:
f.write(tflite_model)
`

This the code of my model , after executing this code I'm getting .tflite model . 
But when I'm trying to interpret the tflite model I'm getting the error. 
`interpreter = tf.lite.Interpreter(model_path=""./assignment.tflite"")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
`
This is the tflite inference code I'm using 

2021-09-18 20:55:19.573161: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
  File ""C:\Users\itmee\anaconda3\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\Users\itmee\anaconda3\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""c:\Users\itmee\.vscode\extensions\ms-python.python-2021.9.1230869389\pythonFiles\lib\python\debugpy\__main__.py"", line 45, in <module>
    cli.main()
  File ""c:\Users\itmee\.vscode\extensions\ms-python.python-2021.9.1230869389\pythonFiles\lib\python\debugpy/..\debugpy\server\cli.py"", line 444, in main
    run()
  File ""c:\Users\itmee\.vscode\extensions\ms-python.python-2021.9.1230869389\pythonFiles\lib\python\debugpy/..\debugpy\server\cli.py"", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str(""__main__""))
  File ""C:\Users\itmee\anaconda3\lib\runpy.py"", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""C:\Users\itmee\anaconda3\lib\runpy.py"", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""C:\Users\itmee\anaconda3\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""d:\MVNS\assignment\tf_Asg-1.py"", line 17, in <module>
    interpreter.invoke()
  File ""C:\Users\itmee\anaconda3\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 833, in invoke
    self._interpreter.Invoke()
RuntimeError: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (16 != 1)Node number 13 (CONV_2D) failed to prepare.
 Error traceback details are as mentioned above. 

"
52057,"ValueError: ('Input has undefined rank:', TensorShape(None))","```
def identity_block(X, f, filters, training=True, initializer=random_uniform):
    """"""
    Implementation of the identity block as defined in Figure 4
    
    Arguments:
    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)
    f -- integer, specifying the shape of the middle CONV's window for the main path
    filters -- python list of integers, defining the number of filters in the CONV layers of the main path
    training -- True: Behave in training mode
                False: Behave in inference mode
    initializer -- to set up the initial weights of a layer. Equals to random uniform initializer
    
    Returns:
    X -- output of the identity block, tensor of shape (m, n_H, n_W, n_C)
    """"""
    
    # Retrieve Filters
    F1, F2, F3 = filters
    
    # Save the input value. You'll need this later to add back to the main path. 
    X_shortcut = X
    
    # First component of main path
    X = Conv2D(filters = F1, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3)(X, training = training) # Default axis
    X = Activation('relu')(X)
    
    ### START CODE HERE
    ## Second component of main path (≈3 lines)
    X = Conv2D(filters= F1,kernel_size= f ,strides=(1,1),padding='same',kernel_initializer=glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis=3)(X,training=training)
    X = Activation('relu')(X)

    ## Third component of main path (≈2 lines)
    X = Conv2D(filters=F3,kernel_size= 1,strides=(1,1),padding='valid',kernel_initializer=glorot_uniform(seed=0))
    X = BatchNormalization(axis=3)(X,training=training)
    
    
    ## Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)
    X = Add()([X,X_shortcut])
    X = Activation('relu')(X) 
    ### END CODE HERE

    return X


```
Why I am getting this error,I checked all the docs of tfs.
![3](https://user-images.githubusercontent.com/26819449/133884934-1a3c2cfe-a763-4279-a48b-9ec5e7a18a74.JPG)
"
52055,Autograph could not transform ... will run it as is,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.5.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MacBook Air 2020
- TensorFlow installed from (source or binary): These instructions: https://towardsdatascience.com/installing-tensorflow-on-the-m1-mac-410bb36b776
- 
(tf installed from these 2 links)

tensorflow_addons_macos-0.1a3-cp38-cp38-macosx_11_0_arm64.whl — https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha3/tensorflow_addons_macos-0.1a3-cp38-cp38-macosx_11_0_arm64.whl

tensorflow_macos-0.1a3-cp38-cp38-macosx_11_0_arm64.whl — https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha3/tensorflow_macos-0.1a3-cp38-cp38-macosx_11_0_arm64.whl

- TensorFlow version (use command below): tf_macos-v0.1-alpha2-AS-67-gf3595294ab 2.4.0-rc0
- Python version: 3.8.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: M1 7 cores

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I can not use my model to predict values and also get that warning when I fit, evaluate, and train.
**Describe the expected behavior**
I'm hopefully supposed to be able to predict stuff and not get that warning
**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): I don't know what this is so no.
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = tf.keras.utils.normalize(x_train, axis=1)
x_test = tf.keras.utils.normalize(x_test, axis=1)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax))

model.compile(optimizer= ""adam"",
             loss= ""sparse_categorical_crossentropy"",
             metrics= [""accuracy""])

model.fit(x_train, y_train, epochs=3)

Error appears after this ^ line. I also try to run model.predict() and it runs in definitely long.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

I tried the 'export AUTOGRAPH_VERBOSITY=10' line with and without quotation marks and either get a string output or syntax error so I unfortunately can not attach that. Furthermore, I tried it in terminal with the Conda environment active and got export as not a valid command or no output at all.

WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x156c9e160> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unsupported operand type(s) for -: 'NoneType' and 'int'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x156c9e160> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unsupported operand type(s) for -: 'NoneType' and 'int'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert"
52054,TF probability cannot broadcast last batch dimension if event_shape is not a scalar,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2
- GPU model and memory: GTX 1060M

**Describe the current behavior**
log_prob fails to broadcast to batch dims if event_shape is not a scalar, even if event_shape matches. I'm pretty sure this should be a trivial broadcast, but maybe I'm wrong here.
NotImplementedError: Broadcasting is not supported; unexpected batch and event shape (expected [2 3 2], saw [2 1 2]).

**Describe the expected behavior**
In the example here: https://www.tensorflow.org/probability/examples/Understanding_TensorFlow_Distributions_Shapes#computing_log_prob_for_scalar_distributions
At the section describing ""get the log probability of each value at each point in the batch"", an example is shown of broadcasting a tensor shaped [2, 2, 1] into a distribution with batch_shape [2, 3] and event_shape of (). The broadcasted shape is then [2, 2, 3].

Attempting to do the same where the shape instead is [2, 2, 1, 2] into a distribution with batch_shape [2, 3] and event_shape of [2]. The expected broadcast shape would simply be [2, 2, 3, 2], where the last batch_dim would be broadcast just like in the example.

**Standalone code to reproduce the issue**

```
import tensorflow as tf
from tensorflow_probability import distributions as tfd


if __name__ == '__main__':
    x = tf.constant([[[0.5, 0.75], [0.1, 0.25], [0.35, 0.9]], [[0.4, 0.45], [0.5, 0.7], [0.8, 0.25]]], tf.float32)
    p_x = tfd.Independent(tfd.Normal(x, 1), 1)

    x_dist = tfd.BatchReshape(distribution=p_x, batch_shape=[2, 3], validate_args=True)

    # compare each point against every other point
    transposed_elems = tf.expand_dims(tf.transpose(x, perm=(1, 0, 2)), axis=-2)
    pointwise_log_prob = x_dist.log_prob(transposed_elems)
    pass
```
"
52053,loss=nan issue in Tensorflow 2,"There is a bug in Tensorflow 2 that happens when all of the following conditions are met:
1.	Multi-GPU is enabled. 
2.	Custom loss function is used. 
3.	The number of training samples is not an integer multiple of the batch size (e.g. 3 training samples and a batch size of 2). 

Under these conditions, you'll end up with a nan loss. 
(The bug also happens if you use multiple GPU cards with batch_size=1, nobody would actually do that.)
I have tried Tensorflow 2.1, 2.4, and 2.5, so it seems like the bug is in all versions of Tensorflow 2. 

Here is a sample code you can run to reproduce the issue:

```
import os
os.environ['CUDA_VISIBLE_DEVICES'] = ""0,1"" # use a single GPU card, and bug doesn't happen
import tensorflow as tf
physical_devices = tf.config.list_physical_devices('GPU')
for device in physical_devices:
  tf.config.experimental.set_memory_growth(device, True)
print(""tensorflow.__version__: "" + str(tf.__version__))
import numpy as np
Model = tf.keras.models.Model
Input = tf.keras.layers.Input
Conv2D = tf.keras.layers.Conv2D
K = tf.keras.backend
strategy = tf.distribute.MirroredStrategy()

num_training_samples = 3 # if = 4, bug doesn't happen because 4 is a multiple of 2
batch_size = 2 # bug also happens with batch_size = 1 if you set multiple GPU cards above, regardless of if the number of training samples is an even multiple of the number of GPU cards

inputdata = np.zeros((num_training_samples,5,5,1))
outputdata = np.ones((num_training_samples,5,5,1))

with strategy.scope(): # remove this, and bug doesn't happen
  input_layer = Input(shape=(5,5,1))
  output_layer = Conv2D(filters=1, kernel_size=(3,3), padding='same', activation='tanh', data_format='channels_last', kernel_initializer='glorot_uniform')(input_layer)
  model = Model(inputs=input_layer, outputs=output_layer)

  def custom_loss(y_true, y_pred):
    diff = K.square(y_true - y_pred)
    loss = K.mean(diff)
    return loss

  model.compile(loss=custom_loss, # if loss='mse', bug doesn't happen
                optimizer='sgd')

options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF
train_dataset = tf.data.Dataset.from_tensor_slices((inputdata, outputdata))
train_dataset = train_dataset.with_options(options)
train_dataset = train_dataset.batch(batch_size)
model.fit(train_dataset, epochs=10, verbose=1)
#model.fit(inputdata, outputdata, batch_size=batch_size, epochs=10, verbose=1) # bug still happens even if you don't use Tensorflow Dataset
```
"
52051,Nested list of feature not accepted by tf.from_tensor_slices,"I'm trying to create a Dataset object with the following record structure and the code:
```
ratings_dataset = tf.data.Dataset.from_tensor_slices(np.array(preprocessed))
```
```
preprocessed = [['YoVfDbnISlW0f7abNQACIg', 'RA4V8pr014UyUbDvI-LW2A', 0.0499525, 0.6, 'Framingham', 7, 
['Department Stores', 'Optometrists', 'Home & Garden', 'Discount Store', 'Fashion', 'Furniture Stores', 
'Grocery', 'Food', 'Shopping', 'Drugstores', 'Electronics', 'Health & Medical', '<PAD>', '<PAD>', 
'<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 
'<PAD>', '<PAD>', '<PAD>', '<PAD>']],.........]
```
```
ValueError: Can't convert Python sequence with mixed types to Tensor.
```
I converted to NumPy and I got the following error:
```
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).
```
Without having the category list (the list inside the list), the Dataset object is created, exactly how I want it to.

How do I create a Dataset object with a list of features in it?"
52048,Profile TensorFlow Performance: Multiple occurences of single Conv2D op,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.6
- Python version: 3.8
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: 11.4/8.2
- GPU model and memory: RTX3090/24GB


**Describe the current behavior**
My goal is to profile NN layers runtime on GPU device (I am concerned only with inference). I am using
tf.profiler module.
Even though single convolution operation is used in the network, profile shows multiple Conv2D occurrences.

**Describe the expected behavior**
With one Conv2D used in the nwtrok, shouldn't profiler be showing only one Conv2D?

**Standalone code to reproduce the issue**
I have created a simple network as shown below.
dummy_net.py
```
import tensorflow as tf
import os

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

class Net(tf.keras.Model):
    def __init__(self) -> None:
        super().__init__()
        self.conv = tf.keras.layers.Conv2D(6,3)
    
    def call(self, x):
        x = self.conv(x)
        return x
```
and profiling it on random input.
test_dummy_net.py

```
import tensorflow as tf
import dummy_net

inp = tf.random.uniform([1,300,300,3])
model = dummy_net.Net()
tf.profiler.experimental.start('./dummy_infer_logs')
y = model(inp)
tf.profiler.experimental.stop()
tf.debugging.set_log_device_placement(True)
```

**Other info / logs**
When I check profile logs in the TensorBoard, it shows that Conv2D op has occurred multiple times.
![image](https://user-images.githubusercontent.com/25697952/133830669-c238655e-13bd-4fc8-8bc7-b0932f7e056c.png)

When I add more Conv2D layers, occurrences go on increasing. For example, for 4 Conv2D layers more than 1900 occurrences are shown. This makes it difficult to get exact run times.

Also, why same operations are also run on the host?
![image](https://user-images.githubusercontent.com/25697952/133830755-44f70f07-8d18-41e7-9a90-6cf00e8dae1a.png)
"
52046,"Downloading ""Imdb_reviews"" from Tensorflow_datasets: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd5 in position 30 invalid continuation byte","- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- TensorFlow version (use command below):
- 2.3.0
- Python version:
- 3.8.8

**Describe the current behavior**
When I was downloading ""imbd_reviews"" dataset I am facing the below error,

**'utf-8' codec can't decode byte 0xc5 in position 171: invalid continuation byte**

``` 
import tensorflow_datasets as tfds
datasets, info = tfds.load(""imdb_reviews"",as_supervised=True, with_info=True)

Downloading and preparing dataset imdb_reviews (80.23 MiB) to C:\Users\desig\tensorflow_datasets\imdb_reviews\plain_text\0.1.0...
Dl Completed...:
0/0 [00:00<?, ? url/s]
Dl Size...:
0/0 [00:00<?, ? MiB/s]


---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
<ipython-input-6-f3ae52bd604b> in <module>
      1 import numpy as np
----> 2 datasets, info = tfds.load(""imdb_reviews"",as_supervised=True, with_info=True)
      3 

~\anaconda3\lib\site-packages\tensorflow_datasets\core\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     50     _check_no_positional(fn, args, ismethod, allowed=allowed)
     51     _check_required(fn, kwargs)
---> 52     return fn(*args, **kwargs)
     53 
     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

~\anaconda3\lib\site-packages\tensorflow_datasets\core\registered.py in load(name, split, data_dir, batch_size, in_memory, shuffle_files, download, as_supervised, decoders, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)
    298   if download:
    299     download_and_prepare_kwargs = download_and_prepare_kwargs or {}
--> 300     dbuilder.download_and_prepare(**download_and_prepare_kwargs)
    301 
    302   if as_dataset_kwargs is None:

~\anaconda3\lib\site-packages\tensorflow_datasets\core\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     50     _check_no_positional(fn, args, ismethod, allowed=allowed)
     51     _check_required(fn, kwargs)
---> 52     return fn(*args, **kwargs)
     53 
     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

~\anaconda3\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in download_and_prepare(self, download_dir, download_config)
    305         self.info.size_in_bytes = dl_manager.downloaded_size
    306         # Write DatasetInfo to disk, even if we haven't computed the statistics.
--> 307         self.info.write_to_directory(self._data_dir)
    308     self._log_download_done()
    309 

~\anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback)
    118         if type is None:
    119             try:
--> 120                 next(self.gen)
    121             except StopIteration:
    122                 return False

~\anaconda3\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py in incomplete_dir(dirname)
    198   try:
    199     yield tmp_dir
--> 200     tf.io.gfile.rename(tmp_dir, dirname)
    201   finally:
    202     if tf.io.gfile.exists(tmp_dir):

~\anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py in rename_v2(src, dst, overwrite)
    543     errors.OpError: If the operation fails.
    544   """"""
--> 545   _pywrap_file_io.RenameFile(
    546       compat.as_bytes(src), compat.as_bytes(dst), overwrite)
    547 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc5 in position 171: invalid continuation byte

```
Do any one have  idea, Thank you.
"
52045,constant_folding ignores epsilon,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. [https://gist.github.com/szutenberg/76b35f503195e8c7b2f1d6d764ee1a6f]
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0 (CPU version)
- Python version: 3.6.9

When running the script https://gist.github.com/szutenberg/76b35f503195e8c7b2f1d6d764ee1a6f I'm getting:
```
=== EPSILON =  1e-09  dtype =  <dtype: 'float32'>
in  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]
out =  [-0.       -0.       15.942385       inf]
=== EPSILON =  1e-07  dtype =  <dtype: 'float32'>
in  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]
out =  [-1.1920928e-07 -1.1920928e-07  1.5249238e+01  1.5942385e+01]
=== EPSILON =  1e-09  dtype =  <dtype: 'float64'>
in  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]
out =  [-1.00000008e-09  0.00000000e+00  1.61081453e+01  2.07232658e+01]
=== EPSILON =  1e-07  dtype =  <dtype: 'float64'>
in  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]
out =  [-9.99999951e-08 -9.89999951e-08  1.54249485e+01  1.61180957e+01]
```

Add( Sub(1, logits), epsilon) is simplified by grappler (constant folding) to Sub(1+epsilon, logits)

User doesn't expect inf because epsilon was used for this purpose. It's expected to see:
```
=== EPSILON =  1e-09  dtype =  <dtype: 'float32'>
in  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]
out =  [-0.        -0.        15.9340315 20.723267 ]
=== EPSILON =  1e-07  dtype =  <dtype: 'float32'>
in  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]
out =  [-1.1920928e-07 -1.1920928e-07  1.5333239e+01  1.6118095e+01]
=== EPSILON =  1e-09  dtype =  <dtype: 'float64'>
in  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]
out =  [-1.00000008e-09  0.00000000e+00  1.61081453e+01  2.07232658e+01]
=== EPSILON =  1e-07  dtype =  <dtype: 'float64'>
in  =  [0.000000e+00 1.000000e-09 9.999999e-01 1.000000e+00]
out =  [-9.99999951e-08 -9.89999951e-08  1.54249485e+01  1.61180957e+01]
```

![image](https://user-images.githubusercontent.com/37601244/133803576-2b598fef-247a-4794-9df4-3e6208ec8b5c.png)
As we can see, it's impossible to represent 1+1e-9 in float32. It's rounded to 1.0 which causes inf (which very likely will break the training with NaN).

This case is extracted from model using hard negative mining. The input comes from sigmoid.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): Yes
- Briefly describe your candidate solution(if contributing):
constant folding should not simplify A+B = A when B != 0

**Standalone code to reproduce the issue**
https://gist.github.com/szutenberg/76b35f503195e8c7b2f1d6d764ee1a6f

Does it make sense to prepare patch resolving this issue?"
52043,"Jupyter notebook can't launch the program correctly, but  anaconda prompt can.","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow version: 2.3.0
- Python version: 3.7.11
- Installed using virtualenv? pip? conda?: conda
- CUDA/cuDNN version: 10.1

**Describe the problem**

With Anaconda, I create a virtual environment to install the cpu version of tensorflow. The System information is shown above. I install tensorflow using the command ``conda install tensorflow`` that deals with dependency automatically.

It appears to be normal when I run some basic example in **""anaconda prompt""**.

But when I try to run these in **""Jupyter Notebook""**. The kernel can't restart and turns to be dead finally.

Then I downgrade the version of **""numpy""**. It raises the problem about **""numpy import error""**:

![image](https://user-images.githubusercontent.com/62245023/133754574-ffbd33bc-1dda-4c65-b1ae-6f31e0e2d924.png)


"
52042,TensorFlowLiteC and TensorFlowLiteSelectOps iOS Pods :: Duplicate symbols,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X Mojave (10.14.6)
- TensorFlow installed from (source or binary): Cocoapods (nightly pod)
- TensorFlow version: Tensorflow Lite 0.0.1-nightly.20210915

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

(Following the comment over [here](https://github.com/tensorflow/tensorflow/issues/41876#issuecomment-921400513))

Experiencing duplicate symbols with the current nightly builds, this doesn't occur with pod versions set to 2.6.0.

Current podfile
```
'TensorFlowLiteObjC', '~> 0.0.1-nightly'
'TensorFlowLiteObjC/Metal', '~> 0.0.1-nightly'
'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'
```
Resolves to lockfile
```
  - TensorFlowLiteC (0.0.1-nightly.20210915):
    - TensorFlowLiteC/Core (= 0.0.1-nightly.20210915)
  - TensorFlowLiteC/Core (0.0.1-nightly.20210915)
  - TensorFlowLiteC/Metal (0.0.1-nightly.20210915):
    - TensorFlowLiteC/Core
  - TensorFlowLiteObjC (0.0.1-nightly.20210915):
    - TensorFlowLiteObjC/Core (= 0.0.1-nightly.20210915)
  - TensorFlowLiteObjC/Core (0.0.1-nightly.20210915):
    - TensorFlowLiteC (= 0.0.1-nightly.20210915)
  - TensorFlowLiteObjC/Metal (0.0.1-nightly.20210915):
    - TensorFlowLiteC/Metal (= 0.0.1-nightly.20210915)
    - TensorFlowLiteObjC/Core (= 0.0.1-nightly.20210915)
  - TensorFlowLiteSelectTfOps (0.0.1-nightly.20210915)
```

The duplicate symbols are `_TfLiteXNNPackDelegateCreate `, `_TfLiteXNNPackDelegateDelete `, `_TfLiteXNNPackDelegateGetThreadPool `, and `_TfLiteXNNPackDelegateOptionsDefault`. 

They're duplicated between `/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(xnnpack_delegate.o)` and `Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC`.

(If pod versions are specified as 2.6.0 then they work fine)
"
52041,onnx->tensorflow->tflite,"_Originally posted by @abattery in https://github.com/tensorflow/tensorflow/issues/46006#issuecomment-918006725_

Hi @abattery !
I have try your method

Please try out TensorFlow 2.6 or tf-nightly version with converter.experimental_enable_resource_variables = True.
And it can convert tensorflow to tflite. But when it put some warning and log error when start network forward.

Here is the warning.

```
2021-09-17 10:46:25.251638: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1844] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:
Resource ops: AssignVariableOp, ReadVariableOp, VarHandleOp
Details:
        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> ()
        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> () : {device = """"}
        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()
        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<768x1024xf32>) -> () : {device = """"}
        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>) -> (tensor<1024xf32>) : {device = """"}
        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = """"}
        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<1024xf32>>>) : {allowed_devices = [], container = """", device = """", shared_name = ""lstm_bias_lstm_18""}
        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = """", device = """", shared_name = ""lstm_kernel_lstm_18""}
2021-09-17 10:46:25.252215: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1855] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):
Flex ops: FlexAssignVariableOp, FlexReadVariableOp, FlexVarHandleOp
Details:
        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> ()
        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> () : {device = """"}
        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()
        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<768x1024xf32>) -> () : {device = """"}
        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>) -> (tensor<1024xf32>) : {device = """"}
        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = """"}
        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<1024xf32>>>) : {allowed_devices = [], container = """", device = """", shared_name = ""lstm_bias_lstm_18""}
        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = """", device = """", shared_name = ""lstm_kernel_lstm_18""}
INFO: Created TensorFlow Lite delegate for select TF ops.
2021-09-17 16:00:43.773435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-17 16:00:43.774454: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1829] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 37 nodes with 1 partitions.

INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 4 nodes with 1 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 29 nodes with 1 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 27 nodes with 1 partitions.
Traceback (most recent call last):
  File ""onnx2tflite.py"", line 59, in <module>
    interpreter.invoke()
  File ""/root/anaconda3/envs/tfnight/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py"", line 858, in invoke
    self._interpreter.Invoke()
RuntimeError: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (0 != 256)Node number 25 (RESHAPE) failed to prepare.
Node number 30 (WHILE) failed to invoke.
```

It there any suggestion to solve the problem?

Thanks."
52044,Issue with tflite interpreter,"Hi,
Training a model with MNIST database and converting to tflite, while inferencing I'm getting error.
My code details are as given below 
`
import tensorflow as tf
import tensorflow_datasets as tfds
(ds_train, ds_test), ds_info = tfds.load( 'mnist',
                                         split=['train', 'test'],
                                         shuffle_files=True,
                                         as_supervised=True,
                                         with_info=True,
                                         )
def normalize_img(image, label):
  return tf.cast(image, tf.float32) / 255., label

ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)
ds_train = ds_train.batch(128)
ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)
ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)

ds_test = ds_test.batch(128)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)

#model 
model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Reshape((1,784)),
        tf.keras.layers.Conv1D( 16, 1,use_bias=False,activation = 'relu' ),
		tf.keras.layers.Conv1D(16, 1,use_bias=False ,activation = 'relu' ),
		tf.keras.layers.Conv1D( 16, kernel_size=3,  strides=1, padding= ""causal"", dilation_rate=2**0, groups=16,use_bias=False,activation ='relu'),
		tf.keras.layers.Conv1D( 16, 1 ,use_bias=False) ,
		tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(10)
        ])
model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

model.fit(
    ds_train,
    epochs=1,
    validation_data=ds_test,
)
model.summary()
model.save(""./model.h5"")

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the tflite model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
`

tflite interpreter code 
`import numpy as np
import tensorflow as tf

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""./assignment.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
`


I'm Getting
Exception has occurred: RuntimeError

tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (16 != 1)Node number 13 (CONV_2D) failed to prepare.

When I'm trying tflite convertion and tflite interpreter I'm getting error which is as mentioned .
I tried to debug the issue and understood that if I kept the output of dilated convolution  as the input for the next layers then I'm getting this error. (due to dilated convolution dependency )

1.How to resolve this error?
2. If we cannot do it directly with tflite , then is there any alternative method for post quantization of model such as given above code?

Any help will be greatly appreciable . Thanks in Advance!"
52038,Tensor board wont load log files ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.1
- Python version:3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuDNN v 8100
- GPU model and memory: 3080 10gb 
I am trying to explore the log files generated by 
tf.debugging.experimental.enable_dump_debug_info(
    ""/tmp/tfdbg2_logdir"",
    tensor_debug_mode=""FULL_HEALTH"",
    circular_buffer_size=-1)
but after loading the tensorboard 2.6 with the following command 
tensorboard --logdir /tmp/tfdbg2_logdir/ --load_fast=false

I get a message that there is no data to be loaded 
I have checked and there are log files in this dir 
"
52037,tf.audio.decode_wav mmap like scipy.io.wavfile.read,"would be much faster in my use case to read in just a portion of a WAV file.  scipy has a nice memory map keyword argument which permits this:  `scipy.io.wavfile.read(path_to_wavfile, mmap=True)`.  sadly `tf.audio.decode_wav` does not.  thanks!"
52036,Trying to follow simple Tutorial For LOADING TEXT but tensorflow_text won't import ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary? Not sure, just following https://www.tensorflow.org/tutorials/load_data/text
- TensorFlow version: 
tensorflow                    2.6.0
tensorflow-estimator          2.6.0
tensorflow-hub                0.12.0
tensorflow-io-gcs-filesystem  0.21.0
tensorflow-text               2.6.0
tensorflow-text-nightly       2.7.0.dev20210916
- Python version: Python 3.7.7
- Installed using virtualenv? pip? conda?: Pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

First steps of loading text. ""Be sure you're using stable versions of tf and tf-text""   Followed instructions to uninstall and re install tf-nightly / tensorflow-text-nightly.

First issue was package conflict/dependency issues  about wanting ""flat buffers 2.0"" but I currently had flat buffers 1.12; after updating to flat buffers2.0 I get another package conflict that a separate tevnsofrlow package wants flat buffers 1.12 but I now have flatbuffers 1.12 which is, you guessed it, incompatible. 


**Provide the exact sequence of commands / steps that you executed before running into the problem**
From the tutorial:
'''
import collections
import pathlib
import re
import string

import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import preprocessing
from tensorflow.keras import utils
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

import tensorflow_datasets as tfds
import tensorflow_text as tf_text
'''

warning:2021-09-16 13:16:16.308765: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/dropout/temp_rate_is_zero


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


---------------------------------------------------------------------------
AlreadyExistsError                        Traceback (most recent call last)
/var/folders/wg/tzq4qwy97x3_999pctn35jlh0000gn/T/ipykernel_1983/2446760814.py in <module>
      6 import tensorflow as tf
      7 
----> 8 from tensorflow.keras import layers
      9 from tensorflow.keras import losses
     10 from tensorflow.keras import preprocessing

~/opt/anaconda3/lib/python3.7/site-packages/keras/api/_v2/keras/__init__.py in <module>
      8 import sys as _sys
      9 
---> 10 from keras import __version__
     11 from keras.api._v2.keras import __internal__
     12 from keras.api._v2.keras import activations

~/opt/anaconda3/lib/python3.7/site-packages/keras/__init__.py in <module>
     23 
     24 # See b/110718070#comment18 for more details about this import.
---> 25 from keras import models
     26 
     27 from keras.engine.input_layer import Input

~/opt/anaconda3/lib/python3.7/site-packages/keras/models.py in <module>
     18 import tensorflow.compat.v2 as tf
     19 from keras import backend
---> 20 from keras import metrics as metrics_module
     21 from keras import optimizer_v1
     22 from keras.engine import functional

~/opt/anaconda3/lib/python3.7/site-packages/keras/metrics.py in <module>
     24 
     25 import numpy as np
---> 26 from keras import activations
     27 from keras import backend
     28 from keras.engine import base_layer

~/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py in <module>
     18 
     19 from keras import backend
---> 20 from keras.layers import advanced_activations
     21 from keras.utils.generic_utils import deserialize_keras_object
     22 from keras.utils.generic_utils import serialize_keras_object

~/opt/anaconda3/lib/python3.7/site-packages/keras/layers/__init__.py in <module>
     29 
     30 # Image preprocessing layers.
---> 31 from keras.layers.preprocessing.image_preprocessing import CenterCrop
     32 from keras.layers.preprocessing.image_preprocessing import RandomCrop
     33 from keras.layers.preprocessing.image_preprocessing import RandomFlip

~/opt/anaconda3/lib/python3.7/site-packages/keras/layers/preprocessing/image_preprocessing.py in <module>
     22 from keras.engine import base_layer
     23 from keras.engine import base_preprocessing_layer
---> 24 from keras.preprocessing import image as image_preprocessing
     25 from keras.utils import control_flow_util
     26 from tensorflow.python.ops import stateless_random_ops

~/opt/anaconda3/lib/python3.7/site-packages/keras/preprocessing/__init__.py in <module>
     24 from keras.preprocessing import text
     25 from keras.preprocessing import timeseries
---> 26 from keras.utils import all_utils as utils
     27 
     28 # This exists for compatibility with prior version of keras_preprocessing.

~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/all_utils.py in <module>
     32 from keras.utils.generic_utils import serialize_keras_object
     33 from keras.utils.layer_utils import get_source_inputs
---> 34 from keras.utils.multi_gpu_utils import multi_gpu_model
     35 from keras.utils.np_utils import normalize
     36 from keras.utils.np_utils import to_categorical

~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/multi_gpu_utils.py in <module>
     18 from keras import backend
     19 from keras.engine.training import Model
---> 20 from keras.layers.core import Lambda
     21 from keras.layers.merge import concatenate
     22 

~/opt/anaconda3/lib/python3.7/site-packages/keras/layers/core/__init__.py in <module>
     18 from keras.layers.core.activity_regularization import ActivityRegularization
     19 from keras.layers.core.dense import Dense
---> 20 from keras.layers.core.dropout import Dropout
     21 from keras.layers.core.flatten import Flatten
     22 from keras.layers.core.lambda_layer import Lambda

~/opt/anaconda3/lib/python3.7/site-packages/keras/layers/core/dropout.py in <module>
     26 keras_temporary_dropout_rate = tf.__internal__.monitoring.BoolGauge(
     27     '/tensorflow/api/keras/dropout/temp_rate_is_zero',
---> 28     'Temporarily record if Keras dropout layer was created w/'
     29     'constant rate = 0')
     30 

~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/monitoring.py in __init__(self, name, description, *labels)
    359     """"""
    360     super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,
--> 361                                     len(labels), name, description, *labels)
    362 
    363   def get_cell(self, *labels):

~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/monitoring.py in __init__(self, metric_name, metric_methods, label_length, *args)
    133           self._metric_name, len(self._metric_methods)))
    134 
--> 135     self._metric = self._metric_methods[self._label_length].create(*args)
    136 
    137   def __del__(self):

AlreadyExistsError: Another metric with the same name already exists.
"
52035,bug while summing ragged  tensors,"**System information**
Not needed, check colab example below 

**Current behavior**
When summing 2 ragged tensor of (apparently) the same shape the following error is rised:
`InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'Unable to broadcast: dimension size mismatch in dimension' 
`

**Describe the expected behavior**
Well, the sum to proceed correctly

**Standalone code to reproduce the issue**
colab: https://colab.research.google.com/drive/1U7IVgd7T2Y4LrUXCsgBmlHU0VWBX6Ka4?usp=sharing"
52032,Failed copying input tensor from CPU to GPU,"### 1. System information
When i try use bert pretrain model to do my downstream tasks(what i use to load bert pretrain model is keras-bert), but when i try load my model to CPU and use GPU to train, the mistake occured. I have two GPUs with memory 32g* 2 and CPU with 128g memory,who know this problem,please give me answer,thanks 
1. when i only use CPU, it will consumes about 112g to train;
2. when i only use GPU, the OOM kill will occur;
3. by the way, when i use bert-base pretrain model and bi-GRU net to do my classify task, what memory configuration can i have to do this task

### 2. Code
with tf.device(""/cpu:0""):
   myModel = create_model()
parallel_model = multi_gpu_model(model, gpus=2)
parallel_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
parallel_model.fit(train_data, train_label, validation_data=(valid_data, valid_label), epochs=epoch, batch_size=8, shuffle=True, callbacks=[history])

### 3. (optional) Any other info / logs
the error log is 
![image](https://user-images.githubusercontent.com/30098191/133622256-8970595f-5cd2-4e45-ae4e-0ee29241bf77.png)


"
52031,Code completion of Keras Moudle no longer work since 2.6 (removed keras),Code completion of Keras Module no longer work in VSCode after Keras code is remvoed from TensorFlow 2.6
52030,Error in lowering tf ops to HLO using tf-opt,"I am trying to lower following tf ops to HLO using :tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt  -xla-legalize-tf  sample.mlir
    tf.CropAndResize 
    tf.StridedSlice 
    tf.Unique
    tf.Where 
    tf.SparseToDense 
    tf.NonMaxSuppressionV4 
    tf.TensorListFromTensor 
    tf.TensorListGetItem 
    tf.TensorListReserve       
    tf.TensorListSetItem 
    tf.TensorListStack 
    tf.TopKV2
    tf.ResizeBilinear
    tf.ResizeNearestNeighbor
I am getting error as 
The following operations cannot be legalized. These legalization failure(s) may be due to missing TF to HLO lowerings and/or unsupported attributes, etc.
I am trying to lower these ops with dynamic input shape.
I have attached tf dialect mlir files for each failed ops.
[mlir_files.zip](https://github.com/tensorflow/tensorflow/files/7177750/mlir_files.zip)

Thanks

"
52028,Reshape conversion produces invalid output shape (if one dim is -1),"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4 LTS
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0

### 2. Code

```
import numpy as np
import tensorflow as tf

inputs = tf.keras.Input(shape=(1, 126))
outputs = tf.keras.layers.Reshape((-1, 21))(inputs)
#outputs = tf.reshape(inputs, (-1, 21))
model = tf.keras.Model(inputs=inputs, outputs=outputs)

model.summary()

model.compile(optimizer='sgd', loss='mean_squared_error')

inputs = np.random.rand(126).reshape((1, 1, 126))
outputs = np.random.rand(126).reshape((1, 6, 21))

model.fit(x=inputs, y=outputs, epochs=1)

def representative_dataset_gen():
  for input in inputs:
    input = input.astype(np.float32)
    yield [input]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen

tflite_model = converter.convert()
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

### 3. Failure after conversion
Conversion produces a reshape operation (and all subsequent ops) with incorrect output shape, which makes TFLM fail (as it uses only the output shape as the reshape input parameter):
![reshape](https://user-images.githubusercontent.com/52713197/133601423-e585f707-953e-4b20-9374-7453a5becd54.png)
I tried both, tf.keras.layers.Reshape and tf.reshape, and the result is the same, except the branch in the blue box is generated only for the Keras operator. (The issue is extracted from a detection model with many reshapes.)
"
52027,eigen_mkldnn_contraction_kernel_test has ambigious select,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6.0

**Describe the problem**

The build fails when using `--config=mkl_aarch64` with 

```
Analyzing: 913 targets (107 packages loaded, 28576 targets configured)
INFO: Repository remote_java_tools_linux instantiated at:
  /DEFAULT.WORKSPACE.SUFFIX:237:6: in <toplevel>
  /tmp/tmpon64uebn-bazel-tf/a0cb78cfb3034d90b816abe220cf6cc9/external/bazel_tools/tools/build_defs/repo/utils.bzl:201:18: in maybe
Repository rule http_archive defined at:
  /tmp/tmpon64uebn-bazel-tf/a0cb78cfb3034d90b816abe220cf6cc9/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
ERROR: Analysis of target '//tensorflow/core/kernels:eigen_mkldnn_contraction_kernel_test' failed; build aborted: /tmp/boegel/TensorFlow/2.6.0/foss-2021a/TensorFlow/tensorflow-2.6.0/tensorflow/core/kernels/BUILD:2957:11: Illegal ambiguous match on configurable attribute ""srcs"" in //tensorflow/core/kernels:eigen_mkldnn_contraction_kernel_test:
//tensorflow:arm_any
//tensorflow/core/kernels:no_mkldnn_contraction_kernel
Multiple matches are not allowed unless one is unambiguously more specialized.
```

**Any other info / logs**

Issue is simple: https://github.com/tensorflow/tensorflow/blob/72fd2bfa42a8ad909baf8d2b7b674563d256514d/tensorflow/core/kernels/BUILD#L3028-L3034 is faulty as `no_mkldnn_contraction_kernel` and any of the arch-selects can match, in this case the `arm_any` which makes this invalid
"
52026,cannot import name 'Adadelta' from 'keras.optimizers (Keras-Bidaf),"ImportError                               Traceback (most recent call last)
<ipython-input-26-bc8a96d755fb> in <module>
----> 1 from bidaf.models import BidirectionalAttentionFlow
      2 #bidaf_model = BidirectionalAttentionFlow(400)
      3 #keras_model = bidaf_model.model

~\AppData\Roaming\Python\Python38\site-packages\bidaf\models\__init__.py in <module>
----> 1 from .bidaf import BidirectionalAttentionFlow

~\AppData\Roaming\Python\Python38\site-packages\bidaf\models\bidaf.py in <module>
      1 from keras.layers import Input, TimeDistributed, LSTM, Bidirectional
      2 from keras.models import Model, load_model
----> 3 from keras.optimizers import Adadelta
      4 from keras.callbacks import CSVLogger, ModelCheckpoint
      5 from ..layers import Highway, Similarity, C2QAttention, Q2CAttention, MergedContext, SpanBegin, SpanEnd, CombineOutputs

ImportError: cannot import name **'Adadelta' from 'keras.optimizers' (C:\Users\vishd\AppData\Roaming\Python\Python38\site-packages\keras\optimizers.py)**

****
I have been trying to recreate the Keras-bidaf model in my python notebook and running this code in python **from bidaf.models import BidirectionalAttentionFlow** which keeps giving me the above error and saying Adadelta can't be imported from Keras. I have tried so many options to solve it but no luck. 

I am stuck here. Any suggestions and ideas are highly appreciated. "
52025,"WARNING:tensorflow:AutoGraph could not transform <function canonicalize signatures.<locals>.signature wrapper at 0x7f0a44465cb0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'","WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fdc2c0b8710> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: No module named 'tensorflow_core.estimator'
 using 
tf-nightly-gpu

"
52024,Nested tensor as a feature incompatible with tf.data.Dataset.from_tensor_slices,"I'm trying to create a Dataset object with the following record structure(list):
```
['YoVfDbnISlW0f7abNQACIg', 'RA4V8pr014UyUbDvI-LW2A', 0.0499525, 0.6, 'Framingham', 7, 
['Department Stores', 'Optometrists', 'Home & Garden', 'Discount Store', 'Fashion', 'Furniture Stores', 
'Grocery', 'Food', 'Shopping', 'Drugstores', 'Electronics', 'Health & Medical', '<PAD>', '<PAD>', 
'<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 
'<PAD>', '<PAD>', '<PAD>', '<PAD>']]
```
```
ValueError: Can't convert Python sequence with mixed types to Tensor.
```
I converted to NumPy and I got the following error:
```
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).
```
Without having the category list, the Dataset object is created."
52023,Error in lowering tf.ResizeBilinear and tf.ResizeNearestNeighbor using tf-mlir-translate and tf-opt ,"
Model for tf.ResizeNearestNeighbor looks like:
model = tf.keras.Sequential([
tf.keras.layers.Conv2D(4, (1, 1),input_shape = (10, 10, 3),batch_size=1, name='fpn_c5p5'),
tf.keras.layers.UpSampling2D(size=(2, 2),interpolation='nearest', name=""fpn_p5upsampled"")
])

Model for tf.ResizeBilinear looks like:
model = tf.keras.Sequential([
tf.keras.layers.Conv2D(4, (1, 1),input_shape = (10, 10, 3),batch_size=1, name='fpn_c5p5'),
tf.keras.layers.UpSampling2D(size=(2, 2),interpolation='nearest', name=""fpn_p5upsampled"")
])
Error in lowering tf.ResizeBilinear and tf.ResizeNearestNeighbor to HLO.

I have attached log file and python code to get saved_model.pb
[codeandlog.zip](https://github.com/tensorflow/tensorflow/files/7174773/codeandlog.zip)

1.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --savedmodel-objectgraph-to-mlir --tf-savedmodel-exported-names=predict  -tf-enable-shape-inference-on-import=true $PWD -o sample.mlir
2.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt -canonicalize --tf-executor-to-functional-conversion --tf-shape-inference -xla-legalize-tf  --print-ir-before-all &>1  sample.mlir

If tf.ResizeNearestNeighbor if it has argument half_pixel_centers = false I able to lower to MHLO. But in the above case half_pixel_centers = true. and i am not able to lower to HLO."
52022,Protobuf version conflict between tensorflow and grpc,"**System information**
- Linux Ubuntu 20.04
- TensorFlow installed from source
- TensorFlow version: C 2.5.1
- Python version: 3.8.0
- Bazel version: 3.7.2
- GCC/Compiler version: 9.3.0


**Describe the problem**

I want to use C-GRPC to encapsulate the Tensorflow model（TensorflowC） as a service, compiling is OK. But errors will occur when running server：

> [libprotobuf FATAL /mnt/d/CODE/grpc/third_party/protobuf/src/google/protobuf/stubs/common.cc:87] This program was compiled against version 3.9.2 of the Protocol Buffer runtime library, which is not compatible with the installed version (3.17.3).  Contact the program author for an update.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""bazel-out/k8-opt/bin/tensorflow/core/framework/tensor_shape.pb.cc"".)
> terminate called after throwing an instance of 'google::protobuf::FatalException'
>   what():  This program was compiled against version 3.9.2 of the Protocol Buffer runtime library, which is not compatible with the installed version (3.17.3).  Contact the program author for an update.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""bazel-out/k8-opt/bin/tensorflow/core/framework/tensor_shape.pb.cc"".)
> Aborted (core dumped)

**Any other info / logs**

So I want to know if I can specify a version of protobuf when compiling tensorflow C lib with bazel myself.

If so, which profile should be modified and how?

Thank you for your guidance and help ：)
"
52020,ValueError: Could not find matching function to call loaded from the SavedModel. ,"I ma facing the issue
words_pred = words_model.predict([data_text])
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 909, in predict
    use_multiprocessing=use_multiprocessing)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 462, in predict
    steps=steps, callbacks=callbacks, **kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 444, in _model_iteration
    total_epochs=1)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 408, in _initialize
    *args, **kwds))
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 73, in distributed_function
    per_replica_function, args=(model, x, y, sample_weights))
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 760, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1787, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 2132, in _call_for_each_replica
    return fn(*args, **kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 292, in wrapper
    return func(*args, **kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 162, in _predict_on_batch
    return predict_on_batch(model, x)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 370, in predict_on_batch
    return model(inputs)  # pylint: disable=not-callable
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py"", line 57, in return_outputs_and_add_losses
    outputs, losses = fn(inputs, *args, **kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py"", line 111, in wrap_with_training_arg
    lambda: replace_training_and_call(False))
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py"", line 59, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py"", line 56, in smart_cond
    return false_fn()
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py"", line 111, in <lambda>
    lambda: replace_training_and_call(False))
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py"", line 106, in replace_training_and_call
    return wrapped_call(*args, **kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 494, in _call
    results = self._stateful_fn(*args, **kwds)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1822, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py"", line 262, in restored_function_body
    ""\n\n"".join(signature_descriptions)))
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (3 total):
    * Tensor(""inputs:0"", shape=(None, 40), dtype=int32)
    * False
    * None
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, 40), dtype=tf.float32, name='input_1')
    * False
    * None
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, 40), dtype=tf.float32, name='inputs')
    * False
    * None
  Keyword arguments: {}

Option 3:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, 40), dtype=tf.float32, name='inputs')
    * True
    * None
  Keyword arguments: {}

Option 4:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, 40), dtype=tf.float32, name='input_1')
    * True
    * None
  Keyword arguments: {}

Hi @mrinalsardar 
I was facing a similar issue. What worked for me was changing the way I was saving and loading the model as suggested by another user. Instead of model.save(), I saved the weights and then loaded the model as per guidelines in Part II-Approach 1 here: https://www.tensorflow.org/guide/keras/save_and_serialize.
Seems to be working now.

_Originally posted by @hepbc in https://github.com/tensorflow/tensorflow/issues/35932#issuecomment-605558870_"
52018,libtensorflowlite target compilation failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): from source
- TensorFlow version: 2.4.0, 2.4.1., latest (tried few)
- Python version: Python 3.8.10
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): bazel 3.7.2
- GCC/Compiler version (if compiling from source): 9.3.0

**Describe the problem**

I'm trying to build from source Tensorflow Lite with Select Tensorflow Operation for ARM64 architecture using Bazel build system and I get the compilation error.
```
/home/anastasiia/.cache/bazel/_bazel_anastasiia/db1857d8efe24e737c8a02a07ecf55c7/external/boringssl/BUILD:147:11: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command /home/anastasiia/.cache/bazel/_bazel_anastasiia/db1857d8efe24e737c8a02a07ecf55c7/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 40 argument(s) skipped)
In file included from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/bio.h:687:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'
 DEPRECATEDIN_1_1_0(int BIO_get_port(const char *str, unsigned short *port_ptr))

```
For ARM build I used tutorial: [build_arm](https://www.tensorflow.org/lite/guide/build_arm)
For Select TF operations: [ops_select](https://www.tensorflow.org/lite/guide/ops_select)

Looks like an issue is in the dependency library _openssl_.
Without Select TF ops everything is ok.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1) git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2) Add the TensorFlow ops delegate library dependency to the build dependencies. In the file `tensorflow/lite/BUILD` for target `libtensorflowlite` add dependency : `tensorflow/lite/delegates/flex:delegate.`
3) bazel build --config=elinux_aarch64 --config=monolithic -c opt //tensorflow/lite:libtensorflowlite.so

**Any other info / logs**

```
ERROR: /home/anastasiia/.cache/bazel/_bazel_anastasiia/db1857d8efe24e737c8a02a07ecf55c7/external/boringssl/BUILD:147:11: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command /home/anastasiia/.cache/bazel/_bazel_anastasiia/db1857d8efe24e737c8a02a07ecf55c7/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 40 argument(s) skipped)
In file included from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/bio.h:687:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'
 DEPRECATEDIN_1_1_0(int BIO_get_port(const char *str, unsigned short *port_ptr))
 ^~~~~~~~~~~~~~~~~~
In file included from /usr/include/openssl/asn1.h:23,
                 from /usr/include/openssl/objects.h:15,
                 from /usr/include/openssl/evp.h:28,
                 from /usr/include/openssl/x509.h:18,
                 from /usr/include/openssl/ssl.h:20,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/bn.h:183:43: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?
 int BN_abs_is_word(const BIGNUM *a, const BN_ULONG w);
                                           ^~~~~~~~
                                           SHA_LONG
/usr/include/openssl/bn.h:186:39: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?
 int BN_is_word(const BIGNUM *a, const BN_ULONG w);
                                       ^~~~~~~~
                                       SHA_LONG
/usr/include/openssl/bn.h:214:22: error: 'BN_ULONG' was not declared in this scope
 int BN_num_bits_word(BN_ULONG l);
                      ^~~~~~~~
/usr/include/openssl/bn.h:214:22: note: suggested alternative: 'SHA_LONG'
 int BN_num_bits_word(BN_ULONG l);
                      ^~~~~~~~
                      SHA_LONG
/usr/include/openssl/bn.h:266:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?
 BN_ULONG BN_mod_word(const BIGNUM *a, BN_ULONG w);
 ^~~~~~~~
 SHA_LONG
/usr/include/openssl/bn.h:267:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?
 BN_ULONG BN_div_word(BIGNUM *a, BN_ULONG w);
 ^~~~~~~~
 SHA_LONG
/usr/include/openssl/bn.h:268:28: error: 'BN_ULONG' has not been declared
 int BN_mul_word(BIGNUM *a, BN_ULONG w);
                            ^~~~~~~~
/usr/include/openssl/bn.h:269:28: error: 'BN_ULONG' has not been declared
 int BN_add_word(BIGNUM *a, BN_ULONG w);
                            ^~~~~~~~
/usr/include/openssl/bn.h:270:28: error: 'BN_ULONG' has not been declared
 int BN_sub_word(BIGNUM *a, BN_ULONG w);
                            ^~~~~~~~
/usr/include/openssl/bn.h:271:28: error: 'BN_ULONG' has not been declared
 int BN_set_word(BIGNUM *a, BN_ULONG w);
                            ^~~~~~~~
/usr/include/openssl/bn.h:272:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?
 BN_ULONG BN_get_word(const BIGNUM *a);
 ^~~~~~~~
 SHA_LONG
/usr/include/openssl/bn.h:288:37: error: 'BN_ULONG' has not been declared
 int BN_mod_exp_mont_word(BIGNUM *r, BN_ULONG a, const BIGNUM *p,
                                     ^~~~~~~~
/usr/include/openssl/bn.h:323:24: error: variable or field 'BN_consttime_swap' declared void
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                        ^~~~~~~~
/usr/include/openssl/bn.h:323:24: error: 'BN_ULONG' was not declared in this scope
/usr/include/openssl/bn.h:323:24: note: suggested alternative: 'SHA_LONG'
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                        ^~~~~~~~
                        SHA_LONG
/usr/include/openssl/bn.h:323:46: error: expected primary-expression before '*' token
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                                              ^
/usr/include/openssl/bn.h:323:47: error: 'a' was not declared in this scope
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                                               ^
/usr/include/openssl/bn.h:323:57: error: expected primary-expression before '*' token
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                                                         ^
/usr/include/openssl/bn.h:323:58: error: 'b' was not declared in this scope
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                                                          ^
/usr/include/openssl/bn.h:323:61: error: expected primary-expression before 'int'
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                                                             ^~~
/usr/include/openssl/bn.h:332:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_0_9_8'
 DEPRECATEDIN_0_9_8(int
 ^~~~~~~~~~~~~~~~~~
/usr/include/openssl/bn.h:403:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_0_9_8'
 DEPRECATEDIN_0_9_8(int BN_get_params(int which)) /* 0, mul, 1 high, 2 low, 3
 ^~~~~~~~~~~~~~~~~~
In file included from /usr/include/openssl/objects.h:15,
                 from /usr/include/openssl/evp.h:28,
                 from /usr/include/openssl/x509.h:18,
                 from /usr/include/openssl/ssl.h:20,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/asn1.h:555:7: error: expected constructor, destructor, or type conversion before 'unsigned'
 const unsigned char *ASN1_STRING_get0_data(const ASN1_STRING *x);
       ^~~~~~~~
In file included from /usr/include/openssl/x509.h:22,
                 from /usr/include/openssl/ssl.h:20,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ec.h:274:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_2_0'
 DEPRECATEDIN_1_2_0(int EC_GROUP_get_curve_GFp(const EC_GROUP *group, BIGNUM *p,
 ^~~~~~~~~~~~~~~~~~
/usr/include/openssl/ec.h:543:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_2_0'
 DEPRECATEDIN_1_2_0(int EC_POINT_get_affine_coordinates_GFp(const EC_GROUP *group,
 ^~~~~~~~~~~~~~~~~~
/usr/include/openssl/ec.h:631:1: error: expected constructor, destructor, or type conversion before 'size_t'
 size_t EC_POINT_point2oct(const EC_GROUP *group, const EC_POINT *p,
 ^~~~~~
In file included from /usr/include/openssl/x509.h:25,
                 from /usr/include/openssl/ssl.h:20,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/rsa.h:240:1: error: expected constructor, destructor, or type conversion before 'int'
 int RSA_generate_key_ex(RSA *rsa, int bits, BIGNUM *e, BN_GENCB *cb);
 ^~~
In file included from /usr/include/openssl/dsa.h:25,
                 from /usr/include/openssl/x509.h:26,
                 from /usr/include/openssl/ssl.h:20,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/dh.h:142:1: error: expected constructor, destructor, or type conversion before 'int'
 int DH_generate_parameters_ex(DH *dh, int prime_len, int generator,
 ^~~
In file included from /usr/include/openssl/x509.h:26,
                 from /usr/include/openssl/ssl.h:20,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/dsa.h:103:1: error: expected constructor, destructor, or type conversion before 'int'
 int DSA_sign(int type, const unsigned char *dgst, int dlen,
 ^~~
/usr/include/openssl/dsa.h:127:1: error: expected constructor, destructor, or type conversion before 'int'
 int DSA_generate_parameters_ex(DSA *dsa, int bits,
 ^~~
In file included from /usr/include/openssl/ssl.h:20,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/x509.h:728:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'
 DEPRECATEDIN_1_1_0(ASN1_TIME *X509_CRL_get_nextUpdate(X509_CRL *crl))
 ^~~~~~~~~~~~~~~~~~
In file included from /usr/include/openssl/ssl.h:26,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/hmac.h:33:12: error: expected constructor, destructor, or type conversion before 'int'
 /*__owur*/ int HMAC_Init_ex(HMAC_CTX *ctx, const void *key, int len,
            ^~~
In file included from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ssl.h:991:1: error: expected constructor, destructor, or type conversion before 'typedef'
 typedef enum {
 ^~~~~~~
/usr/include/openssl/ssl.h:1042:3: error: 'OSSL_HANDSHAKE_STATE' does not name a type; did you mean 'SSL_CB_HANDSHAKE_START'?
 } OSSL_HANDSHAKE_STATE;
   ^~~~~~~~~~~~~~~~~~~~
   SSL_CB_HANDSHAKE_START
/usr/include/openssl/ssl.h:1878:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'
 DEPRECATEDIN_1_1_0(__owur const SSL_METHOD *TLSv1_server_method(void))
 ^~~~~~~~~~~~~~~~~~
/usr/include/openssl/ssl.h:1997:8: error: 'OSSL_HANDSHAKE_STATE' does not name a type; did you mean 'SSL_CB_HANDSHAKE_START'?
 __owur OSSL_HANDSHAKE_STATE SSL_get_state(const SSL *ssl);
        ^~~~~~~~~~~~~~~~~~~~
        SSL_CB_HANDSHAKE_START
external/boringssl/src/ssl/bio_ssl.cc: In function 'SSL* get_ssl(BIO*)':
external/boringssl/src/ssl/bio_ssl.cc:16:37: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
   return reinterpret_cast<SSL *>(bio->ptr);
                                     ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/bio.h:20,
                 from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_read(BIO*, char*, int)':
external/boringssl/src/ssl/bio_ssl.cc:40:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->retry_reason = BIO_RR_ACCEPT;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/bio.h:20,
                 from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc:45:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->retry_reason = BIO_RR_CONNECT;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/bio.h:20,
                 from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_write(BIO*, const char*, int)':
external/boringssl/src/ssl/bio_ssl.cc:80:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->retry_reason = BIO_RR_CONNECT;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/bio.h:20,
                 from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc: In function 'long int ssl_ctrl(BIO*, int, long int, void*)':
external/boringssl/src/ssl/bio_ssl.cc:101:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->shutdown = num;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/bio.h:20,
                 from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc:102:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->ptr = ptr;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/bio.h:20,
                 from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc:103:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->init = 1;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/bio.h:20,
                 from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc:107:17: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       return bio->shutdown;
                 ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/bio.h:20,
                 from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc:110:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->shutdown = num;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/bio.h:20,
                 from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_free(BIO*)':
external/boringssl/src/ssl/bio_ssl.cc:148:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
   if (bio->shutdown) {
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/bio.h:20,
                 from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc: At global scope:
external/boringssl/src/ssl/bio_ssl.cc:170:25: error: variable 'const BIO_METHOD ssl_method' has initializer but incomplete type
 static const BIO_METHOD ssl_method = {
                         ^~~~~~~~~~
In file included from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
external/boringssl/src/ssl/bio_ssl.cc:177:6: error: expected identifier before numeric constant
 long BIO_set_ssl(BIO *bio, SSL *ssl, int take_owership) {
      ^~~~~~~~~~~
external/boringssl/src/ssl/bio_ssl.cc:177:6: error: expected ',' or '...' before numeric constant
external/boringssl/src/ssl/bio_ssl.cc: In function 'long int BIO_ctrl(BIO*, int)':
external/boringssl/src/ssl/bio_ssl.cc:178:39: error: 'take_owership' was not declared in this scope
   return BIO_ctrl(bio, BIO_C_SET_SSL, take_owership, ssl);
                                       ^~~~~~~~~~~~~
external/boringssl/src/ssl/bio_ssl.cc:178:54: error: 'ssl' was not declared in this scope
   return BIO_ctrl(bio, BIO_C_SET_SSL, take_owership, ssl);
                                                      ^~~
Target //tensorflow/lite:libtensorflowlite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 368.325s, Critical Path: 108.73s
INFO: 1656 processes: 241 internal, 1415 local.
FAILED: Build did NOT complete successfully

```

Thanks"
52013,Cannot load saved tf model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 33
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`:
`v2.6.0-rc2-32-g919f693420e 2.6.0`

**Describe the current behavior**
`tf.saved_model.load(model_path)` fails with the following error:

```
Traceback (most recent call last):
  File ""convert_to_onnx.py"", line 3, in <module>
    tf.saved_model.load('/home/fmurdaca/work/aicoe/elyra-aidevsecops-tutorial/models/210915165333-7b04047d1220f5cd')
  File ""/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 864, in load
    result = load_internal(export_dir, tags, options)[""root""]
  File ""/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 902, in load_internal
    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,
  File ""/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 162, in __init__
    self._load_all()
  File ""/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 259, in _load_all
    self._load_nodes()
  File ""/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 448, in _load_nodes
    slot_variable = optimizer_object.add_slot(
AttributeError: '_UserObject' object has no attribute 'add_slot'
```

This also breaks tensorflow-onnx package: https://github.com/onnx/tensorflow-onnx/issues/1715

**Describe the expected behavior**
method is able to load model, so that other actions can be performed.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): adjust attribute provided by that object

**Standalone code to reproduce the issue**
1. clone https://github.com/AICoE/elyra-aidevsecops-tutorial
2. pip install tensorflow
3. run python script with import tensoflow as tf and `tf.saved_model.load('./models/210124112759-d97fd1f46b13ee40')`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52011,C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6
- Python version: 3.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: none
- GPU model and memory: none



**Describe the problem**
I followed instructions in https://www.tensorflow.org/install/source until building the pip package, when I got this error
```
ERROR: C:/users/administrator/desktop/tensorflow/tensorflow/core/BUILD:1627:16: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 2): cl.exe failed: error executing command
  cd C:/users/administrator/_bazel_administrator/56cwyie6/execroot/org_tensorflow
````


**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --local_ram_resources=8000 --config=opt  /tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Here's the full log:
```
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from c:\users\administrator\desktop\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/Administrator/anaconda3/python.exe
INFO: Reading rc options for 'build' from c:\users\administrator\desktop\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from c:\users\administrator\desktop\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Administrator/anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Administrator/anaconda3/lib/site-packages --python_path=C:/Users/Administrator/anaconda3/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file c:\users\administrator\desktop\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\administrator\desktop\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file c:\users\administrator\desktop\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX2 --host_copt=/arch:AVX2
INFO: Found applicable config definition build:windows in file c:\users\administrator\desktop\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false --config=no_tfrt
INFO: Found applicable config definition build:monolithic in file c:\users\administrator\desktop\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:no_tfrt in file c:\users\administrator\desktop\tensorflow\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: C:/users/administrator/desktop/tensorflow/tensorflow/core/BUILD:1627:16: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 2): cl.exe failed: error executing command
  cd C:/users/administrator/_bazel_administrator/56cwyie6/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\Tools\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/Administrator/anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Users/Administrator/anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\ADMINI~1\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\ADMINI~1\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio/2019/BuildTools/VC/Tools/MSVC/14.29.30133/bin/HostX64/x64/cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/core/_objs/framework_internal_impl/tensor_util.obj.params
Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'
tensorflow/core/framework/tensor_util.cc(284): error C2668: 'signbit': ambiguous call to overloaded function
C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_math.h(303): note: could be 'bool signbit(float) throw()'
C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_math.h(308): note: or       'bool signbit(double) throw()'
C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_math.h(313): note: or       'bool signbit(long double) throw()'
tensorflow/core/framework/tensor_util.cc(284): note: while trying to match the argument list '(T)'
        with
        [
            T=tensorflow::EnumToDataType<tensorflow::DT_UINT8>::Type
        ]
tensorflow/core/framework/tensor_util.cc(329): note: see reference to function template instantiation 'bool tensorflow::tensor::internal::IsNegativeZero<T>(T)' being compiled
        with
        [
            T=tensorflow::EnumToDataType<tensorflow::DT_UINT8>::Type
        ]
tensorflow/core/framework/tensor_util.cc(372): note: see reference to function template instantiation 'bool tensorflow::tensor::internal::CompressRepeatedField<T>(float,const tensorflow::TensorShape &,tensorflow::TensorProto *)' being compiled
        with
        [
            T=tensorflow::EnumToDataType<tensorflow::DT_UINT8>::Type
        ]
tensorflow/core/framework/tensor_util.cc(396): note: see reference to function template instantiation 'bool tensorflow::tensor::internal::CompressTensorProtoInPlaceImpl<tensorflow::EnumToDataType<tensorflow::DT_UINT8>::Type>(int64_t,float,tensorflow::TensorProto *)' being compiled
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 66.282s, Critical Path: 18.54s
INFO: 27 processes: 8 internal, 19 local.
FAILED: Build did NOT complete successfully
```
"
52010,tf.keras   TypeError: unsupported operand type(s) for +: 'NoneType' and 'float,"<em>Please make sure that this is an issue related to keras.
tag:keras_template</em>

**Important Notice**

Please note that `tf.keras` code was moved entirely to
[keras-team/keras](https://github.com/keras-team/keras) repository

You can open any code/doc bugs, performance issues, and feature requests
 in [keras-team/keras](https://github.com/keras-team/keras/issues) repository

`tf.keras` related issues opened in
[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) repository may
not get attention as [keras-team/keras](https://github.com/keras-team/keras)
repository is dedicated for the development of `keras` code
"
52005,Instance Segmentation for TFLite,"Hello there,
this is not so much of an urgent matter. Nevertheless I wanted to ask, are there any good and maintained open source Instance Segmentation networks that will convert easily to TFLite? I started of with Mask R-CNN but soon realised that the SELECT_OPS flag would slow down the inference time significantly, also because gpu and npu are not available then. I need some model that is natively supported on TFLite (without SELECT_OPS). As far as I understood there are problems with dynamic tensors on GPU/NPU, which is bad because most instance segmenation models use anchors or regions of interest that are resized throughout the model and the segmented individually. I already thought about a single shot detector with sementic segmentation but could not find anything.
I found YOLACT-EDGE (https://github.com/haotian-liu/yolact_edge) until I realized that it is in pytorch :(.

Anybody any ideas? "
52003,shared_embedding_columns ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52001,Computing Jacobian and Gradient with GradientTape is extremely slow,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.6.0
- Python version: 3.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I want to compute the Jacobian and Gradient of my model for a loss function, but the model just throws a warning about retracing and gets stuck

**Describe the expected behavior**
Normal training should occur


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
def compute_loss_theta(tape, parameter, concept, output, x):

    b = x.shape[0]
    in_dim = (x.shape[1], x.shape[2])

    feature_dim = in_dim[0]*in_dim[1]

    J = tape.batch_jacobian(concept, x)
    
    grad_fx = tape.gradient(output, x)
    grad_fx = tf.reshape(grad_fx,shape=(b, feature_dim))
    J = tf.reshape(J, shape=(b, feature_dim, feature_dim))

    parameter = tf.expand_dims(parameter, axis =1)

    loss_theta_matrix = grad_fx - tf.matmul(parameter, J)

    loss_theta = tf.norm(loss_theta_matrix)

    return loss_theta


for i in range(10):
    for x, y in train_dataset:

        with tf.GradientTape(persistent=True) as tape:
            tape.watch(x)
            
            parameter, concept, output = model(x)

            loss_theta = compute_loss_theta(tape, parameter, concept, output , x)

            loss_y = loss_object(y_true=y, y_pred=output)
                
            loss_value = loss_y + eps*loss_theta

        gradients = tape.gradient(loss_value, model.trainable_weights)
        optimizer.apply_gradients(zip(gradients, model.trainable_weights))
```

This function is extremely slow. I want to compute the loss as given in the self-explanatory neural network. Input has a shape of (32, 365, 3) where 32 is the batch size. The loss I want to minimize is Equation 3 of the [paper](https://papers.nips.cc/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf). 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52000,Handle `attrs` objects in `tensorflow.python.util.nest.assert_shallow_structure`,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): *Yes*
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **v1.12.1-63725-gfeb49693266 2.7.0-dev20210914**
- Python version: **3.7.11**
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Error when trying to use the methods in `tensorflow.python.util.nest` that call `assert_shallow_structure`, e.g. `map_structure_up_to`, on an instance of a class created by `attrs`.

This happens when trying to subclass `tensorflow_probability.python.distributions.Distribution` where `_event_shape` is an `attrs` object and the public `event_shape` method calls `map_structure_up_to`.

**Describe the expected behavior**

This call should run without an error on `attrs` objects like `map_structure` does.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): **yes**
- Briefly describe your candidate solution(if contributing): **Add another case before the default one that checks `_is_attrs` and makes sure that the names are the same using `_get_attrs_items`.**

**Standalone code to reproduce the issue**

```python
import attr
import tensorflow as tf
from tensorflow.python.util import nest

@attr.attrs(auto_attribs=True)
class Container:
    a: object
    b: object

shape_object = Container(a=[1, 2], b=[3])
shallow_object = Container(a=None, b=None)
shape_res = nest.map_structure_up_to(shallow_object, tf.TensorShape, shape_object)

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""scratch.py"", line 14, in <module>
    shape_res = nest.map_structure_up_to(shallow_object, tf.TensorShape, shallow_object)
  File ""/home/cswa648/miniconda3/envs/tftest/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 1380, in map_structure_up_to
    **kwargs)
  File ""/home/cswa648/miniconda3/envs/tftest/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 1462, in map_structure_with_tuple_paths_up_to
    expand_composites=expand_composites)
  File ""/home/cswa648/miniconda3/envs/tftest/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 1090, in assert_shallow_structure
    if len(input_tree) != len(shallow_tree):
TypeError: object of type 'Container' has no len()
```
"
51999,tensorflow-macos fails to install due to wheel build failure,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monterey beta 6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip
- TensorFlow version: tensorflow-macos 2.5.0
- Python version: 3.8.10, 3.9.7
- Installed using virtualenv? pip? conda?: Mambaforge 4.10.3-5, conda, pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): Xcode 13 beta 6
- CUDA/cuDNN version:
- GPU model and memory: Apple G13G, 16GB system RAM



**Describe the problem**
When attempting to install tensorflow-macos with either Python 3.8 or 3.9, it fails when it tries to install grpcio.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
conda create -n tf python=3.8
```

or:

```
conda create -n tf
```

Then following with:
```
conda activate tf
conda install -c apple tensorflow-deps
pip install tensorflow-macos
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[pip.log](https://github.com/tensorflow/tensorflow/files/7165907/pip.log)

numpy appears to be trying to install a version of wheels that thinks that anything that isn't x86 has altivec.
"
51997,Wrong output result  to the IDE consoles  from Tensor Stream 2.3.4 and 2.6.0.  and wrong result in IDEs,"Hi all!
**System configuration**
OS Platform and Distribution : Linux Debian 10 4.19.0-17-amd64
TensorFlow installed: binary from pypi.org 
TensorFlow version: 2.3.4 and 2.6.0 
Python version:3.7.12
Installed from: compile from source
Bazel version – not using
GCC/Compiler version – not using
CUDA/cuDNN version – not using
GPU model and memory: AMD FX-8350,  16GB RAM

**Bug describing**
I recently encountered an unexpected problem with output to the IDE console. I prefer to use the Spyder 5 IDE. I started learning DL from the book “Deep Learning with Python Second Edition” by Francois Cholet MEAP 2020. When I compiling the code for Chapter 4 "" Getting started with neural networks: classification and regression"", namely, in 4.1 Classification of film reviews: an example of binary classification and ""4.2.1 The Reuters dataset"" in the Spyder 5.14 IDE,  the values of loss function and accuracy began   different than in the book. 

I spent a lot of time trying changing the CUDA Toolkit of various versions, changing   the  operating system from Debian to Ubuntu and return back – but the results in the Spyder console always was different then the result  in the book. 

For the sake of interest, I typed this code from this book in Google Collaboration - and a miracle happened - the data from the book and from the console of the Colaboratory notebook coincided. 
After that, I installed various versions of Tensorflow without support for CUDA drivers in different Python 3.7.12 virtual environments.

As a result, the output of the Jupyter  notebooks console in both virtual environments for different versions of Tensorflow coincided with the final data in the book. I also used different IDEs for different virtual environments - Eclipse 2021-06 for TF 2.3.4 and Spyder 5.14 for TF2. 6. 0 - in both IDEs, the console output and the final result were different from the results in the book and in Jupiter notebooks.

Please open the case to fix this bug. 
Best regards, Vadim Maklakov.  

[ch_4_IMDB_issue.tar.gz](https://github.com/tensorflow/tensorflow/files/7165248/ch_4_IMDB_issue.tar.gz)"
51996,"A few documentation errors and omissions for ""Install TensorFlow for C""","The page **https://www.tensorflow.org/install/lang_c** ""Install TensorFlow for C"" has several errors and omissions:-
1. The first link ""bindings for other languages"" is a dead link (error 404)
2. It says that it is built nightly, but the second link ""libtensorflow-nightly GCS bucket"" says that it was last built about a year ago
3. It looks as though it was last built about a month ago.  It would be good to have the date that it was last built somewhere on the page
4. For ""Supported Platforms"", it says that it works for ""macOS, Version 10.12.6 (Sierra) or higher"", but the download file says that it is for a x86_64 (ie. Intel) CPU.  The latest Macs (for almost a year now) use the M1 processor.  Does it support the M1 Mac."
51995,Profile TensorFlow Performance: Multiple occurences of single Conv2D op,"
------------------------

### System information

-   **Custom Code**
-   **Linux Ubuntu 20.04**
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: 2.6
-   **Python version**: 3.8
-   **Bazel version (if compiling from source)**: None
-   **GCC/Compiler version (if compiling from source)**: None
-   **CUDA/cuDNN version**: 11.4/8.2
-   **GPU model and memory**: RTX3090/24GB
-   **Exact command to reproduce**:

### Describe the problem
Profile NN layers runtime on GPU device (I am concerned only with inference). I am using
tf.profiler module. 

### Source code / logs
I have created a dummy network with just a single convolution layer.
dummy_net.py
```
import tensorflow as tf
import os

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

class Net(tf.keras.Model):
    def __init__(self) -> None:
        super().__init__()
        self.conv = tf.keras.layers.Conv2D(6,3)
    
    def call(self, x):
        x = self.conv(x)
        return x
```
And testing it on random input.

test_dummy_net.py

```
import tensorflow as tf
import dummy_net

inp = tf.random.uniform([1,300,300,3])
model = dummy_net.Net()
tf.profiler.experimental.start('./dummy_infer_logs')
y = model(inp)
tf.profiler.experimental.stop()
tf.debugging.set_log_device_placement(True)
```
When I check profile logs in the TensorBoard, it shows that Conv2D op has occurred multiple times.
![image](https://user-images.githubusercontent.com/25697952/133319278-cb43ff62-9b35-4909-bfce-84fb9692f5cb.png)

When I add more Conv2D layers, occurrences go on increasing. For example, for 4 Conv2D layers more than 1900 occurrences are shown. This makes it difficult to get exact run times.

Also, why same operations are also run on the host?
![image](https://user-images.githubusercontent.com/25697952/133319852-30e13c82-955f-43ec-8ca5-b561b41bde4b.png)
"
51994,when convert keras model to tflite .. gives me this !!,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MUL, SOFTMAX. Here is a list of operators for which you will need custom implementations: Elu.

"
51992,AttributeError: module 'tensorflow.compat.v2.__internal__.tracking' has no attribute 'DelegatingTrackableMixin',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04.2 LTS** 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **no**
- TensorFlow installed from (source or binary): **pip**
- TensorFlow version (use command below): **2.7.0-dev20210806**
- Python version: **3.6**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **no GPU in use**
- GPU model and memory:

**Describe the current behavior**

**Error when trying to load model.Worked in 2.6.0 and previouse.**

Error message in short: 

**AttributeError: module 'tensorflow.compat.v2.__internal__.tracking' has no attribute 'DelegatingTrackableMixin'**
**logs below** 

**Describe the expected behavior**

**Should just load a model** 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
```
import tensorflow as tf
tfVersion=tf.version.VERSION.replace(""."", """")
print('TensorFlow Version: '+tf.version.VERSION)
import pathlib as path

if __name__ == '__main__':
     saved_model_dir= path.Path.cwd() / 'model' / 'Pump_LSTM_Fapi_OOP_1_tf_260' 
     model=tf.keras.models.load_model(saved_model_dir)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

**MODEL**
[Pump_LSTM_Fapi_OOP_1_tf_260.zip](https://github.com/tensorflow/tensorflow/files/7163177/Pump_LSTM_Fapi_OOP_1_tf_260.zip)


**TRACEBACK**

> TensorFlow Version: 2.7.0-dev20210806
> Traceback (most recent call last):
> 
>   File ""/home/base/Documents/Git/KundenProjekte2021/Ginko/pump_sensor/untitled0.py"", line 41, in <module>
>     model=tf.keras.models.load_model(saved_model_dir)
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__
>     module = self._load()
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _load
>     module = importlib.import_module(self.__name__)
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/importlib/__init__.py"", line 126, in import_module
>     return _bootstrap._gcd_import(name[level:], package, level)
> 
>   File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
> 
>   File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
> 
>   File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
> 
>   File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
> 
>   File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
> 
>   File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
> 
>   File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
> 
>   File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
> 
>   File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
> 
>   File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
> 
>   File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
> 
>   File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
> 
>   File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
> 
>   File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
> 
>   File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
> 
>   File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
> 
>   File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
> 
>   File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/site-packages/keras/__init__.py"", line 25, in <module>
>     from keras import models
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/site-packages/keras/models.py"", line 20, in <module>
>     from keras import metrics as metrics_module
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/site-packages/keras/metrics.py"", line 26, in <module>
>     from keras import activations
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/site-packages/keras/activations.py"", line 20, in <module>
>     from keras.layers import advanced_activations
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/site-packages/keras/layers/__init__.py"", line 23, in <module>
>     from keras.engine.input_layer import Input
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/site-packages/keras/engine/input_layer.py"", line 21, in <module>
>     from keras.engine import base_layer
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/site-packages/keras/engine/base_layer.py"", line 43, in <module>
>     from keras.mixed_precision import loss_scale_optimizer
> 
>   File ""/home/base/anaconda3/envs/AInight/lib/python3.6/site-packages/keras/mixed_precision/loss_scale_optimizer.py"", line 257, in <module>
>     class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,
> 
> AttributeError: module 'tensorflow.compat.v2.__internal__.tracking' has no attribute 'DelegatingTrackableMixin'
"
51991,Add memory optimization flag in tf.config.optimizer.set_experimental_options,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.6 (latest)
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Memory optimizer is not included in tf.config.optimizer.set_experimental_options [link](https://www.tensorflow.org/api_docs/python/tf/config/optimizer/set_experimental_options). We would like to have such useful control option. 
Some similar optimizers, such as shape optimization or loop optimization is already included, we guess it is possible to also have ""memory optimization"".

**Will this change the current api? How?**
It will add a new option ""memory_optimizatior"" in tf.config.optimizer.set_experimental_options. 
User can manually change the option.
```
 tf.config.optimizer.set_experimental_options({'memory_optimizer': False})
```

**Who will benefit with this feature?**
Users who want to do some debug may turn off the memory optimization. It maybe helpful to figure out some issues.


**Any Other info.**
"
51990,IndexError: tuple index out of range,"![9](https://user-images.githubusercontent.com/26819449/133263607-e9733e7c-b6d5-4d11-87e2-8dbc740009c6.JPG)

![9 1](https://user-images.githubusercontent.com/26819449/133263637-5905c37b-0c77-4ff9-a983-426e3aea5f9d.JPG)

 tensor flow = 2.2
 keras = 2.4.3
 
I am not able to solve this error.
Can you please tell me where should I refer?
Thanks!
"
51989,progress bar display error when calling model.evaluate method after a load_model or clone_model,"progress bar display error when calling model.evaluate method after a load_model or clone_model

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
Ubuntu 20.04.3 LTS (GNU/Linux 5.4.0-80-generic x86_64)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary (pip install tensorflow==2.4)
- TensorFlow version (use command below):
v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version:
Python 3.8.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
cuda-11.2
cudnn/11.0-v8.0.0
- GPU model and memory:
GeForce RTX 2070

**Describe the current behavior**
After a clone_model, or a load_model, the model.evaluate method doesn't display the same metrics results in progress_bar and in returned values (wrong ones in progress bar):

results = model_copy.evaluate(test_ds2)
print(results)

40/40 [==============================] - 0s 1ms/step - loss: 0.1934 - accuracy: **0.9419**
[0.16363045573234558, **0.9517999887466431**]

It seems that ProgbarLogger  stateful_metrics is not set for loss and metrics (here accuracy), due to a late assignment to self.model.metrics (tensorflow/python/keras/callbacks.py l.1047) leading to an average value display of these values instead of a single final value (tensorflow/python/keras/utils/generic_utils.py l.551)


**Describe the expected behavior**
This behavior is different when the evaluate function is called after a fit call (which set the model.metrics) and lead to the right display
40/40 [==============================] - 0s 1ms/step - loss: 0.1636 - acc: **0.9518**
[0.16363045573234558, **0.9517999887466431**]

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
NO, not sure to see where to handle this issue
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

few lines codes with MNIST and simplest model.

```
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.layers import ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import layers


physical_devices = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)


from tensorflow.keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()


def prepare_data(x, y):
    x=x.astype('float32')
    y=y.astype('int32')
    # convert from range int[0,255] to float32[-1,1]
    x/=255
    x = 2*x -1
    x=x.reshape((-1,28,28,1))
    y=tf.keras.utils.to_categorical(y,num_classes=10)
    return x, y

# prepare the data
x_train, y_train = prepare_data(x_train, y_train)
x_test, y_test = prepare_data(x_test, y_test)


epochs = 2 #200
batch_size = 256

##simplest model
K.clear_session()
model = tf.keras.Sequential([
    layers.Flatten(),
    layers.Dense(100),
    layers.ReLU(),
    layers.Dense(120),
    layers.ReLU(),
    layers.Dense(10)
]
)

loss_function = tf.losses.CategoricalCrossentropy(from_logits=True)
optimizer = Adam(lr=0.001)
model.compile(loss=loss_function,optimizer=optimizer, metrics=['acc'])


model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test))


print(""Run evaluate on test dataset"")
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)
results = model.evaluate(test_ds)
print(results)

##---------------------------
print(""Cpy model "")
model_copy= tf.keras.models.clone_model(model)
model_copy.build((None, 28,28,1)) # replace 10 with number of variables in input layer
model_copy.compile(loss=loss_function,optimizer=optimizer, metrics=[""accuracy""])
model_copy.set_weights(model.get_weights())

print(""Run evaluate with dataset on copied model"")
test_ds2 = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)
results = model_copy.evaluate(test_ds2)
print(results)  

```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
51988,XLA HLO profiling not supported on GPU,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
when enable hlo profile via export XLA_FLAGS=""--xla_hlo_profile"", log says 
```
2021-09-14 17:15:45.256221: E tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:893] --xla_hlo_profile for GPU is unsupported.
```

**Will this change the current api? How?**
No
**Who will benefit with this feature?**
Machine learning system researchers/engineers would benefit from this feature when profiling machine learning models.
**Any Other info.**
"
51987,Wrong wording of missing CPU instructions,"Have a look at the code at https://github.com/tensorflow/tensorflow/blob/3d3c6db1ca2d50f6f07722cd800144f8f736167c/tensorflow/core/platform/cpu_feature_guard.cc#L150-L157

This outputs e.g. 

> This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

Judging from the naming `missing_instructions` I suppose there is a ""not"" missing in that warning."
51986,Cannot install tensorflow-macos,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac mini(M1) ver.12.0(Monterey)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.5.0
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: Conda and pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I tried to install Tensorflow-metal by following [Getting Started with tensorflow-metal PluggableDevice](https://developer.apple.com/metal/tensorflow-plugin/).

I was successful up to `conda install -c apple tensorflow-deps`,but error occurred in `python -m pip install tensorflow-macos`.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
(Download Miniforge3-MacOSX-arm64.sh)
chmod +x ~/Downloads/Miniforge3-MacOSX-arm64.sh
sh ~/Downloads/Miniforge3-MacOSX-arm64.sh
source ~/miniforge3/bin/activate

conda install -c apple tensorflow-deps
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal
```

**Any other info / logs**

error file is [git hub gist](https://gist.github.com/tetuomi/66ff78288c81ff819f947b70746318e2#file-error-txt)
"
51983,"How to reduce time of Host-side TensorFlow operations(_Send, _HostSend) in GPU reference","Dear all,
       When I do a simple model refence on GPU, it seems that the kernel launch time is much more significant than compute time(kernel launch nearly 90% to overall). Accoring to the profiler result, the Host-side TensorFlow operations such as _Send and _HostSend type  seems to be the problem. Is there any solution to this problem?

Env: tensorflow2.2.0 + cuda10.1

Thanks,
Asher

PS:
overview_page:
![图片](https://user-images.githubusercontent.com/51115601/133186394-887ad5d5-f0cc-40e8-818d-e4b8e8b74b1b.png)
tensorflow_stats:
![图片](https://user-images.githubusercontent.com/51115601/133186502-0679d674-fb30-4574-b526-a4c879ce778a.png)
"
51981,OSError: SavedModel file does not exist but I never explicitly saved a model,"I am trying to load a model from a URL with hub.load, but TF is trying to find a saved version of the model on my computer, even though I never saved the model.

When I did this the very first time, it worked no problem and loaded the model from the URL. But I believe now it's not working because it's looking for some sort of cached model file that doesn't actually exist. Note that I'm running this in a Jupyter notebook.

Any ideas how to solve this? Below find my code and traceback error.

`module_url = ""https://tfhub.dev/google/universal-sentence-encoder/4""`
`model = hub.load(module_url)`

OSError                                   Traceback (most recent call last)
<ipython-input-5-51e1a5214226> in <module>
    198 
    199 module_url = ""https://tfhub.dev/google/universal-sentence-encoder/4""
--> 200 model = hub.load(module_url)
    201 #def embed(input):
    202 #    return model(input)

~/anaconda3/lib/python3.7/site-packages/tensorflow_hub/module_v2.py in load(handle, tags, options)
    104         module_path, tags=tags, options=options)
    105   else:
--> 106     obj = tf.compat.v1.saved_model.load_v2(module_path, tags=tags)
    107   obj._is_hub_module_v1 = is_hub_module_v1  # pylint: disable=protected-access
    108   return obj

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load(export_dir, tags, options)
    862   """"""
    863   metrics.IncrementReadApi(_LOAD_V2_LABEL)
--> 864   result = load_internal(export_dir, tags, options)[""root""]
    865   metrics.IncrementRead()
    866   return result

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, options, loader_cls, filters)
    876     tags = nest.flatten(tags)
    877   saved_model_proto, debug_info = (
--> 878       loader_impl.parse_saved_model_with_debug_info(export_dir))
    879 
    880   if (len(saved_model_proto.meta_graphs) == 1 and

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py in parse_saved_model_with_debug_info(export_dir)
     58     parsed. Missing graph debug info file is fine.
     59   """"""
---> 60   saved_model = _parse_saved_model(export_dir)
     61 
     62   debug_info_path = os.path.join(

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py in parse_saved_model(export_dir)
    119         ""SavedModel file does not exist at: %s%s{%s|%s}"" %
    120         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,
--> 121          constants.SAVED_MODEL_FILENAME_PB))
    122 
    123 

OSError: SavedModel file does not exist at: /var/folders/6z/8r9107bj20dcmrqn9kp8l2n40000gp/T/tfhub_modules/063d866c06683311b44b4992fd46003be952409c/{saved_model.pbtxt|saved_model.pb}

"
51979,How can I use tf.config.experimental.VirtualDeviceConfiguration and keras.model.fit together,"I am trying to use tf.config.experimental.VirtualDeviceConfiguration to let my model run faster.
My original model was built like this method:
```python

def deepfm_model(xxx):
    ······

tf.config.set_soft_device_placement(True)
tf.debugging.set_log_device_placement(True)

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],
        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096),
         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(len(gpus), ""Physical GPU,"", len(logical_gpus), ""Logical GPUs"")

strategy = tf.distribute.MirroredStrategy()
dataset=xxx
dataset = strategy.experimental_distribute_dataset(dataset)

model = deepfm_model(inputs, outputs)
model.compile(optimizer=""adam"", loss=""mse"", metrics=[xxx])
model.fit(dataset, epochs=10, verbose=0,use_multiprocessing=True,workers=2,)
```

when I  set_log_device_placement,I saw model has been build in both 2 Vgpus,but only one gpu use for compute:

```
2021-09-12 18:17:38.908813: I tensorflow/core/common_runtime/placer.cc:114] Identity_76: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2021-09-12 18:17:38.908821: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink1: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2021-09-12 18:17:38.908829: I tensorflow/core/common_runtime/placer.cc:114] FakeSink1: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink2: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2021-09-12 18:17:38.908837: I tensorflow/core/common_runtime/placer.cc:114] FakeSink2: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink3: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2021-09-12 18:17:38.908844: I tensorflow/core/common_runtime/placer.cc:114] FakeSink3: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
```

Can I have an example to see how can I use tf.config.experimental.VirtualDeviceConfiguration
in tf.keras like  model.compile && model.fit   method?

My model is simple and I want to use a vgpu to improve its computational efficiency，thank you"
51978,"""Unimplemented:  Deterministic GPU implementation of unsorted segment reduction op not available"" with AUC metric and TF_DETERMINISTIC_OPS","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OpenSUSE LEAP 15.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: Python 3.9.6
- CUDA/cuDNN version: 11.2 and 8.1.1, I believe
- GPU model and memory: Quadro RTX 6000

Reproduces on Colab with GPU.

**Describe the current behavior**

```
Traceback (most recent call last):
[...]
  File ""/home/bers/proj/bug.py"", line 12, in <module>
    model.fit(x=data, y=data)
  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/keras/engine/training.py"", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 950, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 3039, in __call__
    return graph_function._call_flat(
  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 591, in call
    outputs = execute.execute(
  File ""/data2/bers/opt/pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnimplementedError: 2 root error(s) found.
  (0) Unimplemented:  Deterministic GPU implementation of unsorted segment reduction op not available.
	 [[node UnsortedSegmentSum (defined at home/bers/proj/bug.py:12) ]]
	 [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_39]]
  (1) Unimplemented:  Deterministic GPU implementation of unsorted segment reduction op not available.
	 [[node UnsortedSegmentSum (defined at home/bers/proj/bug.py:12) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_513]

Function call stack:
train_function -> train_function
```

**Describe the expected behavior**

No error (works in TF 2.5.0)

**Standalone code to reproduce the issue**
```python
import os

os.environ[""TF_DETERMINISTIC_OPS""] = ""True""

import tensorflow as tf

data = tf.ones((1, 1))
layer = tf.keras.layers.Input(shape=[1])
model = tf.keras.models.Model(inputs=layer, outputs=layer)
model.compile(loss=""categorical_crossentropy"", metrics=""AUC"")
model.fit(x=data, y=data)
```
"
51977,ValueError: The first argument to `Layer.call` must always be passed.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
51974,How to extract the weight and bias from GRU and feed to a GRUcell layers?,"Hi ,  I find that in pytorch, there are ways to extract  the bias and weights from a trained GRU layers and apply them to a GRUcell as follows:

    def get_gru_cell(self, gru): 
        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)
        gru_cell.weight_hh.data = gru.weight_hh_l0.data
        gru_cell.weight_ih.data = gru.weight_ih_l0.data
        gru_cell.bias_hh.data = gru.bias_hh_l0.data
        gru_cell.bias_ih.data = gru.bias_ih_l0.data
        return gru_cell

where gru param is a trained pytorch's GRU layers(nn.GRU).
I want to know how to rewrite this implementation by Tensorflow2.3?
For example I try to rewrite it in TF2 as follows:

    def get_gru_cell(self, gru):
        gru_cell = tf.keras.layers.GRUCell(gru.units)
        gru_cell.set_weights(gru.get_weights())
        return gru_cell

But I get value_error:
ValueError: You called `set_weights(weights)` on layer ""gru_cell_2"" with a weight list of length 3, but the layer was expecting 0 weights. Provided weights: [array([[-0.017596  , -0.01114826, -0.05229527, .....

Can anyone tell me how to rewrite it correctly? Thanks!"
51970,Setting OMP_DYNAMIC=true ruins training on TF-MKL,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git
- Python version: 3.9
- Bazel version (if compiling from source): 4.2.1
- GCC/Compiler version (if compiling from source): 11
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
Some background: I notices a big (several times) time difference between TF-Eigen and TF-MKL. With Eigen being faster. Trying to tune MKL to better performance I experimented with OMP environment variables.  While I was able to get much faster times with TF-MKL, I discovered that the convergence is ruined (the same code does not converge anymore). It seems that the offensive flag is `OMP_DYNAMIC=true ` 

Just to give some numbers, running with and without the variable set:
```
OMP_DYNAMIC=true ./demo.py

Epoch 1/100
   1020/Unknown - 37s 36ms/step - loss: nan - accuracy: 0.0928   
```
 And without it:
```
./demo.py
Epoch 1/100
   1003/Unknown - 112s 111ms/step - loss: 12.1739 - accuracy: 0.9124  
```

Note how much faster the first run is (37ms vs 111ms). However, its accuracy and loss are much worse. At some point, the loss becomes NaN...



**Describe the expected behavior**
Convergence should not be affected by OMP flags.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Unfortunately it is not easy to provide a short code that can reproduce the problem. I hope the developers would be able to run some internal tests. 

"
51967,tf.nn.sigmoid_cross_entropy_with_logits should support broadcasting,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6
- Python version: 3.9
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
`tf.nn.sigmoid_cross_entropy_with_logits` does not support broadcasting. 
This leads to some wrong behavior in certain cases (e.g., keras-team/tf-keras#84) 
**Describe the expected behavior**
I would expect it to support boradcasting

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):
Add broadcasting for `labels`

**Standalone code to reproduce the issue**
```python
y = tf.random.uniform((10, 1))

# This one works
tf.keras.losses.binary_crossentropy(0.5, y) 

# This one fails
tf.keras.losses.binary_crossentropy(0.5, y, from_logits=True)
```


"
51963,How to use tensorflow.experimental.numpy in tensorflow?,"I have known `tensorflow.experimental.numpy` in tensorflow, but I don't know how to assign a value to a numpy NDarry, just like this.
`
    def get_dct_filter(self, tile_size_x, tile_size_y, mapper_x, mapper_y, channel):
        dct_numpy_filter = np.zeros(shape=(tile_size_y, tile_size_x, channel), dtype=np.float32)
        c_part = channel // len(mapper_x)

        for i, (u_x, u_y) in enumerate(zip(mapper_x, mapper_y)):
            for t_x in range(tile_size_x):
                for t_y in range(tile_size_y):
                    dct_numpy_filter[t_y, t_x, i * c_part: (i + 1) * c_part] = self.build_filter(t_x, u_x,
                                                                                                 tile_size_x) * \
                                                                               self.build_filter(t_y, u_y, tile_size_y)
        return dct_numpy_filter

    def build_filter(self, pos, frequency, POS):
        result = np.cos(np.pi * frequency * (pos + 0.5) / POS) / np.sqrt(POS)
        if frequency == 0:
            return result
        else:
            return result * np.sqrt(2)
`

When I use `tensorflow.experimental.numpy` do **dct_numpy_filter[t_y, t_x, i * c_part: (i + 1) * c_part] = self.build_filter(t_x, u_x, tile_size_x) * self.build_filter(t_y, u_y, tile_size_y)**, error has occured which told me tensor can't assign. I have used `np.experimental_enable_numpy_behavior(prefer_float32=True)` in code, but it not working, I don't know how to use the mechanism in deep learning training."
51954,GPU Delegate Issue !,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android OS Oreo (API Level 27)
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: Surveillance Camera with QCS605 Qualcomm ship.
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.5
-   **Python version**: 3.8
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**: Adreno GPU 615
-   **Exact command to reproduce**:
-       final Interpreter.Options options = new Interpreter.Options();
        CompatibilityList compatList = new CompatibilityList();

        if (compatList.isDelegateSupportedOnThisDevice()) {
            // if the device has a supported GPU, add the GPU delegate
            GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();
            GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);
            options.addDelegate(gpuDelegate);
            Log.i(LOGTAG, ""-----------Running using GPU Delegate-----------"");
        } else {
            // if the GPU is not supported, run on numThreads threads
            options.setNumThreads(numThreads)
                    .setAllowFp16PrecisionForFp32(allowFp16PrecisionForFp32)
                    .setUseNNAPI(useNNAPI);
            Log.i(LOGTAG, ""------------Running using CPU Delegate---------"");
        }

### Describe the problem

Trying to run [Movinet ](https://tfhub.dev/google/collections/movinet) tflite model on a vendor camera with Adreno GPU 615. 

The network runs on the CPU without any issues. But it can't be run on the GPU.  

I'm using Java TFLite run time with version 2.5. I tried the nightly version but didn't work also. 


### Source code / logs
I/Adreno: QUALCOMM build                   : e3ea17d, I2eff518144
I/Adreno: Build Config                     : S L 4.0.10 AArch64
I/zygote64: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0
E/libEGL: call to OpenGL ES API with no current context (logged once per thread)"
51951,host to device transfer is not utilizing close to PCIE bandwidth,"**System information**
- TensorFlow version (use command below): 2.3.0, 2.5.0, 2.6.0
- Python version: 3.7
- CUDA/cuDNN version: 11.2
- GPU model and memory: see colab on K80s, but this seems to happen on T4s, V100s, and A100s too

**Describe the current behavior**
device memory bandwidth is marketed as quite high, e.g.
> pciBusID: 0000:00:07.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s

**Describe the expected behavior**
cannot get over a few GB/s transferring tensors from CPU -> GPU

**Standalone code to reproduce the issue**
On a K80 in [colab](https://colab.research.google.com/drive/1RXQ79Phc5niKvtiq3FW403KtO36ok5zV#scrollTo=oP2iH3qXYOCn), we perform the following:

1. create a dataset that generates `100` batches of `[1024, 10000]` float32s
2. move tensor to GPU
3. multiple by own transpose

we measure the time it takes to do this `100` batches, effectively moving 
 `10000 float32 * 4 bytes/float32 * 1024 elements / step * 100 step * 1 GB / 1e9 bytes = 4.1 GB` of data from CPU to GPU.
 
we do this in both tensorflow and torch.

I get around `8.7s (0.47 GB/s)` in tensorflow and `2.5s (1.65 GB/s)` in torch; torch is 3.5x faster no matter how I move my tensors to the device in tensorflow.  (I tried a variety of approaches as discussed in https://github.com/tensorflow/tensorflow/issues/43905)

According to [this](https://www.techpowerup.com/gpu-specs/tesla-k80.c2616), K80s have PCIE 3.0x16 which look to be `15.75 GB/s` based on [wikipedia](https://en.wikipedia.org/wiki/PCI_Express)

**Neither pytorch nor tensorflow seem to approach this speed**
 "
51950,"ValueError: Protocol message object_detection.protos.StringIntLabelMapItem has no non-repeated field ""frequency""","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install tensorflow==1.15 (anaconda)
- TensorFlow version (use command below): 1.15
- Python version: 3.7.11

i am trying to run the script: updated_old_example.py from a tutorial without errors (see source:https://github.com/Bengemon825/TF_Object_Detection2020)

after i selected the correct interpreter with **tensorflow 1.15** and **python 3.7** i got the first error: **AttributeError: module 'tensorflow' has no attribute 'GraphDef'.**

Solution: **tf.compat.v1.GraphDef() + tf.compat.v2.io.gfile.GFile()**

at the end i get this error message: **ValueError: Protocol message object_detection.protos.StringIntLabelMapItem has no non-repeated field ""frequency""** , where i didn't find a solution.

thanks for your help!
"
51948,NotImplementedError: quantile is not implemented: StudentT,"Hi, all

**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04
- Tensorflow version : 2.6.0
- Tensorflow probability version: 0.13.0
- Python version: 3.8.10

When trying to compute the [well documented](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/StudentT) quantile of a Student's T distribution, the following error shows in console: `NotImplementedError: quantile is not implemented: StudentT
`

Here is a MWE:

 ``` python
from tensorflow_probability import distributions as d 

t = d.StudentT(4.13, 0.0, 1.0)
t.quantile(0.2)
```
I think this issue is very similar to [20208](https://github.com/tensorflow/tensorflow/issues/20208) and [1259](https://github.com/tensorflow/probability/issues/1259).

Is there something I am missing?

Thanks in advance!
"
51944,Init node head/predictions/class_string_lookup/table_init/LookupTableImportV2 doesn't exist in graph,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):debian 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.6.0
- Python version:3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
2021-09-11 12:21:06.193462: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-09-11 12:21:06.193522: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-09-11 12:21:09.808281: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2021-09-11 12:21:09.808359: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-09-11 12:21:09.808396: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (penguin): /proc/driver/nvidia/version does not exist
2021-09-11 12:21:09.808686: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-11 12:21:10.927031: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2021-09-11 12:21:11.938166: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2021-09-11 12:21:11.938620: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2021-09-11 12:21:11.961714: E tensorflow/core/grappler/grappler_item_builder.cc:669] Init node head/predictions/class_string_lookup/table_init/LookupTableImportV2 doesn't exist in graph
Traceback (most recent call last):
  File ""tflite.py"", line 29, in <module>
    model = converter.convert()
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1396, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 729, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 715, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1201, in convert
    self._freeze_concrete_function())
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/convert_phase.py"", line 218, in wrapper
    raise error from None  # Re-throws the exception.
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/convert_phase.py"", line 208, in wrapper
    return func(*args, **kwargs)
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1177, in _freeze_concrete_function
    self._funcs[0], lower_control_flow=False))
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1229, in convert_variables_to_constants_v2_as_graph
    aggressive_inlining=aggressive_inlining)
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 809, in __init__
    aggressive_inlining)
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1043, in _run_inline_graph_optimization
    return tf_optimizer.OptimizeGraph(config, meta_graph)
  File ""/home/fsx950223/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/grappler/tf_optimizer.py"", line 58, in OptimizeGraph
    graph_id, strip_default_attributes)
ValueError: Failed to import metagraph, check error log for more info.
```
**Describe the expected behavior**
Works same as ```converter = tf.lite.TFLiteConverter.from_saved_model('./saved_model', signature_keys=[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])```
**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```python
import tensorflow as tf
import tensorflow_text as text
model = tf.saved_model.load('./saved_model')
concrete_func = model.signatures[
    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
concrete_func.inputs[0].set_shape([1])
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
# converter = tf.lite.TFLiteConverter.from_saved_model('./saved_model', signature_keys=[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])
converter.optimizations=[tf.lite.Optimize.DEFAULT]
converter.inference_type=tf.float32
converter.target_spec.supported_ops=[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
model = converter.convert()
tf.io.write_file('guesslang.tflite', model)
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Saved model: [saved_model.zip](https://github.com/tensorflow/tensorflow/files/7147206/saved_model.zip)
"
51936,tf.keras.layers.MaxPooling3D crashes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.6.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
`tf.keras.layers.MaxPooling3D` crashes when `pool_size` contains `0`, and outputs a all-inf tensor when `pool_size` contains negative values.

**Describe the expected behavior**
Expect a `ValueError` to be thrown if the input `pool_size` contains zero or negative values.


**Standalone code to reproduce the issue**
If the `pool_size` has `0`:
```
import tensorflow as tf
pool_size = [2, 2, 0]
layer = tf.keras.layers.MaxPooling3D(strides=1, pool_size=pool_size)
input_tensor = tf.random.uniform([3, 4, 10, 11, 12], dtype=tf.float32)
res = layer(input_tensor) # crash
```
Outputs:
```
Floating point exception (core dumped)
```
If the `pool_size` has negative values:
```
import tensorflow as tf
pool_size = [2, 2, -2]
layer = tf.keras.layers.MaxPooling3D(strides=1, pool_size=pool_size,)
input_tensor = tf.random.uniform([3, 4, 10, 11, 12], dtype=tf.float32)
res = layer(input_tensor)
print(res)
```
The output is a tensor with `shape`=`(3, 3, 9, 14, 12)` and all `inf` values."
51935,"""Could not append to the internal temporary file"" When preparing TFDS imagenet dataset on TPU-VM","(1) please update the bug template to include instructs to get TF version properly ```python3 -c ""import tensorflow as tf; print(tf.__version__)"" ```

vs. TF 2.0 instructions in this bug template: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

The former yields : 2.6.0

(2) 

**System information**
TPU-VM v3-8 with alpha 2.0 software and Colab Pro large memory instance.

This is a duplicate of [#37132](https://github.com/tensorflow/tensorflow/issues/37132) , but I believe the details were not flushed out hence I'm writing a new Issue.
I use something like:
```python
import tensorflow_datasets as tfds
source_dir='gs://MY_BUCKET/ILSVRC2012'
dest_dir='gs:/MY_BUCKET/ILSVRC2012_tf_records'
tfds.builder('imagenet2012',data_dir=dest_dir).download_and_prepare(
    download_config=tfds.download.DownloadConfig(
        manual_dir=source_dir))
```
In my use case I'm using TFDS and trying to write the TF records to a bucket using my TPU-VM (granted generously by TRC ❤️ which has 100GB as a root drive so 50-ish GB free space). half way through it completely fills the disk and the process of preparing the TF records fails.

This happens because of the default behavior for the GFile async writes, which is to store some tmp files in `/tmp` like:
```
tmp_file_tensorflow_539_EDt0Pl                                 tmp_file_tensorflow_994_lWEmew
tmp_file_tensorflow_540_RdDgOX                                 tmp_file_tensorflow_995_yaqZEX
tmp_file_tensorflow_541_rwyIRz                                 tmp_file_tensorflow_996_HDijKz
tmp_file_tensorflow_542_XF4R0b                                 tmp_file_tensorflow_997_bxYY9g
tmp_file_tensorflow_543_EVoGfO                                 tmp_file_tensorflow_998_HRem23
tmp_file_tensorflow_544_ebDRzq                                 tmp_file_tensorflow_999_tPD8y1
tmp_file_tensorflow_545_LK3B02                                 tmp_file_tensorflow_99_A7tVko
tmp_file_tensorflow_546_MoRHwF                                 tmp_file_tensorflow_9_GI4Quq
```
 with no ability to change it or override it IIUC, and TPU-VMs we can't edit `fstab` to mount another `/tmp` 😢 
and changing `dest_dir` to a [secondary disk](https://cloud.google.com/tpu/docs/setup-persistent-disk?hl=en) doesn't help as TFDS insists on writing to `/tmp` 
I tried to do this with Colab pro (which has 180-ish GB free space), managed to get away almost with the entire process and after 4h 47m 47s I get:
```
Downloading and preparing dataset imagenet2012/5.1.0 (download: Unknown size, generated: 155.84 GiB, total: 155.84 GiB) to /content/MY_MOUNTED_GCS_FUSE_BUCKET/ILSVRC2012_tf_records/imagenet2012/5.1.0...
Shuffling and writing examples to /content/MY_MOUNTED_GCS_FUSE_BUCKET/ILSVRC2012_tf_records/imagenet2012/5.1.0.incompleteROAOS5/imagenet2012-train.tfrecord
100%
1281166/1281167 [1:49:37<00:00, 305.35 examples/s]
Shuffling and writing examples to /content/MY_MOUNTED_GCS_FUSE_BUCKET/ILSVRC2012_tf_records/imagenet2012/5.1.0.incompleteROAOS5/imagenet2012-validation.tfrecord
100%
49999/50000 [12:36<00:00, 80.58 examples/s]
Shuffling and writing examples to /content/MY_MOUNTED_GCS_FUSE_BUCKET/ILSVRC2012_tf_records/imagenet2012/5.1.0.incompleteROAOS5/imagenet2012-test.tfrecord
100%
99999/100000 [19:05<00:00, 118.47 examples/s]
---------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
<ipython-input-4-08cf32753112> in <module>()
      4 tfds.builder('imagenet2012',data_dir=dest_dir).download_and_prepare(
      5     download_config=tfds.download.DownloadConfig(
----> 6         manual_dir=source_dir))

3 frames
/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py in download_and_prepare(self, download_dir, download_config)
    416           self.info.download_size = dl_manager.downloaded_size
    417           # Write DatasetInfo to disk, even if we haven't computed statistics.
--> 418           self.info.write_to_directory(self._data_dir)
    419     self._log_download_done()
    420 

/usr/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)
    117         if type is None:
    118             try:
--> 119                 next(self.gen)
    120             except StopIteration:
    121                 return False

/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/utils/py_utils.py in incomplete_dir(dirname)
    343   try:
    344     yield tmp_dir
--> 345     tf.io.gfile.rename(tmp_dir, dirname)
    346   finally:
    347     if tf.io.gfile.exists(tmp_dir):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py in rename_v2(src, dst, overwrite)
    622   """"""
    623   _pywrap_file_io.RenameFile(
--> 624       compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)
    625 
    626 

ResourceExhaustedError: /content/MY_MOUNTED_GCS_FUSE_BUCKET/ILSVRC2012_tf_records/imagenet2012/5.1.0.incompleteROAOS5; Too many open files

```
Both issues in my head look like there are too many temp files being created (without cleanup) that it either fills the root drive or trips a requests quota.

Thoughts, Ideas, Advice?

@skye FYI.

"
51934,how i build this custom layer in keras without error?,"I want build this custom Layer in keras to use it in sequential model. While running the code i am getting an error.can someone help me?? Below you can see the Code and the error.   

    class WeightedLayer(Layer):
      def __init__(self, n_input, n_memb, **kwargs):
          super(WeightedLayer, self).__init__( **kwargs)
          self.n = n_input   # 16 features
          self.m = n_memb    # 3

      def build(self, batch_input_shape):
          super(WeightedLayer, self).build(batch_input_shape)

      def call(self, input_):
          CP = []
          self.batch_size = tf.shape(input_)[0]
          for batch in tf.range(self.batch_size):
            xd_shape = [self.m]
            c_shape = [1]
            cp= input_[batch,0,:]
            for d in range(1,self.n):
                c_shape.insert(0,self.m)
                xd_shape.insert(0,1)
                xd = tf.reshape(input_[batch,d,:], (xd_shape))
                c = tf.reshape(cp,(c_shape))
                cp = tf.matmul(c , xd)

            flat_cp = tf.reshape(cp,(1, self.m**self.n))
            CP.append(flat_cp)
          return tf.reshape(tf.stack(CP), (self.batch_size, self.m**self.n))

      def compute_output_shape(self, batch_input_shape):
        return tf.TensorShape([self.batch_size, self.m ** self.n])

    X_train = np.random.uniform(0, 1, (200, 16, 3))
    X_test = np.random.uniform(0, 1, (200, 16, 3))
    y_train = np.random.uniform(0, 1, (200,))
    y_test = np.random.uniform(0, 1, (200,))

    Model = keras.models.Sequential()
    Model.add(WeightedLayer(n_input=16, n_memb=3,input_shape=(16, 3)))
    Model.compile(loss='mean_squared_error', optimizer='adam')
    Model.fit(X_train, y_train,epochs=20,batch_size=10,validation_data=(X_test, y_test))


I am getting a following error:

          /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
          693       except Exception as e:  # pylint:disable=broad-except
          694         if hasattr(e, 'ag_error_metadata'):
    --> 695           raise e.ag_error_metadata.to_exception(e)
          696         else:
          697           raise

    InaccessibleTensorError: in user code:

    <ipython-input-66-5c6ffada5c7a>:29 call  *
     return tf.reshape(tf.stack(CP), (self.batch_size, self.m**self.n))
     /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper  **
    return target(*args, **kwargs)
     /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:1424 stack
    return gen_array_ops.pack(values, axis=axis, name=name)
     /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py:6401 pack
    ""Pack"", values=values, axis=axis, name=name)
     /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper
    attrs=attr_protos, op_def=op_def)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:597 _create_op_internal
    inp = self.capture(inp)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:647 capture
    % (tensor, tensor.graph, self))

    InaccessibleTensorError: The tensor 'Tensor(""weighted_layer_37/while/Reshape_30:0"", shape=(1, 43046721), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=weighted_layer_37_while_body_44082, id=139893449741456); accessed from: FuncGraph(name=weighted_layer_37_scratch_graph, id=139893452012688)."
51933,How to edit Tensorflow SavedModel,"I have a Tensorflow model trained in Python, exported to a .pb file and then used with Tensorflow Serving.

I have written a custom op that greatly speeds up the inference of some operators in this Tensorflow model, but only works for inference -- I can't use this custom op during training time.

I am wondering if it's possible for me to use this custom op with the .pb file in Tensorflow serving. I figure I will probably have to edit the .pb file such that it uses my custom op in place of the original op, and Tensorflow serving should then go about looking for the custom op implementation which I can link against its runtime.

So -- how does one go about modifying a Tensorflow .pb file and swap out operators? Are there example codes doing this that I can refer to?

Please help. Thank you very much.
"
51932,Build on FreeBSD fails with protbuf related error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : FreeBSD
- TensorFlow installed from (source or binary): Source -
- TensorFlow version: 2.6
- Python version:3.8.11
- Installed using virtualenv? pip? conda?: compile 
- Bazel version (if compiling from source): 0.29
- GCC/Compiler version (if compiling from source): gcc10
- CUDA/cuDNN version: none
- GPU model and memory: n/a

**Protobuf version -3.17.3**


**Describe the problem**
During compile, I got the following error - I googled but did not find any related comments that could help me move forward. I did find one comment on gentoo where it was suggested the json file in questions needs to updated. Could not gather what exactly needs to be updated to get the build going.

[Gentoo issue link ](https://bugs.gentoo.org/800824)

Let me know if any additional info you need to support triage.

**ERROR: /usr/home/xxxx/Downloads/TensorFlow/FreeBSD-Tensorflow-master/science/py-tensorflow/work-py38/tensorflow-2.1.4/tensorflow/core/platform/BUILD:53:1: C++ compilation of rule '//tensorflow/core/platform:human_readable_json_impl' failed (Exit 1)
tensorflow/core/platform/default/human_readable_json.cc:36:29: error: no member named 'error_message' in 'google::protobuf::util::status_internal::Status'
   auto error_msg = status.error_message();
                    ~~~~~~ ^
tensorflow/core/platform/default/human_readable_json.cc:54:29: error: no member named 'error_message' in 'google::protobuf::util::status_internal::Status'
   auto error_msg = status.error_message();
                    ~~~~~~ ^
2 errors generated.****



**Provide the exact sequence of commands/steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
51931,Why is there no get in tensorflow version 2.3_ output_ Shape this function？,https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/engine/base_layer.py#L1463-L1480
51930,multiple tflite_runtime interpreters perform bad when loaded and invoked at the same time,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: YOCTO Linux
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: not installed (tflite runtime only)
-   **TensorFlow version (use command below)**: not installed (tflite runtime only)
-   **Python version**: 3.6
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:


### Describe the problem
I have a ARM64 board running YOCTO Linux equipped with a NPU and I have some quantized tflite models (one detection model and two classification models). I am testing a script that acquires images and makes inferences using the tflite_runtime module (with NNAPI delegate). Since the first inferences are slow, I introduced a warmup phase where I load the models and make an inference for each of them. However, this causes the models to produce wrong/unstable outputs during the normal functioning of the script. The same does not seem to happen when the interpreters are invoked at different times.

The detection model is a SSDMobileNetV2. The classification models are very simple custom architectures.

### Source code / logs

```
# relevant import
import tflite_runtime.interpreter as tflite

class DetectionModel(object):
	def __init__(self, path):
		self.interpreter = tflite.Interpreter(model_path=path)
		self.interpreter.allocate_tensors()
		self.input_details = self.interpreter.get_input_details()
		self.output_details = self.interpreter.get_output_details()
		self.input_shape = self.input_details[0]['shape']

	def predict(self, input_img):
		R, C, _ = input_img.shape
		img = cv2.resize(input_img, (self.input_shape[2], self.input_shape[1]))	
		if not self.input_details[0]['dtype'] == np.uint8:
			img = img.astype(np.float32)
			img = (img-128.0)/128.0
		img = np.expand_dims(img, 0)
		self.interpreter.set_tensor(self.input_details[0]['index'], img)			
		self.interpreter.invoke()			
		boxes = self.interpreter.get_tensor(self.output_details[0]['index'])
		classes = self.interpreter.get_tensor(self.output_details[1]['index'])
		scores = self.interpreter.get_tensor(self.output_details[2]['index'])

		return boxes, classes, scores
		
def initialize_classification_interpreter(path):
	interpreter = tflite.Interpreter(model_path=path)
	interpreter.allocate_tensors()
	input_details = interpreter.get_input_details()
	output_details = interpreter.get_output_details()

	input_shape = input_details[0]['shape']


	return interpreter, input_details, output_details

def classify(interpreter, input_details, output_details, img):
	
	input_shape = input_details[0]['shape']		

	img = (img/255.0).astype(np.float32)

	img = np.expand_dims(img, 0)
	interpreter.set_tensor(input_details[0]['index'], img)				
	interpreter.invoke()			
	output = interpreter.get_tensor(output_details[0]['index'])
	output = np.squeeze(output)

	return output

detection_model = DetectionModel(detection_path)
interpreter1, input_details1, output_details1 = initialize_classification_interpreter(classification_path_1)
interpreter2, input_details2, output_details2 = initialize_classification_interpreter(classification_path_2)

# warmup: this causes the issue
img = cv2.imread('test_img.jpg')
detection_model.predict(img)
classify(interpreter1, input_details1, output_details1, img)

cap = cv2.VideoCapture(0)

while cap.isOpened():
    ret, img = cap.read()
    # different conditions require different inferences
    if condition_1:
        result = detection_model.predict(img)
    if condition_2:
        result = classify(interpreter1, input_details1, output_details1, img)
    if condition_3:
        result = classify(interpreter2, input_details2, output_details2, img)

    # other operations...


```"
51929,Semantic segmentation node in ros node opening with model ,"2021-09-10 16:25:17.632603: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2
2021-09-10 16:25:28.226278: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-09-10 16:25:28.235986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-09-10 16:25:28.236470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: 
pciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2
coreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s
2021-09-10 16:25:28.236749: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2
2021-09-10 16:25:28.237093: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.10
2021-09-10 16:25:28.237299: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.10
2021-09-10 16:25:28.237466: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2021-09-10 16:25:28.239481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2021-09-10 16:25:28.245080: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10
2021-09-10 16:25:28.249723: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.10
2021-09-10 16:25:28.250319: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-09-10 16:25:28.251087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-09-10 16:25:28.251756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-09-10 16:25:28.251971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1872] Adding visible gpu devices: 0
2021-09-10 16:25:28.252321: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2
2021-09-10 16:25:30.678805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-09-10 16:25:30.679001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-09-10 16:25:30.679108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-09-10 16:25:30.679730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-09-10 16:25:30.680232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-09-10 16:25:30.680718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero
2021-09-10 16:25:30.680948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 18146 MB memory) -> physical GPU (device: 0, name: Xavier, pci bus id: 0000:00:00.0, compute capability: 7.2)
2021-09-10 16:25:30.837477: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 31250000 Hz
2021-09-10 16:25:32.770741: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-09-10 16:25:34.113545: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Loaded runtime CuDNN library: 8.0.0 but source was compiled with: 8.2.1.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2021-09-10 16:25:34.122043: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Loaded runtime CuDNN library: 8.0.0 but source was compiled with: 8.2.1.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2021-09-10 16:25:34.129232: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Loaded runtime CuDNN library: 8.0.0 but source was compiled with: 8.2.1.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2021-09-10 16:25:34.131259: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.10
2021-09-10 16:25:35.282734: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops.cc:1344 : Not found: No algorithm worked!
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1360, in _run_fn
    target_list, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1453, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node MobilenetV2/Conv/Conv2D}}]]
	 [[ExpandDims_2/_19]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node MobilenetV2/Conv/Conv2D}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./segmentation_node"", line 43, in <module>
    semantic = model.infer([kr.imgmsg_to_cv2(on_image.last_image)])[0]
  File ""/home/agx1/radar/src/ros-semantic-segmentation/semantic_segmentation/nodes/models/mnv2_bdd100k_driveable_513/__init__.py"", line 66, in infer
    feed_dict = { INPUT_TENSOR_NAME: images }
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 968, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1369, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node MobilenetV2/Conv/Conv2D (defined at /home/agx1/radar/src/ros-semantic-segmentation/semantic_segmentation/nodes/models/mnv2_bdd100k_driveable_513/__init__.py:34) ]]
	 [[ExpandDims_2/_19]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node MobilenetV2/Conv/Conv2D (defined at /home/agx1/radar/src/ros-semantic-segmentation/semantic_segmentation/nodes/models/mnv2_bdd100k_driveable_513/__init__.py:34) ]]
0 successful operations.
0 derived errors ignored.

Original stack trace for 'MobilenetV2/Conv/Conv2D':
  File ""./segmentation_node"", line 33, in <module>
    model = getattr(__import__('models', globals(), locals(), fromlist = [MODEL]), MODEL).Model()
  File ""/home/agx1/radar/src/ros-semantic-segmentation/semantic_segmentation/nodes/models/mnv2_bdd100k_driveable_513/__init__.py"", line 34, in __init__
    tf.import_graph_def(self.graph_def, name='')
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 535, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py"", line 513, in _import_graph_def_internal
    _ProcessNewOps(graph)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py"", line 243, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3709, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3709, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3590, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)

"
51928,C++ compile tensorflow r2.5,"tensorflow_cc.dll.if.exp : error LNK2001: 无法解析的外部符号 ""public: __cdecl tensorflow::TensorShapeBase<class tensorflow::TensorShape>::TensorShapeBase<class tensorflow::TensorShape>(class absl::lts_2020_02_25::Span<__int64 const >)"" (??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@V?$Span@$$CB_J@lts_2020_02_25@absl@@@Z)
bazel-out\x64_windows-opt\bin\tensorflow\tensorflow_cc.dll : fatal error LNK1120: 1 个无法解析的外部命令
Target //tensorflow:tensorflow_cc.dll failed to build
INFO: Elapsed time: 28.003s, Critical Path: 25.68s
INFO: 4 processes: 2 internal, 2 local.
FAILED: Build did NOT complete successfully
"
51927,Why can't you perform additional operations on the loss function under tf.gradienttape?,"tf：2.2
env：linux
this is a **Triplet extraction model**
subject_labels is normal:   (batch_size,  max_len,  2)
object_labels is : (batch_size, max_len, len(predicate), 2)
For object_ Model I need to overwrite the padding part and only modify the text part。

However, after I perform the overwrite operation, although the obtained l**oss value** has no problem in the form, it is put into tf.gradienttape, and the weight value of Bert model cannot be obtained

When my loss function is as follows，Only TF related APIs are used:
`
def loss4model(t, p, l, obj=False):
    if obj:
        loss_value = tf.keras.losses.binary_crossentropy(y_true=t, y_pred=p)
        loss_value = tf.reduce_mean(loss_value, axis=2)
    else:
        loss_value = tf.keras.losses.binary_crossentropy(y_true=t, y_pred=p)

    loss_value = tf.reduce_mean(loss_value)

    return loss_value
`

When the model performs the following operations（in the tf.GradientTape）, it is normal and the loss function is normal
![1631267664(1)](https://user-images.githubusercontent.com/34124260/132836145-3ab467d2-e009-467d-9d9e-ce0570f06b47.png)
like this：
![1631268153(1)](https://user-images.githubusercontent.com/34124260/132837171-dcf8277e-f279-4ca1-ad41-32ffe796ad03.png)

But when I need to do some additional operations on the loss function, for example
`
def loss4model(t, p, l, obj=False):
    """"""I need to overwrite the padding part and only lose the text par
    """"""
    if obj:
        loss_value = tf.keras.losses.binary_crossentropy(y_true=t, y_pred=p)
        loss_value = tf.reduce_mean(loss_value, axis=2)
    else:
        loss_value = tf.keras.losses.binary_crossentropy(y_true=t, y_pred=p)
        
    loss_values = []
    for num, weights in enumerate(loss_value):
       # Get the loss value within the specified range through the length of TF. Gather() and len(text)
        indices = np.arange(l[num])
        loss = tf.gather(weights, indices)
        loss = tf.reduce_mean(loss)
        loss_values.append(loss.numpy())

    loss_value = tf.reduce_mean(tf.constant(loss_values))

    return loss_value
`
![1631267758(1)](https://user-images.githubusercontent.com/34124260/132836279-2dcc5c49-85d4-4f5e-af6c-88cf6ad8dd28.png)

You can see that the **loss value is a normal tensor**, but I **can't get the weight value** of the model as above，the weight is None：
![1631267805(1)](https://user-images.githubusercontent.com/34124260/132836401-95a9f696-49a8-4f02-9866-3c2b2b3bb27e.png)


I tried to define a loss directly in the model llike this
![1631268227(1)](https://user-images.githubusercontent.com/34124260/132837321-8134abbc-5b26-4520-aad0-a0604ab703c1.png)
it cant help
![1631268249(1)](https://user-images.githubusercontent.com/34124260/132837364-1dca5332-3abb-4ad5-90bf-de9f7ad79aba.png)

so, i want to know, loss_value is just a tf.tensor, Why can it affect the weight update and acquisition of the model?
and How can I still make loss work after I operate it?
"
51926,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.,"Hello,
I ran this comand in Conda:

(style-transfer) E:\Python\fast-style-transfer>python evaluate.py --checkpoint ./rain-princess.ckpt --in-path <E:\Python\fast-style-transfer\arnius_gali.jpg> --out-path ./arnius_gali2.jpg

Got this error message:

Traceback (most recent call last):
  File ""C:\Users\User\anaconda3\envs\style-transfer\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:\Python\fast-style-transfer\evaluate.py"", line 4, in <module>
    import transform, numpy as np, vgg, pdb, os
  File ""E:\Python\fast-style-transfer\src\transform.py"", line 1, in <module>
    import tensorflow as tf, pdb
  File ""C:\Users\User\anaconda3\envs\style-transfer\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\User\anaconda3\envs\style-transfer\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\User\anaconda3\envs\style-transfer\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\User\anaconda3\envs\style-transfer\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\User\anaconda3\envs\style-transfer\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\User\anaconda3\envs\style-transfer\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

How can I find out what I did wrong and how to fix it?

Thank you"
51925,build pip package failed when setting --config=c++17_gcc,"**System information**
- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installed from source
- TensorFlow version: 2.6.0
- Python version: 3.8.10
- Bazel version: 3.7.2
- GCC/Compiler version: 9.3.0-17ubuntu1~20.04

**Describe the problem**
build failed by 
```
ERROR: /home/ubuntu/tensorflow/tensorflow/python/keras/api/BUILD:147:19: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)
Traceback (most recent call last):
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeEN4absl12lts_2021032411string_viewEOSt6vectorINS_10StackFrameESaIS7_EE
```
**build commands**
```
bazel -s --config=c++17_gcc --copt=-march=native --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package
```
The target `//tensorflow:libtensorflow_cc.so` build finished and worked, so I guess there are some bugs in building the python library not properly setting the c++17 flag. 

"
51922,Sub-classed constraint doesn't appear to be called,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): A proprietary variant of RedHat, outside of my control
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7.2
- CUDA/cuDNN version: 11.1.0/8.1.1
- GPU model and memory: Nvidia K40 12GB

**Describe the current behavior**
I have created a sub-classed layer that performs a weighted sum of the outputs of two sub_models.
In this layer, I create weights with the `add_weight` function which includes the use of a constraint.

The intention of the constraint is to normalize the weights of each input dimension/feature (i.e. sum to one).

I have created a custom weight normalization constraint which is sub-classed from `krs.constraints.Constraint`. The output weights from the constraint function for model consisting of 2 sub-models with each producing a 200 dimension/feature output. Thus, the input to the constraint would be 2 x 200 weight tensor and the columnwise sums should all be 1.0.

When providing an instance of my constraint class to the `add_weight` function, it appears that there it is never called. It is initialized though. When running `VectorWeightedSum_1` below the `tf.print` statement in the call function shows that the weights are never normalized. Also, the `tf.print` statement in `NormalizeSumWeights.call()` doesn't ever produce an output. (It also doesn't exit the program if I put a `sys.exit()` call inside the contraint's `call`)

If I instead use `VectorWeightedSum_2`, the `tf.print` statement demonstrates that weights are indeed normalized properly. This was done by creating a new function inside the `VectorWeightedSum_2` class as opposed to a constraint class. The line for this code is commented out in the code below.

**Describe the expected behavior**
The sub-classed constraint should have it's `call` function called.

The weights produced by the sub-classed constraint should sum to 1.0 along each column.

In the example code, the `tf.print` statements should be printed for both `_constraint` and the use of the `NormalizeSumWeights` class.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
import tensorflow.keras as krs

class NormalizeSumWeights(krs.constraints.Constraint):
    def __init__(self,**kwargs):
        super(NormalizeSumWeights,self).__init__(**kwargs)

    def call(self,w):
        tf.print(""NormalizeSumWeights_________________________________"")
        return tf.math.divide(w,tf.math.reduce_sum(w,axis=0))

class WeightedSum(krs.layers.Layer):
    def __init__( self, n_models = 2, **kwargs):
        super( WeightedSum, self ).__init__( **kwargs)
        self.n_models = n_models

    def build(self,input_shape):
        self.sum_weights = self.add_weight(name=""sum_weights"",shape=(self.n_models,1),
                                    initializer = krs.initializers.RandomUniform(0,1.0),
                                    constraint = NormalizeSumWeights(),
                                    trainable = True)

    def call(self,inputs):
        output = tf.multiply(tf.cast(self.sum_weights[0,:],inputs[0].dtype),inputs[0])
        for i in range(1,len(inputs)):
            output += tf.multiply(inputs[i],tf.cast(self.sum_weights[i,0],inputs[0].dtype))

        return output

    def get_config(self):
        data = { ""n_models"": self.n_models}
        return data

class VectorWeightedSum_1(krs.layers.Layer):
    def __init__( self, n_models = 2,n_dimensions=200, **kwargs):
        super( VectorWeightedSum_1, self ).__init__( **kwargs)
        self.n_models = n_models
        self.n_dimensions = n_dimensions

    def build(self,input_shape):
        self.sum_weights = self.add_weight(name=""sum_weights"",shape=(self.n_models,self.n_dimensions),
                                    initializer = krs.initializers.RandomUniform(0,1.0),
                                    constraint = NormalizeSumWeights(),
                                    trainable = True)
    def _constraint(self,w):
        tf.print(""Weights Normalization__________________________"")
        return tf.math.divide(w,tf.math.reduce_sum(w,axis=0))

    def call(self,inputs):
        output = tf.multiply(tf.cast(self.sum_weights[0,:],inputs[0].dtype),inputs[0])

        for i in range(1,len(inputs)):
            output += tf.multiply(inputs[i],tf.cast(self.sum_weights[i,:],inputs[0].dtype))
        tf.print(""vectorWeightedSum___weights________________"")
        tf.print(self.sum_weights)

        return output

class VectorWeightedSum_2(krs.layers.Layer):
    def __init__( self, n_models = 2,n_dimensions=200, **kwargs):
        super( VectorWeightedSum_2, self ).__init__( **kwargs)
        self.n_models = n_models
        self.n_dimensions = n_dimensions

    def build(self,input_shape):
        self.sum_weights = self.add_weight(name=""sum_weights"",shape=(self.n_models,self.n_dimensions),
                                    initializer = krs.initializers.RandomUniform(0,1.0),
                                    constraint = self._constraint,
                                    trainable = True)
    def _constraint(self,w):
        tf.print(""Weights Normalization__________________________"")
        return tf.math.divide(w,tf.math.reduce_sum(w,axis=0))

    def call(self,inputs):

        output = tf.multiply(tf.cast(self.sum_weights[0,:],inputs[0].dtype),inputs[0])

        for i in range(1,len(inputs)):
            output += tf.multiply(inputs[i],tf.cast(self.sum_weights[i,:],inputs[0].dtype))
        tf.print(""vectorWeightedSum___weights________________"")
        tf.print(self.sum_weights)

        return output

input_lf = krs.Input((4,))

x = input_lf
x = krs.layers.Dense(10,activation = 'relu')(x)
x = krs.layers.Dense(10,activation = 'relu')(x)
lf_out = krs.layers.Dense(200,activation = 'relu')(x)

lf_mod = krs.Model(input_lf,lf_out,name='lf')

input_hf_lin = krs.Input((204,))
x = input_hf_lin
x = krs.layers.Dense(10)(x)
x = krs.layers.Dense(10)(x)
hf_lin_out = krs.layers.Dense(200,activation = 'relu')(x)

hf_lin_mod = krs.Model(input_hf_lin,hf_lin_out,name='hf_linear')

input_hf_nonlin = krs.Input((14,))

x = input_hf_nonlin
x = krs.layers.Dense(10,activation = 'relu')(x)
x = krs.layers.Dense(10,activation = 'relu')(x)
hf_nonlin_out = krs.layers.Dense(200,activation = 'relu')(x)

hf_nonlin_mod = krs.Model(input_hf_lin,hf_lin_out,name='hf_nonlinear')

input_hf = krs.Input((204,))

x = input_hf
lin = hf_lin_mod(x)
nonlin = hf_nonlin_mod(x)
summed_out = VectorWeightedSum_1(n_models=2)([lin,nonlin])
#summed_out = VectorWeightedSum_2(n_models=2)([lin,nonlin])

hf_mod = krs.Model(input_hf,summed_out,name='hf')

input_full_mod = krs.Input((4,))
x = input_full_mod

low = lf_mod(x)
x = krs.layers.Concatenate()([low,x])
full_out = hf_mod(x)

full_mod = krs.Model(input_full_mod,outputs = {'low_fidelity':low,'high_fidelity':full_out},name='full_model')

opt = krs.optimizers.Adam()
loss = krs.losses.MSE
full_mod.compile(optimizer = opt,loss = loss)

x_train = np.random.uniform(0,10,(20,4))
y_train_low = np.random.uniform(0,10,(20,200))
y_train_high = np.random.uniform(0,10,(20,200))
y = {""low_fidelity"": y_train_low,
     ""high_fidelity"": y_train_high}

full_mod.fit(x_train,y,epochs=5,batch_size=1)
```
"
51921,How can I the outputs of tf_logging  on a TestCase,"I write a `test.TestCase` file, and i could't find the outputs of tf_logging.

code:
```
from tensorflow.python.platform import tf_logging
import logging
import sys
tf_logging.set_verbosity(tf_logging.INFO)
logger = logging.getLogger(""tensorflow"")
if len(logger.handlers) == 1:
  logger.handlers = []
  logger.setLevel(logging.INFO)
  formatter = logging.Formatter(
      ""%(asctime)s - [%(filename)s:%(lineno)d] - %(name)s - %(levelname)s - %(message)s"")
  ch = logging.StreamHandler(sys.stdout)
  ch.setLevel(logging.INFO)
  ch.setFormatter(formatter)
  logger.addHandler(ch)

class PipelineTest(test.TestCase):
  def __init__(self, method_name=""runTest""):
    super(PipelineTest, self).__init__(method_name)

  def testBuild(self):
      tf_logging.info(""Output"")
```

compile command:
`
bazel test --test_arg=--v=3 --test_output=streamed //tensorflow/python:pipeline_embedding_ops_test
`
But I couldn't find the output of ""logging.info(""Output"")"".

throw a error : No handlers could be found for logger ""tensorflow"""
51915,ModuleNotFoundError: No module named 'tensorflow.python.eager',"

**System information**
-  I wrote a custom code
- Windows 10
- TensorFlow installed from source
- TensorFlow version : 2.6.0
- Python version:3.9


**Describe the current behavior**

When i run the code, the following error appears :
from tensorflow.python.eager import context
ModuleNotFoundError: No module named 'tensorflow.python.eager'

**Describe the expected behavior**

This error shouldn't happen. The tensorflow was correct installed. I also tried to re-install it, and the same error continue.


**Standalone code to reproduce the issue**
This is a load error, i cant run any tensorflow code.

**Other info / logs** 
  from tensorflow.python.eager import context
ModuleNotFoundError: No module named 'tensorflow.python.eager'
"
51913,Attention and AdditiveAttention Layers not well documented.,"The attention layers (Attention and AdditiveAttention Layers) are not well documented. For example, I am able to pass a 4D input without issue when the current documentation say we should pass a 3D input.

What does it mean when I pass 4D input? Please provide more examples.

Thank you!

Nektarios


## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
51911,TensorFlow throws an exception when loading a model that was normalized like this.,"See my stackoverflow question that covers up everything:
https://stackoverflow.com/questions/69112466/why-tensorflow-throws-this-exception-when-loading-a-model-that-was-normalized-li"
51909,crash in (loading?) libtensorflow_framework.so.2,"**System information**
- No custom code - just loading
- Ubuntu 20.0.4
- Intel(R) Atom(TM) CPU D2500   @ 1.86GHz
- Installed from Ubuntu release binary apt-get
- Cannot determine (see below)
- Python version: 3.8
- CUDA/cuDNN version: None
- GPU model and memory: None
- Everything very ""stock"" - new, clean ground-up install from binary repos
- CPU flags:  `fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts nopl nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl tm2 ssse3 cx16 xtpr pdcm movbe lahf_lm dtherm arat`

Any attempt to load TensorFlow Python module, or run `tensorboard` (no parameters - no files) results in Illegal Instruction crash.

Both crashes appear to be within `libtensorflow_framework.so.2` - possibly calling into `librt-2.31.so`

Some sort of ""python"" core dump for `import tensorflow` here:
[https://s3.amazonaws.com/s3.bradgoodman.com/_usr_bin_python3.8.0.crash](https://s3.amazonaws.com/s3.bradgoodman.com/_usr_bin_python3.8.0.crash)

Standard Core dump for `tensorboard` crash here:
[https://s3.amazonaws.com/s3.bradgoodman.com/tensorboard.core](https://s3.amazonaws.com/s3.bradgoodman.com/tensorboard.core)
"
51908,tf.pad crashes with large paddings,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
`tf.pad` crashes when the argument ""paddings"" has large values.

**Describe the expected behavior**
Expect an exception to be thrown if the input `paddings` is unexpected.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
input_tensor = tf.random.uniform([1, 32, 32, 3], dtype=tf.float32)
paddings = [[125106557, 1415887920], [747509374, 2136925906], [413308538, 904601717], [1900762018, 831358864]]
res = tf.pad(input_tensor,paddings)
```
outputs:
```
2021-09-09 12:46:38.123113: F tensorflow/core/framework/tensor_shape.cc:352] Check failed: 0 <= new_num_elements (0 vs. -1)
Aborted (core dumped)
```
"
51907,Failed to load the native TensorFlow runtime,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.9.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.2.1
- Python version: 3.6.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

I have created a conda env with py 3.6.4 installing several dependencies. I wanted to install tensorflow 2.2.0rc4 but it was not available, instead I did 2.2.1. I got errors from gprcio saying:

```
Building wheels for collected packages: grpcio
  Building wheel for grpcio (setup.py) ...
ERROR: Command errored out with exit status 1:
```
And a huge amount of other messages. Needless to say tf did not install.

I thought I found a workaround by first installing with

`pip3 install --no-cache-dir  --force-reinstall -Iv grpcio==1.24.3`

This succesfully installed and allowed me to succesfully install tf 2.2.1. Or so I thought.

When I try import tensorflow I get these errors

`Traceback (most recent call last):
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /Users/nsabherwal/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.`

**Describe the expected behavior**

Expect simple install and running of tensorflow.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
51906,Cannot find tensorflow 2.2.0rc4 as available option with pip install,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.9.5 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version: 2.2.0rc4
- Python version: 3.6.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 



**Describe the problem**

I am installing a programme which requires a conda env with tensorflow 2.2.0rc4. When I do this it says no matching distribution can be found.

ERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0rc4 (from versions: 1.15.4, 1.15.5, 2.0.1, 2.0.3, 2.1.2, 2.2.1, 2.3.1)
ERROR: No matching distribution found for tensorflow==2.2.0rc4

**Provide the exact sequence of commands / steps that you executed before running into the problem**

pip install tensorflow==2.2.0rc4


How can I install this version?"
51904,Convert Tensorflow functions to Tensorflow Lite functions,"### 1. System information

- Linux
- TensorFlow version: 2.5.0, also TFLite 2..5.0

### 2. Code
`    masks = tf.image.crop_and_resize(tf.cast(roi_masks, tf.float32), boxes, box_ids, config.MASK_SHAPE)`

Hi there,
I am currently trying to convert the Mask R CNN model from Matterport/Leekunhee (https://github.com/leekunhee/Mask_RCNN) to Tensorflow Lite. I fixed the problem temporarily by allowing Select Ops for the tensorflow lite interpreter. Now I need to speed up the entire process of inference. If I remember correctly last time I got this error message:

> RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 275 (FlexCropAndResize) failed to prepare.

Can anybody help me translate this one line of code to something the tflite Interpreter and converter will accept? Any workaround is appreciated.

Thank you all for reading."
51902,val_sample_weight not used in calculation of custom validation metrics,"**System information**
-  Custome code using standard keras and tfa.metrics
-  Operating System: CentOS Linux 7 (Core), Kernel: Linux 3.10.0-1160.el7.x86_64
- Tensorflow installed with pip install tf-nightly-gpu
- v1.12.1-63317-g034300c177d 2.7.0-dev20210907
- NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3
- A100-40GB

**Describe the current behavior**
All validation metrics other than loss (set in model.compile(metrics=['accuracy', 'another_metrics'])) and reported during model.fit(......,val_data=(x_val, y_val, val_sample_weight)) do not seem to take the val_sample_weight into consideration. The loss metrics works as expected. 
 
**Describe the expected behavior**
The val_sample_weight associated with the x_val and y_val should be used in the calculation of 'accuracy' and other metrics such as tfa.metrics.CohenKappa. This is important for early stopping of model based on validation metrics. 

**[Contributing](https://www.tensorflow.org/community/contribute)**
- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
import os 
import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa

inputs = tf.keras.Input(shape=(10))
x = tf.keras.layers.Dense(100, activation='relu')(inputs)
x = tf.keras.layers.Dense(10, activation='relu')(x)
outputs = tf.keras.layers.Dense(3, activation='softmax')(x)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
model.compile('adam', loss='sparse_categorical_crossentropy', metrics=['accuracy', tfa.metrics.CohenKappa(num_classes=3, sparse_labels=True)], sample_weight_mode='temporal')
model.summary()
X =tf.random.uniform(shape=[100,10], minval=0, maxval=1, dtype=tf.float32)
Y = tf.random.uniform(shape=[100,1], minval=0, maxval=3, dtype=tf.int64)
W = tf.ones([100])
W2 = tf.zeros([100])
model.fit(X, Y, sample_weight=W, validation_data=(X,Y,W2), epochs=5, verbose=1)
probs = model.predict(X)
kappa = tfa.metrics.CohenKappa(num_classes=3, sparse_labels=True)
kappa.update_state(Y, probs)
print(""Kappa Without Weights"", kappa.result())
kappa = tfa.metrics.CohenKappa(num_classes=3, sparse_labels=True)
kappa.update_state(Y, probs,W2)
print(""Kappa With Weights"", kappa.result())

**Other info / logs** Include any logs or source code that would be helpful to
>>>W2 is all zeros. Notice that the val_loss is 0.000 as expected, but the val_accuracy and val_kappa are not. See prints the end showing kappa behavior with and without weights.   

Epoch 1/5
4/4 [==============================] - 1s 215ms/step - loss: 1.0857 - accuracy: 0.4000 - cohen_kappa: -0.0130 - val_loss: 0.0000e+00 - val_accuracy: 0.3900 - val_cohen_kappa: -0.0281
Epoch 2/5
4/4 [==============================] - 0s 31ms/step - loss: 1.0766 - accuracy: 0.4000 - cohen_kappa: -0.0096 - val_loss: 0.0000e+00 - val_accuracy: 0.4000 - val_cohen_kappa: -0.0052
Epoch 3/5
4/4 [==============================] - 0s 30ms/step - loss: 1.0736 - accuracy: 0.3900 - cohen_kappa: -0.0214 - val_loss: 0.0000e+00 - val_accuracy: 0.4000 - val_cohen_kappa: -0.0059
Epoch 4/5
4/4 [==============================] - 0s 31ms/step - loss: 1.0706 - accuracy: 0.4000 - cohen_kappa: -0.0037 - val_loss: 0.0000e+00 - val_accuracy: 0.4000 - val_cohen_kappa: -0.0059
Epoch 5/5
4/4 [==============================] - 0s 29ms/step - loss: 1.0666 - accuracy: 0.4000 - cohen_kappa: -0.0081 - val_loss: 0.0000e+00 - val_accuracy: 0.4200 - val_cohen_kappa: 0.0213
Kappa Without Weights tf.Tensor(0.021262169, shape=(), dtype=float32)
Kappa With Weights tf.Tensor(0.0, shape=(), dtype=float32)
"
51901,TF 2.5 Asks for nightly build,"
Now that Keras 2.5 and 2.6 have been released, can we update the setup.py to a stable Keras version? It causes errors when trying to use package managers such as poetry since they rely on pip. Thanks!

https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/tools/pip_package/setup.py#L107"
51900,Multi gpus can not run parallel when using run_eagerly=True in model.fit() in MultiWorkerMirroredStrategy,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: 11.3.0/8.2.1
- GPU model and memory: Tesla P100(16GB)

My code is very simple:
```

import tensorflow as tf
import tensorflow_datasets as tfds
import os
import json

gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

num_epochs = 50
batch_size_per_replica = 32
learning_rate = 0.001

num_workers = 1
os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [""172.37.2.20:8888""]
    },
    'task': {'type': 'worker', 'index': 0}
})
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
print('~~' * 10)
print('Number of devices: %d.' % strategy.num_replicas_in_sync)
print('~~' * 10)
batch_size = batch_size_per_replica * num_workers * strategy.num_replicas_in_sync


def resize(image, label):
    image = tf.image.resize(image, [224, 224]) / 255.0
    return image, label


dataset = tfds.load(""cats_vs_dogs"", split=tfds.Split.TRAIN, as_supervised=True)
dataset = dataset.map(resize).shuffle(1024).batch(batch_size)

with strategy.scope():
    model = tf.keras.applications.MobileNetV2(weights=None, classes=2)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=[tf.keras.metrics.sparse_categorical_accuracy],
        run_eagerly=True
    )

model.fit(dataset, epochs=num_epochs)
```


**Describe the current behavior**
If I remove the ` run_eagerly=True` in model.compile, the multi gpus will run parallel. But after add this kwarg to make the model is runing in eager mode, the multi gpus will run one by one.

**Describe the expected behavior**
The parameter should not determine the gpu running mode.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
51899,BatchNormalization,"I am trying to understand how BatchNormalization works. With code:
`x=tf.constant([2.,3.,4.])`
`x=keras.layers.BatchNormalization()(x)`
`print(x)`
I get response: 
`tf.Tensor([1.9990008 2.9985013 3.9980016], shape=(3,), dtype=float32)`
Which is not what i expect since it doesn't have mean 0 or variance 1. Can somebody explain?"
51898,the @tf.function annotation weird performance,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Ubuntu 18.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- binary
- TensorFlow version (use command below):
- Python version:
- Python 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

tf version 2.6.0

**Describe the current behavior**

tf version 2.6

when the model call function has no @tf.function annotation like this:
![image](https://user-images.githubusercontent.com/7404433/132669008-2043ccca-c588-4809-bbd9-e8255d0b3342.png)

train the model the loss and auc log is like this:

![image](https://user-images.githubusercontent.com/7404433/132669307-b105084e-ec5a-4084-bf21-fe87a65feac2.png)

as you can see from the picture,at epoch start the loss value is high, after epoch end the loss value is low,and auc >0.5
![image](https://user-images.githubusercontent.com/7404433/132669640-23a22f16-ec0c-4920-8780-204eb859edc2.png)

**All the code remains unchanged, just add @tf.function annotation like this:** 
![image](https://user-images.githubusercontent.com/7404433/132669988-e8725067-e4e0-40cd-b701-ae38c0b555d2.png)

train the model the loss and auc log is like this:
![image](https://user-images.githubusercontent.com/7404433/132670270-689f4e48-132a-4462-9c4d-ba891d91e125.png)

as you can see from the picture,at epoch start the loss value is different ,Its value is much lower，and after epoch end the loss value **is optimizer much lower, but the auc is always 0.5**

![image](https://user-images.githubusercontent.com/7404433/132670962-4b2c520b-6252-472a-a3ca-374dbb312c0d.png)

it is too confusing!!!

**Describe the expected behavior**

I think their results should be exactly the same，

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

The code needed to reproduce this problem：

Just modify the @tf.function annotation (add or remove) reproduce this problem

![image](https://user-images.githubusercontent.com/7404433/132793559-66cc46cc-1544-4330-9089-aef2d80ab6bb.png)


https://colab.research.google.com/drive/1i6PnUrkvNszW03wnS6ZyQBsrI1HYnTpq?usp=sharing



**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.



"
51897,official guide on developer.apple.com/metal/tensorflow-plugin fails to build grpcio,"Apple Silicon steps it fails to install base tensorflow:

````
python -m pip install tensorflow-macos
````

with the following error message:

````

Building wheels for collected packages: grpcio
  Building wheel for grpcio (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /Users/sascha/miniforge3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/bj/kl9gdt8145n_pgcjlm9j4_x00000gn/T/pip-install-b32jlj8u/grpcio_6d3ece3d284848398f436707643d6149/setup.py'""'""'; __file__='""'""'/private/var/folders/bj/kl9gdt8145n_pgcjlm9j4_x00000gn/T/pip-install-b32jlj8u/grpcio_6d3ece3d284848398f436707643d6149/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/bj/kl9gdt8145n_pgcjlm9j4_x00000gn/T/pip-wheel-efohlhk7
       cwd: /private/var/folders/bj/kl9gdt8145n_pgcjlm9j4_x00000gn/T/pip-install-b32jlj8u/grpcio_6d3ece3d284848398f436707643d6149/
````

What is the proposed solution?

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac M1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): https://developer.apple.com/metal/tensorflow-plugin/
- TensorFlow version: unknown as they is no 
- Python version: https://pypi.org/project/tensorflow-macos/ 2.5
- Installed using virtualenv? pip? conda?: miniforge3 https://developer.apple.com/metal/tensorflow-plugin/
- Bazel version (if compiling from source): ?
- GCC/Compiler version (if compiling from source): ?
- CUDA/cuDNN version: `
- GPU model and memory:?"
51896,Tensorflow does not work with python -OO because of __doc__ usage,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): any
- TensorFlow installed from (source or binary): pip wheel
- TensorFlow version (use command below): 2.6.0
- Python version: 3.9

**Describe the current behavior**

Tensorflow crashes when being executed in a context where python was started with the [`-OO`](https://docs.python.org/3/using/cmdline.html#cmdoption-oo) flag. This happens because tensorflow is dynamically changing [`__doc__`](https://github.com/tensorflow/tensorflow/search?q=__doc__) variables during runtime and all of these variables become `None` when `python -OO` is used. An exception will be raised whenever a source file is loaded which tries to modify a `__doc__` variable, usually something along the lines of

    TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'

which is raised by e.g. [/tensorflow/python/ops/array_ops.py#L452](https://github.com/tensorflow/tensorflow/blob/1f0c1d8ec778b029028a27a47a89acf6a93637ad/tensorflow/python/ops/array_ops.py#L452)

    listdiff.__doc__ = gen_array_ops.list_diff.__doc__ + ""\n"" + listdiff.__doc__

but there are lots of other places in the code where this can happen.

**Describe the expected behavior**

Tensorflow should play nicely with `python -OO`. All code that modifies [`__doc__`](https://github.com/tensorflow/tensorflow/search?q=__doc__) variables during runtime would require `None` checks.

**Standalone code to reproduce the issue**

`python -OO -c ""from tensorflow.python.ops import array_ops""` fails with

    TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'

But `python -O -c ""from tensorflow.python.ops import array_ops""` works fine, because [`-O`](https://docs.python.org/3/using/cmdline.html#cmdoption-o) does not discard docstrings. I would expect tensorflow to work with both `-O` and `-OO`.

*edit*: updated code to reproduce"
51895,tf.nn.max_pool_with_argmax to support multiple dimensions (ND) ,"**System information**
- TensorFlow version (you are using): 2.x
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Current Behavior:

Currently, there is a function called [tf.nn.max_pool_with_argmax](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax). 

Feature Request:

However, it would be great if the function supported several dimensions, not only 2D images.  [`tf.nn.max_pool`](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) for example supports any dimensionality. 


**Will this change the current api? How?**

I think the easiest will be to implement a new method like `tf.nn.max_pool_with_argmax_nd` not to have compatibility issues. 
Because it is a new method, this will also enable the fixing of the `tf.nn.max_pool_with_argmax` bug:

> This is a bug, but fixing it is difficult to do in a safe backwards compatible way, especially due to flattening.

**Who will benefit with this feature?**

This function is useful to, for example, implement ND [MaxUnpooling](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/6476c4d5-19f2-455f-8590-c6f99504b7a5.xhtml).

In particular, it will help me implement the [feature request](https://github.com/NEGU93/cvnn/issues/10) asked on my repository.

**Any Other info.**

Correct formatting of [this](https://github.com/tensorflow/tensorflow/issues/51787) feature request.
"
51892,how to barrier when run distribution trainning by estimator api,"When run distribution trainning by tf.estimator api ，other workers will be hanging when one worker finished training.So i add sleep function below estimator funtion.But it seemed tricky this way.
```
tf.estimator.train_and_evaluate(self._tf_estimator, train_spec, eval_spec)
 time.sleep(300)

```
Any other good ways to fix this estimator issue?
TF version is 1.14"
51889,CUBLAS_STATUS_INVALID_VALUE when using `tf.matmal` to calculate large size of tensors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Feroda 34
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.0
- Python version: 3.9.6
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 11.3.1/8.2.1
- GPU model and memory: 2080 Super 8GB

**Describe the current behavior**
We are developers of [deepmd-kit](https://github.com/deepmodeling/deepmd-kit), a deep learning package based on TensorFlow in areas of computational chemistry and computational material. To give convenience to our users, we use `conda-build` to [build TensorFlow](https://github.com/deepmd-kit-recipes/tensorflow-base-feedstock/blob/master/recipe/build.sh) and our programs from source, and distribute them using an offline package, ensuring our users have the same library environment. As reported by our users in deepmodeling/deepmd-kit#1061 and deepmodeling/deepmd-kit#1062, when using `tf.matmal` to calculate large size of tensors, an error happened below. The original error should be ` failed to run cuBLAS routine: CUBLAS_STATUS_INVALID_VALUE`, and this causes `Blas xGEMV launch failed`. When we try to descrease the size of the tensor, and it works fine. However, I believe the error is not related to the memory, as some one also got this error in 40GB A100, where the memory is enough and there is no OOM message. And also, **it works fine with CUDA 10.1 and cuDNN 7**. I think it may be an internal error in TensorFlow or cuBLAS.

```
2021-09-08 17:04:15.418750: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-09-08 17:04:15.794003: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2021-09-08 17:04:15.839986: E tensorflow/stream_executor/cuda/cuda_blas.cc:564] failed to run cuBLAS routine: CUBLAS_STATUS_INVALID_VALUE
Traceback (most recent call last):
  File ""/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1375, in _do_call
    return fn(*args)
  File ""/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Blas xGEMV launch failed : a.shape=[1,1228800,2], b.shape=[1,1,2], m=1228800, n=1, k=2
	 [[{{node gradients/filter_type_all/MatMul_6_grad/MatMul}}]]
	 [[l2_virial_test/_45]]
  (1) Internal: Blas xGEMV launch failed : a.shape=[1,1228800,2], b.shape=[1,1,2], m=1228800, n=1, k=2
	 [[{{node gradients/filter_type_all/MatMul_6_grad/MatMul}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jz748/anaconda3/envs/dpdev113/bin/dp"", line 33, in <module>
    sys.exit(load_entry_point('deepmd-kit', 'console_scripts', 'dp')())
  File ""/home/jz748/codes/deepmd-kit/deepmd/entrypoints/main.py"", line 437, in main
    train_dp(**dict_args)
  File ""/home/jz748/codes/deepmd-kit/deepmd/entrypoints/train.py"", line 102, in train
    _do_work(jdata, run_opt, is_compress)
  File ""/home/jz748/codes/deepmd-kit/deepmd/entrypoints/train.py"", line 163, in _do_work
    model.train(train_data, valid_data)
  File ""/home/jz748/codes/deepmd-kit/deepmd/train/trainer.py"", line 497, in train
    self.valid_on_the_fly(fp, [train_batch], valid_batches, print_header=True)
  File ""/home/jz748/codes/deepmd-kit/deepmd/train/trainer.py"", line 591, in valid_on_the_fly
    train_results = self.get_evaluation_results(train_batches)
  File ""/home/jz748/codes/deepmd-kit/deepmd/train/trainer.py"", line 643, in get_evaluation_results
    results = self.loss.eval(self.sess, feed_dict, natoms)
  File ""/home/jz748/codes/deepmd-kit/deepmd/loss/ener.py"", line 140, in eval
    error, error_e, error_f, error_v, error_ae, error_pf = run_sess(sess, run_data, feed_dict=feed_dict)
  File ""/home/jz748/codes/deepmd-kit/deepmd/utils/sess.py"", line 20, in run_sess
    return sess.run(*args, **kwargs)
  File ""/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1190, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1368, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1394, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Blas xGEMV launch failed : a.shape=[1,1228800,2], b.shape=[1,1,2], m=1228800, n=1, k=2
	 [[node gradients/filter_type_all/MatMul_6_grad/MatMul (defined at /codes/deepmd-kit/deepmd/descriptor/se_t.py:353) ]]
	 [[l2_virial_test/_45]]
  (1) Internal: Blas xGEMV launch failed : a.shape=[1,1228800,2], b.shape=[1,1,2], m=1228800, n=1, k=2
	 [[node gradients/filter_type_all/MatMul_6_grad/MatMul (defined at /codes/deepmd-kit/deepmd/descriptor/se_t.py:353) ]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node gradients/filter_type_all/MatMul_6_grad/MatMul:
 filter_type_all/matrix_1_1_1/read (defined at /codes/deepmd-kit/deepmd/utils/network.py:174)

Input Source operations connected to node gradients/filter_type_all/MatMul_6_grad/MatMul:
 filter_type_all/matrix_1_1_1/read (defined at /codes/deepmd-kit/deepmd/utils/network.py:174)

Original stack trace for 'gradients/filter_type_all/MatMul_6_grad/MatMul':
  File ""/anaconda3/envs/dpdev113/bin/dp"", line 33, in <module>
    sys.exit(load_entry_point('deepmd-kit', 'console_scripts', 'dp')())
  File ""/codes/deepmd-kit/deepmd/entrypoints/main.py"", line 437, in main
    train_dp(**dict_args)
  File ""/codes/deepmd-kit/deepmd/entrypoints/train.py"", line 102, in train
    _do_work(jdata, run_opt, is_compress)
  File ""/codes/deepmd-kit/deepmd/entrypoints/train.py"", line 158, in _do_work
    model.build(train_data, stop_batch)
  File ""/codes/deepmd-kit/deepmd/train/trainer.py"", line 329, in build
    self._build_network(data)
  File ""/codes/deepmd-kit/deepmd/train/trainer.py"", line 353, in _build_network
    = self.model.build (self.place_holders['coord'],
  File ""/codes/deepmd-kit/deepmd/model/ener.py"", line 229, in build
    = self.descrpt.prod_force_virial (atom_ener, natoms)
  File ""/codes/deepmd-kit/deepmd/descriptor/se_t.py"", line 353, in prod_force_virial
    [net_deriv] = tf.gradients (atom_ener, self.descrpt_reshape)
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gradients_impl.py"", line 169, in gradients
    return gradients_util._GradientsHelper(
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gradients_util.py"", line 681, in _GradientsHelper
    in_grads = _MaybeCompile(grad_scope, op, func_call,
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gradients_util.py"", line 338, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gradients_util.py"", line 682, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py"", line 1733, in _MatMulGrad
    grad_a = gen_math_ops.mat_mul(grad, b, transpose_b=True)
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5716, in mat_mul
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper
    op = g._create_op_internal(op_type_name, inputs, dtypes=None,
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal
    ret = Operation(
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)

...which was originally created as op 'filter_type_all/MatMul_6', defined at:
  File ""/anaconda3/envs/dpdev113/bin/dp"", line 33, in <module>
    sys.exit(load_entry_point('deepmd-kit', 'console_scripts', 'dp')())
[elided 4 identical lines from previous traceback]
  File ""/codes/deepmd-kit/deepmd/train/trainer.py"", line 353, in _build_network
    = self.model.build (self.place_holders['coord'],
  File ""/codes/deepmd-kit/deepmd/model/ener.py"", line 159, in build
    = self.descrpt.build(coord_,
  File ""/codes/deepmd-kit/deepmd/descriptor/se_t.py"", line 316, in build
    self.dout, self.qmat = self._pass_filter(self.descrpt_reshape,
  File ""/codes/deepmd-kit/deepmd/descriptor/se_t.py"", line 388, in _pass_filter
    layer, qmat = self._filter(tf.cast(inputs_i, self.filter_precision), type_i, name='filter_type_all'+suffix, natoms=natoms, reuse=reuse, trainable = trainable, activation_fn = self.filter_activation_fn)
  File ""/codes/deepmd-kit/deepmd/descriptor/se_t.py"", line 499, in _filter
    ebd_env_ij = embedding_net(ebd_env_ij,
  File ""/codes/deepmd-kit/deepmd/utils/network.py"", line 188, in embedding_net
    hidden = tf.reshape(activation_fn(tf.matmul(xx, w) + b), [-1, outputs_size[ii]])
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py"", line 3489, in matmul
    return gen_math_ops.mat_mul(
  File ""/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5716, in mat_mul
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
```

**Describe the expected behavior**

It works without errors.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no, I don't have the idea about the reason
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
I haven't successfully created a minimum code to reproduce the error. Maybe provide later...

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

`Blas xGEMV launch failed` is raised here:
https://github.com/tensorflow/tensorflow/blob/0b6b491d21d6a4eb5fbab1cca565bc1e94ca9543/tensorflow/core/kernels/matmul_op_impl.h#L431-L436"
51886,GET returned 404 Not Found,"
**System information**
- OS Platform and Distribution: manylinux2014
- TensorFlow version: git HEAD
- Python version: 3.6 - 3.9
- Installed using virtualenv
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10



**Describe the problem**

File missing on mirror?

```
20:49:09     Analyzing: target //tensorflow/tools/pip_package:build_pip_package (243 packages loaded, 3941 targets configured)
20:49:09     Analyzing: target //tensorflow/tools/pip_package:build_pip_package (409 packages loaded, 16799 targets configured)
20:49:09     WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/dfe763f462d3569323de6caa085d8b06ce38eb7b.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
20:49:09     INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (433 packages loaded, 25206 targets configured).
```


**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
20:49:09     bazel clean --expunge
20:49:09     export BAZEL_LINKLIBS=-l%:libstdc++.a
20:49:09     bazel build --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures
20:49:09     mkdir tensorflow-pkg
20:49:09     bazel-bin/tensorflow/tools/pip_package/build_pip_package --cpu --project_name tensorflow_aarch64 ./tensorflow-pkg
```"
51881,tensorflow/compiler/mlir/tensorflow:gen_gen_mlir_passthrough_op_py_py_wrappers_cc' failed ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux, Kylin 4.4.131 (a system like Ubuntu16.04, which is based on the aarch64 architecture)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.6
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: no cuda, just with cpu
- GPU model and memory: 



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. bazel clean
2. ./configure
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is /home/b517-120/miniforge3/envs/tensorflow-build/bin/python3]: 

Found possible Python library paths:
  /home/b517-120/miniforge3/envs/tensorflow-build/lib/python3.9/site-packages
Please input the desired Python library path to use.  Default is [/home/b517-120/miniforge3/envs/tensorflow-build/lib/python3.9/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished

3.  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (427 packages loaded, 25634 targets configured).
INFO: Found 1 target...
ERROR: /home/b517-120/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:2037:21: Linking of rule '//tensorflow/compiler/mlir/tensorflow:gen_gen_mlir_passthrough_op_py_py_wrappers_cc' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/tensorflow/gen_gen_mlir_passthrough_op_py_py_wrappers_cc-2.params
bazel-out/aarch64-opt/bin/_solib_aarch64/_U_S_Stensorflow_Scompiler_Smlir_Stensorflow_Cgen_Ugen_Umlir_Upassthrough_Uop_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `std::allocator<absl::lts_20210324::string_view>::allocator()'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 4758.311s, Critical Path: 2844.81s
INFO: 12916 processes: 3232 internal, 9684 local.
FAILED: Build did NOT complete successfully

The above is all the errors I met."
51880,IOS tflite model doesn't have ResetVariableTensors() attribute,"- OS Platform and Distribution : Mac os(xcode)
- TensorFlow installed from (source or binary): pod install
- TensorFlow version : TensorFlowLiteSwift 2.6.0

I am using model that contain two LSTM layers, if I don't use a function that has same functionality like reset_all_variables()(python), ResetVariableTensors()(java, and c++), the output will be different from what I want.

I need the function using swift or objective-c, I just want a way to reset variables inside tflite model in IOS app

I hope there is a way to do that, thanks

Links to the function that I want :
python : https://github.com/tensorflow/tensorflow/blob/v2.6.0/tensorflow/lite/python/interpreter.py#L877-L878
java : https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter#resetVariableTensors()
c++ : https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter#classtflite_1_1_interpreter_1a3f5386d1e3569a55cfe0990dda8c4d92"
51879,Bug: Loading the  older versions of tfs and keras,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: No
![4 1](https://user-images.githubusercontent.com/26819449/132501060-50a9033b-f984-4828-9715-a77b50fa1c8a.JPG)
![4 2](https://user-images.githubusercontent.com/26819449/132501064-9121e22f-bf76-4648-92fb-2b68456a500f.JPG)

I wanted to run a Bioinformatics library, a deep learning model which supports particular versions.
The before model which I wanted to run showed error in TensorFlow also but this model also requirement can't be met.
I was using google-collab.
I think the issue is with integrating with google-collab or maybe with TensorFlow only.
Thanks!




"
51878,"For the model trained on tensorflow 1. X, how to quickly infer on tensorflow 2. X","For the model trained on tensorflow 1. X, how to quickly infer on tensorflow 2. X？

Based on the model trained on tensorflow 1. X, how to infer on tensorflow 2. X, and Quantization Compression Based on tflite 2. X，Is there any relevant example reference, thanks!

"
51877,tensorflow/python/kernel_tests/metrics_test.py fail in assertAlmostEqual comparison; E TypeError: type numpy.ndarray doesn't define _round_ method,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.6.0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
E TypeError: type numpy.ndarray doesn't define _round_ method

Above failure can potentially come from assert all close comparision when expected scalar value is compared against the actual numpy output from the device via self.eval.

Issue may not be seen on some devices if actual output is identical to expected value and so round call in assert all close implementation will not be called.

**Describe the expected behavior**
 One quick place where we can quickly reproduce the issue is in tensorflow/python/kernel_tests/metrics_test.py::PrecisionRecallThresholdsTest::testWithMultipleUpdates

Overwrite expected prec as an example  - 0.9393305437365417
Overwirte prec Actual output as a  numpy array with value [0.93933064]

self.assertAlmostEqual(expected_prec, self.evaluate(prec), 2)

Based upon the values, assert almost equal call should pass but ""E TypeError: type numpy.ndarray doesn't define _round_ method"" will be thrown.


**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): 
To fix it, we can first compare the shapes of actual output as [1] and then in assertAlmostEqual call, change as below:
self.assertAlmostEqual(expected_prec, self.evaluate(prec)[0], 2).

There are other test files too which have this issue whenever assertAllClose will be called and will fail if actual output is passed as numpy array and is not identical to expected ouput.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Reproducible by just running the test itself.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
NA"
51876,M1 Mac TensorFlow metal,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>


- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): M1 MAC
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): mini forge
- TensorFlow version: 2.5
- Python version: 3.9
- Installed using virtualenv? pip? conda?: condo
- I HAVE INSTALLED THROUGH :  https://github.com/jeffheaton/t81_558_deep_learning/blob/master/install/tensorflow-install-mac-metal-jul-2021.ipynb


CODE 
import numpy as np
from keras.preprocessing import image
import matplotlib.pyplot as plt
from pathlib import Path
import tensorflow as tf
import sys
import tensorflow.keras
import pandas as pd
import sklearn as sk




print(tf.test.gpu_device_name())
print(tf.config.list_physical_devices('GPU'))
print(f""Tensor Flow Version: {tf.__version__}"")
print(f""Keras Version: {tensorflow.keras.__version__}"")
print()
print(f""Python {sys.version}"")
print(f""Pandas {pd.__version__}"")
print(f""Scikit-Learn {sk.__version__}"")
gpu = len(tf.config.list_physical_devices('GPU'))>0
print(""GPU is"", ""available"" if gpu else ""NOT AVAILABLE"")


OUTPUT

/device:GPU:0
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Tensor Flow Version: 2.5.0
Keras Version: 2.5.0

Python 3.9.7 | packaged by conda-forge | (default, Sep  2 2021, 17:55:16) 
[Clang 11.1.0 ]
Pandas 1.3.2
Scikit-Learn 0.24.2
GPU is available
2021-09-08 11:25:29.831941: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2021-09-08 11:25:29.832018: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)


I am not sure what above 2 warning means . Is there any problem with installation.

"
51872,Hexagon delegate,"I am trying to build a simple ""Hexagon delegate"" based android app by using https://www.tensorflow.org/lite/performance/hexagon_delegate link. 
I have pixel 2xl phone for test this. I have rooted my phone for access all facility but i was fail to make this app. 
I am facing 1 error which is  ""java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""dlopen"" referenced by ""/data/app/~~2R44BqWkRYHRwpMUOwTYfg==/com.example.myapplication-2UccuMnXIaXolmx6-6ftSQ==/lib/arm64/libtensorflowlite_hexagon_jni.so""...""
Now, requesting help from anybody. Thank you
"
51871,Not able to lower tf.sets.intersection to HLO,"Tensorflow version 2.6

My python code snippet is like:
import os
import numpy as np
import tensorflow as tf
from tensorflow.python.eager import context
from tensorflow.python.framework import dtypes
from tensorflow.python.platform import test
from tensorflow.python.framework import config
import tensorflow.compat.v1 as tf
config.enable_mlir_bridge()
tf.config.experimental.enable_mlir_bridge()

class CustomModule(tf.Module):

def init(self):
super(CustomModule, self).init()
self.condition = tf.Variable(np.array([[True, False, False],[False, True, False],[True, True, True]]), dtype = tf.bool)
self.x = tf.Variable(np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9]]), dtype = tf.int32)
self.y =tf.Variable(np.array([[11, 12, 13],[14, 15, 16],[17, 18, 19]]), dtype = tf.int32)

@tf.function
def call(self, x):
r = tf.where(self.condition, self.x, self.y)
m= tf.where(self.condition, self.x, self.y)

c=tf.sets.intersection(tf.expand_dims(r, 0),tf.expand_dims(m, 0))

return c

module = CustomModule()

module_with_signature_path = os.path.join(""/data/aruna/tf_ops"", ‘sets_intersection’)
call = module.call.get_concrete_function(tf.TensorSpec(shape=(), dtype=tf.int32))
signatures = {‘predict’: call}
tf.saved_model.save(module, module_with_signature_path, signatures=call)
print(‘Saving model…’)

if name == ‘main’:
test.main()
I ran this python code and got saved_model.pb.
Then I used following commands:
tensorflow/compiler/mlir/tf-mlir-translate --savedmodel-objectgraph-to-mlir --tf-savedmodel-exported-names=predict -tf-enable-shape-inference-on-import=true $PWD -o sample.mlir
tensorflow/compiler/mlir/tf-opt --tf-executor-to-functional-conversion --tf-shape-inference -xla-legalize-tf --print-ir-before-all sample.mlir

TF dialect looks like:
// -----// IR Dump Before LegalizeTF //----- //
builtin.func private @__inference___call___750(%arg0: tensor {tf._user_specified_name = “x”}, %arg1: tensor<!tf_type.resource>, %arg2: tensor<!tf_type.resource>, %arg3: tensor<!tf_type.resource>) → (tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>) attributes {tf._construction_context = “kEagerRuntime”, tf._input_shapes = [#tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
%cst = “tf.Const”() {device = “”, value = dense<0> : tensor} : () → tensor
%cst_0 = “tf.Const”() {device = “”, value = dense<0> : tensor}2021-09-08 09:56:50.579733: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
: () → tensor
%0 = “tf.ReadVariableOp”(%arg2) {device = “”} : (tensor<!tf_type.resource>) → tensor<3x3xi32>
%1 = “tf.ReadVariableOp”(%arg2) {device = “”} : (tensor<!tf_type.resource>) → tensor<3x3xi32>
%2 = “tf.ReadVariableOp”(%arg3) {device = “”} : (tensor<!tf_type.resource>) → tensor<3x3xi32>
%3 = “tf.ReadVariableOp”(%arg3) {device = “”} : (tensor<!tf_type.resource>) → tensor<3x3xi32>
%4 = “tf.ReadVariableOp”(%arg1) {device = “”} : (tensor<!tf_type.resource>) → tensor<3x3xi1>
%5 = “tf.Select”(%4, %0, %2) {device = “”} : (tensor<3x3xi1>, tensor<3x3xi32>, tensor<3x3xi32>) → tensor<3x3xi32>
%6 = “tf.ExpandDims”(%5, %cst) {device = “”} : (tensor<3x3xi32>, tensor) → tensor<1x3x3xi32>
%7 = “tf.ReadVariableOp”(%arg1) {device = “”} : (tensor<!tf_type.resource>) → tensor<3x3xi1>
“tf.NoOp”() {_acd_function_control_output = true, device = “”} : () → ()
%8 = “tf.Select”(%7, %1, %3) {device = “”} : (tensor<3x3xi1>, tensor<3x3xi32>, tensor<3x3xi32>) → tensor<3x3xi32>
%9 = “tf.ExpandDims”(%8, %cst_0) {device = “”} : (tensor<3x3xi32>, tensor) → tensor<1x3x3xi32>
%10:3 = “tf.DenseToDenseSetOperation”(%6, %9) {T = i32, device = “”, set_operation = “intersection”, validate_indices = true} : (tensor<1x3x3xi32>, tensor<1x3x3xi32>) → (tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>)
%11 = “tf.Identity”(%10#0) {device = “”} : (tensor<?x3xi64>) → tensor<?x3xi64>
%12 = “tf.Identity”(%10#1) {device = “”} : (tensor<?xi32>) → tensor<?xi32>
%13 = “tf.Identity”(%10#2) {device = “”} : (tensor<3xi64>) → tensor<3xi64>
return %11, %12, %13 : tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>
}

Error is:
sample.mlir:5:3: error: The following operations cannot be legalized: tf.DenseToDenseSetOperation (count: 1); tf.NoOp (count: 1); tf.ReadVariableOp (count: 6). These legalization failure(s) may be due to missing TF to HLO lowerings and/or unsupported attributes, etc.
builtin.func private @__inference___call___340(%arg0: tensor {tf._user_specified_name = “x”}, %arg1: tensor<!tf_type.resource>, %arg2: tensor<!tf_type.resource>, %arg3: tensor<!tf_type.resource>) → (tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>) attributes {tf._construction_context = “kEagerRuntime”, tf._input_shapes = [#tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
^
sample.mlir:5:3: error: Emitting more detail about one op that failed to legalize…
builtin.func private @__inference___call___340(%arg0: tensor {tf._user_specified_name = “x”}, %arg1: tensor<!tf_type.resource>, %arg2: tensor<!tf_type.resource>, %arg3: tensor<!tf_type.resource>) → (tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>) attributes {tf._construction_context = “kEagerRuntime”, tf._input_shapes = [#tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
^
sample.mlir:20:61: error: ‘tf.DenseToDenseSetOperation’ op is not legalizable
%outputs_23:3, %control_24 = tf_executor.island wraps “tf.DenseToDenseSetOperation”(%outputs_14, %outputs_21) {T = i32, device = “”, set_operation = “intersection”, validate_indices = true} : (tensor<1x3x3xi32>, tensor<1x3x3xi32>) → (tensor<?x3xi64>, tensor<?xi32>, tensor<3xi64>)
"
51870,[TF SavedModel Export] - 2GB Tensor size constraint ,"Hi,

CC: @nluehr @WhiteFangBuck

We discussed this already a few times with @bixia1 @sanjoy and @pkanwar23.

With the recent progress in dynamic shapes support for TF-TRT, we are trying to process larger models (e.g. BERT Large, DLRM, TransformerXL) and some of them have issues to be saved in SavedModel format due to the 2GBs tensor size limit.

We have compiled a Google Colab to expose this issue: https://colab.research.google.com/drive/1NUrzK_m8HMBTx44WNQOIZ8baJHO60gDm#scrollTo=dqgB2jkpGDIj

```bash
---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

<ipython-input-3-de5a306b1fff> in <module>()
     50 print('\n3. Converting')
     51 converter = trt.TrtGraphConverterV2(input_saved_model_dir=args.saved_model_path)
---> 52 converter.convert()
     53 
     54 # def input_fn():

4 frames

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    526     if nparray.size * nparray.itemsize >= (1 << 31):
    527       raise ValueError(
--> 528           ""Cannot create a tensor proto whose content is larger than 2GB."")
    529     tensor_proto.tensor_content = nparray.tobytes()
    530     return tensor_proto

ValueError: Cannot create a tensor proto whose content is larger than 2GB.
```

@fang @sanjoy could you please orient us on what should be a way forward to address this situation in the short term and potentially come up with a path forward more robust on the long term. I don't believe only TF-TRT runs into this issue"
51863,"Bus error when TFLite library compiled as ""opt""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Poky (Yocto Project Reference Distro) 3.1.3 (dunfell)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **current (commit id 27b2309)**
- Python version:
- Bazel version (if compiling from source): **bazel 3.7.2**
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
The bug is pretty well described here: https://stackoverflow.com/questions/68687294/google-cloud-vision-api-object-detection-model-gives-bus-error-on-raspberry-pi
(although I have the same problem on a different platform than raspberry Pi).
Since this summer (I cannot tell exactly), Google changed the models in their Vision AutoML service, from:
```
TensorFlow Lite v3
Image Object Detection Model
TOCO Converted. Model built using AutoML Vision
1.14.0
```
to
```
TensorFlow Lite v3
Image Object Detection Model
MLIR Converted. Model built using AutoML Vision
2.5.0
``` 
Since then when I try to allocate tensors from these models, e.g.:
```
// Load model
std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""model.tflite"");

// Build the interpreter
tflite::ops::builtin::BuiltinOpResolver resolver;
tflite::InterpreterBuilder(*model.get(), resolver)(&interpreter);

interpreter->AllocateTensors();
```
it immediatelly goes to a bus error. GDB shows:
```
Program received signal SIGBUS, Bus error.
0xb6d5d5b4 in tflite::ops::builtin::broadcastto::ResizeOutputTensor(TfLiteContext*, tflite::ops::builtin::broadcastto::BroadcastToContext*) () from libtensorflowlite.so
(gdb) bt
#0  0xb6d5d5b4 in tflite::ops::builtin::broadcastto::ResizeOutputTensor(TfLiteContext*, tflite::ops::builtin::broadcastto::BroadcastToContext*) () from libtensorflowlite.so
#1  0xb6d5d978 in tflite::ops::builtin::broadcastto::Prepare(TfLiteContext*, TfLiteNode*) () from libtensorflowlite.so
#2  0xb6f89f64 in tflite::Subgraph::PrepareOpsStartingAt(int, std::vector<int, std::allocator<int> > const&, int*) ()
   from libtensorflowlite.so
#3  0xb6f8a220 in tflite::Subgraph::PrepareOpsAndTensors() () from libtensorflowlite.so
#4  0xb6f8cbc4 in tflite::Subgraph::AllocateTensors() () from libtensorflowlite.so
#5  0x0041a7a2 in allocate_interpreter (interpreter=0xbefffb78, model=..., num_threads=2) at inference/inference.cpp:55
```

I found out however that when I compile the TFLite library using ""fastbuild"" or ""dbg"", everything works, i.e., 
This produces a library which goes into bus error:
`bazel build --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so`
but the library produced like this works just fine (no bus error):
`bazel build --config=elinux_armhf -c dbg //tensorflow/lite:libtensorflowlite.so`

This shows that there is some bug in how the optimized code is generated for embedded linux with the new AutoML models (maybe something to do with MLIR?)


**Describe the expected behavior**
Also the optimized libtensorflowlite library should work with the latest AutoML models.

**Standalone code to reproduce the issue**
https://stackoverflow.com/questions/68687294/google-cloud-vision-api-object-detection-model-gives-bus-error-on-raspberry-pi

**Other info / logs**
I attached the `model.tflite` which causes the problem (one needs to rename it from .zip to .tflite)
[model.zip](https://github.com/tensorflow/tensorflow/files/7120665/model.zip)"
51862,Custom metrics doc doesn't mention `assign` method,"## URL(s) with the issue:
https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_metrics

## Description of issue (what needs changing):

**First small issue**:  `reset_states` is deprecated, il should be `reset_state`, shouldn't it? Furthermore, this method is not required when creating a custom metrics.

**Second issue**:
It is not necessary to subclass Metric (cf https://github.com/tensorflow/tensorflow/issues/28601#issuecomment-505098700). This should be written in the documentation. I am sure many users would be interested by the ""easy"" way, i.e. passing a function `def my_metric(y_true, y_pred)` as a metric

**Third issue**:
The doc shows an example with `assign_add` when updating the metrics. However, it may not fit users need.
For example, let's consider I want to compute a Peak Signal to Noise Ratio metric (a kind of logarithmic MSE). If I follow the doc, I would write something like this: 

```python
import tensorflow as tf
from tensorflow import keras
import math

class PSNR(keras.metrics.Metric):
    """"""
    Peak Signal to Noise Ratio metric
    """"""
    def __init__(self, name='PSNR', **kwargs):
        super().__init__(name, **kwargs)
        self.psnr = self.add_weight(name='PSNR', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred, y_true = tf.cast(y_pred, tf.float32), tf.cast(y_true, tf.float32)
        mse = tf.reduce_mean(keras.metrics.mean_squared_error(y_true, y_pred))
        psnr = 10.0 * tf.divide(tf.math.log(tf.divide(10000**2, mse)), math.log(10))

        self.psnr.assign_add(psnr)

    def result(self):
        return self.psnr
```

As a high level API user, I am not familiar with `tf.Variable` methods. Thus, I used `assign_add` like in the documentation instead of `assign` because I had no clue `assign` existed. And it took me some time to figure why my metric was so high and increasing so fast...


### Clear description
- The documentation should mention the ""easy"" way to create a metric, i.e. passing a function `def my_metric(y_true, y_pred)`. It should be quite the same as the custom losses section (https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses)
- I think an exemple with `assign` would benefit some users who would need to implement a metric that is not a sum

### Correct links
Yes

### Parameters defined
Yes

### Returns defined
Yes

### Raises listed and defined
Yes

### Usage example
Yes

### Request visuals, if applicable
No

### Submit a pull request?
don't know
"
51860,Wrong training count in eager execution mode!,"Hi,

I am using MNIST dataset with 54,000 training data on a CNN model. TensorFlow version 2.6.0.

When the code runs in enabled eager execution, in the training step only 844 data are used in each epoch!
 I think the output shots is clear:

`1- When eager execution is enabled in default`
![image](https://user-images.githubusercontent.com/84374136/132291914-1b899fd0-ef9d-4975-acfe-f5d7c1aca693.png)



`2- tf.compat.v1.disable_eager_execution()`
![image](https://user-images.githubusercontent.com/84374136/132291580-30fbd06e-c704-4f39-824e-f02621aac804.png)

What's the problem? Does anyone know the reason for this difference in numbers?
"
51859,'Failed to apply delegate' error occurred when training with tflite using nnapi,"### 1. System information

#### Converter
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS
- TensorFlow installation (pip package or built from source): pip package (pip install tf-nightly)
- TensorFlow library (version, if pip package or github SHA, if built from source):
![image](https://user-images.githubusercontent.com/37358784/132276541-58ff757e-6b7d-404f-ad5d-6a3b9151a4b0.png)

#### Mobile phone
 - Model: Samsung galaxy z-flip 3(SM-F711N)
 - OS&SDK version: Android11, API30

### 2. Code

#### - model & tflite converter
```
import tensorflow as tf

IMG_SIZE = 28

class Model(tf.Module):

  def __init__(self):
    self.model = tf.keras.Sequential([
        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    self.model.compile(
        optimizer='sgd',
        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])
    self._LOSS_FN = tf.keras.losses.CategoricalCrossentropy()
    self._OPTIM = tf.optimizers.SGD()

  @tf.function(input_signature=[
      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),
      tf.TensorSpec([None, 10], tf.float32),
  ])
  def train(self, x, y):
    with tf.GradientTape() as tape:
      prediction = self.model(x)
      loss = self._LOSS_FN(prediction, y)
    gradients = tape.gradient(loss, self.model.trainable_variables)
    self._OPTIM.apply_gradients(
        zip(gradients, self.model.trainable_variables))
    result = {""loss"": loss}
    for grad in gradients:
      result[grad.name] = grad
    return result

  @tf.function(input_signature=[tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32)])
  def predict(self, x):
    return {
        ""output"": self.model(x)
    }

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def save(self, checkpoint_path):
    tensor_names = [weight.name for weight in self.model.weights]
    tensors_to_save = [weight.read_value() for weight in self.model.weights]
    tf.raw_ops.Save(
        filename=checkpoint_path, tensor_names=tensor_names,
        data=tensors_to_save, name='save')
    return {
        ""checkpoint_path"": checkpoint_path
    }

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def restore(self, checkpoint_path):
    restored_tensors = {}
    for var in self.model.weights:
      restored = tf.raw_ops.Restore(
          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,
          name='restore')
      var.assign(restored)
      restored_tensors[var.name] = restored
    return restored_tensors

SAVED_MODEL_DIR = ""saved_model""
m= Model()
tf.saved_model.save(
    m,
    SAVED_MODEL_DIR,
    signatures={
        'train':
            m.train.get_concrete_function(),
        'infer':
            m.predict.get_concrete_function(),
        'save':
            m.save.get_concrete_function(),
        'restore':
            m.restore.get_concrete_function(),
    })

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()
with open('model.tflite','wb') as f:
    f.write(tflite_model)
# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    print(""main"")
```

#### - android code

```
package com.tvstorm.tflitetest
import android.os.Bundle
import android.util.Log
import androidx.appcompat.app.AppCompatActivity
import org.apache.commons.io.FileUtils.copyInputStreamToFile
import org.apache.commons.io.IOUtils
import org.tensorflow.lite.Interpreter
import java.io.File
import java.io.InputStream
import java.nio.FloatBuffer
class MainActivity : AppCompatActivity() {
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)
        runTFLite()
    }
    val NUM_EPOCHS = 100
    val NUM_TRAININGS = 60000
    val trainImages = Array(NUM_TRAININGS) { Array(28) { FloatArray(28) } }
    val trainLabels = Array(NUM_TRAININGS) { FloatArray(10) }
    var NUM_TESTS = 10
    var testImages = Array(NUM_TESTS) { Array(28) { FloatArray(28) } }
    var output = Array(NUM_TESTS) { FloatArray(10) }
    fun runTFLite() {
        val options = Interpreter.Options().apply { // use nnapi option
            setUseNNAPI(true)
        }
        val interpreter =
            Interpreter(convertInputStreamToFile(resources.openRawResource(R.raw.model)), options)
        for (i in 0 until NUM_EPOCHS) {
            val inputs: MutableMap<String, Any> = HashMap()
            inputs[""x""] = trainImages
            inputs[""y""] = trainLabels
            val outputs: MutableMap<String, Any> = HashMap()
            val loss: FloatBuffer = FloatBuffer.allocate(1)
            outputs[""loss""] = loss
            interpreter.runSignature(inputs, outputs, ""infer"")
            Log.d(""LOSS"", ""loss: ${loss[0]}"")
        }
    }
    fun convertInputStreamToFile(inputStream: InputStream): File {
        val tempFile = File.createTempFile(java.lang.String.valueOf(inputStream.hashCode()), "".tmp"")
        tempFile.deleteOnExit()
        copyInputStreamToFile(inputStream, tempFile)
        return tempFile
    }

}
```

### 3. Failure after conversion

https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb

We have succeeded in training on android device with tflite according to the above document.

we tested using nnapi with the interpreter option, but it raises error with the message 'Failed to apply delegate' that is relevant to static tensor size.

Does 'On device training' not support nnapi yet? Is there a loadmap for nnapi?

### 4. error log

```
2021-09-07 11:28:23.521 20502-20502/com.tvstorm.tflitetest E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.tvstorm.tflitetest, PID: 20502
    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.tvstorm.tflitetest/com.tvstorm.tflitetest.MainActivity}: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#3 is a dynamic-sized tensor).
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3832)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:4008)
        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:85)
        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2317)
        at android.os.Handler.dispatchMessage(Handler.java:106)
        at android.os.Looper.loop(Looper.java:247)
        at android.app.ActivityThread.main(ActivityThread.java:8618)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:602)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1130)
     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#3 is a dynamic-sized tensor).
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:487)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:88)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:51)
        at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:40)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:196)
        at com.tvstorm.tflitetest.MainActivity.runTFLite(MainActivity.kt:32)
        at com.tvstorm.tflitetest.MainActivity.onCreate(MainActivity.kt:17)
        at android.app.Activity.performCreate(Activity.java:8215)
        at android.app.Activity.performCreate(Activity.java:8199)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1309)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3805)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:4008) 
        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:85) 
        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135) 
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95) 
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2317) 
        at android.os.Handler.dispatchMessage(Handler.java:106) 
        at android.os.Looper.loop(Looper.java:247) 
        at android.app.ActivityThread.main(ActivityThread.java:8618)
```"
51858,[tflite conversion] Output order changes with multiple outputs,"
I maybe wrong... is so, could anyone provide how to fix this?

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip package version 2.6.0
- TensorFlow library (version, if pip package or github SHA, if built from source): N/A

### 2. Code

```py
import tensorflow as tf

print(tf.version.VERSION)

input = tf.keras.layers.Input(shape=(3,3,32), name=""input"")

o1 = tf.keras.layers.Conv2D(2, (1,1), activation='relu', input_shape=(1,3,3,32))(input)
o2 = tf.keras.layers.Conv2D(16, (1,1), activation='relu', input_shape=(1,3,3,32))(input)
o3 = tf.keras.layers.Conv2D(32, (1,1), activation='relu', input_shape=(1,3,3,32))(input)

model = tf.keras.Model(inputs=input, outputs=[o1,o2,o3])
model.summary()

tf.keras.models.save_model(model, ""test_saved_model"")
```

```py
import tensorflow as tf

print(tf.version.VERSION)

converter = tf.lite.TFLiteConverter.from_saved_model(""test_saved_model"")
converter.allow_custom_ops = True
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]

tflite_model = converter.convert()
open(""test_saved_model.tflite"", ""wb"").write(tflite_model)
```

Provide code to help us reproduce your issues using one of the following options:



### 3. Failure after conversion
conversion is successful, but the generated model is wrong;

- Output order has changed (order of last axis: 2, 32, 16)

![image](https://user-images.githubusercontent.com/4616940/132270784-01d0bfcf-c036-4cd9-9cd7-98eea93fd075.png)

- Expected something like this (order of last axis: 2, 16, 32)
![image](https://user-images.githubusercontent.com/4616940/132270636-fdff902f-c468-4006-abd8-58a58f0df919.png)

### 4. (optional) RNN conversion support


### 5. (optional) Any other info / logs
"
51856,Error when reading .wav files using TPU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab

**Describe the current behavior**

I am trying to read .wav files in my input pipeline and train on TPU. I am using tfio.audio.AudioIOTensor but it is not working with TPU.

Here is a colab that shows the error:
https://colab.research.google.com/drive/1gSP3bhmDkMFwHwxWWc3B3hdR9SNA0hM9#scrollTo=FItPzgJ1xJzZ

The error is the following:
```
NotFoundError: Op type not registered 'IO>AudioReadableInit' in binary running on n-6a025964-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. [Op:BatchDatasetV2]
```

Is there another way to decode .wav files when using TPU?
"
51850,"tf-nightly dev version installed with mismatch keras-nightly version, which cause importing error.","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1045-aws x86_64)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: tf-nightly-
- Python version: 3.6
- Installed using virtualenv? no pip? yes conda? no 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory: None



**Describe the problem**

Recently I install by `pip3.6 install tf-nightly`, then run:
```
from tensorflow.keras import layers
```
will get error:
```
AttributeError: module 'tensorflow.compat.v2.__internal__.tracking' has no attribute 'DelegatingTrackableMixin'
```

I check the installed version, `keras-nightly-2.7.0.dev2021090607` and `tf-nightly-2.7.0.dev20210806` was installed. The devXXX part is mismatched.

When I reinstall keras-nightly with `keras-nightly==2.7.0.dev2021080600`, it seems address this issue.

So can we ensure when run `pip install tf-nightly`, it will always install the matched version of `keras-nightly` ?

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
51849,Cannot create interpreter error occurred when test 'On-Device Training in TensorFlow Lite',"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS
- TensorFlow installation (pip package or built from source): pip package (pip install tf-nightly)
- TensorFlow library (version, if pip package or github SHA, if built from source): 
![image](https://user-images.githubusercontent.com/37358784/132177843-72c65706-a28e-43f2-9ec4-b6285ca12b5f.png)

### 2. Code

#### - model & tflite converter
```
import tensorflow as tf

IMG_SIZE = 28

class Model(tf.Module):

  def __init__(self):
    self.model = tf.keras.Sequential([
        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    self.model.compile(
        optimizer='sgd',
        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])
    self._LOSS_FN = tf.keras.losses.CategoricalCrossentropy()
    self._OPTIM = tf.optimizers.SGD()

  @tf.function(input_signature=[
      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),
      tf.TensorSpec([None, 10], tf.float32),
  ])
  def train(self, x, y):
    with tf.GradientTape() as tape:
      prediction = self.model(x)
      loss = self._LOSS_FN(prediction, y)
    gradients = tape.gradient(loss, self.model.trainable_variables)
    self._OPTIM.apply_gradients(
        zip(gradients, self.model.trainable_variables))
    result = {""loss"": loss}
    for grad in gradients:
      result[grad.name] = grad
    return result

  @tf.function(input_signature=[tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32)])
  def predict(self, x):
    return {
        ""output"": self.model(x)
    }

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def save(self, checkpoint_path):
    tensor_names = [weight.name for weight in self.model.weights]
    tensors_to_save = [weight.read_value() for weight in self.model.weights]
    tf.raw_ops.Save(
        filename=checkpoint_path, tensor_names=tensor_names,
        data=tensors_to_save, name='save')
    return {
        ""checkpoint_path"": checkpoint_path
    }

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def restore(self, checkpoint_path):
    restored_tensors = {}
    for var in self.model.weights:
      restored = tf.raw_ops.Restore(
          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,
          name='restore')
      var.assign(restored)
      restored_tensors[var.name] = restored
    return restored_tensors

SAVED_MODEL_DIR = ""saved_model""
m= Model()
tf.saved_model.save(
    m,
    SAVED_MODEL_DIR,
    signatures={
        'train':
            m.train.get_concrete_function(),
        'infer':
            m.predict.get_concrete_function(),
        'save':
            m.save.get_concrete_function(),
        'restore':
            m.restore.get_concrete_function(),
    })

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()
with open('model.tflite','wb') as f:
    f.write(tflite_model)
# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    print(""main"")
```
#### - android root gradle

```
// Top-level build file where you can add configuration options common to all sub-projects/modules.
buildscript {
    ext.kotlin_version = ""1.4.32""
    repositories {
        mavenCentral()
        maven {  // Only for snapshot artifacts
            name 'ossrh-snapshot'
            url 'https://oss.sonatype.org/content/repositories/snapshots'
        }
        google()
        jcenter()
    }
    dependencies {
        classpath ""com.android.tools.build:gradle:7.0.2""
        classpath ""org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version""

        // NOTE: Do not place your application dependencies here; they belong
        // in the individual module build.gradle files
    }
}

allprojects {
    repositories {
        google()
        jcenter()
    }
}

task clean(type: Delete) {
    delete rootProject.buildDir
}
```
#### - android app gradle

```
plugins {
    id 'com.android.application'
    id 'kotlin-android'
}

android {
    compileSdkVersion 30
    buildToolsVersion ""30.0.3""

    defaultConfig {
        applicationId ""com.tvstorm.tflitetest""
        minSdkVersion 28
        targetSdkVersion 30
        versionCode 1
        versionName ""1.0""

        testInstrumentationRunner ""androidx.test.runner.AndroidJUnitRunner""
    }

    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'
        }
    }
    compileOptions {
        sourceCompatibility JavaVersion.VERSION_1_8
        targetCompatibility JavaVersion.VERSION_1_8
    }
    kotlinOptions {
        jvmTarget = '1.8'
    }
    buildFeatures {
        mlModelBinding true
    }
}

dependencies {
    implementation('commons-io:commons-io:2.6')
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly-SNAPSHOT'
    implementation ""org.jetbrains.kotlin:kotlin-stdlib:$kotlin_version""
    implementation 'androidx.core:core-ktx:1.6.0'
    implementation 'androidx.appcompat:appcompat:1.3.1'
    implementation 'com.google.android.material:material:1.3.0'
    implementation 'androidx.constraintlayout:constraintlayout:2.1.0'
    testImplementation 'junit:junit:4.+'
    androidTestImplementation 'androidx.test.ext:junit:1.1.2'
    androidTestImplementation 'androidx.test.espresso:espresso-core:3.3.0'
}
```
#### - android activity code

```
class MainActivity : AppCompatActivity() {
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)
        runTFLite()
    }

    val NUM_EPOCHS = 100
    val NUM_TRAININGS = 60000
    val trainImages = Array(NUM_TRAININGS) { Array(28) { FloatArray(28) } }
    val trainLabels = Array(NUM_TRAININGS) { FloatArray(10) }

    fun runTFLite() {
        val interpreter = Interpreter(convertInputStreamToFile(resources.openRawResource(R.raw.model)))
        for (i in 0 until NUM_EPOCHS) {
            val inputs: MutableMap<String, Any> = HashMap()
            inputs[""x""] = trainImages
            inputs[""y""] = trainLabels
            val outputs: MutableMap<String, Any> = HashMap()
            val loss: FloatBuffer = FloatBuffer.allocate(1)
            outputs[""loss""] = loss
            interpreter.runSignature(inputs, outputs, ""train"")
        }
    }

    fun convertInputStreamToFile(inputStream: InputStream): File {
        val tempFile = File.createTempFile(java.lang.String.valueOf(inputStream.hashCode()), "".tmp"")
        tempFile.deleteOnExit()
        copyInputStreamToFile(inputStream, tempFile) // commons-io:commons-io
        return tempFile
    }
}
```

### 3. Failure after conversion

We have referenced the article in the link below.
https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb

A few warnings occurred in the process of creating the tflite model, but they were made. When loading this model in android tflite, the following error log occurs.

### 4. error logs

#### - Converter side

```
2021-09-06 16:34:59.531193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.1/lib64
2021-09-06 16:34:59.531227: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-09-06 16:34:59.532327: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-06 16:35:01.916373: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:absl:Importing a function (__inference_internal_grad_fn_1051) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_internal_grad_fn_1079) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
2021-09-06 16:35:03.747781: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.
2021-09-06 16:35:03.747821: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.
2021-09-06 16:35:03.747826: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.
2021-09-06 16:35:03.749030: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model
2021-09-06 16:35:03.752172: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }
2021-09-06 16:35:03.752276: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: saved_model
2021-09-06 16:35:03.769526: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.
2021-09-06 16:35:03.815918: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: saved_model
2021-09-06 16:35:03.826002: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 76975 microseconds.
2021-09-06 16:35:03.861382: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2021-09-06 16:35:03.957610: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1890] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexBroadcastGradientArgs, FlexReluGrad, FlexRestore, FlexSave
Details:
	tf.BroadcastGradientArgs(tensor<2xi32>, tensor<2xi32>) -> (tensor<?xi32>, tensor<?xi32>) : {device = """"}
	tf.ReluGrad(tensor<?x128xf32>, tensor<?x128xf32>) -> (tensor<?x128xf32>) : {device = """"}
	tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<*xf32>) : {device = """", preferred_shard = -1 : i64}
	tf.Save(tensor<!tf_type.string>, tensor<4x!tf_type.string>, tensor<784x128xf32>, tensor<128xf32>, tensor<128x10xf32>, tensor<10xf32>) -> () : {device = """"}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded
main
```

#### - mobile side error log

```

2021-09-06 16:23:34.298 18282-18282/com.tvstorm.tflitetest D/AndroidRuntime: Shutting down VM
2021-09-06 16:23:34.298 18282-18282/com.tvstorm.tflitetest E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.tvstorm.tflitetest, PID: 18282
    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.tvstorm.tflitetest/com.tvstorm.tflitetest.MainActivity}: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Op builtin_code out of range: 142. Are you using old TFLite binary with newer model?
    Registration failed.
    
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3792)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3968)
        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:85)
        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2307)
        at android.os.Handler.dispatchMessage(Handler.java:106)
        at android.os.Looper.loop(Looper.java:246)
        at android.app.ActivityThread.main(ActivityThread.java:8512)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:596)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1130)
     Caused by: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Op builtin_code out of range: 142. Are you using old TFLite binary with newer model?
    Registration failed.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:75)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:51)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:227)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:202)
        at com.tvstorm.tflitetest.MainActivity.runTFLite(MainActivity.kt:34)
        at com.tvstorm.tflitetest.MainActivity.onCreate(MainActivity.kt:20)
        at android.app.Activity.performCreate(Activity.java:8198)
        at android.app.Activity.performCreate(Activity.java:8182)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1309)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3765)
        	... 11 more
```
"
51848,Cannot load saved tflite model when model contains dropout. However quantization-aware model is fine.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- TensorFlow installed from (source or binary): No 
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.9
- CUDA/cuDNN version:  cuDNN-8.1.1.33. CUDA-11.2.0

**Describe the current behavior**
If a model contains dropout layers, if can be converted to tflite, but the tflite model cannot be loaded. The error message is:
```
ValueError: Did not get operators or tensors in subgraph 1.
```

Confusingly, if I make the model quantization-aware, the model can be successfully converted to tflite and loaded. Does QAT automatically remove dropout?

**Describe the expected behavior**
If a tflite model that contain dropout is not supported, then the desired behavior are:

1.  Raise an error during tflite conversion instead of at runtime. Or
2. Remove the dropout operation during tflite conversion.


**Standalone code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
import tensorflow_model_optimization as tfmot


quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer
quantize_apply = tfmot.quantization.keras.quantize_apply



def get_model(quantization_aware: bool = False):
    input_embeddings = tf.keras.Input(shape=(512,), dtype=tf.float32, name=""input_embeddings"")
    is_training = tf.keras.Input(shape=(), dtype=tf.bool, name=""is_training"")

    x = tf.keras.layers.Dropout(rate=0.5)(inputs=input_embeddings, training=is_training)

    if quantization_aware:  # apply quantization to dense layer and return quantization-aware model
        out = quantize_annotate_layer(
            tf.keras.layers.Dense(units=512,name=""dense"")
        )(x)

        model = quantize_apply(
            tf.keras.Model(
                inputs=[input_embeddings, is_training],
                outputs=[x, out],
                name=""toy_model"",
            )
        )
    else: # return vanilla model
        out = tf.keras.layers.Dense(units=512,name=""dense"")(x)
        model = tf.keras.Model(
            inputs=[input_embeddings, is_training],
            outputs=[x, out],
            name=""toy_model"",
        )

    return model

def convert_to_tflite(saved_model_path, output_model_path):
    converter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_path))
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    tflite_quant_model = converter.convert()

    with open(output_model_path, ""wb"") as fh:
        fh.write(tflite_quant_model)



model = get_model(quantization_aware=False)  # set to False will raise the error. Set to True the code runs successfully.
input_data = np.random.rand(16, 512)
embeddings, out = model([input_data, True])

model.save(""toy_model"")

convert_to_tflite(""toy_model"", ""toy_model.tflite"")
interpreter = tf.lite.Interpreter(""toy_model.tflite"")
```

I'm wondering how should I deal with dropout layer in tflite? As dropout is essential for training, how can I export a trained model without dropout? This is [a similar issue](https://github.com/tensorflow/tensorflow/issues/44232), and I'm not satisfied by the workaround proposed there.
"
51847,set_visible_device_list causing session create failure,"**System information**
- Standard code (as shown in the ""gpu_device_test.cc"" provided with TF source code)
- Windows 10 64 bit
- Tensorflow built from source
- TF 2.5
- Python 3.8.7
- Bazel 3.7.2
- CUDA 11.2 cudnn 8
- GeForce GTX 1050

The session create function fails if I set a specific GPU with the function set_visible_device_list, even if I have a single GPU.
For instance if I set the GPU like this
```
SessionOptions options;
ConfigProto* config = &options.config;
(*config->mutable_device_count())[""GPU""] = 1;
config->mutable_gpu_options()->set_visible_device_list(""0"");
```

The subsequent piece of code and in the specific the session create fails. If I don't set, it doesn't fail.

```
architecture->session.reset(NewSession(options));
graph_def.reset(new GraphDef());
ReadBinaryProto(Env::Default(), (string)architecture_path, graph_def.get());
architecture->session->Create(*graph_def.get());
```

The GPU is working and visbile by Tensorflow
```
auto gpu_factory = DeviceFactory::GetFactory(""GPU"");
gpu_factory->ListPhysicalDevices(&devs_gpu);
```

How can I set a specific GPU?"
51845,Convert Functional API to Model Subclassing in TensorFlow Tutorial?,Could anyone please teach me how to convert the Functional API to Model subclassing in this [TensorFlow Official Tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers)? I suppose an elegant chuck of code should be what combines the Normalization layer with the remaining layers. Thank you very much!
51843,tf.linalg.diag issue,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8.11

**Describe the current behavior**
When providing the tf.linalg.diag function an input > length of 32, the function returns this error:

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [32,32] vs. shape[1] = [1,1] [Op:ConcatV2] name: concat
```

**Describe the expected behavior**
The function should return a tensor of shape [n,n] no mater how large the tensor is.

- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**

```
import tensorflow as tf
import numpy as np

#input
input_data = np.ones(33)

#model
input = tf.keras.layers.Input([],dtype=tf.float32)
out = tf.linalg.diag(input)
model = tf.keras.Model(inputs=input, outputs=out)

#inference
pred = model.predict(input_data)
```
"
51842,The ModifyGraphWithDelegate function fails on some devices.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac OS Big sur (11.3.1)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  Galaxy s21 ultra
- TensorFlow installed from (source or binary): source  
- TensorFlow version: r2.6
- Python version: 3.8.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.7.2

I currently use libtensorflowlite_gpu_delegate.so which compiled from source(r2.6).
ModifyGraphWithDelegate function works fine on Galaxy Note 10.
But, it's not working in Galaxy s21 ultra.
(returns kTfLiteApplicationError.)

Does it work according to the device? 
(Can't use gpu delegate for Galaxy s21 ultra?)
Or do I need to set additional options?

(Currently I am setting it as the default as below.)
```
  TfLiteGpuDelegateOptionsV2 option = TfLiteGpuDelegateOptionsV2Default();
  delegate_ = TfLiteGpuDelegateV2Create(/*default options=*/&option);
  TfLiteStatus result = interpreter_->ModifyGraphWithDelegate(delegate_);

  if (result != kTfLiteOk) {
    return kAudioNodeError;
  }
```

I built the libraries as follows.

libtensorflowlite.so(3.7MB): bazel build -c opt --config android_x86 android_x86_64 android_arm android_arm64 --define tflite_with_xnnpack=true
libtensorflowlite_gpu_delegate.so(98.5MB): bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so 

Thanks!!

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
51841,OS Error while installing tensorflow on virtualenv,"**System information**
- OS Platform and Distribution : Windows 10
- TensorFlow version: 2.6.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: virtualenv with pip install --upgrade tensorflow
- GPU model and memory: none (Using CPU)


**Describe the problem** : 
I tried installing tensorflow on my local machine in a virtual environment I got this error  : 
```
Installing collected packages: tensorflow
ERROR: Could not install packages due to an OSError: [Errno 2] 
No such file or directory: 'E:\\Pathik\\KJ\\internships\\Facial Expression recognition\\Github Repo\\Emotion-recognition\\venv\\Lib\\sitepackages\\tensorflow\\include\\external\\cudnn_frontend_archive\\_virtual_includes\\cudnn_frontend\\third_party\\cudnn_frontend\\include\\cudnn_frontend_EngineConfigGenerator.h'
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
virtualenv venv
.\venv\Scripts\activate 
pip install --upgrade tensorflow
```

**Any other info / logs**
All the dependencies were successfully installed except the tensorflow.
"
51840,"Saving a composite model that includes a custom layer results in error - None has NoneType, but expected one of: bytes, unicode","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): A custom variant of RedHat, outside of my control
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7.2
- CUDA/cuDNN version: 11.1.0/8.1.1
- GPU model and memory: Nvidia K40 12GB

**Describe the current behavior**
I'm trying to save a model which is a composite model of composite models.

The first model is a sequential model of two sequential models. Both of the two sub-models have custom layers that perform scaling operations like MinMax and cube root. This model saves and loads without any issues.

The first model is then loaded in a different script without compiling. There is not issue with this. Let's call this model MODEL_1.

The next step may be a bit confusing. There are then two more models added in parallel to each other but sequentially with MODEL_1. Let's call these models MODEL_2a and MODEL_2b. The output of MODEL_1 has the input of MODEL_1 concatenated to it and this serves as the input to MODEL_2a and MODEL_2b. It should be noted that that the ""MODEL_1 input"" goes through a custom scaling layer before being concatenated to the output of MODEL_1. The is scaling layer that was also implemented in MODEL_1 without any issues.

Finally, there is a single, custom layer that performs a 'simple' weighted sum of the outputs of MODEL_2a and MODEL_2b to produce the model output. This weighting happens via `alpha*OUTPUT_2a + [1-alpha]*OUTPUT_2b`. 'alpha' is a trainable, scalar parameter. I haven't used this before in a model that I have saved, so I'm guessing this is the cause.

The model compiles and trains, but fails to save.

The custom weighted sum layer is this,

```
class WeightedSum(krs.layers.Layer):
    def __init__( self, n_models = 2, name = 'weighted_sum_0' ):
        super( WeightedSum, self ).__init__( name = name)
        self.n_models = n_models
        self.ensemble_weights = []
        self.output_init = tf.Variable(0.,validate_shape=False,trainable=False)

    def build(self,input_shape):
        for i in range(self.n_models):
            self.ensemble_weights.append( self.add_weight(shape=(1,),
                                    initializer = 'ones',
                                    trainable = True) )

    def call(self,inputs):
        new_normalizer = tf.convert_to_tensor(0.,dtype = inputs[0].dtype)
        for i in range(self.n_models):
            new_normalizer = new_normalizer + self.ensemble_weights[i]
        new_normalizer = tf.constant(1.,dtype=new_normalizer.dtype)/new_normalizer
        output = self.output_init

        for i in range(self.n_models):
            output = tf.add(output,tf.multiply(self.ensemble_weights[i],inputs[i]))
        output = tf.multiply( output, new_normalizer )
        return output
```

The save command is this, (NOTE: I use `import tensorflow.keras as krs`)
```
krs.models.save_model(linked_model,""test_failed_save.mdl"")
```

The error that is produced is this.

```
Traceback (most recent call last):
  File ""multi_fidelity_training_full_link.py"", line 304, in <module>
    main()
  File ""multi_fidelity_training_full_link.py"", line 265, in main
    krs.models.save_model(linked_model,""test_failed_save.mdl"")
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 151, in save_model
    signatures, options, save_traces)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py"", line 90, in save
    model, filepath, signatures, options)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1104, in save_and_return_nodes
    raise_metadata_warning))
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1291, in _build_meta_graph
    raise_metadata_warning)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1225, in _build_meta_graph_impl
    options.namespace_whitelist)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 713, in _fill_meta_graph_def
    _call_function_with_mapped_captures, resource_map=resource_map)))
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 424, in frozen_saveable_objects
    call_with_mapped_captures)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 375, in _serialize_gathered_objects
    slot_variables=slot_variables)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 355, in _fill_object_graph_proto
    child_proto.local_name = child.name
TypeError: None has type NoneType, but expected one of: bytes, unicode
```


**Describe the expected behavior**
It should save the model.

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow.keras as krs
import tensorflow as tf




class WeightedSum(krs.layers.Layer):
    def __init__( self, n_models = 2, **kwargs):
        super( WeightedSum, self ).__init__( **kwargs)
        self.n_models = n_models
        self.ensemble_weights = []
        self.output_init = tf.Variable(0.,validate_shape=False,trainable=False)

    def build(self,input_shape):
        for i in range(self.n_models):
            self.ensemble_weights.append( self.add_weight(shape=(1,),
                                    initializer = 'ones',
                                    trainable = True) )

    def call(self,inputs):
        new_normalizer = tf.convert_to_tensor(0.,dtype = inputs[0].dtype)
        for i in range(self.n_models):
            new_normalizer = new_normalizer + self.ensemble_weights[i]
        new_normalizer = tf.constant(1.,dtype=new_normalizer.dtype)/new_normalizer
        output = tf.cast(self.output_init,dtype=inputs[0].dtype)

        for i in range(self.n_models):
            output = tf.add(output,tf.multiply(tf.cast(self.ensemble_weights[i],dtype=inputs[i].dtype),inputs[i]))
        output = tf.multiply( output, new_normalizer )
        return output


input_lf = krs.Input((4,))

x = input_lf
x = krs.layers.Dense(10,activation = 'relu')(x)
x = krs.layers.Dense(10,activation = 'relu')(x)
lf_out = krs.layers.Dense(10,activation = 'relu')(x)

lf_mod = krs.Model(input_lf,lf_out,name='lf')

input_hf_lin = krs.Input((14,))
x = input_hf_lin
x = krs.layers.Dense(10)(x)
x = krs.layers.Dense(10)(x)
hf_lin_out = krs.layers.Dense(10,activation = 'relu')(x)

hf_lin_mod = krs.Model(input_hf_lin,hf_lin_out,name='hf_linear')

input_hf_nonlin = krs.Input((14,))

x = input_hf_nonlin
x = krs.layers.Dense(10,activation = 'relu')(x)
x = krs.layers.Dense(10,activation = 'relu')(x)
hf_nonlin_out = krs.layers.Dense(10,activation = 'relu')(x)

hf_nonlin_mod = krs.Model(input_hf_lin,hf_lin_out,name='hf_nonlinear')

input_hf = krs.Input((14,))

x = input_hf
lin = hf_lin_mod(x)
nonlin = hf_nonlin_mod(x)
summed_out = WeightedSum(n_models=2)([lin,nonlin])

hf_mod = krs.Model(input_hf,summed_out,name='hf')

input_full_mod = krs.Input((4,))
x = input_full_mod

low = lf_mod(x)
x = krs.layers.Concatenate()([low,x])
full_out = hf_mod(x)

full_mod = krs.Model(input_full_mod,outputs = {'low_fidelity':low,'high_fidelity':full_out},name='full_model')

opt = krs.optimizers.Adam()
loss = krs.losses.MSE
full_mod.compile(optimizer = opt,loss = loss)

x_train = np.random.uniform(0,10,(20,4))
y_train_low = np.random.uniform(0,10,(20,10))
y_train_high = np.random.uniform(0,10,(20,10))
y = {""low_fidelity"": y_train_low,
     ""high_fidelity"": y_train_high}



full_mod.fit(x_train,y,epochs=5)

krs.models.save_model(full_mod,""test_model.mdl"")
```

EDIT: I goofed during my copy and paste from VIM. I didn't get all of the code orginally.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
51839,memory leak in tf.py_function,"tensorflow 2.7 python 3.7

minimum code

```
import tensorflow as tf
import numpy as np
import os

def test(xyz_batch,  k):
    indices = np.zeros((1000, 1000, 20), dtype=np.int64)
    dist = np.zeros((1000, 1000, 20), dtype=np.float32)
    return indices, dist


while True:
    ret = tf.py_function(test, [0,0], [tf.int64, tf.int64])
```"
51838,tensorflow support multi-evaluator.,"Issues:
1. **OOM**: The model from distributed training is too large (memory) to be restored to the evaluator.
2. **Performance**: The evaluation data is too large (TB), and the stand-alone evaluator is too slow to process the data.
so:
Do we have plans to support distributed evaluator?

"
51837,tensorflow support multi-evaluator.,"Issues:
1. **OOM**: The model from distributed training is too large (memory) to be restored to the evaluator.
2. **Performance**: The evaluation data is too large (TB), and the stand-alone evaluator is too slow to process the data.
so:
Do we have plans to support distributed evaluator?

"
51835,self.add_weight in custom Keras Layers breaks model saving in TF format,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Debian 10
- TensorFlow installed from: binary
- TensorFlow version: 2.5.0
- Python version: 3.7.3

**Describe the current behavior**
I have a model that seems to fail in saving with TF format (.h5 is completely fine) when I call self.add_weight in a custom Keras layer:
```
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Input
import tensorflow as tf
from model import load_gen

class ResAdd(Layer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.res_gain = self.add_weight(shape=(), initializer='zeros', trainable=True)

    def call(self, inputs):
        res, skip = inputs
        gain = tf.cast(self.res_gain, res.dtype)
        out = res * gain + skip
        return out 

inp = Input((32, 32, 3)) 
out = ResAdd()([inp, inp])
model = Model(inp, out)

model.save('model')
```

The error states:
```
Traceback (most recent call last):
  File ""bring.py"", line 23, in <module>
    model.save('model')
  File ""/home/tgpu/.local/lib/python3.7/site-packages/keras/engine/training.py"", line 2146, in save
    signatures, options, save_traces)
  File ""/home/tgpu/.local/lib/python3.7/site-packages/keras/saving/save.py"", line 150, in save_model
    signatures, options, save_traces)
  File ""/home/tgpu/.local/lib/python3.7/site-packages/keras/saving/saved_model/save.py"", line 91, in save
    model, filepath, signatures, options)
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1228, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def))
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1399, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 1349, in _build_meta_graph_impl
    wrapped_functions)
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 200, in __init__
    self._trace_all_concrete_functions()
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 289, in _trace_all_concrete_functions
    for obj in self.checkpoint_view.list_objects():
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 460, in list_objects
    trackable_objects, _, _ = self.objects_ids_and_slot_variables()
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 455, in objects_ids_and_slot_variables
    self.objects_ids_and_slot_variables_and_paths())
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 443, in objects_ids_and_slot_variables_and_paths
    object_names[obj] = _object_prefix_from_path(path)
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 64, in _object_prefix_from_path
    for trackable in path_to_root))
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 64, in <genexpr>
    for trackable in path_to_root))
  File ""/home/tgpu/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 57, in _escape_local_name
    return (name.replace(_ESCAPE_CHAR, _ESCAPE_CHAR + _ESCAPE_CHAR)
AttributeError: 'NoneType' object has no attribute 'replace'
```

Note that I fixed this error by replacing:
```
self.res_gain = self.add_weight(shape=(), initializer='zeros', trainable=True)
```
with
```
self.res_gain = tf.Variable(0.0, trainable=True)
```

**Describe the expected behavior**

That the behavior of 
```
self.res_gain = self.add_weight(shape=(), initializer='zeros', trainable=True)
```
is equal to 
```
self.res_gain = tf.Variable(0.0, trainable=True)
```
when initializing a model and that saving the model in TF format won't raise an error"
51834,staged_predict for Boosted Tree classifier,"<emThe issue related to the performance of the [BoostedTree classifier](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1932-L2101) in terms of accuracy.</em>

**System information**

- OS Platform and Distribution: Linux Ubuntu 16.04, Windows 10
- TensorFlow installed from (source or binary): pip install TensorFlow
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6

** Describe the model**
BoostedTree classifier is a model introduced by [N Ponomareva, T Colthurst, G Hendry.2017](https://ieeexplore.ieee.org/abstract/document/8257910).
It is built over the Xgboost idea and learning is done through building one layer of decision tree regressor over N boosting iteration.

**Describe the current behavior**
Return the accuracy of the ensemble model with [n_tree](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1932-L2101)

**Describe the expected behavior**
I need to have the accuracy of each individual tree in the ensemble model.
For example, the Gradient Boosting of Sklearn has the [staged_predict method](https://github.com/scikit-learn/scikit-learn/blob/2beed5584/sklearn/ensemble/_gb.py#L1270).
Is there any similar method in tf.BoostedTree as well?


**Standalone code to reproduce the issue**
You may find an example of the model, [here](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding)"
51833,"ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]","I'm trying to build a neural network for colour recognition, I have 3 categories to sort
part1 of my code is building the data set
----------
import numpy as np
import matplotlib.pyplot as plt
import os 
import cv2
import random
import pickle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D,MaxPool2D


DataDIR = ""/Users/zhiranbai/Downloads/SmartSyringeSystem-master/notebooks""
CATES = [""Advil"",""Demazin"",""Panadol""]

tf.get_logger().setLevel('ERROR')

for CATE in CATES:
    path = os.path.join(DataDIR,CATE)  #path to medicine dir
    for img in os.listdir(path):
        img_array = cv2.imread(os.path.join(path,img))
        #plt.imshow(img_array, cmap=""gray"")
        #plt.show()
        break
    break
IMG_SIZE= 200
new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))
#plt.imshow(new_array, cmap = 'gray')
#plt.show()


training_data = []

def create_traning_data():
    for CATE in CATES:
        path = os.path.join(DataDIR,CATE)
        class_num = CATES.index(CATE)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path,img))
                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))
                training_data.append([new_array, class_num])
            except Exception as e:
                pass
create_traning_data()
#print(len(training_data))

random.shuffle(training_data)

for sample in training_data[:10]:
    print(sample[1])


x = []
y= []
# y= np.array(y)
for features, label in training_data :
    x.append(features)
    y.append(label)
    # np.array((y,label))
x = np.array(x).reshape(-1, IMG_SIZE, IMG_SIZE,3) #-1 fea num, 3 RGB


pickle_out = open(""x.pickle"", ""wb"")
pickle.dump(x, pickle_out)
pickle_out.close()

pickle_out = open(""y.pickle"", ""wb"")
pickle.dump(y, pickle_out)
pickle_out.close()
-----------
part 2 is try to train the NN
-------
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D

import pickle

X = pickle.load(open(""x.pickle"",""rb""))
y = pickle.load(open(""y.pickle"",""rb""))

#nomalize data --scale

X = X/255.0

model = Sequential()
model.add(Conv2D(64,(3,3),input_shape = X.shape[1:]))
model.add(Activation(""relu"")) #activation layer rectify linear
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64,(3,3)))
model.add(Activation(""relu"")) #activation layer rectify linear
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Flatten())
model.add(Dense(64))

model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])

model.fit(X,y,batch_size=32, epochs=1, validation_split=0.3) #valid--out of sample data
----------------
and I got this error
------
python3 roberttest3.py
2021-09-04 22:52:41.530069: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""roberttest3.py"", line 34, in <module>
    model.fit(X,y,batch_size=32, epochs=1, validation_split=0.3) #valid--out of sample data
  File ""/usr/local/lib/python3.7/site-packages/keras/engine/training.py"", line 1121, in fit
    (x, y, sample_weight), validation_split=validation_split))
  File ""/usr/local/lib/python3.7/site-packages/keras/engine/data_adapter.py"", line 1479, in train_validation_split
    ""arrays, found following types in the input: {}"".format(unsplitable))
ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]


-------

"
51832,Forked tf script deadlocks unless disabling intra op parallelism,"**System information**
- Have I written custom code: **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04.2 LTS**
- Mobile device: **No**
- TensorFlow installed from: **pip**
- TensorFlow version (use command below): **v2.6.0-rc2-32-g919f693420e 2.6.0**
- Python version: **3.9.6**
- Bazel/GCC/Compiler version: **Not compiling from source**
- CUDA/cuDNN/GPU version: **Using CPU only**

**Describe the current behavior**

Me and the R&D team of our company are trying do a submission for an standardized benchmark, namely the [NIST FRVT 1:1](https://pages.nist.gov/frvt/html/frvt11.html) verification. Their benchmark suite employs this architecture:
- They run a custom submitter `initialize` function, where one can prepare the environment an run expensive functions that loads models and prepare temporary structures;
- They fork the process with unix `fork()` a number of times and they run either a `create_template` or `match` function in the forked children. According to the rules of the benchmark, the elaboration must be performed in **CPU** only. No GPU elaboration is allowed.

Since we can't modify the architecture because it's not in our control, we are trying to fit tensorflow so it will work according to these rules, but we are finding the children processes to deadlock trying to elaborate some layers. The only workaround we found is setting intra op parallelism to 1 with `tf.config.threading.set_intra_op_parallelism_threads(1)`, which appears to disable parallelism for operations like matrix multiplications. This workaround will not work in all scenarios, though. Running the model loading in the forked children will also workaround the issue but will penalize us in the benchmark since the time needed for the loading will be accounted for the elaboration.

**Describe the expected behavior**
Since we are enforcing CPU only elaboration and there are no resources that require exclusive access we are expecting tensorflow to correctly fit in this architecture and be able to run correctly in forked processes.

**Standalone code to reproduce the issue**
The following minimal python script will mimic the architecture and reproduce the issue. [Model](https://github.com/JonneOkkonen/MachineLearningProjects/blob/main/models/cats_vs_dogs_model_86_83.h5) and [test](https://github.com/JonneOkkonen/MachineLearningProjects/blob/main/testImages/cat.jpg) image used are linked.
It's available also as a Colab [notebook](https://colab.research.google.com/drive/1tvB9kZQvq6tiS7S4aMr761DQ9-DRcKkY?usp=sharing), which features a very similar behavior as running in a local machine, with the difference that in Colab notebook the `os.waitpid()` call never works but that could be an environment limitation.

```
import os
import tensorflow as tf
import cv2
import numpy

tf.config.set_visible_devices(tf.config.list_physical_devices('CPU'))
#tf.config.threading.set_intra_op_parallelism_threads(1) # Decomment this line and the child will not deadlock
#tf.debugging.set_log_device_placement(True)             # Decomment to see job placements

model = None

def initialize():
    global model
    model = tf.keras.models.load_model('cats_vs_dogs_model_86_83.h5')
    print('Model Loaded')

def child():
    print('Child spawned')
    imageSize = 128
    testImage = cv2.resize(src=cv2.imread('cat.jpg'), dsize=(imageSize, imageSize), interpolation=cv2.INTER_LINEAR) / 255
    result = model(testImage.reshape(-1, imageSize, imageSize, 3))[0]
    print('Result: Cat: ' + str(result[0]) + '| Dog: ' + str(result[1]))
    print('Child finished')

def parent():
    initialize()
    newpid = os.fork()
    if newpid == 0:
        child()
    else:
        pids = (os.getpid(), newpid)
        print(""Parent: %d, Waiting for child: %d\n"" % pids)
        os.waitpid(newpid, 0)

parent()
```
**Other info / logs**
Enabling log device placement will reveal that the elaboration stops elaborating in a convolution layer but with other scenarios the elaboration can deadlock in different operations.

```
Child spawned
2021-09-04 12:10:03.207086: I tensorflow/core/common_runtime/eager/execute.cc:1161] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:CPU:0
2021-09-04 12:10:03.209115: I tensorflow/core/common_runtime/eager/execute.cc:1161] Executing op Cast in device /job:localhost/replica:0/task:0/device:CPU:0
2021-09-04 12:10:03.210155: I tensorflow/core/common_runtime/eager/execute.cc:1161] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2021-09-04 12:10:03.211392: I tensorflow/core/common_runtime/eager/execute.cc:1161] Executing op Conv2D in device /job:localhost/replica:0/task:0/device:CPU:0
```

We are using Numpy version: 1.19.5 as installed automatically from pip. We also tried to enforce updated numpy 1.21.2 as suggested [here](https://github.com/tensorflow/tensorflow/issues/13802#issuecomment-881665082) in an other `fork()` related issue but that didn't help.

**[Contributing](https://www.tensorflow.org/community/contribute)**
- Do you want to contribute a PR? (yes/no): **No**
"
51828,Build fails with internal compiler error on file lstm_ops.cc,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6.0
- Python version: 3.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.2.0
- CUDA/cuDNN version: 11.4/8.2.2
- GPU model and memory: gtx 970 4GB



**Describe the problem**
Build failes after 4~ hours with error message
```
ERROR: C:/users/zaksm/source/repos/tensorflow/tensorflow/core/kernels/rnn/BUILD:42:18: C++ compilation of rule '//tensorflow/core/kernels/rnn:lstm_ops' failed (Exit -1): python.exe failed: error executing command
  cd C:/users/zaksm/_bazel_zaksm/arvgy65w/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.4
    SET GCC_HOST_COMPILER_PATH=C:/msys64/usr/bin/gcc.exe
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/zaksm/AppData/Local/Programs/Python/Python39/python.exe
    SET PYTHON_LIB_PATH=C:/Users/zaksm/AppData/Local/Programs/Python/Python39/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\zaksm\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.2
    SET TMP=C:\Users\zaksm\AppData\Local\Temp
  C:/Users/zaksm/AppData/Local/Programs/Python/Python39/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/mkl_dnn_v1 /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1 /Iexternal/cudnn_frontend_archive /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_headers_virtual /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/mkl_dnn_v1/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/include /Iexternal/mkl_dnn_v1/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src /Iexternal/mkl_l_dnn_v1/src/cpu/x64/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /Iexternal/local_config_cuda/cuda/cufft/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cufft/include /Iexternal/local_config_cuda/cuda/curand/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/curand/include /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /arch:AVX /std:c++14 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_USE_XLA=1 -DINTEL_MKL -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -DNV_CUDNN_DISABLE_EXCEPTION -DGOOGLE_CUDA=1 -DNV_CUDNN_DISABLE_EXCEPTION -DTENSORFLOW_USE_XLA=1 -DINTEL_MKL=1 /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/rnn/_objs/lstm_ops/lstm_ops.obj /c tensorflow/core/kernels/rnn/lstm_ops.cc
Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'
C:\users\zaksm\_bazel_zaksm\arvgy65w\execroot\org_tensorflow\tensorflow\core\kernels\rnn\lstm_ops.cc(56) : fatal error C1001: Internal compiler error.
(compiler file 'd:\a01\_work\2\s\src\vctools\Compiler\Utc\src\p2\main.c', line 213)
 To work around this problem, try simplifying or changing the program near the locations listed above.
If possible please provide a repro here: https://developercommunity.visualstudio.com
Please choose the Technical Support command on the Visual C++
 Help menu, or open the Technical Support help file for more information

Target //tensorflow:tensorflow.dll failed to build
INFO: Elapsed time: 13605.853s, Critical Path: 6621.50s
INFO: 11659 processes: 3915 internal, 7744 local.
FAILED: Build did NOT complete successfully
```


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel build --config=opt --config=cuda tensorflow:tensorflow.dll
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
51827,Custom signatures not working with Subclasssed Keras Model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3
- Python version: 3.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
When saving keras_models (checked for Subclassed Model) and providing custom signatures, `keras_model.save(export_dir, signatures)`  does not work and the saved model does not have the exported signature. The custom signature however works if we are using `tf.saved_model.save(keras_model, export_dir, signatures)`.

**Describe the expected behavior**
tf.saved_model.save(keras_model, export_dir, signatures) and `keras_model.save(export_dir, signatures)` should have similar signatures.
"
51826,TF-32 Tensor Cores not used,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 11.1
- GPU model and memory: RTX A6000, 48GB

**Describe the current behavior**
I'm training a fairly large convnet, and with long training times, one thing I've noticed is apparent lack of tensor core utilization. I have tried using float16, but my model greatly suffers from the reduced precision. However, since this is an Ampere GPU I expect to be able to use tensor cores in 32 bit as well. Using the tensorboard profiler, I see many of my kernels are tensor core eligible but none are actually using tensor cores. 
<img width=""785"" alt=""Screen Shot 2021-09-03 at 11 50 49 AM"" src=""https://user-images.githubusercontent.com/19317207/132033324-c31d0d45-d1cc-4bee-80da-c747028f69b7.png"">

All of my convolution layers have multiples of 8 for filters (32, 64, 128, etc.) so dimensions should not be affecting tensor cores. One other thing I though worth noting was lots of memory being taken up by NHWC->NCHW transpose layers. It's my understanding that tensor cores should operate entirely in NHWC, so my model should not be doing any computation in NCHW and yet it seems to be.

<img width=""750"" alt=""Screen Shot 2021-09-03 at 11 54 19 AM"" src=""https://user-images.githubusercontent.com/19317207/132033733-18ce0744-ed68-467b-9f1f-b802198061dc.png"">


**Describe the expected behavior**
I expect Tensor Cores to be used for most convolution and matmul operations provided the multiple-of-8 condition is met. One thing I'm wondering: is it possible that the bug is actually in reporting tensor core usage rather than not using tensor cores in the first place? I.e. is tensorboard only looking for float16 tensor cores and not reporting tf-32 operations as using tensor cores when they actually are? That still doesn't explain the NCHW behavior though.  

**Standalone code to reproduce the issue**
Working on producing an example. Colab is not an option due to lack of Ampere GPUs

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Here is my TensorBoard log file. This includes the profiler data as well as a model checkpoint for inspecting the architecture
[20210902-145514.zip](https://github.com/tensorflow/tensorflow/files/7107228/20210902-145514.zip)"
51825,Grouped convolutions generate seriously obscure errors on CPU,"Hello there :wave: 

Today I ran into a cumbersome error that only happens when running on CPU instead of GPUs. I tracked the source of the error to grouped convolutions and managed to make a reproducible minimal snippet. I happened to suspect that it was because of grouped convolutions since I ran into some problems a few days ago with those using SavedModels but it's pure luck. 

It would be good to improve the error message or even get this fixed if possible :pray: 

Happy to help provided some directions!

**System information**
- Have I written custom code: yes, the code snippet
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from: binary, via pip
- TensorFlow version: 2.5.0
- Python version: 3.8
- CUDA/cuDNN version: CUDA 11.4 (cuDNN 8.2.0)
- GPU model and memory: NVIDIA GeForce RTX 2070 with Max-Q Design

**Describe the current behavior**

As of now, running the snippet further down below throws an error on CPU but not on GPU.

**Describe the expected behavior**

Simple:
- having a better error (pointing the lack of support of grouped convolutions on CPU)
- or even better, if that could get fixed :) 

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

samples = tf.zeros((1, 256, 256, 3), dtype=tf.float32)
model = Sequential([layers.Conv2D(18, padding='same', kernel_size=3, groups=1), layers.GlobalAveragePooling2D(), layers.Dense(1)])
trouble_model = Sequential([layers.Conv2D(18, padding='same', kernel_size=3, groups=3), layers.GlobalAveragePooling2D(), layers.Dense(1)])

# Backprop on classic model
with tf.GradientTape() as tape:
    out = model(samples, training=True)
grads = tape.gradient(out, model.trainable_weights)

# Now with grouped conv
with tf.GradientTape() as tape:
    out = trouble_model(samples, training=True)
grads = tape.gradient(out, trouble_model.trainable_weights)
```

which runs successfully on GPU but on CPU throws the following:
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-e03a8706f9a2> in <module>
     19 with tf.GradientTape() as tape:
     20     out = trouble_model(samples, training=True)
---> 21 grads = tape.gradient(out, trouble_model.trainable_weights)

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
   1072                           for x in nest.flatten(output_gradients)]
   1073 
-> 1074     flat_grad = imperative_grad.imperative_grad(
   1075         self._tape,
   1076         flat_targets,

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     69         ""Unknown value for unconnected_gradients: %r"" % unconnected_gradients)
     70 
---> 71   return pywrap_tfe.TFE_Py_TapeGradient(
     72       tape._tape,  # pylint: disable=protected-access
     73       target,

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)
    157       gradient_name_scope += forward_pass_name_scope + ""/""
    158     with ops.name_scope(gradient_name_scope):
--> 159       return grad_fn(mock_op, *out_grads)
    160   else:
    161     return grad_fn(mock_op, *out_grads)

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py in _Conv2DGrad(op, grad)
    579   # in Eager mode.
    580   return [
--> 581       gen_nn_ops.conv2d_backprop_input(
    582           shape_0,
    583           op.inputs[1],

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py in conv2d_backprop_input(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)
   1245       return _result
   1246     except _core._NotOkStatusException as e:
-> 1247       _ops.raise_from_not_ok_status(e, name)
   1248     except _core._FallbackException:
   1249       pass

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6895   message = e.message + ("" name: "" + name if name is not None else """")
   6896   # pylint: disable=protected-access
-> 6897   six.raise_from(core._status_to_exception(e.code, message), None)
   6898   # pylint: enable=protected-access
   6899 

~/miniconda3/lib/python3.8/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Computed input depth 3 doesn't match filter input depth 1 [Op:Conv2DBackpropInput]
```

"
51824,Keras model with TPUStrategy get InternalError,"When I fit a Keras Model with TPUStrategy, it got following errors:
```
InternalError: 7 root error(s) found.
  (0) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1630677855.531027249"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3009,""referenced_errors"":[{""created"":""@1630677855.531025671"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":398,""grpc_status"":14}]}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Executing non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[Pad_9/paddings/_152]]
  (1) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1630677855.531027249"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3009,""referenced_errors"":[{""created"":""@1630677855.531025671"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":398,""grpc_status"":14}]}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Executing non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[cond_14/switch_pred/_140/_90]]
  (2) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1630677855.531027249"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3009,""referenced_errors"":[{""created"":""@1630677855.531025671"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":398,""grpc_status"":14}]}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Executing non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[TPUReplicate/_compile/_10521736726166110026/_4/_194]]
  (3) Internal: {{function_node __inference_train_function_163744}} failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
: ... [truncated]
```

TensorFlow Version: 
v2.6.0-0-g919f693420e 2.6.0

**Standalone code to reproduce the issue**

The code used to reproduce this error can be found at https://drive.google.com/file/d/1ceB5VBwFd87Z0ANa_91vhvrOdG4SJQLY/view?usp=sharing
"
51823,[Tensorflow Executor Dialect]  ,"**MLIR operation format is :**
operation            ::= op-result-list? (generic-operation | custom-operation)
                         trailing-location?
generic-operation    ::= string-literal `(` value-use-list? `)`  successor-list?
                         region-list? dictionary-attribute? `:` function-type
custom-operation     ::= bare-id custom-operation-format
op-result-list       ::= op-result (`,` op-result)* `=`
op-result            ::= value-id (`:` integer-literal)
successor-list       ::= `[` successor (`,` successor)* `]`
successor            ::= caret-id (`:` bb-arg-list)?
region-list          ::= `(` region (`,` region)* `)`
dictionary-attribute ::= `{` (attribute-entry (`,` attribute-entry)*)? `}`
trailing-location    ::= (`loc` `(` location `)`)?


**here is tf_executor.graph operation :**
%fetches = tf_executor.graph : tensor<*xf32> {
  // Operations in the current block execute when their inputs are ready,
  // possibly concurrently.
  // Only operations in the tf_executor dialect are expected here.
  // Ops can return multiple outputs and a control token for control
  // dependencies.
  // We don’t mention the control token in the return type here, it is implicit.
  %0, %ctl0 = tf_executor.opA %feed#0, %feed#1 : tensor<*xf32>
  %1, %ctl1 = tf_executor.opB : tensor<*xf32>
  %2, %ctl2 = tf_executor.opC %1, %ctl0 : tensor<*xf32>
  %3, %ctl3 = tf_executor.opD %2 : tensor<*xf32>
  tf_executor.fetch %3 : tensor<*xf32>
} // end of the “tf_executor.graph"" operation/region

According to Operation format,   {} stands for dictionary-attribute
obviously, tf_executor.graph operation looks like abnormal case
the {} of tf_executor consists of several operations

could anyone explain and point out how to understand tf_executor.graph operation
many thanks
"
51821,ctc_loss: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 11.2
- GPU model and memory: on CPU


**Describe the current behavior**
When using ctc_loss with dense tensor, it functions correctly. But when using it with sparse tensor, although the result is the right but it raised a warning saying: 
```
W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.
```
I'm not sure if this is a real bug or something else. 

**Describe the expected behavior**
It shouldn't raise the warning. 

**Standalone code to reproduce the issue**
```
import tensorflow as tf
label = [
	[1, 2, 1, 0, 0],
	[1, 1, 0, 0, 0],
	[1, 1, 1, 1, 1],
]
logits = [
	[[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]],
	[[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]],
	[[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0]],
]
labels_length = [3, 2, 5]
logits_length = [5, 5, 5]
labels_tensor = tf.convert_to_tensor(label, dtype=tf.int32)
labels_tensor_sparse = tf.sparse.from_dense(labels_tensor)
logits_tensor = tf.convert_to_tensor(logits, dtype=tf.float32)

labels_length_tensor = tf.convert_to_tensor(labels_length, dtype=tf.int32)
logits_length_tensor = tf.convert_to_tensor(logits_length, dtype=tf.int32)

loss_dense = tf.nn.ctc_loss(labels_tensor, logits_tensor, labels_length_tensor, logits_length_tensor, logits_time_major=False)
print(loss_dense.numpy()[0])
loss_sparse = tf.nn.ctc_loss(labels_tensor_sparse, logits_tensor, None, logits_length_tensor, logits_time_major=False, blank_index=0)
print(loss_sparse.numpy()[0])
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

![image](https://user-images.githubusercontent.com/6693605/132005260-d30cd4e5-e7f9-4e52-afc5-7668fd22122b.png)

"
51820,Tensorflow sort changes values in output list to 0 when the tensor datatype is tf.float32 but not for tf.float64,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): M1 Macbook Air where is the issue is seen / Intel Mac where I don't see any issue
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
When n<=16 it sorts the values properly but when n>16 it sorts the list and then changes the values at 
position 16 and higher to  value  -0.

import tensorflow as tf

n=17
a = tf.random.uniform(shape=[n], dtype=tf.float32)
print(a)
print(tf.sort(a))

tf.Tensor(
[ 0.00088847  0.03644979  0.06494105  0.07945895  0.13768506  0.153288
  0.27231824  0.39109504  0.4003389   0.41191268  0.48915362  0.528106
  0.5799836   0.6125376   0.65293264  0.8135816  -0.        ], shape=(17,), dtype=float32)

on M1 Macbook Air ( tensorflow 2.5.0 ) and

tf.Tensor(
[0.41191268 0.48915362 0.65293264 0.6125376  0.00088847 0.03644979
 0.13768506 0.528106   0.27231824 0.4003389  0.5799836  0.83420205
 0.06494105 0.39109504 0.8135816  0.153288   0.07945895], shape=(17,), dtype=float32)

on Intel Mac ( tensorflow 2.4,3 )

```
**Describe the expected behavior**

```
tf.Tensor(
[0.41191268 0.48915362 0.65293264 0.6125376  0.00088847 0.03644979
 0.13768506 0.528106   0.27231824 0.4003389  0.5799836  0.83420205
 0.06494105 0.39109504 0.8135816  0.153288   0.07945895], shape=(17,), dtype=float32)



```

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): 
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
51819,Converting Hard Swish activation to TFLite efficiently,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

when using a hard swish activation:
```
x = 0.16667 * x * tf.nn.relu6(x+3)
```
what shows up in the Tensorflow Lite model is is just the computation graph of the above definition:
x
 -> multiply by 0.167 
 -> add 3-> relu6 (those are fused together)
         -> multiply results from above
(see diagram [here](https://imgur.com/a/OZcWZWQ))

 ** Expected behavior ** 

Tensorflow Lite has a hard swish operator already. Question is, how should I define the computation in Tensorflow such that when converted to TFLite, the output would be a single hard swish node for this activation?

"
51818,Low performance when using persistent mode GradientTape with LSTM/GRU layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8.10
- CUDA/cuDNN version: 11.0/8.2.2
- GPU model and memory: NVIDIA RTX Titan 24GB

**Describe the current behavior**
The performance was very low in graph mode when using persistent mode tf.GradientTape or create multi-GradientTape objects in one with block.
This phenomenon only happens when the model includes a LSTM or GRU layers.

**Standalone code to reproduce the issue**
```python
import time
import numpy as np
import tensorflow as tf

model0 = tf.keras.models.Sequential(
    tf.keras.layers.LSTM(128, input_shape=(300, 40))
)
model1 = tf.keras.models.Sequential(
    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))
)
loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)
optimizer=tf.keras.optimizers.Adam()

@tf.function
def train_step_0():
  with tf.GradientTape() as tape0, tf.GradientTape() as tape1:
    f = model0(tf.random.normal([256, 300, 40]))
    y_p0 = model1(f)
    y_p1 = model1(tf.random.normal((256, 128)))
    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)
    loss1 = loss_object(tf.ones_like(y_p1), y_p1)
  grad0 = tape0.gradient(loss0, model0.trainable_variables)
  grad1 = tape1.gradient(loss1, model1.trainable_variables)
  optimizer.apply_gradients(zip(grad0,model0.trainable_variables))
  optimizer.apply_gradients(zip(grad1,model1.trainable_variables))

t0=time.time()
for i in range(100):
  train_step_0()
print(time.time()-t0)
```
**output**
```
2021-09-03 12:39:22.262342: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.
2021-09-03 12:39:22.287055: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.
2021-09-03 12:39:22.300477: W tensorflow/core/common_runtime/process_function_library_runtime.cc:841] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.
17.58007049560547
```
```python
import time
import numpy as np
import tensorflow as tf

model0 = tf.keras.models.Sequential(
    tf.keras.layers.LSTM(128, input_shape=(300, 40))
)
model1 = tf.keras.models.Sequential(
    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))
)
loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)
optimizer=tf.keras.optimizers.Adam()

@tf.function
def train_step_1():
  with tf.GradientTape(persistent=True) as tape:
    f = model0(tf.random.normal([256, 300, 40]))
    y_p0 = model1(f)
    y_p1 = model1(tf.random.normal((256, 128)))
    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)
    loss1 = loss_object(tf.ones_like(y_p1), y_p1)
  grad0 = tape.gradient(loss0, model0.trainable_variables)
  grad1 = tape.gradient(loss1, model1.trainable_variables)
  optimizer.apply_gradients(zip(grad0,model0.trainable_variables))
  optimizer.apply_gradients(zip(grad1,model1.trainable_variables))

t0=time.time()
for i in range(100):
  train_step_1()
print(time.time()-t0)
```
**output**
```
2021-09-03 12:43:44.947280: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.
2021-09-03 12:43:44.972180: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.
2021-09-03 12:43:44.985573: W tensorflow/core/common_runtime/process_function_library_runtime.cc:841] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.
16.632988929748535
```
```python
import time
import numpy as np
import tensorflow as tf

model0 = tf.keras.models.Sequential(
    tf.keras.layers.LSTM(128, input_shape=(300, 40))
)
model1 = tf.keras.models.Sequential(
    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))
)
loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)
optimizer=tf.keras.optimizers.Adam()

@tf.function
def train_step_2():
  with tf.GradientTape() as tape0:
    f = model0(tf.random.normal([256, 300, 40]))
    y_p0 = model1(f)
    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)
  with tf.GradientTape() as tape1:
    y_p1 = model1(tf.random.normal((256, 128)))
    loss1 = loss_object(tf.ones_like(y_p1), y_p1)
  grad0 = tape0.gradient(loss0, model0.trainable_variables)
  grad1 = tape1.gradient(loss1, model1.trainable_variables)
  optimizer.apply_gradients(zip(grad0,model0.trainable_variables))
  optimizer.apply_gradients(zip(grad1,model1.trainable_variables))

t0=time.time()
for i in range(100):
  train_step_2()
print(time.time()-t0)
```
**output**
```
4.321804523468018
```
**Other info**
Both train_step_0 and train_step_1 show the error, while train_step_2 doesn't. In my GPU, the first 2 approaches take around 17 in doing 100 training steps, while the third one takes 4.3s.
Furthermore, we can only reproduce this performace drop when using GRU/LSTMs in graph mode. Which is, if we remove the tf.function decorator from the train_step functions or if we switch the LSTM by a dense layer, all 3 examples take the same time and none of them outputs any error.
As an additional info, this problem happens running both in CPU and in GPU

By the way, this issue is an updated version of #35928 which addressed a very similar problem"
51817,Facing issue while initializing parameters in tensorflow.,"![2](https://user-images.githubusercontent.com/26819449/131990627-7504f592-80e1-478f-ab3d-19a48d7b50b9.JPG)

![1](https://user-images.githubusercontent.com/26819449/131990652-bc585eb8-0be2-4ace-b3c2-b88f05da8e9c.JPG)

Where i am lacking.
I have already ready tensor flow documentation.
Just tell me what should I do.
tf_version_2.3.0
Thanks!
"
51816,Building TFlite for windows with flex delegate error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.6.0
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: None
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): MSVC 2019 (Not sure how to check more in detail)
- CUDA/cuDNN version: 10.1/ 7.6.5
- GPU model and memory: Nvidia GTX 1050



**Describe the problem**
I want to build tflite for windows with support for Tensorflow ops outside of standard tflite operation.
So, I followed this guide: https://www.tensorflow.org/lite/guide/ops_select#c
Although it is ambiguous and since I am pretty new in this field, I looked around for other site and do the following.

Note that I would like to build TF with no CUDA support. (Only using cpu)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Add ""//tensorflow/lite/delegates/flex:delegates"" to `deps` in `tensorflow/tensorflow/lite/BUILD`
2. Using following command to build tflite
`bazel build --config=monolithic -c opt --cxxopt='--std=c++11' //tensorflow/lite:libtensorflowlite.so`


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here are some part of the result logs:
```   
 SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\jobpa\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\jobpa\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/HostX64/x64/link.exe @bazel-out/x64_windows-opt/bin/tensorflow/lite/libtensorflowlite.so-2.params
Execution platform: @local_execution_config_platform//:platform
LINK : warning LNK4044: unrecognized option '/s'; ignored
builtin_ops.lib(register.obj) : error LNK2005: ""public: __cdecl tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver(void)"" (??0BuiltinOpResolver@builtin@ops@tflite@@QEAA@XZ) already defined in builtin_ops_all_linked.lo.lib(register.obj)
builtin_ops.lib(register.obj) : error LNK2005: ""public: virtual class std::vector<class std::unique_ptr<struct TfLiteDelegate,void (__cdecl*)(struct TfLiteDelegate *)>,class std::allocator<class std::unique_ptr<struct TfLiteDelegate,void (__cdecl*)(struct TfLiteDelegate *)> > > __cdecl tflite::ops::builtin::BuiltinOpResolver::GetDelegates(int)const "" (?GetDelegates@BuiltinOpResolver@builtin@ops@tflite@@UEBA?AV?$vector@V?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@std@@V?$allocator@V?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@std@@@2@@std@@H@Z) already defined in builtin_ops_all_linked.lo.lib(register.obj)
builtin_ops.lib(register.obj) : error LNK2005: ""public: virtual class std::vector<class std::unique_ptr<struct TfLiteDelegate,void (__cdecl*)(struct TfLiteDelegate *)>,class std::allocator<class std::unique_ptr<struct TfLiteDelegate,void (__cdecl*)(struct TfLiteDelegate *)> > > __cdecl tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates::GetDelegates(int)const "" (?GetDelegates@BuiltinOpResolverWithoutDefaultDelegates@builtin@ops@tflite@@UEBA?AV?$vector@V?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@std@@V?$allocator@V?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@std@@@2@@std@@H@Z) already defined in builtin_ops_all_linked.lo.lib(register.obj)
LINK : fatal error LNK1189: library limit of 65535 objects exceeded
Target //tensorflow/lite:libtensorflowlite.so failed to build
```

According to this [post](https://github.com/tensorflow/tensorflow/issues/43367), I tried to add `--copt=/bigobj` but that does not help either.

I'm not sure if I did something wrong, or if there is a limitation of adding flex delegate to tflite on windows...

"
51815,ValueError: Shape must be rank 1 but is rank 0 for ,"Value Error: in user code:
    <ipython-input-18-4013e653d9ce>:18 one_hot_matrix  *
        one_hot = tf.reshape(tf.one_hot(label,depth,axis=0), (depth))
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **
        return target(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:195 reshape
        result = gen_array_ops.reshape(tensor, shape, name)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:8234 reshape
        ""Reshape"", tensor=tensor, shape=shape, name=name)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal
        compute_device)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal
        op_def=op_def)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1975 __init__
        control_input_ops, op_def)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1815 _create_c_op
        raise ValueError(str(e))

    ValueError: Shape must be rank 1 but is rank 0 for '{{node Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](one_hot, Reshape/shape)' with input shapes: [6], [].

tf version : '2.3.0'

Is this a bug ?"
51814,Connection to pythong,"I got folowing error using tensorflow in R
```
> library(tensorflow)
> tf$constant(""Hellow Tensorflow"")
Error in system2(python, stdout = TRUE, args = c(""-c"", shQuote(""import sys; import platform; sys.stdout.write(platform.architecture()[0])""))) : 
  'CreateProcess' failed to run 'C:\Python27\ArcGIS10.8\python.exe -c ""import sys; import platform; sys.stdout.write(platform.architecture()[0])""'
"
51813,tf.io.decode_image will flip the image,"Please look at [colab](https://colab.research.google.com/drive/1gybGlto8ol7lnE82G9LLx0-DhRWtweUd#scrollTo=gecIt24H9fHO)

![IMG_20210729_194824430_MFNR](https://user-images.githubusercontent.com/33624574/131965649-813d23db-d14d-4c12-8dd5-25c3bf313e90.jpg)
"
51812,Is there any update on profiler's TensorCores eligible rules?,"@yisitu
I find the rules code to judge whether a kernel uses Tensor Cores is 1~2 years old. I'm not sure whether there should be new updates, along with latest hardward chips or library updates.
BTW, how these rules come from? Is there any doc that lists all kernels which uses Tensor Cores?

https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/core/profiler/utils/kernel_stats_utils.cc#L95-L119
"
51810,RuntimeError: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (64 != 1)Node number 13 (CONV_2D) failed to prepare.,"Hello I am using semantic segmentation model. The model is suclass custom model. It has been trained and saved successfully. i also convert it into tflite version. But when I tried for inference of tflite model it shows the mentioned error in allocating tensors. i have also tried it using tf-nightly 2.7 and tf version 2.5 but it shows same error while allocation tensor. Any help will be highly appreciated. Thanks

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow version = 2.5:

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab]
https://colab.research.google.com/drive/1v8SvJbMmjTyYVnRBeGwwCGSnRpkpR80p#scrollTo=L_PMowpPmaFx


"
51809,Gradient accmulation with LSTM achieving lower accuracy on testset,"Hi,

Sorry if this is not the right place. I'm really at a loss for my problems.
I'm using TF 2.4, and I'm trying to do gradient accumulation with my model, which is three layers of LSTM connected to three Dense layers. I'm also using a ""sub-network"" trick-- in each layer of my model, the first x% neurons are taken to form a sub-network, with its weights shared with the full network. (For example, if the full net is two Dense layers with 10 and 20 neurons each, then a sub-network with x being 50% is just two Dense layers with 5 and 10 neurons each.) There are three sub-networks currently, each with a different value of said x (named width_mult in the following code). In the training, I add up the gradients for the full network and the three sub-networks, and then update the weights.

The problem is, by training in this way, I'm getting lower accuracy on testset (~82% or even lower) than direct training (~89%, i.e., without gradient accumulation and the sub-network thing). But it seems that the accuracies for both full and sub-network during training are high. Also, if I get rid of the LSTM layers, the testing accuracy goes up. So I'm wondering if there is something wrong with my implementation (especially in the gradients padding part, see below), but I can't spot a bug myself. Can someone help? Perhaps a glaring error that I overlooked?

[original code deleted]"
51805,Can not install python3-pip,"Hello, I am new to python. I was trying to install pip, but I get and error whitch I can not find a way to fix it
Code that I am running:
`sudo apt install python3 python3-pip`
What I am getting:
```
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
python3 is already the newest version (3.9.4-1).
You might want to run 'apt --fix-broken install' to correct these.
The following packages have unmet dependencies:
 nodejs : Depends: libnode72 (= 12.21.0~dfsg-3ubuntu1) but it is not going to be installed
 python3-pip : Depends: python3-setuptools but it is not going to be installed
               Depends: python3-wheel but it is not going to be installed
               Depends: python-pip-whl (= 20.3.4-1ubuntu2) but it is not going to be installed
               Recommends: python3-dev (>= 3.2) but it is not going to be installed
E: Unmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution).

```
Also tried to do `sudo apt --fix-broken install` like statet in the last line, but got this:
```

Unpacking nodejs (16.8.0-deb-1nodesource1) over (12.21.0~dfsg-3ubuntu1) ...
dpkg: error processing archive /var/cache/apt/archives/nodejs_16.8.0-deb-1nodesource1_amd64.deb (
--unpack):
 trying to overwrite '/usr/share/doc/nodejs/api/process.html', which is also in package nodejs-do
c 12.21.0~dfsg-3ubuntu1
dpkg-deb: error: paste subprocess was killed by signal (Broken pipe)
Errors were encountered while processing:
 /var/cache/apt/archives/nodejs_16.8.0-deb-1nodesource1_amd64.deb
E: Sub-process /usr/bin/dpkg returned an error code (1)


```

Any ideas on how to fix it?
Running Linux PopOs
npm -v: 6.14.15"
51804,Hexagon Delegate crashes on max pooling operation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Snapdragon 855 dev platform
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8.0
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
After converting YOLOv4 from Tensorflow to TfLite, the model runs well on mobile device, on CPU.
When running on DSP using hexagon delegate, I get the following error:

INFO: Initialized TensorFlow Lite runtime.
INFO: TfLiteHexagonDelegate delegate: 535 nodes delegated out of 568 nodes with 2 partitions.
INFO: Replacing 535 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 5 partitions.

----------------
Timestamp: Sun May 30 16:21:13 2021


Log
hexagon/ops/src/op_maxpool_d32.c:567:insufficient width padding
hexagon/src/execute.c:167:execute() failed on node id=689 err=-1
hexagon/src/interface.c:1297:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph..
ERROR: Node number 568 (TfLiteHexagonDelegate) failed to invoke.


**Describe the expected behavior**

I expect it to run successfully. 


**Standalone code to reproduce the issue**
See attached model. 

**Other info / logs** 
I've narrowed it down to a single max pool operation that is later concatenated. The operation in question takes a 1x13x13x512 input tensor and computes tf.nn.max_pool(input_data, ksize=13,padding = ""SAME"", strides=1). other instances of this operator with ksize = 9 and ksize =5 (all other parameters are the same) work just fine. 

Another thing I noticed is that depending on what else is going on in the model, sometimes the converter emits a TfLite model that works. However, it is intermittent and I was unable to reproduce this behavior reliably 

See model attached [here](https://drive.google.com/file/d/1sAh0i-hV2936tVGKliyX5sMcWbfxqU3-/view?usp=sharing)."
51803,GPU Error: Check failed: work_element_count > 0 (0 vs. 0) ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.6.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2/8.1.0.77
- GPU model and memory: GTX 1060 6GB



Hey everyone,

I am experiencing a strange issue when trying to train  a 3DConv with Custom data generator.
Whne I call `model.fit(train_data,epochs=10)`  where `train_data` is the generator I get this GPU related error
`2021-09-02 12:27:24.281505: F .\tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)`
The error seems to disappear when not using GPU 
I

The generator is this:

```python
class Dataset(tf.keras.utils.Sequence):
    def __init__(self, data, batch_size=BATCH_SIZE, shuffle=True):
        self.data = np.array(data)
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indices = data.index.tolist()

    # @staticmethod
    def __load_dicom_image(self,path, img_size=IMAGE_SIZE, voi_lut=True, rotate=0):
        dicom = pydicom.read_file(path)
        data = dicom.pixel_array
        if voi_lut:
            data = apply_voi_lut(dicom.pixel_array, dicom)
        else:
            data = dicom.pixel_array

        if rotate > 0:
            rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]
            data = cv2.rotate(data, rot_choices[rotate])

        data = cv2.resize(data, (img_size, img_size))
        return data

    

    def __load_dicom_images_3d(self, scan_id, num_imgs=NUM_IMAGES, img_size=IMAGE_SIZE, mri_type=""FLAIR"", split=""train"",
                               rotate=0):

        files = sorted(glob.glob(f""{data_directory}/{split}/{scan_id}/{mri_type}/*.dcm""),
                       key=lambda var: [int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])

        middle = len(files) // 2
        num_imgs2 = num_imgs // 2
        p1 = max(0, middle - num_imgs2)
        p2 = min(len(files), middle + num_imgs2)
        img3d = np.stack([self.__load_dicom_image(f, rotate=rotate) for f in files[p1:p2]]).T
        if img3d.shape[-1] < num_imgs:
            n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))
            img3d = np.concatenate((img3d, n_zero), axis=-1)

        if np.min(img3d) < np.max(img3d):
            img3d = img3d - np.min(img3d)
            img3d = img3d / np.max(img3d)

        return np.expand_dims(img3d, 0)

    def __len__(self):
        return len(self.indices) // self.batch_size

    def __get_data(self, data):
        data = np.array(data)
        images = []
        X = []
        Y = []
        for id in data:
            images.append([self.__load_dicom_images_3d(scan_id=id[0]), id[1]])
        for img in images:
            X.append(img[0])
            Y.append(img[1])
        Y = list(map(int,Y))
        return np.array(X), np.array(Y)

    def __getitem__(self, index):
        print(index)
        data = self.data[index * self.batch_size:(index + 1) * self.batch_size]
        x, y = self.__get_data(data)
       
        return x, y
```

and the 3D CovNet is this:

```python
class MultiBranchCNN(tf.keras.Model):
    def __init__(self):
        super(MultiBranchCNN,self).__init__()
        # self.inputA = tf.keras.Input(shape=(1,256,256,64))

        self.conv3d = Conv3D(64, input_shape=(1,256,256,64),kernel_size=(3, 3,3), activation='relu', padding='same')
        self.maxpool3d = MaxPool3D(pool_size=(3,3, 3))
        self.conv3d2 = Conv3D(64, kernel_size=(3,3, 3), activation='relu', padding='same')
        self.maxpool3d2 = MaxPool3D(pool_size=(3,3 ,3))
        self.conv3d3 = Conv3D(64, kernel_size=(3,3, 3), activation='relu', padding='same')
        self.maxpool3d3 = MaxPool3D(pool_size=(3,3, 3))
        self.Flatten = Flatten()
        self.Dense = Dense(512, activation='relu')
        self.Dropout = Dropout(0.1)
        self.Dense2 = Dense(1, activation='sigmoid')

    def call(self, inputs):
        print(type(inputs))
        # x = self.inputA(inputs)
        x = self.conv3d(inputs)
        x = self.maxpool3d(x)
        x = self.conv3d2(x)
        x = self.maxpool3d2(x)
        x = self.conv3d3(x)
        x = self.maxpool3d3(x)
        x = self.Flatten(x)
        x = self.Dense(x)
        x = self.Dropout(x)
        x = self.Dense2(x)
        return x

```
This error occurs in `Epoch 1/10` (no train happens at all)
I've been trying to change the model architecture and Dataset shape but with no luck.
 
Is this a bug or I am doing something wrong?
Thanks in advance
"
51802,how to use SparseTensor with ctc loss where the label's length varies within one batch,"how to use SparseTensor with ctc loss where the label's length varies within one batch. 

IDK how to formulate a sparse tensor with different lengths and pass into the ctc loss. any example here? 
"
51801,"When loading a saved model with multiple outputs, outputs order is not deterministic","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.5
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 11.2
- GPU model and memory: Nvidia Quadro T1000

**Describe the current behavior**
I have a model that outputs several tensors, each with a different shape and dtype, let's call it model A.
Model's A output tensors are the input of another model B.
What I would like to do is to be able to save model A (using model.save(""..."")), and then use it later by loading it.
I found out that when loading model A and calling it, the outputs order in non deterministic, which causes a problem when I try to use those outputs as inputs to model B.

**Describe the expected behavior**
I'd like the model to maintain the original outputs order

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import tensorflow as tf
from tensorflow import keras

class A_Pow(keras.layers.Layer):
    def __init__(self, **kwargs):
        super(A_Pow, self).__init__(**kwargs)
        self.input_spec = tf.keras.layers.InputSpec(shape=(1, 2), dtype=tf.float32)

    def call(self, input):
        return input * input

class B_AddX(keras.layers.Layer):
    def __init__(self, x, **kwargs):
        super(B_AddX, self).__init__(**kwargs)
        self.x = x
        self.input_spec = tf.keras.layers.InputSpec(shape=(1, 3), dtype=tf.int32)

    def call(self, input):
        return input + self.x


pow_input = tf.keras.Input(shape=(2), batch_size=1, dtype=tf.float32)
addx_input = tf.keras.Input(shape=(3), batch_size=1, dtype=tf.int32)
pow_output = A_Pow()(pow_input)
addx_output = B_AddX(2)(addx_input)
model = tf.keras.Model(inputs=[pow_input, addx_input], outputs=[pow_output, addx_output])

model_output = model([tf.constant([1,2], shape=(1,2), dtype=tf.float32), tf.constant([3,4,5], shape=(1,3), dtype=tf.int32)])

model.compile()
model.save(""multiple_outputs_model"")

loaded_model = tf.saved_model.load(""multiple_outputs_model"")
loaded_model = loaded_model.signatures[""serving_default""]

loaded_model_output = loaded_model(input_1=tf.constant([1,2], shape=(1,2), dtype=tf.float32), input_2=tf.constant([3,4,5], shape=(1,3), dtype=tf.int32))

print(model_output)
print(loaded_model_output)

# The order of the outputs in the loaded model is not determoinistic
# one run output:
#   [<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>]
#   {'b__add_x': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>, 'a__pow': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>}

# another run output:
#   [<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>]
#   {'a__pow': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 4.]], dtype=float32)>, 'b__add_x': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[5, 6, 7]], dtype=int32)>}

# The second run has the valid output (a__pow before b_pow)


```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
51800,Android GPU delegate gives same output from intermediate layers output ,"Hi Team

I have trained a mobilnetv2  model using the tfod API.  Along with the normal detector output (bbox, score,num_detection, confidences), I also wanted an intermediate layers output, so  while converting the model to tflite i specified the intemediate node from where i needed the output ('FeatureExtractor/MobilenetV2/expanded_conv_9/add') as illustrated below : 

`tflite_convert --graph_def_file=$1/tflite_graph/tflite_graph.pb --output_file=$1/tflite_graph/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3','FeatureExtractor/MobilenetV2/expanded_conv_9/add' --inference_type=FLOAT --allow_custom_ops
`

the resulted graph is like this : 

<img width=""1659"" alt=""Screenshot 2021-09-02 at 7 38 20 PM"" src=""https://user-images.githubusercontent.com/74127861/131858728-3b53a171-2e37-4848-ab57-2932f20d52b4.png"">

When I run the tflite model on the android with the XNNPack or CPU, I am able to get the different output  of the intermediate layer which is coming at index 4 
code snippet : 
```
try {
      Interpreter.Options options = new Interpreter.Options();
      options.setNumThreads(NUM_THREADS);
      options.setUseXNNPACK(true);
      d.tfLite = new Interpreter(modelFile, options);
      // 4th index for intermediate output
      int[] shape = d.tfLite.getOutputTensor(4).shape();
      d.tfLiteOptions = options;
    } catch (Exception e) {
      throw new RuntimeException(e);
    }

```

But when i run the same model on GPU delegate, I am getting the same output for every different frame I am passing. The other output like (bbox, score,num_detection, confidences) changes but the intermediate output remains the same 
```
try {
      Interpreter.Options options = new Interpreter.Options();
      CompatibilityList compatList = new CompatibilityList();
      GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();
      GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);
      options.addDelegate(gpuDelegate);

      d.tfLite = new Interpreter(modelFile, options);
      // 4th index for intermediate output
      int[] shape = d.tfLite.getOutputTensor(4).shape();
      d.tfLiteOptions = options;
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
```

**System information**
- Mobile device: OnePlus 6T, Mi Note5 pro
- TFOD Api TensorFlow version : 1.15
- Android Tensorflow Lite version: 2.4.0

Need help to figure out what may be going wrong

[detect_10.tflite.zip](https://github.com/tensorflow/tensorflow/files/7099629/detect_10.tflite.zip)
"
51799,Compiling TF 2.6 in debug mode on Windows env.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): r2.6.0
- Python version: 3.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.3/8
- GPU model and memory: GTX 1060

**Describe the current behavior**
I try to compile my example program that utilises TF:

```
// tensorflow/cc/example/example.cc

#include ""tensorflow/cc/client/client_session.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/tensor.h""

int main() {
  using namespace tensorflow;
  using namespace tensorflow::ops;
  Scope root = Scope::NewRootScope();
  // Matrix A = [3 2; -1 0]
  auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });
  // Vector b = [3 5]
  auto b = Const(root, { {3.f, 5.f} });
  // v = Ab^T
  auto v = MatMul(root.WithOpName(""v""), A, b, MatMul::TransposeB(true));
  std::vector<Tensor> outputs;
  ClientSession session(root);
  // Run and fetch v
  TF_CHECK_OK(session.Run({v}, &outputs));
  // Expect outputs[0] == [19; -3]
  LOG(INFO) << outputs[0].matrix<float>();
  return 0;
}
```

```
bazel build --local_ram_resources=HOST_RAM*.7 --config=dbg --config=windows --copt=/FS --copt=-nvcc_options=disable-warnings --linkopt=/DEBUG:FULL --strip=never --define=no_tensorflow_py_deps=true -s --verbose_explanations --subcommands=pretty_print //tensorflow/cc/example:example
```
The compilation fails with exception:
```
LINK : warning LNK4286: symbol 'TF_DeleteShapeHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'
LINK : warning LNK4286: symbol 'TF_DeleteShapeHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'
LINK : warning LNK4217: symbol 'TF_DeleteDimensionHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '""void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)"" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextScalar' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)' in function '""void __cdecl histogram_summary_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?histogram_summary_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextScalar' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)' in function '""void __cdecl merge_summary_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?merge_summary_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_ShapeInferenceContextScalar' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)' in function '""void __cdecl scalar_summary_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)"" (?scalar_summary_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'
LINK : warning LNK4217: symbol 'TF_NumDims' defined in 'tf_tensor.lib(tf_tensor.obj)' is imported by 'tensor_shape_utils.lib(tensor_shape_utils.obj)' in function '""class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::ShapeDebugString(struct TF_Tensor *)"" (?ShapeDebugString@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAUTF_Tensor@@@Z)'
LINK : warning LNK4217: symbol 'TF_Dim' defined in 'tf_tensor.lib(tf_tensor.obj)' is imported by 'tensor_shape_utils.lib(tensor_shape_utils.obj)' in function '""class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::ShapeDebugString(struct TF_Tensor *)"" (?ShapeDebugString@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAUTF_Tensor@@@Z)'
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@H$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned char,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@E$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBE$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@E$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@E@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::Variant,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::tstring,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,bool,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@_N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<double>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double>,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@N@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class std::complex<double> >::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@V?$complex@N@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<float>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float>,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@M@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class std::complex<float> >::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@V?$complex@M@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,double,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<double,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBN$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,double>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,float,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<float,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@M$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBM$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@M$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,float>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@M@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@Ubfloat16@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUbfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Ubfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Ubfloat16@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::half,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@Uhalf@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Uhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,struct Eigen::half>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Uhalf@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,signed char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<signed char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<signed char,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@C$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBC$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@C$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<short,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@F$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBF$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@F$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,short>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@F@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@G$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBG$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@G$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned short>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@G@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned int,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@I$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBI$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@I$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned int>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@I@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,__int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<__int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<__int64,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@_J$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,__int64>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_J@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned __int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@_K$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned __int64>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_K@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@H$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::Variant,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::tstring,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,bool,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@_N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<double>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double>,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@N@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class std::complex<double> >::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@V?$complex@N@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<float>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float>,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@M@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class std::complex<float> >::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@V?$complex@M@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,double,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<double,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBN$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,double>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,float,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<float,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@M$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBM$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@M$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,float>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@M@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@Ubfloat16@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUbfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Ubfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Ubfloat16@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::half,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@Uhalf@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Uhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,struct Eigen::half>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Uhalf@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,signed char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<signed char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<signed char,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@C$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBC$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@C$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned char,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@E$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBE$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@E$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@E@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<short,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@F$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBF$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@F$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,short>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@F@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@G$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBG$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@G$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned short>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@G@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned int,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@I$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBI$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@I$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned int>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@I@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,__int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<__int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<__int64,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@_J$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,__int64>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_J@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned __int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@_K$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned __int64>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_K@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
bazel-out\x64_windows-dbg\bin\tensorflow\cc\example\example.exe : fatal error LNK1120: 36 unresolved externals
Target //tensorflow/cc/example:example failed to build
INFO: Elapsed time: 5521.801s, Critical Path: 3434.58s
INFO: 2724 processes: 112 internal, 2612 local.
FAILED: Build did NOT complete successfully
```"
51798,tensorflow2 memory leak when using tf.function,"I post this question here because no one is able to answer it for over a month. Hope someone here would give the solution.
https://stackoverflow.com/questions/68379996/how-to-profile-tensorflow-memory"
51797,Interpreter invocation error after input tensor resizing ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0

Error: Post reshaping the input sizes of my tensors - the interpreter invocation fails with an input-output shape mismatch. [ Stack trace in the linked colab file ]. 

### 2. Code

[Here](https://colab.research.google.com/drive/1Vsinmju5FsbiEtfD9BqBHWeKxM3LES-U?usp=sharing ) is the end-end flow of conversion. You can refer to the test files used to generate the test data ( for dummy input ) here: [dev.hi](https://drive.google.com/file/d/1HJtbqzF3X-51S01DFGn5XOUambvMQriO/view?usp=sharing ) and [dev.en](https://drive.google.com/file/d/1IKqg2c-SJiKmd2fsFmODxeB0af3rBATl/view?usp=sharing)

### 3. Failure after conversion

The model gets converted ( and quantized ) successfully though despite setting the model inputs with a valid input sample ( during model.predict(), I have also explicitly tried using model._set_inputs() - that doesn't work either ) the input shapes of the graph are [1,1] for all the arguments. I am forced to reshape them hence, and that's when the error occurs. 

Note: In the colab file, I have shown the input details pre and post resizing - while the argument shape seems to be altered according to my resizing shapes - the 'shape_signature' does not change. Could this be the source of the error ? ( Is this expected behaviour ) 

### 5. (optional) Any other info / logs

The linked colab file has the exact stack traces. Note that if I do not reshape my tensors - the invocation runs but due to the [1,1] limitation - there is no meaningful input that I can provide to the model. "
51796,Attributes section in Keras Model (documentation) is not generated properly,"Model attributes section in the documentation has 2 issues:

1. It is duplicated: [attributes](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#attributes) and [attributes_1](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#attributes_1). You might have to look at table of content to spot the difference.
2. Some attributes are missing even though they have been documented and have @property decorator in the github code. E.g [metrics](https://github.com/bizzyvinci/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L635-L683) is missing but [metrics_name](https://github.com/bizzyvinci/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L685-L723) is included.

Note that the documentation link is nightly and the code link is the current master. I guess there might be issue with how the attribute section is generated.
"
51795,tf.GradientTape not working properly.,"Recently I made new network & trained it with tf.Gradient.

But losses didn't decrease.

So I tested in different examples in [tensorflow tutorial](https://www.tensorflow.org/tutorials/images/cnn?hl=en)

When I tested, I used tensorflow turorial code except training part.

Please let me know if there is any wrong usage.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 20.04 
- TensorFlow installed from (source or binary): pip install 
- TensorFlow version (use command below): 2.4.0 / 2.6.0
- Python version: Python 3.8
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 11.4 
- GPU model and memory: GeForce 1080 Ti * 2

v2.6.0-rc2-32-g919f693420e 2.6.0

**Describe the current behavior**
![Screenshot from 2021-09-02 19-45-42](https://user-images.githubusercontent.com/18378446/131830787-5992bee1-7967-403b-b75e-81c74c0cc27e.png)

**Describe the expected behavior**
![Screenshot from 2021-09-02 19-46-27](https://user-images.githubusercontent.com/18378446/131830878-9d5a2349-4cff-4bd5-bced-48138beefb60.png)
"
51793,Unable to restore the model with shared ops using from_config(),"**System information**
- OS Platform and Distribution:  Linux Ubuntu 18.04
- TensorFlow installed from pip
- TensorFlow version: 2.6.0
- Python version: 3.7.11
- CUDA Version: 11.2
-  Driver Version: 460.80

**Describe the current behavior**
Can not load model with shared ops from config. During loading, the outputs of the shared operation are placed in the wrong positions.

**Describe the expected behavior**
The original model and the restored model are the same

**Standalone code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

print(tf.__version__)

def get_test_model():
    input_1 = tf.keras.Input(shape=(5, 5, 2))
    input_2 = tf.keras.Input(shape=(5, 5, 2))
    image_shape = tf.keras.Input(shape=(1, 2), name='image_shape')

    clips = []
    relu = tf.keras.layers.ReLU()
    for inp in [input_1, input_2]:
        dy = inp[..., 0:1]
        dx = inp[..., 1:2]

        y = relu(relu(dy) + dx)
        x = relu(relu(dx) + dy)
        cat = tf.concat([y, x], -1)

        height, width = tf.unstack(image_shape, axis=-1)
        max_length = tf.stack(
            [relu(height), relu(width)], axis=-1)

        clip = tf.math.minimum(cat, max_length)
        clips.append(clip)

    out = tf.concat(clips, axis=1)
    return tf.keras.Model(inputs=[input_1, input_2, image_shape], outputs=out)


input_1 = np.random.rand(1, 5, 5, 2)
input_2 = np.random.rand(1, 5, 5, 2)
image_shape = np.random.rand(1, 1, 2)
inputs = [input_1, input_2, image_shape]

model = get_test_model()

ref = model.predict(inputs)
print(ref.shape)  # succsess (1, 10, 5, 2)

restored_model = tf.keras.Model.from_config(model.get_config())  # failed
out = restored_model.predict(inputs)
print(out.shape)
```
original frozen graph
![image](https://user-images.githubusercontent.com/22346860/131807492-4cc0a0e4-c94b-4928-9deb-c96945770510.png)


Error with restore model from config:
```bash
Traceback (most recent call last):
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1880, in _create_c_op
    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 4 but is rank 2 for '{{node tf.concat/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](Placeholder, Placeholder_1, tf.concat/concat/axis)' with input shapes: [?,5,5,1], [?,1], [].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tf_bug.py"", line 42, in <module>
    restored_model = tf.keras.Model.from_config(model.get_config())  # failed
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/training.py"", line 2397, in from_config
    functional.reconstruct_from_config(config, custom_objects))
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/functional.py"", line 1283, in reconstruct_from_config
    process_node(layer, node_data)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/functional.py"", line 1231, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/base_layer.py"", line 977, in __call__
    input_list)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/base_layer.py"", line 1115, in _functional_construction_call
    inputs, input_masks, args, kwargs)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/base_layer.py"", line 848, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/base_layer.py"", line 888, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/layers/core.py"", line 1350, in _call_wrapper
    return self._call_wrapper(*args, **kwargs)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/layers/core.py"", line 1382, in _call_wrapper
    result = self.function(*args, **kwargs)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1769, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1228, in concat_v2
    ""ConcatV2"", values=values, axis=axis, name=name)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 750, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 601, in _create_op_internal
    compute_device)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3569, in _create_op_internal
    op_def=op_def)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2042, in __init__
    control_input_ops, op_def)
  File ""/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1883, in _create_c_op
    raise ValueError(str(e))
ValueError: Shape must be rank 4 but is rank 2 for '{{node tf.concat/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](Placeholder, Placeholder_1, tf.concat/concat/axis)' with input shapes: [?,5,5,1], [?,1], [].
```
"
51792,weighted_moments produces NaNs when weights are all zeros,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git
- Python version: 3.9
- Bazel version (if compiling from source): 4.2
- GCC/Compiler version (if compiling from source): gcc-10
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
`tf.nn.weighted_moments` produces NaNs when all weights are zeros.
**Describe the expected behavior**
Correct result in this case should be zeros.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):
Do not divide by 0 unless you are Chuck Norris.

**Standalone code to reproduce the issue**
```python
x = tf.random.uniform((5, 3))
w = tf.zeros((5, 1))

tf.nn.weighted_moments(x, axes=0, frequency_weights=w)

(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([nan, nan, nan], dtype=float32)>,
 <tf.Tensor: shape=(3,), dtype=float32, numpy=array([nan, nan, nan], dtype=float32)>)


```


"
51790,Transfer learning ResNet mode performance drop in recent versions,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04, 18.04, 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
2.0.0, 2.2.0, 2.4.0, 2.5.0, 2.6.0
- Python version:
3.7, 3.8, 3.9
- Bazel version (if compiling from source):
No
- GCC/Compiler version (if compiling from source):
No
- CUDA/cuDNN version:
10.2, 11.1
- GPU model and memory:
1660 Super (6GB), 3070 Laptop (8GB), 3090 (24GB), whatever google's colab provides

**Describe the current behavior**
ResNet (50, 101 and 152) perform at about 40-50% Accuracy and F1 score after transfer learning.
**Describe the expected behavior**
6-8 months ago, using the same exactly dataset and script, the same models performed above 75% at the same metrics.
The initial good results were obtained using google colab.
Now, I have tested the **same code and dataset** in google colab using all the tensorflow versions mentioned above.
Have also tested on Ubuntu 16.04 with CUDA 10.2 and CUDNN 7.6 and Ubuntu 18.04 and 20.04 with CUDA 11.1 CUDNN 8.2.

The same results (old and new) are consistent (+- 1%) for all the other models I have used (All DenseNets, VGG16 and 19 and MobileNet).

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
No
- Briefly describe your candidate solution(if contributing):
Not sure

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Dataset is private so I can't.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Transfer learning process follows the one provided [by keras](https://keras.io/guides/transfer_learning/).
"
51787,tf.nn.max_pool_with_argmax to support multiple dimensions (ND),"Currently, there is a function called [tf.nn.max_pool_with_argmax](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax). 
This function is useful to, for example, implement [MaxUnpooling](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/6476c4d5-19f2-455f-8590-c6f99504b7a5.xhtml).


However, it would be great if the function supported several dimensions, not only 2D images.  [`tf.nn.max_pool`](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) for example supports any dimensionality. 

This feature will enable the implementation of ND max unpooling.
In particular, it will help me implement the [feature request](https://github.com/NEGU93/cvnn/issues/10) asked on my repository.


"
51786,Type Error: __init__() got an unexpected keyword argument 'epochs',"
![1](https://user-images.githubusercontent.com/26819449/131669104-e1037302-ddde-4eda-9ecf-bfd57a583653.JPG)
![2](https://user-images.githubusercontent.com/26819449/131669108-9b95b63c-1054-4195-95bc-445a29095346.JPG)
![3](https://user-images.githubusercontent.com/26819449/131669115-228d7a39-91f2-4aa3-9482-86fc1dc9944e.JPG)
![4](https://user-images.githubusercontent.com/26819449/131669118-a39aab97-1181-4cb3-b600-4284912004e8.JPG)
 
I tried to solve this particular error. I was trying to implement a particular library from Github.
tensorflow=2.2 
keras=2.4.3
I am only using these two libraries in this Code.

And please also tell me which documentation to refer to, it helps.
Thank You.



"
51784,Repeat weights in dense layer / Forum registration not possible,"Hello I tried to ask this question in the Forum, but somehow the Forum won't let me register saying something about timeout even if I refresh the page for registering. So instead I post this here as feature request.

What I want to achieve is that say for a 3x8x8 input for a dense layer I want the same 8x8 weights to be trained and repeated over the whole input. Is there a condition I can set for the training to repeat the weights multiple times so I train 8x8 weights over a Nx8x8 input instead of training Nx8x8 individual weights over the input?"
51783,TypeError: 'NoneType' object is not callable,"
![3](https://user-images.githubusercontent.com/26819449/131661895-8f6100c7-9d01-43c4-bf4f-034853cafaa4.JPG)

![2](https://user-images.githubusercontent.com/26819449/131661655-91576b86-8977-475a-b107-13dad63be3d1.JPG)

![1](https://user-images.githubusercontent.com/26819449/131661662-97ec164b-8596-4b7b-b7ab-e17b39b65d82.JPG)
The error is in this part only before that is nothing just library import.
This is a bug I think.

"
51780,"2021-09-01 16:39:11.050592: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operatiWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3554913A0> and will run it as-is. Please report this to the TensorFlow team. To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
51779,SavedModel imported in ML.NET throws vacuous error,"I create a model in Python and save it with `tf.saved_model.save(model, model_save_path)` into a `.pb` file.
Then I load the model in ML.NET, create a pipeline, and make a prediction with the following code:
```csharp
MLContext ml_context = new MLContext();
TensorFlowModel tensor_flow_model = ml_context.Model.LoadTensorFlowModel(model_save_path);
IEstimator<ITransformer> pipeline = tensor_flow_model.ScoreTensorFlowModel(new[] { output }, new[] { input_1, input_2, input_3, input_4 }, addBatchDimensionInput: true);
var estimator = pipeline.Fit(data);
var transformed_values = estimator.Transform(data);
var out_scores = ml_context.Data.CreateEnumerable<Prediction>(transformed_values, reuseRowObject: false);
foreach (var prediction in out_scores)
{
    int num_classes = 0;
    foreach (var class_score in prediction.StatefulPartitionedCall)
    {
        Console.WriteLine($""Class #{num_classes++} score = {class_score}"");
    }
    Console.WriteLine(new string('-', 10));
}
```
The first `foreach` loop throws an error, which I do not understand. The error message reads:
```
2021-09-01 09:38:03.364008: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at partitioned_function_ops.cc:118 : Invalid argument: Expected input[1] == 'TensorArrayV2_1/element_shape:output:0' to be a control input.
    In {{node TensorArrayV2Stack/TensorListStack}}
```
What does the error message mean? What is happening? And how do I fix it?

EDIT:
I just found out, that `out_scores` is empty at the time reaching `foreach (var prediction in out_scores)` and therefore causing this error."
51778,"When I check the TensorBoard for a Worker of ParameterServer traning, I can't see any send/recv or push/pull op to the PS? Is this normal?","Hi, developers of tensorflow.
The environment, code are listed below:

TF version: nightly
OS: ubuntu16.04
cuda: 11.0


Worker code:

```
import multiprocessing
import os
import json
import random
import portpicker
import tensorflow as tf
from tensorflow.keras.layers.experimental import preprocessing


def create_in_process_cluster(num_workers, num_ps):
  """"""Creates and starts local servers and returns the cluster_resolver.""""""
  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]
  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]

  cluster_dict = {}
  cluster_dict[""worker""] = [""ip:port""]
  if num_ps > 0:
    cluster_dict[""ps""] = [""ip:port""]
  print(""=========================="", cluster_dict[""worker""], cluster_dict[""ps""])
  cluster_spec = tf.train.ClusterSpec(cluster_dict)

  # Workers need some inter_ops threads to work properly.
  worker_config = tf.compat.v1.ConfigProto()
  if multiprocessing.cpu_count() < num_workers + 1:
    worker_config.inter_op_parallelism_threads = num_workers + 1
  
  os.environ[""TF_CONFIG""] = json.dumps({
    ""cluster"": {
        ""worker"": cluster_dict[""worker""],
        ""ps"": cluster_dict[""ps""],
    },
    ""task"": {""type"": ""worker"", ""index"": 0}
  })
  cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()
  return cluster_resolver

# Set the environment variable to allow reporting worker and ps failure to the
# coordinator. This is a workaround and won't be necessary in the future.
os.environ[""GRPC_FAIL_FAST""] = ""use_caller""

NUM_WORKERS = 1
NUM_PS = 1

cluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)
os.environ[""GRPC_FAIL_FAST""] = ""use_caller""
if cluster_resolver.task_type in (""worker"", ""ps""):
    server = tf.distribute.Server(
        cluster_resolver.cluster_spec(),
        job_name=cluster_resolver.task_type,
        task_index=cluster_resolver.task_id,
        protocol=cluster_resolver.rpc_layer or ""grpc"",
        start=True)

    
print(""=========================="")

strategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)
    
def dataset_fn(input_context):
  global_batch_size = 64
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)

  x = tf.random.uniform((10, 10))
  y = tf.random.uniform((10,))

  dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(10).repeat()
  dataset = dataset.shard(
      input_context.num_input_pipelines,
      input_context.input_pipeline_id)
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(2)

  return dataset

dc = tf.keras.utils.experimental.DatasetCreator(dataset_fn)

with strategy.scope():
  model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])

model.compile(tf.keras.optimizers.SGD(), loss='mse', metrics=['accuracy'], steps_per_execution=10)


working_dir = './my_working_dir'
log_dir = os.path.join(working_dir, 'log')
ckpt_filepath = os.path.join(working_dir, 'ckpt')
backup_dir = os.path.join(working_dir, 'backup')
tb_dir = os.path.join(working_dir, 'tb')

callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir=log_dir),
    tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_filepath),
    tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=backup_dir),
    tf.keras.callbacks.TensorBoard(log_dir=tb_dir, profile_batch = 100000000)
]

model.fit(dc, epochs=10, steps_per_epoch=20, callbacks=callbacks)
```

The graph of tensorboard:

![graph](https://user-images.githubusercontent.com/14342657/131599322-5e9ca91b-c49c-4a2c-9f79-171cb828c321.PNG)


I can't confirm whether this Worker process is interacting with PS to update the parameters in each step.
Are the communication operations not compiled into the graph?


"
51776,Building from source failed on Python 3.10,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
  - [`manylinux2014` docker image](https://github.com/pypa/manylinux#manylinux2014-centos-7-based) from [pypa / manylinux](https://github.com/pypa/manylinux)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 B
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6.0
- Python version: 3.10.0rc1
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 8.3.1
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

### `distutils` is deprecated

The `distutils` is deprecated in Python 3.10. In [python_configure.bzl](https://github.com/tensorflow/tensorflow/blob/master/third_party/py/python_configure.bzl#L150), the deprecation message will be printed prior to the include path, causing error on return.

### ABC is removed

(detailed logs below)

According to the [collections](https://docs.python.org/3/library/collections.html) package:

> Deprecated since version 3.3, will be removed in version 3.10: Moved Collections Abstract Base Classes to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.9.

` _message.so` from `protobuf` seems to cause this problem.

```
    from google.protobuf.pyext import _message
AttributeError: module 'collections' has no attribute 'MutableSequence'
```

**Any other info / logs**

```
[root@129a8b5b98ad tensorflow]# BAZEL_LINKLIBS=-l%:libstdc++.a bazel build --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=91
INFO: Reading rc options for 'build' from /opt/tf/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /opt/tf/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /opt/tf/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/opt/python/cp310-cp310/bin/python3.10 --action_env PYTHON_LIB_PATH=/opt/python/cp310-cp310/lib/python3.10/site-packages --python_path=/opt/python/cp310-cp310/bin/python3.10
INFO: Found applicable config definition build:short_logs in file /opt/tf/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /opt/tf/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:noaws in file /opt/tf/tensorflow/.bazelrc: --define=no_aws_support=true
INFO: Found applicable config definition build:nogcp in file /opt/tf/tensorflow/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:nohdfs in file /opt/tf/tensorflow/.bazelrc: --define=no_hdfs_support=true
INFO: Found applicable config definition build:nonccl in file /opt/tf/tensorflow/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:linux in file /opt/tf/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false
INFO: Found applicable config definition build:dynamic_kernels in file /opt/tf/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /opt/tf/tensorflow/tensorflow/python/keras/api/BUILD:133:19: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 26, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 32, in <module>
    from tensorflow.core.framework import function_pb2
  File ""/root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/core/framework/function_pb2.py"", line 7, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
AttributeError: module 'collections' has no attribute 'MutableSequence'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /opt/tf/tensorflow/tensorflow/python/tools/BUILD:81:10 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)
INFO: Elapsed time: 4.857s, Critical Path: 2.92s
INFO: 5 processes: 5 internal.
FAILED: Build did NOT complete successfully
[root@129a8b5b98ad tensorflow]#
```"
51774,Sizing problem with BatchDataset in Tensorflow 2.3.1,"I want to extract the tensor contained within a BatchDataset. It appears I run into a sizing issue -- if I call a batch of 4500 elements no problem (i.e., extractable), but if I create a batch of 5000 elements no dice.

![image](https://user-images.githubusercontent.com/6710629/131588513-c9ddacbd-c5ec-4ff0-b5de-bc9a75def56b.png)

If `inp` does not exist in the second example above, then when trying to call `inp` there is a `NameError: name 'inp' not defined.`

Broadly speaking, anyone know a work around? I am looking to create (and draw from) with a batch size of ~7500.

**System information**
- Stock Tensorflow installed using `pip`
- Ubuntu 20.04
- Sony Vaio Laptop
- Tensorflow: `v2.3.0-54-gfcc4b966f1 2.3.1`
- Python: `3.8.5`
"
51773,tensorboard no runs found for PyTorch 1.9 in VSCode in CentOS 7,"<img width=""1127"" alt=""Screen Shot 2021-08-31 at 7 22 57 PM"" src=""https://user-images.githubusercontent.com/1892917/131588221-3152f224-9a08-40d6-8474-f404e26dd9a7.png"">

Any tips on how to use Tensorboard in VSCode for PyTorch 1.9?
"
51772,AttributeError: 'NoneType' object has no attribute 'label_map',"I was running the notebook in Jupyter Notebook (the version that can be downloaded from the Colab notebook), and ran into this error in step 3 shown in the image.

![label_map error](https://user-images.githubusercontent.com/81659220/131559999-97b99027-43f1-4ff0-8198-c79286fdb1b4.png)
 
Anyone know how to fix this?  "
51771,tf.vectorized_map fails on ragged tensors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 21.04
- TensorFlow installed from (source or binary): pip install tensorflow==2.6.0
- TensorFlow version (use command below): 2.6.0
- Python version: 3.9
- CUDA/cuDNN version: 11.4
- GPU model and memory: 2080ti

**Describe the current behavior**

tf.vectorized_map fails on RaggedTensor

**Describe the expected behavior**

tf.vectorized_map succeeds on RaggedTensor

**Standalone code to reproduce the issue**

```
import tensorflow as tf

tf.__version__ # 2.6.0

A1 = tf.constant([[1,3], [1,5]], dtype=tf.float32)

b1 = tf.constant([3,4], shape = (2,1), dtype=tf.float32)

tf.linalg.lstsq(A1,b1) # SUCCEEDS, single task

A2 = tf.constant([[1,3], [1,4], [1,5]], dtype=tf.float32)

b2 = tf.constant([2,3,4], shape = (3,1), dtype=tf.float32)

tf.linalg.lstsq(A2,b2) # SUCCEEDS, single task

A1A1 = tf.stack([A1,A1])

b1b1 = tf.stack([b1,b1])

tf.linalg.lstsq(A1A1, b1b1) # SUCCEEDS, parallel task

A1A1ragged = tf.ragged.stack([A1,A1])

tf.linalg.lstsq(A1A1ragged, b1b1) # FAILS TypeError: object of type 'RaggedTensor' has no len()

@tf.function
def least_squares(task):
    A, b = task
    return tf.linalg.lstsq(A.to_tensor(), b)

tf.stack([least_squares(task) for task in zip(A1A1ragged, b1b1)]) # SUCCEEDS, serial task

tf.map_fn(least_squares, (A1A1ragged, b1b1), dtype = tf.float32) # SUCCEEDS, possible but not guaranteed parallel task

tf.vectorized_map(least_squares, (A1A1ragged, b1b1))  # FAILED, ValueError: Received a shape scalar with unknown static value

A1A2ragged = tf.ragged.stack([A1,A2])

b1b2 = tf.ragged.stack([b1,b2])

@tf.function
def least_squares_ragged(task):
    A, b = task
    return tf.linalg.lstsq(A.to_tensor(), b.to_tensor())

tf.ragged.stack([least_squares_ragged(task) for task in zip(A1A2ragged, b1b2)])  # SUCCEED, but serial

tf.map_fn(least_squares_ragged, (A1A2ragged, b1b2), dtype = tf.float32) # SUCCEED,not guaranteed parallel

tf.vectorized_map(least_squares_ragged, (A1A2ragged, b1b2)) # FAIL  ValueError: Received a shape scalar with unknown static value

```

"
51770,TensorFlow master build fails on s390x due to boringssl issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

Reference issues: https://github.com/tensorflow/tensorflow/issues/14039, https://github.com/tensorflow/tensorflow/issues/50351

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source 
- TensorFlow version: 2.7.0 (from master branch)
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): bazel 3.7.2- (@non-git)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

TensorFlow master build is failing on `s390x` due to `boringssl` issue. It looks like some recent commits in master branch have triggered it. 

I made some tests and it looks like this commit (https://github.com/tensorflow/tensorflow/commit/738e5aa37935f713a2366fc00a26d1d5830cb971) trigged the issue (the previous commit didn't trigger it). This commit has the dependency of `@com_github_grpc_grpc` which needs `boringssl` and the code appears to be in the `experimental` folder.

Since `boringssl` is still not supported on `s390x`, is there any way to disable this part on `s390x` build? Thank you very much!

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" build  --define=tensorflow_mkldnn_contraction_kernel=0 //tensorflow/tools/pip_package:build_pip_package`

or

`bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" build  --define=tensorflow_mkldnn_contraction_kernel=0 --config=noaws //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```bash
ERROR: /home/test/Tensorflow_tmp/_bazel_test/78e38be8987e210d400487db993cf28d/external/com_github_grpc_grpc/BUILD:2013:16: C++ compilation of rule '@com_github_grpc_grpc//:tsi' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 46 argument(s) skipped)
In file included from external/boringssl/src/include/openssl/ssl.h:145:0,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl/session_cache/ssl_session.h:29,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl/session_cache/ssl_session_cache.cc:23:
external/boringssl/src/include/openssl/base.h:122:2: error: #error ""Unknown target CPU""
 #error ""Unknown target CPU""
  ^~~~~
In file included from external/boringssl/src/include/openssl/asn1.h:68:0,
                 from external/boringssl/src/include/openssl/x509.h:70,
                 from external/boringssl/src/include/openssl/pem.h:67,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl/session_cache/ssl_session.h:29,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl/session_cache/ssl_session_cache.cc:23:
external/boringssl/src/include/openssl/bn.h:165:2: error: #error ""Must define either OPENSSL_32_BIT or OPENSSL_64_BIT""
 #error ""Must define either OPENSSL_32_BIT or OPENSSL_64_BIT""
```
```bash
ERROR: /home/test/Tensorflow_tmp/_bazel_test/78e38be8987e210d400487db993cf28d/external/boringssl/BUILD:130:11: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 24 argument(s) skipped)
In file included from external/boringssl/src/include/openssl/digest.h:60:0,
                 from external/boringssl/src/crypto/x509v3/v3_skey.c:61:
external/boringssl/src/include/openssl/base.h:122:2: error: #error ""Unknown target CPU""
 #error ""Unknown target CPU""
  ^~~~~
In file included from external/boringssl/src/include/openssl/asn1.h:68:0,
                 from external/boringssl/src/include/openssl/x509.h:70,
                 from external/boringssl/src/include/openssl/x509v3.h:60,
                 from external/boringssl/src/crypto/x509v3/v3_skey.c:64:
external/boringssl/src/include/openssl/bn.h:165:2: error: #error ""Must define either OPENSSL_32_BIT or OPENSSL_64_BIT""
 #error ""Must define either OPENSSL_32_BIT or OPENSSL_64_BIT""
```
"
51769,Saving a NSL model,"**NotImplementedError: Saving `AdversarialRegularization` models is currently not supported. Consider using `save_weights` or saving the `base_model`.**
I trained a Neural Structured Learning model with AdversarialRegularization and tried to save the model as a SavedModel format as well as a HDF5 model. And in both case I faced this error. Can anyone let me know how to save the model or a workaround.
A simple example can be found in this [notebook](https://colab.research.google.com/drive/1CV6uCAys9lcbD33oZv39BvU5AfnHDYya?usp=sharing)
"
51768,"When use keras + TensorBoard with parameter server strategy, error on worker is raised and core is dumped.","Hi, developers of tensorflow.
The environment, code, and issue I encountered are listed below:

TF version: nightly
OS: ubuntu16.04
cuda: 11.0

Worker code: (PS code is almost the same as this one)
```
import multiprocessing
import os
import json
import random
import portpicker
import tensorflow as tf
from tensorflow.keras.layers.experimental import preprocessing


def create_in_process_cluster(num_workers, num_ps):
  """"""Creates and starts local servers and returns the cluster_resolver.""""""
  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]
  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]

  cluster_dict = {}
  cluster_dict[""worker""] = [""ip:port""]
  if num_ps > 0:
    cluster_dict[""ps""] = [""ip:port""]
  print(""=========================="", cluster_dict[""worker""], cluster_dict[""ps""])
  cluster_spec = tf.train.ClusterSpec(cluster_dict)

  # Workers need some inter_ops threads to work properly.
  worker_config = tf.compat.v1.ConfigProto()
  if multiprocessing.cpu_count() < num_workers + 1:
    worker_config.inter_op_parallelism_threads = num_workers + 1
  
  os.environ[""TF_CONFIG""] = json.dumps({
    ""cluster"": {
        ""worker"": cluster_dict[""worker""],
        ""ps"": cluster_dict[""ps""],
    },
    ""task"": {""type"": ""worker"", ""index"": 0}
  })
  cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()
  return cluster_resolver

# Set the environment variable to allow reporting worker and ps failure to the
# coordinator. This is a workaround and won't be necessary in the future.
os.environ[""GRPC_FAIL_FAST""] = ""use_caller""

NUM_WORKERS = 1
NUM_PS = 1

cluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)
os.environ[""GRPC_FAIL_FAST""] = ""use_caller""
if cluster_resolver.task_type in (""worker"", ""ps""):
    server = tf.distribute.Server(
        cluster_resolver.cluster_spec(),
        job_name=cluster_resolver.task_type,
        task_index=cluster_resolver.task_id,
        protocol=cluster_resolver.rpc_layer or ""grpc"",
        start=True)


strategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)
    
def dataset_fn(input_context):
  global_batch_size = 64
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)

  x = tf.random.uniform((10, 10))
  y = tf.random.uniform((10,))

  dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(10).repeat()
  dataset = dataset.shard(
      input_context.num_input_pipelines,
      input_context.input_pipeline_id)
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(2)

  return dataset

dc = tf.keras.utils.experimental.DatasetCreator(dataset_fn)

with strategy.scope():
  model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])

model.compile(tf.keras.optimizers.SGD(), loss='mse', metrics=['accuracy'], steps_per_execution=10)


working_dir = './my_working_dir'
log_dir = os.path.join(working_dir, 'log')
ckpt_filepath = os.path.join(working_dir, 'ckpt')
backup_dir = os.path.join(working_dir, 'backup')

callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir=log_dir),
    tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_filepath),
    tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=backup_dir),
    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
]

tf.summary.trace_on()

model.fit(dc, epochs=50000, steps_per_epoch=20, callbacks=callbacks)

import time
time.sleep(100000)
```

The error of the Worker process:

```
Traceback (most recent call last):
  File ""worker_in_process_ps.py"", line 99, in <module>
    model.fit(dc, epochs=50000, steps_per_epoch=20, callbacks=callbacks)
  File ""keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""tensorflow/python/ops/summary_ops_v2.py"", line 1342, in trace_export
    raise ValueError(""Must enable trace before export."")
ValueError: Must enable trace before export.
terminate called without an active exception
Aborted (core dumped)
```

I want to check the graph on the worker by TensorBoard. Anyone knows how to fix this issue?

"
51766,Issue about using Nsight System on Tensorflow2.6.0,"**System information**


\- OS Platform Linux Ubuntu 18.04.5

\- TensorFlow installed from **docker**(tensorflow/tensorflow:2.6.0rc2-gpu)

\- TensorFlow version :**2.6.0rc2-gpu** (default)

\- Python version:**3.6.9** (default)

\- CUDA version:**11.2** (default)

\- cuDNN version:**8.1.0** (default)

\- GPU model and memory:**NVIDIA A6000** / 48685MiB

 

**Describe the current behavior**

When using Nsight System to profile my training, I found that there were only cuDNN and cuBLAS in my profiling file like this, but CUDA API and CUDA HW was missing.

 
![1](https://user-images.githubusercontent.com/89841382/131491418-ef079c92-c7b9-47e8-a557-7bf57950ff0c.png)


But when I change the tensorflow version from 2.6.0rc2 to 2.4.3(with CUDA 11.0), the CUDA API and CUDA HW appeared.

 
![2](https://user-images.githubusercontent.com/89841382/131491433-f980c222-5c22-480c-9661-af95bf80bf4e.jpg)


I'd like to know the cause of this problem and what  I can do to make CUDA API and CUDA HW appear on Tensorflow2.6.0.rc2.

Thank you.

**Standalone code to reproduce the issue**

The run scripts is like this.


```
nsys profile -t cuda,osrt,nvtx,cudnn,cublas \
       -o $1 \
       -f true \
       -w true \
       --sampling-period 2000000\
       python train.py

```

The core code of train.py is like this.

```python
# ---preprocess dataset---
# ---get model and compile
# Add nvtx to nsys
import ctypes
_cuda_tools_ext = ctypes.CDLL(""libnvToolsExt.so"")
_cuda_tools_ext.nvtxRangePushA(ctypes.c_char_p(""start train"".encode('utf-8')))

model.fit(train_db, 
	epochs=1, 
	validation_data = test_db,
	validation_freq=1)
_cuda_tools_ext.nvtxRangePop()

```"
51763,How is TensorFlow compatible with Python2 and Python3 ？,"I'm doing some development based on the community's TensorFlow, but it only supports a single version of Python.It seems that the community’s TensorFlow supports both Python2 and Python3：
![image](https://user-images.githubusercontent.com/54703662/131481441-bf9f99a2-5456-4e1d-b3d1-c7b3e35240e8.png)

I have read the [Google Python Style Guide](https://github.com/google/styleguide/blob/gh-pages/pyguide.md#s2.20-modern-python), and know that ""from __future__ import"" is used to make Python 2 and 3 compatible. However, it seems to involve only a few aspects, but there are many differences between Python2 and Python3(e.g. zip()). I want to know, in addition to the above, what work has TensorFlow done to achieve compatibility between Python 2 and 3 ?

Besides, what should I do to test and verify the compatibility of my code?"
51760,tf.distribute.MirroredStrategy does not work via srun (SLURM) causes NCCL error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (Ubuntu 20.04.2 LTS)
- TensorFlow installed from container 21.05 (https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_21-05.html#rel_21-05)
- TensorFlow version (2.5):
- Python version: 3.8.5
- CUDA/cuDNN version: 11.3.0/8.2.0.51
- GPU model and memory: A100

The distributed training runs fails when training via slurm (using srun).

The code is run inside an enroot container. Due to slurm this container has a number of slurm specific environment variables set.

So, using ```MirroredStrategy``` to distribute training fails due to NCCL errors on a simple example.

Note, a number of other distributed options work as is highlighted in the code.

NOTE, this code works fine outside of the slurm environment (in the exact same container). The slurm environment variables seem to be creating an issue with NCCL. 

The srun command looks like

```
srun \
        --container-image $container_path \
        -N1 \
        --nodelist=$node_name \
        --gpus-per-node=$gpus_per_node \
        --cpus-per-task=$cpus_per_task \
        --mem=$DEFAULT_RAM \
```

```python
#!/usr/bin/env python
# coding: utf-8

# Can't use tf.distribute.MirroredStrategy in srun (slurm) enviroment

# Tried with tf 2.5 and tf nightly.

import tensorflow as tf

# Force dynamic memory growth
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

tf.__version__


# op 1 . NCCL error in slurmn enviroment. Works fine inside enroot container (not submitted via srun)
strategy = tf.distribute.MirroredStrategy()

# op 2. Not using NCCL. Works.
#strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())

# op 2. Works in slurmn enviroment. Needs to be optimized
#slurm_resolver = tf.distribute.cluster_resolver.SlurmClusterResolver()
#strategy = tf.distribute.MultiWorkerMirroredStrategy(cluster_resolver=slurm_resolver)

# op 3 # Works in slurmn enviroment
#strategy = tf.distribute.MultiWorkerMirroredStrategy()


from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt


(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0


with strategy.scope():

    model = models.Sequential()
    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu'))
    
    model.add(layers.Dense(10))
    # ADD sync bn..
    model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])



history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
```

Error

```
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2021-08-31 15:13:58.031219: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2021-08-31 15:13:58.050668: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2245715000 Hz
Epoch 1/10
2021-08-31 15:14:03.739139: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-08-31 15:14:04.423163: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200
2021-08-31 15:14:05.254400: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200
2021-08-31 15:14:05.933033: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-08-31 15:14:06.133786: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200
2021-08-31 15:14:07.302772: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200
2021-08-31 15:14:07.895167: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2021-08-31 15:14:08.600313: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200
2021-08-31 15:14:09.692392: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200
2021-08-31 15:14:10.554939: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200
2021-08-31 15:14:11.510503: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200
2021-08-31 15:14:12.416170: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
hai-a100-3:3778851:3779455 [6] NCCL INFO Bootstrap : Using enp226s0:10.16.2.21<0>
hai-a100-3:3778851:3779455 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
hai-a100-3:3778851:3779455 [6] NCCL INFO P2P plugin IBext
hai-a100-3:3778851:3779455 [6] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_6:1/IB/SHARP [3]mlx5_8:1/IB/SHARP [4]mlx5_4:1/RoCE [5]mlx5_10:1/RoCE ; OOB enp226s0:10.16.2.21<0>
hai-a100-3:3778851:3779455 [6] NCCL INFO Using network IBext
NCCL version 2.8.3+cudaCUDA_MAJOR.CUDA_MINOR

hai-a100-3:3778851:3779883 [4] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed
hai-a100-3:3778851:3779883 [4] NCCL INFO ib_plugin.c:196 -> 2
hai-a100-3:3778851:3779883 [4] NCCL INFO ib_plugin.c:273 -> 2
hai-a100-3:3778851:3779883 [4] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:3778851:3779883 [4] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:3778851:3779883 [4] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:3778851:3779883 [4] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:3778851:3779883 [4] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:3778851:3779883 [4] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]

hai-a100-3:3778851:3779884 [5] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed

hai-a100-3:3778851:3779885 [6] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed
hai-a100-3:3778851:3779884 [5] NCCL INFO ib_plugin.c:196 -> 2
hai-a100-3:3778851:3779884 [5] NCCL INFO ib_plugin.c:273 -> 2
hai-a100-3:3778851:3779884 [5] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:3778851:3779885 [6] NCCL INFO ib_plugin.c:196 -> 2
hai-a100-3:3778851:3779885 [6] NCCL INFO ib_plugin.c:273 -> 2
hai-a100-3:3778851:3779885 [6] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:3778851:3779885 [6] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:3778851:3779885 [6] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:3778851:3779885 [6] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:3778851:3779885 [6] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:3778851:3779884 [5] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:3778851:3779884 [5] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:3778851:3779884 [5] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:3778851:3779884 [5] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:3778851:3779885 [6] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]
hai-a100-3:3778851:3779884 [5] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]

hai-a100-3:3778851:3779880 [1] ibvwrap.c:106 NCCL WARN Call to ibv_reg_mr failed
hai-a100-3:3778851:3779880 [1] NCCL INFO ib_plugin.c:284 -> 2
hai-a100-3:3778851:3779880 [1] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2

hai-a100-3:3778851:3779886 [7] ibvwrap.c:118 NCCL WARN Call to ibv_create_cq failed
hai-a100-3:3778851:3779886 [7] NCCL INFO ib_plugin.c:174 -> 2
hai-a100-3:3778851:3779880 [1] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:3778851:3779880 [1] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:3778851:3779880 [1] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:3778851:3779880 [1] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2

hai-a100-3:3778851:3779879 [0] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed
hai-a100-3:3778851:3779880 [1] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]

hai-a100-3:3778851:3779881 [2] ibvwrap.c:106 NCCL WARN Call to ibv_reg_mr failed
hai-a100-3:3778851:3779879 [0] NCCL INFO ib_plugin.c:196 -> 2
hai-a100-3:3778851:3779879 [0] NCCL INFO ib_plugin.c:273 -> 2
hai-a100-3:3778851:3779879 [0] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:3778851:3779879 [0] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:3778851:3779879 [0] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:3778851:3779879 [0] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:3778851:3779886 [7] NCCL INFO ib_plugin.c:322 -> 2
hai-a100-3:3778851:3779886 [7] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:22 -> 2
hai-a100-3:3778851:3779886 [7] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:52 -> 2
hai-a100-3:3778851:3779886 [7] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:3778851:3779886 [7] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:3778851:3779886 [7] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:3778851:3779886 [7] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]
hai-a100-3:3778851:3779881 [2] NCCL INFO ib_plugin.c:284 -> 2
hai-a100-3:3778851:3779879 [0] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:3778851:3779881 [2] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:3778851:3779881 [2] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:3778851:3779881 [2] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:3778851:3779881 [2] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:3778851:3779881 [2] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:3778851:3779879 [0] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]
hai-a100-3:3778851:3779881 [2] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]

hai-a100-3:3778851:3779882 [3] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed
hai-a100-3:3778851:3779882 [3] NCCL INFO ib_plugin.c:196 -> 2
hai-a100-3:3778851:3779882 [3] NCCL INFO ib_plugin.c:273 -> 2
hai-a100-3:3778851:3779882 [3] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:3778851:3779882 [3] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:3778851:3779882 [3] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:3778851:3779882 [3] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:3778851:3779882 [3] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:3778851:3779882 [3] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]
2021-08-31 15:14:17.616013: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-08-31 15:14:17.616262: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-08-31 15:14:17.616307: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-08-31 15:14:17.616347: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-08-31 15:14:17.616383: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-08-31 15:14:17.616413: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-08-31 15:14:17.616444: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-08-31 15:14:17.616475: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
Traceback (most recent call last):
  File ""basic_distributed_minst_v4.py"", line 102, in <module>
    history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py"", line 1183, in fit
    tmp_logs = self.train_function(iterator)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py"", line 889, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py"", line 950, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 3023, in __call__
    return graph_function._call_flat(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1960, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 591, in call
    outputs = execute.execute(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: 8 root error(s) found.
  (0) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]
  (1) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]
         [[Adam/Adam/group_deps/_291]]
  (2) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]
         [[Adam/Adam/group_deps/_295]]
  (3) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]
         [[Adam/Adam/group_deps/_299]]
  (4) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]
         [[Adam/Adam/group_deps/_303]]
  (5) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]
         [[Adam/Adam/group_deps/_307]]
  (6) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]
         [[Adam/Adam/group_deps/_311]]
  (7) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]
         [[Adam/Adam/group_deps/_315]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_9445]
```
Error when using 21.07 container and tf-nightly
```
 Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/10
2021-09-01 09:42:11.001143: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2021-09-01 09:42:12.030493: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2021-09-01 09:42:13.145586: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2021-09-01 09:42:14.594286: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2021-09-01 09:42:15.853778: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2021-09-01 09:42:17.049535: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2021-09-01 09:42:17.237328: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2021-09-01 09:42:18.567093: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
2021-09-01 09:42:19.610511: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202
hai-a100-3:376586:376986 [6] NCCL INFO Bootstrap : Using enp226s0:10.16.2.21<0>
hai-a100-3:376586:376986 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
hai-a100-3:376586:376986 [6] NCCL INFO P2P plugin IBext
hai-a100-3:376586:376986 [6] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_6:1/IB/SHARP [3]mlx5_8:1/IB/SHARP [4]mlx5_4:1/RoCE [5]mlx5_10:1/RoCE ; OOB enp226s0:10.16.2.21<0>
hai-a100-3:376586:376986 [6] NCCL INFO Using network IBext
NCCL version 2.8.3+cudaCUDA_MAJOR.CUDA_MINOR

hai-a100-3:376586:377446 [7] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed
hai-a100-3:376586:377446 [7] NCCL INFO ib_plugin.c:196 -> 2
hai-a100-3:376586:377446 [7] NCCL INFO ib_plugin.c:273 -> 2
hai-a100-3:376586:377446 [7] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:376586:377446 [7] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:376586:377446 [7] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:376586:377446 [7] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:376586:377446 [7] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2

hai-a100-3:376586:377440 [1] ibvwrap.c:106 NCCL WARN Call to ibv_reg_mr failed
hai-a100-3:376586:377440 [1] NCCL INFO ib_plugin.c:284 -> 2
hai-a100-3:376586:377440 [1] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:376586:377440 [1] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:376586:377440 [1] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:376586:377440 [1] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:376586:377440 [1] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2

hai-a100-3:376586:377444 [5] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed
hai-a100-3:376586:377444 [5] NCCL INFO ib_plugin.c:196 -> 2
hai-a100-3:376586:377444 [5] NCCL INFO ib_plugin.c:273 -> 2
hai-a100-3:376586:377444 [5] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:376586:377444 [5] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:376586:377444 [5] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:376586:377444 [5] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:376586:377444 [5] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2

hai-a100-3:376586:377439 [0] ibvwrap.c:106 NCCL WARN Call to ibv_reg_mr failed
hai-a100-3:376586:377439 [0] NCCL INFO ib_plugin.c:284 -> 2
hai-a100-3:376586:377439 [0] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:376586:377439 [0] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:376586:377439 [0] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:376586:377439 [0] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:376586:377440 [1] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]
hai-a100-3:376586:377446 [7] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]
hai-a100-3:376586:377444 [5] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]

hai-a100-3:376586:377445 [6] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed
hai-a100-3:376586:377445 [6] NCCL INFO ib_plugin.c:196 -> 2
hai-a100-3:376586:377445 [6] NCCL INFO ib_plugin.c:273 -> 2
hai-a100-3:376586:377445 [6] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:376586:377445 [6] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:376586:377445 [6] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:376586:377445 [6] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:376586:377445 [6] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:376586:377439 [0] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:376586:377445 [6] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]
hai-a100-3:376586:377439 [0] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]

hai-a100-3:376586:377443 [4] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed

hai-a100-3:376586:377442 [3] ibvwrap.c:118 NCCL WARN Call to ibv_create_cq failed
hai-a100-3:376586:377442 [3] NCCL INFO ib_plugin.c:174 -> 2
hai-a100-3:376586:377442 [3] NCCL INFO ib_plugin.c:322 -> 2
hai-a100-3:376586:377442 [3] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:22 -> 2
hai-a100-3:376586:377442 [3] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:52 -> 2
hai-a100-3:376586:377442 [3] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:376586:377442 [3] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:376586:377442 [3] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:376586:377443 [4] NCCL INFO ib_plugin.c:196 -> 2
hai-a100-3:376586:377443 [4] NCCL INFO ib_plugin.c:273 -> 2
hai-a100-3:376586:377443 [4] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:376586:377443 [4] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:376586:377443 [4] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:376586:377443 [4] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:376586:377443 [4] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:376586:377442 [3] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]
hai-a100-3:376586:377443 [4] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]

hai-a100-3:376586:377441 [2] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed
hai-a100-3:376586:377441 [2] NCCL INFO ib_plugin.c:196 -> 2
hai-a100-3:376586:377441 [2] NCCL INFO ib_plugin.c:273 -> 2
hai-a100-3:376586:377441 [2] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2
hai-a100-3:376586:377441 [2] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2
hai-a100-3:376586:377441 [2] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2
hai-a100-3:376586:377441 [2] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2
hai-a100-3:376586:377441 [2] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2
hai-a100-3:376586:377441 [2] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]
2021-09-01 09:42:23.237419: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-09-01 09:42:23.237581: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-09-01 09:42:23.237606: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-09-01 09:42:23.237628: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-09-01 09:42:23.237649: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-09-01 09:42:23.237670: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-09-01 09:42:23.237689: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
2021-09-01 09:42:23.237711: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
Traceback (most recent call last):
  File ""basic_distributed_minst_v5.py"", line 97, in <module>
    history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: 8 root error(s) found.
  (0) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce
 (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)
]]
  (1) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce
 (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)
]]
         [[Adam/Adam/group_deps/_307]]
  (2) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce
 (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)
]]
         [[Adam/Adam/group_deps/_311]]
  (3) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce
 (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)
]]
         [[Adam/Adam/group_deps/_315]]
  (4) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce
 (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)
]]
         [[Adam/Adam/group_deps/_291]]
  (5) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce
 (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)
]]
         [[Adam/Adam/group_deps/_295]]
  (6) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce
 (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)
]]
         [[Adam/Adam/group_deps/_299]]
  (7) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
         [[node Adam/NcclAllReduce
 (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)
]]
         [[Adam/Adam/group_deps/_303]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_9449]

Errors may have originated from an input operation.
Input Source operations connected to node Adam/NcclAllReduce:
In[0] Adam/split:

Operation defined at: (most recent call last)
>>>   File ""basic_distributed_minst_v5.py"", line 97, in <module>
>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1185, in fit
>>>     tmp_logs = self.train_function(iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 851, in train_function
>>>     return step_function(self, iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 840, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py"", line 151, in _all_reduce_sum_fn
>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,
>>>

Input Source operations connected to node Adam/NcclAllReduce:
In[0] Adam/split:

Operation defined at: (most recent call last)
>>>   File ""basic_distributed_minst_v5.py"", line 97, in <module>
>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1185, in fit
>>>     tmp_logs = self.train_function(iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 851, in train_function
>>>     return step_function(self, iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 840, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py"", line 151, in _all_reduce_sum_fn
>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,
>>>

Input Source operations connected to node Adam/NcclAllReduce:
In[0] Adam/split:

Operation defined at: (most recent call last)
>>>   File ""basic_distributed_minst_v5.py"", line 97, in <module>
>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1185, in fit
>>>     tmp_logs = self.train_function(iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 851, in train_function
>>>     return step_function(self, iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 840, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py"", line 151, in _all_reduce_sum_fn
>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,
>>>

Input Source operations connected to node Adam/NcclAllReduce:
In[0] Adam/split:

Operation defined at: (most recent call last)
>>>   File ""basic_distributed_minst_v5.py"", line 97, in <module>
>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1185, in fit
>>>     tmp_logs = self.train_function(iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 851, in train_function
>>>     return step_function(self, iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 840, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py"", line 151, in _all_reduce_sum_fn
>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,
>>>

Input Source operations connected to node Adam/NcclAllReduce:
In[0] Adam/split:

Operation defined at: (most recent call last)
>>>   File ""basic_distributed_minst_v5.py"", line 97, in <module>
>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1185, in fit
>>>     tmp_logs = self.train_function(iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 851, in train_function
>>>     return step_function(self, iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 840, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py"", line 151, in _all_reduce_sum_fn
>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,
>>>

Input Source operations connected to node Adam/NcclAllReduce:
In[0] Adam/split:

Operation defined at: (most recent call last)
>>>   File ""basic_distributed_minst_v5.py"", line 97, in <module>
>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1185, in fit
>>>     tmp_logs = self.train_function(iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 851, in train_function
>>>     return step_function(self, iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 840, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py"", line 151, in _all_reduce_sum_fn
>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,
>>>

Input Source operations connected to node Adam/NcclAllReduce:
In[0] Adam/split:

Operation defined at: (most recent call last)
>>>   File ""basic_distributed_minst_v5.py"", line 97, in <module>
>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1185, in fit
>>>     tmp_logs = self.train_function(iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 851, in train_function
>>>     return step_function(self, iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 840, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py"", line 151, in _all_reduce_sum_fn
>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,
>>>

Input Source operations connected to node Adam/NcclAllReduce:
In[0] Adam/split:

Operation defined at: (most recent call last)
>>>   File ""basic_distributed_minst_v5.py"", line 97, in <module>
>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1185, in fit
>>>     tmp_logs = self.train_function(iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 851, in train_function
>>>     return step_function(self, iterator)
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 840, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>>
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py"", line 151, in _all_reduce_sum_fn
>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,
>>>

Function call stack:
train_function -> train_function -> train_function -> train_function -> train_function -> train_function -> train_function -> train_function

```

Function call stack:
train_function -> train_function -> train_function -> train_function -> train_function -> train_function -> train_function -> train_function
"
51757,mips toolchain compile tflite2.6 error,"I extract tflite 2.6 source code, and make it with different toolchains. Arm and linux toolchains works fine. But mips-mti-elf-gcc/g++ appears the following error:

I confirm that my mips toolchain has supported c++11. Because mips toolchain not define _GLIBCXX11_USE_C99_STDIO,  it does not support to_string function. 

I don't know why mips not support _GLIBCXX11_USE_C99_STDIO, and if tf can help solve this problem?


tflite/tf_latest/tensorflow/lite/kernels/kernel_util.cc: In function ‘std::__cxx11::string tflite::GetShapeDebugString(const TfLiteIntArray*)’:
tflite/tf_latest/tensorflow/lite/kernels/kernel_util.cc:423:24: error: ‘to_string’ is not a member of ‘std’
       str = ""["" + std::to_string(shape->data[d]);
                        ^~~~~~~~~
tflite/tf_latest/tensorflow/lite/kernels/kernel_util.cc:423:24: note: suggested alternative: ‘__sso_string’
       str = ""["" + std::to_string(shape->data[d]);
                        ^~~~~~~~~
                        __sso_string
tflite/tf_latest/tensorflow/lite/kernels/kernel_util.cc:425:26: error: ‘to_string’ is not a member of ‘std’
       str += "", "" + std::to_string(shape->data[d]);
                          ^~~~~~~~~
tflite/tf_latest/tensorflow/lite/kernels/kernel_util.cc:425:26: note: suggested alternative: ‘__sso_string’
       str += "", "" + std::to_string(shape->data[d]);
                          ^~~~~~~~~
                          __sso_string
<builtin>: recipe for target 'tflite/tf_latest/tensorflow/lite/kernels/kernel_util.o' failed
make: *** [tflite/tf_latest/tensorflow/lite/kernels/kernel_util.o] Error 1"
51756,[tflite conversion] Invalid file dropped for invalid shape without error,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip package version 2.6.0
- TensorFlow library (version, if pip package or github SHA, if built from source): N/A

### 2. Code
_Provide code to help us reproduce your issues using one of the following options:_

Download model:
```
wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz

tar zxvf inception_v3_2018_04_27.tgz
```


```py
import tensorflow as tf

input_path = ""./inception_v3.pb""
output_path = ""./inception_v3.tflite""

input_shapes = {""input"": [0, 299, 299, 3]}
input_nodes = [""input""]
output_nodes = [""InceptionV3/Predictions/Reshape_1""]

converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
            input_path, input_nodes, output_nodes, input_shapes)
converter.allow_custom_ops = True
tflite_model = converter.convert()
open(output_path, ""wb"").write(tflite_model)
```
I suppose `input_shapes = {""input"": [0, 299, 299, 3]}` is invalid where batch is 0.
And expect there be some error reporting.

### 3. Failure after conversion
_If the conversion is successful, but the generated model is wrong, then state what is wrong:_

- tflite model has only input and output nodes without any valid network

### 5. (optional) Any other info / logs
_Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached._

Invalid input shape is a NEGATIVE test to check TensorFlow is working OK.
This time it didn't.

TF2.3.0 showed this error. But TF 2.6.0 ended without error but dropped invalid tflite file.

> loc(""InceptionV3/InceptionV3/Mixed_7c/concat""): error: 'tfl.concatenation' op failed to verify that values and output must have same element type
Traceback (most recent call last):
File "".../tensorflow/lite/python/convert.py"", line 199, in toco_convert_protos
enable_mlir_converter)
File "".../tensorflow/lite/python/wrap_toco.py"", line 38, in wrapped_toco_convert
enable_mlir_converter)
Exception: :0: error: loc(""InceptionV3/InceptionV3/Mixed_7c/concat""): 'tfl.concatenation' op failed to verify that values and output must have same element type
:0: note: loc(""InceptionV3/InceptionV3/Mixed_7c/concat""): see current operation: %0 = ""tfl.concatenation""() {axis = 3 : i32, fused_activation_function = ""NONE""} : () -> tensor<0x8x8x2048xf32>

"
51755,Import tensor error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version: 2.x
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Mircrosoft Hyper-V video 


**Describe the problem**
I used pip install tensorflow for installation.  Run import tensorflow as tf have the following error.  Attached below the trace log.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

ImportError                               Traceback (most recent call last)
~\anaconda3\envs\py37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     63   try:
---> 64     from tensorflow.python._pywrap_tensorflow_internal import *
     65   # This try catch logic is because there is no bazel equivalent for py_extension.

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>
----> 1 import tensorflow as tf

~\anaconda3\envs\py37\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~\anaconda3\envs\py37\lib\site-packages\tensorflow\python\__init__.py in <module>
     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     39 
---> 40 from tensorflow.python.eager import context
     41 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     42 

~\anaconda3\envs\py37\lib\site-packages\tensorflow\python\eager\context.py in <module>
     33 from tensorflow.core.protobuf import config_pb2
     34 from tensorflow.core.protobuf import rewriter_config_pb2
---> 35 from tensorflow.python import pywrap_tfe
     36 from tensorflow.python import tf2
     37 from tensorflow.python.client import pywrap_tf_session

~\anaconda3\envs\py37\lib\site-packages\tensorflow\python\pywrap_tfe.py in <module>
     26 
     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import
---> 28 from tensorflow.python import pywrap_tensorflow
     29 from tensorflow.python._pywrap_tfe import *

~\anaconda3\envs\py37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     81 for some common reasons and solutions.  Include the entire stack trace
     82 above this error message when asking for help."""""" % traceback.format_exc()
---> 83   raise ImportError(msg)
     84 
     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\jimmy\anaconda3\envs\py37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
51751,Tensorflow Lite C API + GPU delegate crash on exit,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master/nightly
- Python version: N/A
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): gcc 8.4.0
- CUDA/cuDNN version: cuda 11.1
- GPU model and memory: 4GB VRAM, 8GM system RAM

**Describe the current behavior**
Using the C API of tensorflow lite, with GPU delegate (GPU = NVidia GTX 1650), I get a crash on exit. This only happens when I build the latest mater or nightly branch. **The v2.6.0 tag does not have this problem.** 
Here is my code to reproduce the issue:
```
#include ""tensorflow/lite/c/c_api.h""
#include ""tensorflow/lite/delegates/gpu/delegate.h""

int main(void)
{
    TfLiteGpuDelegateOptionsV2 opts = TfLiteGpuDelegateOptionsV2Default();

    TfLiteDelegate* gpuDelegate = TfLiteGpuDelegateV2Create(&opts);
    TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();

    TfLiteInterpreterOptionsAddDelegate(options, gpuDelegate);

    //This problem should be be reproducible using any convolutional model
    TfLiteModel* model = TfLiteModelCreateFromFile(""../mymodel.tflite"");
    TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);

    TfLiteInterpreterAllocateTensors(interpreter);

    TfLiteInterpreterInvoke(interpreter);

    TfLiteInterpreterDelete(interpreter);
    TfLiteGpuDelegateV2Delete(gpuDelegate);
    TfLiteInterpreterOptionsDelete(options);
    TfLiteModelDelete(model);
}
```

The stacktrace I get from gdb is:
```
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.

Thread 1 ""dlib_test"" received signal SIGSEGV, Segmentation fault.
0x00007fffed25d1a0 in ?? ()
   from /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1
(gdb) bt
#0  0x00007fffed25d1a0 in ?? ()
   from /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1
#1  0x00007fffed15655c in ?? ()
   from /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1
#2  0x00007fffed153b08 in ?? ()
   from /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1
#3  0x00007fffed155c22 in ?? ()
   from /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1
#4  0x00007ffff7531a06 in tflite::gpu::cl::Buffer::Release() ()
   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so
#5  0x00007ffff752fadf in tflite::gpu::cl::InferenceContext::~InferenceContext() ()
   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so
#6  0x00007ffff7529369 in tflite::gpu::cl::(anonymous namespace)::InferenceRunnerImpl::~InferenceRunnerImpl() ()
   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so
#7  0x00007ffff75222e2 in tflite::gpu::(anonymous namespace)::DelegatePrepare(TfLiteContext*, TfLiteDelegate*)::{lambda(TfLiteContext*, void*)#2}::_FUN(TfLiteContext*, void*) ()
   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlit---Type <return> to continue, or q <return> to quit---
e_c.so
#8  0x00007ffff751c654 in tflite::Subgraph::CleanupNode(int) ()
   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so
#9  0x00007ffff751c6bf in tflite::Subgraph::~Subgraph() ()
   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so
#10 0x00007ffff751c879 in tflite::Subgraph::~Subgraph() ()
   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so
#11 0x00007ffff7a00697 in tflite::Interpreter::~Interpreter() ()
   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so
#12 0x00007ffff7516ef1 in TfLiteInterpreterDelete ()
   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so
#13 0x0000555555554bf8 in main ()
```
When I run my program using valgrind, I get:
```
==19931== Conditional jump or move depends on uninitialised value(s)
==19931==    at 0x4FCAA64: tflite::gpu::ConvPowerVR::GenerateConv[abi:cxx11](tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&, bool, tflite::gpu::ConvPowerVR::ConvParams const&)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)#3}::operator()(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const [clone .isra.283] (in /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so)
```
"
51750,"[AArch64] error: inlining failed in call to 'always_inline' 'uint8x16_t vaeseq_u8(uint8x16_t, uint8x16_t)': target specific option mismatch","I did a build today and got hit by same issue as in #41409 one. I use manylinux2014 image for builds.

top commit:

```
commit 4e8bba06da6c3212c1d4e19ef86c42d905d47c7a (grafted, HEAD -> master, origin/master, origin/HEAD)
Author: TensorFlower Gardener <gardener@tensorflow.org>
Date:   Mon Aug 30 03:37:48 2021 -0700

    Merge pull request #50961 from nouiz:upstream-cudaMallocAsync-no_stats_error
    
    PiperOrigin-RevId: 393736407
    Change-Id: I31f5ba2b885683cc587a30cdce70e0f20ffefef9
```

Error:

```
      /opt/rh/devtoolset-10/root/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-o
pt/bin/external/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.o' -fPIC -iquoteexternal/com_google_absl -iquotebazel-
out/aarch64-opt/bin/external/com_google_absl -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwr
ite-strings -DNOMINMAX -Wno-pass-failed -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_absl/absl/random/internal/randen_hwaes.cc -o bazel-out/aarch64-opt/bin/external
/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.o)                                                                                                                                                                                                     
    Execution platform: @local_execution_config_platform//:platform                                                                        
    In file included from external/com_google_absl/absl/random/internal/randen_hwaes.cc:229:
    /opt/rh/devtoolset-10/root/usr/lib/gcc/aarch64-redhat-linux/10/include/arm_neon.h: In function 'Vector128 {anonymous}::AesRound(const Vector128&, const Vector128&)':
    /opt/rh/devtoolset-10/root/usr/lib/gcc/aarch64-redhat-linux/10/include/arm_neon.h:12332:1: error: inlining failed in call to 'always_inline' 'uint8x16_t vaesmcq_u8(uint8x16_t)': target specific option mismatch
    12332 | vaesmcq_u8 (uint8x16_t data)                                                                                                                                                                                                                                              
          | ^~~~~~~~~~                                                                                                                                                                                                                                                                
    external/com_google_absl/absl/random/internal/randen_hwaes.cc:255:20: note: called from here                                                                                                                                                                                      
      255 |   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;                                                                                                                                                                                                          
          |          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                                                                       
    In file included from external/com_google_absl/absl/random/internal/randen_hwaes.cc:229:                                                                                                                                                                                          
    /opt/rh/devtoolset-10/root/usr/lib/gcc/aarch64-redhat-linux/10/include/arm_neon.h:12318:1: error: inlining failed in call to 'always_inline' 'uint8x16_t vaeseq_u8(uint8x16_t, uint8x16_t)': target specific option mismatch                                                     
    12318 | vaeseq_u8 (uint8x16_t data, uint8x16_t key)                                                                                                                                                                                                                               
          | ^~~~~~~~~                                                                                                                                                                                                                                                                 
    external/com_google_absl/absl/random/internal/randen_hwaes.cc:255:20: note: called from here                                                                                                                                                                                      
      255 |   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;                                                                                                                                                                                                          
          |          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                                                                       
    At global scope:                                                                                                                                                                                                                                                                  
    cc1plus: note: unrecognized command-line option '-Wno-pass-failed' may have been intended to silence earlier diagnostics                                                                                                                                                          
    Target //tensorflow/tools/pip_package:build_pip_package failed to build                                                                                                                                                                                                           
    INFO: Elapsed time: 5671.673s, Critical Path: 500.29s                                                                                                                                                                                                                             
    INFO: 9206 processes: 996 internal, 8210 local.                                                                                                                                                                                                                                   

```

_Originally posted by @hrw in https://github.com/tensorflow/tensorflow/issues/41409#issuecomment-908364569_"
51746,Error loading saved model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4
- GPU model and memory: NVIDIA GeForce RTX 2060, 6144 MB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
After saving a Quant Aware model to saved_model using model.save(), I get this error while loading it with tf.keras.models.load_model() - KeyError: '__inference_depthwise_conv2d_layer_call_and_return_conditional_losses_8913'. This happens only when the model has SeparableConv2D layer.

**Describe the expected behavior**
The model should load without any error. It does if the model has only Conv2D layers, but doesn't if it has SeparableConv2D layers

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
https://colab.research.google.com/drive/1_afuGL5IUrquuXEhAJkOExN5HXJUbtOt?usp=sharing 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
KeyError: '__inference_depthwise_conv2d_layer_call_and_return_conditional_losses_8913'
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-23-39c24960e684> in <module>
      1 with tfmot.quantization.keras.quantize_scope():
----> 2     q_model = tf.keras.models.load_model(saved_model_dir.format(epoch=epoch))
      3 
      4 PATH = '/media/dash/027A71F47A71E537/Dash_ext/sonyliv/exp/vids/input/KBC_1_1080p_640x360_664_kbps/input_2.png'
      5 im = cv2.imread(PATH)

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)
    210       if isinstance(filepath, six.string_types):
    211         loader_impl.parse_saved_model(filepath)
--> 212         return saved_model_load.load(filepath, compile, options)
    213 
    214   raise IOError(

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in load(path, compile, options)
    142   for node_id, loaded_node in keras_loader.loaded_nodes.items():
    143     nodes_to_load[keras_loader.get_path(node_id)] = loaded_node
--> 144   loaded = tf_load.load_partial(path, nodes_to_load, options=options)
    145 
    146   # Finalize the loaded layers and remove the extra tracked dependencies.

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load_partial(export_dir, filters, tags, options)
    763     A dictionary mapping node paths from the filter to loaded objects.
    764   """"""
--> 765   return load_internal(export_dir, tags, options, filters=filters)
    766 
    767 

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, options, loader_cls, filters)
    888       try:
    889         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,
--> 890                             ckpt_options, filters)
    891       except errors.NotFoundError as err:
    892         raise FileNotFoundError(

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in __init__(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, filters)
    158       self._concrete_functions[name] = _WrapperFunction(concrete_function)
    159 
--> 160     self._load_all()
    161     self._restore_checkpoint()
    162 

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _load_all(self)
    254   def _load_all(self):
    255     """"""Loads all nodes and functions from the SavedModel and their edges.""""""
--> 256     self._load_nodes()
    257     self._load_edges()
    258     # TODO(b/124045874): There are limitations with functions whose captures

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _load_nodes(self)
    432         # interface.
    433         continue
--> 434       node, setter = self._recreate(proto, node_id)
    435       nodes[node_id] = node
    436       node_setters[node_id] = setter

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _recreate(self, proto, node_id)
    550     if kind not in factory:
    551       raise ValueError(""Unknown SavedObject type: %r"" % kind)
--> 552     return factory[kind]()
    553 
    554   def _recreate_user_object(self, proto, node_id):

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in <lambda>()
    539             lambda: self._recreate_user_object(proto.user_object, node_id)),
    540         ""asset"": lambda: self._recreate_asset(proto.asset),
--> 541         ""function"": lambda: self._recreate_function(proto.function),
    542         ""bare_concrete_function"": functools.partial(
    543             self._recreate_bare_concrete_function,

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _recreate_function(self, proto)
    578   def _recreate_function(self, proto):
    579     return function_deserialization.recreate_function(
--> 580         proto, self._concrete_functions), setattr
    581 
    582   def _recreate_bare_concrete_function(self, proto):

~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py in recreate_function(saved_function, concrete_functions)
    275   concrete_function_objects = []
    276   for concrete_function_name in saved_function.concrete_functions:
--> 277     concrete_function_objects.append(concrete_functions[concrete_function_name])
    278 
    279   for cf in concrete_function_objects:

KeyError: '__inference_depthwise_conv2d_layer_call_and_return_conditional_losses_8913'
"
51745,NameError:,"![4](https://user-images.githubusercontent.com/26819449/131285254-1698b895-d3d7-4c10-9251-8c169d7d5dd8.JPG)

![5](https://user-images.githubusercontent.com/26819449/131285259-61b09421-68ae-490e-9a1d-30aeefa364a5.JPG)

TensorFlow version : 2.2
Keras version: 2.4.3
Even after defining it. It's telling to define it.
Please assign someone who can reply fast.
Thank You.
"
51744,NameError:,"![4](https://user-images.githubusercontent.com/26819449/131285254-1698b895-d3d7-4c10-9251-8c169d7d5dd8.JPG)

![5](https://user-images.githubusercontent.com/26819449/131285259-61b09421-68ae-490e-9a1d-30aeefa364a5.JPG)

TensorFlow version : 2.2
 Keras version : 2.4.3
Even after defining it. It's telling to define it.

"
51743,typing-extensions package is conflicted,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `macOS 11.5.2`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version: `2.6.0`
- Python version: `3.8.10`
- Installed using virtualenv? pip? conda?: `virtualenv` and `pip`
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -


**Describe the problem**
I installed `tensorflow` using `pip` on the virtualenv. The `typing-extensions` package is conflicted with `black` package dependency. `black==21.8b0` is required `typing-extensions>=3.10.0.0`. But, `tensorflow` has `'typing_extensions ~= 3.7.4` dependency. Are there any plans to upgrade the `typing-extensions` dependency package version?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```bash
$ python3 -m venv venv
$ . venv/bin/activate
$ pip install -U pip
$ pip install black
$ pip install tensorflow
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```bash
$ pip install tensorflow
Requirement already satisfied: tensorflow in ./venv/lib/python3.8/site-packages (2.6.0)
Requirement already satisfied: keras-preprocessing~=1.1.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.1.2)
Requirement already satisfied: astunparse~=1.6.3 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: opt-einsum~=3.3.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.3.0)
Collecting typing-extensions~=3.7.4
  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)
Requirement already satisfied: numpy~=1.19.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.19.5)
Requirement already satisfied: wrapt~=1.12.1 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.12.1)
Requirement already satisfied: tensorflow-estimator~=2.6 in ./venv/lib/python3.8/site-packages (from tensorflow) (2.6.0)
Requirement already satisfied: six~=1.15.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.15.0)
Requirement already satisfied: wheel~=0.35 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.37.0)
Requirement already satisfied: google-pasta~=0.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: flatbuffers~=1.12.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.12)
Requirement already satisfied: tensorboard~=2.6 in ./venv/lib/python3.8/site-packages (from tensorflow) (2.6.0)
Requirement already satisfied: keras~=2.6 in ./venv/lib/python3.8/site-packages (from tensorflow) (2.6.0)
Requirement already satisfied: protobuf>=3.9.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.17.3)
Requirement already satisfied: gast==0.4.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.4.0)
Requirement already satisfied: termcolor~=1.1.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.1.0)
Requirement already satisfied: grpcio<2.0,>=1.37.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.39.0)
Requirement already satisfied: clang~=5.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (5.0)
Requirement already satisfied: absl-py~=0.10 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.13.0)
Requirement already satisfied: h5py~=3.1.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)
Requirement already satisfied: werkzeug>=0.11.15 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)
Requirement already satisfied: google-auth<2,>=1.6.3 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)
Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)
Requirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (56.0.0)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.5)
Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./venv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)
Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)
Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)
Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venv/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.5)
Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)
Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)
Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)
Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)
Installing collected packages: typing-extensions
  Attempting uninstall: typing-extensions
    Found existing installation: typing-extensions 3.10.0.1
    Uninstalling typing-extensions-3.10.0.1:
      Successfully uninstalled typing-extensions-3.10.0.1
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
black 21.8b0 requires typing-extensions>=3.10.0.0, but you have typing-extensions 3.7.4.3 which is incompatible.
Successfully installed typing-extensions-3.7.4.3
```
"
51742,"TransformGraphWithStringInputs is available in tf2.0 however I am planning to jump to tf2.2 or tf2.3 but I couldn't find ""TransformGraphWithStringInputs"" in these 2 versions. Any suggestions ?","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
51741,CAST: Not supported Cast case.,"Version: TFLite 2.6.0
Device: Huawei Mate30


08-30 11:00:38.255 28325 28842 I tflite  : Initialized TensorFlow Lite runtime.
08-30 11:00:38.257 28325 28842 E tflite  : Following operations are not supported by GPU delegate:
08-30 11:00:38.257 28325 28842 E tflite  : CAST: Not supported Cast case.
08-30 11:00:38.257 28325 28842 E tflite  : DEQUANTIZE: 
08-30 11:00:38.257 28325 28842 E tflite  : 6 operations will run on the GPU, and the remaining 8 operations will run on the CPU."
51740,Reshape output tensor in Tensorflow lite.,"

**System information**
- TensorFlow version (you are using): master branch
- Are you willing to contribute it (Yes/No): YEs



**Describe the feature and the current behavior/state.**

I'm currently working on this [pull request](https://github.com/tensorflow/tflite-support/pull/612). I want to resize both the input and output tensor of an interpreter. The current API supports only `ResizeInputTensor()` for some reason. Any leads?

**Will this change the current api? How?**
   This is an internal API, there won't be any visible change.

**Who will benefit with this feature?**
   
  Those working with core TFLite C++ API.

**Any Other info.**
"
51738,Importing layers from keras instead of tensorflow.keras causes erroneous model.summary() behavior,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 21H1
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.9.6

**Describe the current behavior**

```
from tensorflow import keras 
from keras import layers
model = keras.Sequential()
model.add(layers.Dense(1,input_shape=(10, 1)))
model.summary()
```

This code results in a ""model has not yet been built"" error, even though `input_shape` is specified in the first layer. I noticed that changing the `from keras import layers` to `from tensorflow.keras import layers` solves the problem. But this seems like a trivial change so I don't see why it solves the problem, the error shouldn't be present even if the import is directly from `keras`"
51737,Failed to build minimal example of Tensorflow Lite on Qualcomm robotic kit rb5,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Device: Qualcomm Robotic kit rb5 with 16Gb RAM (I connect the board directly to a monitor using HDMI) 
- TensorFlow installed from (source or binary): 2.6 build from source using Cmake (Also I tried 2.5)
- TensorFlow version: - 
- Python version:  2.7
- Installed using virtualenv? pip? conda?: -
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): Cmake 3.20.5
- CUDA/cuDNN version: - 
- GPU model and memory: GPU: Qualcomm Adreno 650         DSP: Qualcomm Hexagon 698

Dear All,
I’v followed the instruction in this [link](https://www.tensorflow.org/lite/guide/build_cmake) and successfully build Tensorflow Lite using Cmake. I tried to build the minimal example in this [link](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal), but I get a black screen after the build process is 98% completed. Then the black screen remains permanently. 
Anyone has an idea why this happens?

My ultimate goal is to link Tensorflow lite as a static library to a ROS(melodic) package and run an inference for Tensorflow Lite models using Hexagon delegate on the Qualcomm rb5 DSP in order to do an object detection task. Anyone has an idea what is the best way to do it?

Thanks
Hamoun
"
51736,Image transformations require SciPy. Install SciPy.,"I am running tensorflow with the help of miniforge3 on MacBook Pro M1.

I have followed the steps to install tensorflow on M1 from here: https://github.com/jeffheaton/t81_558_deep_learning/blob/master/install/tensorflow-install-mac-metal-jul-2021.ipynb

- TensorFlow version (use command below): tensorflow-mac 2.5.0
- Python version: 3.9
- Bazel version (if compiling from source):
- GPU model and memory: M1 8gb

ImportError                               Traceback (most recent call last)
/var/folders/k9/t0sd91511s16j207y0y2wps40000gn/T/ipykernel_91808/2572958566.py in <module>
      9 import scipy
     10 
---> 11 history = model.fit(
     12     train_generator,
     13     steps_per_epoch = 15,

/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1131          training_utils.RespectCompiledTrainableState(self):
   1132       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.
-> 1133       data_handler = data_adapter.get_data_handler(
   1134           x=x,
   1135           y=y,

/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py in get_data_handler(*args, **kwargs)
   1362   if getattr(kwargs[""model""], ""_cluster_coordinator"", None):
   1363     return _ClusterCoordinatorDataHandler(*args, **kwargs)
-> 1364   return DataHandler(*args, **kwargs)
   1365 
   1366 

/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1152     adapter_cls = select_data_adapter(x, y)
   1153     self._verify_data_adapter_compatibility(adapter_cls)
-> 1154     self._adapter = adapter_cls(
   1155         x,
   1156         y,

/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    930     self._keras_sequence = x
    931     self._enqueuer = None
--> 932     super(KerasSequenceAdapter, self).__init__(
    933         x,
    934         shuffle=False,  # Shuffle is handed in the _make_callable override.

/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    807     # Since we have to know the dtype of the python generator when we build the
    808     # dataset, we have to look at a batch to infer the structure.
--> 809     peek, x = self._peek_and_restore(x)
    810     peek = self._standardize_batch(peek)
    811     peek = _process_tensorlike(peek)

/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py in _peek_and_restore(x)
    941   @staticmethod
    942   def _peek_and_restore(x):
--> 943     return x[0], x
    944 
    945   def _handle_multiprocessing(self, x, workers, use_multiprocessing,

/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/keras_preprocessing/image/iterator.py in __getitem__(self, idx)
     63         index_array = self.index_array[self.batch_size * idx:
     64                                        self.batch_size * (idx + 1)]
---> 65         return self._get_batches_of_transformed_samples(index_array)
     66 
     67     def __len__(self):

/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/keras_preprocessing/image/iterator.py in _get_batches_of_transformed_samples(self, index_array)
    236             if self.image_data_generator:
    237                 params = self.image_data_generator.get_random_transform(x.shape)
--> 238                 x = self.image_data_generator.apply_transform(x, params)
    239                 x = self.image_data_generator.standardize(x)
    240             batch_x[i] = x

/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/keras_preprocessing/image/image_data_generator.py in apply_transform(self, x, transform_parameters)
    861         img_channel_axis = self.channel_axis - 1
    862 
--> 863         x = apply_affine_transform(x, transform_parameters.get('theta', 0),
    864                                    transform_parameters.get('tx', 0),
    865                                    transform_parameters.get('ty', 0),

/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/keras_preprocessing/image/affine_transformations.py in apply_affine_transform(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)
    279     """"""
    280     if scipy is None:
--> 281         raise ImportError('Image transformations require SciPy. '
    282                           'Install SciPy.')
    283     transform_matrix = None

ImportError: Image transformations require SciPy. Install SciPy."
51731,TF RNN subclass model called twice although sequence length is one,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2 / 8
- GPU model and memory: RTX 2070 8 GB

**Describe the current behavior**
I am building a custom RNN layer using tensorflow's model subclass documentation. My input is a 1D vector while the RNN states are also fed into the model at each timestep i.e. my input is generated at each timestep, the RNN's state is obtained from the previous timestep and I need the model output at each timestep. The issue I am facing is that when a single input is given each timestep, the RNN's call function is called twice instead of just once i.e. print('--------- Loop --------') is displayed twice instead of just once when the input is only given once. This becomes a bigger issue as runtime is doubled when I am running a bigger network for a reinforcement learning problem where weights are updated at each timestep instead of the Monte Carlo method.

**Describe the expected behavior**
The expected behaviour would be that the call function is only called once at each timestep so that print('--------- Loop --------') runs once at each timestep.

**Standalone code to reproduce the issue**

`

    class MinimalRNNCell(tf.keras.layers.Layer):
        def __init__(self, units=100, **kwargs):
            self.units = units
            self.state_size = units
            super(MinimalRNNCell, self).__init__(**kwargs)

        def build(self, input_shape):
            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                      initializer='uniform',
                                      name='kernel')
            self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units),
            initializer='uniform',
            name='recurrent_kernel')
            self.built = True

        def call(self, inputs, states):
            prev_output = states[0]
            h = tf.matmul(inputs, self.kernel)
            output = h + tf.matmul(prev_output, self.recurrent_kernel)
            print('--------- Loop --------')
            return output, [output]

    rnncell = MinimalRNNCell()
    rnn = tf.keras.layers.RNN(rnncell, return_state=True, return_sequences=False, time_major=False, stateful=False)

    x = tf.random.normal(shape=[1,67],stddev=1)
    state = tf.random.normal(shape=[1,100],stddev=1)
    for t in range(3):
        r, state = rnn(x[None,:], state)
        print('{} timestep done'.format(t+1))
`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Run log: 
--------- Loop --------
--------- Loop --------
1 timestep done
--------- Loop --------
--------- Loop --------
2 timestep done
--------- Loop --------
--------- Loop --------
3 timestep done
"
51730,Failed to get compute capability major for device: UNKNOWN ERROR (1); 0,"Dear All,

I have installed tensorflow-cpu 2.6.0 on my anaconda. Both Tensorflow and Keras will be imported correctly, but running a cell containing them raises error:

> RuntimeError Traceback (most recent call last) in 1 from tensorflow.python.client import device_lib ----> 2 print(device_lib.list_local_devices())
> D:\Programs\Anaconda\lib\site-packages\tensorflow\python\client\device_lib.py in list_local_devices(session_config) 43 serialized_config = session_config.SerializeToString() 44 return [ ---> 45 _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config) 46 ]
> 
> RuntimeError: failed to get compute capability major for device: UNKNOWN ERROR (1); 0

**System information**
- Python 3.8.5 (Anaconda)
- Tensorflow 2.6.0
- keras 2.6.0
- Windows 7 64

Thank alot
Silvio"
51729,"Unable to generate train.record, but for text.record it works fine","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): nope
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): I used the guide at https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html
- TensorFlow version (use command below): 2.6.0
- Python version: Python 3.8.8
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): nope 
- CUDA/cuDNN version: nope
- GPU model and memory: nope

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior** When following the guide command to change format from xml to record, it doesn't work for train but works for text

**Describe the expected behavior**

it should output something like: Successfully created the TFRecord file: /home/jrhin/Tensorflow/workspace/training_demo/annotations/train.record


**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

python generate_tfrecord.py -x ~/Tensorflow/workspace/training_demo/images/train -l ~/Tensorflow/workspace/training_demo/annotations/label_map.pbtxt -o ~/Tensorflow/workspace/training_demo/annotations/train.record

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""generate_tfrecord.py"", line 172, in <module>
    tf.app.run()
  File ""/home/jrhin/anaconda3/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/jrhin/anaconda3/lib/python3.8/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/home/jrhin/anaconda3/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""generate_tfrecord.py"", line 162, in main
    tf_example = create_tf_example(group, path)
  File ""generate_tfrecord.py"", line 136, in create_tf_example
    classes.append(class_text_to_int(row['class']))
  File ""generate_tfrecord.py"", line 105, in class_text_to_int
    return label_map_dict[row_label]
KeyError: 'w'
"
51728,More freedom in Dtype of indices of sparse.SparseTensors,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): v2.6.0
- Are you willing to contribute it (Yes/No):
Yes


**Describe the feature and the current behavior/state.**
sparse.SparseTensors  'indices dtype are hardcoded as tf.int64. I would like to allow other integers types (tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.int32)
see https://github.com/tensorflow/tensorflow/blob/919f693420e35d00c8d0a42100837ae3718f7927/tensorflow/python/framework/sparse_tensor.py#L130

**Will this change the current api? How?**
need to be investigated. I am not sure why this is hard-coded.
**Who will benefit with this feature?**
Smaller memory useage of SparseTensors which is needed in a lot of application with limited RAM.
**Any Other info.**
"
51727,"Unable to save custom model using save_model, even though documentation states it is possible","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.6.0
- Python version: 3.7

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Throws error

Unable to save function b'__inference_paraphrase_detector_1_layer_call_fn_2757942' because it captures graph tensor Tensor(""dense_83/BiasAdd_3:0"", shape=(16, 32), dtype=float32) from a parent function which cannot be converted to a constant with `tf.get_static_value`.

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Colab: https://colab.research.google.com/drive/11VGeylggSZ-0yb9CJmg3fE79CiRAvERX?usp=sharing


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

No need for the database, even if you simply build the model with no training and try to save it, it throws the same error. "
51726,Several links to the github code are broken in https://www.tensorflow.org/,"As the title says, some links from https://www.tensorflow.org/ to the corresponding code on github are broken.
[here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation) an example. 
I believe this is due to the removal of the `python` module.

Also, some other links have a line offset, such as [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).
This is probably because some imports were removed. "
51725,How can I increase  throughput for multi sessions/models on single GPU of online inference？,"Environment:
TF Version: 1.14.0
Cuda Version: 10.0
GPU Device: NVIDIA-T4

Dear all,
     When I got multi models (4 models for example) online inference on single GPU, It turned out that the overall throughput can be increased (despite of batching) and got very low GPU utilization. I invested on this issue and found it occurs because tensorflow use only one compute stream for a physical gpu device according to https://stackoverflow.com/questions/55907275/can-multiple-tensorflow-inferences-run-on-one-gpu-in-parallel. Do you have any solutions for this issue? 

Thanks a lot,
Asher

"
51724,GPU not found when using TF 2.5.0 in Google Colab,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7.11
- CUDA/cuDNN version: V11.0.221
- GPU model and memory: Tesla K80
<br>

**Describe the current behavior**
- I installed `tensorflow 2.5.0` in colab
- After installation, I imported `tensorflow` and checked available GPU device
- However, `tf.config.experimental.list_physical_devices('GPU')` returns an empty list.
- I already checked GPU deivce with `nvidia-smi`
<br>

<ins>Is there any problem the way I ran below code?</ins>
<br>

**Standalone code to reproduce the issue**
```
!pip install tensorflow==2.5.0
import tensorflow as tf
!nvidia-smi
print(f""tf ({tf.__version__})"")
print(f""gpus : {tf.config.experimental.list_physical_devices('GPU')}"")
```"
51723,Build stuck at Linking _pywrap_tensorflow_internal.so step,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): From source
- TensorFlow version: 2.6.0
- Python version: 3.9.2
- Installed using virtualenv? pip? conda?: Bazel
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): VS 2019

**Describe the problem**

The build gets stuck at ""Linking tensorflow/python/ _pywrap_tensorflow_internal.so"" step. It takes more than 5000s and never completes the build

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r2.6
python ./configure.py
bazel --output_user_root=C:\tfb build --jobs=3 --config=opt --define=no_tensorflow_py_deps=true /tensorflow/tools/pip_package:build_pip_package
```
In the configuration , I have chosen all the default options

**Any other info / logs**
![2021-08-27 21_22_53-Window](https://user-images.githubusercontent.com/64022998/131206786-a30fa5fc-213c-4b0c-b9b4-1fa40ccfcdb2.png)

"
51721,NameError: name 'trainer' is not defined,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2+
- Python version: 3.9.2
- Installed using virtualenv? pip? conda?: python
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4
- GPU model and memory:



**Describe the problem**

I'm trying to get python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config to build but it gives me the error ""NameError: name 'trainer' is not defined.""

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I followed this tutorial for object detection: https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10 and this is step 6 of 7, so I'm almost done.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

This is the code I'm working with, and the problem is in line 168 in main, trainer.train(.

r""""""Training executable for detection models.

This executable is used to train DetectionModels. There are two ways of
configuring the training job:

1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file
can be specified by --pipeline_config_path.

Example usage:
    ./train \
        --logtostderr \
        --train_dir=path/to/train_dir \
        --pipeline_config_path=pipeline_config.pbtxt

2) Three configuration files can be provided: a model_pb2.DetectionModel
configuration file to define what type of DetectionModel is being trained, an
input_reader_pb2.InputReader file to specify what training data will be used and
a train_pb2.TrainConfig file to configure training parameters.

Example usage:
    ./train \
        --logtostderr \
        --train_dir=path/to/train_dir \
        --model_config_path=model_config.pbtxt \
        --train_config_path=train_config.pbtxt \
        --input_config_path=train_input_config.pbtxt
""""""

import functools
import json
import os
import tensorflow.compat.v1 as tf
from tensorflow.python.util.deprecation import deprecated


from object_detection.builders import dataset_builder
from object_detection.builders import graph_rewriter_builder
from object_detection.builders import model_builder
from object_detection.utils import config_util

tf.logging.set_verbosity(tf.logging.INFO)

flags = tf.app.flags
flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')
flags.DEFINE_integer('task', 0, 'task id')
flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')
flags.DEFINE_boolean('clone_on_cpu', False,
                     'Force clones to be deployed on CPU.  Note that even if '
                     'set to False (allowing ops to run on gpu), some ops may '
                     'still be run on the CPU if they have no GPU kernel.')
flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '
                     'replicas.')
flags.DEFINE_integer('ps_tasks', 0,
                     'Number of parameter server tasks. If None, does not use '
                     'a parameter server.')
flags.DEFINE_string('train_dir', '',
                    'Directory to save the checkpoints and training summaries.')

flags.DEFINE_string('pipeline_config_path', '',
                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '
                    'file. If provided, other configs are ignored')

flags.DEFINE_string('train_config_path', '',
                    'Path to a train_pb2.TrainConfig config file.')
flags.DEFINE_string('input_config_path', '',
                    'Path to an input_reader_pb2.InputReader config file.')
flags.DEFINE_string('model_config_path', '',
                    'Path to a model_pb2.DetectionModel config file.')

FLAGS = flags.FLAGS


@deprecated(None, 'Use object_detection/model_main.py.')
def main(_):
  assert FLAGS.train_dir, '`train_dir` is missing.'
  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)
  if FLAGS.pipeline_config_path:
    configs = config_util.get_configs_from_pipeline_file(
        FLAGS.pipeline_config_path)
    if FLAGS.task == 0:
      tf.gfile.Copy(FLAGS.pipeline_config_path,
                    os.path.join(FLAGS.train_dir, 'pipeline.config'),
                    overwrite=True)
  else:
    configs = config_util.get_configs_from_multiple_files(
        model_config_path=FLAGS.model_config_path,
        train_config_path=FLAGS.train_config_path,
        train_input_config_path=FLAGS.input_config_path)
    if FLAGS.task == 0:
      for name, config in [('model.config', FLAGS.model_config_path),
                           ('train.config', FLAGS.train_config_path),
                           ('input.config', FLAGS.input_config_path)]:
        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),
                      overwrite=True)

  model_config = configs['model']
  train_config = configs['train_config']
  input_config = configs['train_input_config']

  model_fn = functools.partial(
      model_builder.build,
      model_config=model_config,
      is_training=True)

  def get_next(config):
    return dataset_builder.make_initializable_iterator(
        dataset_builder.build(config)).get_next()

  create_input_dict_fn = functools.partial(get_next, input_config)

  env = json.loads(os.environ.get('TF_CONFIG', '{}'))
  cluster_data = env.get('cluster', None)
  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None
  task_data = env.get('task', None) or {'type': 'master', 'index': 0}
  task_info = type('TaskSpec', (object,), task_data)

  # Parameters for a single worker.
  ps_tasks = 0
  worker_replicas = 1
  worker_job_name = 'lonely_worker'
  task = 0
  is_chief = True
  master = ''

  if cluster_data and 'worker' in cluster_data:
    # Number of total worker replicas include ""worker""s and the ""master"".
    worker_replicas = len(cluster_data['worker']) + 1
  if cluster_data and 'ps' in cluster_data:
    ps_tasks = len(cluster_data['ps'])

  if worker_replicas > 1 and ps_tasks < 1:
    raise ValueError('At least 1 ps task is needed for distributed training.')

  if worker_replicas >= 1 and ps_tasks > 0:
    # Set up distributed training.
    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',
                             job_name=task_info.type,
                             task_index=task_info.index)
    if task_info.type == 'ps':
      server.join()
      return

    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)
    task = task_info.index
    is_chief = (task_info.type == 'master')
    master = server.target

  graph_rewriter_fn = None
  if 'graph_rewriter_config' in configs:
    graph_rewriter_fn = graph_rewriter_builder.build(
        configs['graph_rewriter_config'], is_training=True)

  trainer.train(
      create_input_dict_fn,
      model_fn,
      train_config,
      master,
      task,
      FLAGS.num_clones,
      worker_replicas,
      FLAGS.clone_on_cpu,
      ps_tasks,
      worker_job_name,
      is_chief,
      FLAGS.train_dir,
      graph_hook_fn=graph_rewriter_fn)


if __name__ == '__main__':
  tf.app.run()


# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================="
51720,Inconsistency In CategoricalCrossentropy Calculation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below):  **2.6.0**
- Python version:  **3.6.13**
- GPU model and memory:  **CPU-only**

**Describe the current behavior**
The `categorical_crossentropy` loss values calculated with different ways is inconsistent in a model built by myself. 

1. In the first way, I calculate the loss value using `model.predict` and `CategoricalCrossentropy`
2. In tne second way, I calculate the loss value using `model.evaluate`
3. In the third way, I show the loss value using `model.fit`

The small model, input data `x`, target data `y_true` and the testing code are here:  [cce_test.zip](https://github.com/tensorflow/tensorflow/files/7068915/cce_test.zip)

The three results are: 
```
14.506287     287.1078     287.1078
```
Besides, I calculate the loss with the same model and data using `CNTK2.7` and Theano`1.0.4`，and their result are both `14.506287`. This makes me more suspicious to the calculation result of tensorflow.

**Describe the expected behavior**
```
14.506287    14.506287     14.506287
```

**Standalone code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

model = tf.keras.models.load_model(""model.h5"")
x = np.load(""x.npy"")
y_true = np.load(""y_true.npy"")

model.compile(loss='categorical_crossentropy', optimizer='adam')

# calculate loss using model.predict and categorical_crossentropy
cce = tf.keras.losses.CategoricalCrossentropy()
y_pred = model.predict(x=x)
loss1 = cce(y_true, y_pred)
print(""loss1 = "", loss1.numpy())

# calculate loss using model.evaluate
loss2 = model.evaluate(x=x, y=y_true)
print(""loss2 = "", loss2)

# show loss using model.fit
model.fit(x=x, y=y_true)
```
The result is:
```
loss1 =  14.506287
1/1 [==============================] - 0s 461ms/step - loss: 287.1078
loss2 =  287.10784912109375
1/1 [==============================] - 2s 2s/step - loss: 287.1078
```

**Other info / logs** 
Here is the code do the same thing using CNTK2.7:
```python
import numpy as np
import os
os.environ['KERAS_BACKEND'] = 'cntk'
import keras

model = keras.models.load_model(""model.h5"")
x = np.load(""x.npy"")
y_true = np.load(""y_true.npy"")

model.compile(loss='categorical_crossentropy', optimizer='adam')

# calculate loss using model.predict and categorical_crossentropy
cce = keras.losses.CategoricalCrossentropy()
y_pred = model.predict(x=x)
loss1 = cce([y_true], [y_pred])
print(""loss1 = "", loss1.eval())

# calculate loss using model.evaluate
loss2 = model.evaluate(x=x, y=y_true)
print(""loss2 = "", loss2)

# show loss using model.fit
model.fit(x=x, y=y_true)
```
The result is:
```
loss1 =  14.506287
convolution engine.
10/10 [==============================] - 0s 5ms/step
loss2 =  14.50628662109375
Epoch 1/1
10/10 [==============================] - 0s 21ms/step - loss: 14.5063
```

Any repiies will be appreciated.
Thanks."
51719,TensorRT model outputs/performance keeps changing across conversions,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): TensorFlow 2.6 , 2.7.0.dev20210825
- Python version: 3.7
- CUDA/cuDNN version: 11.0/8.0 and / 11.2/8.1
- TensorRT: LIBNVINFER=7.2.2-1
- GPU model and memory: v100 16GB

**Describe the current behavior**
I have been trying to convert an object detection model using the TensorRT converter.
I can successfully convert the model and also get the desired boost in latency while still maintaining the same outputs/performance. But more often or so, I see that my model shows degraded performance.

mAP score for `tf saved_model`: 40.3  (this does not change no matter how many times I run `tf.saved_model.save(...)`)
mAP score for `tf-trt saved_model`:  the results keep changing on each conversion [40.4, 36.0] (detailed metrics below)

```
conversion run 1 (FP16)
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.360
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.528
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.391
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.175
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.323
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.488
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.507
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.571
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.713
```

```
conversion run 2 (FP16)
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.404
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.598
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.434
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.220
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.453
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.575
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.333
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.522
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.549
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.331
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.610
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.732
```
This happens for both FP16 and FP32. And I have made sure that there is absolutely nothing that is changed/different in my environment across runs.

I am attaching the conversion logs for both the runs with
`TF_CPP_VMODULE=trt_engine_op=2,convert_nodes=2,convert_graph=2,segment=2,trt_shape_optimization_profiles=2,trt_engine_resource_ops=2`

**Describe the expected behavior**
Consistent outputs/performance across conversions

**Standalone code to reproduce the issue**
[`saved_model` download_link](https://github.com/srihari-humbarwadi/retinanet-tensorflow2.x/releases/download/v0.1.0/mscoco-retinanet-resnet50-640x640-30x-256.zip)

code to convert 
```python
import os
import sys

import tensorflow as tf

os.environ['TF_TRT_ALLOW_NMS_TOPK_OVERRIDE'] = '1'
print(tf.__version__)

physical_devices = tf.config.list_physical_devices('GPU')
[tf.config.experimental.set_memory_growth(x, True)
 for x in physical_devices]

PRECISION = 'FP16'

params = tf.experimental.tensorrt.ConversionParams(
    minimum_segment_size=10,
    precision_mode=PRECISION)


input_saved_model_dir = 'mscoco-retinanet-resnet50-640x640-30x-256'
output_saved_model_dir = 'tf-trt-' + PRECISION
print('Using saved_model from : ', input_saved_model_dir)

converter = tf.experimental.tensorrt.Converter(
    input_saved_model_dir=input_saved_model_dir,
    use_dynamic_shape=False,
    conversion_params=params)


def input_fn(steps=1):
    for i in range(steps):
        yield (
            tf.random.uniform([1, 640, 640, 3]), tf.constant(1, dtype=tf.int32),
            tf.ones([1, 4]))


converter.convert()
converter.build(input_fn=input_fn)

print('Saving converted saved_model to : ', output_saved_model_dir)
converter.save(output_saved_model_dir)
```

[36.0_run.log](https://github.com/tensorflow/tensorflow/files/7068995/36.0_run.log)
[40.4_run.log](https://github.com/tensorflow/tensorflow/files/7068996/40.4_run.log)

I cannot submit a standalone colab notebook because my model uses `tf.image.combined_non_max_suppression` which needs the TensorRT `batchedNMSPlugin` to run and that cannot be loaded in colab since
colab needs to have tensorrt python client installed to call so that it can load the plugin
```
import tensorrt as trt
trt.init_libnvinfer_plugins(None, '')
```
which is not possible currently https://github.com/googlecolab/colabtools/issues/1844"
51718,Symmetric convolved with symmetric returns non-symmetric,"**System information**
- OS Platform and Distribution: Arch Linux 5.13.12
- TensorFlow installed from: pip
- TensorFlow version: v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.9
- CUDA/cuDNN: running on CPU for now

When convolving a symmetric kernel with itself I expect the result to be also symmetric. This is what I get when using numpy, scipy or JAX, but for some reason it is not the case when using TensorFlow. I don't know if I could not understand something in the documentation, if it is like this on purpose, but it is surely not obvious. 

If this is not a bug, why does TensorFlow work like this, and how could I overcome this behaviour to get a 2D cross-correlation/convolution right?

**Standalone code to reproduce the issue**

See comparison bellow considering a (symmetric) laplacian filter being convolved with itself:

```python
import numpy as np

def test_convolution_jax(filt):
    from jax import numpy as jnp, lax
    filt = jnp.asarray(filt)
    return lax.conv(filt[:, :, None, None],
                    filt[:, :, None, None],
                    window_strides=(1, 1),
                    padding='SAME')[:, :, 0, 0]

def test_convolution_tf(filt):
    import tensorflow as tf
    filt = tf.Variable(filt)
    return tf.nn.convolution(filt[:, :, None, None],
                             filt[:, :, None, None],
                             strides=(1, 1),
                             padding='SAME')[:, :, 0, 0]

filt = np.asarray([[0,  1, 0],
                   [1, -4, 1],
                   [0,  1, 0]],
                   dtype=np.float32)

print('JAX:', test_convolution_jax(filt), sep='\n', end=2 * '\n')
print('TF:', test_convolution_tf(filt), sep='\n',)
```

Output:
```
JAX:
[[ 1. -4.  1.]
 [-4. 18. -4.]
 [ 1. -4.  1.]]

TF:
tf.Tensor(
[[ 1. -4.  1.]
 [-8. 18. -8.]
 [ 1. -4.  1.]], shape=(3, 3), dtype=float32)
```"
51716,Grappler graph optimizations may add nondeterministic behavior,"**System information**
- OS windows 10
- tensorflow-gpu==2.5.1
- python=3.7
- CUDA/cuDNN version: 11.2
- GPU model and memory: 471.68-desktop-win10-win11-64bit-international-dch-whql

**Got non-determinism on GPU.I found out it's the problem of Grappler graph optimizations,then i set this option false,but the train is more slower than cpu.**
options = {'implementation_selector':False}
tf.config.optimizer.set_experimental_options(options)

https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md#grappler-changes
**I found out this document may help me to fix this problem ,but don't know how to disabled timeout with a RewriteConfig option on tensorflow-gpu==2.5.1.**  
"
51713,How to modify ckpt value,"### First,
I can get variable name by  `tf.train.list_variables('./ckpt/ckpt-30')`

### Second,
[https://www.tensorflow.org/guide/checkpoint#manual_checkpointing](url)
I can get value by :
```
reader = tf.train.load_checkpoint('./ckpt/ckpt-30')
shape_from_key = reader.get_variable_to_shape_map()
dtype_from_key = reader.get_variable_to_dtype_map()
sorted(shape_from_key.keys())
key = 'net/l1/kernel/.ATTRIBUTES/VARIABLE_VALUE'
reader.get_tensor(key)
```

result is `array([[4.715253 , 4.832879 , 4.754512 , 4.891385 , 4.8498716]], dtype=float32)`

### So
I want to modify ckpt value,  but I don't know how to do it.

```
model = Model()
ckpt = tf.train.Checkpoint(model=model)
status = ckpt.restore('./ckpt/ckpt-30')

```
Can I get the value from ckpt, and modify value, and  ckpt.save() or ckpt.write(),   get a new ckpt file?
Please show me simple code, thanks!!!!!!!
"
51712,[tf2.6] Loaded subclassed-Model fails to be retrained,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
-- Dockerfile: 
```
FROM python:3.8

WORKDIR /usr/src/app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ ""python"", ""./test.py"" ]
```
-- requirements.txt:
```
numpy
tensorflow
```
and
`docker build -t python-docker .`

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested
- TensorFlow installed from (source or binary): Docker on Mac mini (2018)
- TensorFlow version (use command below): TF2.6
- Python version: 3.8
- Bazel version (if compiling from source): Not tested, unlikely to be related to this issue
- GCC/Compiler version (if compiling from source): Not tested, unlikely to be related to this issue
- CUDA/cuDNN version: No CUDA on this Mac mini
- GPU model and memory: N/A

**Describe the current behavior**
Typing `docker run -it --rm -v ""$PWD"":/usr/src/app -w /usr/src/app python-docker python test.py` in my environment will end up getting the following error. More specifically, `predict(x, y)` works perfectly, however, `fit(x, y)` is the cause.

```
ValueError: No gradients provided for any variable: ['abs:0', 'non_distributional_model/dense_1/kernel:0', 'non_distributional_model/dense_1/bias:0', 'non_distributional_model/dense_2/kernel:0', 'non_distributional_model/dense_2/bias:0', 'non_distributional_model/output_layer/kernel:0', 'non_distributional_model/output_layer/bias:0'].
```

**Describe the expected behavior**
The loaded model should be retrainable.


**Standalone code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense

BATCH_SIZE = 2
NUM_ACTION = 11
STATE_DIM = 1

""""""Customer Loss functions.""""""
def _huber_loss(y_true, y_pred, max_grad=1.):
    a = tf.abs(y_true - y_pred)
    less_than_max = 0.5 * tf.square(a)
    greater_than_max = max_grad * (a - 0.5 * max_grad)
    return tf.where(a <= max_grad, x=less_than_max, y=greater_than_max)

@tf.keras.utils.register_keras_serializable()
class MeanHuberLoss(keras.losses.Loss):
    def __init__(self, name='mean_huber_loss', **kwargs):
        super(MeanHuberLoss, self).__init__(name=name, **kwargs)

    def call(self, y_true, y_pred):
        error = _huber_loss(y_true, y_pred)

        # The reduce_mean is automatically done as default
        return error

@tf.keras.utils.register_keras_serializable()
class DirectMappingForAbs(keras.metrics.Metric):
    def __init__(self, name='direct_map_for_abs', **kwargs):
        super(DirectMappingForAbs, self).__init__(name=name, **kwargs)
        self.output_value = tf.Variable(initial_value=[], name='abs', shape=(None, ), validate_shape=False, dtype=tf.float32)

    def update_state(self, values, sample_weight=None):
        self.output_value.assign(values)

    def result(self):
        return self.output_value

    def reset_state(self):
        self.output_value.assign([])

@tf.keras.utils.register_keras_serializable()
class NonDistributionalModel(keras.Model):
    def __init__(self, 
                 name=""non_distributional_model"",
                 num_output=None,
                 trainable=True):
        super(NonDistributionalModel, self).__init__(name=name)

        self.loss_tracker = keras.metrics.Mean(name=""loss"")
        self.abs_metric = DirectMappingForAbs(name=""abs"") # Returns a tensor with the same shape of the input tensors
        self.criterion = MeanHuberLoss()

        self.layer_1 = Dense(10, trainable=trainable, activation='relu', name=""dense_1"")  
        self.layer_2 = Dense(10, trainable=trainable, activation='relu', name=""dense_2"")
        self.output_layer = Dense(num_output, trainable=trainable, activation=None, name=""output_layer"")

    def call(self, inputs):
        inputs = tf.cast(inputs, tf.float32)
        layer_1 = self.layer_1(inputs)
        layer_2 = self.layer_2(layer_1)
        output = self.output_layer(layer_2)

        return output

    @tf.function
    def train_step(self, data):
        states, targets = data

        targets = tf.stop_gradient(targets)
        with tf.GradientTape() as tape:
            logits = self(states, training=True)  # Forward pass
            loss = self.criterion(targets, logits)

        trainable_vars = self.trainable_variables
        grads = tape.gradient(loss, trainable_vars)
        self.optimizer.apply_gradients(zip(grads, trainable_vars))

        self.loss_tracker.update_state(loss)
        self.abs_metric.update_state(tf.cast(tf.reduce_mean(tf.math.abs(targets - logits), axis=-1), tf.float32))
        
        return {""loss"": self.loss_tracker.result(), ""abs"": self.abs_metric.result()}  

    @property
    def metrics(self):
        return [self.loss_tracker, self.abs_metric]   

class History(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
        self.abs = []

    def on_train_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
        self.abs.append(logs.get('abs'))

class _DQN_Model:
    def __init__(self, 
                 alpha, 
                 batch_size, 
                 num_output, 
                 trainable=True):
        self.alpha = alpha
        self.batch_size = batch_size
        self.num_output = num_output
        self.trainable = trainable

        self.model = self._build_model()

    def _build_model(self):
        lr_schedule = keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=self.alpha,
                                                                       first_decay_steps=1000)

        model = NonDistributionalModel(num_output=self.num_output, trainable=self.trainable)
        model.compile(optimizer=Adam(lr_schedule))

        return model

    def predict(self, state):
        return self.model.predict(state)

    def train(self, states, targets):
        history = History()

        return self.model.fit(states, targets, batch_size=self.batch_size, epochs=1, verbose=0, callbacks=[history])

class Critic(object):
    def __init__(self, alpha, batch_size):
        self.alpha = alpha
        self.batch_size = batch_size

        self._eval_model = _DQN_Model(alpha=alpha, batch_size=batch_size, num_output=NUM_ACTION, trainable=True)

    def learn(self):
        x = np.random.random((self.batch_size, 1))
        y = np.random.random((self.batch_size, NUM_ACTION))
        history = self._eval_model.train(x, y)

        return tf.squeeze(history.history['loss'])


critic = Critic(alpha=0.1, batch_size=BATCH_SIZE)
critic.learn()

critic._eval_model.model.save(""model_saved"")
loaded_model = keras.models.load_model(""model_saved"")
print(""Saved Model Weights: {}, Type: {}"".format(len(critic._eval_model.model.get_weights()), type(critic._eval_model.model)))
print(""Loaded Model Weights: {}, Type: {}"".format(len(loaded_model.get_weights()), type(loaded_model)))

critic._eval_model.model.summary()
loaded_model.summary()

x=np.random.random((BATCH_SIZE, 1))
y=np.random.random((BATCH_SIZE, NUM_ACTION))

# Let's check:
np.testing.assert_allclose(
    critic._eval_model.predict(x), loaded_model.predict(x)
)

print(""Continue to train the loaded model\n"")
history = History()
result = loaded_model.fit(x, y, batch_size=BATCH_SIZE, epochs=1, verbose=0, callbacks=[history])

```

**Other info / logs** 
```
2021-08-27 12:28:38.619800: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-08-27 12:28:38.619860: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-08-27 12:28:40.667183: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2021-08-27 12:28:40.667238: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-08-27 12:28:40.667269: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a3f95dfc3a77): /proc/driver/nvidia/version does not exist
2021-08-27 12:28:40.667437: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-08-27 12:28:40.722464: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
WARNING:tensorflow:Gradients do not exist for variables ['abs:0'] when minimizing the loss.
2021-08-27 12:28:41.133216: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Saved Model Weights: 9, Type: <class '__main__.NonDistributionalModel'>
Loaded Model Weights: 9, Type: <class 'keras.saving.saved_model.load.Custom>NonDistributionalModel'>
Model: ""non_distributional_model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              multiple                  20
_________________________________________________________________
dense_2 (Dense)              multiple                  110
_________________________________________________________________
output_layer (Dense)         multiple                  121
=================================================================
Total params: 253
Trainable params: 251
Non-trainable params: 2
_________________________________________________________________
Model: ""non_distributional_model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              multiple                  20
_________________________________________________________________
dense_2 (Dense)              multiple                  110
_________________________________________________________________
output_layer (Dense)         multiple                  121
=================================================================
Total params: 253
Trainable params: 251
Non-trainable params: 2
_________________________________________________________________
Continue to train the loaded model

Traceback (most recent call last):
  File ""test2.py"", line 182, in <module>
    result = loaded_model.fit(x, y, batch_size=BATCH_SIZE, epochs=1, verbose=0, callbacks=[history])
  File ""/usr/local/lib/python3.8/site-packages/keras/engine/training.py"", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 759, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3066, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3463, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3298, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1007, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 668, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 994, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:853 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:842 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:835 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:791 train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    /usr/local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:522 minimize
        return self.apply_gradients(grads_and_vars, name=name)
    /usr/local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:622 apply_gradients
        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    /usr/local/lib/python3.8/site-packages/keras/optimizer_v2/utils.py:72 filter_empty_gradients
        raise ValueError(""No gradients provided for any variable: %s."" %

    ValueError: No gradients provided for any variable: ['abs:0', 'non_distributional_model/dense_1/kernel:0', 'non_distributional_model/dense_1/bias:0', 'non_distributional_model/dense_2/kernel:0', 'non_distributional_model/dense_2/bias:0', 'non_distributional_model/output_layer/kernel:0', 'non_distributional_model/output_layer/bias:0'].
```

Thank you for your time."
51710,Separable Conv2D allows dilation_rate!=1 and strides !=1,"Hey Tensorflow Community :-),

Nothing serious :).

I just noticed in tensorflow 2.4 (sry can not update to 2.7 currently since I am stuck to this version because of other compatibilty issues, so I can't tell if this also affects your newest version) that the layer Separable Conv2D states in it's documentations that if I assign another value for the dilation rate than 1 the strides are not to be allowed to have another value than 1 and vice versa. 

_dilation_rate: An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1._

https://keras.io/api/layers/convolution_layers/separable_convolution2d/

But in fact it is possible to set both parameters with values > 1. 
(Since this is also true (and you will get an exception if you try to apply other values than 1 for both parameters) for the normal covolutional layers I think it is either a bug or a documentation copycat issue.)

Cheers :)"
51709,'@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a',"Hi,
   I want to compiler the run_eval for android edgetu device test. but encouter the ""does not contain a toolchain for cpu 'arm64-v8a'"". Could you reference the following fail and then give me some suggestions about this?
Thank a lot!

**System information**
- OS Platform: ubuntu 20.04
- Bazel version : 3.7.2

** Setup environment **
Android SDK:
    - sudo apt-get install android-sdk
Android NDK
    -  wget -q https://dl.google.com/android/repository/android-ndk-r21e-linux-x86_64.zip
Setup the $PATH evironment:
    - echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:$:/usr/lib/android-sdk//tools:/usr/lib/android-sdk//platform-tools:/usr/lib/android-sdk//tools:/usr/lib/android-sdk//platform-tools:::/home/yihsin_hung/android-ndk-r21e

** Build command **
    cd tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification
    bazel build -c opt --config=android_arm64 --cxxopt='--std=c++17' //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval

** Build fail Log **

w/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
ERROR: /home/yihsin_hung/.cache/bazel/_bazel_yihsin_hung/d610790c850d0b74b4f90630e4b19108/external/local_config_cc/BUILD:47:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /home/yihsin_hung/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/yihsin_hung/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  /home/yihsin_hung/.cache/bazel/_bazel_yihsin_hung/d610790c850d0b74b4f90630e4b19108/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  /home/yihsin_hung/.cache/bazel/_bazel_yihsin_hung/d610790c850d0b74b4f90630e4b19108/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
ERROR: Analysis of target '//tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed
INFO: Elapsed time: 0.267s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)


"
51708,Error while trying to convert TF-Hub MRCNN model to tflite,"System infoemartion:

- Ubuntu 18.04:
- TensorFlow :2.1.0


Got tf model from https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1

import tensorflow as tf
import tensorflow_hub as hub
inputs = tf.keras.Input(shape=(1024, 1024, 3),batch_size=1,dtype=tf.uint8)
m_l = hub.KerasLayer(""https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1"")
x = m_l(inputs)
model = tf.keras.Model(inputs=inputs, outputs=x, name=""mrcnn"")
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()


When I run above python code:
got error as 2021-08-27 12:49:16.478141: F tensorflow/lite/toco/tooling_util.cc:2277] Check failed: array.data_type == array.final_data_type Array “input_1” has mis-matching actual and final data types (data_type=uint8, final_data_type=float).
Fatal Python error: Aborted

"
51707,07502269190,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
[git_test-master.zip](https://github.com/tensorflow/tensorflow/files/7064174/git_test-master.zip)
"
51704,Tensorflow not installing correctly,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): not sure
- TensorFlow version: current
- Python version: 3
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: unknown
- GPU model and memory: unknown, using online IDE



**Describe the problem**
Tensorflow not installing correctly
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Tried to import the tensorflow module

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
EnvCommandError

  Command ['/opt/virtualenvs/python3/bin/pip', 'install', '--no-deps', 'file:///home/runner/.cache/pypoetry/artifacts/0a/85/65/9562a53ef2caf00c826443d78d6f2fae460ab81e7bcf44770c09799f9c/tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl'] errored with the following return code 1, and output: 
  Processing /home/runner/.cache/pypoetry/artifacts/0a/85/65/9562a53ef2caf00c826443d78d6f2fae460ab81e7bcf44770c09799f9c/tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl
  ERROR: Could not install packages due to an EnvironmentError: [Errno 122] Disk quota exceeded: '/tmp/pip-install-q4y7r3fp/tensorflow/tensorflow/_api/v2/mlir/experimental'
  
  WARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.
  You should consider upgrading via the 'pip install --upgrade pip' command.
  

  at /opt/virtualenvs/python3/lib/python3.8/site-packages/poetry/utils/env.py:1075 in _run
      1071│                 output = subprocess.check_output(
      1072│                     cmd, stderr=subprocess.STDOUT, **kwargs
      1073│                 )
      1074│         except CalledProcessError as e:
    → 1075│             raise EnvCommandError(e, input=input_)
      1076│ 
      1077│         return decode(output)
      1078│ 
      1079│     def execute(self, bin, *args, **kwargs):

exit status 1
```
"
51700,How to use StreamingFilesDataset,"**System information**
- TensorFlow version (you are using):
2.6
- Python version (you are using):
3.7


**Describe the feature and the current behavior/state.**
I open a colab tpu runtime and try to run this [notebook](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/keras_recipes/ipynb/creating_tfrecords.ipynb?hl=en)

I use your sample code of tpu setting and follow the instructions of
**StreamingFilesDataset** in **tensorflow.python.tpu.datasets.py**
and create similar function get_dataset2() like original one.

But it didn't work. [My adapted notebook](https://colab.research.google.com/drive/1lEUjjWffl5Vsh1JuGLzshl2bYaduCZD6?usp=sharing)
It raised errors like
1. InvalidArgumentError: `cycle_length` must be > 0 [Op:LegacyParallelInterleaveDatasetV2]
or
2. source_iterator has no attribute string_handle()


You can quickly find my adaptions about tpu setting and get_dataset2()
from table. 

I have checked some issue reports and know that it probably is a conflict of version. But I downgrade the tensorflow version to 1.x, it still didn't work.

Would you help me find the solution of using StreamingFilesDataset().
Thanks a lot!"
51699,kernel error,"WARNING:tensorflow:AutoGraph could not transform <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x134a4d520>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
5 6
WARNING:tensorflow:AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x134af03a0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2021-08-26 19:43:01.304618: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-08-26 19:43:01.304771: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz
2021-08-26 19:43:02.016427: I tensorflow/compiler/tf2mlcompute/kernels/mlc_subgraph_op.cc:537] Compute: Failed in processing TensorFlow graph model/prediction_layer/MLCSubgraphOp_0_24 with error: Internal: AddLayerToMLCGraph: Failed to add layer to MLCGraph for node: model/prediction_layer/Reshape (Reshape). (error will be reported 5 times unless TF_MLC_LOGGING=1).
2021-08-26 19:43:02.017183: I tensorflow/compiler/tf2mlcompute/kernels/mlc_subgraph_op.cc:537] Compute: Failed in processing TensorFlow graph gradient_tape/model/prediction_layer/MLCSubgraphOp_0_26 with error: Internal: PerformGradientPassNodeRoutine: Failed to find forward-pass output for node: model/prediction_layer/Reshape (error will be reported 5 times unless TF_MLC_LOGGING=1).
1/4 [======>.......................] - ETA: 2s - loss: 0.69472021-08-26 19:43:02.020231: I tensorflow/compiler/tf2mlcompute/kernels/mlc_subgraph_op.cc:537] Compute: Failed in processing TensorFlow graph model/prediction_layer/MLCSubgraphOp_0_24 with error: Internal: AddLayerToMLCGraph: Failed to add layer to MLCGraph for node: model/prediction_layer/Reshape (Reshape). (error will be reported 5 times unless TF_MLC_LOGGING=1).
/Users/tal/miniforge3/envs/jianliang/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '"
51698,"kernel shutdown, error","when i run this code(recall model)

from deepmatch.models import *
model = DSSM(user_feature_columns, item_feature_columns)
model.compile(optimizer='adam', loss='binary_crossentropy')
history = model.fit(train_model_input, train_label,  batch_size=128, epochs=1, verbose=1 )

kernel shutdown

WARNING:tensorflow:AutoGraph could not transform <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x15b884c10>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x15b884c10>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x15b8f2c10>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x15b8f2c10>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
1/8 [==>...........................] - ETA: 5s - loss: 0.6866"
51694,Training Mask R Cnn ,"I find Mask r cnn in TF hub. But It looks like I can't training this model. Only Using trained model. 
How can train the my dataset at model Mask R cnn?

[TF hub](https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1)
[Code in Colab](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb#scrollTo=Gb_siXKcnnGC)

"
51693,Support resizing images on TPU using `tf.image.resize` when `size` is not a compile-time constant,"**System information**
- TensorFlow version (you are using): 2.6
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
XLA compilation fails when the `size` arg of `tf.image.resize` is not a compile-time constant.

**Who will benefit with this feature?**
This will allow users to resize images to dynamic sizes on TPUs

**Any Other info.**
code to reproduce the issue
```python
import tensorflow as tf
print('TensorFlow:', tf.__version__)

resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect('local')
strategy = tf.distribute.TPUStrategy(resolver)


def _run(inputs):
    mask, height, width = inputs
    return tf.image.resize(mask, size=[height, width], method='nearest')

@tf.function
def _distributed_run(inputs):
    outputs = strategy.run(_run, args=(inputs,))
    return strategy.gather(outputs, axis=0)

mask = tf.random.normal((1, 100, 100, 3))
height = 200
width = 200
inputs = (mask, height, width)

outputs = _distributed_run(inputs)
print(outputs.shape)
```
output:
```
TensorFlow: 2.6.0

D0826 06:54:39.426496290   15038 ev_posix.cc:173]            Using polling engine: epollex
D0826 06:54:39.426573151   15038 lb_policy_registry.cc:42]   registering LB policy factory for ""grpclb""
D0826 06:54:39.426583436   15038 lb_policy_registry.cc:42]   registering LB policy factory for ""priority_experimental""
D0826 06:54:39.426587482   15038 lb_policy_registry.cc:42]   registering LB policy factory for ""weighted_target_experimental""
D0826 06:54:39.426591102   15038 lb_policy_registry.cc:42]   registering LB policy factory for ""pick_first""
D0826 06:54:39.426594552   15038 lb_policy_registry.cc:42]   registering LB policy factory for ""round_robin""
D0826 06:54:39.426606683   15038 dns_resolver_ares.cc:499]   Using ares dns resolver
D0826 06:54:39.426633671   15038 certificate_provider_registry.cc:33] registering certificate provider factory for ""file_watcher""
D0826 06:54:39.426644953   15038 lb_policy_registry.cc:42]   registering LB policy factory for ""cds_experimental""
D0826 06:54:39.426649442   15038 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_impl_experimental""
D0826 06:54:39.426653456   15038 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_resolver_experimental""
D0826 06:54:39.426658294   15038 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_manager_experimental""
I0826 06:54:39.426774965   15038 server_builder.cc:332]      Synchronous server. Num CQs: 1, Min pollers: 1, Max Pollers: 2, CQ timeout (msec): 10000
I0826 06:54:39.426889181   15038 socket_utils_common_posix.cc:353] TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter
I0826 06:54:39.472098409   15342 subchannel.cc:1065]         New connected subchannel at 0x62a3ce0 for subchannel 0x393c600
D0826 06:54:43.380396347   15830 init.cc:226]                grpc_shutdown starts clean-up now
D0826 06:54:46.843676713   15830 ev_posix.cc:173]            Using polling engine: epollex
D0826 06:54:46.843732206   15830 lb_policy_registry.cc:42]   registering LB policy factory for ""grpclb""
D0826 06:54:46.843754461   15830 lb_policy_registry.cc:42]   registering LB policy factory for ""priority_experimental""
D0826 06:54:46.843758619   15830 lb_policy_registry.cc:42]   registering LB policy factory for ""weighted_target_experimental""
D0826 06:54:46.843761981   15830 lb_policy_registry.cc:42]   registering LB policy factory for ""pick_first""
D0826 06:54:46.843768556   15830 lb_policy_registry.cc:42]   registering LB policy factory for ""round_robin""
D0826 06:54:46.843777491   15830 dns_resolver_ares.cc:499]   Using ares dns resolver
D0826 06:54:46.843795296   15830 certificate_provider_registry.cc:33] registering certificate provider factory for ""file_watcher""
D0826 06:54:46.843810614   15830 lb_policy_registry.cc:42]   registering LB policy factory for ""cds_experimental""
D0826 06:54:46.843814307   15830 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_impl_experimental""
D0826 06:54:46.843818589   15830 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_resolver_experimental""
D0826 06:54:46.843825122   15830 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_manager_experimental""
I0826 06:54:46.843933271   15830 server_builder.cc:332]      Synchronous server. Num CQs: 1, Min pollers: 1, Max Pollers: 2, CQ timeout (msec): 10000
I0826 06:54:46.844822787   15631 subchannel.cc:1065]         New connected subchannel at 0x63af920 for subchannel 0x3867c80
F0826 06:54:47.241484   15823 image_resize_ops.cc:42] Check failed: out_size.size() == 2 (0 vs. 2) Invalid argument: Input 1 to node `resize/ResizeNearestNeighbor` with op ResizeNearestNeighbor must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.
*** Check failure stack trace: ***
    @     0x7f0a1cc2ad87  (unknown)
    @     0x7f0a1cc29914  (unknown)
    @     0x7f0a1cc292c3  (unknown)
    @     0x7f0a1cc2b709  (unknown)
    @     0x7f0a18de0146  (unknown)
    @     0x7f0a18ddfbcf  (unknown)
    @     0x7f0a193f29da  (unknown)
    @     0x7f0a19c5e92c  (unknown)
    @     0x7f0a193d4295  (unknown)
    @     0x7f0a193e0ba5  (unknown)
    @     0x7f0a193dc526  (unknown)
    @     0x7f0a18e4f8d7  (unknown)
    @     0x7f0a129d5c5e  (unknown)
    @     0x7f0a129d7373  (unknown)
    @     0x7f0a129cecd3  TpuCompile_CompileAndBuild
    @     0x7f0a25aa81a5  tensorflow::tpu::TpuProgramGroup::CompileAndBuild()
    @     0x7f0a25a3bed9  tensorflow::tpu::TpuCompileOpKernelImpl::Compile()
    @     0x7f0a25ab2122  tensorflow::tpu::TpuCompileOpKernelCommon::CompileLocallyAndFillHostCache()
    @     0x7f0a25ab27a8  tensorflow::tpu::TpuCompileOpKernelCommon::ComputeInternal()::{lambda()#3}::operator()()
    @     0x7f0a25ab287c  std::_Function_handler<>::_M_invoke()
    @     0x7f0a25a6d25a  tensorflow::tpu::TpuCompilationCacheExternal::InitializeEntry()
    @     0x7f0a25abc45a  tensorflow::tpu::TpuCompilationCacheInterface::CompileIfKeyAbsentHelper()
    @     0x7f0a25abcf4a  tensorflow::tpu::TpuCompilationCacheInterface::CompileIfKeyAbsent()
    @     0x7f0a25ab4ca4  tensorflow::tpu::TpuCompileOpKernelCommon::ComputeInternal()
    @     0x7f0a25ab608d  tensorflow::tpu::TpuCompileOpKernelCommon::Compute()
    @     0x7f0a2c8fb330  tensorflow::(anonymous namespace)::ExecutorState<>::Process()
    @     0x7f0a2c8eed82  std::_Function_handler<>::_M_invoke()
    @     0x7f0a2cc12e65  Eigen::ThreadPoolTempl<>::WorkerLoop()
    @     0x7f0a2cc10b37  std::_Function_handler<>::_M_invoke()
    @     0x7f0a2cbf41ef  tensorflow::(anonymous namespace)::PThread::ThreadFn()
    @     0x7f0c4f7a7609  start_thread
https://symbolize.stripped_domain/r/?trace=7f0a1cc2ad87,7f0a1cc29913,7f0a1cc292c2,7f0a1cc2b708,7f0a18de0145,7f0a18ddfbce,7f0a193f29d9,7f0a19c5e92b,7f0a193d4294,7f0a193e0ba4,7f0a193dc525,7f0a18e4f8d6,7f0a129d5c5d,7f0a129d7372,7f0a129cecd2,7f0a25aa81a4,7f0a25a3bed8,7f0a25ab2121,7f0a25ab27a7,7f0a25ab287b,7f0a25a6d259,7f0a25abc459,7f0a25abcf49,7f0a25ab4ca3,7f0a25ab608c,7f0a2c8fb32f,7f0a2c8eed81,7f0a2cc12e64,7f0a2cc10b36,7f0a2cbf41ee,7f0c4f7a7608&map=b7c22d7954df6b6961e4435041132cf899ee4a5e:7f0a1d725000-7f0a31424270,ca1b7ab241ee28147b3d590cadb5dc1b:7f0a0ff1c000-7f0a1cf4eb20
https://symbolize.stripped_domain/r/?trace=7f0c4f80718b,7f0c4f80720f,7f0a1cc2aec7,7f0a1cc29913,7f0a1cc292c2,7f0a1cc2b708,7f0a18de0145,7f0a18ddfbce,7f0a193f29d9,7f0a19c5e92b,7f0a193d4294,7f0a193e0ba4,7f0a193dc525,7f0a18e4f8d6,7f0a129d5c5d,7f0a129d7372,7f0a129cecd2,7f0a25aa81a4,7f0a25a3bed8,7f0a25ab2121,7f0a25ab27a7,7f0a25ab287b,7f0a25a6d259,7f0a25abc459,7f0a25abcf49,7f0a25ab4ca3,7f0a25ab608c,7f0a2c8fb32f,7f0a2c8eed81,7f0a2cc12e64,7f0a2cc10b36,7f0a2cbf41ee,7f0c4f7a7608&map=b7c22d7954df6b6961e4435041132cf899ee4a5e:7f0a1d725000-7f0a31424270,ca1b7ab241ee28147b3d590cadb5dc1b:7f0a0ff1c000-7f0a1cf4eb20
*** SIGABRT received by PID 15038 (TID 15823) on cpu 2 from PID 15038; ***
E0826 06:54:47.512108   15823 coredump_hook.cc:292] RAW: Remote crash data gathering hook invoked.
E0826 06:54:47.512125   15823 coredump_hook.cc:384] RAW: Skipping coredump since rlimit was 0 at process start.
E0826 06:54:47.512134   15823 client.cc:222] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.
E0826 06:54:47.512143   15823 coredump_hook.cc:447] RAW: Sending fingerprint to remote end.
E0826 06:54:47.512149   15823 coredump_socket.cc:124] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket
E0826 06:54:47.512154   15823 coredump_hook.cc:451] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?
E0826 06:54:47.512158   15823 coredump_hook.cc:525] RAW: Discarding core.
F0826 06:54:47.241484   15823 image_resize_ops.cc:42] Check failed: out_size.size() == 2 (0 vs. 2) Invalid argument: Input 1 to node `resize/ResizeNearestNeighbor` with op ResizeNearestNeighbor must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a p
E0826 06:54:47.998539   15823 process_state.cc:771] RAW: Raising signal 6 with default behavior
Aborted (core dumped)
```"
51692,TFLite for conv1D and DepthwiseConv1D,"**System information**
- OS Platform and Distribution  Linux Ubuntu 16.04:
- TensorFlow installed from binary:
- TensorFlow version (tf-nightly 2.7.0.dev20210824):

The h5 is:
![image](https://user-images.githubusercontent.com/4407779/130913616-d18620ec-cdab-45e5-8db8-dc25849cc155.png)
When I convert it to tflite. It becomes 
![image](https://user-images.githubusercontent.com/4407779/130913708-57624af5-60d8-4551-a7b2-3cd3b1413c04.png)

My input is only a speech spectrum. I have to add one additional dim if I use conv2d. So I think conv1d is enough and use conv1d/depthwiseconv1d to generate model.  But the problem is that I still get conv2d and ds2d in tflite. It isn't helpful to deal with my high MIPS issue if it is still conv2d.  Any idea on it? Thanks. "
