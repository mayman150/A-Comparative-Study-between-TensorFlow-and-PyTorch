Issue Number,Issue Title,Issue Body
38403,model.reset_states() does not work for bidirectional-RNNs in tf.keras,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- TensorFlow installed from (source or
binary): binary
- TensorFlow version (use command below):  
TF 2.1
and
tf-nightly==2.2.0.dev20200407
(both have bug around this issue, but different issues)

- Python version:  3.7.4

- CUDA/cuDNN version: 10.1, 7.6.5

- GPU model and memory: 2080ti, 11GB.  Bug is on both CPU/GPU.

**Describe the current behavior**

model.reset_states() does not work for bidirectional, stateful recurrent layers (bidi-RNNs).

TF 2.1: model.reset_states() does nothing for stateful bidi-RNNs.
tf-nightly: calling model.reset_states() for stateful bidi-RNNs causes a crash

**Describe the expected behavior**
This was reported as a bug in TF 2.0 -- [model.reset_states() does nothing for bidi-RNNs](https://github.com/tensorflow/tensorflow/issues/34055).  I thought this was fixed in tf-nightly at that time, but has returned in TF 2.1.  

model.reset_states() for standard RNNs changed in TF 2.1. has the following behavior:

* if model is stateful with NO initial state input: resets state to zero
* if model is stateful with initial state input: resets state to state input
* otherwise the state is carried over form the last call.

Thus the **expected behavior** for stateful bidi-RNNs is:

* if model is stateful with NO initial state input: resets fwd and bwd state to zero
* if model is stateful with initial state input: resets fwd state to fwd state input and  resets bwd state to bwd state input
* otherwise the fwd state and bwd state are carried over form the last call (as is done in stateful bidi-RNNs).  

**Standalone code to reproduce the issue** 

Code to show this behavior with no state-inputs:
```python
import os
os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES']=''

import numpy as np
from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional
from tensorflow.keras.models import Model

REC = LSTM

sequence_length = 3
feature_dim = 1
features_in = Input(batch_shape=(1, sequence_length, feature_dim)) 

rnn_out = Bidirectional( REC(1, activation=None, use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in)
stateless_model = Model(inputs=[features_in], outputs=[rnn_out])

stateful_rnn_out = Bidirectional( REC(1, activation=None, use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in)
stateful_model = Model(inputs=features_in, outputs=stateful_rnn_out)

stateful_model.set_weights( stateless_model.get_weights() )

x_in = np.random.normal(0,10,sequence_length)
x_in = x_in.reshape( (1, sequence_length, feature_dim) )

def print_bidi_out(non_stateful_out, stateful_out):
	fb = ['FWD::', 'BWD::']

	for i in range(2):
		print(fb[i])
		print(f'non_stateful: {non_stateful_out.T[i]}')
		print(f'stateful: {stateful_out.T[i]}')
		print(f'delta: {stateful_out.T[i]-non_stateful_out.T[i]}')


non_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))
stateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))
print_bidi_out(non_stateful_out, stateful_out)

non_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))
stateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))
print_bidi_out(non_stateful_out, stateful_out)

print('\n** RESETING STATES in STATEFUL MODEL **\n')
stateful_model.reset_states()
non_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))
stateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))
print_bidi_out(non_stateful_out, stateful_out)
```

Code to demo with initial-state inputs:
```python
import os
os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES']=''

import numpy as np
from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional
from tensorflow.keras.models import Model

REC = LSTM

sequence_length = 3
feature_dim = 1
features_in = Input(batch_shape=(1, sequence_length, feature_dim)) 
state_h_fwd_in = Input(batch_shape=(1, 1))
state_h_bwd_in = Input(batch_shape=(1, 1))
state_c_fwd_in = Input(batch_shape=(1, 1))
state_c_bwd_in = Input(batch_shape=(1, 1))

four_state_shape = [state_h_fwd_in, state_c_fwd_in, state_h_bwd_in, state_c_bwd_in]
two_state_shape = [state_h_fwd_in, state_h_bwd_in]

if REC == LSTM:
    rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=four_state_shape)
    stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=four_state_shape)
    rnn_inputs = [features_in, state_h_fwd_in, state_c_fwd_in, state_h_bwd_in, state_c_bwd_in]
else:
    if REC == SimpleRNN:
        rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=two_state_shape)
        stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=two_state_shape)
    else:
        rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=two_state_shape)
        stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=two_state_shape)
    rnn_inputs = [features_in, state_h_fwd_in, state_h_bwd_in]

stateless_model = Model(inputs=rnn_inputs, outputs=rnn_out)
stateful_model = Model(inputs=rnn_inputs, outputs=stateful_rnn_out)


# toy_weights = [np.asarray([[ 1.0]], dtype=np.float32), np.asarray([[0.5 ]], dtype=np.float32), np.asarray([[ -1.0 ]], dtype=np.float32), np.asarray([[ -0.5 ]], dtype=np.float32)]
# stateless_model.set_weights(toy_weights)
# stateful_model.set_weights(toy_weights)

stateful_model.set_weights( stateless_model.get_weights() )

stateful_model.save('temp_stateful.h5')
stateless_model.save('temp_stateless.h5')

x_in = np.random.normal(0,10,sequence_length)
x_in = np.asarray([1,0,0])
x_in = x_in.reshape( (1, sequence_length, feature_dim) )

fwd_initial_h = np.asarray(2.75).reshape(1,1)
fwd_initial_c = np.asarray(1.3).reshape(1,1)
bwd_initial_h = np.asarray(-2.0).reshape(1,1)
bwd_initial_c = np.asarray(-1.2).reshape(1,1)

# fwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)
# fwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)
# bwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)
# fwd_initial_c = np.asarray(np.random.normal(0,10)).reshape(1,1)
# bwd_initial_c = np.asarray(np.random.normal(0,10)).reshape(1,1)

if REC == LSTM:
    rnn_input = [x_in, fwd_initial_h, fwd_initial_c, bwd_initial_h, bwd_initial_c]
else:
    rnn_input = [x_in, fwd_initial_h, bwd_initial_h] 
    

def print_bidi_out(non_stateful_out, stateful_out):
	fb = ['FWD::', 'BWD::']

	for i in range(2):
		print(fb[i])
		print(f'non_stateful: {non_stateful_out.T[i]}')
		print(f'stateful: {stateful_out.T[i]}')
		print(f'delta: {stateful_out.T[i]-non_stateful_out.T[i]}')

non_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))
stateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))
print_bidi_out(non_stateful_out, stateful_out)

non_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))
stateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))
print_bidi_out(non_stateful_out, stateful_out)

print('\n** RESETING STATES in STATEFUL MODEL **\n')
stateful_model.reset_states()
non_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))
stateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))
print_bidi_out(non_stateful_out, stateful_out)
```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

sample output for a SimpleRNN with input states -- using TF 2.1:

```
FWD::
non_stateful: [7.375   3.6875  1.84375]
stateful: [7.375   3.6875  1.84375]
delta: [0. 0. 0.]
BWD::
non_stateful: [ 11.5 -25.   50. ]
stateful: [ 11.5 -25.   50. ]
delta: [0. 0. 0.]
FWD::
non_stateful: [7.375   3.6875  1.84375]
stateful: [1.921875   0.9609375  0.48046875]
delta: [-5.453125  -2.7265625 -1.3632812]
BWD::
non_stateful: [ 11.5 -25.   50. ]
stateful: [-2.4375  2.875  -5.75  ]
delta: [-13.9375  27.875  -55.75  ]

** RESETING STATES in STATEFUL MODEL **

FWD::
non_stateful: [7.375   3.6875  1.84375]
stateful: [1.2402344 0.6201172 0.3100586]
delta: [-6.1347656 -3.0673828 -1.5336914]
BWD::
non_stateful: [ 11.5 -25.   50. ]
stateful: [-0.6953125 -0.609375   1.21875  ]
delta: [-12.1953125  24.390625  -48.78125  ]
```

Crash when using 4/7 tf-nightly:

```
Traceback (most recent call last):
  File ""temp_bidi_state_in.py"", line 89, in <module>
    stateful_model.reset_states()
  File ""/home/keith/.pyenv/versions/tfn/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 473, in reset_states
    layer.reset_states()
  File ""/home/keith/.pyenv/versions/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 676, in reset_states
    self.forward_layer.reset_states()
  File ""/home/keith/.pyenv/versions/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 903, in reset_states
    spec_shape = nest.flatten(self.input_spec[0])[0].shape
AttributeError: 'NoneType' object has no attribute 'shape'
```

"
38402,Smart Reply AAR example for tensorflow lite won't build ,"I have not changed any code, and followed the procedure as per: https://github.com/tensorflow/examples/blob/master/lite/examples/smart_reply/android/how-to-build.md

I tried to build on Windows, and Linux (16.04) and I get the following error from bazel: 
Linux: 
amsha@amsha-linux:/local/mnt/workspace/workspace/tensorflowliteExamples/smartReply/examples/lite/examples/smart_reply/android/app$ bazel build libs/cc:smartreply_runtime_aar
WARNING: Output base '/usr2/amsha/.cache/bazel/_bazel_amsha/09e97388c3884f6fff92e89b26f572b5' is on NFS. This may lead to surprising failures and undetermined behavior.
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/316e6133888bfc39fb860a4f1a31cfcbae485aef.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
ERROR: /local/mnt/workspace/workspace/tensorflowliteExamples/smartReply/examples/lite/examples/smart_reply/android/app/libs/cc/BUILD:163:1: //libs/cc:smartreply_runtime_aar_dummy_app_for_so: no such attribute 'aapt_version' in 'android_binary' rule
ERROR: error loading package 'libs/cc': Package 'libs/cc' contains errors
INFO: Elapsed time: 0.401s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)

Windows:
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/316e6133888bfc39fb860a4f1a31cfcbae485aef.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
ERROR: C:/workspace/bazelbuilds/smart_reply/android/app/libs/cc/BUILD:163:1: //libs/cc:smartreply_runtime_aar_dummy_app_for_so: no such attribute 'aapt_version' in 'android_binary' rule
ERROR: error loading package 'libs/cc': Package 'libs/cc' contains errors
INFO: Elapsed time: 88.009s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded)

"
38400,[tf-2.2] XNNPack doesn't compile on AArch64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry Pi 4 or cross-compiling on Ubuntu 18.04 using #38399
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.2 branch 
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): Arm gcc 9.2.1

**Describe the problem**
Currently XNNPack on the TensorFlow 2.2 branch doesn't compile on AArch64. When trying to compile the TFLite benchmark binary for the Raspberry PI 4 64-bit, compilation fails with:
```
ERROR: /root/.cache/bazel/_bazel_root/ba9f8c20da37904a26a4b29ac8536dec/external/XNNPACK/BUILD.bazel:1643:1: C++ compilation of rule '@XNNPACK//:neonfp16arith_ukernels' failed (Exit 1)
external/XNNPACK/src/f16-spmm/gen/32x1-neonfp16arith-unroll2.c: In function 'xnn_f16_spmm_ukernel_32x1__neonfp16arith_unroll2':
external/XNNPACK/src/f16-spmm/gen/32x1-neonfp16arith-unroll2.c:219:59: error: incompatible type for argument 1 of 'vreinterpret_f32_f16'
  219 |             const float16x4_t va01 = vreinterpret_f32_f16(vld1_dup_f32(__builtin_assume_aligned(a, 1)));
      |                                                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      |                                                           |
      |                                                           float32x2_t
In file included from external/XNNPACK/src/f16-spmm/gen/32x1-neonfp16arith-unroll2.c:12:
/root/.cache/bazel/_bazel_root/ba9f8c20da37904a26a4b29ac8536dec/external/aarch64_compiler/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/include/arm_neon.h:4176:35: note: expected 'float16x4_t' but argument is of type 'float32x2_t'
 4176 | vreinterpret_f32_f16 (float16x4_t __a)
      |                       ~~~~~~~~~~~~^~~
external/XNNPACK/src/f16-spmm/gen/32x1-neonfp16arith-unroll2.c:227:76: error: incompatible type for argument 1 of 'vreinterpret_f16_f32'
  227 |         vst1_lane_f32(__builtin_assume_aligned(c, 1), vreinterpret_f16_f32(vout01), 0);
      |                                                                            ^~~~~~
      |                                                                            |
      |                                                                            float16x4_t
In file included from external/XNNPACK/src/f16-spmm/gen/32x1-neonfp16arith-unroll2.c:12:
/root/.cache/bazel/_bazel_root/ba9f8c20da37904a26a4b29ac8536dec/external/aarch64_compiler/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/include/arm_neon.h:4022:35: note: expected 'float32x2_t' but argument is of type 'float16x4_t'
 4022 | vreinterpret_f16_f32 (float32x2_t __a)
      |                       ~~~~~~~~~~~~^~~
Target //tensorflow/lite/tools/benchmark:benchmark_model failed to build
```
I believe this problem was fixed in https://github.com/google/XNNPACK/commit/d21fdcb4af35a8764333c3bb24fa54c55bb5db52 and tensorflow master compiles without a problem.

Since AArch64 was supported in TF 2.1, @Maratyszcza do you think it would be possible to backport the fixes to the 2.2 release branch?

It probably will be enough to cherry-pick 06ca7fc73ca96d9a468d771273be377ed4cc6ed2 and cd59b293b5b7f9f337ba37becdb850e1278ac0c0 in order to upgrade XNNPack, but not sure if this would have any side effects.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

This can be reproduced on a normal linux system by using the cross-compile toolchain from PR #38399 and compiling the TFLite benchmark binary using:
```
bazelisk build -c opt tensorflow/lite/tools/benchmark:benchmark_model \
    --cpu=aarch64 --crosstool_top=@local_config_arm_compiler//:toolchain
```"
38397,"In Tensorflow Lite, Instance norm takes a lot of time","**System information**
- OS Platform and Distribution: window10
- TensorFlow installed from (source or binary):binary
- TensorFlow version (or github SHA if from source):1.15.0


**Provide the text output from tflite_convert**
```
2020-04-09 23:44:06.327454: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-04-09 23:44:06.620150: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-04-09 23:44:06.631412: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-04-09 23:44:06.870610: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-04-09 23:44:06.879972: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 295 nodes (-88), 348 edges (-90), time = 80.397ms.
2020-04-09 23:44:06.889699: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 295 nodes (0), 348 edges (0), time = 26.95ms.
```

**Standalone code to reproduce the issue** 
I'm trying to run a [Fast Style Transfer](https://github.com/lengstrom/fast-style-transfer) model in my Android phone. I find when I use batchnorms in this network, it takes very little time (40ms), but when I use instance norms, it takes a lot of time (215ms). 

The Batch norm mentioned here is `tf.layers.batch_normalization()`. The CPU of my Android phone is snapdragon 845, and in TensorFlow Lite I use the GPU mode (GpuDelegate). 

[Image Transform network](https://github.com/lengstrom/fast-style-transfer/blob/master/src/transform.py):
```
def net(image):
    conv1 = _conv_layer(image, 32, 9, 1)
    conv2 = _conv_layer(conv1, 64, 3, 2)
    conv3 = _conv_layer(conv2, 128, 3, 2)
    resid1 = _residual_block(conv3, 3)
    resid2 = _residual_block(resid1, 3)
    resid3 = _residual_block(resid2, 3)
    resid4 = _residual_block(resid3, 3)
    resid5 = _residual_block(resid4, 3)
    conv_t1 = _conv_tranpose_layer(resid5, 64, 3, 2)
    conv_t2 = _conv_tranpose_layer(conv_t1, 32, 3, 2)
    conv_t3 = _conv_layer(conv_t2, 3, 9, 1, relu=False)
    preds = tf.add(tf.nn.tanh(conv_t3) * 150,255./2, name=""output"")
    return preds
```
Instance norm:
```
def _instance_norm(net, train=True):
    batch, rows, cols, channels = [i.value for i in net.get_shape()]
    var_shape = [channels]
    mu, sigma_sq = tf.nn.moments(net, [1, 2], keep_dims=True)
    shift = tf.Variable(tf.zeros(var_shape))
    scale = tf.Variable(tf.ones(var_shape))
    epsilon = 1e-3
    normalized = (net-mu)*tf.rsqrt((sigma_sq + epsilon))
    return scale * normalized + shift
```
I used the following version of TensorFlow Lite:
```
implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'
implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'
```

**Any other info / logs**
When I use the Instance norm, the model requires an average of 215ms in CPU mode and 205ms in GPU mode (stylize a image of 128*128 pixels). I'm sure the GPU is working, but it seems like the Instance norm is running on the CPU, so there's no obvious time decrease. How can I improve the speed of the instance norm in TensorFlow Lite？
"
38396,Keras `model_from_json` ignores distribution strategy,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: N/A
- TensorFlow installed from (source or
binary): Binary
 - TensorFlow version (use command below): 
- Python version: 2.0.1
 - Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from
source):  N/A
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla V100 (affects multiple sizes)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When loading a model using `tf.keras.models.model_from_json`, TensorFlow ignores the distribution strategy.  If you try to train or predict with such a loaded model with a `MirroredStrategy`, for example, it will still only use a single GPU.

**Describe the expected behavior**

`tf.keras.models.model_form_json` should use the provided strategy scope and distribute accordingly.

**Standalone code to reproduce the issue** 

First we'll make a silly model, here with a strategy and verify it uses it properly.  How we create it here, however, is irrelevant to the strategy under which we load it later.

```python
import tensorflow as tf
import numpy as np

## silly model of sufficient size to notice spike in memory
with tf.distribute.MirroredStrategy().scope():
    m = tf.keras.models.Sequential()
    m.add(tf.keras.layers.Conv2D(256,5, input_shape=(256,256,1)))
    m.add(tf.keras.layers.Conv2D(256,5))
    m.add(tf.keras.layers.Conv2D(256,5))
    m.add(tf.keras.layers.Conv2D(256,5))
    m.add(tf.keras.layers.Conv2D(256,5))
    m.add(tf.keras.layers.Conv2D(256,5))
    m.add(tf.keras.layers.Conv2D(256,5))
    m.compile(optimizer='sgd', loss='mse')

## save model to json
with open('foo_model.json','w') as f:
    print(m.to_json(), file=f)

## test that we're using multiple GPUs
data = np.random.random(size=(1024,256,256,1))
data = np.array(data,dtype=np.float32)

ans = m.predict(data) # observe multiple GPUs spike in memory and utilization
```

Now, we load the saved model, using MirroredStrategy, but we find that TensorFlow ignores this strategy.  It seems to load the models initially onto all available GPUs (taking up a nominal amount of memory), but then only does computations on a single GPU.  The data never makes it to multiple GPUs, slowing down computations and causing memory problems.

```python
import tensorflow as tf
import numpy as np

data = np.random.random(size=(1024,256,256,1))
data = np.array(data,dtype=np.float32)

with open('foo_model.json','r') as f:
    J = f.read()

# use a strategy, see it be ignored
with tf.distribute.MirroredStrategy().scope():
    m = tf.keras.models.model_from_json(J) # see modest spike in memory as seems to load onto multiple gpu

ans = m.predict(data) # only a single GPU receives data
```

I understand that loading from a TF-saved model is supposed to work.  Because of other TensorFlow bugs, however, I have to save my models in the portable JSON format.  I believe this more or less rebuilds the whole model and reloads weights.  That should mean that the strategy gets applied properly, but we see that's not the case.  Is `model_from_json` overwriting or otherwise ignoring the strategy somewhere?

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
38395,Generalize compiling libtensorflow-lite.a for arm32,"**System information**
- TensorFlow version (you are using):N/A
- Are you willing to contribute it (Yes/No): Yes (maybe if I can figure it out haha)

**Describe the feature and the current behavior/state.**
Right now cross compiling `libtensorflow-lite.a` will be either for [arm64](https://www.tensorflow.org/lite/guide/build_arm64) (which works great, thanks) and [rpi](https://www.tensorflow.org/lite/guide/build_rpi). I understand the build for rpi will mostly be for arm32 bits, why not make that more generalized for any arm32 system?
Also, with rpi4 in the market, we are running into issues where processor can support armv8 (64 bits) but the official OS is yet to support 64 bits. What happens when users start installing 64bits OS and realized that their rpi build isn't working?

**Will this change the current api? How?**
No changes to the api at all as far as I'm concern, the tutorial may just be more generalized as a user.

**Who will benefit with this feature?**
Anybody with a 32bits arm machine :)

Please correct me if I understand this incorrectly or if this issue has been raised before!
Thanks
"
38394,how to know the GPU's num and specify with tf ?,"hi,dear
haven't seen the methed of tf codes,
So I just want to solve the problem.
Could you please help me ?

thx"
38393,[C++] set_visible_device_list raise RegisterAlreadyLocked when creating a session,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): ubuntu 18.04
- TensorFlow installed from (source or
binary): source
- TensorFlow version (use command below): 1.15.2 or 2.2.0
- GCC/Compiler version (if compiling from
source): 7.5.0
- CUDA/cuDNN version: cuda 10.0 or 10.1
- GPU model and memory: Nvidia 1080 ti

**Describe the current behavior**

The function NewSession raise an exception when setting `set_visible_device_list`:
```
2020-04-09 14:06:36.112937: F tensorflow/core/framework/op.cc:214] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: No attr with name '0' for input 'constants'; in OpDef: name: ""XlaLaunch"" input_arg { name: ""constants"" description: ""0"" type_attr: ""0"" number_attr: ""0"" type_list_attr: ""Tconstants"" } input_arg { name: ""args"" description: ""0"" type_attr: ""0"" number_attr: ""0"" type_list_attr: ""Targs"" } input_arg { name: ""resources"" description: ""0"" type: DT_RESOURCE type_attr: ""0"" number_attr: ""Nresources"" type_list_attr: ""0"" } output_arg { name: ""results"" description: ""0"" type_attr: ""0"" number_attr: ""0"" type_list_attr: ""Tresults"" } attr { name: ""Tconstants"" type: ""list(type)"" description: ""0"" has_minimum: true } attr { name: ""Targs"" type: ""list(type)"" description: ""0"" has_minimum: true } attr { name: ""Nresources"" type: ""int"" description: ""0"" has_minimum: true } attr { name: ""Tresults"" type: ""list(type)"" description: ""0"" has_minimum: true } attr { name: ""function"" type: ""func"" description: ""0"" } summary: ""XLA Launch Op. For use by the XLA JIT only."" description: ""0"" is_stateful: true
Aborted (core dumped)
```

**Describe the expected behavior**

This should not raise. It was working in tf 1.13.2. But since 1.15.2 and 2.2.0 it doesn't work.

**Standalone code to reproduce the issue** 

```c++
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/public/session_options.h""

int main()
{
    tensorflow::SessionOptions session_options;
    session_options.config.mutable_gpu_options()->set_visible_device_list(""0"");

    auto session = tensorflow::NewSession(session_options);
}
```

You can use `floopcz/tensorflow_cc:ubuntu-cuda-2.2.0` (you will need to install protobuf.so it seems) and compile with 
```
g++ new_session.cpp -I/usr/local/include/tensorflow/bazel-bin/tensorflow/include -I/usr/local/include/eigen3 -I/usr/local/include/tensorflow/bazel-bin/tensorflow/include/src/ -ltensorflow_cc -lprotobuf
```

I can't thus load a saved model using `LoadSavedModel` (which call `NewSession`) on a specific gpu."
38392,Quantization give float32 weights(instead of int8)  on TF 2.2.0-rc2,"**System information**
- Google colab
- TF version: 2.2.0-rc2

**Problem summary**
Running the `hello_world` application for TFLite for Microcontrolllers. A model is generated successfully. The model is supposed to be quantized but the weights of all layers are `float32` and not `int8`. I am unable to generate a truly quantized model with `int8` weights. 
Getting the same problem in TF 2.1.0

**Command used to run the converter or code if you’re using the Python API**

```
https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/create_sine_model.ipynb
```


**Also, please include a link to the saved model or GraphDef**

```
http://s000.tinyupload.com/?file_id=25815880594812659996
```

**Failure details**
The mode is supposed to be quantized. However, the weights of the model are `float32`
How can I obtain a truly quantized model? 

"
38391,[TF 2.1] Jacobian / Hessian with Conv1D layers,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Can only compute 2nd derivatives (Jacobian of Gradient) with convolutional layers (Dense works fine) when experimental_use_pfor is set to False, while for Dense layers this is not necessary and seems to be faster (hard to compare though). Here is an executable code building a model and computing Hessian as Jacobian of Gradient as discussed here [[TF 2.0] tf.hessians #29781](https://github.com/tensorflow/tensorflow/issues/29781#issuecomment-611312435)

**Executable example**

```
import tensorflow as tf
print(tf.__version__)

loss_object = tf.keras.losses.MeanSquaredError()
def loss_function(real, pred):
    return tf.reduce_mean(loss_object(real, pred))

x_train = tf.random.normal((10,20,1))

inp = tf.keras.layers.Input(shape=x_train.shape[1:])
x1 = tf.keras.layers.Conv1D(32, 3, activation=""relu"", padding='same')(inp)
x1 = tf.keras.layers.Conv1D(1,3,activation=""relu"",padding=""same"")(x1)
cnn = tf.keras.models.Model(inp, x1)
cnn.compile(loss=""mse"",optimizer=""sgd"")

# hessian computation via jacobian
with tf.GradientTape(persistent=True) as tape:
    prediction = cnn(x_train)
    loss = loss_function(x_train,prediction)
    grads = tape.gradient(loss, cnn.trainable_variables)

hessians = [tape.jacobian(grad, cnn.trainable_variables) for grad in grads]
del tape
```

produces 

```
2.1.0
WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.
ERROR:tensorflow:Got error while pfor was converting op name: ""loop_body/Conv2D""
op: ""Conv2D""
input: ""loop_body/Shape_3/17537""
input: ""loop_body/Reshape_3""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""data_format""
  value {
    s: ""NHWC""
  }
}
attr {
  key: ""dilations""
  value {
    list {
      i: 1
      i: 1
      i: 1
      i: 1
    }
  }
}
attr {
  key: ""explicit_paddings""
  value {
    list {
    }
  }
}
attr {
  key: ""padding""
  value {
    s: ""SAME""
  }
}
attr {
  key: ""strides""
  value {
    list {
      i: 1
      i: 1
      i: 1
      i: 1
    }
  }
}
attr {
  key: ""use_cudnn_on_gpu""
  value {
    b: true
  }
}
with inputs (<tf.Tensor 'loop_body/Shape_3/17537:0' shape=(10, 1, 20, 1) dtype=float32>, <tf.Tensor 'loop_body/Reshape_3:0' shape=(1, 3, 1, 32) dtype=float32>)
, converted inputs [WrappedTensor(t=<tf.Tensor 'loop_body/Conv2D/pfor/Tile:0' shape=(96, 10, 1, 20, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/Reshape_3/pfor/Reshape:0' shape=(96, 1, 3, 1, 32) dtype=float32>, is_stacked=True, is_sparse_stacked=False)]
Input ""filter"" of op ""Conv2D"" expected to be loop invariant
Here are the pfor conversion stack traces:
ERROR:tensorflow:name: ""loop_body/Conv2D""
op: ""Conv2D""
input: ""loop_body/Shape_3/17537""
input: ""loop_body/Reshape_3""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""data_format""
  value {
    s: ""NHWC""
  }
}
attr {
  key: ""dilations""
  value {
    list {
      i: 1
      i: 1
      i: 1
      i: 1
    }
  }
}
attr {
  key: ""explicit_paddings""
  value {
    list {
    }
  }
}
attr {
  key: ""padding""
  value {
    s: ""SAME""
  }
}
attr {
  key: ""strides""
  value {
    list {
      i: 1
      i: 1
      i: 1
      i: 1
    }
  }
}
attr {
  key: ""use_cudnn_on_gpu""
  value {
    b: true
  }
}

created at:
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/traitlets/config/application.py"", line 664, in launch_instance
    app.start()
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/kernelapp.py"", line 583, in start
    self.io_loop.start()
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/platform/asyncio.py"", line 153, in start
    self.asyncio_loop.run_forever()
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/asyncio/base_events.py"", line 538, in run_forever
    self._run_once()
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/asyncio/base_events.py"", line 1782, in _run_once
    handle._run()
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/asyncio/events.py"", line 88, in _run
    self._context.run(self._callback, *self._args)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/ioloop.py"", line 690, in <lambda>
    lambda f: self._run_callback(functools.partial(callback, future))
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/ioloop.py"", line 743, in _run_callback
    ret = callback()
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/gen.py"", line 787, in inner
    self.run()
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/gen.py"", line 748, in run
    yielded = self.gen.send(value)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/kernelbase.py"", line 361, in process_one
    yield gen.maybe_future(dispatch(*args))
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/kernelbase.py"", line 541, in execute_request
    user_expressions, allow_stdin,
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/ipkernel.py"", line 300, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/zmqshell.py"", line 536, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 2858, in run_cell
    raw_cell, store_history, silent, shell_futures)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 2886, in _run_cell
    return runner(coro)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
    coro.send(None)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3063, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3254, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-32-9fcc71f67cfa>"", line 22, in <module>
    hessians = [tape.jacobian(grad, cnn.trainable_variables) for grad in grads]
    File ""<ipython-input-32-9fcc71f67cfa>"", line 22, in <listcomp>
    hessians = [tape.jacobian(grad, cnn.trainable_variables) for grad in grads]
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py"", line 1113, in jacobian
    parallel_iterations=parallel_iterations)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py"", line 189, in pfor
    return f()
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2362, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 964, in wrapper
    user_requested=True,
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py"", line 183, in f
    return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py"", line 224, in _pfor_impl
    loop_fn_outputs = loop_fn(loop_var)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py"", line 1103, in loop_fn
    unconnected_gradients=unconnected_gradients)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py"", line 1029, in gradient
    unconnected_gradients=unconnected_gradients)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py"", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py"", line 141, in _gradient_function
    return grad_fn(mock_op, *out_grads)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_grad.py"", line 92, in _Conv2DBackpropFilterGrad
    data_format=op.get_attr(""data_format"").decode())
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py"", line 969, in conv2d
    data_format=data_format, dilations=dilations, name=name)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 742, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 595, in _create_op_internal
    compute_device)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3322, in _create_op_internal
    op_def=op_def)
    File ""/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1756, in __init__
    self._traceback = tf_stack.extract_stack()

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)
   1112         output = pfor_ops.pfor(loop_fn, target_size,
-> 1113                                parallel_iterations=parallel_iterations)
   1114       except ValueError as err:

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, parallel_iterations)
    188     f = function.defun(f)
--> 189   return f()
    190 

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   2361     with self._lock:
-> 2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2702       self._function_cache.missed.add(call_context_key)
-> 2703       graph_function = self._create_graph_function(args, kwargs)
   2704       self._function_cache.primary[cache_key] = graph_function

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2592             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2593             capture_by_value=self._capture_by_value),
   2594         self._function_attributes,

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    977 
--> 978       func_outputs = python_func(*func_args, **func_kwargs)
    979 

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
    967             if hasattr(e, ""ag_error_metadata""):
--> 968               raise e.ag_error_metadata.to_exception(e)
    969             else:

ValueError: in converted code:

    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:183 f  *
        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:256 _pfor_impl
        outputs.append(converter.convert(loop_fn_output))
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1280 convert
        output = self._convert_helper(y)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1482 _convert_helper
        six.reraise(e.__class__, e, sys.exc_info()[2])
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/six.py:703 reraise
        raise value
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1466 _convert_helper
        new_outputs = converter(pfor_inputs)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:902 _f
        return converter(pfor_input, self.op_type, *self._args, **self._kw_args)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1592 _convert_flatten_batch
        inputs = _inputs_with_flattening(pfor_input, dims)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1576 _inputs_with_flattening
        inp = pfor_input.unstacked_input(i)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:776 unstacked_input
        input_name, op_type))

    ValueError: Input ""filter"" of op ""Conv2D"" expected to be loop invariant


During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-32-9fcc71f67cfa> in <module>
     20     grads = tape.gradient(loss, cnn.trainable_variables)
     21 
---> 22 hessians = [tape.jacobian(grad, cnn.trainable_variables) for grad in grads]
     23 del tape

<ipython-input-32-9fcc71f67cfa> in <listcomp>(.0)
     20     grads = tape.gradient(loss, cnn.trainable_variables)
     21 
---> 22 hessians = [tape.jacobian(grad, cnn.trainable_variables) for grad in grads]
     23 del tape

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)
   1119                 ""jacobian computation. Vectorization can be disabled by setting""
   1120                 "" experimental_use_pfor to False.""),
-> 1121             sys.exc_info()[2])
   1122     else:
   1123       if context.executing_eagerly() and not self._persistent:

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/six.py in reraise(tp, value, tb)
    700                 value = tp()
    701             if value.__traceback__ is not tb:
--> 702                 raise value.with_traceback(tb)
    703             raise value
    704         finally:

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)
   1111       try:
   1112         output = pfor_ops.pfor(loop_fn, target_size,
-> 1113                                parallel_iterations=parallel_iterations)
   1114       except ValueError as err:
   1115         six.reraise(

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, parallel_iterations)
    187   if context.executing_eagerly() or _is_under_xla_context():
    188     f = function.defun(f)
--> 189   return f()
    190 
    191 

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   2360     """"""Calls a graph function specialized to the inputs.""""""
   2361     with self._lock:
-> 2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2364 

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2701 
   2702       self._function_cache.missed.add(call_context_key)
-> 2703       graph_function = self._create_graph_function(args, kwargs)
   2704       self._function_cache.primary[cache_key] = graph_function
   2705       return graph_function, args, kwargs

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2591             arg_names=arg_names,
   2592             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2593             capture_by_value=self._capture_by_value),
   2594         self._function_attributes,
   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    976                                           converted_func)
    977 
--> 978       func_outputs = python_func(*func_args, **func_kwargs)
    979 
    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--> 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

ValueError: in converted code:

    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:183 f  *
        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:256 _pfor_impl
        outputs.append(converter.convert(loop_fn_output))
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1280 convert
        output = self._convert_helper(y)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1482 _convert_helper
        six.reraise(e.__class__, e, sys.exc_info()[2])
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/six.py:703 reraise
        raise value
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1466 _convert_helper
        new_outputs = converter(pfor_inputs)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:902 _f
        return converter(pfor_input, self.op_type, *self._args, **self._kw_args)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1592 _convert_flatten_batch
        inputs = _inputs_with_flattening(pfor_input, dims)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1576 _inputs_with_flattening
        inp = pfor_input.unstacked_input(i)
    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:776 unstacked_input
        input_name, op_type))

    ValueError: Input ""filter"" of op ""Conv2D"" expected to be loop invariant

Encountered an exception while vectorizing the jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.
```
"
38390,tflm vc compilation error and how to fix,"in file tensorflow\lite\micro\kernels\svdf.cc
there is __restrict__ keword, so it can't be compiled with VC.
I added 

```
#ifdef _MSC_VER
	#define RESTRICT __restrict
#else
	#define RESTRICT __restrict__
#endif
```

and replaced __restrict__  with RESTRICT  - now compiles without problems"
38389,"Wrong x_lerp computation in ""ResizeBilinearKernel_faster"" and ""ResizeBilinearKernel""","This is a bug on compute the **x_lerp** in CUDA version of resize_bilinear. See the code in 
[ResizeBilinearKernel_faster](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc#L62) and [ResizeBilinearKernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc#L141). 

The computation of **x_lerp** is different from **y_lerp** in cuda version, which should regarded the symmetric operation on different axis.  When computing the grad, the **x_lerp** has been computed correctly, see code [ResizeBilinearGradKernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc#L199)

Besides, its also different from the cpu version of resize_bilinear, see code in [ResizeGradCore](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op.cc#L339).

**System information** 
Ubuntu 18.04
standard TF 2.1 (installed from pip)

**Describe the current behavior**
`const float x_lerp = in_x - left_x_index;`

**Describe the expected behavior**
`const float x_lerp = in_x - floorf(in_x);`

The bug comes from the incorrect computation of x_lerp (as described above). And the code link is also listed above (The links on ResizeBilinearKernel and ResizeBilinearKernel_faster)

**Standalone code to reproduce the issue**
The bug comes from the CUDA code, which is found by reading the code myself."
38388,TFLite New Converter with 1.X frozen graph,"I tried to covert 1.X frozen graph with TFLite New Converter following this guidelines(https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt).

Here is my full python script.
```python
graph_def = tf.compat.v1.GraphDef()
graph_def.ParseFromString(open(flags.input_path, 'rb').read())

wrap_func = wrap_frozen_graph(
    graph_def,
    inputs=[_str + "":0"" for _str in _parse_array(flags.input_arrays)],
    outputs=[_str + "":0"" for _str in _parse_array(flags.output_arrays)])
converter = tf.lite.TFLiteConverter.from_concrete_functions([wrap_func])
```
It works well. But, is there any method to set input shapes from users? It's very hard to find this:(

like `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph` method in 1.X version.
```python
def from_frozen_graph(cls,
                        graph_def_file,
                        input_arrays,
                        output_arrays,
                        input_shapes=None)
```

I found something similar like
```python
# Set the correct data type and shape; shape can be (None, 224, 224, 3) also
new_placeholder = tf.placeholder(tf.float32, shape=(1, 224, 224, 3), name='inputs_new_name') 
# here you need to state the name of the placeholder you used in your original input placeholder  

saver = tf.import_graph_def(path/to/.meta, input_map={""original_inputs_placeholder_name:0"": new_placeholder})
```
But, this is very inconvenient to use.

I'm looking for similar features in 2.X converter, but I haven't found any:(
"
38386,Distributed training causes cuda OOM,"**System information** 
- Have I written custom code: Yes
- OS Platform and Distribution: CentOS 
- TensorFlow installed from:
`pip3 install tensorflow-gpu==2.0.0` 
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0/7.6

**Describe the current behavior**
When starting the worker using v1 distirbuted training, we could observe the following error:
```bash
WARNING:tensorflow:From /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0409 18:55:45.554535 140176829970240 deprecation.py:323] From /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
2020-04-09 18:55:45.563568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:0d:00.0
2020-04-09 18:55:45.563627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-04-09 18:55:45.563644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-04-09 18:55:45.563658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-04-09 18:55:45.563671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-04-09 18:55:45.563687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-04-09 18:55:45.563702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-04-09 18:55:45.563716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-09 18:55:45.568025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
INFO:tensorflow:Graph was finalized.
I0409 18:55:45.699084 140176829970240 monitored_session.py:240] Graph was finalized.
2020-04-09 18:55:47.161048: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 22.40G (24056830208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.162735: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 20.16G (21651146752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.164113: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 18.15G (19486031872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.165473: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 16.33G (17537427456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.166829: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 14.70G (15783684096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.168178: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 13.23G (14205315072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.169530: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 11.91G (12784783360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.170874: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 10.72G (11506305024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.172225: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 9.64G (10355674112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.173575: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 8.68G (9320105984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.174945: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 7.81G (8388094976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.176291: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 7.03G (7549285376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.177639: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 6.33G (6794356736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.178980: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 5.69G (6114920960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.180325: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 5.12G (5503428608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.181714: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 4.61G (4953085440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.183053: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 4.15G (4457776640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.184402: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.74G (4011998976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.185768: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.36G (3610799104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.187108: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.03G (3249719040 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.188455: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 2.72G (2924747008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.189805: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 2.45G (2632272128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.191145: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 2.21G (2369044736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.192492: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.99G (2132140288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.193864: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.79G (1918926336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.195203: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.61G (1727033600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.196724: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.45G (1554330368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.198186: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.30G (1398897408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.199649: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.17G (1259007744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.201138: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.05G (1133106944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.202527: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 972.55M (1019796224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
INFO:tensorflow:Running local_init_op.
I0409 18:55:47.254083 140176829970240 session_manager.py:500] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I0409 18:55:47.262292 140176829970240 session_manager.py:502] Done running local_init_op.
2020-04-09 18:55:51.072444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-04-09 18:55:51.096914: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.110052: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.122872: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.135759: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.148557: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.161419: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.174027: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.187117: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.262033: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.262068: W tensorflow/stream_executor/stream.cc:1919] attempting to perform BLAS operation using StreamExecutor without BLAS support
Traceback (most recent call last):
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: From /job:worker/replica:0/task:0:
Blas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 500), m=32, n=500, k=784
         [[{{node dense/MatMul}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""distributed_mnist.py"", line 123, in <module>
    tf.app.run()
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""distributed_mnist.py"", line 116, in main
    _, ls, step = mon_sess.run([train_op, loss, global_step])#,
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py"", line 756, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py"", line 1261, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py"", line 1362, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python3.6/site-packages/six.py"", line 703, in reraise
    raise value
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py"", line 1347, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py"", line 1420, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py"", line 1178, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: From /job:worker/replica:0/task:0:
Blas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 500), m=32, n=500, k=784
         [[node dense/MatMul (defined at /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]

Original stack trace for 'dense/MatMul':
  File ""distributed_mnist.py"", line 123, in <module>
    tf.app.run()
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""distributed_mnist.py"", line 82, in main
    logits = model(images)
  File ""distributed_mnist.py"", line 28, in model
    net = tf.layers.dense(images, 500, activation=tf.nn.relu)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/layers/core.py"", line 187, in dense
    return layer.apply(inputs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 234, in wrapper
    return converted_call(f, options, args, kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted
    return f(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py"", line 1056, in call
    outputs = gen_math_ops.mat_mul(inputs, self.kernel)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 6136, in mat_mul
    name=name)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 793, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 3360, in create_op
    attrs, op_def, compute_device)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 3429, in _create_op_internal
    op_def=op_def)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 1751, in __init__
    self._traceback = tf_stack.extract_stack()
```
We've tried with different task number but observed the same problem.

**Standalone code to reproduce the issue** 
```python
import tensorflow.compat.v1 as tf
tf.disable_eager_execution()

mnist = tf.keras.datasets.mnist

tf.app.flags.DEFINE_string(""ps_hosts"", ""localhost:2222"", ""ps hosts"")
tf.app.flags.DEFINE_string(""worker_hosts"", ""localhost:2223,localhost:2224"", ""worker hosts"")
tf.app.flags.DEFINE_string(""job_name"", ""worker"", ""'ps' or'worker'"")
tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
tf.app.flags.DEFINE_integer(""num_workers"", 2, ""Number of workers"")
tf.app.flags.DEFINE_boolean(""is_sync"", False, ""using synchronous training or not"")

FLAGS = tf.app.flags.FLAGS

def model(images):
    """"""Define a simple mnist classifier""""""
    net = tf.layers.dense(images, 500, activation=tf.nn.relu)
    net = tf.layers.dense(net, 500, activation=tf.nn.relu)
    net = tf.layers.dense(net, 10, activation=None)
    return net

def main(_):
    ps_hosts = FLAGS.ps_hosts.split("","")
    worker_hosts = FLAGS.worker_hosts.split("","")

    # create the cluster configured by `ps_hosts' and 'worker_hosts'
    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})

    # create a server for local task
    server = tf.train.Server(cluster, job_name=FLAGS.job_name,
                             task_index=FLAGS.task_index)

    if FLAGS.job_name == ""ps"":
        server.join()  # ps hosts only join
    elif FLAGS.job_name == ""worker"":
        # workers perform the operation
        # ps_strategy = tf.contrib.training.GreedyLoadBalancingStrategy(FLAGS.num_ps)

        # Note: tf.train.replica_device_setter automatically place the paramters (Variables)
        # on the ps hosts (default placement strategy:  round-robin over all ps hosts, and also
        # place multi copies of operations to each worker host
        with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % (FLAGS.task_index),
                                                      cluster=cluster)):
            # load mnist dataset
            (x_train, y_train), (x_test, y_test) = mnist.load_data()
            x_train, x_test = x_train / 255.0, x_test / 255.0

            x_train = x_train.reshape(x_train.shape[0], -1)
            x_test = x_test.reshape(x_test.shape[0], -1)

            train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)
            train_iter = train_ds.make_initializable_iterator()
            train_init = train_iter.make_initializer(train_ds)
            test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)

            print(x_train.shape)
            print(y_train.shape)

            # the model
            #images = tf.placeholder(tf.float32, [None, 784])
            #labels = tf.placeholder(tf.int32, [None, 10])
            xs, ys = train_iter.get_next()
            xs = tf.cast(xs, dtype=tf.float32)
            ys = tf.cast(ys, dtype=tf.int32)
            images = xs
            labels = tf.one_hot(ys, 10)

            logits = model(images)
            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))

            # The StopAtStepHook handles stopping after running given steps.
            hooks = [tf.train.StopAtStepHook(last_step=2000)]

            global_step = tf.train.get_or_create_global_step()
            optimizer = tf.train.AdamOptimizer(learning_rate=1e-04)

            if FLAGS.is_sync:
                # synchronous training
                # use tf.train.SyncReplicasOptimizer wrap optimizer
                # ref: https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer
                optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=FLAGS.num_workers,
                                                       total_num_replicas=FLAGS.num_workers)
                # create the hook which handles initialization and queues
                hooks.append(optimizer.make_session_run_hook((FLAGS.task_index==0)))

            train_op = optimizer.minimize(loss, global_step=global_step,
                                          aggregation_method=tf.AggregationMethod.ADD_N)

            # The MonitoredTrainingSession takes care of session initialization,
            # restoring from a checkpoint, saving to a checkpoint, and closing when done
            # or an error occurs.
            with tf.train.MonitoredTrainingSession(master=server.target,
                                                   is_chief=(FLAGS.task_index == 0),
                                                   #checkpoint_dir=""./checkpoint_dir"",
                                                   hooks=hooks) as mon_sess:
                mon_sess.run(train_init)
                while not mon_sess.should_stop():
                    # mon_sess.run handles AbortedError in case of preempted PS.
                    _, ls, step = mon_sess.run([train_op, loss, global_step])
                    if step % 100 == 0:
                        print(""Train step %d, loss: %f"" % (step, ls))

if __name__ == ""__main__"":
    tf.app.run()
```
The bash used to run this code (this is a one worker one ps example, we've varied the number of tasks and observed the similar behavior.)
```bash
python3 distributed_mnist.py --ps_hosts=localhost:2222 --worker_hosts=localhost:2224 --job_name=ps --task_index=0 --num_worker=1
python3 distributed_mnist.py --ps_hosts=localhost:2222 --worker_hosts=localhost:2224 --job_name=worker --task_index=0 --num_worker=1
```
"
38385,"docker image to build manylinux, ubuntu16","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04

**Describe the problem**
I am building tensorflow pip wheel from source and I would like to build it for Ubuntu 16.04.
I found the docker image with ubuntu16 and installed auditwheel that looks like a fit for my purpose:
tensorflow:custom-op-ubuntu16,
but it is very heavy - 6Gb.

Could you please recommend which tensorflow docker image should I use to build manylinux wheel ?
Is there an easy way to build tensorflow pip package for Ubuntu 16.04 from source ? 


"
38384,Cannot save to SavedModel when custom layers accepts args and kwargs,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes and no
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or
binary): pip
- TensorFlow version (use command below): 2.2.0rc2
- Python version: 3.7.3

**Describe the current behavior**
I was having trouble with saving a custom tf.keras model to the SavedModel format, while saving to .h5 would go fine. The model is a custom ResNet model, subclassing tf.keras.Model.
https://stackoverflow.com/a/55386355/11470044 and https://www.tensorflow.org/guide/keras/save_and_serialize#part_ii_saving_and_loading_of_subclassed_models have led me to believe that it should now be possible to save such custom imperative models.
After some digging around, I've found the issue to be the following: A custom keras layer's `call()` method may not be defined like `def call(self, *args, **kwargs)`, but should strictly adhere to `def call(self, inputs, **kwargs)`.

I can show with the following [gist](https://gist.github.com/kriskorrel-cw/c90e349fddec1c2a20a031660431f80a), which is almost entirely the same as in https://www.tensorflow.org/guide/keras/save_and_serialize#part_ii_saving_and_loading_of_subclassed_models 
Using `BatchNormalization` or `BatchNormalization1`, all goes well. But when using `BatchNormalization2`, we get the following stack trace:
```
Traceback (most recent call last):
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1008, in save
    signatures, options)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 115, in save_model
    signatures, options)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py"", line 78, in save
    save_lib.save(model, filepath, signatures, options)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 886, in save
    checkpoint_graph_view)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_serialization.py"", line 74, in find_function_to_export
    functions = saveable_view.list_functions(saveable_view.root)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 142, in list_functions
    self._serialization_cache)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2420, in _list_functions_for_serialization
    .list_functions_for_serialization(serialization_cache))
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py"", line 91, in list_functions_for_serialization
    fns = self.functions_to_serialize(serialization_cache)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py"", line 80, in functions_to_serialize
    serialization_cache).functions_to_serialize)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py"", line 95, in _get_serialized_attributes
    serialization_cache)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py"", line 53, in _get_serialized_attributes_internal
    serialization_cache))
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py"", line 104, in _get_serialized_attributes_internal
    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py"", line 162, in wrap_layer_functions
    original_fns = _replace_child_layer_functions(layer, serialization_cache)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py"", line 258, in _replace_child_layer_functions
    serialization_cache).functions)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py"", line 95, in _get_serialized_attributes
    serialization_cache)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py"", line 104, in _get_serialized_attributes_internal
    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py"", line 172, in wrap_layer_functions
    '{}_layer_call_and_return_conditional_losses'.format(layer.name))
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py"", line 513, in add_function
    self.add_trace(*self._input_signature)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py"", line 428, in add_trace
    trace_with_training(True)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py"", line 426, in trace_with_training
    fn.get_concrete_function(*args, **kwargs)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py"", line 557, in get_concrete_function
    return super(LayerCall, self).get_concrete_function(*args, **kwargs)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 909, in get_concrete_function
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 497, in _initialize
    *args, **kwds))
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py"", line 523, in wrapper
    inputs = call_collection.get_input_arg_value(args, kwargs)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py"", line 463, in get_input_arg_value
    self._input_arg_name, args, kwargs, inputs_in_args=True)
  File ""/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1991, in _get_call_arg_value
    return args_dict[arg_name]
KeyError: 'inputs'
```

Although this is somewhat easy to debug in the toy example, you could imagine that such an exception is hard to find in a large codebase.

**Describe the expected behavior**
I therefore think that either (a) using the `*args, **kwargs` signature should be supported, (b) a warning/exception should be raised whenever a `Model` is defined or compiled with incorrect signatures, or (c) the error message above must be improved such that a similar issue can be tracked down more easily.

**Standalone code to reproduce the issue** 
[gist](https://gist.github.com/kriskorrel-cw/c90e349fddec1c2a20a031660431f80a)

**Other info / logs**
P.S. I first tried to reproduce the issue by subclassing `Dense` instead of `BatchNormalization`, but this does not work since `Dense.call()` only accepts `inputs`. It does not support kwargs (like 'training'). I also find this inconsistency between different Layers weird, but I'm sure there's a reason for it.
"
38383,TFX on Taxi_pipelines_interactive,"model_resolver = ResolverNode(
      instance_name='latest_blessed_model_resolver',
      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(type=ModelBlessing))
context.run(model_resolver)

evaluator = Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    #baseline_model=model_resolver.outputs['model'],
    # Change threshold will be ignored if there is no baseline (first run).
    eval_config=eval_config)
context.run(evaluator)

INFO:absl:Running driver for ResolverNode.latest_blessed_model_resolver
INFO:absl:MetadataStore with DB connection initialized
WARNING:absl:Artifact type ModelBlessing not registered
INFO:absl:Running publisher for ResolverNode.latest_blessed_model_resolver
INFO:absl:MetadataStore with DB connection initialized
INFO:absl:Running driver for Evaluator
INFO:absl:MetadataStore with DB connection initialized
INFO:absl:Running executor for Evaluator
---------------------------------------------------------------------------
UnboundLocalError                         Traceback (most recent call last)
<ipython-input-44-205b472f5776> in <module>
     19     # Change threshold will be ignored if there is no baseline (first run).
     20     eval_config=eval_config)
---> 21 context.run(evaluator)

~/taxi_pipeline/lib/python3.6/site-packages/tfx/orchestration/experimental/interactive/interactive_context.py in run_if_ipython(*args, **kwargs)
     64       # __IPYTHON__ variable is set by IPython, see
     65       # https://ipython.org/ipython-doc/rel-0.10.2/html/interactive/reference.html#embedding-ipython.
---> 66       return fn(*args, **kwargs)
     67     else:
     68       absl.logging.warning(

~/taxi_pipeline/lib/python3.6/site-packages/tfx/orchestration/experimental/interactive/interactive_context.py in run(self, component, enable_cache, beam_pipeline_args)
    166         component, pipeline_info, driver_args, metadata_connection,
    167         beam_pipeline_args, additional_pipeline_args)
--> 168     execution_id = launcher.launch().execution_id
    169 
    170     return execution_result.ExecutionResult(

~/taxi_pipeline/lib/python3.6/site-packages/tfx/orchestration/launcher/base_component_launcher.py in launch(self)
    203                          execution_decision.input_dict,
    204                          execution_decision.output_dict,
--> 205                          execution_decision.exec_properties)
    206 
    207     absl.logging.info('Running publisher for %s',

~/taxi_pipeline/lib/python3.6/site-packages/tfx/orchestration/launcher/in_process_component_launcher.py in _run_executor(self, execution_id, input_dict, output_dict, exec_properties)
     65         executor_context)  # type: ignore
     66 
---> 67     executor.Do(input_dict, output_dict, exec_properties)

~/taxi_pipeline/lib/python3.6/site-packages/tfx/components/evaluator/executor.py in Do(self, input_dict, output_dict, exec_properties)
    161         else:
    162           models[model_spec.name] = _get_eval_saved_model(
--> 163               input_dict[MODEL_KEY], tags)
    164           absl.logging.info('Using {} for model eval.'.format(
    165               models[model_spec.name].model_path))

UnboundLocalError: local variable 'tags' referenced before assignment
"
38382,Google Developers Certification,"Google Developers Certification plugin is not available in PyCharm 2020.1 (it is needed to take the Tensorfow Certification exam):

![gdc](https://user-images.githubusercontent.com/50177956/78879616-69daa600-7a54-11ea-9ad4-1212c1e289bf.png)
"
38381,Casting from float32 to uint8 results in wrong value despite being from 0 to 255,"I'm using tensorflow-gpu 1.14 with Cuda 10 on Ubuntu 18.04. I'm running tf in **eager mode**.

I'm training a model using Trax. In my input processing pipeline, I read a batch of uint8 images (3-channel each value from 0 to 255) and resize them using `tf.image.resize_images` which results in `float32` but values are still from 0 to 255. Then I cast the `float32` values to `uint8` :

```
 print(""=================="")
 print(""0,0,0 before cast:  "", image_target_before_cast[0][0][0])
 image_target = tf.cast(image_target_before_cast, dtype=tf.dtypes.uint8)
 print(""0,0,0 after cast:  "", image_target[0][0][0])
 print(""=================="")
```

 On first batch of data (16 prints but i'm bringing only one of them) everything is fine:
```
==================
0,0,0 before cast:   tf.Tensor(146.21089, shape=(), dtype=float32)
0,0,0 after cast:   tf.Tensor(146, shape=(), dtype=uint8)
==================
```

Now here is the weird part. From the second batch of images the problem happens:
```
==================
0,0,0 before cast:   tf.Tensor(139.61038, shape=(), dtype=float32)
0,0,0 after cast:   tf.Tensor(115, shape=(), dtype=uint8)
==================
```

I also tried to conver the tensor to numpy array and then cast it using ` image_target_before_cast[0][0][0].numpy().astype(np.uint8)` and it worked. But still don't know what is wrong with the tf cast.

Any idea what might be the issue? 
"
38379,Regarding Ear Landmark Dataset,"I have made ear landmark dataset as shown below image.
![image](https://user-images.githubusercontent.com/29560869/78864360-1cad0300-7a59-11ea-96ad-d68f3be53bb3.png)
I want to make a model to predict the landmarks. So I want some advice on whether I should create my own model or should I use any pre-built model to train it with this dataset.
Thank you"
38377,Error about repository 'mkl_dnn' occurred during the process of building Tensorflow from source,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04 LTS
- TensorFlow installed from (source or binary): source
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a
- CPU model: AMD Opteron 4122



**Describe the problem**
The following problems occurred when using bazel to build Tensorflow:
>**ERROR:** An error occurred during the fetch of repository 'mkl_dnn':
>   java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f

>**ERROR:** /root/tensorflow/tensorflow/core/kernels/BUILD:799:1: //tensorflow/core/kernels:eigen_contraction_kernel_with_mkl depends on @mkl_dnn//:mkldnn_single_threaded in repository @mkl_dnn which failed to fetch. no such package '@mkl_dnn//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f

>**ERROR:** Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@mkl_dnn//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f


**Provide the exact sequence of commands / steps that you executed before running into the problem**
`bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`


**Any other info / logs**
>**INFO:** Options provided by the client:
>  Inherited 'common' options: --isatty=1 --terminal_columns=241

>**INFO:** Reading rc options for 'build' from /root/tensorflow/.bazelrc:
>  Inherited 'common' options: --experimental_repo_remote_exec

>**INFO:** Reading rc options for 'build' from /root/tensorflow/.bazelrc:
>  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2

>**INFO:** Reading rc options for 'build' from /root/tensorflow/.tf_configure.bazelrc:
>  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0

>**INFO:** Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1

>**INFO:** Found applicable config definition build:xla in file /root/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true

>**INFO:** Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels

>**INFO:** Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS

>**WARNING:** Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz failed: class java.io.IOException connect timed out

>**WARNING:** Download from https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f

>**ERROR:** An error occurred during the fetch of repository 'mkl_dnn':
>   java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f

>**INFO:** Call stack for the definition of repository 'icu' which is a third_party_http_archive (rule definition at /root/tensorflow/third_party/repo.bzl:219:28):
> \- /root/tensorflow/third_party/icu/workspace.bzl:11:5
> \- /root/tensorflow/tensorflow/workspace.bzl:59:5
> \- /root/tensorflow/tensorflow/workspace.bzl:103:5
> \- /root/tensorflow/WORKSPACE:19:1

>**INFO:** Call stack for the definition of repository 'llvm-project' which is a tf_http_archive (rule definition at /root/tensorflow/third_party/repo.bzl:134:19):
> \- /root/tensorflow/tensorflow/workspace.bzl:600:5
> \- /root/tensorflow/WORKSPACE:19:1

>**INFO:** Call stack for the definition of repository 'functools32_archive' which is a tf_http_archive (rule definition at /root/tensorflow/third_party/repo.bzl:134:19):
> \- /root/tensorflow/tensorflow/workspace.bzl:378:5
> \- /root/tensorflow/WORKSPACE:19:1

>**ERROR:** /root/tensorflow/tensorflow/core/kernels/BUILD:799:1: //tensorflow/core/kernels:eigen_contraction_kernel_with_mkl depends on @mkl_dnn//:mkldnn_single_threaded in repository @mkl_dnn which failed to fetch. no such package '@mkl_dnn//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f

>**ERROR:** Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@mkl_dnn//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f

>**INFO:** Elapsed time: 102.424s

>**INFO:** 0 processes.

>**FAILED:** Build did NOT complete successfully (312 packages loaded, 10125 targets configured)

"
38376,Extract Model Variable from Serialize Graph File,"**System information**
- TensorFlow version (you are using): 1.14.0
- Are you willing to contribute it (Yes/No): Yes.



**Describe the feature and the current behavior/state.**
![image](https://user-images.githubusercontent.com/16629752/78857533-2edc7080-7a5c-11ea-80a7-a69ccd51fc63.png)
When I save a graph in to keras.pb. Then I wan to acquire <b>model</b> variable. However, I cannot find a way to achieve.

**Will this change the current api? How?**
I am not sure.

**Who will benefit with this feature?**
This feature can help people transfer released model parameters into other format, such as ML flow, Keras etc.

**Any Other info.**
"
38375,tf.tile each dimension individual  operation not supported,"**System information** 

     OS Platform and Distribution : macOS Catalina 10.15.3

    TensorFlow installed from : binary

    TensorFlow version : 1.15.0

    Python version: 3.7.3

**Describe the current behavior**

I have tensor 

a = `tf.Tensor(
[[[ 16  15  16  15  87]
  [  3   4   3   4  87]
  [  3   4   8   9 133]
  [  3   4   8   9  871]
  [  3   4  14   95  87]
  [  3   7   9  250  87]]
 [[  3   4   3   41  87]
  [  3   4   8   93 133]
  [  3   4   18   9  87]
  [  3   4  141   5  87]
  [  3   7   19  25  87]
  [  0   0   0   0   0]]], shape=(2, 6, 5), dtype=int64)`

b = `tf.Tensor(
[[[ 1]
  [2]
  [ 3]
  [ 1]
  [ 2]
  [2]]
 [[2]
  [ 2]
  [ 2]
  [ 2]
  [1]
  [ 0]]], shape=(2, 6, 1), dtype=int64)`
**Describe the expected behavior**

I want vector c such that 

[ 16  15  16  15  87] is repeated [1] times , 
[  3   4   3   4  87] is repeated [2] times , 
[  3   4   8   9 133] is repeated [3] times,
[  3   4   8   9  871] is repated [1] times,
[  3   4  14   95  87] is repeated [2] times,
[  3   7   9  250  87] is repated [2] times and so on ............

so c should be 

c= `tf.Tensor(
[[[ 16  15  16  15  87]
[  3   4   3   4  87]
[  3   4   3   4  87]
[  3   4   8   9 133]
[  3   4   8   9 133]
[  3   4   8   9 133]
[  3   4   8   9  871]
[  3   4  14   95  87]
[  3   4  14   95  87]
[  3   7   9  250  87]
[  3   7   9  250  87]]
 [[  3   4   3   41  87]
[  3   4   3   41  87]
  [  3   4   8   93 133]
 [  3   4   8   93 133]
  [  3   4   18   9  87]
 [  3   4   18   9  87]
  [  3   4  141   5  87]
[  3   4  141   5  87]
  [  3   7   19  25  87]
 [  3   7   19  25  87]
  [  0   0   0   0   0]]])`


I tried using tf.tile but it only takes same repeation for each dimension . 
"
38372,Windows 10 shuts down when using GPU (tf 2.1.0 with tf.keras and tf 1.14 with keras),"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes, custom code
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10 64 bit
- TensorFlow installed from (source or
binary): Tensorflow was installed using `pip install tensorflow`
- TensorFlow version (use command below): 2.1.0 (but occurs on 1.14, 1.15, 2.0 as well)
- Python version: Occurs on Python 3.5, 3.6, 3.7
- Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: I have the following installed: CUDA 9.2, CUDA 10.0, CUDA 10.1, CUDA 10.2 (with appropriate CuDNN versions)
- GPU model and memory: Occurs on new RTX 2080 ti (11 GB) and my old GTX 1070 (8GB)

**Describe the current behavior**: When using either tensorflow or tensorflow-gpu with two of my graphics cards (1070, 2080 ti) my system shuts off completely (no warning, as if I unplugged the power) if I train a model with a high batch size (~32-42). Specifically using tf.keras's Conv2D and CuDNNLSTM/CuDNNGRU. It does not seem to have any problems training a fully connected model with batch sizes up to 512. Everything is fine if I train my Conv2D model with a batch size of 16 or lower. Also, if I use nvidia-smi to limit my GPU's power consumption from stock 250W to 150W it works fine, but is very slow. However when it trains with 16 batch size, it goes above 200W regularly and has no problem.

**Describe the expected behavior**: System does not shut off and should spit back a memory error if it's running out of memory.

**Standalone code to reproduce the issue**:
```
import tensorflow as tf
import numpy as np
import random

from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten
from tensorflow.keras import Sequential

num_samples = 500
h = 665
w = 814
c = 3

x = np.random.rand(num_samples, h, w, c)
y = [[0] * 4 for _ in range(num_samples)]
for i in range(len(y)):
    y[i][random.randint(0, 3)] = 1
y = np.array(y)

kernel_size = (3, 3)
model = Sequential()
model.add(Conv2D(12, kernel_size, strides=1, activation='relu', input_shape=x.shape[1:]))

model.add(Conv2D(24, kernel_size, strides=1, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(48, kernel_size, strides=1, activation='relu'))
model.add(MaxPooling2D(pool_size=(3, 3)))

model.add(Conv2D(64, kernel_size, strides=1, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(12, kernel_size, strides=1, activation='relu'))

model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(4, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.fit(x, y, batch_size=64, epochs=500)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem: No errors are generated when it shuts down, it just cuts power immediately.

Extra info: I've fully replaced my CPU, power supply, graphics card, motherboard, and RAM and this issue still occurs. PSU was purchased about a month ago brand new. Only thing that is the same in the system since it last occurred is my PC case, storage (SSDs), and the Windows 10 install.

It also *never* occurs in GPU and CPU stress tests and neither GPU or CPU overheats when it shuts down. It's always after the first epoch starts training, or just before."
38370,Cannot convert between a TensorFlowLite buffer with 76800 bytes and a Java Buffer with 230400 bytes,"I modify the image classification code https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android to run inference on a float type grayscale bitmap with size (1, 120, 160, 1). However, an exception is throw regarding ""Cannot convert between a TensorFlowLite buffer with 76800 bytes and a Java Buffer with 230400 bytes"". I checked the input tensor image shape which is indeed (1, 120, 160, 1).

Basically, java buffer is still assuming the input size is (1, 120, 160, 3). Therefore, what need be modified in order to run inference on grayscale image for image classification code? I feel like TensorImage class is somehow hard-coded for RBG image instead of grayscale one.

    Process: org.tensorflow.lite.examples.classification, PID: 20064
    java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 76800 bytes and a Java Buffer with 230400 bytes.
        at org.tensorflow.lite.Tensor.throwIfShapeIsIncompatible(Tensor.java:402)
        at org.tensorflow.lite.Tensor.throwIfDataIsIncompatible(Tensor.java:369)
        at org.tensorflow.lite.Tensor.setTo(Tensor.java:187)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:150)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:314)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:275)
        at org.tensorflow.lite.examples.classification.tflite.Classifier.recognizeImage(Classifier.java:266)
        at org.tensorflow.lite.examples.classification.ClassifierActivity$3.run(ClassifierActivity.java:98)
        at java.lang.Thread.run(Thread.java:919)
"
38369,AttributeError: 'Tensor' object has no attribute '_in_graph_mode',"I am having an error: 'Tensor' object has no attribute '_in_graph_mode'. I've debugged the code, and I think it's in this GradientTape function, but I don't know why. If anyone knows, please help me! :)

**System information** 
- TensorFlow version: 2.0 - '2.2.0-dev20200407'
- OS Platform and Distribution: Linux Mint 
- Python version: Python 3.7.4

**Describe the current behavior**
I am trying to minimize a function using opt = tf.keras.optimizers.Adam() and I am getting a TypeError when I apply _opt.apply_gradients_.

**Standalone code to reproduce the issue** 

  ```

 def explain(
        self,
        validation_data,
        model,
        class_index,
        layer_name=None,
        colormap=cv2.COLORMAP_VIRIDIS,
        image_weight=0.7,
        _grid=True
    ):


# Returns: numpy.ndarray: Grid of all the inverted image or 4D array (batch_size, height, width, channels)
       
        tf.executing_eagerly()
        images, _ = validation_data

        if layer_name is None:
            layer_name = self.infer_target_layer(model)
        
        inverted_image = InvertedImage.get_optimize_image(
            images, model, class_index, layer_name
        )

        if _grid:
            return grid_display(inverted_image)
        else:
            return inverted_image

 @staticmethod
  def infer_target_layer(model):
   
       # Returns: str: Name of the target layer

        for layer in reversed(model.layers):
            # Select closest 4D layer to the end of the network.
            if len(layer.output_shape) == 4 and layer.name.count('conv') > 0:
                return layer.name

        raise ValueError(
            ""Model does not seem to contain 4D layer. Inverted image cannot be applied.""
        )

 @tf.function
  def get_optimize_image(images, model, class_index, layer_name):
   
        grad_model = tf.keras.models.Model(
            [model.inputs], [model.get_layer(layer_name).output]
        )

        opt = tf.keras.optimizers.SGD(learning_rate=1-4, momentum=0.9)
        dtype = model.get_layer(layer_name).output.dtype
        tensor_image = tf.convert_to_tensor(images)

        opt_img = tf.Variable(1e-1 * tf.random.normal((tensor_image.shape[0], tensor_image.shape[1], tensor_image.shape[2], tensor_image.shape[3])), trainable=True)

        steps = 50
        for i in range(steps):
             with tf.GradientTape() as tape:
                
                inverted_feature = tf.cast(opt_img, dtype)
                content_feature = tf.cast(images, dtype)
                    
                conv_inverted_outputs = grad_model(inverted_feature)
                conv_content_outputs = grad_model(content_feature)
            
                loss = InvertedImage.get_loss(conv_content_outputs, conv_inverted_outputs, content_feature, inverted_feature)
                #print(""Initial loss: {:.3f}"".format(loss))

            grad = tape.gradient(loss, [conv_inverted_outputs, conv_content_outputs])
            print(grad)
            processed_grads = [g for g in grad]
            opt.apply_gradients(zip(processed_grads, [conv_inverted_outputs, conv_content_outputs]))

return opt_img
```

**Loss function**
```
def get_loss(conv_content_outputs, conv_inverted_outputs, content_feature, inverted_feature):
        euclidian = tf.norm(conv_content_outputs - conv_inverted_outputs, ord='euclidean') / tf.norm(conv_content_outputs, ord='euclidean')
        reg_alpha = 1e-7 * tf.math.reduce_sum(tf.norm(inverted_feature, ord=6))
        total_variation = 1e-8 * tf.math.reduce_sum(tf.image.total_variation(content_feature+inverted_feature))

        return euclidian + reg_alpha + total_variation 
```

**Traceback** 


```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/helena/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/__main__.py"", line 45, in <module>
    cli.main()
  File ""/home/helena/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/../ptvsd/server/cli.py"", line 361, in main
    run()
  File ""/home/helena/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/../ptvsd/server/cli.py"", line 203, in run_file
    runpy.run_path(options.target, run_name=""__main__"")
  File ""/usr/local/lib/python3.7/runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""/usr/local/lib/python3.7/runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/helena/Documents/LAR_Celesc/lar-computer-vision/objdet-api/test_inverted_image.py"", line 20, in <module>
    data, model, class_index=tabby_cat_class_index, layer_name=""block5_conv3""
  File ""/home/helena/Documents/LAR_Celesc/lar-computer-vision/objdet-api/tf_explain/core/inverted_image.py"", line 54, in explain
    images, model, class_index, layer_name
  File ""/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 497, in _initialize
    *args, **kwds))
  File ""/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in converted code:
    /home/helena/Documents/LAR_Celesc/lar-computer-vision/objdet-api/tf_explain/core/inverted_image.py:125 get_optimize_image  *
        opt.apply_gradients(grads_and_vars)
    /home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:434 apply_gradients
        self._create_slots(var_list)
    /home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/gradient_descent.py:100 _create_slots
        self.add_slot(var, ""momentum"")
    /home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:574 add_slot
        var_key = _var_key(var)
    /home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:1065 _var_key
        if var._in_graph_mode:

    AttributeError: 'Tensor' object has no attribute '_in_graph_mode'

```"
38367,Memory Growth and Memory Limit incompatibility,"**System information** 
- Custom Code
- OS Platform and Distribution: Windows 10 Home 1909
- TensorFlow installed with pip
- TensorFlow version: v2.1.0-rc2-17-ge5bf8de410, 2.1.0
- Python version: 3.7.6
- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5
- GPU model and memory: NVIDIA GTX 1050 notebook, 4GB memory

**Describe the current behavior**
Tensorflow ""forgets"" about the setting on memory_growth once you create a vritual device to further limit maximum VRAM usage.

**Describe the expected behavior**
In practice I would like to be able to both limit maximum VRAM usage AND to tell TF to only use the VRAM that it needs with the memory_growth option. This is something that I was able to do in TF 1.x by passing an appropriate config to a Session, but it doesn't seem like it's possible in TF 2.x (without using 1.x code that is...)

**Standalone code to reproduce the issue** 
https://colab.research.google.com/drive/1MJgZiHernOjT_3vB57YMnjjTRJhgpspo
"
38366,Strange behavior of ODE solving toy model,"**System information** 
- OS Platform and Distribution Windows 10
- TensorFlow installed from source TensorFlow 2.2.0 
- Python version: 3.8
- CUDA/cuDNN version: 10.1

```
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

N = 1000
x = tf.convert_to_tensor(np.linspace(0, 1, num=N), dtype=tf.float32)

with tf.GradientTape() as t:
    x_inp = keras.Input(shape=(1,), name='indvar')
    t.watch(x_inp)
    dense = layers.Dense(100, activation='tanh')(x_inp)
    dense = layers.Dense(100, activation='tanh')(dense)
    dense = layers.Dense(100, activation='tanh')(dense)
    dense = layers.Dense(100, activation='tanh')(dense)
    dense = layers.Dense(100, activation='tanh')(dense)
    dense = layers.Dense(100, activation='tanh')(dense)
    dense = layers.Dense(100, activation='tanh')(dense)
    yhat = layers.Dense(1, name='y')(dense)
    dyhat = t.gradient(yhat, x_inp)
    model = keras.Model(inputs=x_inp, outputs=[yhat, dyhat])
    opt=tf.keras.optimizers.Adam()
    model.compile(optimizer=opt, loss='MSE')
    model.summary()
    model.fit(x=x, y=[x, np.sin(10*x)], epochs=10000, batch_size=N)
    y, dy = model(x)
    plt.plot(x, np.array(y))
    plt.plot(x, np.array(dy))
    plt.show()
```
I try to make model output y=x, while y'=sin(10*x). Surely this is impossible so the expected behavior of the model is NOT converge. However model converges and have both y=x and y'=sin(10*x) satisfied, however it is impossible.    "
38365,AutoGraph Warning on Tensorflow-gpu==2.0.0b1,"Running the Image Captioning tutorial (https://www.tensorflow.org/tutorials/text/image_captioning) on google colab with tensorflow-gpu==2.0.0b1 produces the next warning.


WARNING: Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'




"
38363,TFLite - number of detections,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary):binary
 - TensorFlow version (use command below):  1.14
- Python version:3.6
 - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version:10.0
 - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Hi
I've trained a model to produce only one detection. I have converted it to tflite, I used this command to freeze the model: ""export_tflite_ssd_graph.py --pipeline_config_path=/pipeline.config --trained_checkpoint_prefix=plate_detector/training/model.ckpt-33539 --output_directory=tflite --add_postprocessing_op=true --max_detections=1""
And I have changed NUM_DETECTIONS  to 1 in TFLiteObjectDetectionAPIModel,
but still getting this error:""Cannot copy between a TensorFlowLite tensor with shape [1, 10, 4] and a Java object with shape [1, 1, 4]"".
Any help?
**Describe the expected behavior**

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
38362,what's the difference between tensorflow functions and tensorflow.keras.backend functions?,"I know this is not a Technical Issue, but i found no answers regarding this, hence i came here for support.


Many of the tensorflow functions overlap with the tensorflow.keras.backend functions.

For eg:
tensorflow.keras.backend have sum
tensorflow have reduced_sum

tensorflow.keras.backend have square
tensorflow have math.square

tensorflow.keras.backend have mean
tensorflow have reduced_mean

tensorflow.keras.backend have exp
tensorflow have exp

and many more functions overlap.

are these functions different in some perspective?"
38361,"2 different ""no module"" errors in two different versions","**System information** 
- I wrote custom code
- Google Colab
- TensorFlow installed from default, then I tried multiple versions using Pip 
- TensorFlow version (use command below): 
1.4, 1.9, 1.11, 2.0
- Python version: 3.6

**Describe the current behavior**
If I use a version of Tensorflow 1, I get the error ""tensorflow.data has no attribute: experimental"" and if I use tensorflow 2 I get: tensorflow has no atrribute: placeholder.

**Describe the expected behavior**
I was hoping for the script to work without errors.

**Standalone code to reproduce the issue** 
https://colab.research.google.com/drive/1N0CtpO6VZvcyE72r3eEkBQthQsUlm6Hf

"
38359,Marginal Probablities in CRF,"After training a CRF model, I can get the best tag sequence y and its unormalized score for each test input sequence x through tf.contrib.crf.viterbi_decode() or tf.contrib.crf.crf_decode() but what changes might require in CRF implementation to get the marginal probabilities?

Tensorflow version: 1.15.0


"
38357,[MixedPrecision] DynamicLossScale should accept scale_loss smaller than one,"https://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/python/training/experimental/loss_scale.py#L391

``` python
new_loss_scale = math_ops.maximum(
      self._current_loss_scale / self._multiplier, 1)
```

`self._current_loss_scale >=1` is unnecessary and wrong.
In some use cases, loss is possible overflow `inf` in float16, therefore `self._current_loss_scale<1` is necessary for mixed precision training.
"
38356,Add MPI cluster resolver,"**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

There should be a cluster resolver that works inside any MPI jobs by querying properties of MPI_COM_WORLD.

This allows running TF easily inside any HPC cluster environment independent of the Batch system

**Will this change the current api? How?**

New class added.

**Who will benefit with this feature?**

HPC users.

**Any Other info.**

Existing PR with implementation: https://github.com/tensorflow/tensorflow/pull/38112

CCing  @jhseu @frankchn "
38353,Support input-based loss-functions in tf.keras.Model,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 2.2.0rc2
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

In `tf.keras.Model`, there should be an API for supplying loss functions and metrics that are based on the input `x_input` and not just the output `y_pred`. 

Currently when supplying custom loss functions in `tf.keras.Model` or metrics they need to be on the form `loss(y_true, y_pred)`. The most import example is in `Model.compile()` which accepts a functional argument `loss` of this type but not one having access to `x_input`.

**Will this change the current api? How?**

Yes, but there are many possible implementations in which compatibility can be preserved. For example `Model.compile()` could accept an argument `loss_x` (supplying both this and `loss` in the same call throws an error). This should then accept a function as follows.

`def custom_loss_x(y_true, y_pred, x_input): ... `

or preferably

`def custom_loss_x(y_true, x_input): ... `

as y_pred can be retained by `y_pred = model(x_input)`.

In an analogous way custom metrics for models can be constructed and supplied.

**Who will benefit with this feature?**

Any model with a loss function or metrics that uses information available in the input layer but not in the output layer.

An example is when the input consists of a segmented image and the output is an image of the same shape. If different segments are to be weighted differently in the loss function this loss function will depend on `x_input`. 

**Any Other info.**
"
38352,The docker image provided to compile tensorflow custom ops is too big.,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): no
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: None
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): TF 2.1.0 from pypi
- Python version: - Bazel 
version (if compiling from source): None
- GCC/Compiler version (if compiling from
source): None 
- CUDA/cuDNN version: - GPU model and memory: CUDA 10.1 CUDNN 7.6, no gpu, just compiling

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Follow-up on the SIG-build meeting of 07/04/2020.

In TF Addons, we use the docker image provided by tensorflow to build our ""manylinux"" wheels:

```
docker pull tensorflow/tensorflow:2.1.0-custom-op-gpu-ubuntu16
```

Uncompressed, this image is around 9.1GB. This poses several issues:
* Our disk quota of github-actions can't handle more than 14GB, we reached this limit multiple times and when we do, our CI build fails. We had to remove files from the docker image and squash it to make our CI work again.
* Downloading and decompressing the docker image takes a significant amount of time in the CI 2-3m. As a reference point, running the full build takes 12m.
* It makes it hard for developers to use the image locally because they need to download 4-5 GB of data (the docker image compressed is 4.24GB).


**Describe the expected behavior**

We should have multiple images depending on what python versions we want to use to compile. Those docker images should be around 3-4GB uncompressed.

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
docker pull tensorflow/tensorflow:2.1.0-custom-op-gpu-ubuntu16  && docker image ls
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

I digged into the docker image, see what was taking space, and I believe that I managed to find the biggest directories:

```
Total: 9.1G

1.7G    /usr/local/lib/python2.7
417M    /usr/local/lib/python3.6
157M    /usr/local/lib/python3.7
2.0G    /usr/local/cuda-10.1
2.0G    /usr/lib/x86_64-linux-gnu
591M    /root/.cache/pip
529M    /dt8  (devtoolset-8)
469M    /dt7  (devtoolset-9)

Those directories together take 7.8G, so 85% of the total size.
```

Possible actions:

-> Do not use the pip cache
-> Is there a need for the two devtoolset? Can one be enough?
-> Many pip installs are done in the Dockerfile for py2.7 (tf and other). 
   Those do not seem necessary as in TF Addons, we can build ""manylinux"" wheels with the py3.7
   which has nothing installed on it with pip (bare install).
-> Make a docker image for each python version, should save ~200MB per python version. e.g. 
`docker pull tensorflow/tensorflow:2.1.0-py3.6-custom-op-gpu-ubuntu16`

I would be nice to take care of this before the custom op docker image for tf 2.2.0 is released.

I was asked during the sig-build meeting to tag @yifeif .

Tagging @seanpmorgan  because he manages TF Addons."
38351,"Add public API to get CUDA version and path, CUDNN version and path.","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 2.2.0rc2
- Are you willing to contribute it (Yes/No): no

**Describe the feature and the current behavior/state.**

Follow-up of the SIG build meeting of the 07/04/2020.

Currently TensorFlow is compiled against a specific CUDA and CUDNN version. When people want to build TensorFlow Addons from source, they have to specify manually the CUDA and CUDNN paths and version to use: https://github.com/tensorflow/addons/blob/master/configure.py#L159 

But it should not be needed to ask this information to the user. Because the user have to use the exact CUDA and CUDNN versions that TF was compiled against. Also TF is able to detect where are the CUDA and CUDNN directories on the user filesystem. If we could get this information out of TensorFlow, that would make the building of custom ops easier. We could also do some version checking at runtime and improve the user experience when there is a version mismatch.

**Will this change the current api? How?**

We would need 4 new functions/attributes (the names are examples or course):

* `tf.config.get_cuda_version_used_to_compile_tf()`
* `tf.config.get_cudnn_version_used_to_compile_tf()`
* `tf.get_cuda_path()`
* `tf.get_cudnn_path()`

**Who will benefit with this feature?**

People writing custom ops, as it makes the configuration easier for compilation, it also allows checking that people using the TF addons binaries have a compatible TF installed as the cuda and cudnn versions have to match.

**Any Other info.**

I was asked during the sig-build meeting to tag:
@perfinion @gunan @angerson @yifeif

@seanpmorgan as he manages TF Addons
"
38350,ImportError: No module named numpy,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below):  2.1.0
- Python version: - Bazel
version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/api/tests
Numpy is installed still numpy Import Error .

**Describe the expected behavior**

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

bazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
	File ""/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl"", line 263
		_create_local_python_repository(<1 more arguments>)
	File ""/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl"", line 213, in _create_local_python_repository
		_get_numpy_include(<2 more arguments>)
	File ""/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl"", line 187, in _get_numpy_include
		execute(repository_ctx, <3 more arguments>)
	File ""/home/ayushman/Documents/github/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
		fail(<1 more arguments>)
Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named numpy
Is numpy installed?
ERROR: While resolving toolchains for target //tensorflow/tools/api/tests:api_compatibility_test: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):
	File ""/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl"", line 263
		_create_local_python_repository(<1 more arguments>)
	File ""/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl"", line 213, in _create_local_python_repository
		_get_numpy_include(<2 more arguments>)
	File ""/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl"", line 187, in _get_numpy_include
		execute(repository_ctx, <3 more arguments>)
	File ""/home/ayushman/Documents/github/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
		fail(<1 more arguments>)
Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named numpy
Is numpy installed?
ERROR: Analysis of target '//tensorflow/tools/api/tests:api_compatibility_test' failed; build aborted: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):
	File ""/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl"", line 263
		_create_local_python_repository(<1 more arguments>)
	File ""/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl"", line 213, in _create_local_python_repository
		_get_numpy_include(<2 more arguments>)
	File ""/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl"", line 187, in _get_numpy_include
		execute(repository_ctx, <3 more arguments>)
	File ""/home/ayushman/Documents/github/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
		fail(<1 more arguments>)
Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named numpy
Is numpy installed?
INFO: Elapsed time: 0.357s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)

"
38349,`nan` gradient when `tf.where` is used,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0 / v1.12.1-29016-g38797a1c8b 2.2.0-dev20200407
- Python version: 3.7.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Well-defined function with `tf.where` has `nan` gradients at points where `tf.where` inactive branch is undefined.

**Describe the expected behavior**
Inactive branch should be ignored in gradients calculations.

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import tensorflow as tf

for ex in range(-3, 3):
    x = tf.convert_to_tensor(10.**ex)
    with tf.GradientTape() as g:
        g.watch(x)
        y = tf.where(x >= -1., x, tf.math.log1p(-x))
#         y = tf.where(x >= -1., x, tf.math.log(1.-x))
#         y = tf.where(x >= -1., x, 1./(1.-x))
    dy_dx = g.gradient(y, x)
    print(f'y({x})={y}, dy/dx({x})={dy_dx}')
```

All 3 functions above are well defined for positive values used for testing. Still they show no gradient at point `1.`. while it has to be equal to `1.`

```
y(0.0010000000474974513)=0.0010000000474974513, dy/dx(0.0010000000474974513)=1.0
y(0.009999999776482582)=0.009999999776482582, dy/dx(0.009999999776482582)=1.0
y(0.10000000149011612)=0.10000000149011612, dy/dx(0.10000000149011612)=1.0
y(1.0)=1.0, dy/dx(1.0)=nan
y(10.0)=10.0, dy/dx(10.0)=1.0
y(100.0)=100.0, dy/dx(100.0)=1.0
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
38348,Cannot convert tf.math.conj Op in Keras model to tflite,"------------------------

### System information
- **OS Platform and Distribution**: Linux-4.19.0-8-amd64-x86_64-with-debian-10.3
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.2.0-dev20200406
- **Python version**: 3.7.6
- **CUDA/cuDNN version**: 10.0
- **GPU model and memory**: 4x GeForce RTX2080Ti, 12Gb

### Describe the problem
Unsupported Op tf.math.conj when converting keras model to tflite. Is there a workaround for this?

### Source code (MWE)
```python
import tensorflow as tf
from tensorflow.keras.layers import Input, Lambda
from tensorflow.keras.models import Model


if __name__ == '__main__':
    model_in = Input(shape=(128,1))


    fft = Lambda(lambda x: tf.signal.rfft(x), name='RFFT')
    sig_spec = fft(model_in)

    conjugate = Lambda(lambda x: tf.math.conj(x), name='conjugate')(sig_spec)
    correlation_spec = sig_spec * conjugate
    correlation = Lambda(lambda x: tf.signal.irfft(x), name='IRFFT')(correlation_spec)

    model = Model(inputs=model_in, outputs=correlation)

    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.experimental_new_converter = True
    converter.target_spec.supported_ops = set([tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS])
    tflite_model = converter.convert()
```
### Error log
`<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
	tf.Conj {device = """"}
`


"
38347,Waymo dataset: module 'tensorflow' has no attribute 'enable_eager_execution',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I am replicating the waymo dataset tutorial code for understanding and learning
- Using Notebook of Google COLAB (as New user to Google Colab)
- TensorFlow version (use command below): 2.1.0 (as installed by default in Google Colab)

**Describe the current behavior**

This is the line of code: (sharing since it is available for public view as well by google in it's tutorial)
``` 
!pip3 install waymo-open-dataset
import os
import tensorflow as tf
import math
import numpy as np
import itertools

tf.enable_eager_execution()

from waymo_open_dataset.utils import range_image_utils
from waymo_open_dataset.utils import transform_utils
from waymo_open_dataset.utils import frame_utils
from waymo_open_dataset import dataset_pb2 as open_dataset
```

**Error I get upon executing the above lines of code**
```
AttributeError Traceback (most recent call last)
in ()
6 import itertools
7
----> 8 tf.enable_eager_execution()
9
10 from waymo_open_dataset.utils import range_image_utils

AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'
```

Even Upon commenting the line "" tf.enable_eager_execution()"" - as tensor 2.x version doesn't need, a new error pops up.
```
!pip3 install waymo-open-dataset
import os
import tensorflow as tf
import math
import numpy as np
import itertools

#tf.enable_eager_execution()

from waymo_open_dataset.utils import range_image_utils
from waymo_open_dataset.utils import transform_utils
from waymo_open_dataset.utils import frame_utils
from waymo_open_dataset import dataset_pb2 as open_dataset

I get the following error

AttributeError Traceback (most recent call last)
in ()
8 #tf.enable_eager_execution()
9
---> 10 from waymo_open_dataset.utils import range_image_utils
11 from waymo_open_dataset.utils import transform_utils
12 from waymo_open_dataset.utils import frame_utils

/usr/local/lib/python3.6/dist-packages/waymo_open_dataset/utils/range_image_utils.py in ()
57 value,
58 shape,
---> 59 pool_method=tf.unsorted_segment_max):
60 """"""Similar as tf.scatter_nd but allows custom pool method.
61

AttributeError: module 'tensorflow' has no attribute 'unsorted_segment_max'

```

**Describe the expected behavior**
No error should be there.

**Standalone code to reproduce the issue**  
Link to Colab/Jupyter/any notebook.
https://colab.research.google.com/gist/Viki250893/40008cf236a25736d1cdefc7e0e403ea/untitled1.ipynb

"
38346,Not able to load a tf.keras model,"**System information** 
- Have I written custom code: Yes
- OS Platform and Distribution: MacOs Mojave version 10.14.6
- Mobile device if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
 TensorFlow version: v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2
- Python version: - Python 3.6.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: - GPU model and memory:N/A

**Describe the current behavior**
For the below model(using tf.signal.frame), saving to keras format works, but when trying to load it, we get the following error:
```
2020-04-08 00:38:22.847450: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-08 00:38:22.865528: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13ce3feb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-08 00:38:22.865552: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in _create_c_op
    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent values for attr 'Tshape' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Reshape/Reshape' using Op<name=Reshape; signature=tensor:T, shape:Tshape -> output:T; attr=T:type; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 8, in <module>
    keras_model = tf.keras.models.load_model(""test.h5"")
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py"", line 184, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 170, in load_model_from_hdf5
    custom_objects=custom_objects)
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py"", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 109, in deserialize
    printable_module_name='layer')
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 373, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 982, in from_config
    config, custom_objects)
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 2024, in reconstruct_from_config
    process_node(layer, node_data)
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1972, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 922, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 2830, in call
    return self._make_op(inputs)
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 2852, in _make_op
    c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])
  File ""/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1657, in _create_c_op
    raise ValueError(str(e))
ValueError: Inconsistent values for attr 'Tshape' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Reshape/Reshape' using Op<name=Reshape; signature=tensor:T, shape:Tshape -> output:T; attr=T:type; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
```

**Describe the expected behavior**
Model loading should work fine.

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import tensorflow as tf
  
x = tf.keras.Input(shape=(None, 12), dtype=""float32"", name=""input"")
x_frequency = tf.signal.frame(x, 2, 1, axis=1)
model = tf.keras.Model(inputs=x, outputs=x_frequency, name=""test"")
model.save(""test.h5"")

keras_model = tf.keras.models.load_model(""test.h5"")
```
"
38344,tf.data.experimental.make_csv_dataset ERROR,"**TensorFlow2.0-GPU**

**My Code:**
![image](https://user-images.githubusercontent.com/30002208/78754918-e2852800-79aa-11ea-954f-b8293fdf85a1.png)




**My Original CSV Data Example(Lager CSV):**

![image](https://user-images.githubusercontent.com/30002208/78753676-caaca480-79a8-11ea-8cc2-b2c5bd00e162.png)

**Code Result:**
![image](https://user-images.githubusercontent.com/30002208/78753734-eb74fa00-79a8-11ea-8be4-c75b0b739031.png)


"
38343,RuntimeError: Can't copy Tensor with type string to device,"**System information** 
- 
- OS Platform and Distribution: Ubuntu 18.04.2 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.7
- CUDA/cuDNN version: CUDA 10.2 
- GPU model and memory: Tesla V100-SXM2-32GB 



**Describe the current behavior**
RuntimeError: Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.

**Describe the expected behavior**
Runs perfectly on CPU

**Standalone code to reproduce the issue** 
 https://tfhub.dev/google/universal-sentence-encoder-large/5 **ON A GPU**.


    import tensorflow_hub as hub

    embed = hub.load(""https://tfhub.dev/google/universal-sentence-encoder-large/5"")

    with tf.device('GPU:0'):
    
        embeddings = embed([""The quick brown fox jumps over the lazy dog.""])

    print embeddings


**Other info / logs**
Traceback (most recent call last):
  File ""create_sentence_embeddings.py"", line 25, in <module>
    temp = embed(ele)
  File ""/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py"", line 438, in _call_attribute
    return instance.__call__(*args, **kwargs)
  File ""/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 636, in _call
    *args, **kwds)
  File ""/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2185, in canonicalize_function_inputs
    self._flat_input_signature)
  File ""/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2240, in _convert_inputs_to_signature
    value, dtype_hint=spec.dtype)
  File ""/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1302, in convert_to_tensor
    value, dtype=preferred_dtype, name=name, as_ref=as_ref)
  File ""/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 317, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 258, in constant
    allow_broadcast=True)
  File ""/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 266, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
RuntimeError: Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.
"
38342,Generic opencl support by porting all cuda related parts to opencl,"I wonder if it is possible to make **tensorflow support generic opencl devices (not just AMD Gpus) by porting cuda to opencl manually ?**
**What are the challenges here (besides the huge maintance costs)?** 
Can we use tensorflow without Eigen library (i.e. **all the kernels in Eigen are preproduced with opencl kernels**) so that it will overcome the fact that opencl doesn't support C++.
Maybe someone could help me figure them out, thanks :D"
38341,RuntimeError: Encountered unresolved custom op: Dilation2D.Node number 2 (Dilation2D) failed to prepare,"What can i do when i must use Dilation2D op on the tensorflowlite? Thinks.
```
RuntimeError: Encountered unresolved custom op: Dilation2D.Node number 2 (Dilation2D) failed to prepare. 
```"
38340,How to control the cpu core?,"I use tf pipline to augment the training data then training the model, but it's useless to set the tf.ConfigProto(device_count={""CPU"":10},
  inter_op_parallelism_threads=1,
  intra_op_parallelism_threads=1,), all cpu core work as follow;
![image](https://user-images.githubusercontent.com/28778038/78748612-0cd0e880-799f-11ea-8e61-02e30493d372.png)

I expect to limit the cpu core which can not influence other tasks.
work platform:
ubuntu 18.04
 tf-gpu-1.9
NVIDIA GTX1080ti

Anyone have the solution?


"
38338,TFLite BATCH_MATMUL should be BATCH_MAT_MUL,"**Describe the current behavior**

TensorFlow Lite added a new operator `BuiltinOptions.BATCH_MATMUL` which translates to `BatchMatmul` but the corresponding options are named `BatchMatMul`.

**Describe the expected behavior**

The operator should be called `BuiltinOptions.BATCH_MAT_MUL`.

**Standalone code to reproduce the issue** 

https://github.com/tensorflow/tensorflow/commit/828fe43cf3aaf1a60ed0b263d4af4b3442e79121#r38357271

@talumbau"
38337,"mobilenet_v2_140_224/classification: Cannot copy between a TensorFlowLite tensor with shape [1, 120] and a Java object with shape [1, 10, 4]","**System information** 
- Windows 10 10.0.18362 Build 18362
- TensorFlow 2.2.0 installed from pip 
- Python v3.8.2

**Describe the current behavior**
I used the make_image_classifier (https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier) to generate the TFLITE model based on images available on http://vision.stanford.edu/aditya86/ImageNetDogs/:
```
make_image_classifier --image_dir images \
--tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2 \
--image_size 224 \
--saved_model_dir models \
--labels_output_file output/labels.txt \
--tflite_output_file output/retrained.tflite
```

Then I copied the .tflite file to my Android project assets folder and tried to execute the app, but it crashed with the following error:

 ```
Process: org.tensorflow.lite.examples.detection, PID: 9525
    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 120] and a Java object with shape [1, 10, 4].
        at org.tensorflow.lite.Tensor.throwIfShapeIsIncompatible(Tensor.java:412)
        at org.tensorflow.lite.Tensor.throwIfDataIsIncompatible(Tensor.java:369)
        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:247)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:166)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:314)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:195)
        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:193)
        at android.os.HandlerThread.run(HandlerThread.java:65)
```
I looked into almost similar issues, but none of them had the TensorFlowLite with a shape based on two parameters (i.e. [1, 120]) while having the Java object with a shape based on three (i.e. [1, 10, 4]).
  
make_image_classifier: https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier


**Describe the expected behavior**
The app should start with no problems and detect/classify objects based on the provided TFLITE model.

**Standalone code to reproduce the issue** 
https://github.com/LavitzBr/object_detection
"
38336,TFLite Failed to build gpu delegate on Linux,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.1
- Python version: 2.7
- Installed using virtualenv? pip? conda?: docker (linux env)
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source):clang version 7.0.0-3~ubuntu0.18.04.1


**Describe the problem**
I am trying to build tensorflow lite gpu delegate on linux (docker inside imac) but I cannot build using bazel when compiling the gl_delegate. This also happens on v2.2.0rc2 but using bazel 2.0.0 and modifying the configure file to prevent errors from running.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. clone tensorflow
2. checkout v2.1.0 branch
3. install all the python, pip dependenies
4. apt get install `mesa-common-dev libgl1-mesa-dev libgles2-mesa-dev ocl-icd-opencl-dev`
5. run ./configure, press enter to each (so use all default commands), python points to /usr/bin/python etc
6. install `bazel 0.29.1`
7. Run `bazel build tensorflow/lite/delegates/gpu:gl_delegate`

Then it fails with 

```
In file included from tensorflow/lite/delegates/gpu/gl/egl_context.cc:21:
./tensorflow/lite/delegates/gpu/gl/gl_call.h:60:29: error: no viable conversion from returned value of type 'tflite::gpu::Status' to function return type 'int'
    if (status.ok()) return OkStatus();
                            ^~~~~~~~~~
./tensorflow/lite/delegates/gpu/gl/gl_call.h:73:29: error: no viable conversion from returned value of type 'tflite::gpu::Status' to function return type 'int'
    if (status.ok()) return OkStatus();
                            ^~~~~~~~~~
tensorflow/lite/delegates/gpu/gl/egl_context.cc:33:3: error: member reference base type 'const int' is not a structure or union
  RETURN_IF_ERROR(GetOpenGlErrors());
  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
./tensorflow/lite/delegates/gpu/common/status.h:68:17: note: expanded from macro 'RETURN_IF_ERROR'
    if (!status2.ok()) return status2; \
         ~~~~~~~^~~
```
And a lot more of this. 
Not sure why it tries to convert the Status into an int. Maybe i am targeting the wrong build target?"
38335,tflite Interpreter causes iOS app to crash,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Following this tutorial for tflite with C++: https://www.tensorflow.org/lite/guide/inference

- OS Platform and Distribution: MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: iPhone XS Max
- TensorFlow installed from (binary): - TensorFlow version (use command below): 2.1.0
- Python version: - Bazel 
version (if compiling from source): bazel 1.1.0
- GCC/Compiler version (if compiling from
source): 4.2.1

**Describe the current behavior**

When using tflite with Mediapipe for hand tracking, building the interpreter causes the app to crash after being built and opened. 

**Steps to reproduce behavior**

1. Need to clone our project github repo: https://github.com/sne21star/mediapipe
2. Try building the iOS app from command line using this command: bazel build --config=ios_arm64 mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp
3. Download to iOS and try out app, will crash due to these lines in hand_gesture_recognition_calculator.cc

//  Build the interpreter
    tflite::ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr<tflite::Interpreter> interpreter;
    tflite::InterpreterBuilder(*model, resolver)(&interpreter);

Without these lines, app runs fine (same as their out of box hand-tracking example).

**Describe the expected behavior**

Building the desktop app runs fine and classifies the ASL letters, so expect it to work the same on iOS.
"
38332,Update NCCL version to the latest release v2.6.4-1,"**System information**
- TensorFlow version (you are using): master
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
Could you please update NCCL used within TensorFlow building to the latest release v2.6.4-1: https://github.com/NVIDIA/nccl/releases/tag/v2.6.4-1

**Will this change the current api? How?** No

**Who will benefit with this feature?**
Needed to enable new NCCL features (in particular network collectives).

**Any Other info.**
"
38325,Bazel Build Tensorflow CPU-only,"**System information**
- Windows 10
- TensorFlow Build from source
- TensorFlow version 1.x (trying)
- Python version: 3.6
- Installed using chocoletey powershell
- Bazel version : 3.0

ERROR: Config value opt is not defined in any .rc file

`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

might be helpful, I received error when I was executing tensorflow 1.x version
`bazel build --config=v1 //tensorflow/tools/pip_package:build_pip_package`


[bazel build tensorflow cpu-only.txt](https://github.com/tensorflow/tensorflow/files/4446724/bazel.build.tensorflow.cpu-only.txt)

"
38324,Performance issue with cross compiled benchmark_model and label_img,"**System information** 
- OS Platform and Distribution 
**Host** : Linux  4.15.0-96-generic #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
**Target**: Linux ubuntu 5.3.0-1019-raspberry4 #21-Ubuntu SMP Mon Feb 17 14:05:03 UTC 2020 aarch64 aarch64 aarch64 GNU/Linux

- Mobile device : Raspberry Pi 4
The performance issue happens on mobile device:   Raspberry Pi 4

- TensorFlow installed from: source, commit 9d2bdd0b244c759ca54c6f10f58c069ec2200452
follow the instructions in https://www.tensorflow.org/lite/guide/build_arm64 

- GCC/Compiler version 
**Host**: gcc version 7.5.0 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) Target: aarch64-linux-gnu
**Target:** gcc version 9.2.1 20191008 (Ubuntu 9.2.1-9ubuntu2) 

**Describe the current behavior**

When I run the benchmark_model or label_img applications,the inference time is longer in cross compiled application than native compile applications.

Surprisingly, the time is greater in float point models. In quantized models there is not much difference.

Compiler issue ??

Benchmark | model | Native compiled | Cross compiled
-- | -- | -- | --
benchmark_model | mobilenet v1 | 161564 | 195817
benchmark_model | mobilenet v1 quant | 95125.1 | 99856.7
label_img | mobilenet v1 | 72.938 ms | 217.212 ms
label_img | mobilenet v1 quant | 30.668 ms | 31.723 ms




"
38321,Lambda layer as loss_function - Invalid Argument Error,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution: AWS SageMaker with Tensorflow 2.1.0 and Python 3.6
- TensorFlow version: 2.1.0
- Python version: - Python 3.6
- CUDA/cuDNN version: - GPU model and memory: Tesla K80

**Describe the current behavior**
I am trying to use Lambda layer as the last layer and to use its output for loss function. The below code is a dummy demo that I expected that the model should be compiled normally. However, it raises the InvalidArgumentError for loss.  The code is below or available at the following gist: https://gist.github.com/quocdat32461997/10358455066ccc76817b54d20613c1dd

**Standalone code to reproduce the issue** 
def loss_fn(args):
        return K.constant(1, dtype = 'float32')

inputs = Input(shape = (784,))
dense1 = Dense(512, activation = 'relu')(inputs)
dense2 = Dense(128, activation = 'relu')(dense1)
dense3 = Dense(32, activation = 'relu')(dense2)

classification_output = Dense(10, activation = 'softmax')(dense3)

outputs = Lambda(loss_fn, name = 'loss', output_shape = (1,))(classification_output)
model = Model(inputs = inputs, outputs = outputs)

model.compile(tensorflow.keras.optimizers.Adam(learning_rate = 0.01), loss = lambda y_true, y_pred: y_pred)

**Other info / logs** 
Traceback (most recent call last):
  File ""demo.py"", line 21, in <module>
    model.compile(tensorflow.keras.optimizers.Adam(learning_rate = 0.01), loss = lambda y_true, y_pred: y_pred)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 446, in compile
    self._compile_weights_loss_and_weighted_metrics()
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1592, in _compile_weights_loss_and_weighted_metrics
    self.total_loss = self._prepare_total_loss(masks)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1652, in _prepare_total_loss
    per_sample_losses = loss_fn.call(y_true, y_pred)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py"", line 220, in call
    y_pred, y_true)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/util.py"", line 79, in squeeze_or_expand_dimensions
    is_last_dim_1 = math_ops.equal(1, array_ops.shape(y_pred)[-1])
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py"", line 898, in _slice_helper
    name=name)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py"", line 1064, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 9535, in strided_slice
    shrink_axis_mask=shrink_axis_mask, name=name)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 742, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 595, in _create_op_internal
    compute_device)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 3322, in _create_op_internal
    op_def=op_def)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 1786, in __init__
    control_input_ops)
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 1622, in _create_c_op
    raise ValueError(str(e))
ValueError: slice index -1 of dimension 0 out of bounds. for 'loss_1/loss_loss/strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <-1>, input[2] = <0>, input[3] = <1>.
"
38320,TFLite Conversion: ValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development. ,"Hi I am trying to convert savemodel to tflite but get the below error info:

ValueError: This converter can only   convert a single ConcreteFunction. Converting multiple functions is under   development.

How to solve this problem? thanks.

"
38319,"Keras model compiled with custom loss raises ""object has no attribute '__name__'"" error.","**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:
- TensorFlow installed from (source or
binary): binary
- TensorFlow version (use command below): v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: 10.1, 7.6
- GPU model and memory: RTX 2080 Ti

**Describe the current behavior**
When trying to train a keras model compiled with a custom loss wrapped in a functools.partial, `model.fit` raises the following exception (full traceback is below):

```
/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:261 _get_loss_object
        loss_name = loss.__name__

    AttributeError: 'functools.partial' object has no attribute '__name__'
```

If, on the other hand, the loss is passed as lambda, the function executes as expected.

**Standalone code to reproduce the issue** 

```
from tensorflow import keras
import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist
from functools import partial

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train[...,None] / 255 - 1
x_test = x_test[...,None] / 255 - 1
y_train = keras.utils.to_categorical(y_train)
y_test = keras.utils.to_categorical(y_test)
model = keras.models.Sequential(
  [
    keras.layers.Conv2D(6, (3,3), activation='relu'),
    keras.layers.AveragePooling2D(),
    keras.layers.Conv2D(16, (3,3), activation='relu'),
    keras.layers.AveragePooling2D(),
    keras.layers.Flatten(),
    keras.layers.Dense(units=120, activation='relu'),
    keras.layers.Dense(units=84, activation='relu'),
    keras.layers.Dense(units=10, activation='softmax')
  ]
)

def dummy_loss(y,y2,a):
  return y+y2

model.compile(
  loss=partial(dummy_loss, a=2),
  #loss=lambda v1, v2 : dummy_loss(v1,v2,4),
  optimizer=keras.optimizers.Adam(), metrics={'output_1':'accuracy'})
model.train_on_batch(x_train[:2,...], y_train[:2,...]) # error here
```

**Other info / logs** 

The error seems to come from [this line](https://github.com/tensorflow/tensorflow/blob/33c47b42e04d11622a01ea279ad26e7c3c02a687/tensorflow/python/keras/engine/compile_utils.py#L261), which assumes the existence of an attribute `__name__` on any object passed by the user. The example with lambda works because lambdas have a default `__name__` value `<lambda>`, while `partial`s (and generic callables) do not.

Full traceback of the exception:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1285, in train_on_batch
    logs = train_function(iterator)
  File ""/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 506, in _initialize
    *args, **kwds))
  File ""/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in user code:

    /home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:505 train_function  *
        outputs = self.distribute_strategy.run(
    /home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:467 train_step  **
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:187 __call__
        self._build(y_pred)
    /home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:140 _build
        self._losses = nest.map_structure(self._get_loss_object, self._losses)
    /home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/util/nest.py:617 map_structure
        structure[0], [func(*x) for x in entries],
    /home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/util/nest.py:617 <listcomp>
        structure[0], [func(*x) for x in entries],
    /home/slr/anaconda3/envs/tf_rc/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:261 _get_loss_object
        loss_name = loss.__name__

    AttributeError: 'functools.partial' object has no attribute '__name__'
```"
38318,New project inquiries,"Hello :) 

I am totally new at programming, and wanted some advice. 
I want to create a program using the google maps api. 

Is Python the adequate programming language to use? 

Thanks in advance for your answer :) "
38317,Loss not changing for an adversarial example,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I am trying to port some of the examples from [this NIPS tutorial](https://adversarial-ml-tutorial.org/introduction/) to TensorFlow 2.x. I have been able to port some of them (from chapter 1). However, when creating the perturbation vector, the loss wouldn't change and I unable to figure out why. 

**Describe the expected behavior**

**Standalone code to reproduce the issue** 
Colab Notebook: https://colab.research.google.com/drive/14WEpzKW7IHV4M08QXoTLEUX4qzfSnTB-

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
38316,ABSL clock.h undeclared identifier when building from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.x
- Python version: 3.6
- Installed using virtualenv? pip? conda?: chocolatey
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): LLVM
- CUDA/cuDNN version: 10, 7
- GPU model and memory: Quadro M5000M, 8192MB



**Describe the problem**
Unable to build because of an undeclared identifier in clock.h file from absl. This appears to be an issue with the dependencies for tensorflow.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
When running the configure script, errors appeared for versions of Bazel < 2.0.0 and also > 2.0.0. Hence Bazel 2.0.0 was installed. The configuration step contains the following output:

```
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 2.0.0 installed.
Please specify the location of python. [Default is C:\Python36\python.exe]:


Found possible Python library paths:
  C:\Python36\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Python36\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 10.0 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include
Found cuDNN 7 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 5.2


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /arch:AVX2


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
```

Subsequently, build of the C++ API was attempted with `bazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so`. However, this yielded the following error:

```
.\tensorflow/core/platform/env.h(368): warning: overloaded virtual function ""tensorflow::Env::RegisterFileSystem"" is only partially overridden in class ""tensorflow::EnvWrapper""

external/com_google_absl\absl/types/optional.h(428): warning: expression has no effect
          detected during instantiation of ""const T &absl::lts_2020_02_25::optional<T>::operator*() const & [with T=stream_executor::dnn::AlgorithmDesc]""
.\tensorflow/stream_executor/dnn.h(804): here

external/com_google_absl\absl/types/optional.h(428): warning: expression has no effect
          detected during instantiation of ""const T &absl::lts_2020_02_25::optional<T>::operator*() const & [with T=size_t]""
.\tensorflow/stream_executor/dnn.h(858): here

external/com_google_absl\absl/time/clock.h(70): error C2065: 'Duration': undeclared identifier
external/com_google_absl\absl/time/clock.h(70): error C2146: syntax error: missing ')' before identifier 'duration'
external/com_google_absl\absl/time/clock.h(70): error C2143: syntax error: missing ';' before '{'
external/com_google_absl\absl/time/clock.h(70): error C2447: '{': missing function header (old-style formal list?)
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 1231.090s, Critical Path: 124.55s
INFO: 1539 processes: 1539 local.
FAILED: Build did NOT complete successfully
```

Some suggestions from other posts have been to change the version of Bazel. However, based on the error messages received, it appears that only Bazel 2.0.0 is suitable. Are there any suggestions on how to address this issue?

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
38315,install_tensorflow() error: Collecting package metadata (current_repodata.json): ...working... failed,"**System information**
- Windows 10 Pro:
- TensorFlow installed from (source or binary): use install_tensorflow() in r
- TensorFlow version:
- Python version: Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]:: Anaconda
- Installed using virtualenv? pip? conda?: in r version 3.6.0 
- GPU model and memory: 
Intel(R) HD Graphics 620, 
total available graphics memory: 8259 MB



**Describe the problem**
no able to successfully install tensorflow in r. Have tried various ways, it always shows error as below. We will have training tomorrow. 15 out 18 are not able to install and have the similar errors as me. Thanks for your help!

**Provide the exact sequence of commands / steps that you executed before running into the problem**
![Untitled](https://user-images.githubusercontent.com/46570223/78681144-f7ca5b80-78b1-11ea-8596-5598b13086b0.png)


"
38313,Keras mismatches outputs and targets,"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.6.9

**Describe the current behavior**

- We build a keras model with outputs like `task_a_logits`, `task_a_probs`, `task_b_logits`, `task_b_probs`
- We compile our model by passing a `loss` dict like `{""task_a_logits"": ..., ""task_b_logits"": ...}` (Note that no loss is applied on the probabilities). All losses are applied on the logits.
- We build a dataset that yields tuples like `(x, {""task_a_logits"": ..., ""task_b_logits"": ...}`
- To `keras.fit()`, we pass a dataset that yields targets for each task, also in a dict.

This works without eager execution, but using TF 2.0's defaults, keras gets confused because there are less targets and losses than outputs. [This patch](https://gist.github.com/georgwiese/57568ac518813b9d9f0e6785d8f707fa) fixes the issue.

With the original code and the example above, `loss_fns` and `targets` have two elements and `outs` have four. They get zipped together and then shapes mismatch: `logits and labels must be broadcastable: logits_size=[18,4] labels_size=[18,3]`

**Describe the expected behavior**

Keras matches outputs and targets correctly.

**Standalone code to reproduce the issue** 
I provided a patch that points to the bug in the Keras source code.
"
38312,"Cannot convert lstm with ""stateful=True"" to tflite","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Window 7
- TensorFlow installed from (source or binary):binary
- TensorFlow version (or github SHA if from source):2.2.0-rc0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
model= tf.keras.Sequential()
model.add(tf.keras.layers.Input(shape=(None, 32), batch_size=1,name='input'))
model.add(tf.keras.layers.LSTM(256, return_sequences=True, stateful=True))
model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])
print(model.input)
print(model_ctor.summary())
print(tf.__version__)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
converter.experimental_new_converter = True
tflite_model = converter.convert()
```


**Copy and paste the output here.

```
Tensor(""input_12:0"", shape=(1, None, 32), dtype=float32)
Model: ""sequential_12""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_15 (LSTM)               (1, None, 256)            295936    
=================================================================
Total params: 295,936
Trainable params: 295,936
Non-trainable params: 0
_________________________________________________________________
None
2.2.0-rc0

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
c:\program files\python37\lib\site-packages\tensorflow\python\framework\importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)
    496         results = c_api.TF_GraphImportGraphDefWithResults(
--> 497             graph._c_graph, serialized, options)  # pylint: disable=protected-access
    498         results = c_api_util.ScopedTFImportGraphDefResults(results)

InvalidArgumentError: Input 0 of node sequential_13/lstm_16/AssignVariableOp was passed float from sequential_13/lstm_16/23648:0 incompatible with expected resource.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-26-c3e3cbbc4fe1> in <module>
     10 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,                                    tf.lite.OpsSet.SELECT_TF_OPS]
     11 converter.experimental_new_converter = True
---> 12 tflite_model = converter.convert()

c:\program files\python37\lib\site-packages\tensorflow\lite\python\lite.py in convert(self)
    462     frozen_func, graph_def = (
    463         _convert_to_constants.convert_variables_to_constants_v2_as_graph(
--> 464             self._funcs[0], lower_control_flow=False))
    465     input_tensors = [
    466         tensor for tensor in frozen_func.inputs

c:\program files\python37\lib\site-packages\tensorflow\python\framework\convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)
    705   graph_def, converted_inputs = _convert_variables_to_constants_v2_impl(
    706       func, lower_control_flow, aggressive_inlining)
--> 707   frozen_func = _construct_concrete_function(func, graph_def, converted_inputs)
    708   return frozen_func, graph_def

c:\program files\python37\lib\site-packages\tensorflow\python\framework\convert_to_constants.py in _construct_concrete_function(func, output_graph_def, converted_input_indices)
    404   new_func = wrap_function.function_from_graph_def(output_graph_def,
    405                                                    new_input_names,
--> 406                                                    new_output_names)
    407 
    408   # Manually propagate shape for input tensors where the shape is not correctly

c:\program files\python37\lib\site-packages\tensorflow\python\eager\wrap_function.py in function_from_graph_def(graph_def, inputs, outputs)
    631     importer.import_graph_def(graph_def, name="""")
    632 
--> 633   wrapped_import = wrap_function(_imports_graph_def, [])
    634   import_graph = wrapped_import.graph
    635   return wrapped_import.prune(

c:\program files\python37\lib\site-packages\tensorflow\python\eager\wrap_function.py in wrap_function(fn, signature, name)
    609           signature=signature,
    610           add_control_dependencies=False,
--> 611           collections={}),
    612       variable_holder=holder,
    613       signature=signature)

c:\program files\python37\lib\site-packages\tensorflow\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--> 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

c:\program files\python37\lib\site-packages\tensorflow\python\eager\wrap_function.py in __call__(self, *args, **kwargs)
     84 
     85   def __call__(self, *args, **kwargs):
---> 86     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
     87 
     88   def call_with_variable_creator_scope(self, fn):

c:\program files\python37\lib\site-packages\tensorflow\python\eager\wrap_function.py in wrapped(*args, **kwargs)
     90     def wrapped(*args, **kwargs):
     91       with variable_scope.variable_creator_scope(self.variable_creator_scope):
---> 92         return fn(*args, **kwargs)
     93 
     94     return wrapped

c:\program files\python37\lib\site-packages\tensorflow\python\eager\wrap_function.py in _imports_graph_def()
    629 
    630   def _imports_graph_def():
--> 631     importer.import_graph_def(graph_def, name="""")
    632 
    633   wrapped_import = wrap_function(_imports_graph_def, [])

c:\program files\python37\lib\site-packages\tensorflow\python\util\deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--> 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

c:\program files\python37\lib\site-packages\tensorflow\python\framework\importer.py in import_graph_def(***failed resolving arguments***)
    403       return_elements=return_elements,
    404       name=name,
--> 405       producer_op_list=producer_op_list)
    406 
    407 

c:\program files\python37\lib\site-packages\tensorflow\python\framework\importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)
    499       except errors.InvalidArgumentError as e:
    500         # Convert to ValueError for backwards compatibility.
--> 501         raise ValueError(str(e))
    502 
    503     # Create _DefinedFunctions for any imported functions.

ValueError: Input 0 of node sequential_13/lstm_16/AssignVariableOp was passed float from sequential_13/lstm_16/23648:0 incompatible with expected resource.


```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**

When I try to convert a lstm model with ""stateful=False"", it can convert success.But when I change stateful to True, it convert fail. I need lstm with stateful. How can I do?


"
38311,Loading a quantized TFlite model fails when allocating tensors,"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.2.0-dev20200407**
- Python version: **3.6** / **3.8**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
Loading a `tf.keras` model to tflite causes an error when trying to allocate the tensors for inference. The error is:
`RuntimeError: tensorflow/lite/kernels/conv.cc:337 bias->type != kTfLiteInt32 (9 != 2)Node number 1 (CONV_2D) failed to prepare.`

**Describe the expected behavior**
The exported model should load without error. Inference with the same model in tensorflow is working.

**Standalone code to reproduce the issue** 
Colab gist reproducing the error can be found [here](https://colab.research.google.com/gist/moberweger/d75235cd637c0689e0984cfb3a388144/untitled0.ipynb) Slight modifications to the network structure (ie removing random layers) make the error go, but without any understandable scheme. 

**Other info / logs** 
```
RuntimeError                              Traceback (most recent call last)

<ipython-input-2-6b478363f7a3> in <module>()
     68     interpreter = tf.lite.Interpreter(model_path=""model.tflite"")
     69     input_details = interpreter.get_input_details()
---> 70     interpreter.allocate_tensors()
     71     print(""DONE"")

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)
    241   def allocate_tensors(self):
    242     self._ensure_safe()
--> 243     return self._interpreter.AllocateTensors()
    244 
    245   def _safe_to_run(self):

RuntimeError: tensorflow/lite/kernels/conv.cc:337 bias->type != kTfLiteInt32 (9 != 2)Node number 1 (CONV_2D) failed to prepare.
```
"
38310,google colab  can not install TFX (taxi_pipeline_interactive.ipynb),"1.!pip install ""tfx>=0.21.1,<0.22"" ""tensorflow>=2.1,<2.2"" ""tensorboard>=2.1,<2.2""
Install successfully
2.Restart run time
continuously shown busy
Please help me solved this problems 
"
38309,tf.saved_model.save()/load() Issue,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
-  CNN model is build under subclass 
- OS Platform and Distribution: Win10/Anaconda/Python3.7/Tensorflow 2.1.0
- TensorFlow installed from (source or binary): pip install
- Python version: - 3.7
- CUDA/cuDNN version: Run under CPU mode

**Describe the current behavior**
1. When finish 1 epoch of training for model, use **tf.saved_model.save(model, save_model_dir)**  to save the whole model to the folder
2. Try to load the pb file generated by the system called **saved_model.pb** by command:
    model = tf.saved_model.load(save_model_dir)
3. Try to predict the result by using command:
    model(inputs)
4. The system return following error:
TypeError: '_UserObject' object is not callable
5. From the API document, the example is clearly use the function as I mentioned I think

What's the error then?

**Describe the expected behavior**

**Standalone code to reproduce the issue** 
from __future__ import absolute_import, division, print_function
import tensorflow as tf
import math

# User defined packages
from configuration import IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS, \
    EPOCHS, BATCH_SIZE, save_model_dir, save_every_n_epoch
from prepare_data import generate_datasets, load_and_preprocess_image
from models import mobilenet_v1, mobilenet_v2, mobilenet_v3_large, mobilenet_v3_small, \
    efficientnet, resnext, inception_v4, inception_resnet_v1, inception_resnet_v2, \
    se_resnet, squeezenet, densenet, shufflenet_v2, resnet
from models.model_selection import get_model
from frozen_pb import get_frozen_model


def print_model_summary(network):
    network.build(input_shape=(None, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))
    network.summary()


def process_features(features, data_augmentation):
    image_raw = features['image_raw'].numpy()
    image_tensor_list = []
    for image in image_raw:
        image_tensor = load_and_preprocess_image(image, data_augmentation=data_augmentation)
        image_tensor_list.append(image_tensor)
    images = tf.stack(image_tensor_list, axis=0)
    labels = features['label'].numpy()

    return images, labels


if __name__ == '__main__':
    # GPU settings
    gpus = tf.config.list_physical_devices(""GPU"")
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

    # get the dataset
    train_dataset, valid_dataset, test_dataset, train_count, valid_count, test_count = generate_datasets()

    # create model
    model = get_model()
    print_model_summary(network=model)

    # define loss and optimizer
    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
    # Tried RMSprop for optimizer, the result is not so good, finetune the optimizer to Adam or Momentum
    #optimizer = tf.keras.optimizers.RMSprop(learning_rate = GLOBAL_LEARNING_RATE,
    #                                        momentum = MOMENTUM,
    #                                        name = 'rms_optimizer')
    
    optimizer = tf.keras.optimizers.Adam(lr = 0.0001, decay = 0.4)

    train_loss = tf.keras.metrics.Mean(name='train_loss')
    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

    valid_loss = tf.keras.metrics.Mean(name='valid_loss')
    valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')

    # @tf.function
    def train_step(image_batch, label_batch):
        with tf.GradientTape() as tape:
            predictions = model(image_batch, training=True)
            loss = loss_object(y_true=label_batch, y_pred=predictions)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))

        train_loss.update_state(values=loss)
        train_accuracy.update_state(y_true=label_batch, y_pred=predictions)
        
        return predictions, tf.math.argmax(predictions, axis =1).numpy()

    # @tf.function
    def valid_step(image_batch, label_batch):
        predictions = model(image_batch, training=True)
        v_loss = loss_object(label_batch, predictions)

        valid_loss.update_state(values=v_loss)
        valid_accuracy.update_state(y_true=label_batch, y_pred=predictions)
        
        return tf.math.argmax(predictions, axis =1).numpy()

    # start training
    for epoch in range(EPOCHS):
        step = 0
        for features in train_dataset:
            step += 1
            images, labels = process_features(features, data_augmentation=False)
            predictions, predict_labels = train_step(images, labels)
            print(""Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}"".format(epoch,
                                                                                     EPOCHS,
                                                                                     step,
                                                                                     math.ceil(train_count / BATCH_SIZE),
                                                                                     train_loss.result().numpy(),
                                                                                     train_accuracy.result().numpy()),
                  predictions,
                  predict_labels,
                  labels)

        for features in valid_dataset:
            valid_images, valid_labels = process_features(features, data_augmentation=False)
            valid_step(valid_images, valid_labels)

        print(""Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, ""
              ""valid loss: {:.5f}, valid accuracy: {:.5f}"".format(epoch,
                                                                  EPOCHS,
                                                                  train_loss.result().numpy(),
                                                                  train_accuracy.result().numpy(),
                                                                  valid_loss.result().numpy(),
                                                                  valid_accuracy.result().numpy()))
        train_loss.reset_states()
        train_accuracy.reset_states()
        valid_loss.reset_states()
        valid_accuracy.reset_states()

        if epoch % save_every_n_epoch == 0:
            tf.saved_model.save(model, save_model_dir)
            


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
38308,Tensorflow 2.1.0 build issue ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : Linux , RHEL6
- TensorFlow installed from (source or binary):  Source
- TensorFlow version: 2.1.0
- Python version: 3.6.1
- Bazel version (if compiling from source): 0.28.1
- GCC/Compiler version (if compiling from source): 6.3.0

**Describe the problem**

/tensorflow/tensorflow/compiler/tf2xla/cc/BUILD:8:1: Linking of rule '//tensorflow/compiler/tf2xla/cc:ops/xla_ops_gen_cc' failed (Exit 1) bazel-out/host/bin/external/com_google_absl/absl/time/_objs/time/clock.o:clock.cc:function absl::GetCurrentTimeNanosSlowPath(): error: undefined reference to 'clock_gettime'
bazel-out/host/bin/tensorflow/core/platform/_objs/env_time_impl/env_time.o:env_time.cc:function tensorflow::(anonymous namespace)::PosixEnvTime::NowNanos() const: error: undefined reference to 'clock_gettime'
bazel-out/host/bin/external/com_google_absl/absl/base/_objs/base/sysinfo.o:sysinfo.cc:function absl::base_internal::ReadMonotonicClockNanos(): error: undefined reference to 'clock_gettime'
collect2: error: ld returned 1 exit status Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 5992.496s, Critical Path: 85.82s
INFO: 3149 processes: 3149 local.
FAILED: Build did NOT complete successfully



**Provide the exact sequence of commands / steps that you executed before running into the problem**


bazel build --linkopt='-lrt' --verbose_failures --config=monolithic -c opt //tensorflow:libtensorflow_cc.so
"
38307,Waymo dataset:  module 'tensorflow' has no attribute 'enable_eager_execution',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I am replicating the waymo dataset tutorial code for understanding and learning 
- Using Notebook of Google COLAB (as New user to Google Colab)
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below):  2.1.0 (as installed by default in Google Colab)

**This is the line of code: (sharing since it is available for public view as well by google in it's tutorial)**

!pip3 install waymo-open-dataset
import os
import tensorflow as tf
import math
import numpy as np
import itertools

tf.enable_eager_execution()

from waymo_open_dataset.utils import range_image_utils
from waymo_open_dataset.utils import transform_utils
from waymo_open_dataset.utils import frame_utils
from waymo_open_dataset import dataset_pb2 as open_dataset

**Describe the current behavior**
**Error I get upon executing the above lines of code**

AttributeError                            Traceback (most recent call last)
<ipython-input-8-8d310d062fcf> in <module>()
      6 import itertools
      7 
----> 8 tf.enable_eager_execution()
      9 
     10 from waymo_open_dataset.utils import range_image_utils

AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'
"
38306,Build manylinux whl from source,"**System information**
- OS Platform and Distribution:
centos
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
1.15.0
- Python version:
3.6
- Bazel version (if compiling from source):
0.26
- GCC/Compiler version (if compiling from source):
7.4
- CUDA/cuDNN version:
10.0
- GPU model and memory:



**Describe the problem**
Is it possible to build a manylinux package from source using some extra flags? I'm in a environment where it is much easier to build in one enviroment and test in another. It would be really nice if I can build a manylinux pip whl from source.
"
38305,tf.lookup.StaticHashTable: Cannot convert a Tensor of dtype resource to a NumPy array,"**System information** 

- Have I written custom code:  Yes
- OS Platform and Distribution: macOS 10.14.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version: 3.7.4

**Describe the current behavior**
I have some input that I need to translate,
```
""Dog"" -> 0
""Cat"" -> 1
...
```
and then embed.

I tried to use the `tf.lookup.StaticHashTable` for this, but it works when the input is a `tf.Variable`, not when using `tf.keras.layers.Input`, see the following output: 

```
In [6]:
inputs1 = tf.Variable([[3]], dtype=tf.int64)
translate_and_embed(inputs1)
Out[6]:
<tf.Tensor: shape=(1, 1, 10), dtype=float32, numpy=
array([[[ 0.01451602,  0.04537327, -0.00232627,  0.00584463,
         -0.04128218, -0.03868868,  0.04147324, -0.02444596,
          0.03310961,  0.01144157]]], dtype=float32)>
In [7]:
inputs2 = tf.keras.layers.Input(shape=(1,), dtype=tf.int64)
translate_and_embed(inputs2)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-10-b2c4cfd055e5> in <module>()
      1 inputs2 = tf.keras.layers.Input(shape=(1,), dtype=tf.int64)
----> 2 translate_and_embed(inputs2)

<ipython-input-8-340bf85dfb2f> in translate_and_embed(inputs)
     21         output_dim=10,
     22     )
---> 23     return embedder(translated)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    718     # framework.
    719     if build_graph and base_layer_utils.needs_keras_history(inputs):
--> 720       base_layer_utils.create_keras_history(inputs)
    721 
    722     # Clear eager losses on top level model call.

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in create_keras_history(tensors)
    185     keras_tensors: The Tensors found that came from a Keras Layer.
    186   """"""
--> 187   _, created_layers = _create_keras_history_helper(tensors, set(), [])
    188   return created_layers
    189 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops, created_layers)
    245           else:
    246             with ops.init_scope():
--> 247               constants[i] = backend.function([], op_input)([])
    248       processed_ops, created_layers = _create_keras_history_helper(
    249           layer_inputs, processed_ops, created_layers)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in __call__(self, inputs)
   3733     return nest.pack_sequence_as(
   3734         self._outputs_structure,
-> 3735         [x._numpy() for x in outputs],  # pylint: disable=protected-access
   3736         expand_composites=True)
   3737 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in <listcomp>(.0)
   3733     return nest.pack_sequence_as(
   3734         self._outputs_structure,
-> 3735         [x._numpy() for x in outputs],  # pylint: disable=protected-access
   3736         expand_composites=True)
   3737 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _numpy(self)
    908       return self._numpy_internal()
    909     except core._NotOkStatusException as e:
--> 910       six.raise_from(core._status_to_exception(e.code, e.message), None)
    911 
    912   @property

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.
```

**Describe the expected behavior**
I expect the second function call to return a symbolic tensor.

**Standalone code to reproduce the issue** 
- https://colab.research.google.com/gist/jeanmn/6d2821feaf4828f43524ada267db266f/tf-lookup-statichhashtable-cannot-convert-a-tensor-of-dtype-resource-to-a-numpy-array.ipynb
- https://gist.github.com/jeanmn/6d2821feaf4828f43524ada267db266f

**Other info / logs**
- Similar issue? https://github.com/tensorflow/tensorflow/issues/37441
- Related? #37844 
"
38303,How to limit GPU memory usage in java?,"Hello everyone,
    In python, I can use bellow strategy to limit GPU memory usage.
```
 tf.config.experimental.set_memory_growth(gpu, True)
 tf.config.experimental.per_process_gpu_memory_fraction = 0.5
```
But how can I do this in java?
I training my model with python, then convert model to pb format, last I load pb model in java.
so I want limit GPU memory usage in java . I can't find any document about this.
Any suggestions will help me, thanks."
38302,How to get libhexagon_interface.so for non Android OS (e.g. arm linux)?,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 2.2-rc2
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Qualcomm 845 SOC

**Describe the problem**
Try to get hexagon delegate working on qualcomm 845 soc with arm linux environment. The official supported hexagon delegate only works on Android and the provided libhexagon_interface.so links to Android libraries like liblog.so etc. Could you provide a libhexagon_interface.so that supports pure arm linux? Or could you provide documentation how to build libhexagon_interface.so if I have hexagon sdk from qualcomm downloaded?

Thanks!


**Please provide the exact sequence of commands/steps when you ran into the problem**

"
38300,Master build failed with user local specific GCC,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Both `CentsOS 7.3` and `Ubuntu 18.04` 
- TensorFlow installed from (source or binary): build from source
- TensorFlow version: master (f32548266f25890fc654342296dc112f26ad9bf6)
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): GCC 6.3


**Describe the problem**

**After the upgrade of protobuf, we can't use the user local GCC to build TF master branch**
I have already set the `env` for specified GCC as below
```
export PATH=""/home/chuanqiw/lib/gcc-build-6.3.0/bin:$PATH""
export LD_LIBRARY_PATH=""/home/chuanqiw/lib/gcc-build-6.3.0/lib64:$LD_LIBRARY_PATH""
```
Build command I used:
`bazel build --config=mkl --define=build_with_mkl_dnn_v1_only=true --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512bw --copt=-mavx512dq --copt=-DENABLE_INTEL_MKL_BFLOAT16 -c opt //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
```
ERROR: /home/chuanqiw/tensorflow/gitlab/private-tensorflow/tensorflow/core/data/service/BUILD:308:1: Action tensorflow/core/data/service/worker.grpc.pb.h failed (Exit 1)
bazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)
bazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)
bazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)
bazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/chuanqiw/tensorflow/gitlab/private-tensorflow/tensorflow/lite/toco/python/BUILD:77:1 Action tensorflow/core/data/service/worker.grpc.pb.h failed (Exit 1)
INFO: Elapsed time: 996.555s, Critical Path: 102.83s
INFO: 8409 processes: 8409 local.
FAILED: Build did NOT complete successfully
```
"
38298,"ERROR: Did not get operators, tensors, or buffers in subgraph 0.","@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- Tensorflow version (commit SHA if source): 2.01
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Android

**Describe the problem**
While loading the interpreter from to read the detect.tflite file i'm getting the following error. The link to the model file is https://drive.google.com/open?id=1xxMLG1JLB_6ssCJ8xMOhtJGZo1P_dewc
**Please provide the exact sequence of commands/steps when you ran into the problem**
The detect.tflite file and the respective frozen graph is hereby i'm attaching. 
Also, the code to read the same is as follows:

#include <math.h>
#include <stdio.h>

#include <algorithm>
#include <chrono>
#include <fstream>
#include <memory>
#include <string>
#include <vector>

//#include <gflags/gflags.h>
#include <glog/logging.h>
#include <tensorflow/lite/delegates/gpu/gl_delegate.h>
#include <tensorflow/lite/kernels/register.h>
#include <tensorflow/lite/model.h>
#include <opencv2/core.hpp>
#include <opencv2/imgcodecs.hpp>
#include <opencv2/imgproc.hpp>

//#include ""test_video.hpp""
//#include ""video_encoder.hpp""
#include ""config_parser.h""
#include ""lane_detection.h""

#define IMAGE_MEAN 128.0f
#define IMAGE_STD 128.0f

template <typename T>
T* TensorData(TfLiteTensor* tensor, int batch_index);

template <>
float* TensorData(TfLiteTensor* tensor, int batch_index) {
  int nelems = 1;
  for (int i = 1; i < tensor->dims->size; i++)
    nelems *= tensor->dims->data[i];
  switch (tensor->type) {
    case kTfLiteFloat32:
      return tensor->data.f + nelems * batch_index;
    default:
      LOG(FATAL) << ""Should not reach here!"";
  }
  return nullptr;
}

template <>
uint8_t* TensorData(TfLiteTensor* tensor, int batch_index) {
  int nelems = 1;
  for (int i = 1; i < tensor->dims->size; i++)
    nelems *= tensor->dims->data[i];
  switch (tensor->type) {
    case kTfLiteUInt8:
      return tensor->data.uint8 + nelems * batch_index;
    default:
      LOG(FATAL) << ""Should not reach here!"";
  }
  return nullptr;
}

bool LaneDetection::init(const ConfigParser& config, bool is_quantized) {
  using config_content = std::map<std::string, std::string>;
  config_content config_section;

  LOGI(""LaneDetect: reading config"");
  std::cout << ""LaneDetect: reading config"" << std::endl;
  try {
    config_section = config[""LaneNet""];
  } catch (const std::out_of_range& e) {
    LOGE(e.what());
    LOGE(
        ""Can not get LaneNet section content in config file, please ""
        ""check again"");
    _m_successfully_initialized = false;
    return false;
  }

  if (config_section.find(""dbscan_neighbor_radius"") == config_section.end()) {
    LOGE(""Can not find \""dbscan_neighbor_radius\"" field in config section"");
    _m_successfully_initialized = false;
    return false;
  } else {
    _m_dbscan_eps = std::stof(config_section[""dbscan_neighbor_radius""]);
  }

  if (config_section.find(""dbscan_core_object_min_pts"") ==
      config_section.end()) {
    LOGE(
        ""Can not find \""dbscan_core_object_min_pts\"" field in config ""
        ""section"");
    _m_successfully_initialized = false;
    return false;
  } else {
    _m_dbscan_min_pts =
        std::atoi(config_section[""dbscan_core_object_min_pts""].c_str());
  }

  if (config_section.find(""pix_embedding_feature_dims"") ==
      config_section.end()) {
    LOGE(
        ""Can not find \""pix_embedding_feature_dims\"" field in config ""
        ""section"");
    _m_successfully_initialized = false;
    return false;
  } else {
    _m_lanenet_pix_embedding_feature_dims =
        std::atoi(config_section[""pix_embedding_feature_dims""].c_str());
  }

  if (config_section.find(""model_file_path"") == config_section.end()) {
    LOGE(""Can not find \""model_file_path\"" field in config section"");
    _m_successfully_initialized = false;
    return false;
  } else {
    _m_lanenet_model_file_path = config_section[""model_file_path""];
  }

  LOGI(""LaneDetect: read config file"");
  std::cout << ""LaneDetect: read config file"" << std::endl;

  ///////////////////////////////
  model_ = tflite::FlatBufferModel::BuildFromFile(
      _m_lanenet_model_file_path.c_str());
  if (!model_) {
    LOGI(""LaneDetect: Failed to load model: %s"",
         _m_lanenet_model_file_path.c_str());
    std::cout << ""LaneDetect: Failed to load model: ""
              << _m_lanenet_model_file_path << std::endl;
    return false;
  }

  // Create interpreter.
  tflite::ops::builtin::BuiltinOpResolver resolver;
  tflite::InterpreterBuilder(*model_, resolver)(&interpreter_);
  if (!interpreter_) {
    LOGI(""LaneDetect: Failed to create interpreter!"");
    std::cout << ""LaneDetect: Failed to create interpreter!"" << std::endl;
    return false;
  }
  /* if (interpreter_->AllocateTensors() != kTfLiteOk)
  {
      LOG(ERROR) << ""Failed to allocate tensors!"";
      return false;
  } */
  interpreter_->SetNumThreads(1);

  const TfLiteGpuDelegateOptions options = {
      .metadata = NULL,
      .compile_options =
          {
              .precision_loss_allowed = 1,  // FP16
              .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,
              .dynamic_batch_enabled = 0,  // Not fully functional yet
          },
  };
  auto* delegate = TfLiteGpuDelegateCreate(&options);
  if (interpreter_->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {
    interpreter_->UseNNAPI(true);
    LOGI(""LaneDetect: gpu delegate failed ! adding nnAPI support"");
    std::cout << ""LaneDetect: gpu delegate failed ! adding nnAPI support""
              << std::endl;
  }

  // Find input tensors.
  if (interpreter_->inputs().size() != 1) {
    LOGI(""LaneDetect: Graph needs to have 1 and only 1 input!"");
    std::cout << ""LaneDetect: Graph needs to have 1 and only 1 input!""
              << std::endl;
    return false;
  }
  input_tensor_ = interpreter_->tensor(interpreter_->inputs()[0]);
  if (is_quantized) {
    if (input_tensor_->type != kTfLiteUInt8) {
      LOGI(""LaneDetect: Quantized graph's input should be kTfLiteUInt8!"");
      std::cout << ""LaneDetect: Quantized graph's input should be kTfLiteUInt8!""
                << std::endl;
      return false;
    }
  } else {
    if (input_tensor_->type != kTfLiteFloat32) {
      LOGI(""LaneDetect: Quantized graph's input should be kTfLiteFloat32!"");
      std::cout
          << ""LaneDetect: Quantized graph's input should be kTfLiteFloat32!""
          << std::endl;
      return false;
    }
  }

  // Find output tensors.
  if (interpreter_->outputs().size() != 2) {
    LOGI(
        ""LaneDetect: Graph needs to have 2 and only 2 outputs! It has %d ""
        ""outputs though!!!"",
        interpreter_->outputs().size());
    std::cout << ""LaneDetect: Graph needs to have 2 and only 2 outputs! It has ""
              << interpreter_->outputs().size() << "" outputs though!!!""
              << std::endl;
    return false;
  }
  output_binary_mask_ = interpreter_->tensor(interpreter_->outputs()[0]);
  output_instance_mask_ = interpreter_->tensor(interpreter_->outputs()[1]);

  _m_input_node_size_host.width = width();
  _m_input_node_size_host.height = height();

  return true;
}

void LaneDetection::preprocess(const cv::Mat& input_image,
                               cv::Mat& output_image) {
  if (input_image.type() != CV_32FC3) {
    input_image.convertTo(output_image, CV_32FC3);
  } else {
    input_image.copyTo(output_image);
  }
  // BGR2RGB
  cv::cvtColor(output_image, output_image, cv::COLOR_BGR2RGB);

  LOGI(""LaneDetect: output image size : %d x %d  check size : %d x %d"",
       output_image.size().width, output_image.size().height,
       _m_input_node_size_host.width, _m_input_node_size_host.height);

  // resizing to the input size of the model
  if (output_image.size() != _m_input_node_size_host) {
    cv::resize(output_image, output_image, _m_input_node_size_host);
  }

  // // making mean 0 and range 2
  // cv::divide(output_image, cv::Scalar(127.5, 127.5, 127.5), output_image);
  // cv::subtract(output_image, cv::Scalar(1.0, 1.0, 1.0), output_image);
  return;
}

void LaneDetection::FeedInMat(const cv::Mat& mat, int batch_index) {
  switch (input_tensor_->type) {
    case kTfLiteFloat32: {
      float* dst = TensorData<float>(input_tensor_, batch_index);
      const int row_elems = width() * input_channels();
      for (int row = 0; row < height(); row++) {
        const uchar* row_ptr = mat.ptr(row);
        for (int i = 0; i < row_elems; i++) {
          dst[i] = (row_ptr[i] - IMAGE_MEAN) / IMAGE_STD;
        }
        dst += row_elems;
      }
    } break;
    case kTfLiteUInt8: {
      uint8_t* dst = TensorData<uint8_t>(input_tensor_, batch_index);
      const int row_elems = width() * input_channels();
      for (int row = 0; row < height(); row++) {
        memcpy(dst, mat.ptr(row), row_elems);
        dst += row_elems;
      }
    } break;
    default:
      LOG(FATAL) << ""Should not reach here!"";
  }
}

void LaneDetection::detect(cv::Mat& image,
                           cv::Mat& binary_seg_result,
                           cv::Mat& instance_seg_result) {
  // preprocess
  cv::Mat input_image_copy;
  preprocess(image, input_image_copy);

  FeedInMat(input_image_copy, 0);
  if (interpreter_->Invoke() != kTfLiteOk) {
    LOGI(""LaneDetect: Unable to invoke TfLite"");
    return;
  }
  std::cout << ""Lane detection ran successfully"" << std::endl;
}






"
38297,Cant install tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):Pip
- TensorFlow version:2.1.0
- Python version:3.7
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I cant install tensorflow. It says that an error occured and tensorflow isn't found. 
**Provide the exact sequence of commands / steps that you executed before running into the problem**
python pip install tensorflow

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
38296,"tf.keras custom layer does not use ""compute_output_shape""","**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 
- Python version: v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: `Driver Version: 440.33.01    CUDA Version: 10.2`
- GPU model and memory: GTX 1080 Ti 11162MiB ram

**Describe the current behavior**

When implementing a custom layer where the output shape is not computable directly from the code within the `call` method, the `compute_output_shape` function is _not_ used to determine the output shape.

This causes issues when passing to a layer which must know some of the shape, e.g. a `Conv*D` layer which needs to know the number of channels ahead of time.

**Describe the expected behavior**

The `compute_output_shape` function _is_ used to determine the output shape.

**Standalone code to reproduce the issue** 
```python
import tensorflow as tf


class Spectrogram(tf.keras.layers.Layer):
    def __init__(self, num_freqs, max_freq, **kwargs):
        super(Spectrogram, self).__init__(**kwargs)

        self.num_freqs = num_freqs
        self.max_freq = max_freq

        self.input_spec = [
            tf.keras.layers.InputSpec(ndim=2), tf.keras.layers.InputSpec(ndim=2)
        ]

    def call(self, x_fs):
        x, fs = x_fs
        nfft = tf.cast(
            fs[0,0] * (self.num_freqs - 1) / self.max_freq,
            tf.int32
        )
        y = tf.signal.stft(x, nfft, 256, nfft, pad_end=True)
        y = tf.sqrt(tf.abs(y))[:, :, :self.num_freqs]
        return y

    def compute_output_shape(self, input_shape):
        return (input_shape[0], None, self.num_freqs)


signal = tf.keras.layers.Input(shape=(None,))
fs = tf.keras.layers.Input(shape=(1,))
x = Spectrogram(257, 10_000)([signal, fs])
y = tf.keras.layers.Conv1D(16, 3)(x)

tf.keras.models.Model([signal, fs], [y]).summary()
```

**Logs**

```
Traceback (most recent call last):
  File ""chorus/repro.py"", line 32, in <module>
    y = tf.keras.layers.Conv1D(16, 3)(x)
  File ""/home/kevin/.pyenv/versions/chorus/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 897, in __call__
    self._maybe_build(inputs)
  File ""/home/kevin/.pyenv/versions/chorus/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 2416, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File ""/home/kevin/.pyenv/versions/chorus/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 153, in build
    input_channel = self._get_input_channel(input_shape)
  File ""/home/kevin/.pyenv/versions/chorus/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 293, in _get_input_channel
    raise ValueError('The channel dimension of the inputs '
ValueError: The channel dimension of the inputs should be defined. Found `None`.
```

**Other info**

This was originally opened in #19961 but was unfortunately closed as ""not a bug"", but I'm pretty sure this is not expected behavior according to the [keras docs](https://keras.io/layers/writing-your-own-keras-layers/)."
38295,tensorflow1.13 add an op which use mkldnn cannot build sucessfully,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, in tensorflow/core/user_ops addop
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows10
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.13
- **Python version**:3.6
- **Bazel version (if compiling from source)**:0.20.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:None
- **GPU model and memory**:None
- **Exact command to reproduce**:
I just add an matmul op which use a function in mkldnn,and I can build successfully with `bazel build  -c opt --copt=-msse4.1 --copt=-msse4.2  tensorflow:libtensorflow.so`  
But, when I build the target with `bazel build  -c opt --copt=-msse4.1 --copt=-msse4.2   //tensorflow/tools/pip_package:build_pip_package` , I got an error
`ERROR: D:/tf_install/tensorflow_addop/tensorflow/python/BUILD:4057:1: in cmd attribute of genrule rule //tensorflow/python:gen__pywrap_tensorflow_internal.pyd: variable '$<' : more than one input file. Since this rule was created by the macro 'tf_py_wrap_cc', the error might have been caused by the macro implementation in D:/tf_install/tensorflow_addop/tensorflow/tensorflow.bzl:1704:15
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '//tensorflow/python:gen__pywrap_tensorflow_internal.pyd' failed; build aborted`

I just add some codes in `tensorflow/python/user_ops/user_ops.py`
`@tf_export(v1=['user_ops.own_dnnl_mul'])
def own_dnnl_mul(arg1, arg2):
  """"""Example of overriding the generated code for an Op.""""""
return _gen_user_ops.own_dnnl_mul(arg1, arg2)
`"
38294,Tensorflow keras.callbacks.ModelCheckPoint with wrong path doesn't save anything instead of raising error,"**System information** 
- Have I written custom code: NO
- OS Platform and Distribution: Ubuntu 18.04
- Mobile device: N/A
- TensorFlow installed from: `pip install --upgrade tf-nightly`
- TensorFlow version: 2.2.0-dev20200403
- Python version: 3.7.7
- CUDA version: cuda 10.1
- GPU model and memory: NVIDIA 2080 and TITAN X


**Describe the current behavior**
```
model_checkpoint = tf.keras.callbacks.ModelCheckpoint('Output/task1/Model/Task1_ResNet50_ImageNet/Task1_ResNet50-{epoch:02d}-{val_loss:.2f}.h5', verbose=1, save_best_only=True)
```
**The path here in ModelCheckpoint is a wrong path since there is NO sub-folder named ""Task1_ResNet50_ImageNet"" in ""Model"" folder. While after 30 epochs training, there is no error raised, neither model saved.**

**Describe the expected behavior**
It was supposed to raise error since there is no such a directory to save modelcheckpoint. While after 3 days training, I got nothing saved! It wastes me lots of time.

**Standalone code to reproduce the issue** 
Since my own code is too complicated, I use the example code from [Keras.io](https://keras.io/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification) to explain this problem: [stand alone gist](https://colab.research.google.com/gist/MakeCent/5ac59661a91e1d5126b89e6b0026e1c0/38294.ipynb)

**Other info / logs**
During the training, it print save model information like below, which looks like working well:
`Epoch 00008: val_loss improved from 9.19709 to 8.58308, saving model to Output/task1/Model/Task1_ResNet50_ImageNet/Task1_ResNet50_ImageNet-08-8.58.h5`
"
38293,Tensorflow 2.0 make_csv_dataset reads files too slowly,"It takes 10 seconds to read a batch of 1024 data from the csv file. After setting the num_parallel_reads parameter to 2, the time becomes 5 seconds. But continue to increase the reading thread, the time will not be reduced. Am I setting something wrong?"
38287,Embedding layer ~10x slower in tf.function,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Standard colab environment
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: N/A
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): tf-nightly pip
- Python version: - Bazel
version (if compiling from source): N/A
- GCC/Compiler version (if compiling from
source): N/A
- CUDA/cuDNN version: - GPU model and memory: N/A

This is a follow-up to https://github.com/tensorflow/tensorflow/issues/29075#issuecomment-608976001, to capture a separate issue identified by @szc11121.

**Describe the current behavior**

```
import time
import tensorflow as tf
tf.__version__

class Toymodel(tf.keras.Model):
    def __init__(self, use_embedding):
        super(Toymodel, self).__init__()
        if use_embedding:
          self.emb = tf.keras.layers.Embedding(100000, 512)
        self.use_embedding = use_embedding
        self.fc = tf.keras.layers.Dense(1)

    def call(self, constant_input):
        if self.use_embedding:
          constant_input_emb = self.emb(constant_input)
        else:
          constant_input_emb = tf.expand_dims(constant_input, -1)
        logit = self.fc(constant_input_emb)
        output = tf.keras.activations.sigmoid(logit)
        return logit, output

def run_step(model, optimizer, constant_input, y):
    with tf.GradientTape() as tape:
        logit, output = model(constant_input)
        loss = tf.reduce_mean(
                        tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
                                                                labels=tf.cast(y, dtype=tf.float32)))
    gradient = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradient, model.trainable_weights))
    return loss, output

def run_loop(in_graph, use_embedding):
  model = Toymodel(use_embedding)
  opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.0)

  if in_graph:
    f = tf.function(run_step)
  else:
    f = run_step

  for _ in range(20):
      start = time.time()
      loss, output  = f(model, opt, tf.random.uniform([100], minval=0, maxval=100000), tf.random.uniform([100, 1]))
      end = time.time()
      print(end - start)

print('Eager, no embedding')
run_loop(False, False)
print('Graph, no embedding')
run_loop(True, False)
print('Eager, embedding')
run_loop(False, True)
print('Graph, embedding')
run_loop(True, True)
```

**Describe the expected behavior**

The graph versions sound be in faster in both cases (ignoring initial calls). However, the case which includes an embedding layer shows a 10x performance drop compared to the same model running eagerly:

Eager, no embedding
0.010893821716308594
0.0032660961151123047
...
Graph, no embedding
0.34873294830322266
0.0013270378112792969
...
Eager, embedding
0.4676947593688965
0.006254434585571289
...
Graph, embedding
0.5206460952758789
0.07123064994812012
..."
38285,Unsupported Full-Integer TensorFlow Lite models in TF 2,"**Describe the issue**
**In TF2, the full-integer quantized models produced by the TFLite Converter can only have *float* input and output type. This is a blocker for users who require *int8* or *uint8* input and/or output type.**

**UPDATE**: We now support this workflow.

**End-to-End Tutorial**: https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb

**Only TFLite Conversion: Convert TF Models to TFLite Full-Integer models**
You can refer to the code [here](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only), also given below:


```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
  for _ in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    yield [input]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
tflite_model = converter.convert()
```

**Only TFLite Inference: Run inference on the TFLite model**
**Note** that the one caveat with integer-only models is this -- you need to manually map (aka quantize) the float inputs to integer inputs during inference. To understand how this can be done -- refer to the equation provided in [TensorFlow Lite 8-bit quantization specification](https://www.tensorflow.org/lite/performance/quantization_spec) document and it's equivalent code in python below:
```
import numpy as np
import tensorflow as tf

# Input to the TF model are float values in the range [0, 10] and of size (1, 100)
np.random.seed(0)
tf_input = np.random.uniform(low=0, high=10, size=(1, 100)).astype(np.float32)

# Output of the TF model.
tf_output = keras_model.predict(input)

# Output of the TFLite model.
interpreter = tf.lite.Interpreter(model_content=tflite_model) 
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()[0]
# Manually quantize the input from float to integer
scale, zero_point = input_details['quantization']
tflite_integer_input = tf_input / scale + zero_point
tflite_integer_input = tflite_integer_input.astype(input_details['dtype'])
interpreter.set_tensor(input_details['index'], tflite_integer_input)
interpreter.invoke()
output_details = interpreter.get_output_details()[0]
tflite_integer_output = interpreter.get_tensor(output_details['index'])
# Manually dequantize the output from integer to float
scale, zero_point = output_details['quantization']
tflite_output = tflite_integer_output.astype(np.float32)
tflite_output = (tflite_output - zero_point) * scale
 
# Verify that the TFLite model's output is approximately (expect some loss in 
# accuracy due to quantization) the same as the TF model's output
assert np.allclose(tflite_output, tf_output, atol=1e-04) == True
```
"
38281,TFlite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```

**The output from the converter invocation**

```
# Copy and paste the output here.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
38280,TF2.2.0rc2: exception if shape of input to tf.linag.set_diag not fully specified ,"Hello, 

it's now possible to use tf.linalg.set_diag to set not only the diagonal of a matrix, but also bands (i.e a diagonal + sub- and superdiagonals) using the parameter ""k"". 

I would like to use this within a keras layer, yet when the model containing that layer is built, an exception (see below) is raised as long as the shape of the input parameter of tf.linalg.set_diag is not fully specified (i.e. the batch size should be dynamic, depending on the input at ""runtime""). If I prespecify the axis size, everything seems to work fine
 
**Standalone code to reproduce the issue** 
```python 
import tensorflow as tf
import tensorflow.keras as k

def construct_band_mat(band):
    # Dynamic batch size does not work
    batch_size = tf.shape(band)[0]

    # hard coded batch size works
    #batch_size = 3

    base = tf.zeros([batch_size, 5, 5])
    return tf.linalg.set_diag(base, band, k=(-1, 1))

input = k.Input(shape=[3, 5])
output = k.layers.Lambda(construct_band_mat)(input)

model = k.models.Model(inputs=[input], outputs=[output])

band_in = tf.ones([3, 3, 5], dtype=tf.float32)
print(model(band_in))
```

**System information** 
- Have I written custom code:  yes 
- OS Platform and Distribution: Ubuntu 18.04 
- TensorFlow version: 2.2.0rc2
- TensorFlow installed from: pip
- Python version:  3.6.9 
- CUDA/cuDNN version: None, CPU only 



**Describe the current behavior**

If the shape of the ""input"" is fully specified when the model is build, the function acts as expected.
If the size of one axis (e.g. the batch axis) of ""input"" is unspecified during build, an exception is raised :

Traceback (most recent call last):
  File ""/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in _create_c_op
    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 3 and 4 for '{{node lambda/set_diag}} = MatrixSetDiagV3[T=DT_FLOAT, align=""RIGHT_LEFT""](lambda/zeros, input_1, lambda/set_diag/k)' with input shapes: [?,5,5], [?,3,5], [2].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/philipp/projects/RKN_internal/banded_test.py"", line 16, in <module>
    output = k.layers.Lambda(construct_band_mat)(input)
  File ""/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 922, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py"", line 888, in call
    result = self.function(inputs, **kwargs)
  File ""/home/philipp/projects/RKN_internal/banded_test.py"", line 13, in construct_band_mat
    return tf.linalg.set_diag(base, band, k=(-1, 1))
  File ""/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 2654, in matrix_set_diag
    input=input, diagonal=diagonal, k=k, align=align, name=name)
  File ""/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 5864, in matrix_set_diag_v3
    name=name)
  File ""/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 595, in _create_op_internal
    compute_device)
  File ""/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3327, in _create_op_internal
    op_def=op_def)
  File ""/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1817, in __init__
    control_input_ops, op_def)
  File ""/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1657, in _create_c_op
    raise ValueError(str(e))
ValueError: Shapes must be equal rank, but are 3 and 4 for '{{node lambda/set_diag}} = MatrixSetDiagV3[T=DT_FLOAT, align=""RIGHT_LEFT""](lambda/zeros, input_1, lambda/set_diag/k)' with input shapes: [?,5,5], [?,3,5], [2].

Process finished with exit code 1

Note that it says ""Shapes must be equal rank, but are 3 and 4 "" but then "" input shapes: [?,5,5], [?,3,5], [2]"" i.e. two shapes of rank 3.

Also note that the first dimension of the diagonal argument is also unspecified, which seems to work fine.

**Describe the expected behavior**
tf.linalg.set_diag should be able to handle inputs whose size is not fully specified while the graph is build up. 
(If there is some fundamental reason why this is not possible, which I am currently not seeing, the documentation should be adapted and the exception should be more expressive. ) 


Best wishes,
Philipp"
38279,MutableGraphView::SortTopologically error with tf.nn.conv2d in custom RNN cell,"**System information**

- Have I written custom code: yes
- OS Platform and Distribution: Ubuntu 18.04 LTS
- TensorFlow installed from: binary (`conda install tensorflow-gpu=2.1`)
- TensorFlow version: 2.1.0
- Python version: 3.7.7
- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6
- GPU model and memory: GeForce GTX 1080, 8 GB

**Describe the current behavior**

Calling `predict` on a model containing a custom RNN layer whose cell calls `tf.nn.conv2d` results in the following error being printed to the console:
```
2020-04-06 12:40:50.843591: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] layout failed: Invalid argument: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'Func/sequential/rnn/while/body/_1/input/_55' -> 'sequential/rnn/while/body/_1/add'}.
```
The prediction proceeds (and appears to be correct), but has poor performance.

This does not occur if the `tf.nn.conv2d` is replaced with other similar operations (`tf.nn.conv1d` for example).

**Describe the expected behavior**

The expected behavior is for no error to be produced.

**Standalone code to reproduce the issue** 

The following code produces the error:
```python
import tensorflow as tf


class CustomCell(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        self.state_size = [tf.TensorShape((16, 16, 1))]
        super(CustomCell, self).__init__(**kwargs)

    def call(self, inputs, states, **kwargs):
        output = states[0] + tf.nn.conv2d(inputs, tf.ones((3, 3, 1, 1)), (1, 1), ""SAME"")
        new_state = output
        return output, new_state


model = tf.keras.models.Sequential()
model.add(tf.keras.layers.RNN(CustomCell(), batch_input_shape=(1, 1, 16, 16, 1)))

# Error here
model.predict(tf.ones(model.input_shape))
```

If the `conv2d` is replaced with `conv1d`, however, no error occurs:
```python
import tensorflow as tf


class CustomCell(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        self.state_size = [tf.TensorShape((16, 1))]
        super(CustomCell, self).__init__(**kwargs)

    def call(self, inputs, states, **kwargs):
        output = states[0] + tf.nn.conv1d(inputs, tf.ones((3, 1, 1)), (1,), ""SAME"")
        new_state = output
        return output, new_state


model = tf.keras.models.Sequential()
model.add(tf.keras.layers.RNN(CustomCell(), batch_input_shape=(1, 1, 16, 1)))

# No error
model.predict(tf.ones(model.input_shape))
```"
38272,TPU Bug when using categorical_crossentropy,"ResourceExhaustedError: {{function_node _inferencedistributedfunction153395}} Compilation failure: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.

Largest program allocations in vmem:

XLA label: register allocator spill slots
Allocation type: scoped

XLA label: %fusion.11963 = f32[4,192,1024]{2,0,1:T(4,128)} fusion(f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, …(+188)), kind=kLoop, calls=%fused_computation.10858
Allocation type: scoped

XLA label: %fusion.11963 = f32[4,192,1024]{2,0,1:T(4,128)} fusion(f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, …(+188)), kind=kLoop, calls=%fused_computation.10858
Allocation type: scoped

XLA label: %fusion.11963 = f32[4,192,1024]{2,0,1:T(4,128)} fusion(f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, …(+188)), kind=kLoop, calls=%fused_computation.10858
Allocation type: scoped

XLA label: %fusion.11963 = f32[4,192,1024]{2,0,1:T(4,128)} fusion(f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, …(+188)), kind=kLoop, calls=%fused_computation.10858
Allocation type: scoped

TPU compilation failed
 [[{{node tpu_compile_succeeded_assert/_5292462253713114861/_5}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add reporttensorallocationsuponoom to RunOptions for current allocation info."
38271,ImportError: DLL load failed: The specified module could not be found.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Source and then through PyCharm
- **TensorFlow version (use command below)**: 2.1.0
- **Python version**: 3.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:  10.1 / 7.6.5.32
- **GPU model and memory**: GeForce 1050
- **Exact command to reproduce**: Keras DQN agent with tensorflow gpu


### Describe the problem
I run my agent and this error comes out

### Source code / logs
```
import random
import gym
import gym_zombiediceAI
import numpy as np
import os
from collections import deque
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
```
.
.
.
```
if __name__ == ""__main__"":
    env = gym.make('zb-v0')
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n
    agent = DQNAgent(state_size, action_size)
    # agent.load(""./save/cartpole-dqn.h5"")
    done = False
    batch_size = 32

    for e in range(EPISODES):
        state = env.reset()
        state = np.reshape(state, [1, state_size])
        for time in range(500):
            # env.render()
            action = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            reward = reward if not done else -10
            next_state = np.reshape(next_state, [1, state_size])
            agent.memorize(state, action, reward, next_state, done)
            state = next_state
            if done:
                print(""episode: {}/{}, score: {}, e: {:.2}""
                      .format(e, EPISODES, time, agent.epsilon))
                break
            if len(agent.memory) > batch_size:
                loss = agent.replay(batch_size)
                # Logging training loss every 10 timesteps
                if time % 10 == 0:
                    print(""episode: {}/{}, time: {}, loss: {:.4f}""
                        .format(e, EPISODES, time, loss))

```
Error Message:
```
C:\Users\Krakas\AppData\Local\Programs\Python\Python37\python.exe C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\pydevconsole.py --mode=client --port=52043
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\Users\\Krakas\\gym-zombiediceAI', 'C:/Users/Krakas/gym-zombiediceAI'])
Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 7.8.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.8.0
Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)] on win32
runfile('C:/Users/Krakas/gym-zombiediceAI/gym_zombiediceAI/envs/kerasAgent.py', wdir='C:/Users/Krakas/gym-zombiediceAI/gym_zombiediceAI/envs')
Using TensorFlow backend.
Traceback (most recent call last):
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-85efe6c332ce>"", line 1, in <module>
    runfile('C:/Users/Krakas/gym-zombiediceAI/gym_zombiediceAI/envs/kerasAgent.py', wdir='C:/Users/Krakas/gym-zombiediceAI/gym_zombiediceAI/envs')
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:/Users/Krakas/gym-zombiediceAI/gym_zombiediceAI/envs/kerasAgent.py"", line 7, in <module>
    from keras.models import Sequential
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Krakas\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\192.7142.42\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Krakas\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Krakas\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors
for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
pip show tensorflow
Name: tensorflow
Version: 2.1.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: c:\users\krakas\appdata\roaming\python\python37\site-packages
Requires: absl-py, six, tensorflow-estimator, keras-preprocessing, google-pasta, astor, termcolor, gast, grpcio, numpy, scipy, opt-einsum, protobuf, wrapt, wheel, tensorboard, keras-applications
Required-by: huskarl
Note: you may need to restart the kernel to use updated packages.
```"
38270,Train simple audio recognition ,"@petewarden 
@tensorflow/micro

**System information**

- OS Platform and Distribution -Windows 8
- TensorFlow installed from (source or binary): - 
-TensorFlow version - 2.2.0.dev20200210
- Python version: - 3.6 version (if compiling from source):
- CUDA/cuDNN version: - GPU not supported in my system trying to train model in colab


###  Describe the current behavior
I am trying to train a simple audio recognition model as described in book ""TinyML"" . I am using colab to train the model. 
First I got the error while Install Dependencies ""ERROR: Could not find a version that satisfies the requirement tf-nightly-gpu==1.15.0.dev20190729 (from versions: 2.2.0.dev20200210, 2.2.0.dev20200211, ...)
ERROR: No matching distribution found for tf-nightly-gpu==1.15.0.dev20190729""
As temsorflow version 1.5 is not supported is replaced the version by 2.2.0.dev20200210 , this error got resolved.

Again while ""Begin Training"" I got error ""Traceback (most recent call last):
  File ""tensorflow/tensorflow/examples/speech_commands/train.py"", line 81, in <module>
    import input_data
  File ""/content/tensorflow/tensorflow/examples/speech_commands/input_data.py"", line 35, in <module>
    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio
ModuleNotFoundError: No module named 'tensorflow.contrib""

Please provide the exact sequence of commands/steps to solve above problem and train the simple audio in colab.



"
38269,TF 2.2.0-rc2 tf.Keras API:  model.loss_functions no longer exists,"

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  `Yes`
- OS Platform and Distribution:  `Ubuntu 18.04`
- TensorFlow installed from (binary): `v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2`
- CUDA/cuDNN version: `cuda-10.1`

**Describe the current behavior**
In `tf.Keras` API in  TensorFlow 2.2.0, `model.loss_functions` no longer exists.  Instead the loss functions are buried and seem to be only accessible with code like this:
```python
model.compiled_loss._get_loss_object(model.compiled_loss._losses).fn
```
Being able to easily access loss functions associated with a model is important for many applications that are built on TensorFlow.  Moreover, this is an undocumented breaking change.

**Describe the expected behavior**
The loss functions associated with a model should be more directly and easily accessible like `model.loss_functions`, which is the way loss functions are accessed in TensorFlow 2.1.0.

**Standalone code to reproduce the issue** 
```python
from tensorflow.keras.models import Sequential  
from tensorflow.keras.layers import Dense, Activation  
model = Sequential()  
model.add(Dense(10, input_dim=10, activation='softmax'))  
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) 

# works in TF 2.2.0-rc2, but does NOT work in previous versions like 2.1.0
model.compiled_loss._get_loss_object(model.compiled_loss._losses).fn

# does NOT work in TF 2.2.0-rc2, but works in TF 2.1.0
model.loss_functions[0].fn

"
38265,java.lang.IllegalArgumentException: Cannot assign a device for operation,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 


- Linux version : 3.10.0-862.el7.x86_64 
- TensorFlow installed from : binary
- TensorFlow version (use command below): tensorflow_gpu==1.14.0
- Python version: - 2.7.5

- CUDA/cuDNN version: NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1
- GPU model and memory:


**Describe the current behavior**
I convert a pre-trained tensorflow model to .pb file with python code as bellow

`
import tensorflow as tf
from argparse import ArgumentParser

def main():
    parser = ArgumentParser()
    parser.add_argument('--checkpoint', type=str,
                        dest='checkpoint',
                        help='dir or .ckpt file to load checkpoint from',
                        metavar='CHECKPOINT', required=True)
    parser.add_argument('--model', type=str,
                        dest='model',
                        help='.meta for your model',
                        metavar='MODEL', required=True)
    parser.add_argument('--out-path', type=str,
                        dest='out_path',
                        help='model output directory',
                        metavar='MODEL_OUT', required=True)
    opts = parser.parse_args()
    tf.reset_default_graph()
    saver = tf.train.import_meta_graph(opts.model)
    #builder = tf.saved_model.builder.SavedModelBuilder(opts.out_path)
    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:
        # Restore variables from disk.
        saver.restore(sess, opts.checkpoint)
        print(""Model restored."")
        #builder.add_meta_graph_and_variables(sess,['tfckpt2pb'],strip_default_attrs=False)
        #builder.save()
        constant_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, ['Tower_0/parsing_fc/BiasAdd','Tower_0/parsing_rf_fc/BiasAdd','Tower_0/edge_rf_fc/BiasAdd'])

        with tf.gfile.FastGFile(opts.out_path, mode='wb') as f:
                f.write(constant_graph.SerializeToString())
                print(""pb Model saved."")



if __name__ == '__main__':
    main()

   ` and I import the .pb model with java code as bellow
 `         //import model
              byte[] graphBytes = IOUtils.toByteArray(new FileInputStream(MODEL_PATH));

              graph.importGraphDef(graphBytes);


              //create session
              try(Session session = new Session(graph)){

                  ConfigProto config = ConfigProto.newBuilder()
                          .setGpuOptions(GPUOptions.newBuilder().setAllowGrowth(true))
                          .build();
                   //get the output
                  Tensor<?> output = session.runner()
                          .setOptions(config.toByteArray())
                          .feed(""Tower_0/strided_slice"", imageTensor)
                          .fetch(""Tower_0/parsing_fc/BiasAdd"").run().get(0);
                  System.out.println(output);
              }`

then error as bellow
`here are error message 2020-04-06 12:13:50.269556: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA 2020-04-06 12:13:50.281419: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz 2020-04-06 12:13:50.288167: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f968ded1db0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-04-06 12:13:50.288206: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version java.lang.IllegalArgumentException: Cannot assign a device for operation Tower_0/strided_slice: {{node Tower_0/strided_slice}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device. The requested device appears to be a GPU, but CUDA is not enabled. [[Tower_0/strided_slice]] at org.tensorflow.Session.run(Native Method) at org.tensorflow.Session.access$100(Session.java:48) at org.tensorflow.Session$Runner.runHelper(Session.java:326) at org.tensorflow.Session$Runner.run(Session.java:276) at `

**Describe the expected behavior**
  it can correctly run up 

"
38264,Train a Simple Audio Recognition Model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
38263,Add a Mutable Hash Table of Ragged Tensors ,"**System information**
- TensorFlow version 2.1:
- Are you willing to contribute it Yes:

**Describe the feature and the current behavior/state.**

`tf-nightly` has `MutableHashTableOfTensors` for fast lookups of key/tensor pairs. This feature request is to add similar functionality for `RaggedTensor`s, either by adding `RaggedTensor` support to `MutableHashTableOfTensors` or creating new ops for e.g. a `MutableHashTableOfRaggedTensors`. 

**Will this change the current api? How?** I don't think so. If extra `RaggedTensor` functionality is added `MutableHashTableOfTensors` I think the current `MutableHashTableOfTensors` API won't need to change. 

**Who will benefit with this feature?** Users who need to perform fast lookups on irregular data will benefit from this feature.

Personally I'd like to store a graph adjacency list as a `MutableHashTableOfRaggedTensors`, and then create a `tf.data.Dataset` pipeline to pre-process the graph data for machine learning. I'll implement this in [StellarGraph](https://github.com/stellargraph/stellargraph) to make our graph backend natively tensorflow! "
38262,Import Error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.1
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
ImportError: Could not find the DLL(s) 'msvcp140.dll or msvcp140_1.dll'. TensorFlow requires that these DLLs be installed in a directory that is named in your %PATH% environment variable. You may install these DLLs by downloading ""Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019"" for your platform from this URL: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Installed python 3.7.7 from python.org
installed Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019 from the said platform.
Installed Tensorflow using powershell command ""pip install tensorflow "" .
Ran Powershell Command python -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"" to check if the installation was successfull.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
38261,estimator.train_and_evaluate() sometimes throws errors during training ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Linux 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): pip - TensorFlow version (use command below): 2
- Python version: 3 

**Describe the current behavior**
I am trying to run several ML model training in a python script using tensorflow 2. To do this I dockerize my script and setup a tfjob on Kubeflow. For some models the training goes to completion but for others the training fails and I get different kinds of errors somewhere along the training:

`: Unable to connect to endpoint
[[node save/SaveV2_1 (defined at usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/keras.py:334) ]]

Errors may have originated from an input operation.
Input Source operations connected to node save/SaveV2_1:
Const (defined at usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/keras.py:333) ()`


`Read less bytes than requested [[node save/RestoreV2 (defined at usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/keras.py:334) ]] ()`

`XAmzContentSHA256Mismatch: Unable to parse ExceptionName: XAmzContentSHA256Mismatch Message: The provided 'x-amz-content-sha256' header does not match what was computed. [[node SaveV2_1 (defined at usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/keras.py:403) ]] ()`

`There was no new checkpoint after the training. Eval status: no new checkpoint ('There was no new checkpoint after the training. Eval status: no new checkpoint',)`

**Describe the expected behavior**
I expect the s3 connection errors to be handled by tensorflow so that it doesn't error out when it can't connect to the endpoint during checkpoint saving.

"
38260,Failed to find bogomips warning,"Hi,
Tensorflow fails to find the bogomips because the word ""bogomips"" is in uper-case in my OS:
""""""
processor       : 1
BogoMIPS        : 100.00
""""""
tensorflow/core/platform/profile_utils/cpu_utils.cc:106] Failed to find bogomips or clock in /proc/cpuinfo; cannot determine CPU frequency
"
38257,[TF2.1][TPU] Connection reset by peer during training,"**System information** 
- TensorFlow version: 2.1
- Python version: 3.6.9
- TPU version: - 2.0

**Describe the current behavior**
I'm using TPU to train a model for Named Entity Disambiguation task using a BERT model from the huggingface. During training I randomly get this error saying the connection is reset by peer.

**Standalone code to reproduce the issue** 
Training code: 
```
with strategy.scope():
    model, ent_model, comp_model = distilbert_ned(seq_len_1 = entity_text_len,
                                                  seq_len_2 = company_text_len,
                                                  num_finetune_bert_layers=0)
    adam = tf.keras.optimizers.Adam(clipnorm=.5, lr=lr)
    model.compile(optimizer=adam,
                  loss=triplet_loss,
                  )
    if cuda_dev == 'tpu':
        cb = ACC_Callback(comp_model, ent_model, model, val_set, unique_entities, best_model_dir='gs://{}/ned/out/'.format(cloud_bucket,))
    else:
        cb = ACC_Callback(comp_model, ent_model, model, val_set, unique_entities, best_model_dir='./')
    ds_train, ds_val = get_tfrecord_dataset(batch_size)
    model.fit(ds_train,
          validation_data=ds_val,
          epochs=8, verbose=1,
          callbacks=[cb],
    )
```

What the callback does is it calls `comp_model.predict()` and `ent_model.predict` to make predictions and calculate the accuracy on the validation set.

**Other info / logs**:
```
2020-04-05 06:25:27.447473: E tensorflow/core/common_runtime/eager/context.cc:459] Failed to register function remotely due to Connection reset by peer
This shouldn't happen, please file a bug to tensorflow team.
2020-04-05 06:25:41.771929: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:79] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find a context_id matching the specified one (4329004720033007128). Perhaps the worker was restarted, or the context was GC'd?
Additional GRPC error information:
{""created"":""@1586067941.771818685"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Unable to find a context_id matching the specified one (4329004720033007128). Perhaps the worker was restarted, or the context was GC'd?"",""grpc_status"":3}
Traceback (most recent call last):
  File ""train_ned.py"", line 696, in <module>
    model.save_weights('last_ned.tf')
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1123, in save_weights
    self._trackable_saver.save(filepath, session=session)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py"", line 1168, in save
    file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py"", line 1108, in _save_cached_when_graph_building
    object_graph_tensor=object_graph_tensor)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py"", line 1076, in _gather_saveables
    feed_additions) = self._graph_view.serialize_object_graph()
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py"", line 381, in serialize_object_graph
    trackable_objects, path_to_root)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py"", line 344, in _serialize_gathered_objects
    object_names=object_names)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py"", line 96, in _serialize_slot_variables
    or hasattr(trackable, ""_create_or_restore_slot_variable"")):
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py"", line 830, in __getattr__
    return super(TPUVariableMixin, self).__getattr__(name)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py"", line 392, in __getattr__
Exception ignored in: <bound method IteratorResourceDeleter.__del__ of <tensorflow.python.data.ops.iterator_ops.IteratorResourceDeleter object at 0x7fd3173f00b8>>
Traceback (most recent call last):
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 537, in __del__
    handle=self._handle, deleter=self._deleter)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 1148, in delete_iterator
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: Connection reset by peer [Op:DeleteIterator]
Exception ignored in: <bound method IteratorResourceDeleter.__del__ of <tensorflow.python.data.ops.iterator_ops.IteratorResourceDeleter object at 0x7fd3173e6c18>>
Traceback (most recent call last):
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 537, in __del__
    handle=self._handle, deleter=self._deleter)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 1148, in delete_iterator
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: Connection reset by peer [Op:DeleteIterator]
Exception ignored in: <bound method IteratorResourceDeleter.__del__ of <tensorflow.python.data.ops.iterator_ops.IteratorResourceDeleter object at 0x7fd3173e67b8>>
Traceback (most recent call last):
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 537, in __del__
    handle=self._handle, deleter=self._deleter)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 1148, in delete_iterator
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: Connection reset by peer [Op:DeleteIterator]
Exception ignored in: <bound method IteratorResourceDeleter.__del__ of <tensorflow.python.data.ops.iterator_ops.IteratorResourceDeleter object at 0x7fd3173e6358>>
Traceback (most recent call last):
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 537, in __del__
    handle=self._handle, deleter=self._deleter)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 1148, in delete_iterator
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: Connection reset by peer [Op:DeleteIterator]
Exception ignored in: <bound method IteratorResourceDeleter.__del__ of <tensorflow.python.data.ops.iterator_ops.IteratorResourceDeleter object at 0x7fd31735aeb8>>
Traceback (most recent call last):
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 537, in __del__
    handle=self._handle, deleter=self._deleter)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 1148, in delete_iterator
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: Connection reset by peer [Op:DeleteIterator]
Exception ignored in: <bound method IteratorResourceDeleter.__del__ of <tensorflow.python.data.ops.iterator_ops.IteratorResourceDeleter object at 0x7fd31735a4e0>>
Traceback (most recent call last):
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 537, in __del__
    handle=self._handle, deleter=self._deleter)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 1148, in delete_iterator
    _ops.raise_from_not_ok_status(e, name)
  File ""train_ned.py"", line 685, in <module>
    if False:
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 819, in fit
2020-04-06 01:31:10.561321: E tensorflow/core/common_runtime/eager/context.cc:459] Failed to register function remotely due to Unable to find a context_id matching the specified one (10922957604321251792). Perhaps the worker was restarted, or the context was GC'd?
This shouldn't happen, please file a bug to tensorflow team.
    use_multiprocessing=use_multiprocessing)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 397, in fit
    prefix='val_')
  File ""/usr/lib/python3.6/contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 771, in on_epoch
    self.callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/callbacks.py"", line 302, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""train_ned.py"", line 619, in on_epoch_end

  File ""train_ned.py"", line 636, in get_company_embeddings
    return entity_embd
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1013, in predict
    use_multiprocessing=use_multiprocessing)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 498, in predict
    workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 475, in _model_iteration
    total_epochs=1)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 98, in execution_function
    distributed_function(input_fn))
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py"", line 568, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py"", line 568, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 130, in _non_none_constant_value
    constant_value = tensor_util.constant_value(v)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py"", line 822, in constant_value
    return tensor.numpy()
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 942, in numpy
    maybe_arr = self._numpy()  # pylint: disable=protected-access
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 910, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.AbortedError: RecvTensor expects a different device incarnation: 15132506705170928619 vs. 17767066468299283119. Your worker job (""/job:tpu_worker/replica:0/task:0"") was probably restarted. Check your worker job for the reason why it was restarted.
Additional GRPC error information:
{""created"":""@1586136670.549528961"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""RecvTensor expects a different device incarnation: 15132506705170928619 vs. 17767066468299283119. Your worker job (""/job:tpu_worker/replica:0/task:0"") was probably restarted. Check your worker job for the reason why it was restarted."",""grpc_status"":10}
2020-04-06 01:31:11.529445: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:79] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find a context_id matching the specified one (10922957604321251792). Perhaps the worker was restarted, or the context was GC'd?
Additional GRPC error information:
{""created"":""@1586136671.529360273"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Unable to find a context_id matching the specified one (10922957604321251792). Perhaps the worker was restarted, or the context was GC'd?"",""grpc_status"":3}
Exception ignored in: <bound method IteratorResourceDeleter.__del__ of <tensorflow.python.data.ops.iterator_ops.IteratorResourceDeleter object at 0x7fc6e47cdef0>>
Traceback (most recent call last):
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 537, in __del__
    handle=self._handle, deleter=self._deleter)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 1148, in delete_iterator
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Unable to find a context_id matching the specified one (10922957604321251792). Perhaps the worker was restarted, or the context was GC'd? [Op:DeleteIterator]
Exception ignored in: <bound method IteratorResourceDeleter.__del__ of <tensorflow.python.data.ops.iterator_ops.IteratorResourceDeleter object at 0x7fc700031c18>>
Traceback (most recent call last):
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 537, in __del__
    handle=self._handle, deleter=self._deleter)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 1148, in delete_iterator
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/ethan/SEER/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Unable to find a context_id matching the specified one (10922957604321251792). Perhaps the worker was restarted, or the context was GC'd? [Op:DeleteIterator]
```"
38255,tensorflow issue,"Hey, I can't install tensorflow
If I write pip install tensorflow the error message is 
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
 please help
thank you "
38254,tensorflow build from source issue on Window 10 (Taking too much time),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.27.2
- GCC/Compiler version (if compiling from source): VS2017
- CUDA/cuDNN version: NA
- GPU model and memory: NA

I am trying to build tensorflow from source on Window 10. I override the eigen strong inline to reduce the compile time. The build is stuck at:
[4,915 / 4,918] Compiling tensorflow/core/kernels/mkl_cwise_ops_common.cc; 72789s local

Any ideas why it is taking long time to compile this file?

I used the following command to start the build process:
bazel build --config=mkl -c opt --copt=/arch:AVX --copt=/arch:AVX2 --copt=/std:c++17 --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --define=override_eigen_strong_inline=true //tensorflow:tensorflow.dll
"
38253,Import error in tensorflow_datasets,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): MAC OS Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: NA
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): pip install tensorflow


**Describe the current behavior**

> ModuleNotFoundError: No module named 'tensorflow.compat'

I encountered this error after trying to import tensorflow datasets



**Describe the expected behavior**
No import error

**Standalone code to reproduce the issue** 
> import tensorflow_datasets as tfds

**Other info / logs** 

<img width=""550"" alt=""Screen Shot 2020-04-05 at 17 55 04"" src=""https://user-images.githubusercontent.com/32034407/78510896-96e23c80-7766-11ea-8402-17708436becd.png"">
"
38252,problem of compling tensorflow due to installation,"I have a problem with compling with tensorflow in jupyternotebook. This is the error message
Traceback (most recent call last):

```
File ""c:\users\home\miniconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3331, in run_code
exec(code_obj, self.user_global_ns, self.user_ns)

File """", line 1, in
import tensorflow as tf

File ""c:\users\home\miniconda3\lib\site-packages\tensorflow_init_.py"", line 22, in
from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import

File ""c:\users\home\miniconda3\lib\site-packages\tensorflow\python_init_.py"", line 49, in
from tensorflow.python import pywrap_tensorflow

File ""c:\users\home\miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in
from tensorflow.python.pywrap_tensorflow_internal import *

File ""c:\users\home\miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 114
def TFE_ContextOptionsSetAsync(arg1, async):
^
SyntaxError: invalid syntax
```

just to make it clearer, maybe it is an installation problem.
well pip install tensorflow didn't work at first, I had this messag error

```
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```

so I used this line that I found somewhere to install tensorflow

```
pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl
```

and then I needed the version 1.10.0 of tensorflow so I wrote the same line above but with the different version

```
pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.10.0-py3-none-any.whl
```

that's it, then I tried to compile in jupyternotebook with tensorflow and it didn't work :("
38251,Error Loading Weights with `by_name=True`,"Here is a short Colab notebook with this error

https://colab.research.google.com/drive/1Z9RoOtyPw-MXnDgPK5irbuZ9I6tpz-zM

**Describe the current behavior**
Error message complains that I must have `TensorFlow-formatted weights` however when I am saving weights I am saving this as TensorFlow formatted weights, using the below code:

```py
model.save_weights('model_weights', save_format='tf')`
```

**Describe the expected behavior**


**Standalone code to reproduce the issue** 

https://colab.research.google.com/drive/1Z9RoOtyPw-MXnDgPK5irbuZ9I6tpz-zM


"
38250,Incompatible shapes when loading model with TextVectorization and Embedding + Conv1D,"There is **ValueError: Shapes incompatible** when trying to load_model with **TextVectorization** and **Embedding** + **Conv1D** (or **LSTM** or **GRU**).

It can be easily replicated using _TextVectorization layer examples_ from TF documentation. You just need to add _model save & load_ to _Using the TextVectorization layer in an Embedding + Conv1D model_ sample.
I created [copy of TextVectorization layer examples colab with these errors](https://colab.research.google.com/drive/1MavOxANGMkDGo8Ob4Lm3H43CY41dON7t). Code added to original example:
```python
model.save('my_model')
loaded_model = tf.keras.models.load_model('my_model')
```
Error from load_model execution:
```python
WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. 
Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.

Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f31fe4d8320> and <tensorflow.python.keras.saving.saved_model.load.TextVectorization object at 0x7f31fe4d8b70>).

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)
   1115     """"""
   1116     if not self.is_compatible_with(other):
-> 1117       raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
   1118 
   1119   def most_specific_compatible_shape(self, other):

ValueError: Shapes (128, 128) and (7, 128, 128) are incompatible
```
This error do NOT occur when trying to do _save & load_ in example: [_Using the TextVectorization layer in a bigram TF-IDF densely-connected model_](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3). This example contains only Dense and Dropout layers and do not contain Embedding and Conv1D.

I also found same problem for my custom models using LSTM and GRU.

Problem occurred for nightly TF version `2.2.0-dev20200405`"
38249,problem of compling with tensorflow ,"I have a problem with compling with tensorflow in jupyternotebook. This is the error message
Traceback (most recent call last):

File ""c:\users\home\miniconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3331, in run_code
exec(code_obj, self.user_global_ns, self.user_ns)

File """", line 1, in
import tensorflow as tf

File ""c:\users\home\miniconda3\lib\site-packages\tensorflow_init_.py"", line 22, in
from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import

File ""c:\users\home\miniconda3\lib\site-packages\tensorflow\python_init_.py"", line 49, in
from tensorflow.python import pywrap_tensorflow

File ""c:\users\home\miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in
from tensorflow.python.pywrap_tensorflow_internal import *

File ""c:\users\home\miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 114
def TFE_ContextOptionsSetAsync(arg1, async):
^
SyntaxError: invalid syntax

Help please <3"
38248,"in nmt_with_attention, the gru in decoder is not connected, last step state is not passed to this step","https://www.tensorflow.org/tutorials/text/nmt_with_attention

the decoder is trained step by step, and it's not passing last step state to this step

```
# passing the concatenated vector to the GRU
output, state = self.gru(x)
```

is this a feature or a bug? I checked a lot of NMT with attention paper, unlike the document those decoder are connected.

thanks in advance!"
38247,gpu-jupyter.Dockerfile has bugs - docker build fails.,"After yesterday's edit, few bugs surfaced in gpu-jupyter.Dockerfile
file url tensorflow/tools/dockerfiles/dockerfiles/gpu-jupyter.Dockerfile

1) line #81 should have \ 
python3 \
2) line #100 should be 
RUN python3 -m pip  install --no-cache-dir ${TF_PACKAGE}${TF_PACKAGE_VERSION:+==${TF_PACKAGE_VERSION}} 

without these changes, docker build fails.
"
38246,Train a Simple Audio Recognition model for microcontroller use build issue,"I'm not able to install dependencies in Colab in this Train a Simple Audio Recognition model for microcontroller model.
Error shows:
1. Could not find a version that satisfies the requirement tf-nightly-gpu==1.15.0.dev20190729
2. No matching distribution found for tf-nightly-gpu==1.15.0.dev20190729

Can someone guide how to resolve it?"
38245,register keras serializables is module_objects,"**System information**
- TensorFlow version (you are using): 2.1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
I've been using keras serialization and while it mostly works well, I have two issues:
* there seems to be duplicated functionality in `get` and `deserialize` across `tf.keras` submodules, and inconsistent support (e.g. no `tf.keras.layers.get`).
* use with custom implementations via  `tf.keras.utils.register_keras_serializable` gives odd and potentially surprising results.

These are illustrated in the below examples:
```python
import tensorflow as tf

register = tf.keras.utils.register_keras_serializable('my_package.layers')

@register
class MyLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

layer = MyLayer()
config = tf.keras.utils.serialize_keras_object(layer)

print(tf.keras.layers.deserialize(config))      # works - but why no layers.get?
print(tf.keras.optimizers.get(config))          # works - but should it?
```

Something like the following could reduce code duplication across different modules and make them extensible and more resilient to errors.

```python
class Serializer(object):
    def __init__(self, accepted_types=None, module_name=None):
        self._module_objects = {}
        self._module_name = module_name
        self._accepted_types = accepted_types
    
    def register(self, fn_or_class: Callable):
        assert callable(fn_or_class)
        name = fn_or_class.__name__
        if name in self._module_objects:
            raise ValueError(
                f'Could register {name} - already in {self._module_name} '
                'module_objects')
        self._module_objects[name] = fn_or_class
        return fn_or_class
    
    def deserialize(self, config, custom_objects=None):
        return tf.keras.utils.deserialize_keras_object(
            config, module_objects=self._module_objects,
            custom_objects=custom_objects,
            printable_module_name=self._module_name)
    
    def _get_fallback(self, identifier):
        raise ValueError(
            f'Could not interpret optimizer identifier: {identifier}')
    
    def get(self, identifier):
        if isinstance(identifier, self._accepted_types):
            return identifier
        elif isinstance(identifier, str):
            identifier = dict(class_name=identifier, config={})
        if isinstance(identifier, dict):
            return self.deserialize(identifier)
        else:
            return self._get_fallback(identifier)

# in layers.py
_serializer = Serializer(Layer, 'layers')
get = _serializer.get
deserialize = _serializer.deserialize
register = _serializer.register

# in optimizers.py
class OptimizerSerializer(Serializer):

    def __init__(self):
        self.__init__(Optimizer, 'optimizers')

    def _get_fallback(self, identifier):
        if isinstance(identifier, tf_optimizer_module.Optimizer):
            opt = TFOptimizer(identifier)
            K.track_tf_optimizer(opt)
            return opt
        else:
            return super()._get_fallback(identifier)

_serializer = OptimizerSerializer()
get = _serializer.get
deserialize = _serializer.deserialize
register = _serializer.register
```

**Will this change the current api? How?**
* Add `register` function to each of `tf.keras.[layers,optimizers,...]`
* Add `tf.keras.layers.get`

**Who will benefit with this feature?**
People who want to use keras serialization/deserialization with custom objects and retain error checking

**Any Other info.**
More than happy to put the PR together myself if the idea is likely to be accepted. Also happy to put in `package` support similar to `tf.keras.utils.register_keras_serializable`. "
38244,How to deal with variable length weight category features in tensorflow?,"How to deal with variable length weight category features in tensorflow?

For example:

 `vocabulary_list=['A','B','C']`

The original feature format is as follows:

`    [
    'A:1.5,B:0.3,C:0.8',
    'D:0.9,B:1.8',
    'A:1.2,B:3.2,E:1.5'
    ]`

After treatment, the results are as follows:

`    [
    [1.5,0.3,0.8,0.0],
    [0.0,1.8,0.0,0.9],
    [1.2,3.2,0.0,1.5]
    ]`

Please help me~~~

Thank you very much~~~"
38241,TensorFlow Lite image_segmentation example for iOS doesn't build,"## URL(s) with the issue:
https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/ios
## Description of issue (what needs changing):

The README must be provided with an explanation of how to change the settings of the project. At the moment I have problems adjusting the development team (""**Signing**"").
Error message: No profile for team 'Rob De Putter (Personal Team)' matching 'Wildcard' found: Xcode couldn't find any provisioning profiles matching 'GPC87JXMXD/Wildcard'. Install the profile (by dragging and dropping it onto Xcode's dock item) or select a different one in the Signing & Capabilities tab of the target editor.

<img width=""1421"" alt=""Schermafbeelding 2020-04-05 om 14 54 09"" src=""https://user-images.githubusercontent.com/36565271/78500273-4f9b8200-774d-11ea-8b6a-4f214fa2fe81.png"">
<img width=""1154"" alt=""Schermafbeelding 2020-04-05 om 14 54 37"" src=""https://user-images.githubusercontent.com/36565271/78500283-6641d900-774d-11ea-9bca-6aec48736816.png"">
<img width=""1152"" alt=""Schermafbeelding 2020-04-05 om 14 54 30"" src=""https://user-images.githubusercontent.com/36565271/78500289-6cd05080-774d-11ea-984b-d6f3710d8f08.png"">

"
38240,Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version:1.14.0(CPU)
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?:sudo pip3 tensorflow==1.14.0
- Bazel version (if compiling from source):0.24.1
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version: 0
- GPU model and memory:GTX1660ti 6G



**Describe the problem**
I am follwing this [instruction](https://programmer.ink/think/tensorflow-android-side-compile-procedure-record.html#1Bazel_0241_18) to build so and jar file, but i got following error:
![Snipaste_2020-04-05_21-46-33](https://user-images.githubusercontent.com/43233772/78500087-1a138a80-7787-11ea-85be-64240302a05c.png)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
As shown in above image

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
38239,Adding shape to _tensor_array_scatter,"How can I add shape to tensor_array_scatter? 
I have tried with the following code but this doesn't work 
![Screenshot from 2020-04-05 15-43-33](https://user-images.githubusercontent.com/41699212/78499993-4530b680-7754-11ea-9fc0-88f3c1f5274d.png)
is there another way to add shapes for graph"
38238, Floating point calculation error,"tensorflow2.0 and python3.7 
when i wanto calculate 165 * 3.1 or 165 * 2.1, i got a fake answer.
Like this:
tf.cast(165, tf.float32) * 2.1
Out: <tf.Tensor: id=43186, shape=(), dtype=float32, numpy=346.49997>
tf.cast(165, tf.float32) * 3.1
Out: <tf.Tensor: id=43261, shape=(), dtype=float32, numpy=511.49997>

i make sure its a error, but i wanto know why.
"
38237,[Mismatch of Codes] built libtensorflow_cc.so does not match the interface of Eigen ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: python3.6 （maintained by conda）
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.29.1 (according to requirements of tensorflow 2.2.0)
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: ON 7 GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I built tensorflow from sources and exported both dependancies of files and the shared library to /usr/local/include/tensorflow, /usr/local/lib (soft link to /usr/lib/ ...)

Now I am building a cmake projects using tensorflow. Here is a cmake snopshot:

```txt
 cmake ..
[Semantic_Relocalization] Build type: Release
[Semantic_Relocalization] Install path: /usr/local
-- Using libstdc++(gcc). You can change the tool chain with clang/clang++ in Ubuntu/CentOS/MacOS
-- Could NOT find GTest (missing: GTEST_LIBRARY GTEST_MAIN_LIBRARY)
-- Found gflags (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libgflags.so)
-- GFLAGS_INCLUDE_DIRS: /usr/include/gflags
-- GFLAGS_LIBRARIES: /usr/lib/x86_64-linux-gnu/libgflags.so
-- Found glog (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)
-- GLOG_INCLUDE_DIRS: /usr/include/glog
-- GLOG_LIBRARIES: /usr/lib/x86_64-linux-gnu/libglog.so
-- Home directory : /home/yiakwy
CMake Warning at cmake/Relies.cmake:17 (message):
  System variable EXTERNAL_DIR is nonexist, using
  /home/yiakwy/mapping_external instead.
Call Stack (most recent call first):
  CMakeLists.txt:123 (include)


-- EXTERNAL_LIBS_DIR : /home/yiakwy/mapping_external/linux
-- Find Opencv (ver.) (include: /usr/local/include/opencv4, library: opencv_calib3d
-- opencv_core
-- opencv_dnn
-- opencv_features2d
-- opencv_flann
-- opencv_gapi
-- opencv_highgui
-- opencv_imgcodecs
-- opencv_imgproc
-- opencv_ml
-- opencv_objdetect
-- opencv_photo
-- opencv_stitching
-- opencv_video
-- opencv_videoio
-- opencv_aruco
-- opencv_bgsegm
-- opencv_bioinspired
-- opencv_ccalib)
CMake Warning at /usr/local/share/cmake-3.9/Modules/FindBoost.cmake:769 (message):
  Imported targets not available for Boost version 106501
Call Stack (most recent call first):
  /usr/local/share/cmake-3.9/Modules/FindBoost.cmake:873 (_Boost_COMPONENT_DEPENDENCIES)
  /usr/local/share/cmake-3.9/Modules/FindBoost.cmake:1501 (_Boost_MISSING_DEPENDENCIES)
  cmake/Relies.cmake:58 (find_package)
  CMakeLists.txt:123 (include)


CMake Warning at /usr/local/share/cmake-3.9/Modules/FindBoost.cmake:769 (message):
  Imported targets not available for Boost version 106501
Call Stack (most recent call first):
  /usr/local/share/cmake-3.9/Modules/FindBoost.cmake:873 (_Boost_COMPONENT_DEPENDENCIES)
  /usr/local/share/cmake-3.9/Modules/FindBoost.cmake:1501 (_Boost_MISSING_DEPENDENCIES)
  cmake/Relies.cmake:58 (find_package)
  CMakeLists.txt:123 (include)


-- Boost version: 1.65.1
-- Found the following Boost libraries:
--   system
--   filesystem
-- EIGEN3_VERSION: 3.3.90
-- Eigen3_DIR: /usr/local/share/eigen3/cmake
-- Found Eigen3, inc: /usr/local/include/eigen3
-- Found Eigen3: inc /usr/local/include/eigen3 (ver: 3.3.90)
-- Protobuf compiler version: libprotoc 3.8.0

-- Found Protobuf: /usr/local/lib/libprotobuf.so;-lpthread (found version ""3.8.0"")
-- Protobuf compiler version: libprotoc 3.8.0

-- Found Protobuf: /usr/local/lib/libprotobuf.so;-lpthread;-lpthread (found version ""3.8.0"")
-- Using protobuf 3.8.0
-- Protobuf_PROTOC_EXECUTABLE: /usr/local/bin/protoc
-- Found Tensorflow, inc: /usr/local/include/google/tensorflow, lib: /usr/local/lib/libtensorflow_all.so
-- PROTO_FILES: /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/proto/runtime_block.proto
-- hw_proto_srcs: /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/build/proto_codec/runtime_block.pb.cc
-- hw_proto_hdrs: /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/build/proto_codec/runtime_block.pb.h
-- PROTOBUF_INCLUDE_DIRS: /usr/local/include
-- PROTOBUF_LIBRARIES: /usr/local/lib/libprotobuf.so
-- -lpthread
-- -lpthread
-- models PYTHON_INCLUDE_DIR: /home/yiakwy/anaconda3/envs/py36/include/python3.6m
-- models PYTHON_LIBRARY: /home/yiakwy/anaconda3/envs/py36/lib/libpython3.6m.so.1.0
-- PYTHONHOME: /home/yiakwy/anaconda3/envs/py36
-- Configuring done
-- Generating done
-- Build files have been written to: /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/build

```

However when I try to make it, something goes wrong with Eigen interface:

```text
▶ make
[ 50%] Built target base
[ 62%] Building CXX object modules/models/CMakeFiles/models.dir/main.cpp.o
In file included from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:0,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/numeric_types.h:24,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h:49:76: error: wrong number of template arguments (5, should be at least 3)
 class TensorContractionBlocking<QInt16, QInt16, QInt16, Index, ShardingType> {
                                                                            ^
In file included from /usr/include/eigen3/unsupported/Eigen/CXX11/Tensor:112:0,
                 from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorContractionBlocking.h:25:7: note: provided for ‘template<class LhsMapper, class RhsMapper, class Index, int ShardingType> class Eigen::internal::TensorContractionBlocking’
 class TensorContractionBlocking {
       ^~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:0,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/numeric_types.h:24,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h:151:42: error: wrong number of template arguments (9, should be at least 6)
                      Conjugate, PanelMode> {
                                          ^
In file included from /usr/include/eigen3/unsupported/Eigen/CXX11/../../../Eigen/Core:429:0,
                 from /usr/include/eigen3/unsupported/Eigen/CXX11/Tensor:14,
                 from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/include/eigen3/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/BlasUtil.h:28:8: note: provided for ‘template<class Scalar, class Index, class DataMapper, int Pack1, int Pack2, int StorageOrder, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_lhs’
 struct gemm_pack_lhs;
        ^~~~~~~~~~~~~
In file included from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:0,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/numeric_types.h:24,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h:160:76: error: wrong number of template arguments (9, should be at least 6)
                                      QInt16, ColMajor, Conjugate, PanelMode>::
                                                                            ^
In file included from /usr/include/eigen3/unsupported/Eigen/CXX11/../../../Eigen/Core:429:0,
                 from /usr/include/eigen3/unsupported/Eigen/CXX11/Tensor:14,
                 from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/include/eigen3/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/BlasUtil.h:28:8: note: provided for ‘template<class Scalar, class Index, class DataMapper, int Pack1, int Pack2, int StorageOrder, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_lhs’
 struct gemm_pack_lhs;
        ^~~~~~~~~~~~~
In file included from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:0,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/numeric_types.h:24,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h:162:38: error: ‘void Eigen::internal::operator()(Eigen::QInt16*, const DataMapper&, Index, Index, Index, Index)’ must be a nonstatic member function
            Index stride, Index offset) {
                                      ^
/usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h:549:24: error: wrong number of template arguments (5, should be at least 3)
     Index, ShardingType> {
                        ^
In file included from /usr/include/eigen3/unsupported/Eigen/CXX11/Tensor:112:0,
                 from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorContractionBlocking.h:25:7: note: provided for ‘template<class LhsMapper, class RhsMapper, class Index, int ShardingType> class Eigen::internal::TensorContractionBlocking’
 class TensorContractionBlocking {
       ^~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:0,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/numeric_types.h:24,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h:1793:42: error: wrong number of template arguments (9, should be at least 6)
                      Conjugate, PanelMode> {
                                          ^
In file included from /usr/include/eigen3/unsupported/Eigen/CXX11/../../../Eigen/Core:429:0,
                 from /usr/include/eigen3/unsupported/Eigen/CXX11/Tensor:14,
                 from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/include/eigen3/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/BlasUtil.h:28:8: note: provided for ‘template<class Scalar, class Index, class DataMapper, int Pack1, int Pack2, int StorageOrder, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_lhs’
 struct gemm_pack_lhs;
        ^~~~~~~~~~~~~
In file included from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:0,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/numeric_types.h:24,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h:1802:75: error: wrong number of template arguments (9, should be at least 6)
                                      QInt8, ColMajor, Conjugate, PanelMode>::
                                                                           ^
In file included from /usr/include/eigen3/unsupported/Eigen/CXX11/../../../Eigen/Core:429:0,
                 from /usr/include/eigen3/unsupported/Eigen/CXX11/Tensor:14,
                 from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/include/eigen3/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/BlasUtil.h:28:8: note: provided for ‘template<class Scalar, class Index, class DataMapper, int Pack1, int Pack2, int StorageOrder, bool Conjugate, bool PanelMode> struct Eigen::internal::gemm_pack_lhs’
 struct gemm_pack_lhs;
        ^~~~~~~~~~~~~
In file included from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:0,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/numeric_types.h:24,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:22,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h:1804:38: error: ‘void Eigen::internal::operator()(Eigen::QInt8*, const DataMapper&, Index, Index, Index, Index)’ must be a nonstatic member function
            Index stride, Index offset) {
                                      ^
In file included from /usr/include/eigen3/unsupported/Eigen/CXX11/Tensor:105:0,
                 from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h: In instantiation of ‘struct Eigen::TensorEvaluator<const Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, Eigen::DefaultDevice>’:
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorForcedEval.h:96:65:   required from ‘struct Eigen::TensorEvaluator<const Eigen::TensorForcedEvalOp<const Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer>, Eigen::DefaultDevice>’
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorIO.h:61:42:   required from ‘std::ostream& Eigen::operator<<(std::ostream&, const Eigen::TensorBase<Derived, 0>&) [with T = Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>; std::ostream = std::basic_ostream<char>]’
/home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:48:54:   required from here
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:162:71: warning: ignoring attributes on template argument ‘Eigen::PacketType<float, Eigen::DefaultDevice>::type {aka __vector(8) float}’ [-Wignored-attributes]
     PacketAccess = (internal::unpacket_traits<PacketReturnType>::size > 1),
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~
In file included from /usr/include/eigen3/unsupported/Eigen/CXX11/Tensor:133:0,
                 from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorForcedEval.h: In instantiation of ‘const int Eigen::TensorEvaluator<const Eigen::TensorForcedEvalOp<const Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer>, Eigen::DefaultDevice>::PacketSize’:
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorForcedEval.h:104:32:   required from ‘struct Eigen::TensorEvaluator<const Eigen::TensorForcedEvalOp<const Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer>, Eigen::DefaultDevice>’
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorIO.h:61:42:   required from ‘std::ostream& Eigen::operator<<(std::ostream&, const Eigen::TensorBase<Derived, 0>&) [with T = Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>; std::ostream = std::basic_ostream<char>]’
/home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:48:54:   required from here
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorForcedEval.h:100:20: warning: ignoring attributes on template argument ‘Eigen::PacketType<float, Eigen::DefaultDevice>::type {aka __vector(8) float}’ [-Wignored-attributes]
   static const int PacketSize = internal::unpacket_traits<PacketReturnType>::size;
                    ^~~~~~~~~~
In file included from /usr/include/eigen3/unsupported/Eigen/CXX11/Tensor:139:0,
                 from /usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,
                 from /home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:6:
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h: In instantiation of ‘static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorEvalToOp<const Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer>]’:
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorForcedEval.h:128:109:   required from ‘bool Eigen::TensorEvaluator<const Eigen::TensorForcedEvalOp<ArgType, MakePointer_>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorForcedEvalOp<ArgType, MakePointer_>, Device>::CoeffReturnType*) [with ArgType = const Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::DefaultDevice; MakePointer_ = Eigen::MakePointer; Eigen::TensorEvaluator<const Eigen::TensorForcedEvalOp<ArgType, MakePointer_>, Device>::CoeffReturnType = float]’
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorIO.h:66:3:   required from ‘std::ostream& Eigen::operator<<(std::ostream&, const Eigen::TensorBase<Derived, 0>&) [with T = Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>; std::ostream = std::basic_ostream<char>]’
/home/yiakwy/WorkSpace/Github/SEMANTIC_SLAM/modules/models/main.cpp:48:54:   required from here
/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:61:17: warning: ignoring attributes on template argument ‘Eigen::TensorEvaluator<const Eigen::TensorEvalToOp<const Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer>, Eigen::DefaultDevice>::PacketReturnType {aka __vector(8) float}’ [-Wignored-attributes]
       const int PacketSize = unpacket_traits<typename TensorEvaluator<Expression, DefaultDevice>::PacketReturnType>::size;
                 ^~~~~~~~~~
cc1plus: warning: unrecognized command line option ‘-Wno-deprecated-declaration’
modules/models/CMakeFiles/models.dir/build.make:62: recipe for target 'modules/models/CMakeFiles/models.dir/main.cpp.o' failed
make[2]: *** [modules/models/CMakeFiles/models.dir/main.cpp.o] Error 1
CMakeFiles/Makefile2:1163: recipe for target 'modules/models/CMakeFiles/models.dir/all' failed
make[1]: *** [modules/models/CMakeFiles/models.dir/all] Error 2
Makefile:140: recipe for target 'all' failed
make: *** [all] Error 2

```

I have already downloaded and installed Eigen specified in `tensorlfow/workspace.bzl` (3.9.0, I have printed the version I extracted from ""Eigen/src/Core/util/Macros.h"")

However after searching a while, only to find some interfaces like ""TensorContractionBlocking"" defined in installed eigen3 (/usr/include/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorContractionBlocking.h) was different from the google version (/usr/local/include/google/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h)

Here is the captured snapshot from screen:

![Screenshot 2020-04-05 at 6 03 36 PM](https://user-images.githubusercontent.com/61530411/78472569-8fbe2d00-776c-11ea-890c-fbdde9569b42.png)


**Any other info / logs**
See above

**Conclution**

Since the source codes are incorrect, It is impossible to compile it in my cmake project. Any help ?
"
38236,very slow training if recurrent_dropout above 0.0 in LSTM keras using GPU,"**System information** 
- Have I written custom code: yes
- OS Platform and Distribution: Linux Mint 19
- TensorFlow installed from: pip ( `pip install tensorflow-gpu` )
- TensorFlow version: 2.1.0 (same issue in 2.0.0 though)

- CUDA/cuDNN version: 10.2
- GPU model and memory: RTX 2080 8GB (same issue on RTX 2080Ti 11GB )


Hi!

When I use LSTM `recurrent_dropout` not set to 0.0, training time is very long (~25 times longer compared to `recurrent_dropout` 0.0). I suppose it shouldn't slow down the training that much I have the same issue on two separate machines

For example

`model.add(layers.LSTM(128, recurrent_dropout=0.0))` training time ~40 sec
`model.add(layers.LSTM(128, recurrent_dropout=0.1))` training time is above ~17 min.


```
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
import tensorflow
import os
os.environ[""TF_FORCE_GPU_ALLOW_GROWTH""]=""true""


model = tf.keras.Sequential()
model.add(layers.Embedding(input_dim=1000, output_dim=64))
model.add(layers.LSTM(128, recurrent_dropout=0.1))
model.add(layers.Dense(1))

model.compile('sgd', 'mse')
model.summary()

x = np.random.rand(100000, 1000)
y = np.random.randint(1, size = (100000,1))
model.fit(x,y,10)
```
"
38235,Tensorflow and Pip Error,"**System information**
- Windows 10 Basic Updated 
- TensorFlow installed from (source or binary): Anaconda 4.8.2
- TensorFlow version: 2.1
- Python version: 3.5
- Installed using Pip

```
(tensorflow1) c:\>pip install --ignore-installed --upgrade tensorflow
Collecting tensorflow
  Downloading tensorflow-2.1.0-cp35-cp35m-win_amd64.whl (355.8 MB)
     |██████                          | 67.7 MB 62 kB/s eta 1:16:58ERROR: Exception:
Traceback (most recent call last):
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_vendor\urllib3\response.py"", line 425, in _error_catcher
    yield
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_vendor\urllib3\response.py"", line 507, in read
    data = self._fp.read(amt) if not fp_closed else b""""
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_vendor\cachecontrol\filewrapper.py"", line 62, in read
    data = self.__fp.read(amt)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\http\client.py"", line 448, in read
    n = self.readinto(b)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\http\client.py"", line 488, in readinto
    n = self.fp.readinto(b)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\socket.py"", line 576, in readinto
    return self._sock.recv_into(b)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\ssl.py"", line 937, in recv_into
    return self.read(nbytes, buffer)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\ssl.py"", line 799, in read
    return self._sslobj.read(len, buffer)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\ssl.py"", line 583, in read
    v = self._sslobj.read(len, buffer)
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\cli\base_command.py"", line 186, in _main
    status = self.run(options, args)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\commands\install.py"", line 331, in run
    resolver.resolve(requirement_set)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\legacy_resolve.py"", line 177, in resolve
    discovered_reqs.extend(self._resolve_one(requirement_set, req))
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\legacy_resolve.py"", line 333, in _resolve_one
    abstract_dist = self._get_abstract_dist_for(req_to_install)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\legacy_resolve.py"", line 282, in _get_abstract_dist_for
    abstract_dist = self.preparer.prepare_linked_requirement(req)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\operations\prepare.py"", line 482, in prepare_linked_requirement
    hashes=hashes,
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\operations\prepare.py"", line 287, in unpack_url
    hashes=hashes,
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\operations\prepare.py"", line 159, in unpack_http_url
    link, downloader, temp_dir.path, hashes
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\operations\prepare.py"", line 303, in _download_http_url
    for chunk in download.chunks:
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\utils\ui.py"", line 160, in iter
    for x in it:
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_internal\network\utils.py"", line 39, in response_chunks
    decode_content=False,
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_vendor\urllib3\response.py"", line 564, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_vendor\urllib3\response.py"", line 529, in read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""c:\users\acer\anaconda3\envs\tensorflow1\lib\site-packages\pip\_vendor\urllib3\response.py"", line 430, in _error_catcher
    raise ReadTimeoutError(self._pool, None, ""Read timed out."")
pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.
```

New Conda Environment

```
(base) c:\>conda create -n tensorflow1 pip python=3.5
Collecting package metadata (current_repodata.json): done
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: done

  added / updated specs:
    - pip
    - python=3.5

The following NEW packages will be INSTALLED:

  certifi            pkgs/main/win-64::certifi-2018.8.24-py35_1
  pip                pkgs/main/win-64::pip-10.0.1-py35_0
  python             pkgs/main/win-64::python-3.5.6-he025d50_0
  setuptools         pkgs/main/win-64::setuptools-40.2.0-py35_0
  vc                 pkgs/main/win-64::vc-14.1-h0510ff6_4
  vs2015_runtime     pkgs/main/win-64::vs2015_runtime-14.16.27012-hf0eaf9b_1
  wheel              pkgs/main/win-64::wheel-0.31.1-py35_0
  wincertstore       pkgs/main/win-64::wincertstore-0.2-py35hfebbdb8_0

(base) c:\>activate tensorflow1
```
"
38234,ctc_loss compilation failure on TPU,"Hello! I'm trying to compile my simple model with custom loss that uses tf.nn.ctc_loss and get the following error:
```
UnimplementedError: {{function_node __inference_train_function_34365}} Compilation failure: Dynamic input dimension to reshape that is both splitted and combined is not supported: output: f32[1186,36,0], input: f32[1186,<=36,0], input_dim: 1
	 [[{{node gradient_tape/ctc/ctc_loss_dense/strided_slice_2/StridedSliceGrad}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_8793276995691903576/_5]]
```

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution: Colaboratory on *TPU*
- TensorFlow installed from (source or
binary): binary
- TensorFlow version:  2.2.0-rc2


**Loss**:
```
class CTCLoss(tf.keras.losses.Loss):
    def __init__(self, logits_time_major=False, reduction=tf.keras.losses.Reduction.SUM, name='ctc'):
        super().__init__(reduction=reduction, name=name)
        self.logits_time_major = logits_time_major

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.int32)
        logit_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1])
        label_length = tf.fill([tf.shape(y_true)[0]], tf.shape(y_true)[1])
        loss = tf.nn.ctc_loss((
            labels=y_true,
            logits=y_pred,
            label_length=label_length,
            logit_length=logit_length,
            logits_time_major=self.logits_time_major,
            blank_index=-1)
        return tf.reduce_mean(loss)
```

**Model**:
```
class TestNN(tf.keras.Model):
  def __init__(self):
    super(TestNN, self).__init__()
    self.seq = Sequential()
    self.b_lstm1 = Bidirectional(LSTM(128, return_sequences=True, implementation=2), input_shape=(None, 13))
    self.b_lstm2 = Bidirectional(LSTM(128, return_sequences=True, implementation=2))
    self.tmd = TimeDistributed(Dense(len(inv_mapping) + 2))
  
  def call(self, x):
    x = self.seq(x)
    x = self.b_lstm1(x)
    x = self.b_lstm2(x)
    x = self.tmd(x)
    return x
```

**Compilation model**:
```
strategy = tf.distribute.experimental.TPUStrategy(resolver)
with strategy.scope():
  model = TestNN()
  model.compile(optimizer=tf.optimizers.Adam(1e-2), loss=CTCLoss())
```

**I got the error when running**:
```
model.fit(input_tensor, label_tensor, batch_size=36*8, epochs=1)
```
- input_tensor is  tf.Tensor with **shape**=(2241, 1186, 13), **dtype**=float32
- label_tensor is tf.Tensor with **shape**=(2241, 59), **dtype**=int32

It's a bug, or I'm doing something wrong?"
38233,"ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0',","**System information** 
Colab tensorflow 2.2.0

**Describe the current behavior**:
I faced this error when i tried   to solve my own data issues, which is multiple label semantic segmentations. When I ran on Jupiter notebook on my local Mac Book with Keras installation (not tf.keras but keras only), model could train normally as expected as below. 

![image](https://user-images.githubusercontent.com/43133053/78468464-fdf1f800-774a-11ea-9dc3-315a49fc430e.png)
 
However, I stopped training the whole model on local Mac Book due to its limited memory and capability and I switched to Colab, **Tensorflow version:  2.2.0-rc2** and faced this error:
`    ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0', 'conv2d_6/kernel:0', 'conv2d_6/bias:0', 'conv2d_7/kernel:0', 'conv2d_7/bias:0', 'conv2d_8/kernel:0', 'conv2d_8/bias:0', 'conv2d_9/kernel:0', 'conv2d_9/bias:0', 'conv2d_transpose/kernel:0', 'conv2d_transpose/bias:0', 'conv2d_10/kernel:0', 'conv2d_10/bias:0', 'conv2d_11/kernel:0', 'conv2d_11/bias:0', 'conv2d_transpose_1/kernel:0', 'conv2d_transpose_1/bias:0', 'conv2d_12/kernel:0', 'conv2d_12/bias:0', 'conv2d_13/kernel:0', 'conv2d_13/bias:0', 'conv2d_transpose_2/kernel:0', 'conv2d_transpose_2/bias:0', 'conv2d_14/kernel:0', 'conv2d_14/bias:0', 'conv2d_15/kernel:0', 'conv2d_15/bias:0', 'conv2d_transpose_3/kernel:0', 'conv2d_transpose_3/bias:0', 'conv2d_16/kernel:0', 'conv2d_16/bias:0', 'conv2d_17/kernel:0', 'conv2d_17/bias:0', 'conv2d_18/kernel:0', 'conv2d_18/bias:0'].`

**Full error:**

Epoch 1/40
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-27-f05367a1db71> in <module>()
      5     callbacks=callbacks,
      6     validation_data=valid_dataloader,
----> 7     validation_steps=(no_of_validation_images//1),verbose=1
      8 )

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    783                 batch_size=batch_size):
    784               callbacks.on_train_batch_begin(step)
--> 785               tmp_logs = train_function(iterator)
    786               # Catch OutOfRangeError for Datasets of unknown size.
    787               # This blocks until the batch has finished executing.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--> 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    625       # This is the first call of __call__, so we have to initialize.
    626       initializers = []
--> 627       self._initialize(args, kwds, add_initializers_to=initializers)
    628     finally:
    629       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    504     self._concrete_stateful_fn = (
    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 506             *args, **kwds))
    507 
    508     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444       args, kwargs = None, None
   2445     with self._lock:
-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2447     return graph_function
   2448 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-> 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--> 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--> 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:505 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:475 train_step  **
        self.trainable_variables)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1741 _minimize
        trainable_variables))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:525 _aggregate_gradients
        filtered_grads_and_vars = _filter_grads(grads_and_vars)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1203 _filter_grads
        ([v.name for _, v in grads_and_vars],))

    ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0', 'conv2d_6/kernel:0', 'conv2d_6/bias:0', 'conv2d_7/kernel:0', 'conv2d_7/bias:0', 'conv2d_8/kernel:0', 'conv2d_8/bias:0', 'conv2d_9/kernel:0', 'conv2d_9/bias:0', 'conv2d_transpose/kernel:0', 'conv2d_transpose/bias:0', 'conv2d_10/kernel:0', 'conv2d_10/bias:0', 'conv2d_11/kernel:0', 'conv2d_11/bias:0', 'conv2d_transpose_1/kernel:0', 'conv2d_transpose_1/bias:0', 'conv2d_12/kernel:0', 'conv2d_12/bias:0', 'conv2d_13/kernel:0', 'conv2d_13/bias:0', 'conv2d_transpose_2/kernel:0', 'conv2d_transpose_2/bias:0', 'conv2d_14/kernel:0', 'conv2d_14/bias:0', 'conv2d_15/kernel:0', 'conv2d_15/bias:0', 'conv2d_transpose_3/kernel:0', 'conv2d_transpose_3/bias:0', 'conv2d_16/kernel:0', 'conv2d_16/bias:0', 'conv2d_17/kernel:0', 'conv2d_17/bias:0', 'conv2d_18/kernel:0', 'conv2d_18/bias:0'].


**Describe the expected behavior**:
Could train model

**Standalone code to reproduce the issue** 


Thank you for looking into this"
38232,AttributeError: 'TFLiteConverterV2' object has no attribute 'experimental_new_quantizer' due to TensorFlow 2.2.0rc2  in colab.research.google.com ,"**Describe the problem**
In colab, the default TF version is not a stable version (2.2.0rc2)
You can check this by opening a new notebook in [colab.research.google.com](colab.research.google.com) and running:

```
pip list | grep tensorflow

tensorflow               2.2.0rc2    
.
.
   
```

**Solution: **
Change the tensorflow version from the unstable 2.2.0rc2 to stable 2.2.0

**Provide the exact sequence of commands / steps that you executed before running into the problem**

As a result of the default unstable version, if I try to convert a model to a full-integer quantized model it fails due to this error:
```
# Convert the model to the TensorFlow Lite format with quantization
converter = tf.lite.TFLiteConverter.from_keras_model(model)
def representative_dataset():
  for i in range(500):
    yield([x_train[i].reshape(1,1)])
converter.optimizations = [tf.lite.Optimize.DEFAULT] 
converter.representative_dataset = representative_dataset
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
model_tflite = converter.convert()

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-22-f5d45cb6910e> in <module>()
     12 converter.representative_dataset = representative_dataset
     13 converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
---> 14 model_tflite = converter.convert()
     15 
     16 # Save the model to disk

1 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type)
    265       return calibrate_quantize.calibrate_and_quantize(
    266           self.representative_dataset.input_gen, inference_input_type,
--> 267           inference_output_type, allow_float, self.experimental_new_quantizer)
    268 
    269   def _is_unknown_shapes_allowed(self):

AttributeError: 'TFLiteConverterV2' object has no attribute 'experimental_new_quantizer'
```

Removing the `converter.representative_dataset = representative_dataset` line, seems to work fine but is not a valid workflow for my use case.


**Current workaround**
If I uninstall `tensorflow==2.2.0rc2` and  instead install `tensorflow==2.2.0`, it seems to work as well. I simply run the following, at the beginning of my colab:

`pip install tensorflow==2.2.0`

This seems to resolve the issue.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
38231,Libs can not download from China when compile from source,"```
ERROR: An error occurred during the fetch of repository 'org_sqlite':
   java.io.IOException: Error downloading [http://mirror.tensorflow.org/www.sqlite.org/2019/sqlite-amalgamation-3280000.zip, https://www.sqlite.org/2019/sqlite-amalgamation-3280000.zip] to /home/jintian/.cache/bazel/_bazel_jintian/879bdee779be1c04fff5ab7e8f48b925/external/org_sqlite/sqlite-amalgamation-3280000.zip: All mirrors are down: [connect timed out]

```

this url is blocked inside china,

why using this download when compile? why not just make it --recursive when clone?"
38230,"TensorFlow hangs: 0% GPU, 1 CPU core @ 100% in cuModuleUnload","**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: No
- TensorFlow installed from (source or
binary): Both. Bug first appeared in TF2 python package, then was reproduced using TF2 compiled from source.
- TensorFlow version (use command below): Both 2.1 (from PyPI) and 2.0 (source) appear affected
- Python version: 3.6.6 x64
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from
source): Visual Studio Professional 2017
- CUDA/cuDNN version: 10.0 & 10.1
- GPU model and memory: 1080 & 1080 TI

**Describe the current behavior**

At a random time during training, TensorFlow just stops progressing. I've seen this happen both after 200 iterations = roughly 1 minute and after 3000 iterations. 

**Describe the expected behavior**

TensorFlow either continues training, or it crashes with an error message.

**Standalone code to reproduce the issue** 

This happens in a rather large and complex project with partially proprietary source code. That said, I do have a full dump of Python & TensorFlow in that state, which is 30 GB. 

**Other info / logs** 

Attached is a dump of the stack traces of all threads, created with VS2017 `Debug.ListCallStack /ShowLineOffset /AllThreads `

[tensorflow callstacks.txt](https://github.com/tensorflow/tensorflow/files/4432702/tensorflow.callstacks.txt)

**My Analysis** 

The Python main thread is stuck in tensorflow::KernelAndDeviceFunc::Run > nsync::nsync_cv_wait, which seems reasonable. The other threads are idle in openblas or Eigen::ThreadPoolTempl::WaitForWork, except for one suspicious thread.

The suspicious thread (ID 7020 in the attached dump) is looping around with 100% CPU usage inside cuModuleUnload, which was called by stream_executor::gpu::GpuDriver::UnloadModule from within stream_executor::KernelBase::~KernelBase inside tensorflow::CheckRedzones and apparently attempting to execute a tensorflow::Conv2DOp

```
Callstack for Thread 74 (Thread Id: 7020 (0x1b6c)):
 Index  Function
--------------------------------------------------------------------------------
 1      [External Code]  =  cuModuleUnload
*2      _pywrap_tensorflow_internal.pyd!stream_executor::gpu::GpuDriver::UnloadModule(stream_executor::gpu::GpuContext * context, CUmod_st * module) Line 762
 3      _pywrap_tensorflow_internal.pyd!stream_executor::gpu::GpuExecutor::UnloadGpuBinary(const void * gpu_binary) Line 339
 4      _pywrap_tensorflow_internal.pyd!stream_executor::gpu::GpuExecutor::UnloadKernel(const stream_executor::KernelBase * kernel) Line 357
 5      _pywrap_tensorflow_internal.pyd!stream_executor::KernelBase::~KernelBase() Line 79
 6      _pywrap_tensorflow_internal.pyd!stream_executor::cuda::RedzoneAllocator::CheckRedzones() Line 340
 7      _pywrap_tensorflow_internal.pyd!tensorflow::CheckRedzones(const stream_executor::cuda::RedzoneAllocator & rz_allocator, tensorflow::AutotuneResult * autotune_result) Line 618
 8      _pywrap_tensorflow_internal.pyd!tensorflow::LaunchConv2DOp<Eigen::GpuDevice,float>::operator()(tensorflow::OpKernelContext * ctx, bool use_cudnn, bool cudnn_use_autotune, const tensorflow::Tensor & input_param, const tensorflow::Tensor & filter, int row_dilation, int col_dilation, int row_stride, int col_stride, const tensorflow::Padding & padding, const std::vector<__int64,std::allocator<__int64> > & explicit_paddings, tensorflow::Tensor * output, tensorflow::TensorFormat data_format) Line 1062
 9      _pywrap_tensorflow_internal.pyd!tensorflow::Conv2DOp<Eigen::GpuDevice,float>::Compute(tensorflow::OpKernelContext * context) Line 546
 10     _pywrap_tensorflow_internal.pyd!tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel * op_kernel, tensorflow::OpKernelContext * context) Line 582
 11     _pywrap_tensorflow_internal.pyd!tensorflow::`anonymous namespace'::ExecutorState::Process(tensorflow::`anonymous-namespace'::ExecutorState::TaggedNode tagged_node, __int64 scheduled_nsec) Line 1844
 12     [External Code]
 13     _pywrap_tensorflow_internal.pyd!Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int thread_id) Line 326
 14     [External Code]
```


So there is a reasonable chance that this is a CUDA 10.1 bug, because both the PyPI TF 2.1 and my self-compiled TF 2.0 use CUDA 10.1 to avoid triggering https://github.com/tensorflow/tensorflow/issues/31166

This might be related to https://github.com/tensorflow/tensorflow/issues/37450 where others also report a CPU-spike upon training freezing with TF2.0 and TF2.1. That would imply the bug to be present in CUDA 10.0 and 10.1.

"
38229,No examples showing how to generate Arduino files from source for Arduino IDE,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):
No examples of generating Arduino IDE specific files from source (.cc) files

### Clear description
There is documentation and code (generate_microlite_projects() function,transform_arduino_source.py, etc.) suggesting that the Make build system allows for easily creating files and a directory from source files to be used in the Arduino IDE but there are no Arduino examples that show how this is or should be done. 

It would be useful to show how the hello_world project example was built for the Arduino IDE from the repo's source using Make.

### Usage example
Is there a usage example?
Not for Arduino."
38228,Named Dimensions,"**System information**

- TensorFlow version (you are using): 2.1
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

Currently, TensorFlow ops receive axes as integer indices, e.g. `tf.reduce_sum(tensor, axis=(2, 3))`. Keep track of axes after applying several operations on them and passing tensors around through multiple functions can be challenging.

I've seen many researchers and developers grab a sheet of paper and manually work through how axes should change. We can avoid this slow and often frustrating process by supporting optional axes names for tensors and having the most common ops annotate their outputs:

```python
image = tf.name_axes(image, ('B', 'H', 'W', 'C'))  # assert len(image.shape) == 4
print(image)  # <tf.Tensor dtype=uint8 shape={B: 500, H: 64, W: 64, C: 3}>

grid = tf.repeat(tf.repeat(image, 4, axis='H'), 4, axis='W')
print(grid)  # <tf.Tensor dtype=uint8 shape={B: 500, H: 256, W: 256, C: 3}>

gray = tf.reduce_mean(grid, axis='C')
print(gray)  # <tf.Tensor dtype=uint8 shape={B: 500, H: 256, W: 256}>

flipped = tf.transpose(gray[0], ('W', 'H'))
print(flipped)  # <tf.Tensor dtype=uint8 shape={W: 256, H: 256}>
```

A function starting with the line `tf.name_axes(...)` for its input tensors will immediately be easier to read. Moreover, developers could place `assert tensor.shape.names == ('B', 'F')` statements throughout their code so it becomes immediately clear from the written code how shapes are changing.

An advanced version of named axes would be to keep track of combined axes:

<details>

```python
video = tf.name_axes(image, ('B', 'T', 'H', 'W', 'C'))
print(video)  # <tf.Tensor dtype=uint8 shape={B: 500, T: 100, H: 64, W: 64, C: 3}>

frames = tf.reshape(frames, ('B*T', 'H', 'W', C'))
print(frames)  # <tf.Tensor dtype=uint8 shape={B*T: 50000, H: 64, W: 64, C: 3}>

logits = model(frames)
print(logits)  # <tf.Tensor dtype=float32 shape={B*T: 50000, 10}>

logits = tf.reshape(logits, ('B', 'T', 10))
print(logits)  # <tf.Tensor dtype=float32 shape={B: 500, T: 100, 10}>

logits = tf.name_axes(logits, ('B, 'T', 'K'))
print(logits)  # <tf.Tensor dtype=float32 shape={B: 500, T: 100, K: 10}>
```

</details>

**Will this change the current api? How?**

This would extend the current API in a fully backward compatible way. Common low-level TensorFlow operations (e.g. reduce_sum, concat, slice, tile) have to check for input tensors with named axes to annotate their outputs correctly.

**Who will benefit with this feature?**

Named axes have the potential to increase productivity of both researchers and engineers. Especially when developing large TensorFlow applications, named axes can help keep the overview. For product development with multiple developers, it can increase readability.

**Any Other info.**

JAX supports axes names. PyTorch 1.3 added experimental support for [named dimensions](https://pytorch.org/docs/stable/named_tensor.html)."
38227,train_speech_model.ipynb TUTORIAL with colab not working,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
 I am trying to use this to train a set, the base tutorial will not compile the train.py section. I am copying and pasting the commands to colab to test and get a MODULE NOT FOUND ERROR.
I have used this in the past about a month or so ago and it worked fine. Something must have been changed recently to cause this issue.

Here is the output I get when trying to run the ""Begin Training"" section:

2020-04-04 20:08:28.890032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""tensorflow/tensorflow/examples/speech_commands/train.py"", line 81, in <module>
    import input_data
  File ""/content/tensorflow/tensorflow/examples/speech_commands/input_data.py"", line 35, in <module>
    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio
ModuleNotFoundError: No module named 'tensorflow.contrib'

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
38226,from .pyrfc import *,"i already install 
C:\Python37>pip3 install pyrfc
Collecting pyrfc
  Downloading pyrfc-0.1.2.tar.gz (3.5 kB)
Installing collected packages: pyrfc
    Running setup.py install for pyrfc ... done
Successfully installed pyrfc-0.1.2

but when import i see this issue 

Traceback (most recent call last):
  File ""<pyshell#2>"", line 1, in <module>
    from .pyrfc import *
ModuleNotFoundError: No module named '__main__.pyrfc'; '__main__' is not a package"
38225,Creating a dynamic weight matrix that can be optimised,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- I have written the following code myself
- I am running it on google colab
- 
-pip installed tensorflow==1.15

- python 3.6

-Cuda 10.0 Cudnn 7:
- K80 GPU


### Describe the problem
This is a bug i am receiving. i would like to create a dynamic weight matrix that is isomorphic with the input vector. to that end i split all the individual integer digits from their numbers and use them as indices in tf.gather() in order to reorder the weight matrix. i encounter the problem that when the result of tf.gather() is set as the trainable weights i get the error ""ValueError: Tensor conversion requested dtype float64_ref for Tensor with dtype float64: <tf.Tensor 'training_18/Nadam/my_layer2_88/Variable_1/m/Initializer/zeros:0' shape=(400, 400) dtype=float64>""

I beleived that what this meant is that it needed the trainable weights to be a tensor. So i converted it to a tensor but then i got the error that tensor object doesnt support the operation in_graph_mode.

In the larger picture is what i am trying to do , have a different weight matrix permutation for each new input possible. if it is , what is the best way to go about it and is it amenable to batch training or can it only apply to batch sizes of one. 

### Source code / logs
this is my call function in my custom layer:

def call(self, y):
                 
            paddings = [[0,0],[0, 10-tf.shape(y)[0]]]
            out = tf.pad(y, paddings, 'CONSTANT', constant_values=2)
            x =tf.map_fn(lambda xi: tf.strings.format('{}', xi), out, dtype=tf.string)
            b= tf.compat.v1.string_split(x)
            b=tf.sparse.to_dense(b)
            b= tf.compat.v1.strings.to_number(b,tf.int32)
            print(b)
               
            S =  tf.Variable(lambda :tf.reshape((tf.gather((tf.reshape(self.kernels, [-1])),(b))), (400, 
                                             400)),trainable=True)
            #S=tf.convert_to_tensor(S)
            self._trainable_weights = [S]
            y= tf.cast(y, tf.float64)
                               
                     
             return K.dot(y,S)"
38224,Ensemble of custom sequential models,"I am trying to split the sequential models and combine them back in reference to the documentation on [CNN](https://www.tensorflow.org/tutorials/images/cnn)

```
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
tf.keras.backend.set_floatx('float64')
```
### Data
```
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()
# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0
```
### Model 1:

```
class Model1(tf.keras.Model):

  def __init__(self):
    super(Model1,self).__init__(name = 'Model1')
    self.model = models.Sequential()

  def call(self,inputs):
    # self.model.add(layers.Input(shape=(32, 32, 3)))
    self.model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(32,32,3)))
    self.model.add(layers.MaxPooling2D((2, 2)))
    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    self.model.add(layers.MaxPooling2D((2, 2)))
    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    
    return self.model
```
### Model 2:
```
class Model2(tf.keras.Model):
  def __init__(self):
    super(Model2,self).__init__(name = 'Model2')
    self.model = models.Sequential()
  def call(self,inputs):
    self.model.add(layers.Reshape((32,32,1),input_shape=(32,32,1))
    self.model.add(layers.Flatten())
    self.model.add(layers.Dense(64, activation='relu')) 
    self.model.add(layers.Dense(10))
    return self.model
```
### Ensemble of above two models:

```
class Ensemble(tf.keras.Model):
  def __init__(self,model1, model2):
    super(Ensemble,self).__init__(name = 'Ensemble')
    self.model = models.Sequential()
    self.model.add(model1)
    self.model.add(model2)

  def call(self,inputs):
    output = self.model(inputs)
    return output

# class Ensemble(tf.keras.Model):

#   def __init__(self,model1, model2):
#     super(Ensemble,self).__init__(name = 'Ensemble')
#     self.model1 = model1
#     self.model2 = model2
#   def call(self,inputs):
#     output = self.model1(inputs)
#     output = self.model2(output)
#     return output
```
Initialize and build the model

```
model1 = Model1()
model2 = Model2()
model = Ensemble(model1,model2)
model.build((32,32,3))
model.summary()
```
```Model: ""Ensemble""```
`_________________________________________________________________`
`Layer (type)                 Output Shape              Param #`   
`=================================================================`
`Model1 (Model1)              multiple                  56320`     
`_________________________________________________________________`
`Model2 (Model2)              multiple                  66250`     
`=================================================================`
`Total params: 122,570`
`Trainable params: 122,570`
`Non-trainable params: 0`

### Compile and fit the model
```
# Compile model
model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

# Train the model
history = model.fit(train_images, train_labels, epochs=10, batch_size=64,
                      validation_data=(test_images, test_labels))
```
### Returns:
`ValueError: The name ""conv2d"" is used 2 times in the model. All layer names should be unique.`

### Error trace below: 
```
Epoch 1/10
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-12-e612d6023946> in <module>()
      5 # Train the model
      6 history = model.fit(train_images, train_labels, epochs=10, batch_size=64,
----> 7                       validation_data=(test_images, test_labels))

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    783                 batch_size=batch_size):
    784               callbacks.on_train_batch_begin(step)
--> 785               tmp_logs = train_function(iterator)
    786               # Catch OutOfRangeError for Datasets of unknown size.
    787               # This blocks until the batch has finished executing.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--> 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    625       # This is the first call of __call__, so we have to initialize.
    626       initializers = []
--> 627       self._initialize(args, kwds, add_initializers_to=initializers)
    628     finally:
    629       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    504     self._concrete_stateful_fn = (
    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 506             *args, **kwds))
    507 
    508     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444       args, kwargs = None, None
   2445     with self._lock:
-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2447     return graph_function
   2448 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-> 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--> 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--> 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:505 train_function  *
        outputs = self.distribute_strategy.run(
    <ipython-input-5-a5fe0db529f5>:38 call  *
        output = self.model(inputs)
    <ipython-input-3-e360f03e53e3>:9 call  *
        self.model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(32,32,3)))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py:456 _method_wrapper  **
        result = method(self, *args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:224 add
        self._init_graph_network(self.inputs, self.outputs, name=self.name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py:456 _method_wrapper
        result = method(self, *args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:302 _init_graph_network
        self.inputs, self.outputs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:1798 _map_graph_network
        str(all_names.count(name)) + ' times in the model. '

    ValueError: The name ""conv2d"" is used 2 times in the model. All layer names should be unique.
```
### Bit of troubleshooting:

```
for i, la in enumerate(model.layers):
  print(la.name)
  for j, laye in enumerate(la.layers):
    print(laye.name)
    for k, lay in enumerate(laye.layers):
      print(lay.name) 
```
Returns:

```model1
sequential
conv2d
max_pooling2d
conv2d_1
max_pooling2d_1
conv2d_2
model2
sequential_1
reshape
flatten
dense
dense_1
```

#### The name scope 'conv2d' is invoked only once? Is there any documentation/ feature to combine two sequential models?
 
**System information** 
Tensorflow '2.2.0-rc2' : Colab 
 (https://colab.research.google.com/drive/1mlZJBh2BAsOGKziXZ9V60Web53c7vRI0)


"
38223,TF2.0 frozen ckpt model or h5 model to pb model appear wrong,"> freeze the graph reference from the below code
> 
> ```python
> from keras import backend as K
> import tensorflow as tf
> 
> def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):
>     """"""
>     Freezes the state of a session into a pruned computation graph.
> 
>     Creates a new computation graph where variable nodes are replaced by
>     constants taking their current value in the session. The new graph will be
>     pruned so subgraphs that are not necessary to compute the requested
>     outputs are removed.
>     @param session The TensorFlow session to be frozen.
>     @param keep_var_names A list of variable names that should not be frozen,
>                           or None to freeze all the variables in the graph.
>     @param output_names Names of the relevant graph outputs.
>     @param clear_devices Remove the device directives from the graph for better portability.
>     @return The frozen graph definition.
>     """"""
>     from tensorflow.python.framework.graph_util import convert_variables_to_constants
>     graph = session.graph
>     with graph.as_default():
>         freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))
>         output_names = output_names or []
>         output_names += [v.op.name for v in tf.global_variables()]
>         # Graph -> GraphDef ProtoBuf
>         input_graph_def = graph.as_graph_def()
>         if clear_devices:
>             for node in input_graph_def.node:
>                 node.device = """"
>         frozen_graph = convert_variables_to_constants(session, input_graph_def,
>                                                       output_names, freeze_var_names)
>         return frozen_graph
> 
> 
> frozen_graph = freeze_session(K.get_session(),
>                               output_names=[out.op.name for out in model.outputs]
> 
> tf.train.write_graph(frozen_graph, ""model"", ""tf_model.pb"", as_text=False)
> ```

hi，i use your demo complete frozen the ckpt model(checkpoint = tf.train.Checkpoint(myAwesomeModel=me), checkpoint.restore(tf.train.latest_checkpoint('examples'    ))) or h5 model (model.load_weights('model_enc.h5'))to pb model。after i run ckpt model and pb model，i find their result is different。and the result of ckpt model  is right。i check the pb model parmas， and find some op variable parmas is initial value，like “bias” is vector contain zero，“layer normal beta”  is vector contain zero eg.  i think it is  wrong to restore model, hope your give me some advice,thx"
38222,MutableGraphView::MutableGraphView error: node has self cycle fanin - But graph is not cyclic,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 

 - Python 3.6.6 on Fedora
 - `python3 -c 'import tensorflow as tf; print(tf.__version__)'` -> 2.1.0
 - `tf.version.GIT_VERSION` -> v2.1.0-rc2-17-ge5bf8de
 - TensorFlow installed using `pip3 install --upgrade tensorflow`
 - Running on AMD CPU, no GPU acceleration

**Describe the current behavior**

For graphs that are not cyclic, I sometimes receive messages like this:

```
2020-04-04 11:20:02.351817: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'model_4/up_sampling1d_2/concat' has self cycle fanin 'model_4/up_sampling1d_2/concat'.
2020-04-04 11:20:02.355486: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:02.357350: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:02.371116: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2020-04-04 11:20:02.372145: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'model_4/up_sampling1d_2/concat' has self cycle fanin 'model_4/up_sampling1d_2/concat'.
2020-04-04 11:20:02.373345: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:02.374334: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:02.377600: W tensorflow/core/common_runtime/process_function_library_runtime.cc:697] Ignoring multi-device function optimization failure: Invalid argument: The graph couldn't be sorted in topological order.
```

**Describe the expected behavior**

I expect that TensorFlow should not print these messages (and presumably, be able to topologically order the non-cyclic graph).

**Standalone code to reproduce the issue** 

```
import tensorflow as tf
import numpy as np
import random

random.seed(1)
np.random.seed(1)

input_data = np.random.random(20000).reshape(10000, 2)
output_data = np.sin(input_data)

json = '''{""class_name"": ""Model"", ""config"": {""name"": ""model_4"", ""layers"": [{""class_name"": ""InputLayer"", ""config"": {""batch_input_shape"": [null, 2], ""dtype"": ""float32"", ""sparse"": false, ""ragged"": false, ""name"": ""input_5""}, ""name"": ""input_5"", ""inbound_nodes"": []}, {""class_name"": ""Reshape"", ""config"": {""name"": ""reshape_2"", ""trainable"": true, ""dtype"": ""float32"", ""target_shape"": [1, 2]}, ""name"": ""reshape_2"", ""inbound_nodes"": [[[""input_5"", 0, 0, {}]]]}, {""class_name"": ""Reshape"", ""config"": {""name"": ""reshape_1"", ""trainable"": true, ""dtype"": ""float32"", ""target_shape"": [1, 2]}, ""name"": ""reshape_1"", ""inbound_nodes"": [[[""input_5"", 0, 0, {}]]]}, {""class_name"": ""Reshape"", ""config"": {""name"": ""reshape"", ""trainable"": true, ""dtype"": ""float32"", ""target_shape"": [2, 1]}, ""name"": ""reshape"", ""inbound_nodes"": [[[""input_5"", 0, 0, {}]]]}, {""class_name"": ""UpSampling1D"", ""config"": {""name"": ""up_sampling1d_2"", ""trainable"": true, ""dtype"": ""float32"", ""size"": 3}, ""name"": ""up_sampling1d_2"", ""inbound_nodes"": [[[""reshape_2"", 0, 0, {}]]]}, {""class_name"": ""UpSampling1D"", ""config"": {""name"": ""up_sampling1d_1"", ""trainable"": true, ""dtype"": ""float32"", ""size"": 3}, ""name"": ""up_sampling1d_1"", ""inbound_nodes"": [[[""reshape_1"", 0, 0, {}]]]}, {""class_name"": ""UpSampling1D"", ""config"": {""name"": ""up_sampling1d"", ""trainable"": true, ""dtype"": ""float32"", ""size"": 2}, ""name"": ""up_sampling1d"", ""inbound_nodes"": [[[""reshape"", 0, 0, {}]]]}, {""class_name"": ""Reshape"", ""config"": {""name"": ""reshape_5"", ""trainable"": true, ""dtype"": ""float32"", ""target_shape"": [2, 1]}, ""name"": ""reshape_5"", ""inbound_nodes"": [[[""input_5"", 0, 0, {}]]]}, {""class_name"": ""ZeroPadding1D"", ""config"": {""name"": ""zero_padding1d_1"", ""trainable"": true, ""dtype"": ""float32"", ""padding"": [0, 1]}, ""name"": ""zero_padding1d_1"", ""inbound_nodes"": [[[""up_sampling1d_2"", 0, 0, {}]]]}, {""class_name"": ""ZeroPadding1D"", ""config"": {""name"": ""zero_padding1d"", ""trainable"": true, ""dtype"": ""float32"", ""padding"": [0, 1]}, ""name"": ""zero_padding1d"", ""inbound_nodes"": [[[""up_sampling1d_1"", 0, 0, {}]]]}, {""class_name"": ""Conv1D"", ""config"": {""name"": ""conv1d"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 2, ""kernel_size"": [1], ""strides"": [1], ""padding"": ""valid"", ""data_format"": ""channels_last"", ""dilation_rate"": [1], ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""name"": ""conv1d"", ""inbound_nodes"": [[[""up_sampling1d"", 0, 0, {}]]]}, {""class_name"": ""MaxPooling1D"", ""config"": {""name"": ""max_pooling1d"", ""trainable"": true, ""dtype"": ""float32"", ""strides"": [2], ""pool_size"": [2], ""padding"": ""valid"", ""data_format"": ""channels_last""}, ""name"": ""max_pooling1d"", ""inbound_nodes"": [[[""reshape_5"", 0, 0, {}]]]}, {""class_name"": ""Reshape"", ""config"": {""name"": ""reshape_3"", ""trainable"": true, ""dtype"": ""float32"", ""target_shape"": [2, 1]}, ""name"": ""reshape_3"", ""inbound_nodes"": [[[""input_5"", 0, 0, {}]]]}, {""class_name"": ""Conv1D"", ""config"": {""name"": ""conv1d_1"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 4, ""kernel_size"": [1], ""strides"": [1], ""padding"": ""valid"", ""data_format"": ""channels_last"", ""dilation_rate"": [1], ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""name"": ""conv1d_1"", ""inbound_nodes"": [[[""zero_padding1d_1"", 0, 0, {}]]]}, {""class_name"": ""Dot"", ""config"": {""name"": ""dot"", ""trainable"": true, ""dtype"": ""float32"", ""axes"": -1, ""normalize"": false}, ""name"": ""dot"", ""inbound_nodes"": [[[""zero_padding1d"", 0, 0, {}], [""conv1d"", 0, 0, {}]]]}, {""class_name"": ""SpatialDropout1D"", ""config"": {""name"": ""spatial_dropout1d"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.4, ""noise_shape"": null, ""seed"": null}, ""name"": ""spatial_dropout1d"", ""inbound_nodes"": [[[""max_pooling1d"", 0, 0, {}]]]}, {""class_name"": ""GlobalAveragePooling1D"", ""config"": {""name"": ""global_average_pooling1d"", ""trainable"": true, ""dtype"": ""float32"", ""data_format"": ""channels_last""}, ""name"": ""global_average_pooling1d"", ""inbound_nodes"": [[[""reshape_3"", 0, 0, {}]]]}, {""class_name"": ""Dot"", ""config"": {""name"": ""dot_1"", ""trainable"": true, ""dtype"": ""float32"", ""axes"": -1, ""normalize"": false}, ""name"": ""dot_1"", ""inbound_nodes"": [[[""conv1d_1"", 0, 0, {}], [""dot"", 0, 0, {}]]]}, {""class_name"": ""AlphaDropout"", ""config"": {""name"": ""alpha_dropout"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.24308844217362846}, ""name"": ""alpha_dropout"", ""inbound_nodes"": [[[""spatial_dropout1d"", 0, 0, {}]]]}, {""class_name"": ""Reshape"", ""config"": {""name"": ""reshape_4"", ""trainable"": true, ""dtype"": ""float32"", ""target_shape"": [1, 1]}, ""name"": ""reshape_4"", ""inbound_nodes"": [[[""global_average_pooling1d"", 0, 0, {}]]]}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.4, ""noise_shape"": null, ""seed"": null}, ""name"": ""dropout"", ""inbound_nodes"": [[[""dot_1"", 0, 0, {}]]]}, {""class_name"": ""Flatten"", ""config"": {""name"": ""flatten"", ""trainable"": true, ""dtype"": ""float32"", ""data_format"": ""channels_last""}, ""name"": ""flatten"", ""inbound_nodes"": [[[""alpha_dropout"", 0, 0, {}]]]}, {""class_name"": ""GlobalAveragePooling1D"", ""config"": {""name"": ""global_average_pooling1d_1"", ""trainable"": true, ""dtype"": ""float32"", ""data_format"": ""channels_last""}, ""name"": ""global_average_pooling1d_1"", ""inbound_nodes"": [[[""reshape_4"", 0, 0, {}]]]}, {""class_name"": ""Flatten"", ""config"": {""name"": ""flatten_1"", ""trainable"": true, ""dtype"": ""float32"", ""data_format"": ""channels_last""}, ""name"": ""flatten_1"", ""inbound_nodes"": [[[""dropout"", 0, 0, {}]]]}, {""class_name"": ""Concatenate"", ""config"": {""name"": ""concatenate"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": -1}, ""name"": ""concatenate"", ""inbound_nodes"": [[[""input_5"", 0, 0, {}], [""flatten"", 0, 0, {}], [""global_average_pooling1d_1"", 0, 0, {}], [""flatten_1"", 0, 0, {}]]]}, {""class_name"": ""Dense"", ""config"": {""name"": ""dense_4"", ""trainable"": true, ""dtype"": ""float32"", ""units"": 2, ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""name"": ""dense_4"", ""inbound_nodes"": [[[""concatenate"", 0, 0, {}]]]}], ""input_layers"": [[""input_5"", 0, 0]], ""output_layers"": [[""dense_4"", 0, 0]]}, ""keras_version"": ""2.2.4-tf"", ""backend"": ""tensorflow""}'''

model = tf.keras.models.model_from_json(json)
model.compile(loss = 'mse', optimizer = 'adam')

model.summary()

model.fit(x = input_data.reshape(-1, 2), y = output_data.reshape(-1, 2), epochs = 3, validation_split = 0.2)
```

This runs and produces the following output:

```
2020-04-04 11:19:57.409765: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-04-04 11:19:57.409910: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-04-04 11:19:57.409938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-04-04 11:20:00.326542: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-04-04 11:20:00.326615: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2020-04-04 11:20:00.326705: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sapling6): /proc/driver/nvidia/version does not exist
2020-04-04 11:20:00.357649: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2195855000 Hz
2020-04-04 11:20:00.358616: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b2db3a0250 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-04 11:20:00.358698: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Model: ""model_4""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 2)]          0                                            
__________________________________________________________________________________________________
reshape_2 (Reshape)             (None, 1, 2)         0           input_5[0][0]                    
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 1, 2)         0           input_5[0][0]                    
__________________________________________________________________________________________________
reshape (Reshape)               (None, 2, 1)         0           input_5[0][0]                    
__________________________________________________________________________________________________
up_sampling1d_2 (UpSampling1D)  (None, 3, 2)         0           reshape_2[0][0]                  
__________________________________________________________________________________________________
up_sampling1d_1 (UpSampling1D)  (None, 3, 2)         0           reshape_1[0][0]                  
__________________________________________________________________________________________________
up_sampling1d (UpSampling1D)    (None, 4, 1)         0           reshape[0][0]                    
__________________________________________________________________________________________________
reshape_5 (Reshape)             (None, 2, 1)         0           input_5[0][0]                    
__________________________________________________________________________________________________
zero_padding1d_1 (ZeroPadding1D (None, 4, 2)         0           up_sampling1d_2[0][0]            
__________________________________________________________________________________________________
zero_padding1d (ZeroPadding1D)  (None, 4, 2)         0           up_sampling1d_1[0][0]            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 4, 2)         4           up_sampling1d[0][0]              
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 1, 1)         0           reshape_5[0][0]                  
__________________________________________________________________________________________________
reshape_3 (Reshape)             (None, 2, 1)         0           input_5[0][0]                    
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 4, 4)         12          zero_padding1d_1[0][0]           
__________________________________________________________________________________________________
dot (Dot)                       (None, 4, 4)         0           zero_padding1d[0][0]             
                                                                 conv1d[0][0]                     
__________________________________________________________________________________________________
spatial_dropout1d (SpatialDropo (None, 1, 1)         0           max_pooling1d[0][0]              
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 1)            0           reshape_3[0][0]                  
__________________________________________________________________________________________________
dot_1 (Dot)                     (None, 4, 4)         0           conv1d_1[0][0]                   
                                                                 dot[0][0]                        
__________________________________________________________________________________________________
alpha_dropout (AlphaDropout)    (None, 1, 1)         0           spatial_dropout1d[0][0]          
__________________________________________________________________________________________________
reshape_4 (Reshape)             (None, 1, 1)         0           global_average_pooling1d[0][0]   
__________________________________________________________________________________________________
dropout (Dropout)               (None, 4, 4)         0           dot_1[0][0]                      
__________________________________________________________________________________________________
flatten (Flatten)               (None, 1)            0           alpha_dropout[0][0]              
__________________________________________________________________________________________________
global_average_pooling1d_1 (Glo (None, 1)            0           reshape_4[0][0]                  
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 16)           0           dropout[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20)           0           input_5[0][0]                    
                                                                 flatten[0][0]                    
                                                                 global_average_pooling1d_1[0][0] 
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 2)            42          concatenate[0][0]                
==================================================================================================
Total params: 58
Trainable params: 58
Non-trainable params: 0
__________________________________________________________________________________________________
Train on 8000 samples, validate on 2000 samples
Epoch 1/3
2020-04-04 11:20:02.351817: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'model_4/up_sampling1d_2/concat' has self cycle fanin 'model_4/up_sampling1d_2/concat'.
2020-04-04 11:20:02.355486: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:02.357350: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:02.371116: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2020-04-04 11:20:02.372145: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'model_4/up_sampling1d_2/concat' has self cycle fanin 'model_4/up_sampling1d_2/concat'.
2020-04-04 11:20:02.373345: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:02.374334: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:02.377600: W tensorflow/core/common_runtime/process_function_library_runtime.cc:697] Ignoring multi-device function optimization failure: Invalid argument: The graph couldn't be sorted in topological order.
7904/8000 [============================>.] - ETA: 0s - loss: 0.16842020-04-04 11:20:04.124781: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'model_4/up_sampling1d_2/concat' has self cycle fanin 'model_4/up_sampling1d_2/concat'.
2020-04-04 11:20:04.126366: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:04.127223: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:04.132842: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2020-04-04 11:20:04.133249: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'model_4/up_sampling1d_2/concat' has self cycle fanin 'model_4/up_sampling1d_2/concat'.
2020-04-04 11:20:04.133728: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:04.134178: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-04-04 11:20:04.135585: W tensorflow/core/common_runtime/process_function_library_runtime.cc:697] Ignoring multi-device function optimization failure: Invalid argument: The graph couldn't be sorted in topological order.
8000/8000 [==============================] - 3s 429us/sample - loss: 0.1671 - val_loss: 0.0197
Epoch 2/3
8000/8000 [==============================] - 2s 204us/sample - loss: 0.0260 - val_loss: 0.0089
Epoch 3/3
8000/8000 [==============================] - 2s 204us/sample - loss: 0.0112 - val_loss: 0.0054
```
"
38221,Distributed tensorflow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
38220,Request to have ConvLSTM2D for TFLite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Android 6.0
- TensorFlow installed from (source or binary): Anaconda, 
- TensorFlow version (or github SHA if from source): 1.15


**Provide the text output from tflite_convert**


```
import tensorflow as tf
MODEL_PATH = f'final_{name}.hdf5'
converter = tf.lite.TFLiteConverter.from_keras_model_file(MODEL_PATH)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()
open(""test.tflite"", ""wb"").write(tflite_model)
WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\tensorflow_core\lite\python\util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\tensorflow_core\python\framework\graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 9 variables.
INFO:tensorflow:Converted 9 variables to const ops.
Traceback (most recent call last):

  File ""<ipython-input-8-a668afd61005>"", line 7, in <module>
    tflite_model = converter.convert()

  File ""C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\tensorflow_core\lite\python\lite.py"", line 983, in convert
    **converter_kwargs)

  File ""C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)

  File ""C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))

ConverterError: See console for info.
The system cannot find the path specified.
2020-04-04 18:18:56.937955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-04-04 18:19:02.693081: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.693268: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.693408: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.693578: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.693922: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.694124: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.694364: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-04-04 18:19:02.694576: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-04-04 18:19:02.694796: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-04-04 18:19:02.695011: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-04-04 18:19:02.695242: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.695478: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.695663: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-04-04 18:19:02.695871: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.696067: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.696287: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-04-04 18:19:02.696509: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-04-04 18:19:02.696735: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.696926: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.697133: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-04-04 18:19:02.697378: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-04-04 18:19:02.697585: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-04-04 18:19:02.697834: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-04-04 18:19:02.698063: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-04-04 18:19:02.698388: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-04-04 18:19:02.698730: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-04-04 18:19:02.698982: F tensorflow/lite/toco/tooling_util.cc:1474] Should not get here: 5
Fatal Python error: Aborted

Current thread 0x00004d20 (most recent call first):
  File ""C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 52 in execute
  File ""C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\absl\app.py"", line 250 in _run_main
  File ""C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\absl\app.py"", line 299 in run
  File ""C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40 in run
  File ""C:\Users\user\Anaconda3\envs\tfgpu\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 89 in main
  File ""C:\Users\user\Anaconda3\envs\tfgpu\Scripts\toco_from_protos-script.py"", line 10 in <module>
```

**Standalone code to reproduce the issue** 
```
from keras.layers.wrappers import TimeDistributed
from keras.layers import Dense,Flatten,Input,concatenate,Dot, Conv2D,Reshape, MaxPooling2D, UpSampling2D,Conv3DTranspose, ZeroPadding2D,Conv3D,Conv2DTranspose, BatchNormalization, Dropout
from keras.layers import Conv2D, MaxPooling2D, Flatten, Input, LSTM, Dropout, Dense, Concatenate, TimeDistributed, Permute
from keras.models import Model,Sequential
from keras.layers.convolutional_recurrent import ConvLSTM2D
from keras.callbacks import TensorBoard
import keras.losses as losses
import numpy as np




# create our model here
kernel_size = 3
nfilters_lstm = 16

Nbatch=1
Ntime=10
Nx=32
Ny=32
Nchannel=1


# define an input layer (Ntime, Nbatch, Nx, Ny, Nchannel)
inputs = Input(name='x_input', dtype='float32',batch_shape=(Nbatch, Ntime, Nx, Ny, Nchannel)) 

'''define model'''
# convLSTM 1
ConvLSTM_1= ConvLSTM2D(filters=nfilters_lstm , kernel_size=(kernel_size, kernel_size)
                   , data_format='channels_last'
                   , recurrent_activation='hard_sigmoid'
                   , activation='tanh'
                   , padding='same', return_sequences=False)(inputs)

BatchNorm_1 = BatchNormalization()(ConvLSTM_1)

# conv2D 
Conv_1 = Conv2D(1, kernel_size, strides=(1, 1), padding='same', data_format='channels_last',  activation='relu')(BatchNorm_1)
    
# define the output
model = Model(inputs=inputs, outputs=Conv_1, name='Model ')

model.compile(loss=losses.categorical_crossentropy,
              optimizer='adam',
              metrics=['accuracy'])


model.save('test.hdf5')


import tensorflow as tf
MODEL_PATH = 'test.hdf5'
model = tf.keras.models.load_model(MODEL_PATH)
model.summary()
converter = tf.lite.TFLiteConverter.from_keras_model_file(MODEL_PATH)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""3x9.tflite"", ""wb"").write(tflite_model)
```

**GRAPH:**
```
Model: ""Model ""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
x_input (InputLayer)         [(1, 10, 32, 32, 1)]      0         
_________________________________________________________________
conv_lst_m2d_1 (ConvLSTM2D)  (1, 32, 32, 16)           9856      
_________________________________________________________________
batch_normalization_1 (Batch (1, 32, 32, 16)           64        
_________________________________________________________________
conv2d_1 (Conv2D)            (1, 32, 32, 1)            145       
=================================================================
Total params: 10,065
Trainable params: 10,033
Non-trainable params: 32
_________________________________________________________________
```


**Any other info / logs**

The TFLite converter fails to convert the convLSTM2D layer since it has 5 dimensions. Is there any way to make it work? Any hacky workaround? Would be great to see that in the next TF versions somehow! "
38219,Tensorflow version downgrade ,"ModuleNotFoundError: No module named 'tensorflow.contrib' 
This is the error i keep getting when i try to load my video file for YOLO object detection using darkflow. I tried using pip to downgrade the tensorflow version but its not changing the output.
"
38218,Smart compose using tensorflow,"Hey I am trying to create a API which can smart compose. Anyone that can help me out ?
"
38217,load_model cause memory leak.,"model = load_model('.\Models\LSTM-RAdam-batchsize50-Data5040\Algorithm-LSTM-RAdam-batchsize50-Data5040-0.h5')
if i use tensorflow2.0 will got this bug,and tensorflow2.1.0 no this bug.


save.py 146  2020-04-04 20:14:39.885989
hdf5_format.py 153  2020-04-04 20:14:39.886990
hdf5_format.py 159  2020-04-04 20:14:39.886990
model_config.py 54  2020-04-04 20:14:39.890988
hdf5_format.py 169  2020-04-04 20:14:45.900547
hdf5_format.py 172  2020-04-04 20:14:46.118488
optimizer_v2.py 253  2020-04-04 20:14:46.118488
hdf5_format.py 185  2020-04-04 20:14:48.779033
hdf5_format.py 193  2020-04-04 20:14:48.779033
traing.py 2094  2020-04-04 20:14:48.779033
traing.py 2112  2020-04-04 20:14:48.822994
traing.py 2116  2020-04-04 20:14:48.823996
optimizer_v2.py 501  2020-04-04 20:14:48.823996
optimizer_v2.py 390 2020-04-04 20:14:48.823996
optimizer_v2.py 393  2020-04-04 20:14:48.824993
gradients_impl.py 154  2020-04-04 20:14:48.824993
gradients_impl.py 156  2020-04-04 20:14:48.824993
gradients_util.py 504  2020-04-04 20:14:48.824993 513.27 MB   0
gradients_util.py 680  <function _AddGrad at 0x0000024A9360BD90>
_MaybeCompile 340  513.27 MB  
_MaybeCompile 350  513.27 MB  
_MaybeCompile 358  <function _GradientsHelper.<locals>.<lambda> at 0x0000024AA779C730> 513.27 MB  
gradients_util.py 682 
gradients_util.py 711 
...
...
gradients_util.py 711 
gradients_util.py 680  <function _IfGrad at 0x0000024A9609A488>
_MaybeCompile 340  544.09 MB  
_MaybeCompile 350  544.09 MB  
_MaybeCompile 358  <function _GradientsHelper.<locals>.<lambda> at 0x0000024AAC1750D0> **544.09** MB  
gradients_util.py 504  2020-04-04 20:14:52.229995 **571.5** MB   1
gradients_util.py 680  <function _IdGrad at 0x0000024A935BCE18>
_MaybeCompile 340  571.5 MB  
...
...
gradients_util.py 680  <function _IfGrad at 0x0000024A9609A488>
_MaybeCompile 340  2058.01 MB  
_MaybeCompile 350  2058.01 MB  
_MaybeCompile 358  <function _GradientsHelper.<locals>.<lambda> at 0x0000024B0A7B08C8> **2058.01** MB  
gradients_util.py 504  2020-04-04 20:15:10.104159 **2101.93** MB   84


gradients()-->_GradientsHelper()-->_MaybeCompile()-->grad_fn()(_GradientsHelper)

"
38216,[saved_model_cli] cannot import name 'saved_model_aot_compile' from 'tensorflow.python.tools'  [2.2.0-rc2],"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): yes
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): No
- TensorFlow version (use command below):  v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2
- Python version: 3.7.6
- Bazelversion (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: - GPU model and memory: NA

**Describe the current behavior**
`saved_model_cl`i
```
Traceback (most recent call last):
  File ""/Users/tarrade/anaconda-release/conda-env/env_test/bin/saved_model_cli"", line 5, in <module>
    from tensorflow.python.tools.saved_model_cli import main
  File ""/Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 51, in <module>
    from tensorflow.python.tools import saved_model_aot_compile
ImportError: cannot import name 'saved_model_aot_compile' from 'tensorflow.python.tools' (/Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages/tensorflow/python/tools/__init__.py)
```

**Describe the expected behavior**
with TF 2.1:
`saved_model_cli`
```
usage: saved_model_cli [-h] [-v] {show,run,scan,convert} ...
saved_model_cli: error: too few arguments
```
and if we pass all needed argument then it is working as expected

**Standalone code to reproduce the issue** 
just use the command line:
`saved_model_cli`
"
38215,iterating over `tf.Tensor` is not allowed when using autograph,"I am using tf2.1.0, but when I use `tf.range` within a funtion decorated by `tf.function` it throw an error. My code is blow
```
for batch in tf.range(batch_size):
    pass
```
and here is the error message
```
OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function
```
It works for tf2.0.0

So, I wonder how to use `tf.range` when using **Autograph**."
38213,[XLA:GPU] Memory Leak in r1.15 due to defer host callbacks,"**System information** 
- Have I written custom code: 
yes
- OS Platform and Distribution: 
centos
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
`pip install tensorflow-gpu==1.15.0`
- Python version:
python3.6
- CUDA/cuDNN version: - GPU model and memory:
cuda 10.0

**Describe the current behavior**
There is memory leak observed in `ThenRunAfterNextBlockHostUntilDone` when XLA is turned on. The reason for the memory leak is that in tensorflow `BlockHostUntilDone()` is not called and large amount of defered callback was accumulated.

**Describe the expected behavior**
No memory leaking in the function mentioned above.

**Standalone code to reproduce the issue** 
Probably any code that use XLA in r1.15.

**Other info / logs** 
The memory leaking problem was solved in master branch in commit 80851c0ad. I wonder if I can help to cherry-pick it to r1.15? Because it would be inconvenient to upgrade tf in our environment.
"
38212,tf.py_function could return a dictionary of tensors,"Usually, a transformers tokenizer encodes an input as a dictionary.
```python
{""input_ids"": tf.int32, ""attention_mask"": tf.int32, ""token_type_ids"": tf.int32}
```
And to archive better performance handling with a large dataset, is a good practice implement a pipeline which includes using [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) to apply a tokenizer function to each element of an input dataset. Exactly the same as done in the Tensorflow tutorial: [Load text](https://www.tensorflow.org/tutorials/load_data/text#encode_examples). 

However, the [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) (used to wrap the map python function) doesn't support returning a dictionary of tensors as shown above.

For instance, if the tokenizer (encoder) in the [Load text](https://www.tensorflow.org/tutorials/load_data/text#encode_examples) returns the following dictionary:

```python
{
    ""input_ids"": [ 101, 13366,  2131,  1035,  6819,  2094,  1035,  102 ],
    ""attention_mask"": [ 1, 1, 1, 1, 1, 1, 1, 1 ]
}
```
how can someone set the `Tout` parameter of the [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) to get the desired dictionary of tensors:

```python
{
    'input_ids': <tf.Tensor: shape=(16,), dtype=int32, numpy = array(
    [ 101, 13366,  2131,  1035,  6819,  2094,  1035,  102 ], dtype=int32)>

    'attention_mask': <tf.Tensor: shape=(16,), dtype=int32, numpy=array(
     [ 1, 1, 1, 1, 1, 1, 1, 1 ], dtype=int32)>
}
```
?





"
38211,Missing Trainable Variables and Variables,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 19.04
- TensorFlow installed from (source or
binary): TF 2.1 installed from pip
- Python version: Python 3.7.5


```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import tensorflow as tf

class FooLayer(tf.keras.layers.Layer):
    def __init__(self, siz):
        super(FooLayer, self).__init__()
        self.siz = siz
        self.buildFoo(siz)

    def call(self, in_data):
        Foo0 = tf.multiply(in_data,self.FooTns0)
        FooList = []
        FooList.append(Foo0)
        for it in range(1,self.siz+1):
            tmp = tf.multiply(FooList[it-1],self.FooTns[it-1])
            FooList.append(tmp)
        return FooList[self.siz]

    def buildFoo(self,siz):
        self.FooTns0 = tf.Variable(1, name=""TNS0"")
        self.FooTns = []
        for it in range(0,self.siz):
            self.FooTns.append(tf.Variable(it, name=""TNS""+str(it+1)))

class FooModel(tf.keras.Model):
    def __init__(self, siz):
        super(FooModel, self).__init__()
        self.flayer = FooLayer(siz)

    def call(self, in_data):
        return self.flayer(in_data)

model = FooModel(5)

for v in model.trainable_variables:
    print(v.name)

for v in model.variables:
    print(v.name)
```


The output currently is only:
```
TNS0:0
TNS0:0
```

While the expected output is listing all 6 tensors, ''self.FooTns0'' and ''self.FooTns''."
38210,optimize keyword argument for tf.einsum,"I am trying to use the ```optimize``` keyword argument for the einsum operation, but I receive the following error:

```
    ....
    my_path/file.py:27 basis_transform_conv2d  *
        out = tf.einsum('bhwcfg,fgcu->bhwfgu', x_col, basis, optimize='optimal')
    Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\ops\special_math_ops.py:252 einsum
        [format(key) for key in sorted(list(kwargs.keys()))]))

    TypeError: invalid keyword arguments for this function: optimize
```

I am currently using tensorflow-gpu==2.0.0 with up-to-date CUDA, cudnn (everything else works fine).
It is to my understanding that tensorflow is calling the legacy einsum operation, einsum_v1 : https://github.com/tensorflow/tensorflow/blob/44d3065391ea2188979ff2e747f4e7b789edd2e6/tensorflow/python/ops/special_math_ops.py#L407

How do I ensure my code calls einsum_v2, which supports the ```optimize``` keyword argument?"
38207,Idk,"

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Current revision enterprise:
- **TensorFlow installed from (source or binary)**: Using Pip 
- **TensorFlow version (use command below)**:2.1(Tensorflow wont even let me check
- **Python version**:3.7.7
- **CUDA/cuDNN version**:10.1.243; 7.6.5.32;
- **GPU model and memory**:gtx 1060 6gb
- **TensorRT**: 6.0.1.5
- **Exact command to reproduce**:Legit any tf command I

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I'm unsure what the error in my config is I believe I have everything installed properly everything is in the path directories as they should be but I still get this error.

### Source code / logs
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\H4MST3R\Desktop\Rock-Paper-Scissors-Image-Classifier-Using-Deep-Learning\gather_images.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\H4MST3R\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

.
"
38206,pixelwise-loss weight map part 2,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
This a continuation of 

https://github.com/tensorflow/tensorflow/issues/38060

which I think might have been prematurely closed.  It was said that tensorflow 2.x allows for pixelwise-loss map.  

However, I looked at  https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit and pasted the relevant parts.  These options, I don't think, will be adequate for the weights needed for pixelwise-loss-weights.  For 2D images, weights will need to be shape [None, x_dim, y_dim, 1 or 3 or 4].  You see, the weights also need to be passed through  tf.keras.preprocessing.image. ImageDataGenerator along with X_train, Y_train.  So X_datagen, W_datagen (for weights), and Y_datagen all need all to be passed into Model.fit.  I don't see how to do that.

   - *class_weight*: Optional dictionary mapping class indices (integers)
   to a weight (float) value, used for weighting the loss function (during
   training only). This can be useful to tell the model to ""pay more
   attention"" to samples from an under-represented class.
   - *sample_weight*: Optional Numpy array of weights for the training
   samples, used for weighting the loss function (during training only). You
   can either pass a flat (1D) Numpy array with the same length as the input
   samples (1:1 mapping between weights and samples), or in the case of
   temporal data, you can pass a 2D array with shape (samples,
   sequence_length), to apply a different weight to every timestep of every
   sample. In this case you should make sure to specify
   sample_weight_mode=""temporal"" in compile(). This argument is not
   supported when x is a dataset, generator, or keras.utils.Sequence
   <https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence> instance,
   instead provide the sample_weights as the third element of x.

**Will this change the current api? How?**
It will add more versatility to the api by incorporating an important aspect of computer vision.

**Who will benefit with this feature?**
Those using keras to implement computer vision programs.

**Any Other info.**
Thank you kindly for your consideration.
"
38205,[XLA] bazel tests broken,"When running XLA C++ tests with bazel, many of them fails with an error like:

```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/compiler/xla/client/lib:arithmetic_test_gpu
-----------------------------------------------------------------------------
[==========] Running 4 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 4 tests from ArithmeticTest
[ RUN      ] ArithmeticTest.ArgMinR2Axis0
2020-04-03 16:57:48.451927: I tensorflow/compiler/xla/service/platform_util.cc:72] platform CUDA present but no XLA compiler available: could not find registered compiler for platform CUDA -- check target linkage (hint: try linking in tensorflow/compiler/jit:xla_gpu_jit)
2020-04-03 16:57:48.451981: F tensorflow/compiler/xla/tests/client_library_test_base.cc:48] Non-OK-status: result.status() status: Not found: no platforms found could not create local client for testing
external/bazel_tools/tools/test/test-setup.sh: line 310: 14802 Aborted                 (core dumped) ""${TEST_PATH}"" ""$@"" 2>&1
```

How to reproduce:

```
docker run --gpus all -it --rm tensorflow/tensorflow:devel-gpu /bin/bash
cd /tensorflow_src
./configure
git show
```
```
commit 09910a17749e7836462b843b537ffeccd302d260 (HEAD -> master, origin/master,
origin/HEAD)
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Mar 31 04:44:27 2020 -0700

    Fixed accumulator precision for generic DepthWise implementation.
    Removed inlined constants for kernel sizes.

    PiperOrigin-RevId: 303938922
    Change-Id: I4f1359341b8aa4a9c70b4c9f786657e5cbfdf645
```
```
bazel test -j 24 --config=opt -s  --config=noaws --config=nogcp --config=nohdfs --verbose_failures --config=cuda --cache_test_results=no --runs_per_test=1 --flaky_test_attempts=1 --test_tag_filters=gpu //tensorflow/compiler/xla/client/lib:arithmetic_test
cat /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/xla/client/lib/arithmetic_test_gpu/test.log
```

See error above:

Now try with the current HEAD:

```
git pull
git show
```
```
commit f0571998d0195b5b243cf409a64d5fa17bd44d43 (HEAD -> master, origin/master, origin/HEAD)
Merge: 2921d2ed93 2082d706a5
Author: TensorFlower Gardener <gardener@tensorflow.org>
Date:   Fri Apr 3 09:29:53 2020 -0700

    Merge pull request #34218 from Flamefire:fix_missing_linker_path

    PiperOrigin-RevId: 304630859
    Change-Id: I3e412ed958acd0d60c8ddbdb22fd59ddf9caf05b
```
```
bazel test -j 24 --config=opt -s  --config=noaws --config=nogcp --config=nohdfs --verbose_failures --config=cuda --cache_test_results=no --runs_per_test=1 --flaky_test_attempts=1 --test_tag_filters=gpu //tensorflow/compiler/xla/client/lib:arithmetic_test
cat /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/xla/client/lib/arithmetic_test_gpu/test.log
```

Same error.


I have this on 2 different computers."
38202,tensorflow.keras.constraints.RadialConstraint causes exception when training,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10  Enterprise 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: No
- TensorFlow installed from (source or
binary): pip
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: Could not determine
- GPU model and memory: Intel UHD Graphics 630 8gb / NVIDIA Quadro P1000 12gb

**Describe the current behavior**
Attempting to train a simple CNN with the RadialConstraint as the kernel constraint for the Conv2D layers throws an exception with the first batch:

```
ValueError                                Traceback (most recent call last)
<ipython-input-4-2e0f3adf9bdb> in <module>
     26 
     27 #train the model
---> 28 model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    817         max_queue_size=max_queue_size,
    818         workers=workers,
--> 819         use_multiprocessing=use_multiprocessing)
    820 
    821   def evaluate(self,

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    340                 mode=ModeKeys.TRAIN,
    341                 training_context=training_context,
--> 342                 total_epochs=epochs)
    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    344 

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    126         step=step, mode=mode, size=current_batch_size) as batch_logs:
    127       try:
--> 128         batch_outs = execution_function(iterator)
    129       except (StopIteration, errors.OutOfRangeError):
    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in execution_function(input_fn)
     96     # `numpy` translates Tensors to values in Eager mode.
     97     return nest.map_structure(_non_none_constant_value,
---> 98                               distributed_function(input_fn))
     99 
    100   return execution_function

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
    566         xla_context.Exit()
    567     else:
--> 568       result = self._call(*args, **kwds)
    569 
    570     if tracing_count == self._get_tracing_count():

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
    630         # Lifting succeeded, so variables are initialized and we can run the
    631         # stateless function.
--> 632         return self._stateless_fn(*args, **kwds)
    633     else:
    634       canon_args, canon_kwds = \

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\eager\function.py in __call__(self, *args, **kwargs)
   2360     """"""Calls a graph function specialized to the inputs.""""""
   2361     with self._lock:
-> 2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2364 

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\eager\function.py in _maybe_define_function(self, args, kwargs)
   2701 
   2702       self._function_cache.missed.add(call_context_key)
-> 2703       graph_function = self._create_graph_function(args, kwargs)
   2704       self._function_cache.primary[cache_key] = graph_function
   2705       return graph_function, args, kwargs

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2591             arg_names=arg_names,
   2592             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2593             capture_by_value=self._capture_by_value),
   2594         self._function_attributes,
   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    976                                           converted_func)
    977 
--> 978       func_outputs = python_func(*func_args, **func_kwargs)
    979 
    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\eager\def_function.py in wrapped_fn(*args, **kwds)
    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    438         # the function a weak reference to itself to avoid a reference cycle.
--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    440     weak_wrapped_fn = weakref.ref(wrapped_fn)
    441 

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in distributed_function(input_iterator)
     83     args = _prepare_feed_values(model, input_iterator, mode, strategy)
     84     outputs = strategy.experimental_run_v2(
---> 85         per_replica_function, args=args)
     86     # Out of PerReplica outputs reduce or pick values to return.
     87     all_outputs = dist_utils.unwrap_output_dict(

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)
    761       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),
    762                                 convert_by_default=False)
--> 763       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    764 
    765   def reduce(self, reduce_op, value, axis):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
   1817       kwargs = {}
   1818     with self._container_strategy().scope():
-> 1819       return self._call_for_each_replica(fn, args, kwargs)
   1820 
   1821   def _call_for_each_replica(self, fn, args, kwargs):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)
   2162         self._container_strategy(),
   2163         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):
-> 2164       return fn(*args, **kwargs)
   2165 
   2166   def _reduce_to(self, reduce_op, value, destinations):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\autograph\impl\api.py in wrapper(*args, **kwargs)
    290   def wrapper(*args, **kwargs):
    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 292       return func(*args, **kwargs)
    293 
    294   if inspect.isfunction(func) or inspect.ismethod(func):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics, standalone)
    431       y,
    432       sample_weights=sample_weights,
--> 433       output_loss_metrics=model._output_loss_metrics)
    434 
    435   if reset_metrics:

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)
    310           sample_weights=sample_weights,
    311           training=True,
--> 312           output_loss_metrics=output_loss_metrics))
    313   if not isinstance(outs, list):
    314     outs = [outs]

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)
    271                         loss_scale_optimizer.LossScaleOptimizer):
    272             grads = model.optimizer.get_unscaled_gradients(grads)
--> 273           model.optimizer.apply_gradients(zip(grads, trainable_weights))
    274       else:
    275         logging.warning('The list of trainable weights is empty. Make sure that'

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\optimizer_v2\optimizer_v2.py in apply_gradients(self, grads_and_vars, name)
    442           functools.partial(self._distributed_apply, apply_state=apply_state),
    443           args=(grads_and_vars,),
--> 444           kwargs={""name"": name})
    445 
    446   def _distributed_apply(self, distribution, grads_and_vars, name, apply_state):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in merge_call(self, merge_fn, args, kwargs)
   1947     if kwargs is None:
   1948       kwargs = {}
-> 1949     return self._merge_call(merge_fn, args, kwargs)
   1950 
   1951   def _merge_call(self, merge_fn, args, kwargs):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in _merge_call(self, merge_fn, args, kwargs)
   1954         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access
   1955     try:
-> 1956       return merge_fn(self._strategy, *args, **kwargs)
   1957     finally:
   1958       _pop_per_thread_mode()

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\optimizer_v2\optimizer_v2.py in _distributed_apply(self, distribution, grads_and_vars, name, apply_state)
    486           update_ops.extend(
    487               distribution.extended.update(
--> 488                   var, apply_grad_to_update_var, args=(grad,), group=False))
    489 
    490       any_symbolic = any(isinstance(i, ops.Operation) or

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in update(self, var, fn, args, kwargs, group)
   1541       kwargs = {}
   1542     with self._container_strategy().scope():
-> 1543       return self._update(var, fn, args, kwargs, group)
   1544 
   1545   def _update(self, var, fn, args, kwargs, group):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in _update(self, var, fn, args, kwargs, group)
   2172     # The implementations of _update() and _update_non_slot() are identical
   2173     # except _update() passes `var` as the first argument to `fn()`.
-> 2174     return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)
   2175 
   2176   def _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in _update_non_slot(self, colocate_with, fn, args, kwargs, should_group)
   2178     # once that value is used for something.
   2179     with UpdateContext(colocate_with):
-> 2180       result = fn(*args, **kwargs)
   2181       if should_group:
   2182         return result

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\optimizer_v2\optimizer_v2.py in apply_grad_to_update_var(var, grad)
    471       if var.constraint is not None:
    472         with ops.control_dependencies([update_op]):
--> 473           return var.assign(var.constraint(var))
    474       else:
    475         return update_op

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\constraints.py in __call__(self, w)
    213     w = K.map_fn(
    214         self._kernel_constraint,
--> 215         K.stack(array_ops.unstack(w, axis=-1), axis=0))
    216     return K.reshape(K.stack(array_ops.unstack(w, axis=0), axis=-1),
    217                      (height, width, channels, kernels))

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\backend.py in map_fn(fn, elems, name, dtype)
   5830       Tensor with dtype `dtype`.
   5831   """"""
-> 5832   return map_fn_lib.map_fn(fn, elems, name=name, dtype=dtype)
   5833 
   5834 

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\map_fn.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)
    266         back_prop=back_prop,
    267         swap_memory=swap_memory,
--> 268         maximum_iterations=n)
    269     results_flat = [r.stack() for r in r_a]
    270 

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
   2673         name=name,
   2674         return_same_structure=return_same_structure,
-> 2675         back_prop=back_prop)
   2676 
   2677   with ops.name_scope(name, ""while"", loop_vars):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\while_v2.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, maximum_iterations, name, return_same_structure, back_prop)
    192         func_graph=util.WhileBodyFuncGraph(
    193             body_name, collections=ops.get_default_graph()._collections),  # pylint: disable=protected-access
--> 194         add_control_dependencies=add_control_dependencies)
    195     # Add external captures of body to the list of loop vars.
    196     # Note that external tensors will be treated as loop invariants, i.e.,

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    976                                           converted_func)
    977 
--> 978       func_outputs = python_func(*func_args, **func_kwargs)
    979 
    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\while_v2.py in wrapped_body(loop_counter, maximum_iterations_arg, *args)
    170       # `orig_loop_vars` and `args`, converts flows in `args` to TensorArrays
    171       # and packs it into the structure of `orig_loop_vars`.
--> 172       outputs = body(*_pack_sequence_as(orig_loop_vars, args))
    173       if not nest.is_sequence_or_composite(outputs):
    174         outputs = [outputs]

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\map_fn.py in compute(i, tas)
    255       """"""
    256       packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])
--> 257       packed_fn_values = fn(packed_values)
    258       nest.assert_same_structure(dtype or elems, packed_fn_values)
    259       flat_fn_values = output_flatten(packed_fn_values)

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\constraints.py in _kernel_constraint(self, kernel)
    227         K.cast(math_ops.floormod(kernel_shape, 2), 'bool'),
    228         lambda: kernel[start - 1:start, start - 1:start],
--> 229         lambda: kernel[start - 1:start, start - 1:start] + K.zeros(  # pylint: disable=g-long-lambda
    230             (2, 2), dtype=kernel.dtype))
    231     index = K.switch(

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\backend.py in switch(condition, then_expression, else_expression)
   4234     else:
   4235       else_expression_fn = else_expression
-> 4236     x = control_flow_ops.cond(condition, then_expression_fn, else_expression_fn)
   4237   else:
   4238     # tf.where needs its condition tensor

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\util\deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--> 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\control_flow_ops.py in cond(pred, true_fn, false_fn, strict, name, fn1, fn2)
   1172   if (util.EnableControlFlowV2(ops.get_default_graph()) and
   1173       not context.executing_eagerly()):
-> 1174     return cond_v2.cond_v2(pred, true_fn, false_fn, name)
   1175 
   1176   # We needed to make true_fn/false_fn keyword arguments for

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\cond_v2.py in cond_v2(pred, true_fn, false_fn, name)
     88             false_name, collections=ops.get_default_graph()._collections),  # pylint: disable=protected-access
     89         add_control_dependencies=add_control_dependencies,
---> 90         op_return_value=pred)
     91 
     92     verify_captures(_COND, [true_graph, false_graph])

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    976                                           converted_func)
    977 
--> 978       func_outputs = python_func(*func_args, **func_kwargs)
    979 
    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\constraints.py in <lambda>()
    228         lambda: kernel[start - 1:start, start - 1:start],
    229         lambda: kernel[start - 1:start, start - 1:start] + K.zeros(  # pylint: disable=g-long-lambda
--> 230             (2, 2), dtype=kernel.dtype))
    231     index = K.switch(
    232         K.cast(math_ops.floormod(kernel_shape, 2), 'bool'),

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\backend.py in zeros(shape, dtype, name)
   1300     v = array_ops.zeros(shape=shape, dtype=tf_dtype, name=name)
   1301     if py_all(v.shape.as_list()):
-> 1302       return variable(v, dtype=dtype, name=name)
   1303     track_variable(v)
   1304     return v

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\keras\backend.py in variable(value, dtype, name, constraint)
    812       dtype=dtypes_module.as_dtype(dtype),
    813       name=name,
--> 814       constraint=constraint)
    815   if isinstance(value, np.ndarray):
    816     v._keras_shape = value.shape

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\variables.py in __call__(cls, *args, **kwargs)
    258       return cls._variable_v1_call(*args, **kwargs)
    259     elif cls is Variable:
--> 260       return cls._variable_v2_call(*args, **kwargs)
    261     else:
    262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)
    252         synchronization=synchronization,
    253         aggregation=aggregation,
--> 254         shape=shape)
    255 
    256   def __call__(cls, *args, **kwargs):

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\variables.py in getter(**kwargs)
     63 
     64   def getter(**kwargs):
---> 65     return captured_getter(captured_previous, **kwargs)
     66 
     67   return getter

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in creator(next_creator, *args, **kwargs)
   2078     def creator(next_creator, *args, **kwargs):
   2079       _require_strategy_scope_strategy(strategy)
-> 2080       return next_creator(*args, **kwargs)
   2081 
   2082     self._var_creator_scope = variable_scope.variable_creator_scope(creator)

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\variables.py in getter(**kwargs)
     63 
     64   def getter(**kwargs):
---> 65     return captured_getter(captured_previous, **kwargs)
     66 
     67   return getter

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in creator(next_creator, *args, **kwargs)
   2078     def creator(next_creator, *args, **kwargs):
   2079       _require_strategy_scope_strategy(strategy)
-> 2080       return next_creator(*args, **kwargs)
   2081 
   2082     self._var_creator_scope = variable_scope.variable_creator_scope(creator)

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\variables.py in getter(**kwargs)
     63 
     64   def getter(**kwargs):
---> 65     return captured_getter(captured_previous, **kwargs)
     66 
     67   return getter

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in creator(next_creator, *args, **kwargs)
   2078     def creator(next_creator, *args, **kwargs):
   2079       _require_strategy_scope_strategy(strategy)
-> 2080       return next_creator(*args, **kwargs)
   2081 
   2082     self._var_creator_scope = variable_scope.variable_creator_scope(creator)

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\variables.py in getter(**kwargs)
     63 
     64   def getter(**kwargs):
---> 65     return captured_getter(captured_previous, **kwargs)
     66 
     67   return getter

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in creator(next_creator, *args, **kwargs)
   2078     def creator(next_creator, *args, **kwargs):
   2079       _require_strategy_scope_strategy(strategy)
-> 2080       return next_creator(*args, **kwargs)
   2081 
   2082     self._var_creator_scope = variable_scope.variable_creator_scope(creator)

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\ops\variables.py in getter(**kwargs)
     63 
     64   def getter(**kwargs):
---> 65     return captured_getter(captured_previous, **kwargs)
     66 
     67   return getter

C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow_core\python\eager\def_function.py in invalid_creator_scope(*unused_args, **unused_kwds)
    500       """"""Disables variable creation.""""""
    501       raise ValueError(
--> 502           ""tf.function-decorated function tried to create ""
    503           ""variables on non-first call."")
    504 

ValueError: tf.function-decorated function tried to create variables on non-first call.
```

**Describe the expected behavior**
The model should train without an exception being thrown.

**Standalone code to reproduce the issue** 
Copied and altered example from https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5
```
from tensorflow.keras.datasets import mnist
#download mnist data and split into train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()

#reshape data to fit model
X_train = X_train.reshape(60000,28,28,1)
X_test = X_test.reshape(10000,28,28,1)

from tensorflow.keras.utils import to_categorical
#one-hot encode target column
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten
#create model
model = Sequential()
#add model layers
model.add(Conv2D(64, kernel_constraint='radial_constraint', kernel_size=3, activation='relu', input_shape=(28,28,1)))
model.add(Conv2D(32, kernel_size=3, activation='relu'))
model.add(Flatten())
model.add(Dense(10, activation='softmax'))

#compile model using accuracy to measure model performance
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#train the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)
```

**Other info / logs**
None"
38199,"tf.keras.losses.categorical_hinge mentions [-1, 1] values while it works with one-hot-encoded tensor","Hi!

Please see:
https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_hinge  
https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/losses.py#L866-L882

Both mention:
> y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided they will be converted to -1 or 1.

While the code is --as expected-- a transcription of keras' one:
```
# ...
y_pred = ops.convert_to_tensor(y_pred)
y_true = math_ops.cast(y_true, y_pred.dtype)
pos = math_ops.reduce_sum(y_true * y_pred, axis=-1)
neg = math_ops.reduce_max((1. - y_true) * y_pred, axis=-1)
return math_ops.maximum(0., neg - pos + 1.)
```

And this code is meant to work with one-hot-encoded tensors. See the original discussion here: https://github.com/keras-team/keras/issues/2830"
38198,TF 2.1: inserting into MutableHashTable results into error,"**System information** 
- Have written custom code
- OS Platform and Distribution: Linux Ubuntu 18.04: 
- TensorFlow installed from binary: `pip install tensorflow-gpu==2.1.0` 
- TensorFlow version: 2.1.0
- Python version: 3.6
- CUDA/cuDNN version: 10/7.0
- GPU model and memory: GTX1070 and 6GB

**Describe the current behavior**
I am trying to insert some key:value pairs into a MutableHashTable

**Describe the expected behavior**
Using the contrib equivalent of MutableHashTable does not produce this error.


**Standalone code to reproduce the issue** 
```import tensorflow as tf
tf.compat.v1.disable_eager_execution()

CHARMAP = ['', '', ''] + list('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ')

with tf.device('/gpu:0'):
    table = tf.raw_ops.MutableHashTable(
            key_dtype=tf.int64,
            value_dtype=tf.string,
    )

insert = table.insert(tf.constant(list(range(len(CHARMAP))), dtype=tf.int64),
                     tf.constant(CHARMAP)
                     )
```

**Other info / logs** 
Here is the output:

```Traceback (most recent call last):
 File ""test.py"", line 12, in <module>
    insert = table.insert(tf.constant(list(range(len(CHARMAP))), dtype=tf.int64),
AttributeError: 'Tensor' object has no attribute 'insert'
```"
38197,"Model not deterministic, even though os.environ['TF_DETERMINISTIC_OPS'] = '1' is set","**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Pretty much the MirroredStrategy fmnist example
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): tensorflow/tensorflow:2.2.0rc2-gpu-py3
- TensorFlow installed from (source or
binary): tensorflow/tensorflow:2.2.0rc2-gpu-py3
- TensorFlow version (use command below): tensorflow/tensorflow:2.2.0rc2-gpu-py3
- Python version: tensorflow/tensorflow:2.2.0rc2-gpu-py3
- CUDA/cuDNN version: tensorflow/tensorflow:2.2.0rc2-gpu-py3
- GPU model and memory: 1050M

**Describe the current behavior**
Model is not deterministic/reproducible.
Two runs:
```
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 2s 0us/step
Epoch 1, Loss: 0.17844311892986298, Accuracy: 0.9466999769210815,Test Loss: 0.057941436767578125, Test Accuracy: 0.9815000295639038
Epoch 2, Loss: 0.05286668613553047, Accuracy: 0.9836500287055969,Test Loss: 0.044471099972724915, Test Accuracy: 0.9853000044822693
Epoch 3, Loss: 0.03694676235318184, Accuracy: 0.9883000254631042,Test Loss: 0.034947194159030914, Test Accuracy: 0.9897000193595886
Epoch 4, Loss: 0.028592929244041443, Accuracy: 0.9910500049591064,Test Loss: 0.027234185487031937, Test Accuracy: 0.9907000064849854
Epoch 5, Loss: 0.022629836574196815, Accuracy: 0.9927666783332825,Test Loss: 0.029115190729498863, Test Accuracy: 0.9904000163078308
Epoch 6, Loss: 0.0172086451202631, Accuracy: 0.9944999814033508,Test Loss: 0.027797872200608253, Test Accuracy: 0.9902999997138977
Epoch 7, Loss: 0.013981950469315052, Accuracy: 0.9956499934196472,Test Loss: 0.02764272689819336, Test Accuracy: 0.9909999966621399
Epoch 8, Loss: 0.01210874691605568, Accuracy: 0.9961333274841309,Test Loss: 0.035009630024433136, Test Accuracy: 0.9896000027656555
Epoch 9, Loss: 0.008961305022239685, Accuracy: 0.9971666932106018,Test Loss: 0.034057389944791794, Test Accuracy: 0.9905999898910522
Epoch 10, Loss: 0.00800476036965847, Accuracy: 0.9972166419029236,Test Loss: 0.033878158777952194, Test Accuracy: 0.9900000095367432
GPU Run Time: 70.80781483650208 seconds
```

```
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 2s 0us/step
Epoch 1, Loss: 0.1761329025030136, Accuracy: 0.9478499889373779,Test Loss: 0.05224931612610817, Test Accuracy: 0.9835000038146973
Epoch 2, Loss: 0.05251472815871239, Accuracy: 0.9836666584014893,Test Loss: 0.04059470072388649, Test Accuracy: 0.9860000014305115
Epoch 3, Loss: 0.03771379590034485, Accuracy: 0.98785001039505,Test Loss: 0.03189479187130928, Test Accuracy: 0.9894000291824341
Epoch 4, Loss: 0.027971116825938225, Accuracy: 0.9912333488464355,Test Loss: 0.03176414594054222, Test Accuracy: 0.9890000224113464
Epoch 5, Loss: 0.022653400897979736, Accuracy: 0.9925000071525574,Test Loss: 0.03643624112010002, Test Accuracy: 0.9876999855041504
Epoch 6, Loss: 0.01727919466793537, Accuracy: 0.9942166805267334,Test Loss: 0.02887595444917679, Test Accuracy: 0.9901000261306763
Epoch 7, Loss: 0.01397143118083477, Accuracy: 0.9957500100135803,Test Loss: 0.03118096850812435, Test Accuracy: 0.9905999898910522
Epoch 8, Loss: 0.01202292088419199, Accuracy: 0.9961333274841309,Test Loss: 0.03164077177643776, Test Accuracy: 0.9909999966621399
Epoch 9, Loss: 0.008715414442121983, Accuracy: 0.9971333146095276,Test Loss: 0.04146642982959747, Test Accuracy: 0.9896000027656555
Epoch 10, Loss: 0.008586470037698746, Accuracy: 0.9969000220298767,Test Loss: 0.033046264201402664, Test Accuracy: 0.9902999997138977
GPU Run Time: 72.08828902244568 seconds
```

**Describe the expected behavior**
I expect the model to be reproducible with the same loss, accuracy etc
**Standalone code to reproduce the issue** 
```
#!/usr/bin/env python 
import tensorflow as tf
import numpy as np
import argparse
import time
import random
import os

from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model


def random_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed) # Python general
    np.random.seed(seed)
    random.seed(seed) # Python random
    tf.random.set_seed(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'

# Not yet using click due to Docker issues
parser = argparse.ArgumentParser(description='Tensorflow entry point')
parser.add_argument('--epochs', type=int, default=10)
parser.add_argument('--seed', type=int, default=0)
args = parser.parse_args()

# Detect GPUs
print(f'Num GPUs Available: {len(tf.config.experimental.list_physical_devices(""GPU""))}')

# Load MNIST
mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Adding a dimension to the array -> new shape == (28, 28, 1), since the first layer in our model is a convolutional
# layer and it requires a 4D input (batch_size, height, width, channels).
# batch_size dimension will be added later on.
train_images = train_images[..., None]
test_images = test_images[..., None]

# Normalizing the images to [0, 1] range.
train_images = train_images / np.float32(255)
test_images = test_images / np.float32(255)

# Use MirroredStrategy for multi GPU support
# If the list of devices is not specified in the`tf.distribute.MirroredStrategy` constructor, it will be auto-detected.
strategy = tf.distribute.MirroredStrategy()

BUFFER_SIZE = len(train_images)
BATCH_SIZE_PER_REPLICA = 64
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

# Batch and distribute data
train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) 
test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) 
train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)
test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)

# Fix seeds
random_seed(0)

# Define model
def create_model():
    model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, 3, activation='relu'),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Conv2D(64, 3, activation='relu'),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10)
    ])

    return model

# Define Loss and accuracyc metrics
with strategy.scope():
    # Set reduction to `none` so reduction can be done afterwards and divide by global batch size.
    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=True,
        reduction=tf.keras.losses.Reduction.NONE)
    def compute_loss(labels, predictions):
        per_example_loss = loss_object(labels, predictions)

        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)

    test_loss = tf.keras.metrics.Mean(name='test_loss')

    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')


# Define model, optimizer, training- and test step
with strategy.scope():
  model = create_model()
  optimizer = tf.keras.optimizers.Adam()

  def train_step(inputs):
    images, labels = inputs

    with tf.GradientTape() as tape:
        predictions = model(images, training=True)
        loss = compute_loss(labels, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    train_accuracy.update_state(labels, predictions)

    return loss 

  def test_step(inputs):
    images, labels = inputs

    predictions = model(images, training=False)
    t_loss = loss_object(labels, predictions)
    test_loss.update_state(t_loss)
    test_accuracy.update_state(labels, predictions)


with strategy.scope():
  # `run` replicates the provided computation and runs it with the distributed input.
  @tf.function
  def distributed_train_step(dataset_inputs):
    per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)
 
  @tf.function
  def distributed_test_step(dataset_inputs):
    return strategy.run(test_step, args=(dataset_inputs,))

  gpu_runtime = time.time()
  for epoch in range(args.epochs):
    # TRAIN LOOP
    total_loss = 0.0
    num_batches = 0
    for dist_dataset in train_dist_dataset:
      total_loss += distributed_train_step(dist_dataset)
      num_batches += 1
    train_loss = total_loss / num_batches

    # TEST LOOP
    for dist_dataset in test_dist_dataset:
      distributed_test_step(dist_dataset)

    print(f'Epoch {epoch + 1}, Loss: {train_loss}, Accuracy: {train_accuracy.result()},'
          f'Test Loss: {test_loss.result()}, Test Accuracy: {test_accuracy.result()}')

    # Reset states
    test_loss.reset_states()
    train_accuracy.reset_states()
    test_accuracy.reset_states()

  print(f'GPU Run Time: {str(time.time() - gpu_runtime)} seconds')
```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
def random_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed) # Python general
    np.random.seed(seed)
    random.seed(seed) # Python random
    tf.random.set_seed(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
```
I guess this should cover everything?

The code is currently running on a SINGLE GPU, even though I'm planning to run it on several GPUs."
38196,TESLA T4 Graphics Card,"I am planning to move to Nvidia Tesla T4 16gb from RTX 2080 8gb.
I am working on yolo model with tensorflow.
I am running around 8 parallel video detection simultaneously.
Currently,my present graphics card RTX 2080 8 gb supporting 8 parallel instances successfully.
So what through Tesla 12gb is supposed to give? How many parallel instances can be run on Tesla 12gb VRAM?


Please help me with this."
38195,GPU accelerated LSTM model crashes,"**System information** 
- Have I written custom code: Yes
- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): Windows 10 pro 64 bit 
- Mobile device if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip 
- TensorFlow version (use command below): 2.1.0 
- Python version: - 3.7.3
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 10.2
- GPU model and memory: GeForce GTX 1050ti 4GB OC, driver version 441.22

**Describe the current behavior**

Hi, I have this issue since yesterday. Before that, everything was working fine. I have not updated either the tensorflow version or the gpu driver or anything else for that matter in the last couple of days. This issue suddenly appeared just yesterday on its own. Below is my model,

```
def neural_network(vocab_size, embedding_dim, max_length, train_padded, train_labels, validation_frac, num_epochs):
    model = Sequential()
    model.add(Embedding(vocab_size, embedding_dim, input_length = max_length))
    model.add(Bidirectional(LSTM(64, return_sequences = True)))
    model.add(GlobalAveragePooling1D())
    model.add(Dropout(0.2))
    model.add(Dense(50, activation = 'relu'))
    model.add(Dropout(0.1))
    model.add(Dense(1, activation = 'sigmoid'))
    model.summary()
    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)
    return model, history
```

I am pretty sure I am not running out of memory as previously I have trained the very same model with 10 times the parameters (~25M) compared to the parameters it has right now (~2M). Even the GPU usage barely exceeds 5%. I have GTX 1050ti 4GB. I have successfully run the above model, with varying number of total parameters, plenty of times before but only since yesterday this issue is coming up. Now the model runs fine only if I omit just the LSTM layer.

```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 200, 128)          1920000   
_________________________________________________________________
bidirectional (Bidirectional (None, 200, 128)          98816     
_________________________________________________________________
global_average_pooling1d (Gl (None, 128)               0         
_________________________________________________________________
dropout (Dropout)            (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 50)                6450      
_________________________________________________________________
dropout_1 (Dropout)          (None, 50)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 51        
=================================================================
Total params: 2,025,317
Trainable params: 2,025,317
Non-trainable params: 0
```

Below is the exact error. 

```
Train on 143613 samples, validate on 15958 samples
Epoch 1/5
Traceback (most recent call last):

  File ""C:\Users\admin\Documents\Machine Learning\Projects\Classification\jigsaw-toxic-comment-classification-challenge\toxic_classifier.py"", line 122, in <module>
    model, history = neural_network(vocab_size, embedding_dim, max_length, train_padded, toxicity[col], validation_frac, num_epochs)

  File ""C:\Users\admin\Documents\Machine Learning\Projects\Classification\jigsaw-toxic-comment-classification-challenge\toxic_classifier.py"", line 87, in neural_network
    history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 819, in fit
    use_multiprocessing=use_multiprocessing)

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 342, in fit
    total_epochs=epochs)

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 98, in execution_function
    distributed_function(input_fn))

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 599, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2363, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1611, in _filtered_call
    self.captured_inputs)

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1692, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py"", line 545, in call
    ctx=ctx)

  File ""C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)

  File ""<string>"", line 3, in raise_from

InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] 
	 [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
	 [[StatefulPartitionedCall_1]]
	 [[Reshape_14/_46]] [Op:__inference_distributed_function_5894]

Function call stack:
distributed_function -> distributed_function -> distributed_function
```

Also, once this issue occurs, the kernel keeps crashing on its own _even if I am not compiling anything_. I am using Spyder IDE and even restarting the kernel does not help; it simply crashes after a few seconds. Below is the log for that (it is written on a red background)

```
An error ocurred while starting the kernel
2020󈚨󈚧 09:10:48.429373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020󈚨󈚧 10:25:24.630422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020󈚨󈚧 10:25:24.654232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020󈚨󈚧 10:25:24.655330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020󈚨󈚧 10:25:24.660386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020󈚨󈚧 10:25:24.665212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020󈚨󈚧 10:25:24.667487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020󈚨󈚧 10:25:24.672356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020󈚨󈚧 10:25:24.675255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020󈚨󈚧 10:25:24.685248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020󈚨󈚧 10:25:24.686442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020󈚨󈚧 10:25:24.687128: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020󈚨󈚧 10:25:24.689769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020󈚨󈚧 10:25:24.690853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020󈚨󈚧 10:25:24.691406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020󈚨󈚧 10:25:24.691954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020󈚨󈚧 10:25:24.692497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020󈚨󈚧 10:25:24.693046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020󈚨󈚧 10:25:24.693600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020󈚨󈚧 10:25:24.694155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020󈚨󈚧 10:25:24.695292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020󈚨󈚧 10:25:25.261369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020󈚨󈚧 10:25:25.261990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 
2020󈚨󈚧 10:25:25.262349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N 
2020󈚨󈚧 10:25:25.263472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2990 MB memory) ‑> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)
2020󈚨󈚧 10:25:27.808120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020󈚨󈚧 10:25:28.068884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020󈚨󈚧 10:26:06.218534: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data‑>opaque(), input_h_desc.handle(), input_h_backprop_data‑>opaque(), input_c_desc.handle(), input_c_backprop_data‑>opaque(), workspace.opaque(), workspace.size(), reserve_space_data‑>opaque(), reserve_space_data‑>size())'
2020󈚨󈚧 10:26:06.221888: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] 
2020󈚨󈚧 10:26:06.223371: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] 
[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
2020󈚨󈚧 10:26:06.225063: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] 
[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
[[StatefulPartitionedCall_1]]
[[Reshape_14/_46]]
2020󈚨󈚧 10:26:06.228126: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] 
[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
[[StatefulPartitionedCall_1]]
2020󈚨󈚧 10:35:24.183417: F .\tensorflow/core/kernels/random_op_gpu.h:232] Non‑OK‑status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: unspecified launch failure
```

To get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22. 


**Describe the expected behavior**
should compile to completion

**Standalone code to reproduce the issue** 
[Code](https://github.com/diggee/toxic-comment-classification/blob/master/toxic_classifier.py)

**Other info / logs** 
Already included the logs above. 
"
38194,Will TensorFlow 2.2.0 support CUDA 10.2?,"Hi :) I am going to use neural networking and TensorFlow.
I'm trying to install different versions of tensorflow and tensorflow-gpu using pip (for example, 2.1.0 both tensorflow and tensorflow-gpu, 2.2.0-rc0 both tensorflow and tensorflow-gpu) and in Python (3.7) I get error about loading `cudart64_101.dll`, like this:
`>>> import tensorflow as tf`
`2020-03-31 03:30:42.120394: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-03-31 03:30:42.134395: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.`
I copied cuDNN files, also I set `CUDA_HOME` env. to value of `CUDA_PATH` env. My hardware meets the requirements.
As far as I understand, TensorFlow 2.1.0 should work fine with CUDA 10.1. But I don't want to use CUDA 10.1 unless emergency, I just install 10.2 and don't want to reinstall it to reinstall back to 10.2 again in future.
I ready to wait for 2.2.0 release, if that makes sense in my case. So my question is: Will TensorFlow 2.2.0 support CUDA 10.2?
"
38192,tf.data.Dataset should support attr classes instances as values,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently Datasets only support nested structures of tuples, named tuples, and dictionaries, however attr classes are also supported transparently by tf.function. Ideally, to be consistent, these should also be supported by datasets as they are the primary entry point for data in autographed functions.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Anyone using structured data.

**Any Other info.**
- Code for attr support in tf.function:
https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/utils/computation_utils.py#L47

- Code for supported types in Dataset:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/util/structure.py#L407

- A usage example that should be legit but is not:

```python
import attr
from collections import namedtuple
import tensorflow as tf


@attr.s(frozen=True)
class DataContainer:
    data = attr.ib()
    
NamedDataContainer = namedtuple('NamedDataContainer', ['data'])
        
@tf.function
def f(x):
    return x

x = tf.constant(0.)
f(DataContainer(x)) # legit
f(NamedDataContainer(x)) # legit

dataset = tf.data.Dataset.from_tensor_slices([0., 1., 2., 3.])

dataset.map(NamedDataContainer) # legit
dataset.map(DataContainer) # not legit
```"
38191,Error while passing initial_state to Bidirectional LSTM,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): MacOS Mojave (10.14.6)
- TensorFlow installed from (source or
binary): binary (Anaconda) 
- Python version: 3.6.10
- Tensorflow version: 2.1.0

**Describe the current behavior**
Error while passing `initial_state` to `tf.keras.layers.Bidirectional` with `tf.keras.layers.LSTM` as cell.

**Describe the expected behavior**
Should be able to pass the initial state

**Standalone code to reproduce the issue** 
```
import tensorflow as tf
import tensorflow.keras.layers as layers


encoder_units = 100
batch_size = 5
embedding_dim = 300

lstm =  layers.Bidirectional(layers.LSTM(encoder_units,
                                                  return_sequences=True,
                                                    return_state=True,
                                                     time_major=False))
initial_state = [tf.zeros((batch_size, encoder_units)), tf.zeros((batch_size, encoder_units))]
embedding_inp =  tf.zeros((batch_size, encoder_units, embedding_dim))
encoder_op, state_h, state_c = lstm(embedding_inp, initial_state= initial_state)
```



**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

/anaconda3/envs/tf2/bin/python /Users/jshah02/Library/Preferences/PyCharmCE2019.2/scratches/scratch_8.py
2020-04-03 16:02:31.592278: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-03 16:02:31.606727: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe10d5c24e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-03 16:02:31.606745: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File ""/Users/jshah02/Library/Preferences/PyCharmCE2019.2/scratches/scratch_8.py"", line 15, in <module>
    encoder_op, state_h, state_c = lstm(embedding_inp, initial_state= initial_state)
  File ""/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py"", line 605, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 818, in __call__
    self._maybe_build(inputs)
  File ""/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2116, in _maybe_build
    self.build(input_shapes)
  File ""/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py"", line 697, in build
    self.forward_layer.build(input_shape)
  File ""/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py"", line 574, in build
    self._validate_state_spec(state_size, self.state_spec)
  File ""/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py"", line 605, in _validate_state_spec
    raise validation_error
ValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=ListWrapper([InputSpec(shape=(5, 100), ndim=2)]); however `cell.state_size` is [100, 100]

Process finished with exit code 1
"
38190,RPC failed when using TPU,"I have no idea how this error is triggered, so far it happens more than 10 times to me in the past 2 weeks, sometimes it works by just restart the node or wait for another day. But it seems not the case this time. 

Below is the full message (it can repeat forever and the process won't terminate)
```I0403 09:00:11.077040 140542806840704 multi_init.py:172] Computing metrics...
2020-04-03 09:00:45.480838: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1585904445.480666714"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-04-03 09:00:45.480839: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1585904445.480690662"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
WARNING:tensorflow:An error occurred when attempting to close the session. This may be due to a preemption in a connected worker or parameter server. Error: Session ef9fc537f782c0ae is not found. Possibly, this master has restarted.
W0403 09:01:06.161614 140542806840704 monitored_session.py:1171] An error occurred when attempting to close the session. This may be due to a preemption in a connected worker or parameter server. Error: Session ef9fc537f782c0ae is not found. Possibly, this master has restarted.
```
And besides these lines, there is no other error or exception message. I've also checked all the path on gs bucket and they look normal."
38189,request to add DepthToSpace Spilt and conv2d_transpose operation in post training quantization(full integer quantization)),"****System information****
-  Linux Ubuntu 16.04:
- TensorFlow installed from binary:
- TensorFlow version 1.15.0:


**Provide the text output from tflite_convert**

```
RuntimeError: Quantization not yet supported for op: SPLIT

```
"
38188,tflite ssbo buffer dump,"I`m using tflite gpu ssbo way to inference but the output is not the same as without ssbo
I have referenced this example
https://github.com/gnsmrky/tensorflow-lite-ssbo.git
So I want to inpect the input with ssbo is or not same as the input without ssbo but have no idea how to get ssbo content to evaluate ?

The way to get input without ssbo
![image](https://user-images.githubusercontent.com/17869361/78337233-3f579b80-75c3-11ea-910a-29e67cd7dcdf.png)

Bellow is ssbo way by using compute shader in opengl es 3.1
I have tried to use glMapBufferRange to map the buf to cpu for evaluting but all result is zero.
I am new to opengles 3.1 and googled but not found a solution  
![image](https://user-images.githubusercontent.com/17869361/78337436-89d91800-75c3-11ea-91cc-ae4a8a35fcf2.png)


"
38187,No module named 'tensorflow.keras',"i use : 
win10 os
anaconda 3(python 3.6)
tensorflow 1.5.0
keras 2.1.6
i run 
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, SeparableConv2D
i have error :
    from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, SeparableConv2D
ModuleNotFoundError: No module named 'tensorflow.keras'"
38185,Add GPU-deterministic back-prop for fused softmax/cross-entropy ops,"**SYSTEM INFORMATION**

- TensorFlow version (you are using): `2.2.0-rc2`
- Are you willing to contribute it: Yes (please assign it to me)

**CURRENT BEHAVIOR**

The back-prop of `tf.nn.softmax_cross_entropy_with_logits` and `tf.nn.sparse_softmax_cross_entropy_with_logits` is non-deterministic on GPUs.

**WILL THIS CHANGE THE CURRENT API?**

No. Assuming the deterministic back-prop kernels are slower than the current non-deterministic ones, then the deterministic operation will be selectable using the preferred mechanism at the time. At the time of writing, that mechanism is to set the environment variable `TF_DETERMINISTIC_OPS` to ""1"" or ""true"".

**WHO WILL BENEFIT FROM THIS FEATURE?**

Determinism, for both training and inference, is becoming increasingly important as deep learning systems are moved into production, not only because of regulatory requirements of some markets but also, more broadly, because of the massive potential performance advantages of training with determinism; determinism accelerates and facilitates debug, experimentation (including hyper-parameter tuning and active learning), and regression testing.

With `TF_DETERMINISTIC_OPS` now enabling deterministic functionality of cuDNN convolution, bias addition, max-pooling, and CTC loss, many deep learning models will train deterministically on GPUs. Softmax and cross entropy are both foundational functions in deep learning, and are combined [to ensure performance and numerical stability](https://github.com/tensorflow/tensorflow/issues/2462
). Enabling these fused ops to function deterministically will enhance the ability for TensorFlow to be used for various production systems.

This current issue comes out of [Issue 9](https://github.com/NVIDIA/tensorflow-determinism/issues/9) in the [tensorflow-determinism](https://github.com/NVIDIA/tensorflow-determinism) project, which is focused on making the `SequenceToSequece` model of [OpenNMT-tf](https://github.com/OpenNMT/OpenNMT-tf) train deterministically.

**UNIT TESTS**

What follows are essentially production-ready TensorFlow unit tests that currently fail, but that will pass when this feature is implemented correctly. Depending on the implementation, the parameter-space covered by the tests (e.g. ""batch"" dimensionality) may need to be broadened for coverage. The tests can be seen running (and failing) on TensorFlow version `2.2.0-rc2` in [this colab](https://colab.research.google.com/drive/1syj32Jl7dS6mBa-GhNrq_LLIxvfumIOz).

```
import tensorflow as tf
import numpy as np

class DeterministicTest(tf.test.TestCase):

  def _randomInts(self, shape, high, dtype):
    return tf.constant(
        np.random.randint(low=0, high=high, size=shape).astype(dtype))

  def _randomFloats(self, shape, dtype, normalized_rows=False):
    a = (2 * np.random.random_sample(shape) - 1).astype(dtype)

    if normalized_rows:

      def normalize(row):
        return row / row.sum()

      a = np.apply_along_axis(normalize, 1, a)

    return tf.constant(a)
    
  def _testDeterministicGradients(self, exclusive_labels):
    with self.session(force_gpu=True):
      batch_size = 1024
      classes_count = 1000
      logits_shape = (batch_size, classes_count)
      logits_dtype = np.float32
      logits = self._randomFloats(logits_shape, logits_dtype)
      if exclusive_labels:
        labels_shape = (batch_size)
        labels_dtype = np.int32
        labels = self._randomInts(labels_shape, classes_count, labels_dtype)
      else:
        labels_shape = logits_shape
        labels_dtype = logits_dtype
        labels = self._randomFloats(labels_shape, labels_dtype,
                                    normalized_rows=True)
      output_shape = (batch_size)
      output_dtype = logits_dtype

      def gradients(local_seed):
        np.random.seed(local_seed)
        upstream_gradients = self._randomFloats(output_shape, output_dtype)
        with tf.GradientTape(persistent=True) as tape:
          tape.watch(logits)
          if exclusive_labels:
            tested_op = tf.nn.sparse_softmax_cross_entropy_with_logits
          else:
            tested_op = tf.nn.softmax_cross_entropy_with_logits
          op_output = tested_op(labels=labels, logits=logits)
          gradient_injector_output = op_output * upstream_gradients
        return tape.gradient(gradient_injector_output, logits)

      repeat_count = 5
      for seed in range(repeat_count):
        result_a = gradients(seed)
        result_b = gradients(seed)
        self.assertAllEqual(result_a, result_b)

  def testExclusiveLabelsDeterministicGradients(self):
    self._testDeterministicGradients(exclusive_labels=True)

  def testDistributionLabelsDeterministicGradients(self):
    self._testDeterministicGradients(exclusive_labels=False)

if __name__ == '__main__':
  tf.test.main()
```

**POSSIBLE SOLUTION APPROACHES**

The ultimate solution will be to fix this at the CUDA level by creating deterministic kernels that are exactly functionally equivalent to the existing non-deterministic kernels. I'll soon be upstreaming a solution like this for `tf.image.resize` with `method=ResizeMethod.BILINEAR`; see [PR 39243](https://github.com/tensorflow/tensorflow/pull/39243).

It could also be possible to create a temporary solution at the Python level by combining existing deterministic ops. See deterministic [tf.nn.bias_add](https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/nn_ops.py#L2741-L2755) as an example. Assuming that `tf.nn.softmax` operates deterministically (I have not tested that), then this could be followed by an arithmetically robust implementation of cross-entropy (e.g. [streaming log-sum-exp](http://www.nowozin.net/sebastian/blog/streaming-log-sum-exp-computation.html)). There would, of course, be a performance cost (1/2 speed) compared to a fused softmax and cross-entopy.

**ASSIGNMENT**

I'm willing and able to implement this feature, but I might not be able to get to it for quite a while. I'm happy for someone else to implement it, but please let me know if you start actively working on it (and I will do the same), so that we can avoid duplicating effort."
38184,Error on confliction between custom op argument name and python builtin name,"**System information** 
- Have I written custom code: yes 
- OS Platform and Distribution: Linux 
- TensorFlow installed from: binary
- TensorFlow version: 1.13.1 
- Python version: 3.6

**Describe the current behavior**
If we define custom op as
```
REGISTER_OP(""MyCustomOp"")
    .Input(""list: list({int32, float32})"")
    .Attr(""my_list_attr: list(int)"")
    // other settings ...
```
And invoke in python as
```
lib = load_library.load_op_library('my_custom_op.so')
output = lib.my_custom_op(arguments)
```
The error raises `TypeError: isinstance() arg 2 must be a type or tuple of types`

**Other info / logs** 
Refer to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/python_op_gen.cc#L484
I find python side wrapper will be created as
`def my_custom_op(list, ...)` if we define input name as ""list""

And for attribute ""my_list_attr"", check code is generated as
`if not isinstance(my_list_attr, (list, tuple)):`

But ""list"" do not refer to builtin type anymore :(, which results to python type errors. Can some name checking get conducted to avoid such kind of confusing errors?
"
38183,[PROPOSAL] Go back to cmake and drop support to bazel -- a real memory and time abusing toolkit for building medium size projects,"## Motivation

If we build tensorflow using camke, I guess only 1~2 hours is enough for the first trial considering the current project size. However when we switch to bazel huge memory and time will be consumed.

Bazel by defaults will creates huge number of jobs (you can control by --jobs) and each job consumes nearly 10 GB memory. Besize, a lot of non-binary dependencies downloaded using ""git"", this breaks  philosophy of UNix design.  

Actually using ""git"" while building a large project is a not very good practice. Cmake by defaults does not support SSL and you have build it manually to support curl with ssl . Moreover,  instead of to clone a whole project, we only need to download released versions of that. Using `git clone` inside a complex installer will creates more problems that you can predict: proxy server, memory cache, fire wall, ...

Considering the fact that all-in-one installer should be friendly to a new machine, the bazel has been proved not good at that.

I suggest to use traditional ways to maintain binary dependencies by tracing different binary managers for different OS distribution.

## C++ dependencies

A product manager or project manager can easily figure out that many projects integrated a compiled tensorflow shared library with cmake projects like HDMap localization module. There are little people who really need bazel because it breaks philosophy of Unix design and solve a problem which does not really exist.

## Compiling details

1) config
By default we change jvm memory flags using ""--host_jvm_args"" upon bazel server boot up and refine jobs to a specific number with flags ""--jobs"" following `bazel build`

see discussion https://stackoverflow.com/questions/34382360/decrease-bazel-memory-usage

2) user specific installation may creates problems when `sudo` needed (checking cuda installation for example).
Since default installation of  bazel is user specific , a script executed by root user will no longer be able to detect ${HOME}/bin/bazel automatically. But meanwhile, searching and modifying thirdparty libraries (cuda, cudnn for examples) requires root privileges. The creates a dilemma.

3) The peak memory building tensorflow used by bazel as I observed  with htop:

Max: 13 G
Avg: 5 ~ 7 G

while bazel boot use more than 10GB and using Malloc utilities, we observed that some of memory became free but was not retrieved by the system. 

"
38182,bulid tensorflow failed ,"I follow the instrctons in the ""https://tensorflow.google.cn/federated/install"".
(1)Install TensorFlow Federated using pip: When I run the command ""python -c ""import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())"""" , there is a SyntaxError: invalid syntax os
(2)Using Docker: When I run the command ""docker build .  --tag tensorflow_federated"", it always fails.
These two errors are shown in the shapshot.
<img width=""771"" alt=""无标题11"" src=""https://user-images.githubusercontent.com/35478590/78322895-a0bb4280-75a2-11ea-9886-20e256eae28c.png"">
"
38181,UnimplementedError: Cast string to float is not supported 	 [[{{node Cast_4}}]],"I don't know why I am getting error for steps_per_epoch. I tried to give values in integer directly but still not working.

**Traceback:**

`---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
<ipython-input-57-941407f460bb> in <module>
      5     epochs=EPOCHS,
      6     callbacks=[lr_callback],
----> 7     steps_per_epoch=float(186.0),
      8     #validation_data=valid_dataset
      9 )

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    725         max_queue_size=max_queue_size,
    726         workers=workers,
--> 727         use_multiprocessing=use_multiprocessing)
    728 
    729   def evaluate(self,

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    683         validation_steps=validation_steps,
    684         validation_freq=validation_freq,
--> 685         steps_name='steps_per_epoch')
    686 
    687   def evaluate(self,

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    298           else:
    299             actual_inputs = ins()
--> 300           batch_outs = f(actual_inputs)
    301         except errors.OutOfRangeError:
    302           if is_dataset:

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in __call__(self, inputs)
   3471         feed_symbols != self._feed_symbols or self.fetches != self._fetches or
   3472         session != self._session):
-> 3473       self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)
   3474 
   3475     fetched = self._callable_fn(*array_vals,

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session)
   3408       callable_opts.run_options.CopyFrom(self.run_options)
   3409     # Create callable.
-> 3410     callable_fn = session._make_callable_from_options(callable_opts)
   3411     # Cache parameters corresponding to the generated callable, so that
   3412     # we can detect future mismatches and refresh the callable.

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in _make_callable_from_options(self, callable_options)
   1503     """"""
   1504     self._extend_graph()
-> 1505     return BaseSession._Callable(self, callable_options)
   1506 
   1507 

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in __init__(self, session, callable_options)
   1458       try:
   1459         self._handle = tf_session.TF_SessionMakeCallable(
-> 1460             session._session, options_ptr)
   1461       finally:
   1462         tf_session.TF_DeleteBuffer(options_ptr)

UnimplementedError: Cast string to float is `"
38180,example person_detection_test  can not make form source code of tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
third party's down shell file should update?
![image](https://user-images.githubusercontent.com/24219045/78319275-6600dc80-7599-11ea-8150-b05a7c33eb6e.png)

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
38174,TF looking for CUDA 10.1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (Latest Update)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 2.2.0rc2
- Python version:  Python 3.8.2
- Installed using virtualenv? pip? conda?: venv + pip + conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA: 10.2 cuDNN: 7.6.5.32 
- GPU model and memory:  GeForce GTX 1060 6GB



**Describe the problem**



While following this tutorial: https://www.wandb.com/tutorial/build-a-neural-network and trying to run this file: https://github.com/lukas/ml-class/blob/master/videos/intro/perceptron-single.py

After following all instructions at https://www.tensorflow.org/install/gpu#software_requirements, I get the the following error: 
```
 python perceptron-single.py
Using TensorFlow backend.
2020-04-02 14:09:01.203876: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-04-02 14:09:01.241792: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

2020-04-02 14:09:08.194348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-04-02 14:09:08.236234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1
coreClock: 1.7085GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2020-04-02 14:09:08.259510: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-04-02 14:09:08.310824: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-04-02 14:09:08.345234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-04-02 14:09:08.373030: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-04-02 14:09:08.424593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-04-02 14:09:08.462421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-04-02 14:09:08.483778: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-04-02 14:09:08.493898: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```

I have CUDA 10.2 installed but TF appears to be looking for 10.1. It proceeds to fall back to the CPU. 
How can I configure TF so that this works properly?


"
38173,tf.nn.bias_add does not work right after tf.map_fn,"tf-nightly-gpu 2.2.0.dev20200402
Ubuntu 18.04.4 LTS

Bug:
File "".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 672, in bias_add
    tld.op_callbacks, value, bias, ""data_format"", data_format)
tensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.

During handling of the above exception, another exception occurred:
...
 x = tf.nn.bias_add(x, self.bias)  # Where self.bias is a tf.Variable
  File "".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 3018, in bias_add
    value, bias, data_format=data_format, name=name)
  File "".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 677, in bias_add
    value, bias, data_format=data_format, name=name, ctx=_ctx)
  File "".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 710, in bias_add_eager_fallback
    ctx=ctx, name=name)
  File "".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 75, in quick_execute
    raise e
  File "".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: .../StatefulPartitionedCall_1:0


Hotfix:
x = tf.map_fn(...)
Lambda(lambda x: x)(x) // or any other keras layer
tf.nn.bias_add(x)

"
38172,fftshift is failing for negative axes,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.8

**Describe the current behavior**
When using the `fftshift` op, I would like to specify the shift axes using negative indexes. Right now, the op fails if I specify negative axes.

**Describe the expected behavior**
I would like the op not to fail.

**Standalone code to reproduce the issue** 
```python
import tensorflow as tf 
tf.signal.fftshift(tf.ones([1, 32, 32]), axes=[-2, -1])
```

**Other info / logs** 
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-3-11929d1809ec> in <module>
----> 1 tf.signal.fftshift(tf.ones([1, 32, 32]), axes=[-2, -1])

~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/signal/fft_ops.py in fftshift(x, axes, name)
    389       shift = _array_ops.shape(x)[axes] // 2
    390     else:
--> 391       shift = _array_ops.gather(_array_ops.shape(x), axes) // 2
    392 
    393     return manip_ops.roll(x, shift, axes, name)

~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py in gather(***failed resolving arguments***)
   4106     return params.sparse_read(indices, name=name)
   4107   except AttributeError:
-> 4108     return gen_array_ops.gather_v2(params, indices, axis, name=name)
   4109 
   4110 

~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py in gather_v2(params, indices, axis, batch_dims, name)
   3677       try:
   3678         return gather_v2_eager_fallback(
-> 3679             params, indices, axis, batch_dims=batch_dims, name=name, ctx=_ctx)
   3680       except _core._SymbolicException:
   3681         pass  # Add nodes to the TensorFlow graph.

~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py in gather_v2_eager_fallback(params, indices, axis, batch_dims, name, ctx)
   3715   _attr_Tindices, ""Taxis"", _attr_Taxis)
   3716   _result = _execute.execute(b""GatherV2"", 1, inputs=_inputs_flat,
-> 3717                              attrs=_attrs, ctx=ctx, name=name)
   3718   if _execute.must_record_gradient():
   3719     _execute.record_gradient(

~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: indices[0] = -2 is not in [0, 3) [Op:GatherV2]
```
"
38171,[REGRESSION] Bazel build failure ,"commit 2082d706a5ed3bbe07706cfb694b431b2fe50f89 included 2 days ago in #34218 break bazel tests:

```
$bazel test --config=noaws --config=nogcp --config=nohdfs --verbose_failures -c dbg --copt -DNDEBUG  //tensorflow/compiler/xla/service/gpu:all

...
Execution platform: @local_config_platform//:host
gcc: error: missing argument to '-B'
```

I cannot just comment the offending line: `cuda_defines[""%{linker_bin_path}""] = """"` as elsewhere it expect it to be present. But if I set the old value ""host_compiler_prefix"", it fix my problems.

Can we just revert this commit until a good version is available?

I tried with bazel 2.0.0 and 2.1.0 and 2.2.0. So bumping the bazel version doesn't fix this problem."
38170,"With .experimental_run_functions_eagerly(True), tf.functions run by Dataset doesn't get eager tensors","**System information** 
- Have I written custom code: Yes 
- OS Platform and Distribution: Ubuntu 18.04 
- Mobile device: N/A 
- TensorFlow installed from: binary 
 - TensorFlow version (use command below): 2.1.0 / 2.2.0rc2
- Python version: 3.7.6

The manual suggests switching eager computation on for  `tf.function`s if debugging is desired, via `tf.config.experimental_run_functions_eagerly(True)`. However, this is not possible in situations like shown below.

To me it seems, that the in case of running eagerly, the function passed to `.map` would need to be executed with `tensorflow.python.framework.ops.EagerTensor` tensor arguments, not a regular `tensorflow.python.framework.ops.Tensor`.

**Describe the current behavior**
The example below outputs the following, failing with an exception:
```
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>]
Traceback (most recent call last):
  File ""test.py"", line 14, in <module>
    perform_test()
  File ""test.py"", line 8, in perform_test
    print(list(tf.data.Dataset.from_tensor_slices([1.0,-1.0]).map(non_negative)))
  File ""…tensorflow/python/data/ops/dataset_ops.py"", line 1621, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File ""…tensorflow/python/data/ops/dataset_ops.py"", line 3974, in __init__
    use_legacy_function=use_legacy_function)
  File ""…tensorflow/python/data/ops/dataset_ops.py"", line 3221, in __init__
    self._function = wrapper_fn.get_concrete_function()
  File ""…tensorflow/python/eager/function.py"", line 2532, in get_concrete_function
    *args, **kwargs)
  File ""…tensorflow/python/eager/function.py"", line 2496, in _get_concrete_function_garbage_collected
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""…tensorflow/python/eager/function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""…tensorflow/python/eager/function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""…tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""…tensorflow/python/data/ops/dataset_ops.py"", line 3214, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""…tensorflow/python/data/ops/dataset_ops.py"", line 3156, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
  File ""…tensorflow/python/eager/def_function.py"", line 564, in __call__
    return self._python_function(*args, **kwds)
  File ""test.py"", line 5, in non_negative
    return 1.0 if value > 0.0 else 0.0
  File ""…tensorflow/python/framework/ops.py"", line 778, in __bool__
    self._disallow_bool_casting()
  File ""…tensorflow/python/framework/ops.py"", line 542, in _disallow_bool_casting
    ""using a `tf.Tensor` as a Python `bool`"")
  File ""…tensorflow/python/framework/ops.py"", line 527, in _disallow_when_autograph_disabled
    "" Try decorating it directly with @tf.function."".format(task))
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.
``` 
**Describe the expected behavior**
The example below working as desired, outputting: 
``` 
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>]
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>]
```

**Standalone code to reproduce the issue** 
```python
import tensorflow as tf

@tf.function
def non_negative(value):
    return 1.0 if value > 0.0 else 0.0

def perform_test():
    print(list(tf.data.Dataset.from_tensor_slices([1.0,-1.0]).map(non_negative)))

perform_test()

tf.config.experimental_run_functions_eagerly(True)

perform_test()
``` "
38169,Tensor load wrong CUDNN,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.15
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 7.6.0
- GPU model and memory: 940mx 2GB


So i want to run some project from [this repo](https://github.com/wcwowwwww/Real-Time-Object-Detection-and-Tracking), but i'm having a problem, tensorflow showing error like this:

> E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.5.1 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.

i have installed CUDNN 7.6.5, 7.6.0, 7.5.1 but the error still appears
"
38168,tf.gather Converting sparse IndexedSlices warning,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from: binary (Anaconda)
- TensorFlow version: 2.1
- Python version: 3.7.7
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: GeForce 1080 Ti

**Describe the current behavior**
Whenever `tf.gather` (or `tf.boolean_mask`) is called with `params` (`tensor`) being the output of another function, this throws the warning `UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.` This behavior seems to be [well known](https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation) but largely ignored/accepted. Calling `tf.gather` on the output of another function is perfectly reasonable and should in my opinion *not* throw a warning.

**Describe the expected behavior**
The forward pass should infer the shape of the dense tensor required in the backward pass, if possible (issue with dynamic shapes?). No warning should be thrown or the user should be able to acknowledge/deactivate the warning (ideally a warning that is more specific than the current one).

**Standalone code to reproduce the issue** 
See e.g. https://github.com/klicperajo/dimenet/blob/master/train.ipynb

**Other info / logs**
https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation
Related: https://github.com/tensorflow/tensorflow/issues/23566"
38167,make_csv_dataset ValueError: Received a feature column from TensorFlow v1,"**System information** 
- OS Platform and Distribution: Mac Os 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): pip install 
- TensorFlow version (use command below):  2.1.0 (v2.1.0-rc2-17-ge5bf8de410 2.1.0)
- Python version: 3.6


**Describe the current behavior**

When trying to train a DNNRegressor using a dataset from make_csv_dataset, I obtain a very strange error message:

```python
ValueError: Received a feature column from TensorFlow v1, but this is a TensorFlow v2 Estimator. Please either use v2 feature columns (accessible via tf.feature_column.* in TF 2.x) with this Estimator, or switch to a v1 Estimator for use with v1 feature columns (accessible via tf.compat.v1.estimator.* and tf.compat.v1.feature_column.*, respectively.
```

**Describe the expected behavior**
I was expecting that this would work directly. 

**Standalone code to reproduce the issue** 

```python
def train_input_fn():
    df = tf.data.experimental.make_csv_dataset(
        file_pattern, batch_size, column_names=None, column_defaults=None,
        label_name=label_name[0], select_columns=column_names, field_delim=',',
        use_quote_delim=True,
        na_value='', header=True, num_epochs=None, shuffle=True,
        shuffle_buffer_size=10000, shuffle_seed=None,
        prefetch_buffer_size=None,
        num_parallel_reads=None, sloppy=False,
        num_rows_for_inference=100,
        compression_type=None, ignore_errors=False
        )
    df_batches = (
        df.cache().repeat().shuffle(500)
        .prefetch(tf.data.experimental.AUTOTUNE))
    return df_batches

tf.keras.backend.set_floatx('float32')

nfeat = len(feature_names)
ncovs = nfeat * (nfeat + 1) // 2
model = tf.estimator.DNNRegressor([ncovs, ],
                                  # feature_names,
                                  # activation_fn = tf.nn.relu,
                                  dropout=0.3,
                                  optimizer=""Adam"",
                                  weight_column='weights'
                                  )

history = model.train(train_input_fn, steps=40000)
```

I do not understand how to make this work honestly. I cannot find a end-to-end minimal example that uses a CSV input data file that is too large to fit in memory."
38166,how can I pass logits before I fit and predict model?,"Since I need to train a model with multiple labels, I need to use loss function `tf.nn.sigmoid_cross_entropy_with_logits`. This function has two parameters: `logits` and `loss`.
Does parameter `logits`is the value of predicted y? How can I pass this value before I compile model? I cannot predict y before I compile and fit model, right?

This is my code:

```
m_part1 = keras.Sequential([keras.layers.Dense(50, activation='tanh', input_shape=[100]), 
                            keras.layers.Dense(30, activation='relu'),
                            keras.layers.Dense(50, activation='tanh'),
                            keras.layers.Dense(100, activation='relu')])
m_part2 = keras.Sequential([keras.layers.Dense(8, input_shape=[100])])
model = keras.Sequential([m_part1, m_part2])

model.compile(optimizer='rmsprop', 
              loss=tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred), labels=y),   # <---How to figure out y_pred?
              metrics=['accuracy'])
model.fit(x, y, epochs=10, batch_size=32)
y_pred = model.predict(x)  # <--- Now I got y_pred
```

How to calculate `y_pred` even before compiling model?"
38165,"tf 2 vs tf 1 - different behaviour: model.evaluate(..., batch_size=...)","Consider the following code (you can run it [here](https://colab.research.google.com/drive/1H04RM9TsFpQf8_8ZbExMkW-jWLzMi7eL) on colab):

```
import tensorflow as tf
from tensorflow import keras
import numpy as np

x_train = np.array([[1],[20],[3]],
                   dtype=np.float32)

y_train = np.array([[0.1],[0.2],[0.7]],
                   dtype=np.float32)

model = keras.Sequential([
                          keras.layers.Dense(1,input_shape=[1], activation='sigmoid',kernel_initializer='ones')
])
model.compile(
    loss=keras.losses.binary_crossentropy,
    optimizer=keras.optimizers.Adam(),
    metrics=['binary_crossentropy']
)

x_train.shape
model.evaluate(x_train,y_train,batch_size=3)
model.evaluate(x_train,y_train,batch_size=2)
model.evaluate(x_train[:2],y_train[:2],batch_size=2)
model.evaluate(x_train[2:],y_train[2:],batch_size=2)
```

In TF 2: it gives:

> **1/1** [==============================] - 0s 2ms/step - loss: 6.0539 - binary_crossentropy: 6.0539
**2/2** [==============================] - 0s 1ms/step - loss: **4.7776** - binary_crossentropy: 6.0539
**1/1** [==============================] - 0s 1ms/step - loss: 8.6066 - binary_crossentropy: 8.6066
1/1 [==============================] - 0s 1ms/step - loss: 0.9486 - binary_crossentropy: 0.9486
[0.9485874176025391, 0.9485874176025391]

In TF 1: it gives:

> **3/3** [==============================] - 0s 4ms/sample - loss: 6.0539 - binary_crossentropy: 6.0539
**3/3** [==============================] - 0s 2ms/sample - loss: **6.0539** - binary_crossentropy: 6.0539
**2/2** [==============================] - 0s 321us/sample - loss: 8.6066 - binary_crossentropy: 8.6066
1/1 [==============================] - 0s 713us/sample - loss: 0.9486 - binary_crossentropy: 0.9486
[0.9485874176025391, 0.9485874]

1. So, in tf 1 the evaluation is computed correctly irrespective of the batch_size, but in tf 2 it is not the case. Why?

2. Why the loss and metric are not the same: '2/2 [==============================] - 0s 1ms/step - loss: **4.7776** - binary_crossentropy: 6.0539' ?"
38164,Dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory;,"**System information**
- OS Platform and Distribution: nvidia/cuda:10.1-base-ubuntu18.04 docker base
- TensorFlow installed from (source or binary): Custom
- TensorFlow version: 2.2rc2
- Python version: 3.73
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 10.1, 7.x
- GPU model and memory: 1050M

**Describe the problem**
Hi,

I am trying to create my own Docker container for Tensorflow on the GPU.

My base is: 
```
FROM nvidia/cuda:10.1-base-ubuntu18.04
LABEL authors=""Lukas Heumos (lukas.heumos@posteo.net)"" \
      description=""Docker image containing all requirements for running machine learning on CUDA enabled GPUs""

# Install some basic utilities
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    ca-certificates \
    sudo \
    git \
    bzip2 \
    libx11-6 \
 && rm -rf /var/lib/apt/lists/*

# Create a working directory and set it as default
RUN mkdir /app
RUN chmod 777 /app
WORKDIR /app

# Create a non-root user and switch to it
RUN adduser --disabled-password --gecos '' --shell /bin/bash user 
RUN echo ""user ALL=(ALL) NOPASSWD:ALL"" > /etc/sudoers.d/90-user
USER user

# All users can use /home/user as their home directory
ENV HOME=/home/user
RUN chmod 777 /home/user

 # Install Miniconda
RUN curl -so ~/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh \
 && chmod +x ~/miniconda.sh \
 && ~/miniconda.sh -b -p ~/miniconda \
 && rm ~/miniconda.sh
ENV PATH=/home/user/miniconda/bin:$PATH
ENV CONDA_AUTO_UPDATE_CONDA=false

# Update Conda
RUN conda update conda
```



And my tensorflow container is: 

```
From mlflowcore/base:1.0.0

# Install the conda environment
COPY tensorflow_environment.yml .
RUN conda env create -f tensorflow_environment.yml && conda clean -a

# Activate the environment
RUN echo ""source activate tensorflow-2.1-cuda-10.1"" > ~/.bashrc
ENV PATH /opt/conda/envs/env/bin:$PATH

# Dump the details of the installed packages to a file for posterity
RUN conda env export --name tensorflow-2.1-cuda-10.1 > tensorflow-2.1-cuda-10.1.yml
```
with the environment.yml:

```
name: tensorflow-2.1-cuda-10.1
channels:
    - conda-forge
    - defaults
dependencies:
    - defaults::cudatoolkit=10.1
    #- defaults::tensorflow=2.1.0 -> distribute.MirroredStrategy API changed in 2.2 -> https://www.tensorflow.org/tutorials/distribute/custom_training
    - conda-forge::graphviz=2.40.1
    - conda-forge::python-graphviz=0.13.2
    - pip
    - pip:
      - tensorflow==2.2.0rc2
```

However, when trying to run stuff on the GPU I get:

> 2020-04-02 09:39:54.522822: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
> 2020-04-02 09:39:54.570821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
> 2020-04-02 09:39:54.572960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
> 2020-04-02 09:39:54.573491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
> 2020-04-02 09:39:54.577211: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
> 2020-04-02 09:39:54.578817: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
> 2020-04-02 09:39:54.579250: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64
> 2020-04-02 09:39:54.579268: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. 
> Skipping registering GPU devices...

Why does it not find that file? Where is it?

Help would be highly appreciated.
Thank you very much!
Best

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Simple 

`tf.config.list_physical_devices('GPU')`

Also ran an MNIST example. Without success.

**Any other info / logs**
I am using the very same Base & Docker image structure for Pytorch and XGBoost and it works flawlessly and uses the GPU. Hence, this is a Tensorflow issue.
"
38163,Import fail in Tensorflow,"
**System information**
- OS Platform: Windows 
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.2.0rc2, Build: pypi_0 , Channel: pypi
- Python version:3.8.2
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version:10.2
- GPU model and memory: Nvidia GeForce RTX 2060
GPU memory:13.9GB
Dedicated GPU memory:6GB
Shared GPU Memory:7.9GB

I installed TensorFlow in a new anaconda environment was working fine later that day I installed keras, while installation processes I completed 3 prerequisites,cuDNN ,HDF5 and h5py,graphviz and pydot (used by visualization utilities to plot model graphs). Then while trying to import Keras and Tensorflow simultaneously Kernal keep getting restarted
![image](https://user-images.githubusercontent.com/31595943/78250744-f396ea80-750d-11ea-94fd-e86c8d14f591.png)
 Later in only Tensorflow installation
Then the following error occurred
![image](https://user-images.githubusercontent.com/31595943/78250929-335dd200-750e-11ea-8c53-e71ddf3bb6e7.png)
![image](https://user-images.githubusercontent.com/31595943/78250973-4375b180-750e-11ea-93e9-bf7549a2a181.png)


If possible please let me know the exact suitable version to install for
python
TensorFlow
Keras
cuDNN
and other libraries so that I could easily use TensorFlow and Keras using my GPU
Thanks in advance"
38162,Provide a tensorflow maven package for AWS Lambda or lighter builds in general,"The current build size of tensorflow 1.14.0 makes it impossible to use the library on AWS Lambda without further steps to reduce the depedency size.

It would be great to have for example a tensorflow dependency for AWS Lambda specifically.

What would also work, would be a separate module for tensorflow without all native libraries and a module per native implementation. This way the overhead of the darwin and windows native code is gone and reduces the dependency significantly (~200MB)."
38161,"tflite model crashes before loading .tflite file, with no error or exception","**System information** 
- Have I written custom code: 

Yes I have:

In Activity.java:

=======================================================================

```
gpuDelegate = new GpuDelegate();
Interpreter.Options tfliteOptions = (new Interpreter.Options()
          .addDelegate(gpuDelegate));
tfliteModel
        = FileUtil.loadMappedFile(myContext,
        ""my_model.tflite"");
tfliteInterpreter = new Interpreter(tfliteModel, tfliteOptions);
```
...

```
imageProcessor =
        new ImageProcessor.Builder()
                .add(new NormalizeOp(MY_MEAN, MY_STDDEV))
                .build();

inputImageBuffer.load(bitmap);
imageProcessor.process(inputImageBuffer);
```

...

`tfliteInterpreter.run(inputImageBuffer.getBuffer(), outputBuffer.getBuffer().rewind());`

=======================================================================

In build.gradle:

=======================================================================
```
implementation 'org.tensorflow:tensorflow-lite:+'
implementation 'org.tensorflow:tensorflow-lite-gpu:+'
implementation 'org.tensorflow:tensorflow-lite-support:+'
```
=======================================================================

- OS Platform and Distribution: 

Linux Ubuntu 16.04

- Mobile device: 

Samsung Galaxy S10

- TensorFlow installed:

From binary (pip)

- TensorFlow version: 

TensorFlow 2.0

- Python version:

Python 3.6

- GCC/Compiler version:

Wasn't compiled from source, but gcc 7.4.0

- CUDA/cuDNN version:

CUDA 10.0

- GPU model and memory:

GeForce RTX 2080 Ti
11GB

**Describe the current behavior**

Model input is a 1MP image.
I'm trying to run the model via CPU (tfliteOptions are default), GPU (GPU delegate) and NNAPI (NNAPI delegate).

1MP runs with the GPU delegate only - the other two options crash.
When crashing - the debug process is detached from Android.
There are no exceptions, errors or hints regarding what happened.

After that, I take the same exact model, and I only change the input size, from 1MP to 10MP (in python it's just changing the HxW of the input image). From the 10MP model I create a new .tflite file.
When trying to run the 10MP model - it doesn't even load.

Is it a memory issue? and if so - how can I check that? What can I do to reduce the impact?

**Describe the expected behavior**

Have some exceptions, error messages or any other hint as to what's going on.
Let's say it's a memory issue - how can I handle that? Which part fails exactly?

**Other info / logs**

- Debugging the 1MP model on CPU (the issue is that the app crashes):

Last debugger frames:

getRuntime:166, VMRuntime (dalvik.system)
<init>:69, DirectByteBuffer$MemoryRef (java.nio)
allocateDirect:258, ByteBuffer (java.nio)
allocateMemory:352, TensorBuffer (org.tensorflow.lite.support.tensorbuffer)
<init>:304, TensorBuffer (org.tensorflow.lite.support.tensorbuffer)
<init>:38, TensorBufferFloat (org.tensorflow.lite.support.tensorbuffer)
createDynamic:96, TensorBuffer (org.tensorflow.lite.support.tensorbuffer)
getTensorBuffer:297, TensorImage$ImageContainer (org.tensorflow.lite.support.image)
getTensorBuffer:213, TensorImage (org.tensorflow.lite.support.image)
apply:52, TensorOperatorWrapper (org.tensorflow.lite.support.image.ops)

Process is detached when calling ByteBuffer.allocateDirect (in allocateMemory function):
```
    private void allocateMemory(int[] shape) {
        SupportPreconditions.checkNotNull(shape, ""TensorBuffer shape cannot be null."");
        SupportPreconditions.checkArgument(isShapeValid(shape), ""Values in TensorBuffer shape should be non-negative."");
        int newFlatSize = computeFlatSize(shape);
        if (this.flatSize != newFlatSize) {
            this.flatSize = newFlatSize;
            this.shape = (int[])shape.clone();
            this.buffer = ByteBuffer.allocateDirect(this.flatSize * this.getTypeSize()); <<<< HERE <<<<
            this.buffer.order(ByteOrder.nativeOrder());
        }
    }
```

- Debugging the 1MP model on NNAPI (the issue is that the app crashes):

Process is detached when calling applyDelegate (in applyDelegates function, in NativeInterpreterWapper.class):
```
while(var6.hasNext()) {
                Delegate delegate = (Delegate)var6.next();
                applyDelegate(this.interpreterHandle, this.errorHandle, delegate.getNativeHandle()); <<<< HERE <<<<
                this.delegates.add(delegate);
            }
```

- Debugging the 10MP model on GPU (the issue is that the .tflite model doesn't even gets loaded):

I see that the Asset Manager gets closed.
Last debugger frames:

get:145, ClosedGuard (dalvik.system)
<init>:104, ParcelFileDescriptor (android.os)
<init>:184, ParcelFileDescriptor (android.os)
nativeOpenAssetFd:-1, AssetManager (android.content.res)
openFd:848, AssetManager (android.content.res)
loadMappedFile:154, FileUtil (org.tensorflow.lite.support.common)
"
38160,AttributeError: module 'tensorflow.keras.optimizers' has no attribute 'Αdam',"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 2.2.0-rc2

**Describe the current behavior**
Tensorflow 2.2.0-rc2 cannot find Adam in tf.keras.optimizers.

**Standalone code to reproduce the issue** 
optimizer = tf.keras.optimizers.Αdam(2e-4)
"
38158,Unable to batch tensorflowLite object detection model,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): MacOSX
- TensorFlow installed from (source or
binary): Binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7

**Describe the current behavior**
I'm using the TfLite object detection models in python and upon trying to resize the input to increase the batch size; I'm getting an error as follows: 

```
Traceback (most recent call last):
  File ""test_cropped_face.py"", line 70, in <module>
    interpreter.allocate_tensors()
  File ""/Users/harshitdwivedi/Desktop/tf_env/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py"", line 73, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/Users/harshitdwivedi/Desktop/tf_env/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (3276800 != 65536)Node number 88 (RESHAPE) failed to prepare.
```

**Describe the expected behavior**
Setting a custom batch size for an image classification model works without any issues; so I expect the same thing to happen here as well.

**Standalone code to reproduce the issue** 
```
interpreter = tf.contrib.lite.Interpreter(model_path=""cropped_face.tflite"")

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# resize the input to run inference on more than 1 image at a time; the default size is [1,512,512,3]
interpreter.resize_tensor_input(input_details[0]['index'], [50, 512, 512, 3])
interpreter.allocate_tensors()

```
Here's the model I'm using. I have trained it via the GCP's Cloud Vision Dashboard.
https://drive.google.com/file/d/1teJ34GvWmd-1aZBqNLeqIy-owOj5_RDS/view?usp=sharing"
38157,TF Lite build missing .lib,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): msvc 142 (vs2019)
- CUDA/cuDNN version: 10 / 7
- GPU model and memory: GTX 1080



**Describe the problem**
Trying to build Tensorflow ( lite only ) with bazel in windows using the provide rule
```c++
tflite_cc_shared_object(
    name = ""tensorflowlite"",
...
)
```
There are many outputs and the tensorflowlite.dll. Yet there aint the corresponding tensorflowlite.lib ? Where can I find it or special configuration is needed ? 

The partial output log for compilation

```cmd
INFO: From Linking tensorflow/lite/tensorflowlite.dll:
LINK : warning LNK4044: unrecognized option '/s'; ignored
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/lite/libtensorflowlite.dll.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/lite/libtensorflowlite.dll.exp
Target //tensorflow/lite:tensorflowlite up-to-date:
  bazel-bin/tensorflow/lite/tensorflowlite.dll
INFO: Elapsed time: 112.240s, Critical Path: 50.53s
INFO: 308 processes: 308 local.
INFO: Build completed successfully, 440 total actions

```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Since the missing .lib, simple code will give unresolved external symbol link error. E.g.
```c++
#include <c_api.h>
#include <iostream>

int main(int argc, char *argv[])
{
  std::cout << TfLiteVersion() << std::endl;
  return 0;
}
```
Which gives

``error LNK2019: unresolved external symbol __imp_TfLiteVersion``

-----------------------------------------------


**Any other info / logs**
--------------------------------------------
![image](https://user-images.githubusercontent.com/56542376/78237947-92234b80-750e-11ea-88fe-1f39af92b8d8.png)
"
38156,TFLM: add run-time check that statically allocated buffers are sufficiently large,"It would be great to have a simple run-time check that the statically allocated buffer is sufficient.

This is as simple as adding one line:
```
TFLITE_DCHECK_LE(output_channels, kMaxChannels);
```
after this one:
https://github.com/tensorflow/tensorflow/blob/83e974b61c7b65ee8de87774addf1a0dc6b24c9a/tensorflow/lite/micro/kernels/conv.cc#L99
"
38155,"tf.keras.Model subclassing: when check inputs size inside the model, the size display ""batch number"" as None","Hello. I am new to deep learning with the tf2.0 . And I am currently working on the cifar 10 dataset. 
Recently I encounter a problem with the model subclassing. The sample code is given as 

1. assign inputs from the processing, the batch size is [500, 32, 32, 3] with channel_last format. So each batch has 500 images. 
2. define a CNN model
```
class CNN(tf.keras.Model)
        def __init__(self):
            super(CNN, self).__init__()
            self.conv1 = layers.conv2d(......)
            # and other layers definition
         def call(self, inputs):
             # here i try to test the size of inputs, using tf.shape(inputs) and inputs.shape()
             # however, tf.shape returns (4,) and inputs.shape returns [None, 32, 32, 3]
             #    I don't know why. The inputs should be 500 images. 
              #   And it looks like it only contains one image. 
```
3. I call the model, such as 
``` 
model = CNN()
y = model.predict(inputs)
```

The sample codes are stated above. 
The problem is i cannot input 500 images all in once to the model subclassing. I do not quite sure what is wrong with it. I tried the ""def build(self, input_shapes)"" as listed on the doc to define the input_shapes as [500, 32, 32, 3] but it still fails. I found a similar issue #36991. In one discussion, someone says ""then use batch number = 1"". But i would like to know if there is anyway to fix my code so that i can feed mini batch into the model. 
The full code is below. When you run this code, there will be errors. But I simply would like to address the issue with the batch number dimension. Since if the ""inputs"" inside the model does not have that dimension, i cannot do the 500 images flatten process. 
Thank you ~~
```
# for github issue use

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.datasets import cifar10


if __name__ == '__main__':
    (x_train, y_train), (x_label, y_label) = cifar10.load_data()
    x_train = x_train.astype(np.float32)

    class CNN(tf.keras.Model):
        def __init__(self):
            super(CNN, self).__init__()

            self.conv1 = layers.Conv2D(
                filters=32,
                kernel_size=[5, 5],
                strides=(1, 1),
                padding='same',
                activation=tf.nn.relu,
                kernel_initializer=tf.random_normal_initializer(
                    mean=0.0, stddev=0.01
                ),
                bias_initializer=tf.zeros_initializer(),
                data_format='channels_last'
            )
            # the above output is [n, 32, 32, 32]
            # from the book, the channel number 3 disappears after process
            # 池化输出大小=[（输入大小-卷积核（过滤器）大小）／步长]+1

            self.pool1 = layers.MaxPool2D(
                pool_size=[3, 3], strides=2,
                data_format='channels_last',
                padding='VALID'
            )
            # the output above is [n, 15, 15, 64]

            # self.lrn1 = LRNLayer(
            #     depth_radius=5,
            #     bias=1,
            #     alpha=1,
            #     beta=0.5
            # )
            # during class call, add the lrn layer, output size not change

            self.conv2 = layers.Conv2D(
                filters=64,
                kernel_size=[5, 5],
                strides=(1,1),
                kernel_initializer=tf.random_normal_initializer(
                    mean=0, stddev=0.01
                ),
                bias_initializer=tf.zeros_initializer(),
                padding='same',
                activation=tf.nn.relu,
                data_format='channels_last'
            )
            # padding = same, so output is [n, 15, 15, 64]
            # self.lrn2 = LRNLayer(
            #     depth_radius=5,
            #     bias=1,
            #     alpha=1,
            #     beta=0.5
            # )

            self.pool2 = layers.MaxPool2D(
                pool_size=[3, 3], strides=2,
                data_format='channels_last',
                padding=""VALID""
            )
            # output size = [N, 7, 7, 64]

            # self.flatten = layers.Reshape(target_shape=(-1, 7*7*64))

            self.dense1 = layers.Dense(
                units=784,
                activation=tf.nn.relu,
                kernel_initializer=tf.random_normal_initializer(
                    mean=0.0, stddev=0.05
                ),
                bias_initializer=tf.zeros_initializer()
            )
            # output is [n, 784]
            self.batchNorm = layers.BatchNormalization()

            self.dense2 = layers.Dense(
                units=10,
                activation=tf.nn.relu,
                kernel_initializer=tf.random_normal_initializer(
                    mean=0.0, stddev=0.05
                ),
                bias_initializer=tf.zeros_initializer()
            )
            # output is 10

        # def build(self, input_shape):
        #         #     super(CNN, self).build(input_shape)

        def call(self, inputs):
            # print(inputs[0, ::])
            print(""here inside the model"")
            print(""inputs.shape = {}"".format(inputs.shape)) # (None, 32, 32, 3)
            print(""tf.shape = {}"".format(tf.shape(inputs)))
            print(""type(inputs) = {}"".format(type(inputs)))


            x = self.conv1(inputs) # (None, 32, 32, 32)
            print(""x.shape = {}"".format(x.shape))
            print(""tf.shape = {}"".format(tf.shape(x)))

            # not useful for this issue.
            # x = self.pool1(x)
            # # x = self.lrn1(x)
            # x = self.conv2(x)
            # # x = self.lrn2(x)
            # x = self.pool2(x)
            # x = tf.reshape(tensor=x, shape=(x.shape[0], -1))
            # x = self.dense1(x)
            # x = self.batchNorm(x)
            # x = self.dense2(x)
            # y = tf.nn.softmax(x)

            return y


    iteration = 500
    batch_size = 500
    learning_rate = 0.001

    model = CNN()
    # model.build(input_shape=(None, 32, 32, 3))
    # # print(model.conv1.input_spec())

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    print(""Here try to predict"")
    y = model.predict(x_train[:500, ::])

    # for i in range(iteration + 1):
    #     batch = batch(train, batch_size)
    #
    #     print(batch.data.shape, batch.label.shape) # (500, 32, 32, 3) (500,)
    #     print(type(batch.data))
    #
    #    # get some accuracy to examine
    #     if i % 100 == 0:
    #         batch_predict = model.predict(batch.data)
    #         test_predict = model.predict(test.data)
    #
    #         acc_train = accuracy(
    #             prediction=batch_predict, label=batch.label
    #         )
    #
    #         acc_test = accuracy(
    #             prediction=test_predict, label=test.label
    #         )
    #
    #         train_acc_list.append(acc_train)
    #         test_acc_list.append(acc_test)
    #
    #         print(""Iter {} of {}: train_acc={}, test_acc={}"".format(i, iteration, acc_train, acc_test))
    #
    #     # now do the training
    #     with tf.GradientTape() as tape:
    #         batch_predict = model(batch.data)
    #         loss = tf.keras.losses.sparse_categorical_crossentropy(
    #             y_pred=batch_predict, y_true=batch.label
    #         )
    #         loss = tf.reduce_mean(loss)
    #
    #     train_loss_list.append(loss)
    #
    #     grads = tape.gradient(loss, model.variables)
    #     optimizer.apply_gradients(
    #         grads_and_vars=zip(grads, model.variables)
    #     )
```"
38153,[tf.data] _pywrap_server_lib.so breaks nightly packages,"Our in-house nightly builds were broken since 2020-03-31 when auditwheel tries to repair the nightly packages. The reason under the hood seems to be an incorrect link from the recent change of adding tf.data service support in 0b63458060d7f8400c7e5b37ccdd7ca89c2ffbe1 and enabled in nightly packages in 503179f662708e730fd0a0c98261e9678975e3b8.

Install the latest nightly, and check the dynamic linker:

```
$ ldd /usr/local/lib/python3.8/dist-packages/tensorflow/core/data/service/python/_pywrap_server_lib.so
	linux-vdso.so.1 (0x00007ffd621f4000)
	libtensorflow_framework.so.2 => /usr/local/lib/python3.8/dist-packages/tensorflow/core/data/service/python/../../../../libtensorflow_framework.so.2 (0x00007fda32b70000)
	_pywrap_tensorflow_internal.so => not found
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fda32b5a000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fda32b39000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fda329f2000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fda32825000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fda3280b000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fda32648000)
	/lib64/ld-linux-x86-64.so.2 (0x00007fda34bfd000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fda3263d000)
```

Obviously it links to `_pywrap_tensorflow_internal.so` but it is not found with the relative path.

PS: we are using `auditwheel==3.0.0` to produce manylinux2014 builds, but the official tf-nightly uses an older version which fails to catch this.

Ping @aaudiber @mihaimaruseac. We had met a similar issue in #36067. As mentioned there, the reason seems to be that ``tf_python_pybind_extension`` should only be used for packages in ``//tensorflow/python/...`` but it is used in ``//tensorflow/core/..``  in this case.

Any idea how to avoid this in the future? Also cc @av8ramit."
38152,Java SavedModelBundle import LookupTable core dump,"I use `libtensorflow.jar` to load a model with `saved_model` format. The core dump occurs in `LookupTableImportOp` computation stage. However, this model could be loaded successfully via `c++ tensoflow-serving executable` or `python tf.saved_model.loader`.

* tensorflow 1.12.
* mac os. cpu only.

```
public static void main(String[] args) {
    System.out.println(TensorFlow.version());
    SavedModelBundle b = SavedModelBundle.load(""./model/"", ""serve"");
    b.close();
}
```

Error message:
```
1.12.0
2020-04-02 12:09:07.285904: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: ./model/
2020-04-02 12:09:07.295754: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2020-04-02 12:09:07.342467: I tensorflow/cc/saved_model/loader.cc:162] Restoring SavedModel bundle.
[thread 23555 also had an error][thread 42243 also had an error]#

# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV
 (0xb) at pc=0x00000001163dec0e[thread 23299 also had an error], pid=93091
, tid=0x000000000000a103[thread 41475 also had an error]

#
[thread 41731 also had an error]# JRE version: Java(TM) SE Runtime Environment (8.0_201-b09) (build 1.8.0_201-b09)
[thread 42499 also had an error]

# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.201-b09 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# C  [libtensorflow_framework.so+0x44c0e]  tensorflow::lookup::LookupInterface::CheckKeyAndValueTensorsHelper(tensorflow::Tensor const&, tensorflow::Tensor const&)+0x6e
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again

```

Stack message:
```
Stack: [0x000070000e240000,0x000070000e2c0000],  sp=0x000070000e2bf4b0,  free space=509k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [libtensorflow_framework.so+0x44c0e]  tensorflow::lookup::LookupInterface::CheckKeyAndValueTensorsHelper(tensorflow::Tensor const&, tensorflow::Tensor const&)+0x6e
C  [libtensorflow_framework.so+0x44e2e]  tensorflow::lookup::LookupInterface::CheckKeyAndValueTensorsForImport(tensorflow::Tensor const&, tensorflow::Tensor const&)+0xe
C  [libtensorflow_jni.dylib+0x10454e0]  tensorflow::LookupTableImportOp::Compute(tensorflow::OpKernelContext*)+0x140
C  [libtensorflow_framework.so+0x23b362]  tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)+0x1f12
C  [libtensorflow_framework.so+0x2434ba]  std::__1::__function::__func<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&>, std::__1::allocator<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&> >, void ()>::operator()()+0x3a
C  [libtensorflow_framework.so+0x29c58f]  Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)+0x54f
C  [libtensorflow_framework.so+0x29bf3f]  std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()()+0x2f
C  [libtensorflow_framework.so+0x2c0990]  void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*)+0x30
C  [libsystem_pthread.dylib+0x3305]  _pthread_body+0x7e
C  [libsystem_pthread.dylib+0x626f]  _pthread_start+0x46
C  [libsystem_pthread.dylib+0x2415]  thread_start+0xd

```"
38151,Test deterministic cuDNN CTC loss,"@sanjoy added deterministic cuDNN CTC loss, enabled via `TF_DETERMINISTIC_OPS`, with [this commit](https://github.com/tensorflow/tensorflow/commit/9e096debc4a0909deb69970f38bee7b77e5e5f7d). This issue is a reminder to add test for it in [tensorflow/python/kernel_tests/cudnn_deterministic_base.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/cudnn_deterministic_base.py).

This issue is a replacement for PR [38089](https://github.com/tensorflow/tensorflow/pull/38089)."
38150,Extra metric added to model.metrics in TF 2.2,"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
Ubuntu 18.04 (running on Colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
N/A
- TensorFlow installed from (source or binary):
Running on Colab
- TensorFlow version (use command below): 
2.2.0-rc2
- Python version:
3.6.9
- Bazel version (if compiling from source):
Running on Colab
- GCC/Compiler version (if compiling from source): 
Running on Colab
- CUDA/cuDNN version:
No GPU
- GPU model and memory:
No GPU

**Describe the current behavior**

When I compile my model with `model.compile(..., metrics=[""mae""])` I end up with an extra metric in the first position. This breaks my existing code (for example when accessing `model.metrics[0]`, now I get another metric than the one that was excepted).

**Describe the expected behavior**

When I compile the model with N metrics, I expect `model.metrics` to return a list with N metrics, not 1+N.

**Standalone code to reproduce the issue** 

You can run the following code in [this colab](https://colab.research.google.com/drive/1yKy-TC5PyE6ImQemE89SDSiQkFniXIcb).

```python
import numpy as np
from tensorflow import keras

X_train = np.random.rand(100, 10)
y_train = np.random.rand(100, 1)

model = keras.models.Sequential([
    keras.layers.Dense(2, activation=""relu"", input_shape=[10]),
    keras.layers.Dense(1)
])
model.compile(loss=""mse"", optimizer=""sgd"", metrics=[""mae"", ""mse""])
model.fit(X_train, y_train, epochs=2)

assert len(model.metrics) == 2
```

The assertion fails.

**Other info / logs**

This may be related to issue #37990 , but it feels more severe.

There's another behavior change that really confuses me: the `model.metrics` list is empty until `model.fit()` is called.

In short, `model.metrics` seems really broken and unintuitive now.
"
38149,test,"Traceback (most recent call last):
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
return load_dynamic(name, filename, file)
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""generate_tfrecord.py"", line 17, in
import tensorflow as tf
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow_init_.py"", line 41, in
from tensorflow.python.tools import module_util as module_util
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python_init.py"", line 50, in
from tensorflow.python import pywrap_tensorflow
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
return load_dynamic(name, filename, file)
File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

Failed to load the native TensorFlow runtime."
38148,test ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
38147,`tf_http_archive` in RC breaks with Bazel 2.2.0,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18 (https://hub.docker.com/r/dmadisetti/nvidia-ibazel)
- TensorFlow installed from (source or binary): Source
- TensorFlow version: tensorflow-2.2.0-rc2
- Bazel version (if compiling from source): 2.2.0
- GCC/Compiler version (if compiling from source): clang-9
- CUDA/cuDNN version: 10.1


**Describe the problem**

Tensorflow will not compile as an external repositiory, as `tf_http_repository` fails on sourcing other libraries. I investigated a little but want to post here.


**Provide the exact sequence of commands / steps that you executed before running into the problem**

In my WORKSPACE
```starlark
http_archive(
    name = ""org_tensorflow"",
    strip_prefix = ""tensorflow-2.2.0-rc2"",
    sha256 = ""3e0cffc98ad3767dd16a09b7f25a99961e21d65cfcad67ed3a9b1a01318c7e94"",
    urls = [""https://github.com/tensorflow/tensorflow/archive/v2.2.0-rc2.tar.gz""],
)
```

Attempting to run a test from my workspace, and no cache (or .bazelrc):
`bazel  test --experimental_repo_remote_exec repo/cc:all repo/python:all`

Yields:
```
Starting local Bazel server and connecting to it...
ERROR: /tmp/build_cache/_bazel_root/a08c2e4811c846650b733c6fc815a920/external/org_tensorflow/tensorflow/workspace.bzl:653:5: no such target '@org_tensorflow//third_party:zlib.BUILD': target 'zlib.BUILD' not declared in package 'third_party'; however, a source file of this name exists.  (Perhaps add 'exports_files([""zlib.BUILD""])' to third_party/BUILD?) defined by /tmp/build_cache/_bazel_root/a08c2e4811c846650b733c6fc815a920/external/org_tensorflow/third_party/BUILD and referenced by '//external:zlib'
ERROR: /tmp/build_cache/_bazel_root/a08c2e4811c846650b733c6fc815a920/external/org_tensorflow/tensorflow/workspace.bzl:653:5: no such target '@org_tensorflow//third_party/systemlibs:zlib.BUILD': target 'zlib.BUILD' not declared in package 'third_party/systemlibs'; however, a source file of this name exists.  (Perhaps add 'exports_files([""zlib.BUILD""])' to third_party/systemlibs/BUILD?) defined by /tmp/build_cache/_bazel_root/a08c2e4811c846650b733c6fc815a920/external/org_tensorflow/third_party/systemlibs/BUILD and referenced by '//external:zlib'
```

Cool, cool. So I added `export_files(glob([""*.BUILD""]))` to `third_party/systemlibs/BUILD` and `third_party/BUILD` which gives me:

```
Starting local Bazel server and connecting to it...
ERROR: /tmp/build_cache/_bazel_root/a08c2e4811c846650b733c6fc815a920/external/org_tensorflow/tensorflow/workspace.bzl:653:5: in build_file attribute of tf_http_archive rule //external:zlib: source file '@org_tensorflow//third_party:zlib.BUILD' is misplaced here (expected no files)
ERROR: /tmp/build_cache/_bazel_root/a08c2e4811c846650b733c6fc815a920/external/org_tensorflow/tensorflow/workspace.bzl:653:5: in system_build_file attribute of tf_http_archive rule //external:zlib: source file '@org_tensorflow//third_party/systemlibs:zlib.BUILD' is misplaced here (expected no files)
INFO: Call stack for the definition of repository 'remotejdk11_linux' which is a http_archive (rule definition at /tmp/build_cache/_bazel_root/a08c2e4811c846650b733c6fc815a920/external/bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /tmp/build_cache/_bazel_root/a08c2e4811c846650b733c6fc815a920/external/bazel_tools/tools/build_defs/repo/utils.bzl:201:9
 - /DEFAULT.WORKSPACE.SUFFIX:235:1
ERROR: Analysis of target '//repo/cc:cc_proto' failed; build aborted: Analysis of target '//external:zlib' failed; build aborted
```
Makes sense, `tf_http_archive` isn't set to allow source files. So I changed: `third_party/repo.bzl`, such that the attributes have the `allow_single_file` option.

```starlark
tf_http_archive = repository_rule(
    attrs = {
        ""sha256"": attr.string(mandatory = True),
        ""urls"": attr.string_list(
            mandatory = True,
            allow_empty = False,
        ),
        ""strip_prefix"": attr.string(),
        ""type"": attr.string(),
        ""delete"": attr.string_list(),
        ""patch_file"": attr.label(allow_single_file = True),
        ""build_file"": attr.label(allow_single_file = True),
        ""system_build_file"": attr.label(allow_single_file = True),
        ""system_link_files"": attr.string_dict(),
        ""additional_build_files"": attr.string_dict(),
    },
    environ = [
        ""TF_SYSTEM_LIBS"",
    ],
    implementation = _tf_http_archive,
)
```

But I guess not? This yields:
```
Starting local Bazel server and connecting to it...
ERROR: /tmp/build_cache/_bazel_root/a08c2e4811c846650b733c6fc815a920/external/org_tensorflow/tensorflow/workspace.bzl:653:5: in tf_http_archive rule //external:zlib: Found reference to a workspace rule in a context where a build rule was expected; probably a reference to a target in that external repository, properly specified as @reponame//path/to/package:target, should have been specified by the requesting rule.
ERROR: Analysis of target '//repo/python:repo' failed; build aborted: Analysis of target '//external:zlib' failed; build aborted
INFO: Elapsed time: 5.150s
INFO: 0 processes.
```

At this point I figure I'd just ask. tensorflow-2.1.0 works perfectly, except for a bug that I addressed here: https://github.com/tensorflow/tensorflow/pull/36830

**Any other info / logs**

Done in my docker container! So hopefully repeatable. Altered source in `bazel-workspace/external/org_tensorflow/` as a quick hack"
38145,bug issue,"Traceback (most recent call last):
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""generate_tfrecord.py"", line 17, in <module>
    import tensorflow as tf
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime."
38144,bug issue,"Traceback (most recent call last):
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""generate_tfrecord.py"", line 17, in <module>
    import tensorflow as tf
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

(tensorflow1) C:\tensorflow1\models\research\object_detection>cd C:\tensorflow1\models\research

(tensorflow1) C:\tensorflow1\models\research>cd\

(tensorflow1) C:\>set PYTHONPATH=C:\tensorflow1\models;C:\tensorflow1\models\research;C:\tensorflow1\models\research\slim

(tensorflow1) C:\>set PATH=%PATH%;PYTHONPATH

(tensorflow1) C:\>

(tensorflow1) C:\>cd users

(tensorflow1) C:\Users>cd ahmed

(tensorflow1) C:\Users\ahmed>cd documents

(tensorflow1) C:\Users\ahmed\Documents>set PYTHONPATH=C:\tensorflow1\models;C:\tensorflow1\models\research;C:\tensorflow1\models\research\slim

(tensorflow1) C:\Users\ahmed\Documents>set PATH=%PATH%;PYTHONPATH

(tensorflow1) C:\Users\ahmed\Documents>echo %PYTHONPATH%
C:\tensorflow1\models;C:\tensorflow1\models\research;C:\tensorflow1\models\research\slim

(tensorflow1) C:\Users\ahmed\Documents>python generate_tfrecord.py --csv_input=images\train_labels.csv --image_dir=images\train --output_path=train.record
python: can't open file 'generate_tfrecord.py': [Errno 2] No such file or directory

(tensorflow1) C:\Users\ahmed\Documents>cd object_detection
The system cannot find the path specified.

(tensorflow1) C:\Users\ahmed\Documents> cd C:\tensorflow1\models\research

(tensorflow1) C:\tensorflow1\models\research>cd object_detetction
The system cannot find the path specified.

(tensorflow1) C:\tensorflow1\models\research>cd object_detetcation
The system cannot find the path specified.

(tensorflow1) C:\tensorflow1\models\research>cd object_deteaction
The system cannot find the path specified.

(tensorflow1) C:\tensorflow1\models\research>cd object_detection

(tensorflow1) C:\tensorflow1\models\research\object_detection>python generate_tfrecord.py --csv_input=images\train_labels.csv --image_dir=images\train --output_path=train.record
Traceback (most recent call last):
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""generate_tfrecord.py"", line 17, in <module>
    import tensorflow as tf
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\ahmed\miniconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime."
38143,WideDeepModel cannot be serialized with tf.saved_model.save,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  No
[I used an example script, then tried to save it]
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: - Bazel
version (if compiling from source): v2.1.0-rc2-17-ge5bf8de 
- GCC/Compiler version (if compiling from
source):  N/A
- CUDA/cuDNN version: - GPU model and memory:

**Describe the current behavior**
Serialization fails, apparently because optimizer is a list, rather than a single optimizer.

**Describe the expected behavior**
Serialization works.

**Standalone code to reproduce the issue** 
```
import tensorflow as tf

linear_model = tf.keras.experimental.LinearModel()
linear_model.compile('adagrad', 'mse')
dnn_model = tf.keras.Sequential([tf.keras.layers.Dense(units=1)])
dnn_model.compile('rmsprop', 'mse')
combined_model = tf.keras.experimental.WideDeepModel(dnn_model, linear_model)
combined_model.compile(optimizer=['adagrad', 'rmsprop'], loss='mse')
tf.saved_model.save(combined_model, ""/tmp/saved_model"")
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
AttributeError                            Traceback (most recent call last)
<ipython-input-10-15923fe78379> in <module>
      7 combined_model = tf.keras.experimental.WideDeepModel(dnn_model, linear_model)
      8 combined_model.compile(optimizer=['adagrad', 'rmsprop'], loss='mse')
----> 9 tf.saved_model.save(combined_model, ""/tmp/saved_model"")

/pay/src/zoolander/vendor3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)
    921       compat.as_str(constants.SAVED_MODEL_FILENAME_PB))
    922   object_graph_proto = _serialize_object_graph(
--> 923       saveable_view, asset_info.asset_index)
    924   meta_graph_def.object_graph_def.CopyFrom(object_graph_proto)
    925 

/pay/src/zoolander/vendor3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in _serialize_object_graph(saveable_view, asset_file_def_index)
    651 
    652   for obj, obj_proto in zip(saveable_view.nodes, proto.nodes):
--> 653     _write_object_proto(obj, obj_proto, asset_file_def_index)
    654   return proto
    655 

/pay/src/zoolander/vendor3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in _write_object_proto(obj, proto, asset_file_def_index)
    690           version=versions_pb2.VersionDef(
    691               producer=1, min_consumer=1, bad_consumers=[]),
--> 692           metadata=obj._tracking_metadata)
    693       # pylint:enable=protected-access
    694     proto.user_object.CopyFrom(registered_type_proto)

/pay/src/zoolander/vendor3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _tracking_metadata(self)
   2410   @property
   2411   def _tracking_metadata(self):
-> 2412     return self._trackable_saved_model_saver.tracking_metadata
   2413 
   2414   def _list_extra_dependencies_for_serialization(self, serialization_cache):

/pay/src/zoolander/vendor3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py in tracking_metadata(self)
     55     # object is in the python property)
     56     return json.dumps(
---> 57         self.python_properties,
     58         default=serialization.get_json_type)
     59 

/pay/src/zoolander/vendor3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in python_properties(self)
     38   def python_properties(self):
     39     # TODO(kathywu): Add python property validator
---> 40     return self._python_properties_internal()
     41 
     42   def _python_properties_internal(self):

/pay/src/zoolander/vendor3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py in _python_properties_internal(self)
     36     metadata.update(
     37         saving_utils.model_metadata(
---> 38             self.obj, include_optimizer=True, require_config=False))
     39     return metadata
     40 

/pay/src/zoolander/vendor3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in model_metadata(model, include_optimizer, require_config)
    207         optimizer_config = {
    208             'class_name': model.optimizer.__class__.__name__,
--> 209             'config': model.optimizer.get_config()}
    210       metadata['training_config']['optimizer_config'] = optimizer_config
    211   return metadata
```
"
38141,TensorFlow Developer Certificate,"Does the  TensorFlow exam include Keras?

Are there any sample tests available for review?"
38140,AttributeError: 'list' object has no attribute 'items',"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10 
- TensorFlow installed from (source or
binary): pip
- TensorFlow version (use command below): tensorflow 2.1.0
- Python version: Python 3.7.7 
**Describe the current behavior**
when run the file : app.py , this error happens.
 for key, item in cls_config.items(): AttributeError: 'list' object has no attribute 'items'
how could i solve it ?"
38135,Keras Model Errors on Loading - 'list' object has no attribute 'items' with TF 2.1,"**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ProductName:	Mac OS X, ProductVersion:	10.15.2, BuildVersion:	19C57
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.1.0
Python version: 3.6.8
CUDA/cuDNN version: None
GPU model and memory: None

**Describe the current behavior**

When trying to load one of my models using tf.keras.models.load_model an error is thrown at the following location:

```
tensorflow_core\python\keras\utils\generic_utils.py"", line 254, in class_and_config_for_serialized_keras_object
for key, item in cls_config.items():
**AttributeError: 'list' object has no attribute 'items'**
```
This code expects cls_config to be a dictionary, while for this model it is a list of dictionaries.

I can successfully load and run this model using TensorFlow versions 2.0.0, 1.15.0 and 1.14.0.

This section of code was introduced when adding support for passive serialization in Keras

**Describe the expected behavior**

Can successfully load a model from a hdf5 file.

**Code to reproduce the issue:**

```
import tensorflow as tf

model = tf.keras.models.load_model('cnn_multichannel_dense_f0_b0.h5', compile=False)
```

**Other info / logs:**

**_I am also attaching a dummy hdf5 model below which can be used to test._**



Complete Stacktrace of the error:

```
  File ""/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py"", line 146, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 168, in load_model_from_hdf5
    custom_objects=custom_objects)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/saving/model_config.py"", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/layers/serialization.py"", line 106, in deserialize
    printable_module_name='layer')
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 292, in deserialize_keras_object
    config, module_objects, custom_objects, printable_module_name)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 254, in class_and_config_for_serialized_keras_object
    for key, item in cls_config.items():
AttributeError: 'list' object has no attribute 'items'

```



When loaded with tf.keras in v2.0.0 the layers, model config, inputs, outputs, summary etc. are all parsed correctly, as well as being able to run data through the model."
38134,TensorFlow-gpu for GV100 on SUSE 15,"- **OS Platform and Distribution: SUSE 15
- **TensorFlow installed from (source or binary)**: 2.1.0
- **Python version**: 3.6
- **GCC/Compiler version (if compiling from source)**: 7.5.0
- **CUDA/cuDNN version**: 10.5
- **GPU model and memory**: 100 GV 

![image](https://user-images.githubusercontent.com/14827177/78168197-8441cd80-741d-11ea-9308-35e8da02be49.png)

Wed Apr  1 10:35:13 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro GV100        Off  | 00000000:15:00.0 Off |                  Off |
| 36%   47C    P0    36W / 250W |      0MiB / 32508MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Quadro GV100        Off  | 00000000:2D:00.0 Off |                  Off |
| 40%   53C    P0    36W / 250W |      0MiB / 32500MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

"
38132,test,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
38130,Gradient Compute Error in Embedding Layers,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Ubuntu 16.04.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: No.
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX1080 Ti / 11G

**Describe the current behavior**
I tried to conduct some operations between two tensors. The first one tensor is looked up from an Embedding Layer, and the other one is the weights of the embedding layer. When I try to compute the gradients on all trainable variables, I find that only parameters related to the first tensor are computed. The other parameters in the embedding layer can not be computed and updated. This problem only occurs in Eager Model and the results in static computation graph is correct.

**Describe the expected behavior**
The gradient should be computed and updated on the whole weights in the embedding layer.

**Standalone code to reproduce the issue** 
````
import numpy as np
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras.callbacks import TensorBoard

def loss_func(model, x, y):
    y_ = model(x)
    return tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.AUTO)(y_true=y, y_pred=y_)

def grad(model, inputs, targets):
    with tf.GradientTape() as tape:
        loss_value = loss_func(model, inputs, targets)
    return loss_value, tape.gradient(loss_value, model.trainable_variables)

batch_size = 2
nb_item = 5
nb_hidden = 3

inputs = np.array([[1], [2]])
targets = np.random.randn(batch_size, nb_item)

input_layer = tf.keras.layers.Input((1, ), dtype=tf.int32)

embd_layer = tf.keras.layers.Embedding(nb_item, nb_hidden)

embd1 = tf.reshape(embd_layer(input_layer), [-1, nb_hidden])
all_index = tf.range(nb_item, dtype=tf.int32)
embd2 = embd_layer(all_index)

sco_mat = tf.keras.layers.Lambda(lambda x: tf.matmul(x[0], x[1], transpose_b=True))([embd1, embd2])

model = tf.keras.models.Model(inputs=input_layer, outputs=[sco_mat])

loss, grads = grad(model, inputs, targets)
print(grads[0].values.numpy())
````
Output:
> [[ 0.00897689  0.01056899  0.01004288]
>  [-0.02378326 -0.02354109 -0.01821425]]
Expect output: a tensor with shape (5, 3)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
![bbbbbb](https://user-images.githubusercontent.com/23370871/78166038-cb13df80-747e-11ea-8b5f-5f29f20a00d9.png)
I think the Node MatMul/b should not be treated as a constant value."
38129,Reloading Tensorflow 2.1.0 without restarting interpreter,"In the production system  preinstalled with `tensorflow-gpu==1.13.1`, if I need to run `tensorflow-gpu==2.1.0`, I can only install and reload the library, without restarting the system, but reloading the tensorflow library gives me error, how should I resolve it?

To reproduce the problem on a client computer:


1.First

    pip install tensorflow-gpu==1.13.1

2.Then run the following code

    import tensorflow
    print(tensorflow.__version__) 

it will print

    1.13.1
3. Now run

    pip install tensorflow-gpu==2.1.0 

4.If you run the following code (what I run on production system to reload the module)

    import tensorflow
    from importlib import reload
    tensorflow=reload(tensorflow)
    print(tensorflow.__version__) 


You will get error as follows throw by `reload`, how should I resolve it?

    Traceback (most recent call last):
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
        exec(code_obj, self.user_global_ns, self.user_ns)
      File ""<ipython-input-9-453567e02c5c>"", line 3, in <module>
        tensorflow=reload(tensorflow)
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/importlib/__init__.py"", line 169, in reload
        _bootstrap._exec(spec, module)
      File ""<frozen importlib._bootstrap>"", line 630, in _exec
      File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
      File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/__init__.py"", line 101, in <module>
        from tensorflow_core import *
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
        from tensorflow.python.tools import module_util as _module_util
    ImportError: cannot import name 'module_util' from 'tensorflow.python.tools' (/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/tools/__init__.py)
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 2040, in showtraceback
        stb = value._render_traceback_()
    AttributeError: 'ImportError' object has no attribute '_render_traceback_'
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/ultratb.py"", line 1101, in get_records
        return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/ultratb.py"", line 319, in wrapped
        return f(*args, **kwargs)
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/ultratb.py"", line 353, in _fixed_getinnerframes
        records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/inspect.py"", line 1502, in getinnerframes
        frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/inspect.py"", line 1460, in getframeinfo
        filename = getsourcefile(frame) or getfile(frame)
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/inspect.py"", line 696, in getsourcefile
        if getattr(getmodule(object, filename), '__loader__', None) is not None:
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/inspect.py"", line 733, in getmodule
        if ismodule(module) and hasattr(module, '__file__'):
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
        module = self._load()
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/__init__.py"", line 44, in _load
        module = _importlib.import_module(self.__name__)
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/importlib/__init__.py"", line 127, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
      File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
      File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
      File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
      File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
      File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
      File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
      File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
      File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
      File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
      File ""/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
        from tensorflow.python.tools import module_util as _module_util
    ImportError: cannot import name 'module_util' from 'tensorflow.python.tools' (/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/tools/__init__.py)




"
38126,Convert 0d tensor into float value,How can I convert a tensor of rank 0 and dtype float32 into a 'regular' float value? I am not running a session so tf.eval() does not work. 
38122,'tensorflow._api.v2.compat.v2.compat' has no attribute 'v1',"I am working on Ubuntu 19.10.
I have installed tensorflow version '2.0.0-beta1'
I have installed tensorflow-dataset version '2.1.0'
When I try to load mnist dataset, I get the following error:
 >>> dataset =tfds.load(name=""mnist"")

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py"", line 52, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/registered.py"", line 318, in load
    ds = dbuilder.as_dataset(**as_dataset_kwargs)
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py"", line 52, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py"", line 482, in as_dataset
    datasets = utils.map_nested(build_single_dataset, split, map_tuple=True)
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py"", line 145, in map_nested
    for k, v in data_struct.items()
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py"", line 145, in <dictcomp>
    for k, v in data_struct.items()
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py"", line 159, in map_nested
    return function(data_struct)
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py"", line 541, in _build_single_dataset
    read_config=read_config,
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py"", line 949, in _as_dataset
    shuffle_files=shuffle_files,
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/tfrecords_reader.py"", line 290, in read
    return tf.nest.map_structure(_read_instruction_to_ds, instructions)
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 515, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 515, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/tfrecords_reader.py"", line 287, in _read_instruction_to_ds
    num_examples=file_instructions.num_examples,
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/tfrecords_reader.py"", line 322, in read_files
    num_examples=num_examples,
  File ""/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/tfrecords_reader.py"", line 201, in _read_files
    instruction_ds = tf.compat.v1.data.Dataset.from_tensor_slices(tensor_inputs)
AttributeError: module 'tensorflow._api.v2.compat.v2.compat' has no attribute 'v1'"
38120,TF_GPU_ALLOCATOR=cuda_malloc and the GPUBFC allocator is still used.,"### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When setting the TF_GPU_ALLOCATOR=cuda_malloc env variable, I expect the GPUBFC allocator to not be used. But I have a case where the GPUBFC allocator is still used.

Having it work as expect would help separate OOM that are due to memory fragmentation issue vs other OOM issues.

### Source code / logs
This can be reproduced with the BERT model from https://github.com/tensorflow/models on a computer with 2 GPUs of 16G like this:

Start a container:
```
docker run --gpus all --name tf_gpu_allocator --privileged --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -it nvcr.io/nvidia/tensorflow:20.02-tf2-py3
```

Then run those command into it to prepare the environment:

```
pip install tensorflow_hub sentencepiece gin-config
git clone https://github.com/tensorflow/models.git
cd models/official/nlp/bert/
git checkout b60dc23714d97ca0218a70d912353f388d75b5ef
wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json
wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json
wget https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py
wget https://storage.googleapis.com/cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16.tar.gz
tar -zxf uncased_L-24_H-1024_A-16.tar.gz

export SQUAD_VERSION=v1.1
export SQUAD_DIR=$PWD
export BERT_BASE_DIR=${PWD}/uncased_L-24_H-1024_A-16
export OUTPUT_DIR=/tmp/OUTPUT
mkdir -p /tmp/OUTPUT
PYTHONPATH=../../.. python ../data/create_finetuning_data.py \
              --squad_data_file=${SQUAD_DIR}/train-${SQUAD_VERSION}.json \
              --vocab_file=${BERT_BASE_DIR}/vocab.txt \
              --train_data_output_path=${OUTPUT_DIR}/squad_${SQUAD_VERSION}_train.tf_record \
              --meta_data_file_path=${OUTPUT_DIR}/squad_${SQUAD_VERSION}_meta_data \
              --fine_tuning_task_type=squad --max_seq_length=384

```

Then run the script that doesn't behave like we want:

```
export SQUAD_DIR=/tmp/OUTPUT
TF_CPP_VMODULE=bfc_allocator=1 TF_GPU_ALLOCATOR=cuda_malloc PYTHONPATH=../../.. timeout 30m python run_squad.py \
                --input_meta_data_path=${SQUAD_DIR}/squad_${SQUAD_VERSION}_meta_data \
                --train_data_path=${SQUAD_DIR}/squad_${SQUAD_VERSION}_train.tf_record \
                --predict_file=${SQUAD_DIR}/dev-v1.1.json \
                --vocab_file=${BERT_BASE_DIR}/vocab.txt \
                --bert_config_file=$BERT_BASE_DIR/bert_config.json \
                --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
                --train_batch_size=48 \
                --predict_batch_size=8 \
                --learning_rate=8e-5 \
                --num_train_epochs=1 \
                --distribution_strategy=mirrored \
                --num_gpus=2 --dtype=fp16 --loss_scale=dynamic --learning_rate=8e-5 \
                --all_reduce_alg=nccl  &> OUT

grep gpu_host_bfc OUT
```

I get this as output:
```
2020-03-31 20:57:40.022021: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  4
2020-03-31 20:57:40.023480: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  16
2020-03-31 20:57:40.023520: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  4
2020-03-31 20:57:40.027274: I tensorflow/core/common_runtime/bfc_allocator.cc:549] DeallocateRaw gpu_host_bfc 4
2020-03-31 20:57:40.027325: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  12
2020-03-31 20:57:40.027359: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  4
2020-03-31 20:57:40.027460: I tensorflow/core/common_runtime/bfc_allocator.cc:549] DeallocateRaw gpu_host_bfc 4
2020-03-31 20:57:40.027499: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  12
2020-03-31 20:57:40.027529: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  12
2020-03-31 20:57:40.027554: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  8
2020-03-31 20:57:40.027576: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  8
2020-03-31 20:57:40.027603: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  2
2020-03-31 20:57:40.027681: I tensorflow/core/common_runtime/bfc_allocator.cc:549] DeallocateRaw gpu_host_bfc 2
2020-03-31 20:57:40.027718: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  2
2020-03-31 20:57:40.027782: I tensorflow/core/common_runtime/bfc_allocator.cc:549] DeallocateRaw gpu_host_bfc 2
2020-03-31 20:57:40.027818: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  4
2020-03-31 20:57:40.027845: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  4
2020-03-31 20:57:40.027869: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  16
2020-03-31 20:57:40.027894: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  2
...
```

That show the bfc allocator is still used.


"
38119,Custom loss function with tf.keras,"As far as I have read and researched there is no way to use a custom loss function which uses more than the standard input variables (y_true, y_pred) in a keras model.
Meaning using a Keras Sequential model as it is and just using a custom loss function.
Which makes it difficult to use tf Estimator and gpu computing. 
Please let me know if I am missing something or this is something which can be done or is in progress.

Thanks.
Here is the loss function : - 
```
def get_loss(self,X,Y,lambda_):
        X_tensor = tf.convert_to_tensor(X)
        with tf.GradientTape() as inp_tape:
          inp_tape.watch(X_tensor)
          pred = self.run(X_tensor)
          inp_grad_reg_loss = tf.losses.categorical_crossentropy(Y,pred)
        inpgrad = inp_tape.gradient(inp_grad_reg_loss,X_tensor)
        return inp_grad_reg_loss + lambda_*(tf.norm(inpgrad))
```
**System information**
- TensorFlow version (you are using): TF 2.1
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**
I dont know
**Who will benefit with this feature?**
Everyone I think
**Any Other info.**
"
38117,Using tf.constant in the model causes saving to fail,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): **binary**
- TensorFlow version (use command below): v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2
- Python version: 3.7
 - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 970, 4GB memory

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Whenever I try to save a model which has a tf.constant as input, it fails with ""IndexError: list index out of range"" from inside Keras.
If instead of tf.constant I use a keras Input and pass the same value, everything seems fine.
Note that the model trains and infers successfully, so this is probably an export issue.

**Describe the expected behavior**
Save should succeed.

**Standalone code to reproduce the issue** 

        import numpy as np
        import tensorflow as tf
        from tensorflow.keras.layers import Input, Dense, Concatenate, TimeDistributed, Bidirectional, LSTM
        from tensorflow.keras.models import Model
        from pathlib import Path

        batch_size = 32
        max_sentence_len = 80
        recurrent_size = 96
        embedding_dim = 1024

        encoder_inputs = Input(batch_shape=(batch_size, max_sentence_len, embedding_dim), dtype='float32')
        encoder_lstm = Bidirectional(
            LSTM(recurrent_size, return_sequences=True, return_state=True))
        (encoder_out, encoder_fwd_hstate, encoder_fwd_cstate, encoder_back_hstate, encoder_back_cstate) = encoder_lstm(
            encoder_inputs)

        decoder_lstm = LSTM(recurrent_size * 2, return_sequences=True, return_state=True)

        **inp = tf.constant(np.zeros((batch_size, 2, 2)), dtype='float32')**
        decoder_out, decoder_hstate, decoder_cstate = decoder_lstm(
            inp, initial_state=[Concatenate(axis=-1)([encoder_fwd_hstate, encoder_back_hstate]),
                                Concatenate(axis=-1)([encoder_fwd_cstate, encoder_back_cstate])]
        )

        dense = Dense(2, activation='softmax')
        dense_time = TimeDistributed(dense)
        decoder_pred = dense_time(decoder_out)

        full_model = Model(inputs=encoder_inputs, outputs=decoder_pred)
        full_model.compile(optimizer='adam', loss='binary_crossentropy')

        save_dir = Path('D:\\tmp\\train_logs\\saved_model\\')
        save_dir.mkdir(parents=True, exist_ok=True)

        full_model.save(str(save_dir))


**Other info / logs** 
2020-04-01 16:01:14.975117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
WARNING:tensorflow:From C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2020-04-01 16:01:17.343365: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-04-01 16:01:17.533497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: GeForce GTX 970 computeCapability: 5.2
coreClock: 1.253GHz coreCount: 13 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 208.91GiB/s
2020-04-01 16:01:17.533758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-04-01 16:01:17.537501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-04-01 16:01:17.541404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-04-01 16:01:17.542637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-04-01 16:01:17.546712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-04-01 16:01:17.549502: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-04-01 16:01:17.558631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-04-01 16:01:17.559629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-04-01 16:01:17.559985: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-04-01 16:01:17.568641: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28f8bd6e360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-01 16:01:17.568829: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-04-01 16:01:17.569437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: GeForce GTX 970 computeCapability: 5.2
coreClock: 1.253GHz coreCount: 13 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 208.91GiB/s
2020-04-01 16:01:17.569701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-04-01 16:01:17.569832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-04-01 16:01:17.570177: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-04-01 16:01:17.570366: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-04-01 16:01:17.570541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-04-01 16:01:17.570676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-04-01 16:01:17.570798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-04-01 16:01:17.571827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-04-01 16:01:18.141216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-01 16:01:18.141366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-04-01 16:01:18.141448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-04-01 16:01:18.142398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2991 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:04:00.0, compute capability: 5.2)
2020-04-01 16:01:18.145329: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28fa934abb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-04-01 16:01:18.145500: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 970, Compute Capability 5.2
WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU
WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU
WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU
C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\layers\recurrent.py:820: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  if (isinstance(inputs, collections.Sequence)
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU

Error
Traceback (most recent call last):
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\unittest\case.py"", line 59, in testPartExecutor
    yield
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\unittest\case.py"", line 628, in run
    testMethod()
  File ""D:\Work\Stuff\brain\doctor\models\texttoimprovement\model_test.py"", line 48, in test_reproduce
    full_model.save(str(save_dir))
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 1047, in save
    signatures, options)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\saving\save.py"", line 138, in save_model
    signatures, options)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\saving\saved_model\save.py"", line 78, in save
    save_lib.save(model, filepath, signatures, options)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\saved_model\save.py"", line 951, in save
    obj, export_dir, signatures, options, meta_graph_def)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1008, in _build_meta_graph
    checkpoint_graph_view)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\saved_model\signature_serialization.py"", line 75, in find_function_to_export
    functions = saveable_view.list_functions(saveable_view.root)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\saved_model\save.py"", line 143, in list_functions
    self._serialization_cache)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1593, in _list_functions_for_serialization
    Model, self)._list_functions_for_serialization(serialization_cache)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\engine\base_layer_v1.py"", line 2439, in _list_functions_for_serialization
    .list_functions_for_serialization(serialization_cache))
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\saving\saved_model\base_serialization.py"", line 87, in list_functions_for_serialization
    fns = self.functions_to_serialize(serialization_cache)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 77, in functions_to_serialize
    serialization_cache).functions_to_serialize)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 92, in _get_serialized_attributes
    serialization_cache)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\saving\saved_model\model_serialization.py"", line 47, in _get_serialized_attributes_internal
    default_signature = save_impl.default_save_signature(self.obj)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\saving\saved_model\save_impl.py"", line 203, in default_save_signature
    fn.get_concrete_function()
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\eager\def_function.py"", line 959, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\eager\def_function.py"", line 865, in _get_concrete_function_garbage_collected
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\eager\def_function.py"", line 506, in _initialize
    *args, **kwds))
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\eager\function.py"", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\eager\function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\eager\function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\eager\def_function.py"", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\saving\saving_utils.py"", line 132, in _wrapped_model
    outputs = model(inputs, training=False)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\engine\base_layer_v1.py"", line 778, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 714, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 883, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\engine\base_layer_v1.py"", line 778, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 2830, in call
    return self._make_op(inputs)
  File ""C:\Users\Gilthans\anaconda3\envs\brain\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 2843, in _make_op
    graph = inputs[0].graph
IndexError: list index out of range
"
38116,Error converting MobileNet and MobileNetV2 to tflite (FusedBatchedNormV3),"**System information**
Linux Ubuntu 18.04:
Used pip install, conda install
Used 2.0.0 , 2.1.0, tf-nightly 2.2.0-dev20200401


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import os
import tensorflow as tf
#import tensorflow_addons as tfa
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
import pathlib
from sklearn.utils import class_weight
print(tf.__version__)
print(""Eagerly enabled: "", tf.executing_eagerly())

model.load_weights(""MobileNet_Model3_with_Reg_6_c.h5"") 
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter = True
```

**The output from the converter invocation**

```
ConverterError: See console for info.
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-04-01 07:24:52.175680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183563: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183617: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183639: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183661: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183700: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183739: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183756: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183775: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183795: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183817: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183834: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183853: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183870: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183890: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183907: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183927: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183945: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183965: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183982: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.184002: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.184021: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.184039: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.184058: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.184077: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.185918: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 118 operators, 397 arrays (0 quantized)
2020-04-01 07:24:52.188931: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 118 operators, 397 arrays (0 quantized)
2020-04-01 07:24:52.201162: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 90 operators, 396 arrays (0 quantized)
2020-04-01 07:24:52.205488: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 89 operators, 395 arrays (0 quantized)
2020-04-01 07:24:52.208850: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 88 operators, 393 arrays (0 quantized)
2020-04-01 07:24:52.212215: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 88 operators, 393 arrays (0 quantized)
2020-04-01 07:24:52.215005: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 88 operators, 393 arrays (0 quantized)
2020-04-01 07:24:52.220046: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 4513344 bytes, theoretical optimal value: 4513344 bytes.
2020-04-01 07:24:52.220849: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PAD, RELU6, SOFTMAX. Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.
Traceback (most recent call last):
  File ""/home/vectorweb4/.local/bin/toco_from_protos"", line 11, in <module>
    sys.exit(main())
  File ""/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/vectorweb4/.local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/vectorweb4/.local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PAD, RELU6, SOFTMAX. Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.

"
38114,"[BUG] map method of tf.data.Dataset has a bug, TensorFlow version = 2.1.0","One example of map method in the following official website said that map_func get same shape and dtype as 'tf.Tensor', however it's NOT
https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map
```
dataset = Dataset.range(5) 
# `map_func` takes a single argument of type `tf.Tensor` with the same 
# shape and dtype. 
result = dataset.map(lambda x: x + 1) 
```

According to the official example, I think `item` in `_func` of the following code should be an EagetTensor but it turns out to be a Tensor instead.
```
import tensorflow as tf

def _func(item):
    # I expect an EagerTensor bug get a Tensor here
    print(type(item)) # ==> <class 'tensorflow.python.framework.ops.Tensor'>
    return item

tensor = tf.convert_to_tensor(['hello', 'world'])
print(type(tensor)) # ==> <class 'tensorflow.python.framework.ops.EagerTensor'>
dataset = tf.data.Dataset.from_tensor_slices(tensor)
dataset.map(_func)
```
I want to use .numpy() to convert an EagerTensor to numpy arrays and then make some operations using numpy, but very suprisingly I got a Tensor in `_func` and sadly I don't know how to make it for Tensor"
38113,Custom Implementation for FusedBatchNormV3,"**System information**
- Linux Ubuntu 18.04:
- TensorFlow installed from (source ):
- TensorFlow version 2.0


**Provide the text output from tflite_convert**

```
---------------------------------------------------------------------------
ConverterError: See console for info.
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-04-01 07:24:52.175680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183563: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183617: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183639: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183661: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183700: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183739: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183756: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183775: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183795: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183817: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183834: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183853: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183870: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183890: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183907: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183927: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183945: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183965: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.183982: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.184002: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.184021: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.184039: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.184058: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.184077: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3
2020-04-01 07:24:52.185918: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 118 operators, 397 arrays (0 quantized)
2020-04-01 07:24:52.188931: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 118 operators, 397 arrays (0 quantized)
2020-04-01 07:24:52.201162: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 90 operators, 396 arrays (0 quantized)
2020-04-01 07:24:52.205488: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 89 operators, 395 arrays (0 quantized)
2020-04-01 07:24:52.208850: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 88 operators, 393 arrays (0 quantized)
2020-04-01 07:24:52.212215: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 88 operators, 393 arrays (0 quantized)
2020-04-01 07:24:52.215005: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 88 operators, 393 arrays (0 quantized)
2020-04-01 07:24:52.220046: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 4513344 bytes, theoretical optimal value: 4513344 bytes.
2020-04-01 07:24:52.220849: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PAD, RELU6, SOFTMAX. Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.
Traceback (most recent call last):
  File ""/home/vectorweb4/.local/bin/toco_from_protos"", line 11, in <module>
    sys.exit(main())
  File ""/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/vectorweb4/.local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/vectorweb4/.local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PAD, RELU6, SOFTMAX. Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.

```

**Standalone code to reproduce the issue** 
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter = True
tflite_model = converter.convert()

**Any other info / logs**

---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
<ipython-input-31-e3b422911500> in <module>
----> 1 tflite_model = converter.convert()
      2 # open(""converted_model.tflite"", ""wb"").write(tflite_model)
      3 # interpreter = tf.lite.Interpreter(model_path=""converted_model.tflite"")
      4 # interpreter.allocate_tensors()

~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py in convert(self)
    444         input_tensors=input_tensors,
    445         output_tensors=output_tensors,
--> 446         **converter_kwargs)
    447 
    448     if self._is_calibration_quantize():

~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)
    447       input_data.SerializeToString(),
    448       debug_info_str=debug_info_str,
--> 449       enable_mlir_converter=enable_mlir_converter)
    450   return data
    451 

~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198       stdout = _try_convert_to_unicode(stdout)
    199       stderr = _try_convert_to_unicode(stderr)
--> 200       raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
    201   finally:
    202     # Must manually cleanup files.
"
38110,Semantic search BERT model to TFLITE,"0


I have this code for semantic search engine built using the pre-trained bert model. I want to convert this model into tflite for deploying it to google mlkit. I want to know how to convert it. I want to know if its even possible to convert this into tflite. It might be because its mentioned on the official tensorflow site :https://www.tensorflow.org/lite/convert. But I dont know where to begin

code:
```python
from sentence_transformers import SentenceTransformer

# Load the BERT model. Various models trained on Natural Language Inference (NLI) https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md and 
# Semantic Textual Similarity are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md

model = SentenceTransformer('bert-base-nli-mean-tokens')

# A corpus is a list with documents split by sentences.

sentences = ['Absence of sanity', 
             'Lack of saneness',
             'A man is eating food.',
             'A man is eating a piece of bread.',
             'The girl is carrying a baby.',
             'A man is riding a horse.',
             'A woman is playing violin.',
             'Two men pushed carts through the woods.',
             'A man is riding a white horse on an enclosed ground.',
             'A monkey is playing drums.',
             'A cheetah is running behind its prey.']

# Each sentence is encoded as a 1-D vector with 78 columns
sentence_embeddings = model.encode(sentences)

print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))

print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])

#@title Sematic Search Form

# code adapted from https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py

query = 'Nobody has sane thoughts' #@param {type: 'string'}

queries = [query]
query_embeddings = model.encode(queries)

# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity
number_top_matches = 3 #@param {type: ""number""}

print(""Semantic Search Results"")

for query, query_embedding in zip(queries, query_embeddings):
    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, ""cosine"")[0]

    results = zip(range(len(distances)), distances)
    results = sorted(results, key=lambda x: x[1])

    print(""\n\n======================\n\n"")
    print(""Query:"", query)
    print(""\nTop 5 most similar sentences in corpus:"")

    for idx, distance in results[0:number_top_matches]:
        print(sentences[idx].strip(), ""(Cosine Score: %.4f)"" % (1-distance))
```"
38108,(SOLVE) while convert SavedModel to tflite: ValueError: as_list() is not defined on an unknown TensorShape.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.1.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
tflite_convert --output_file=model.tflite --saved_model_dir . --input_arrays=serving_default_input_1 --input_shapes=1,800,800,3 --output_arrays=StatefulPartitionedCall_1
```

**The output from the converter invocation**

```
2020-04-01 15:38:11.537934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-01 15:38:11.540538: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
2020-04-01 15:38:12.243351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-04-01 15:38:12.251378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:12.251935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s
2020-04-01 15:38:12.252000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:12.252535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s
2020-04-01 15:38:12.252651: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-04-01 15:38:12.252693: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:12.252731: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:12.252767: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:12.252806: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:12.252842: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:12.252869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-01 15:38:12.252875: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-04-01 15:38:12.253276: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-01 15:38:12.259296: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3191935000 Hz
2020-04-01 15:38:12.259632: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563e568f9430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-01 15:38:12.259645: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-04-01 15:38:12.402933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:12.406735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:12.407499: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563e5698fad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-04-01 15:38:12.407515: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-04-01 15:38:12.407520: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-04-01 15:38:12.407689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-01 15:38:12.407697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      
2020-04-01 15:38:13.801484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:13.802395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:13.803088: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2
2020-04-01 15:38:13.803208: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-04-01 15:38:13.804559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:13.805223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s
2020-04-01 15:38:13.805314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:13.805958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s
2020-04-01 15:38:13.806094: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-04-01 15:38:13.806151: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:13.806196: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:13.806240: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:13.806283: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:13.806326: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:13.806339: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-01 15:38:13.806346: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-04-01 15:38:13.806423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-01 15:38:13.806432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 
2020-04-01 15:38:13.806440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N 
2020-04-01 15:38:13.806445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N 
2020-04-01 15:38:13.922212: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-04-01 15:38:13.922539: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 1542 nodes (1413), 2635 edges (2506), time = 71.453ms.
2020-04-01 15:38:13.922555: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 1.68ms.
2020-04-01 15:38:15.070366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:15.071111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:15.071664: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2
2020-04-01 15:38:15.071742: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-04-01 15:38:15.072130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:15.072644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s
2020-04-01 15:38:15.072689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-01 15:38:15.073234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s
2020-04-01 15:38:15.073368: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-04-01 15:38:15.073410: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:15.073445: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:15.073480: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:15.073520: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:15.073552: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-04-01 15:38:15.073562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-01 15:38:15.073568: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-04-01 15:38:15.073658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-01 15:38:15.073664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 
2020-04-01 15:38:15.073668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N 
2020-04-01 15:38:15.073671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N 
2020-04-01 15:38:15.346279: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-04-01 15:38:15.346362: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1300 nodes (-204), 2189 edges (-408), time = 140.857ms.
2020-04-01 15:38:15.346373: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1300 nodes (0), 2189 edges (0), time = 89.953ms.
Traceback (most recent call last):
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 594, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 577, in run_main
    _convert_tf2_model(tflite_flags)
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 235, in _convert_tf2_model
    tflite_model = converter.convert()
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 442, in convert
    shape_list = tensor.shape.as_list()
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 1166, in as_list
    raise ValueError(""as_list() is not defined on an unknown TensorShape."")
ValueError: as_list() is not defined on an unknown TensorShape.

```

**Also, please include a link to the saved model or GraphDef**

```
[model.zip](https://github.com/tensorflow/tensorflow/files/4413909/model.zip)
it's small and about 1.7MB
```

**Failure details**
can not convert to tflite
```
2020-04-01 16:44:20.851662: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-04-01 16:44:20.851708: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1300 nodes (-204), 2189 edges (-408), time = 106.827ms.
2020-04-01 16:44:20.851715: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1300 nodes (0), 2189 edges (0), time = 61.656ms.
Traceback (most recent call last):
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 594, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 577, in run_main
    _convert_tf2_model(tflite_flags)
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 235, in _convert_tf2_model
    tflite_model = converter.convert()
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 442, in convert
    shape_list = tensor.shape.as_list()
  File ""/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 1166, in as_list
    raise ValueError(""as_list() is not defined on an unknown TensorShape."")
ValueError: as_list() is not defined on an unknown TensorShape.
```


**Any other info / logs**

the model was train with tensorflow 2.0.0a0, but I convert it under tensorflow 2.1.0
"
38107, https://www.tensorflow.org/install/errors,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
38106,implicit conversion from 'int' to 'float' changes value from INT_MAX to INT_MAX+1,"Clang warns

```
tensorflow/lite/micro/micro_utils.cc:108:19: warning: implicit conversion from 'int' to 'float' changes value from 2147483647 to 2147483648 [-Wimplicit-int-float-conversion]
  if (quantized > INT_MAX) {
                ~ ^~~~~~~
```
about this code
https://github.com/tensorflow/tensorflow/blob/bda5ba03da606b0d41fa3282de0108c3e98cff8e/tensorflow/lite/micro/micro_utils.cc#L106-L115

So, if `round(value/scale)` ever happens to be INT_MAX+1, it is not saturated as intended, instead has undefined wrap-around behavior (overflow is undefined for signed int,)."
38105,Information about Keras Tuner is missing in tensorflow.org website,"## URL(s) with the issue: 
https://youtu.be/aNrqaOAt5P4?list=PLQY2H8rRoyvzuJw20FG82Lgm2SZjTdIXU&t=660

## Description of issue (what needs changing): 
In the [TF Dev Summit 2020](https://youtu.be/aNrqaOAt5P4?list=PLQY2H8rRoyvzuJw20FG82Lgm2SZjTdIXU&t=660
), Paige Bailey has talked about **Keras Tuner** and has shown its implementation. I liked the functionality but I couldn't information/documentation about it in [tensorflow.org site](https://www.tensorflow.org/).

### Clear description: 
This being a New Functionality, the documentation about that functionality in the Website would help the Community.

### Correct links

Is the link to the source code correct? : N/A

### Parameters defined

Are all parameters defined and formatted correctly?: N/A

### Returns defined

Are return values defined? : N/A

### Raises listed and defined

Are the errors defined? : N/A

### Usage example

Is there a usage example? N/A"
42797, padded_batch() missing 1 required positional argument: 'padded_shapes' in line train_data = train_data.padded_batch(BATCH_SIZE), padded_batch() missing 1 required positional argument: 'padded_shapes' in line train_data = train_data.padded_batch(BATCH_SIZE)
38104,post_training_quant does not change weights from fp32 to int8,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
NO
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): 
ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
conda  tensorflow version: tested on tf1.15 and 2.2.0-dev20200325.
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

**Describe the current behavior**
i follow link:<https://www.tensorflow.org/lite/performance/post_training_quant> ,use code in the doc,
when i use [netron](https://github.com/lutzroeder/netron) open mnist_model_quant.tflite,found that weights/bias of layer conv2d are still float32.
![image](https://user-images.githubusercontent.com/6041743/78113170-e99fb980-7431-11ea-8e3b-8cccfec1ec57.png)


**Describe the expected behavior**
after post-training quantize, weights of conv2d should be int8 rather than float32



"
38103,custom training logic in subclassing model not saved,"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): Google Colab (Windows/Linux as well)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): TensorFlow 2.2.0RC2
- Python version: 3.7

**Describe the current behavior**
Hi all,
I'm using the 2.2 new feature: custom training logic with Model.fit by overriding Model.train_step. It's working well so far. I have the following issue (maybe it's not a feature, you'll tell me):
When I save my model (Model.save) with the custom training logic and then I want to load it, the custom training loop is not saved and when I apply a Model.fit for my loaded model, it comes back to the default one.

**Describe the expected behavior**
I'd expect to save as well the model.train_step (and also test_step and predict_step).

**Standalone code to reproduce the issue** 
[Gist](https://colab.research.google.com/gist/quetil/5e92fb9af7cc959f8fe62f1090ef31e7/custom-train_step-in-subclassing-model.ipynb)

**Others**
Can we expect in the future that the whole class will be saved? (Even variables declared in the `__init__` for instance).

Thank you in advance :-)
"
38102,"tensorflow1.15,  Here is a list of operators for which you will need custom implementations: ReorderAxes.RandomStandardNormal","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04)  : ubuntu18.04
- TensorFlow installed from (source or binary):   conda
- TensorFlow version (or github SHA if from source): 1.15


**Provide the text output from tflite_convert**

```
# Copy and paste here

```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
38101,How to build op in tensorflow's static graph for unknown rank tensor,"## Problem 1
```python
tensor a = [1,2,3,4]
```
I want to split it into:
```python
b1 = [1] 
b2 = [2] 
b3 = [3] 
b4 = [4]
```
However, if the tensor's shape is None, such as
```python
a = tf.placeholder(tf.float32, [1, None], ""a"")
```
How to build this **op** in tensorflow's static graph?

## Problem 2
In addition，
```python
tensor x = [1,2,3,4]
```
I want to build a series tensor based on `tensor x`:
```python
x1 = [1]
x2 = [1,1]
x3 = [1,1,1]
x4 = [1,1,1,1]
```
Just the same as the Problem 1, if the tensor's shape is None, how to build this *op* in tensorflow's static graph?
"
38100,"tensorflow training stuck after ""Successfully opened dynamic library libcublas.so.10.0""","I'm using TF 1.13 with cuda 10. I'm training a model but the TF code freezes after printing `Successfully opened dynamic library libcublas.so.10.0`. The GPU memory was all occupied and the CPU was also fully occupied meaning that it was not a deadlock or something (my problem I think is different than issue #32017).
The training script:

```
CUDA_AVAILABLE_DEVICES=0,1,2,3 t2t-trainer \
  --data_dir=~/t2t_data \
  --output_dir=train_outputs/div2k \
  --problem=img2img_div2k \
  --model=img2img_transformer \
  --hparams_set=img2img_transformer \
  --train_steps=2000000 \
  --eval_steps=5000 --local_eval_frequency=5000 --worker_gpu=4
```

Output:
```
INFO:tensorflow:Building model body
INFO:tensorflow:Transforming body output with identity_modality.top
INFO:tensorflow:Transforming feature 'inputs' with identity_modality.bottom
INFO:tensorflow:Transforming feature 'targets' with identity_modality.targets_bottom
INFO:tensorflow:Building model body
INFO:tensorflow:Transforming body output with identity_modality.top
INFO:tensorflow:Base learning rate: 0.200000
INFO:tensorflow:Trainable Variables Total size: 47118592
INFO:tensorflow:Non-trainable variables Total size: 5
INFO:tensorflow:Using optimizer Adam
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
^[[AINFO:tensorflow:Graph was finalized.
2020-04-01 05:48:23.753790: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-01 05:48:25.319599: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x13dc3370 executing computations on platform CUDA. Devices:
2020-04-01 05:48:25.319671: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla P40, Compute Capability 6.1
2020-04-01 05:48:25.319694: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): Tesla P40, Compute Capability 6.1
2020-04-01 05:48:25.319713: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (2): Tesla P40, Compute Capability 6.1
2020-04-01 05:48:25.319732: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (3): Tesla P40, Compute Capability 6.1
2020-04-01 05:48:25.325520: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593990000 Hz
2020-04-01 05:48:25.330211: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x197d480 executing computations on platform Host. Devices:
2020-04-01 05:48:25.330255: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2020-04-01 05:48:25.330426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0b1c:00:00.0
totalMemory: 22.38GiB freeMemory: 22.23GiB
2020-04-01 05:48:25.330494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties:
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 246c:00:00.0
totalMemory: 22.38GiB freeMemory: 22.23GiB
2020-04-01 05:48:25.330551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 2 with properties:
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 4de1:00:00.0
totalMemory: 22.38GiB freeMemory: 22.23GiB
2020-04-01 05:48:25.330608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 3 with properties:
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 61fa:00:00.0
totalMemory: 22.38GiB freeMemory: 22.23GiB
2020-04-01 05:48:25.330925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1, 2, 3
2020-04-01 05:48:25.337584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-01 05:48:25.337621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 2 3
2020-04-01 05:48:25.337644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N N N N
2020-04-01 05:48:25.337663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   N N N N
2020-04-01 05:48:25.337681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 2:   N N N N
2020-04-01 05:48:25.337700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 3:   N N N N
2020-04-01 05:48:25.337915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21773 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0b1c:00:00.0, compute capability: 6.1)
2020-04-01 05:48:25.338239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 21773 MB memory) -> physical GPU (device: 1, name: Tesla P40, pci bus id: 246c:00:00.0, compute capability: 6.1)
2020-04-01 05:48:25.338604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 21773 MB memory) -> physical GPU (device: 2, name: Tesla P40, pci bus id: 4de1:00:00.0, compute capability: 6.1)
2020-04-01 05:48:25.339518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 21773 MB memory) -> physical GPU (device: 3, name: Tesla P40, pci bus id: 61fa:00:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into train_outputs/model.ckpt.
2020-04-01 05:50:09.675984: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally

```
I enabled `TF_CPP_MIN_VLOG_LEVEL=2` and here is some part of the log which is recurring but with different values everytime:

```
2020-04-01 05:30:33.932882: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 8 step -361974947767986987 {{node ToInt64}} = Cast[DstT=DT_INT64, SrcT=DT_FLOAT, Truncate=false, _device=""/device:CPU:0""](resize/Squeeze) device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933436: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -882416251439127608 kernel_name: ""resize/Squeeze"" tensor { dtype: DT_FLOAT shape { dim {
size: 8 } dim { size: 8 } dim { size: 3 } } allocation_description { requested_bytes: 768 allocator_name: ""cpu"" ptr: 140093260055424 } } }
2020-04-01 05:30:33.933454: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
2020-04-01 05:30:33.931442: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 4 step -2826364024402348450 {{node ParseSingleExample/Reshape}} = Const[dtype=DT_INT64, value=Tensor<type:
int64 shape: [1] values: 0>, _device=""/device:CPU:0""]() device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.931993: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 12 step -9020318172928929357 {{node LogicalOr}} = LogicalOr[_device=""/device:CPU:0""](Equal, Equal_1) device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.932017: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -7271963003795482065 kernel_name: ""TensorDataset"" tensor { dtype: DT_VARIANT shape { } al
location_description { requested_bytes: 8 allocator_name: ""cpu"" ptr: 140092371056192 } } }
2020-04-01 05:30:33.933553: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -9020318172928929357 kernel_name: ""LogicalOr"" tensor { dtype: DT_BOOL shape { } allocation_description { requested_bytes: 1 allocator_name: ""cpu"" ptr: 140093042046528 } } }
2020-04-01 05:30:33.931544: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 7 step -1925456608919473175 {{node resize/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[0], _device=""/device:CPU:0""](resize/ResizeArea) device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933059: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
2020-04-01 05:30:33.933089: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
2020-04-01 05:30:33.933094: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 10 step -7022465578876499613 {{node Equal}} = Equal[T=DT_STRING, _device=""/device:CPU:0""](ParseSingleExample/ParseSingleExample:2, Equal/y) device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933676: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -3410101109738072632 kernel_name: ""ParseSingleExample/ParseSingleExample"" tensor { dtype: DT_INT64 shape { dim { size: 1 } } allocation_description { requested_bytes: 8 allocator_name: ""cpu"" ptr: 140092035786688 } } }
2020-04-01 05:30:33.932257: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: -4867250882158238528 kernel_name: ""TensorDataset"" tensor { dtype: DT_VARIANT shape {
} allocation_description { requested_bytes: 8 allocator_name: ""cpu"" ptr: 140092975893504 } } }
2020-04-01 05:30:33.933150: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 4 step -8238964452958482503 {{node ParseSingleExample/Reshape}} = Const[dtype=DT_INT64, value=Tensor<type:
int64 shape: [1] values: 0>, _device=""/device:CPU:0""]() device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933757: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -4867250882158238528 kernel_name: ""TensorDataset"" tensor { dtype: DT_VARIANT shape { } allocation_description { requested_bytes: 8 allocator_name: ""cpu"" ptr: 140092975893504 } } }
2020-04-01 05:30:33.933777: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 5 step -8238964452958482503 {{node ParseSingleExample/Const_1}} = Const[dtype=DT_STRING, value=Tensor<type: string shape: [0] values: >, _device=""/device:CPU:0""]() device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933793: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 9 step -4867250882158238528 {{node TensorDataset}} = TensorDataset[Toutput_types=[DT_UINT8, DT_INT64], outp
ut_shapes=[[?,?,3], [8,8,3]], _device=""/device:CPU:0""](arg0, ToInt64) device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933814: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -8238964452958482503 kernel_name: ""ParseSingleExample/Const_1"" tensor { dtype: DT_STRING
shape { dim { } } } }
2020-04-01 05:30:33.932821: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
2020-04-01 05:30:33.933305: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 8 step -5799046217055003931 {{node ToInt64}} = Cast[DstT=DT_INT64, SrcT=DT_FLOAT, Truncate=false, _device=""/device:CPU
:0""](resize/Squeeze) device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933380: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 6 step -200002097097137980 {{node resize/ResizeArea}} = ResizeArea[T=DT_UINT8, align_corners=false, _device
=""/device:CPU:0""](resize/ExpandDims, resize/size) device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933917: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: -5799046217055003931 kernel_name: ""ToInt64"" tensor { dtype: DT_INT64 shape { dim { si
ze: 8 } dim { size: 8 } dim { size: 3 } } allocation_description { requested_bytes: 1536 allocator_name: ""cpu"" ptr: 140093916398656 } } }
2020-04-01 05:30:33.933939: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 7 step -200002097097137980 {{node resize/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[0], _device=""/device:CPU:0""](re
size/ResizeArea) device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933954: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -5799046217055003931 kernel_name: ""ToInt64"" tensor { dtype: DT_INT64 shape { dim { size:
8 } dim { size: 8 } dim { size: 3 } } allocation_description { requested_bytes: 1536 allocator_name: ""cpu"" ptr: 140093916398656 } } }
2020-04-01 05:30:33.933479: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 7 step -882416251439127608 {{node resize/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[0], _device=""/device:CPU:0""](resize/ResizeArea) device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933489: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 9 step -361974947767986987 {{node TensorDataset}} = TensorDataset[Toutput_types=[DT_UINT8, DT_INT64], output_shapes=[[
?,?,3], [8,8,3]], _device=""/device:CPU:0""](arg0, ToInt64) device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.933519: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 5 step -2826364024402348450 {{node ParseSingleExample/Const_1}} = Const[dtype=DT_STRING, value=Tensor<type: string sha
pe: [0] values: >, _device=""/device:CPU:0""]() device: /job:localhost/replica:0/task:0/device:CPU:0
2020-04-01 05:30:33.934058: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: -361974947767986987 kernel_name: ""TensorDataset"" tensor { dtype: DT_VARIANT shape { } allocation_description { requested_bytes: 8 allocator_name: ""cpu"" ptr: 140092242034688 } } }
2020-04-01 05:30:33.934073: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -2826364024402348450 kernel_name: ""ParseSingleExample/Const_1"" tensor { dtype: DT_STRING
shape { dim { } } } }
```
I literally have no idea what the problem is. I tried with another dataset and the model started training but with this particular datasets the model is constantly in executor.cc:1161 and executor.cc:1804. Is it doing computations on CPU rather than GPU maybe? but the GPU memory is all occupied and the TF has recognized the GPU devices. 

**P.S**: I also added a signal handler to my python code to print the stack trace upon receiving a specific signal (to see where in the python code it is spending the time), but after a certain point, i guess the signal handlers are overridden as the code does not respond to my manual signal. It does no longer even respond to SIGINT as well.

any idea?"
38099,Use only tensorflow-lite-gpu aar file,"
I am trying to build an image recognition app and have a ML model ready which I have converted to .tflite type for using. I have used tflite and tflite-gpu aar files in my project for performing inference with the model and deployed it on my devices and it works fine. But I am concerned about the apk size after including these two aar. Is it possible to use just tflite-gpu aar file for my app? Or any other work around ?"
38098,"when i use tf.contrib.lite.Interpreter(model_path=""xxxx.lite"") ","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
tf.contrib.lite.Interpreter(model_path=""XXXX.lite"")
interpreter.allocate_tensors()
```

**The output from the converter invocation**

```
return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_InputIndices(self)
ValueError: Interpreter was not initialized.
```



**Failure details**
when i use ```converter.convert()``` without  ```converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] ``` it works. But i use converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE],it appears the error.
this model is from torch‘s to tensorflow.May be related to this

"
38097,How to create a custom distribution strategy on Tensorflow,"I know this question should be asked on Stackoverflow. But it seems that no one has the courage to answer [this](https://stackoverflow.com/questions/57852705/how-to-create-a-custom-distribution-strategy-on-tensorflow) question for half an year. So I'm trying some luck on the github, hoping there are someone who has the expertise that could give some instructions on how to realize the model parallelism strategy, specifically the communication part among different jobs?"
38096,tensorflow::mutex occasionally fails on aarch64 platform,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): **`No`**
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): **`Linux Ubuntu 18.04`**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: **`NA`**
- TensorFlow installed from (source or
binary):**`source`** 
- TensorFlow version (use command below): **`v1.15.0`**
- Python version: **`3.7.5`**
- Bazel version (if compiling from source):**`0.24.1`**
- GCC/Compiler version (if compiling from
source): **`7.5.0`**
- CUDA/cuDNN version:**`NA`** 
- GPU model and memory:**`NA`**

**Describe the current behavior**

I ran tensorflow on aarch64 platform, using ParallelMapDataset to preprocess the data, but after running normally for some time (about a few hours), the tensorflow process core dumps, each time with a different stack(here are two of them):
```
#0  __GI_raise (sig=11) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  <signal handler called>
#2  0x0000ffff8f3fd024 in nsync::nsync_dll_splice_after_(nsync::nsync_dll_element_s_*, nsync::nsync_dll_element_s_*) ()
   from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#3  0x0000ffff8f3fd05c in nsync::nsync_dll_make_first_in_list_(nsync::nsync_dll_element_s_*, nsync::nsync_dll_element_s_*) ()
   from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#4  0x0000ffff8f3fd09c in nsync::nsync_dll_make_last_in_list_(nsync::nsync_dll_element_s_*, nsync::nsync_dll_element_s_*) ()
   from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#5  0x0000ffff8f3fd25c in nsync::nsync_mu_lock_slow_(nsync::nsync_mu_s_*, nsync::waiter*, unsigned int, nsync::lock_type_s*) ()
   from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#6  0x0000ffff8f3fd38c in nsync::nsync_mu_lock(nsync::nsync_mu_s_*) () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#7  0x0000ffff86ff032c in tensorflow::CancellationManager::StartCancel() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
#8  0x0000ffff86ff05d4 in tensorflow::CancellationManager::StartCancel() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
#9  0x0000ffff86ff0a4c in tensorflow::CancellationManager::~CancellationManager() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
#10 0x0000ffff8bded294 in std::_Sp_counted_ptr_inplace<tensorflow::data::IteratorResource::State, std::allocator<tensorflow::data::IteratorResource::State>, (__gnu_cxx::_Lock_policy)2>::_M_dispose() ()
   from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#11 0x0000ffff8994b43c in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#12 0x0000ffff8bdf0f78 in tensorflow::data::IteratorResource::~IteratorResource() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#13 0x0000ffff870717ec in tensorflow::ResourceMgr::Clear() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
#14 0x0000ffff8727ae24 in tensorflow::DeviceMgr::~DeviceMgr() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
#15 0x0000ffff8b9d13a0 in tensorflow::DirectSession::~DirectSession() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#16 0x0000ffff8b9d1484 in tensorflow::DirectSession::~DirectSession() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#17 0x0000ffff89991e90 in tensorflow::SessionRef::Close() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#18 0x0000ffff89f24218 in TF_CloseSession () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
```
```
[10:02:20]#0  0x0000ffffb7c8aad0 in raise () from /lib64/libpthread.so.0
[10:02:20]#1  <signal handler called>
[10:02:20]#2  0x0000ffffb7a12140 in raise () from /lib64/libc.so.6
[10:02:20]#3  0x0000ffffb7a134ec in abort () from /lib64/libc.so.6
[10:02:20]#4  0x0000ffff9dfa21d0 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6
[10:02:20]#5  0x0000ffff9df9fae4 in ?? () from /lib64/libstdc++.so.6
[10:02:20]#6  0x0000ffff9df9fb30 in std::terminate() () from /lib64/libstdc++.so.6
[10:02:20]#7  0x0000ffff9df9fdec in __cxa_throw () from /lib64/libstdc++.so.6
[10:02:20]#8  0x0000ffff9dfcac9c in std::__throw_bad_function_call() () from /lib64/libstdc++.so.6
[10:02:20]#9  0x0000ffff9efd9a00 in tensorflow::UnboundedWorkQueue::PooledThreadFunc() ()
[10:02:20]   from /usr/local/python3.7/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
[10:02:20]#10 0x0000ffff9dfcd144 in ?? () from /lib64/libstdc++.so.6
[10:02:20]#11 0x0000ffffb7c7f8bc in start_thread () from /lib64/libpthread.so.0
[10:02:22]--Type <RET> for more, q to quit, c to continue without paging--
[10:02:22]#12 0x0000ffffb7ab473c in thread_start () from /lib64/libc.so.6
```
The first stack tells me that when the session exits, an invalid callback remains in a CancellationManager(use-after-free).The second stack indicates that the worker thread of the thread pool has received a corrupted task.

After analyzing the code, I thought these were all illogical exceptions, so I tested the tensorflow interface(here are two of them):
```C
#include ""tensorflow/core/framework/cancellation.h""
#include <thread>

using tensorflow::CancellationManager;
using std::thread;

void RegAndUnreg(CancellationManager& manager) {
    for (int i = 0; i < 10000000; i++) {
        auto token = manager.get_cancellation_token();
        manager.RegisterCallback(token, nullptr);
        manager.DeregisterCallback(token);
    }
}

int main() {
    int i = 0;
    while (true) {
        i++;
        CancellationManager manager;
        thread t1([&manager]() {RegAndUnreg(manager);});
        thread t2([&manager]() {RegAndUnreg(manager);});
        thread t3([&manager]() {RegAndUnreg(manager);});
        thread t4([&manager]() {RegAndUnreg(manager);});
        thread t5([&manager]() {RegAndUnreg(manager);});
        thread t6([&manager]() {RegAndUnreg(manager);});
        thread t7([&manager]() {RegAndUnreg(manager);});
        thread t8([&manager]() {RegAndUnreg(manager);});
        t1.join();
        t2.join();
        t3.join();
        t4.join();
        t5.join();
        t6.join();
        t7.join();
        t8.join();
        // I've set the callbacks_ to the public property for easy viewing
        if (manager.callbacks_.size() > 0) {
            LOG(FATAL) << ""Round "" << i << ""["" << manager.callbacks_.size() << ""] callbacks left."" << std::endl;
        }
    }
}
```
The above program soon had a core dump, sometimes because of a residual callback(I think) and sometimes because of a Check failure (token>next_cancellation_token_).

```C
#include ""tensorflow/core/platform/default/unbounded_work_queue.h""
#include ""tensorflow/core/platform/env.h""

#include <thread>
#include <functional>

using tensorflow::UnboundedWorkQueue;
using std::thread;
using WorkFunction = std::function<void()>;

const static WorkFunction work = [](){};

void ScheduleTask(UnboundedWorkQueue& manager) {
    for (int i = 0; i < 10000000; i++) {
        manager.Schedule(work);
    }
}

int main() {
    int i = 0;
    while (true) {
        i++;
        UnboundedWorkQueue manager(tensorflow::Env::Default(), ""bugfix"");
        thread t1([&manager]() {ScheduleTask(manager);});
        thread t2([&manager]() {ScheduleTask(manager);});
        thread t3([&manager]() {ScheduleTask(manager);});
        thread t4([&manager]() {ScheduleTask(manager);});
        thread t5([&manager]() {ScheduleTask(manager);});
        thread t6([&manager]() {ScheduleTask(manager);});
        thread t7([&manager]() {ScheduleTask(manager);});
        thread t8([&manager]() {ScheduleTask(manager);});

        t1.join();
        t2.join();
        t3.join();
        t4.join();
        t5.join();
        t6.join();
        t7.join();
        t8.join();
    }
}

```
The above program soon had a core dump, and the stack showed with successive stack_check_fail, consistent with the second core dump stack above
The exception **`disappeared`** when I replaced the locks held in the CancellationManager and UnboundedWorkQueue from tensorflow::mutex to STD ::mutex.However, I am sure that core dump can occur in other places, and it is difficult to directly replace all tensorflow::mutex, because tensorflow::mutex has some custom methods and members.

**Describe the expected behavior**
These exceptions prevented me from deploying tensorflow on my server，does the community expect tensorflow to be deployed on aarch64 servers?
After my test, it is true that tensorflow::mutex is faster than STD ::mutex, but sadly, it does not work properly on aarch64, which is deeply troubling me. Can you give me some Suggestions?Or does the community plan to offer some options for locks?I see the current implementation of mutex in platform/default, perhaps with platform/aarch64?

**Standalone code to reproduce the issue** 
It is not easy to reproduce in a production environment, such as the test case above

**Other info / logs** Include any logs or source code that would be helpful to
Refer to the core dump stack information above
"
38091,unclear explanation in the Text classification with TensorFlow Hub: Movie reviews example,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/keras/text_classification_with_hub#build_the_model

Please provide a link to the documentation entry, for example:

https://www.tensorflow.org/tutorials/keras/text_classification_with_hub#build_the_model

## Description of issue (what needs changing):

The last layer in the model is `model.add(tf.keras.layers.Dense(1))`. However, in the following description, it says `The last layer is densely connected with a single output node. Using the sigmoid activation function, ...`

I check the api doc and find that the default activation is none for dense layer.

- Without `activation='sigmoid'`, the predictions are not in the range of (0, 1) as shown below, which is not interpretable.

```
pred = model.predict(test_data.batch(512))
print(pred)

[[-0.29496038]
 [ 1.2088487 ]
 [ 0.11580676]
 ...
 [-1.610341  ]
 [-0.8496179 ]
 [ 1.3117154 ]]
```

So shall the example code be `model.add(tf.keras.layers.Dense(1, activation='sigmoid'))`?"
38090,GPU not found on Windows 10 with tf 2.1.0,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code: No
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: pip
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.7 
- CUDA/cuDNN version: 10.2.89
- GPU model and memory: Nvidia GTX 1650

**Describe the current behavior**

When I run 

`tf.config.list_physical_devices(None)`

I get

`[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]`

**Describe the expected behavior**

`tf.config.list_physical_devices(None)` should report a GPU device

**Other info / logs**

Happy to provide more info, but I'm not sure what's relevant."
38087,ImportError: DLL load failed: The specified module could not be found.,"import tensorflow as tf
Error in callback <bound method AutoreloadMagics.post_execute_hook of <autoreload.AutoreloadMagics object at 0x00000251F0EB3708>> (for post_execute):
Traceback (most recent call last):

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""C:\Users\KIIT\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)

  File ""C:\Users\KIIT\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)

ImportError: DLL load failed: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\IPython\extensions\autoreload.py"", line 538, in post_execute_hook
    _, pymtime = self._reloader.filename_and_mtime(sys.modules[modname])

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\IPython\extensions\autoreload.py"", line 184, in filename_and_mtime
    if not hasattr(module, '__file__') or module.__file__ is None:

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)

  File ""C:\Users\KIIT\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import

  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load

  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked

  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed

  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import

  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load

  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked

  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked

  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module

  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)

  File ""C:\Users\KIIT\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KIIT\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KIIT\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KIIT\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime."
38086,Tensorboard does not show the curves,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

Hello, 
I am using gcloud ai-platform jobs submit training to submit my training. parameters are:
```
gcloud ai-platform jobs submit training $JOB_NAME \
        --stream-logs \
        --runtime-version=2.1 \
        --job-dir=$JOB_DIR \
        --package-path=trainer \
        --module-name=trainer.task \
        --region=us-east1 \
        --python-version=3.5 \
        --scale-tier=basic
```

I am using tensorflow keras to build the model and compile it. adding the following metrics:
```
METRICS = [keras.metrics.TruePositives(name='tp'), \
           keras.metrics.FalsePositives(name='fp'), \
           keras.metrics.TrueNegatives(name='tn'), \
           keras.metrics.FalseNegatives(name='fn'), \
           keras.metrics.BinaryAccuracy(name='accuracy'), \
           keras.metrics.Precision(name='precision'), \
           keras.metrics.Recall(name='recall'), \
           keras.metrics.AUC(name='auc'), \
          ]
# this is the call in the task.py (ai-platform format)
executed_model = model.make_model(input_shape,METRICS,model_choice = 5)

The following code adds the tensorboard callbacks and LambdaCallbacks:
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, \
                                                        histogram_freq=1, \
                                                        write_graph=True, \
                                                        write_images=True, \
                                                        write_grads = True, \
                                                        update_freq='batch')
  # writer for each one of the CM
  file_writer_cm_train = tf.summary.create_file_writer(cm_dir_train)
  file_writer_cm_valid = tf.summary.create_file_writer(cm_dir_valid)
  file_writer_cm_test = tf.summary.create_file_writer(cm_dir_test)

def log_confusion_matrix_train(epoch,logs): 
    train_gen_bias.reset()
    train_predictions_bias = executed_model.predict_generator(train_gen_bias, verbose=1)
    for name, value in zip(executed_model.metrics_names, train_predictions_bias):
      print(name, "": "",value)
    print()
  
    save_model_param = dict(zip(executed_model.metrics_names, train_predictions_bias))
    dataset = timestr+""-train""
    saveModel(DISEASE_FOLDER,dataset,save_model_param,executed_model)

    p=0.5
    train_pred = [int(x>p) for x in train_predictions_bias]
    train_labels_int = [CLASSES.index(x) for x in  train_labels] # train_labels is a global parameter
    # Calculate the confusion matrix.
    cm = sklearn.metrics.confusion_matrix(train_labels_int,train_pred)
    print('cm for training is ',cm)
    # Log the confusion matrix as an image summary.
    figure = plot_confusion_matrix(cm, CLASSES)
    cm_image = plot_to_image(figure)
    # Log the confusion matrix as an image summary.
    with file_writer_cm_train.as_default():
      tf.summary.image(""Confusion Matrix for training dataset"", cm_image, step='epoch')

 # Define the per-epoch callback.
  cm_callback_train = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix_train)
  cm_callback_valid = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix_valid) # the same like log_confusion_matrix_valid, just different dataset
  cm_callback_test = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix_test)
# Define the per-batch callback.
  batch_print_callback = keras.callbacks.LambdaCallback(on_batch_begin=lambda batch,logs: \
                                                        print('from callback, batch {} begins'.format(batch))) 

#fit model
 history_executed_mode = executed_model.fit(train_gen, \
                                             steps_per_epoch = STEPS_PER_EPOCH, \
                                             epochs = EPOCHS, \
                                             validation_data = validation_gen, \
                                             validation_steps = VALIDATION_STEPS, \
                                             class_weight=class_weight, \
                                             callbacks=[tensorboard_callback, \
                                                        cm_callback_train, \
                                                        cm_callback_valid, \
                                                        cm_callback_test, \
                                                        batch_print_callback])

```
The execution is completed successfuly. 
I can see that log_confusion_matrix_train is running and generating a file with the metrics that I was checking for the training, validation and test dataset. The only issue is that tensorboard is showing black boxes for the scalar metric (See attached - the tab is called events) while hovering on them, I see numbers and the image tab is empty 
![Screen Shot 2020-03-31 at 1 39 19 PM](https://user-images.githubusercontent.com/30246246/78058604-8db43280-7356-11ea-9a84-ec5bdc1f28ae.png)
![Screen Shot 2020-03-31 at 1 39 33 PM](https://user-images.githubusercontent.com/30246246/78058606-8e4cc900-7356-11ea-8d8c-40570a042971.png)
I run the tensorboard from the google shell.

My questions are:
what is wrong with the way I am producing the metrics?
Do I need three writer for the three confusion matrix?
why is the first tab called events and not scalars?

![Screen Shot 2020-03-31 at 1 52 16 PM](https://user-images.githubusercontent.com/30246246/78058840-e388da80-7356-11ea-94be-b6ce5a79af6b.png)
"
38084,Linking tensorflow/python/_pywrap_tensorflow_internal.so failed on debug build on Windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source 
- TensorFlow version: r2.2
- Python version: 3.7
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 2.0
- GCC/Compiler version (if compiling from source): MSVC 19



**Describe the problem**
When build debug tensorflow on Windows. Bazel get stuck at linking tensorflow/python/_pywrap_tensorflow_internal.so. it generate a `_pywrap_tensorflow_internal.pdb` about 4,7 GB. After that, it throw an error [LNK1201](https://docs.microsoft.com/en-us/cpp/error-messages/tool-errors/linker-tools-error-lnk1201?view=vs-2019) `error writing to program database 'filename'; check for insufficient disk space, invalid path, or insufficient privilege`. In addition, Microsoft Incremental Linker consume 87% of my 32 GB Ram


**Provide the exact sequence of commands / steps that you executed before running into the problem**
I just run the command 
`bazel build --config=opt -c dbg --jobs=8 //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
In fact, I try to build a debug version of tensorflow to resolve an issue [here](https://github.com/tensorflow/tensorflow/issues/19297). I think I only need to build the infrastructure (file system) part of tensorflow and not the logical part. Is there anyways to reduce the overall size of tensorflow ( With me, I do not need the tensorflow::ops for example )
"
38082,'TFLiteConverter' object has no attribute 'experimental_new_quantizer' - TF 2.2.0-rc2,"**System information** 
- Have I written custom code: See below.
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or
binary): source
- TensorFlow version (use command below): 2.2.0-rc2
- Python version: 3.6.9
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version : XCode 11.4
- CUDA/cuDNN version: - GPU model and memory: 10.2, GTX1080ti

**Describe the current behavior**
Using the current code:
`
    converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(dP.model_name) 
    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
    converter.representative_dataset = representative_dataset_gen
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    tflite_quant_model = converter.convert()
`

I get the following error and crash using TF 2.2.0-rc2:
`
    tflite_quant_model = converter.convert()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 1094, in convert
    result, inference_input_type, inference_output_type)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 267, in _calibrate_quantize_model
    inference_output_type, allow_float, self.experimental_new_quantizer)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 944, in __getattribute__
    return object.__getattribute__(self, name)
AttributeError: 'TFLiteConverter' object has no attribute 'experimental_new_quantizer'
`

**Describe the expected behavior**
NOTE: This error is specific to TF 2.2.0-rc2. All works well with TF 2.2.0-rc1.

"
38081,Allow users to specify method name when using tf.saved_model.save,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Today, the `tf.saved_model.save` API does not allow users to specify a method name to use for a model signature. Instead, the method name is always `'tensorflow/serving/predict'`, because of the following line.

https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/saved_model/save.py#L462

In my case, this is particularly troublesome because I wish to use the TensorFlow Serving `classify` API which returns arrays of the form `(batch_size, classes, 2)` where the first entry is always a class name (string) and the second entry is a score (float). TensorFlow Serving seems to require that method name be `'tensorflow/serving/classify'`.

**Will this change the current API? How?**

Yes, but I think it could be done without breaking existing functionality if we add another format option for the `signatures` argument to `tf.saved_model.save` that allows the user to supply a method name.

**Who will benefit with this feature?**

Anyone wanting to use something other than the `predict` API in TensorFlow serving.

**Any Other info.**

For now, I work around this issue with the following function, which implements a hack to get the desired method name saved.

```python
import typing

import tensorflow as tf
from tensorflow import python as tfp

def save_serving_model(export_dir: str, model: tf.keras.Model,
                       classes: typing.List[str]):
    """"""Obtain a TensorFlow Serving function that can be used with
    `tf.saved_models.save`

    Args:
        export_dir: The directory in which to export the saved model.
        model: A classification model
        classes: The list of classification labels
    """"""

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])
    def serve(inputs):
        def map_fn(image):
            image = tf.io.decode_image(
                image['image'], channels=3, expand_animations=False)
            image = tf.cast(image, dtype=tf.float32)
            image = tf.compat.v1.image.resize_image_with_pad(
                image=image,
                target_height=model.input_shape[1],
                target_width=model.input_shape[2],
                align_corners=False,
                method=tf.image.ResizeMethod.BILINEAR)
            return image

        images = tf.io.parse_example(
            inputs,
            features={
                'image': tf.io.FixedLenFeature(shape=[], dtype=tf.string)
            },
            example_names=None,
            name=None)
        X = tf.map_fn(
            fn=map_fn, elems=images, back_prop=False, dtype=tf.float32)
        y = model.call(X)
        labels = tf.constant([classes])
        return {
            'scores': y,
            'classes': tf.repeat(
                labels, repeats=tf.shape(y)[0], axis=0, name=None)
        }

    # This is a super-ugly hack, but we have to do it because for
    # the API does not allow us to specify a method name. See:
    # https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/saved_model/save.py#L462
    try:
        PREDICT_METHOD_NAME = tfp.saved_model.signature_constants.PREDICT_METHOD_NAME
        tfp.saved_model.signature_constants.PREDICT_METHOD_NAME = tf.saved_model.CLASSIFY_METHOD_NAME  # pylint: disable=line-too-long
        tf.saved_model.save(
            model,
            export_dir,
            signatures={
                tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
                serve.get_concrete_function()
            })
    finally:
        tfp.saved_model.signature_constants.PREDICT_METHOD_NAME = PREDICT_METHOD_NAME

```

One can use this function with a Keras model in the following manner.

```python
import tensorflow as tf

inputs = tf.keras.layers.Input((100, 100, 3))
x = tf.keras.layers.Conv2D(kernel_size=(2, 2), filters=3)(inputs)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
outputs = tf.keras.layers.Activation('softmax')(x)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)

save_serving_model(export_dir='export', model=model, classes=['class1', 'class2', 'class3'])
```
I can run the model with TensorFlow Serving using:

```bash
docker run -it --rm -p 8501:8501 -v ""$PWD/export:/models/testmodel/1"" -e MODEL_NAME=testmodel tensorflow/serving
```

And I can make requests to the model using 

```python
import requests
import glob

with open('../tests/images/image1.jpg', 'rb') as f:
    examples = [{
        'image': {
            'b64': base64.b64encode(f.read()).decode('utf-8')
        }
    }]
response = requests.post(
    'http://host.docker.internal:8501/v1/models/testmodel:classify',
    json={
        'examples': examples
    }
).json()
print(response['results'])
```

This yields the following response:

```
[[['class1', 0.0], ['class2', 1.0], ['class3', 0.0]]]
```

*IMPORTANT:* If I'm mistaken on how this all works or is supposed to work, please do let me know. this is just what I was able to put together when transitioning to 2.x."
38080,Trying to convert: RuntTimeError: MetaGraphDef associated with tags {'serve'} could not be found in SavedModel.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 
- TensorFlow installed from (source or binary): anaconda installation
- TensorFlow version (or github SHA if from source): 1.15


**The output from the converter invocation**

```
MetaGraphDef associated with tags {'serve'} could not be found in SavedModel.
```

**Also, please include a link to the saved model or GraphDef**

```
https://drive.google.com/open?id=1fI36hxl9vbIY9KL4Tjvv7HIIBJvYIfW5
```


**Any other info / logs**

When I try to convert this is the error I get, so I cannot successfully convert to Lite.
"
38078,tf.data.experimental.ignore_errors: failed assertions are not logged,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or
binary): binary (colab)
- TensorFlow version (use command below): v2.2.0-rc1-0-gacf4951a2f 2.2.0-rc1
- Python version: 3.6.9

**Describe the current behavior**

No log output is emitted if an assertion fails in a dataset pipeline to which ignore_errors has been applied. While this technically matches the description of ""silently"" ignoring errors, this omission limits the usefulness of ignore_errors severely. It cannot be safely applied without the risk of hiding important errors.

**Describe the expected behavior**

Log output is emitted to stderr (visible in colab-jupyter.log).

**Standalone code to reproduce the issue** 

https://colab.research.google.com/gist/Andreas5739738/af7aec733d0190d41aac25c1a062fe1f/ignore_errors.ipynb#scrollTo=pve9Y2tPUDZT

**Other info / logs**
colab-jupyter.log shows no failed assertions:
```
Mar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.548945: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version
-- | -- | --
Mar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.548902: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2371640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
Mar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.548153: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz
Mar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.497196: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ae332d19dc9b): /proc/driver/nvidia/version does not exist
Mar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.497116: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
Mar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.450050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
Mar 31, 2020, 5:57:41 PM | WARNING | 2020-03-31 15:57:41.766459: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
Mar 31, 2020, 5:57:37 PM | INFO | Adapting to protocol v5.1 for kernel 4e9e6601-f865-41ac-ab32-8a4df7fd32b8
Mar 31, 2020, 5:57:35 PM | INFO | Kernel started: 4e9e6601-f865-41ac-ab32-8a4df7fd32b8
Mar 31, 2020, 5:33:10 PM | INFO | Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
Mar 31, 2020, 5:33:10 PM | INFO | http://172.28.0.2:9000/
Mar 31, 2020, 5:33:10 PM | INFO | The Jupyter Notebook is running at:
Mar 31, 2020, 5:33:10 PM | INFO | 0 active kernels
Mar 31, 2020, 5:33:10 PM | INFO | Serving notebooks from local directory: /
Mar 31, 2020, 5:33:10 PM | INFO | google.colab serverextension initialized.
Mar 31, 2020, 5:33:10 PM | INFO | Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret
Mar 31, 2020, 5:33:09 PM | WARNING | warn(""IPython.utils.traitlets has moved to a top-level traitlets package."")
Mar 31, 2020, 5:33:09 PM | WARNING | /usr/local/lib/python2.7/dist-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.```"
38077,tflite with select_tf_ops enabled fails to build,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GNU/Linux aarch64
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: 3.6
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): gcc 7.4.0


**Describe the problem**
Trying to build TensorFlow Lite shared library on custom aarch64/arm64 board with select_tf_ops enabled via bazel. Reason being a model which uses an operator that is not yet supported by the standard tflite runtime.

During the build, an error occurs that does not seem to be related to the extended runtime options.

```
ERROR: /workspace/tensorflow/tensorflow/core/BUILD:2310:1: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 1)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:161:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/tensor.h:21,
                 from ./tensorflow/core/util/tensor_slice_reader.h:25,
                 from tensorflow/core/util/tensor_slice_reader.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) int; TgtPacket = __vector(16) unsigned char]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:61:49:   required from 'TgtPacket Eigen::PacketConverter<TensorEvaluator, SrcPacket, TgtPacket, SrcCoeffRatio, TgtCoeffRatio>::packet(Index) const [with int LoadMode = 0; Index = long int; TensorEvaluator = Eigen::TensorEvaluator<const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>, Eigen::DefaultDevice>; SrcPacket = __vector(4) int; TgtPacket = __vector(16) unsigned char; int SrcCoeffRatio = 1; int TgtCoeffRatio = 1]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:252:53:   required from 'static TargetPacket Eigen::internal::PacketConv<SrcPacket, TargetPacket, LoadMode, true, IsSameT>::run(const Eigen::TensorEvaluator<ArgType, Device>&, Eigen::Index) [with ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; SrcPacket = __vector(4) int; TargetPacket = __vector(16) unsigned char; int LoadMode = 0; bool IsSameT = false; Eigen::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:382:63:   required from 'Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index) const [with int LoadMode = 0; TargetType = unsigned char; ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType = __vector(16) unsigned char; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1418:18:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::InnerDimAssign<true, Evaluator>::Run(Scalar*, IndexType, const Evaluator&, IndexType) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorConversionOp<unsigned char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >, Eigen::DefaultDevice>; Scalar = unsigned char; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<unsigned char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1523:48:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Run(const Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Target&, const TensorBlockExpr&) [with Scalar = unsigned char; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<unsigned char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:174:27:   [ skipping 3 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:206:9:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, Vectorizable, (Eigen::internal::TiledEvaluation)1>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, Eigen::TensorMap<Eigen::Tensor<unsigned char, 8, 1, long int>, 0, Eigen::MakePointer> >, const Eigen::TensorConversionOp<unsigned char, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > > >; bool Vectorizable = true]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:384:65:   required from 'Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>& Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorConversionOp<unsigned char, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > >; StartIndices = const Eigen::DSizes<long int, 8>; Sizes = const Eigen::DSizes<long int, 8>; XprType = Eigen::TensorMap<Eigen::Tensor<unsigned char, 8, 1, long int>, 0, Eigen::MakePointer>]'
./tensorflow/core/util/tensor_slice_util.h:51:27:   required from 'static void tensorflow::{anonymous}::CopyThatWorksWithStringPointer<DstT>::Copy(const SrcTensor&, Shape, Shape, DstTensor&, Shape) [with SrcTensor = Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer>; DstTensor = Eigen::TensorMap<Eigen::Tensor<unsigned char, 8, 1, long int>, 0, Eigen::MakePointer>; Shape = Eigen::DSizes<long int, 8>; DstT = unsigned char]'
./tensorflow/core/util/tensor_slice_util.h:181:47:   required from 'bool tensorflow::{anonymous}::CopyDataFromTensorSliceToTensorSlice(const tensorflow::TensorShape&, const tensorflow::TensorSlice&, const tensorflow::TensorSlice&, const SrcT*, DstT*) [with SrcT = int; DstT = unsigned char]'
./tensorflow/core/util/tensor_slice_reader.h:184:41:   required from 'bool tensorflow::checkpoint::TensorSliceReader::CopySliceData(const string&, const tensorflow::TensorSlice&, T*) const [with T = unsigned char; std::__cxx11::string = std::__cxx11::basic_string<char>]'
tensorflow/core/util/tensor_slice_reader.cc:264:5:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:145:10: error: invalid static_cast from type 'const __vector(4) int' to type '__vector(16) unsigned char'
   return static_cast<TgtPacket>(a);
          ^~~~~~~~~~~~~~~~~~~~~~~~~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) int; TgtPacket = __vector(8) short int]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:61:49:   required from 'TgtPacket Eigen::PacketConverter<TensorEvaluator, SrcPacket, TgtPacket, SrcCoeffRatio, TgtCoeffRatio>::packet(Index) const [with int LoadMode = 0; Index = long int; TensorEvaluator = Eigen::TensorEvaluator<const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>, Eigen::DefaultDevice>; SrcPacket = __vector(4) int; TgtPacket = __vector(8) short int; int SrcCoeffRatio = 1; int TgtCoeffRatio = 1]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:252:53:   required from 'static TargetPacket Eigen::internal::PacketConv<SrcPacket, TargetPacket, LoadMode, true, IsSameT>::run(const Eigen::TensorEvaluator<ArgType, Device>&, Eigen::Index) [with ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; SrcPacket = __vector(4) int; TargetPacket = __vector(8) short int; int LoadMode = 0; bool IsSameT = false; Eigen::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:382:63:   required from 'Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index) const [with int LoadMode = 0; TargetType = short int; ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType = __vector(8) short int; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1418:18:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::InnerDimAssign<true, Evaluator>::Run(Scalar*, IndexType, const Evaluator&, IndexType) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorConversionOp<short int, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >, Eigen::DefaultDevice>; Scalar = short int; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<short int, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1523:48:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Run(const Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Target&, const TensorBlockExpr&) [with Scalar = short int; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<short int, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:174:27:   [ skipping 3 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:206:9:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, Vectorizable, (Eigen::internal::TiledEvaluation)1>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, Eigen::TensorMap<Eigen::Tensor<short int, 8, 1, long int>, 0, Eigen::MakePointer> >, const Eigen::TensorConversionOp<short int, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > > >; bool Vectorizable = true]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:384:65:   required from 'Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>& Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorConversionOp<short int, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > >; StartIndices = const Eigen::DSizes<long int, 8>; Sizes = const Eigen::DSizes<long int, 8>; XprType = Eigen::TensorMap<Eigen::Tensor<short int, 8, 1, long int>, 0, Eigen::MakePointer>]'
./tensorflow/core/util/tensor_slice_util.h:51:27:   required from 'static void tensorflow::{anonymous}::CopyThatWorksWithStringPointer<DstT>::Copy(const SrcTensor&, Shape, Shape, DstTensor&, Shape) [with SrcTensor = Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer>; DstTensor = Eigen::TensorMap<Eigen::Tensor<short int, 8, 1, long int>, 0, Eigen::MakePointer>; Shape = Eigen::DSizes<long int, 8>; DstT = short int]'
./tensorflow/core/util/tensor_slice_util.h:181:47:   required from 'bool tensorflow::{anonymous}::CopyDataFromTensorSliceToTensorSlice(const tensorflow::TensorShape&, const tensorflow::TensorSlice&, const tensorflow::TensorSlice&, const SrcT*, DstT*) [with SrcT = int; DstT = short int]'
./tensorflow/core/util/tensor_slice_reader.h:184:41:   required from 'bool tensorflow::checkpoint::TensorSliceReader::CopySliceData(const string&, const tensorflow::TensorSlice&, T*) const [with T = short int; std::__cxx11::string = std::__cxx11::basic_string<char>]'
tensorflow/core/util/tensor_slice_reader.cc:265:5:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:145:10: error: invalid static_cast from type 'const __vector(4) int' to type '__vector(8) short int'
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) int; TgtPacket = __vector(16) signed char]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:61:49:   required from 'TgtPacket Eigen::PacketConverter<TensorEvaluator, SrcPacket, TgtPacket, SrcCoeffRatio, TgtCoeffRatio>::packet(Index) const [with int LoadMode = 0; Index = long int; TensorEvaluator = Eigen::TensorEvaluator<const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>, Eigen::DefaultDevice>; SrcPacket = __vector(4) int; TgtPacket = __vector(16) signed char; int SrcCoeffRatio = 1; int TgtCoeffRatio = 1]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:252:53:   required from 'static TargetPacket Eigen::internal::PacketConv<SrcPacket, TargetPacket, LoadMode, true, IsSameT>::run(const Eigen::TensorEvaluator<ArgType, Device>&, Eigen::Index) [with ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; SrcPacket = __vector(4) int; TargetPacket = __vector(16) signed char; int LoadMode = 0; bool IsSameT = false; Eigen::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:382:63:   required from 'Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index) const [with int LoadMode = 0; TargetType = signed char; ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType = __vector(16) signed char; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1418:18:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::InnerDimAssign<true, Evaluator>::Run(Scalar*, IndexType, const Evaluator&, IndexType) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorConversionOp<signed char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >, Eigen::DefaultDevice>; Scalar = signed char; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<signed char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1523:48:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Run(const Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Target&, const TensorBlockExpr&) [with Scalar = signed char; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<signed char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:174:27:   [ skipping 3 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:206:9:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, Vectorizable, (Eigen::internal::TiledEvaluation)1>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, Eigen::TensorMap<Eigen::Tensor<signed char, 8, 1, long int>, 0, Eigen::MakePointer> >, const Eigen::TensorConversionOp<signed char, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > > >; bool Vectorizable = true]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:384:65:   required from 'Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>& Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorConversionOp<signed char, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > >; StartIndices = const Eigen::DSizes<long int, 8>; Sizes = const Eigen::DSizes<long int, 8>; XprType = Eigen::TensorMap<Eigen::Tensor<signed char, 8, 1, long int>, 0, Eigen::MakePointer>]'
./tensorflow/core/util/tensor_slice_util.h:51:27:   required from 'static void tensorflow::{anonymous}::CopyThatWorksWithStringPointer<DstT>::Copy(const SrcTensor&, Shape, Shape, DstTensor&, Shape) [with SrcTensor = Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer>; DstTensor = Eigen::TensorMap<Eigen::Tensor<signed char, 8, 1, long int>, 0, Eigen::MakePointer>; Shape = Eigen::DSizes<long int, 8>; DstT = signed char]'
./tensorflow/core/util/tensor_slice_util.h:181:47:   required from 'bool tensorflow::{anonymous}::CopyDataFromTensorSliceToTensorSlice(const tensorflow::TensorShape&, const tensorflow::TensorSlice&, const tensorflow::TensorSlice&, const SrcT*, DstT*) [with SrcT = int; DstT = signed char]'
./tensorflow/core/util/tensor_slice_reader.h:184:41:   required from 'bool tensorflow::checkpoint::TensorSliceReader::CopySliceData(const string&, const tensorflow::TensorSlice&, T*) const [with T = signed char; std::__cxx11::string = std::__cxx11::basic_string<char>]'
tensorflow/core/util/tensor_slice_reader.cc:266:5:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:145:10: error: invalid static_cast from type 'const __vector(4) int' to type '__vector(16) signed char'
Target //tensorflow/lite/c:tensorflowlite_c failed to build
Use --verbose_failures to see the command lines of failed build steps.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Enable extended runtime in tensorflow/tensorflow/lite/c/BUILD:

```
tflite_cc_shared_object(
    name = ""tensorflowlite_c"",
    linkopts = select({
        ""//tensorflow:macos"": [
            ""-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)"",
        ],
        ""//tensorflow:windows"": [],
        ""//conditions:default"": [
            ""-z defs"",
            ""-Wl,--version-script,$(location //tensorflow/lite/c:version_script.lds)"",
        ],
    }),
    per_os_targets = True,
    deps = [
        "":c_api"",
        "":c_api_experimental"",
        "":exported_symbols.lds"",
        "":version_script.lds"",
	""//tensorflow/lite/delegates/flex:delegate"",
    ],
)
```
Invoke the build command from the tf root dir:
`bazel build --config=monolithic --define=with_select_tf_ops=true -c opt //tensorflow/lite/c:tensorflowlite_c`

Something worth mentioning is that the default tflite runtime can be built on this system, so its probably not an issue of the bazel installation and/or the platform. Help of any kind is appreciated."
38075,Invalid Output shape to Invalid input,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Yes
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: No
- TensorFlow installed from (source or
binary): - TensorFlow 2.1 (source) 
- Python version: - 3.7
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:No

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When`kenerl_size=0` the size of image increases
**Describe the expected behavior**
Value error
**Standalone code to reproduce the issue** 
`import tensorflow as tf`
`inputs=tf.keras.layers.Input(shape=(32, 32, 3))`
`x=tf.keras.layers.Conv2D(64,kernel_size=0)(inputs)`
`x=tf.keras.layers.Flatten()(x)`
`outputs=tf.keras.layers.Dense(64)(x)`
`model=tf.keras.Model(inputs,outputs)`
`model.summary()`

Output:
![issue1](https://user-images.githubusercontent.com/62893143/78043941-98d88400-7391-11ea-99b0-37d10eb84e46.PNG)


"
38074,Tf issue,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""generate_tfrecord.py"", line 17, in <module>
    import tensorflow as tf
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\RONAK JAIN\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime."
38073,TFLite conversion coredumps,"**System information** : Ubuntu18.04, CUDA 10
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No. Problem exists even when using tflite_convert OR writing custom code as described in [documentation](https://www.tensorflow.org/lite/performance/post_training_quantization)
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): Binary/Tensorflow: 2.0.1
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory: Cuda 10/GTX1070Ti, 8G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

Describe the current behavior
Tensorflow lite conversion dumps core both for 8 bit and 16 bit quantizations and no tflite file is generated when using the from_saved_model converter. The model shape (as can be seen from saved_model_cli) is below. The model is a SSD-mobilenetv2 transfer learned by using tensorflow detection API on Tensorflow 1.14
==

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['inputs'] tensor_info:
        dtype: DT_UINT8
        shape: (-1, 300, 300, 3)
        name: image_tensor:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['detection_boxes'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 30, 4)
        name: detection_boxes:0
    outputs['detection_classes'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 30)
        name: detection_classes:0
    outputs['detection_multiclass_scores'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 30, 5)
        name: detection_multiclass_scores:0
    outputs['detection_scores'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 30)
        name: detection_scores:0
    outputs['num_detections'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1)
        name: num_detections:0
    outputs['raw_detection_boxes'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, -1, 4)
        name: raw_detection_boxes:0
    outputs['raw_detection_scores'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, -1, 5)
        name: raw_detection_scores:0
  Method name is: tensorflow/serving/predict
==
**Describe the expected behavior**

I expect the tflite file to be generated by following the public documentation on Tensorflow site and don't expect core dumps.
**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.



**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
2020-03-31 06:46:09.156599: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-03-31 06:46:09.156683: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-03-31 06:46:09.157414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:07:00.0
2020-03-31 06:46:09.157448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-03-31 06:46:09.157459: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-03-31 06:46:09.157469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-03-31 06:46:09.157478: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-03-31 06:46:09.157487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-03-31 06:46:09.157496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-03-31 06:46:09.157505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-31 06:46:09.157937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-03-31 06:46:09.157973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-31 06:46:09.157980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-03-31 06:46:09.157986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-03-31 06:46:09.158409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6624 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:07:00.0, compute capability: 6.1)
2020-03-31 06:46:09.471641: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2020-03-31 06:46:09.471673: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 1765 nodes (-1299), 2239 edges (-1452), time = 191.291ms.
2020-03-31 06:46:09.471681: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 1765 nodes (0), 2239 edges (0), time = 51.045ms.
Traceback (most recent call last):
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/bin/tflite_convert"", line 8, in <module>
    sys.exit(main())
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 515, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 502, in run_main
    _convert_tf2_model(tflite_flags)
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 221, in _convert_tf2_model
    tflite_model = converter.convert()
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 446, in convert
    **converter_kwargs)
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-03-31 06:46:11.117660: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.117740: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.128695: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.128746: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.128807: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.128829: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.128838: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.128846: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.128887: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.128899: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.128907: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.128914: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.128921: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.128930: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.128937: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.128945: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.128951: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.128956: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-31 06:46:11.128965: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.128972: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.128978: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.128993: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.129001: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.129007: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.129011: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.129022: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-03-31 06:46:11.129035: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-31 06:46:11.129045: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-31 06:46:11.129051: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-31 06:46:11.129066: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-31 06:46:11.129075: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-31 06:46:11.129086: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-31 06:46:11.129097: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-31 06:46:11.129114: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-31 06:46:11.129123: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-31 06:46:11.130076: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.130091: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130100: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.130107: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130115: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.130122: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130130: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.130136: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130144: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.130150: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130158: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.130165: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130173: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.130179: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130186: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130196: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.130202: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130208: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.130214: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130220: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-31 06:46:11.130226: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130234: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130240: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130245: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-31 06:46:11.130253: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130259: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130265: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130271: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130276: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-31 06:46:11.130283: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130291: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130297: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130313: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130319: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130323: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130328: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130332: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130336: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-31 06:46:11.130344: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130350: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130354: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-31 06:46:11.130363: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130370: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130376: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130380: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130387: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130395: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130401: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130406: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130412: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-31 06:46:11.130418: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130424: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130435: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130442: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.130455: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-03-31 06:46:11.130474: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-31 06:46:11.130482: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-31 06:46:11.130490: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-31 06:46:11.130499: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-31 06:46:11.130506: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-31 06:46:11.130514: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-31 06:46:11.130521: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-31 06:46:11.130940: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-31 06:46:11.130952: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-31 06:46:11.130960: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-31 06:46:11.130967: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-31 06:46:11.130974: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-31 06:46:11.130981: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-31 06:46:11.130988: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-31 06:46:11.130993: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-31 06:46:11.131050: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-31 06:46:11.131059: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-31 06:46:11.131068: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-31 06:46:11.131078: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-31 06:46:11.131091: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-31 06:46:11.131099: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-31 06:46:11.131105: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-31 06:46:11.131181: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3
2020-03-31 06:46:11.131191: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3
2020-03-31 06:46:11.131199: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3
2020-03-31 06:46:11.131207: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3
2020-03-31 06:46:11.131285: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2020-03-31 06:46:11.131358: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2020-03-31 06:46:11.131403: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-31 06:46:11.131510: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-31 06:46:11.131520: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-31 06:46:11.131531: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-31 06:46:11.131538: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-31 06:46:11.157600: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1120 operators, 2034 arrays (0 quantized)
2020-03-31 06:46:11.214243: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1113 operators, 2018 arrays (0 quantized)
2020-03-31 06:46:11.277956: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1113 operators, 2018 arrays (0 quantized)
2020-03-31 06:46:11.340779: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 610 operators, 1209 arrays (0 quantized)
2020-03-31 06:46:11.358315: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 610 operators, 1209 arrays (0 quantized)
2020-03-31 06:46:11.372161: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 610 operators, 1209 arrays (0 quantized)
2020-03-31 06:46:11.401094: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 1080704 bytes, theoretical optimal value: 1080704 bytes.
2020-03-31 06:46:11.404128: F tensorflow/lite/toco/tooling_util.cc:2275] Check failed: array.data_type == array.final_data_type Array ""image_tensor"" has mis-matching actual and final data types (data_type=uint8, final_data_type=float).
Fatal Python error: Aborted

Current thread 0x00007fbd58971740 (most recent call first):
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/absl/app.py"", line 250 in _run_main
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/absl/app.py"", line 299 in run
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""/home/vk1z/.virtualenvs/tensorflow-lite-convert/bin/toco_from_protos"", line 8 in <module>
Aborted (core dumped)

==
Yes, I am aware of [35376](https://github.com/tensorflow/tensorflow/issues/35736) but I don't think that a coredump is an expected output of any conversion process. "
38072,tf2.1 tf.nn.ctc_loss slower，takes up a lot of GPU memory,"Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template

System information

Have I written custom code : yes
OS Platform and Distribution: centos 7, not Distribution
TensorFlow installed from docker: docker pull nvcr.io/nvidia/tensorflow:20.02-tf2-py3 .tensorflow version is 2.1.0
Describe the current behavior

Just run the forward algorithm and use watch -n 0.2 nvidia-smi to watch the GPU utilization rate always stay at 86% ~ 95%. However, after calculating tf.nn.ctc_loss, the GPU usage jitter is drastically from 0% to 85%. Seeing the decline in gpu utilization is particularly severe, personally guess that this tf.nn.ctc_loss is implemented by cpu, will there be a switch between gpu memory and memory when calculating loss?

Describe the expected behavior

Calculate the loss and update gradient normally, the GPU usage rate remains at 85%. Currently, the performance of calculating the loss using tf.nn.ctc_loss of tf2.1 does not meet expectations.

Here is my test code. The run_forward_noloss function is a simple forward calculation function. Run_forward_withloss is forward calculation +loss calculation.
```
# _*_ coding:utf-8 _*_
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import time
import tensorflow as tf
layers = tf.keras.layers
time_func = lambda: time.clock()*1000

class CRNNEncoder(tf.keras.Model):
    def __init__(self, configs, name=None):
        super(CRNNEncoder, self).__init__(name=name)
        self.vocab_size = configs['vocab_size']
        self.conv1 = layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', name='conv1')
        self.pool1 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1')
        self.conv2 = layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', name='conv2')
        self.pool2 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2')
        self.conv3 = layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', name='conv3')
        self.conv4 = layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', name='conv4')
        self.padd4 = layers.ZeroPadding2D(padding=(0, 1))
        self.pool4 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 1), padding='valid', name='pool3')
        self.conv5 = layers.Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='conv5')
        self.bncv5 = layers.BatchNormalization(axis=-1, name='bnconv5')
        self.conv6 = layers.Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='conv6')
        self.bncv6 = layers.BatchNormalization(axis=-1, name='bnconv6')
        self.pddd6 = layers.ZeroPadding2D(padding=(0, 1))
        self.pool6 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 1), padding='valid', name='pool4')
        self.conv7 = layers.Conv2D(512, kernel_size=(3, 3), activation='relu', padding='valid', name='conv7')
        self.final_layer = tf.keras.layers.Dense(self.vocab_size, name='ctc_decoder_linear')

    def get_feature_step(self, widths):
        return tf.cast((tf.cast(widths, tf.float32)/4.0), dtype=tf.int32)
    @tf.function
    def call(self, inputs, widths, training=True):
        tf.print('call input:', inputs.shape)
        features = self.conv1(inputs)
        features = self.pool1(features)
        features = self.conv2(features)
        features = self.pool2(features)
        features = self.conv3(features)
        features = self.conv4(features)
        features = self.padd4(features)
        features = self.pool4(features)
        features = self.conv5(features)
        features = self.bncv5(features, training=training)
        features = self.conv6(features)
        features = self.bncv6(features, training=training)
        features = self.pddd6(features)
        features = self.pool6(features)
        features = self.conv7(features)
        cnn_features = tf.reduce_max(features, axis=1)
        # rnn_features = self.run_bilstm1(rnn_features)
        rnn_features = cnn_features
        final_logits = self.final_layer(rnn_features)
        widths = self.get_feature_step(widths)
        return cnn_features, rnn_features, widths, final_logits

def run_forward_noloss():
    batch = 24 
    imgh = 48
    imgw = 1024 
    imgc = 3
    vocab_size = 1424
    configs = {'vocab_size': vocab_size}
    model = CRNNEncoder(configs)
    images = tf.random.uniform([batch, imgh, imgw, imgc], minval=-1, maxval=1)
    widths = tf.fill([batch], imgw)
    logit_mean = tf.keras.metrics.Mean('logit_mean')
    init_time = time_func()
    for i in range(200):
        cnn_features, rnn_features, widths, final_logits = model(images, widths)
        final_logits = tf.reduce_mean(final_logits)
        logit_mean.update_state(final_logits)
    fini_time = time_func()
    print('time:', fini_time-init_time)
    print(logit_mean.result().numpy)

def run_forward_withloss():
    batch = 24
    imgh = 48
    imgw = 1024
    imgc = 3
    txtlen = 64
    vocab_size = 1424
    eos_id = vocab_size - 1
    configs = {'vocab_size': vocab_size}
    model = CRNNEncoder(configs)
    images = tf.random.uniform([batch, imgh, imgw, imgc], minval=-1, maxval=1)
    widths = tf.fill([batch], imgw)
    labels = tf.fill([batch, txtlen], 0)
    labels_len = tf.fill([batch], txtlen)
    ctc_loss_mean = tf.keras.metrics.Mean('ctc_loss_mean')
    init_time = time_func()
    for i in range(200):
        cnn_features, rnn_features, widths, final_logits = model(images, widths)
        ctc_loss = tf.nn.ctc_loss(labels=labels,
                                  logits=final_logits,
                                  label_length=labels_len,
                                  logit_length=widths,
                                  blank_index=eos_id,
                                  logits_time_major=False)

        ctc_loss = tf.reduce_mean(ctc_loss)
        ctc_loss_mean.update_state(ctc_loss)
    fini_time = time_func()
    print('time:', fini_time - init_time)
    print(ctc_loss_mean.result().numpy)

if __name__ == '__main__':
    run_forward_noloss()
    run_forward_withloss()
```"
38071,INT8 quantized model for TF Lite,"Hi, I tried to run INT8 model on my device with TFLite but it seems its inference time is very slow and maybe it could only be run on CPU. While I run UINT8 model, it is very fast. How should I do so that it could be run normally for INT8 model? I noticed that there is an option for convert F16 to F32 by using ""setAllowFp16PrecisionForFp32"". is there any similar setting to convert INT8 to UINT8? thanks."
38070,/bin/sh: 1: python: not found error when generate projects,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., WSLv1 Ubuntu 18.04 ):
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source):  head
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): n/a

**Describe the problem**
**Please provide the exact sequence of commands/steps when you ran into the problem**

Python is 3.6.9

After clone a fresh repo, run command:
```make -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn generate_projects```

Reported error:
```/bin/sh: 1: python: not found
tensorflow/lite/micro/examples/person_detection_experimental/Makefile.inc:55: recipe for target 'tensorflow/lite/micro/
tools/make/gen/linux_x86_64/prj/person_detection_test_int8/keil/keil_project.uvprojx' failed
make: *** [tensorflow/lite/micro/tools/make/gen/linux_x86_64/prj/person_detection_test_int8/keil/keil_project.uvprojx]
Error 127```



"
38069,Colab TPU broken on latest tf-nightly,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  6 lines of adaptation to stock example script from [https://www.tensorflow.org/guide/tpu](https://www.tensorflow.org/guide/tpu)
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Google Colaboratory
- TensorFlow version (use command below): 2.2.0.dev20200330
- Python version: 3.6.9 

**Describe the current behavior**
On Google Colaboratory, TPU's no longer work with the tf-nightly builds. I was using the trick as mentioned here to run TPU's on tf-nightly: [https://github.com/tensorflow/tensorflow/issues/34346#issuecomment-598399912](https://github.com/tensorflow/tensorflow/issues/34346#issuecomment-598399912) but it stopped working. 

Suddenly, when doing the same thing as before, it throws an error while trying to initialize the tpu system, giving:
```
InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}
```

**Describe the expected behavior**
The expected behavior is for it to not throw an error so that the TPU works.

**Standalone code to reproduce the issue** 
I have a minimal reproduction based on the [tensorflow TPU tutorial](https://www.tensorflow.org/guide/tpu) with the [trick from above](https://github.com/tensorflow/tensorflow/issues/34346#issuecomment-598399912) added before the first cell: [https://colab.research.google.com/drive/1UzC_KCMkV8LOp7chvhkRsB3orQBlBVef](https://colab.research.google.com/drive/1UzC_KCMkV8LOp7chvhkRsB3orQBlBVef)

**Other info / logs** 
The NodeDef error occurs at this cell:
```
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
```
And the error it throws is:
```
INFO:tensorflow:Initializing the TPU system: grpc://10.79.85.146:8470
INFO:tensorflow:Initializing the TPU system: grpc://10.79.85.146:8470
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Clearing out eager caches
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-f9d179e80e14> in <module>()
      1 resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
      2 tf.config.experimental_connect_to_cluster(resolver)
----> 3 tf.tpu.experimental.initialize_tpu_system(resolver)

3 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}

---------------------------------------------------------------------------
NOTE: Current TensorFlow version is 2.2.0-dev20200330. To use TF 1.x instead,
restart your runtime (Ctrl+M .) and run ""%tensorflow_version 1.x"" before
you run ""import tensorflow"".
---------------------------------------------------------------------------
```


I have been trying to find out in what nightly this was introduced, but I sometimes get errors that I am trying too frequently essentially. Anyway, I will list all the versions I tried.

Tf-nightly versions:
- 2.2.0.dev20200327 works
- Nightlies that fail with NodeDef error
    - 2.2.0.dev20200328
    - 2.2.0.dev20200329
    - 2.2.0.dev20200330




Some potentially related/useful information but maybe unrelated.
I couldn't directly find which ones were working and which ones weren't, so I also tested older versions first, and noticed there was a lot of versions which had a different error. So maybe this was simply explicitly fixed, or maybe implicitly in which case it could add additional information:
- 2.2.0.dev20200311 works
- Nightlies that fail with a different error, namely a mesh_shape error (see error details below):
    - 2.2.0.dev20200312
    - 2.2.0.dev20200313
    - 2.2.0.dev20200316
    - 2.2.0.dev20200319
    - 2.2.0.dev20200323

Mesh_shape error that occurs on certain nightlies (with same notebook and occurring in the same cell):
```
INFO:tensorflow:Initializing the TPU system: grpc://10.18.110.18:8470
INFO:tensorflow:Initializing the TPU system: grpc://10.18.110.18:8470
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Finished initializing TPU system.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-36e427972e34> in <module>()
      1 resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
      2 tf.config.experimental_connect_to_cluster(resolver)
----> 3 tf.tpu.experimental.initialize_tpu_system(resolver)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/topology.py in _parse_topology(self, serialized)
    107     if len(self._mesh_shape) != 4 or any(self._mesh_shape < 1):
    108       raise ValueError(""`mesh_shape` must be a vector of size 4 with positive ""
--> 109                        ""entries; got {}"".format(self._mesh_shape))
    110 
    111     if proto.num_tasks < 0:

ValueError: `mesh_shape` must be a vector of size 4 with positive entries; got [2 2 2]
```"
38068,Tensorflow 2.2 Object detection crash while try to detect object from live feed,"The tensorflow object detection crashes while using the below code 

```
#!/usr/bin/env python
# coding: utf-8

# # Object Detection API Demo


import os
import pathlib


if ""models"" in pathlib.Path.cwd().parts:
  while ""models"" in pathlib.Path.cwd().parts:
    os.chdir('..')
elif not pathlib.Path('models').exists():
  get_ipython().system('git clone --depth 1 https://github.com/tensorflow/models')


import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image
from IPython.display import display


# Import the object detection module.

# In[5]:


from object_detection.utils import ops as utils_ops
from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as vis_util

# Patches:

# In[6]:


# patch tf1 into `utils.ops`
utils_ops.tf = tf.compat.v1

# Patch the location of gfile
tf.gfile = tf.io.gfile


# # Model preparation 

# ## Variables
# 
# Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing the path.
# 
# By default we use an ""SSD with Mobilenet"" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.

# ## Loader

# In[7]:


def load_model(model_name):
  base_url = 'http://download.tensorflow.org/models/object_detection/'
  model_file = model_name + '.tar.gz'
  model_dir = tf.keras.utils.get_file(
    fname=model_name, 
    origin=base_url + model_file,
    untar=True)

  print('Model Dir', model_dir)
  print('Pathlib', pathlib)
  model_dir = pathlib.Path(model_dir)/""saved_model""
  print('Model Dir', model_dir)

  model = tf.saved_model.load(str(model_dir))
  model = model.signatures['serving_default']

  return model


# ## Loading label map
# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine

# In[8]:


# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'
category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)


# For the sake of simplicity we will test on 2 images:

# In[9]:


# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = pathlib.Path('models/research/object_detection/test_images')
TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(""*.jpg"")))
TEST_IMAGE_PATHS


# # Detection

# Load an object detection model:

# In[10]:


model_name = 'ssd_mobilenet_v1_coco_2017_11_17'
detection_model = load_model(model_name)


# Check the model's input signature, it expects a batch of 3-color images of type uint8: 

# In[11]:


print(detection_model.inputs)


# And retuns several outputs:

# In[12]:


detection_model.output_dtypes


# In[13]:


print(detection_model.output_shapes)


# Add a wrapper function to call the model, and cleanup the outputs:

# In[14]:


def run_inference_for_single_image(model, image):
  image = np.asarray(image)
  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.
  input_tensor = tf.convert_to_tensor(image)
  # The model expects a batch of images, so add an axis with `tf.newaxis`.
  input_tensor = input_tensor[tf.newaxis,...]

  # Run inference
  output_dict = model(input_tensor)

  # All outputs are batches tensors.
  # Convert to numpy arrays, and take index [0] to remove the batch dimension.
  # We're only interested in the first num_detections.
  num_detections = int(output_dict.pop('num_detections'))
  output_dict = {key:value[0, :num_detections].numpy() 
                 for key,value in output_dict.items()}
  output_dict['num_detections'] = num_detections

  # detection_classes should be ints.
  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)
   
  # Handle models with masks:
  if 'detection_masks' in output_dict:
    # Reframe the the bbox mask to the image size.
    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
              output_dict['detection_masks'], output_dict['detection_boxes'],
               image.shape[0], image.shape[1])      
    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,
                                       tf.uint8)
    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()
    
  return output_dict

import cv2
cap = cv2.VideoCapture(0)

def run_inference(model, cap):
    while True:
        ret, image_np = cap.read()
        # Actual detection.
        output_dict = run_inference_for_single_image(model, image_np)
        # Visualization of the results of a detection.
        vis_util.visualize_boxes_and_labels_on_image_array(
            image_np,
            output_dict['detection_boxes'],
            output_dict['detection_classes'],
            output_dict['detection_scores'],
            category_index,
            instance_masks=output_dict.get('detection_masks_reframed', None),
            use_normalized_coordinates=True,
            line_thickness=8)
        cv2.imshow('object_detection', cv2.resize(image_np, (800, 600)))
        if cv2.waitKey(1) & 0xFF == ord('q'):
            cap.release()
            cv2.destroyAllWindows()
            break

run_inference(detection_model, cap) 
```

**Tensorflow-Gpu Version:** 
tf.__version__
'2.2.0-rc1'

When I use the above code it shows result  upto 2-3 seconds after that it says not responding"
38067,Custom Loss: Order of arguments,"I've found a little mistake in the documentation. On the following website, the order of y_true and y_pred are reversed:

https://www.tensorflow.org/tutorials/customization/custom_training

```
def loss(predicted_y, target_y):
  return tf.reduce_mean(tf.square(predicted_y - target_y))
```

It's usually the other way around:

https://www.tensorflow.org/api_docs/python/tf/keras/losses/

```
keras.losses.mean_squared_error(y_true, y_pred)
```
It makes no difference for MSE since this loss is symmetric. It does make a difference for MMSE (Masked MSE) where random values of the target are mapped to zero.
"
38066,ctc loss output：No valid path found,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
38065,Failed to build the master branch,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04 tensorflow/tensorflow:latest-devel-gpu-py3):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: master branch
- Python version:3.6.9
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):Build label: 2.0.0
- GCC/Compiler version (if compiling from source):gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04) 
- CUDA/cuDNN version: 10.1
- GPU model and memory:



**Describe the problem**
Master branch build failed
**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
bazel build --jobs=4 --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
NFO: Call stack for the definition of repository 'llvm-project' which is a tf_http_archive (rule definition at /home/tx/Downloads/tensorflow/third_party/repo.bzl:134:19):
 - /home/tx/Downloads/tensorflow/tensorflow/workspace.bzl:598:5
 - /home/tx/Downloads/tensorflow/WORKSPACE:19:1
ERROR: /home/tx/Downloads/tensorflow/tensorflow/lite/toco/BUILD:439:1: Target '//tensorflow/lite/toco:toco' depends on toolchain '@local_config_cc//:cc-compiler-k8', which cannot be found: error loading package '@local_config_cc//': Unable to load file '@local_config_cc//:armeabi_cc_toolchain_config.bzl': file doesn't exist'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: error loading package '@local_config_cc//': Unable to load file '@local_config_cc//:armeabi_cc_toolchain_config.bzl': file doesn't exist
INFO: Elapsed time: 10.477s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (226 packages loaded, 3671 targets configured)
    currently loading: @com_github_grpc_grpc//src/compiler ... (2 packages)
    Fetching @com_google_absl; fetching 4s
```"
38064,UnboundLocalError: local variable 'logs' referenced before assignment on training with little data,"I found an error caused by an attempt of coping training logs from a not yet assigned variable.
The error occurred on my machine (Arch linux, tensorflow v2.2rc2 compiled from source) and I managed to reproduce the error on colab, on a stock environment.
It only happens when the model.fit method is called with very little training / eval data.
The ```logs``` variable is assigned inside a ```for``` loop that never happens when there is no sufficient data. 

The code lives here:
https://github.com/tensorflow/tensorflow/blob/e6e5d6df2ab26620548f35bf2e652b19f6d06652/tensorflow/python/keras/engine/training.py#L793

The notebook gist link for reproducing the bug:
https://colab.research.google.com/gist/naripok/8ce09ec9c3e795b3635a6b1ac11ebd4b/tpu_transformer_model.ipynb"
38063,tflite converter build in ops not support: ResizeNearestNeighbor,"when I convert the keras model to tflite, it is ok when :
     `convert.experimental_new_converter = False`
but when:
    `conver.experimental_new_converter=True`
it shows the error:
 

> <unknown>:0: error: failed while converting: 'main'
> Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor.

I use code:
`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
`
It is ok, but the resize nearest neighbor should be the buildin ops:
`kTfLiteBuiltinResizeNearestNeighbor = 97,
  kTfLiteBuiltinLeakyRelu = 98,
  kTfLiteBuiltinSquaredDifference = 99,
  kTfLiteBuiltinMirrorPad = 100,`
"
38062,A bug: TFLite post-training quantization with calibration,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): r2.2

**Command used to run the converter or code if you’re using the Python API**
```
with qauntize_scope():
  converter = tf.lite.TFLiteConverter.from_keras_model(model)
  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
  converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]
  converter.representative_dataset = calibration_gen
  tflite_model = converter.convert()
```

**The output from the converter invocation**
```
Traceback (most recent call last):
  File ""test_minist_qat.py"", line 171, in <module>
    tflite_model = converter.convert()
  File ""/home/mumu/.miniconda3/envs/hyperfast/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 522, in convert
    result, constants.FLOAT, constants.FLOAT)
  File ""/home/mumu/.miniconda3/envs/hyperfast/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 267, in _calibrate_quantize_model
    inference_output_type, allow_float, self.experimental_new_quantizer)
AttributeError: 'TFLiteConverterV2' object has no attribute 'experimental_new_quantizer'
```

**Also, please include a link to the saved model or GraphDef**
None

**Failure details**
From commit [e6895b](https://github.com/tensorflow/tensorflow/commit/e6895b3648595080143e8f5dd6f56c16e7852e91), some experimental feature flags were made private.
And at line 267, it does not changed properly.
This introduces bugs when apply post-training quantization using TFLite converter.
(Manually fixing it works well)

**Any other info / logs**
None"
38061,GAN Training Loop overriding Model.train_step(),"Hello!

I was trying to wrap my head around how to make an Adversarial Training Loop for GANs using the recently feature of TF2.2.0, the ability to override Model.train_step(). Right now, i feel that is not possible because a GAN are two separate networks, but maybe there is some way of do this, that is cleaner than a Custom Training Loop.

My doubts come to my mind because right now im implementing a GAN for image Deblurring following the Paper of DeblurGANV2, and im feeling frustration because now the performance is poor and maybe i dont plan my implementation very good. I you wish to help me there, this is the repo: https://github.com/ElPapi42/deep-deblurring, im going crazy with this.


If this is a suitable way of creating such implementation, will be awesome to write docs about this."
38060,pixelwise-loss-weight map,"**System information**
- TensorFlow version (you are using): 1.14.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
U-NET (https://www.nature.com/articles/s41592-018-0261-2) has become a popular way to perform semantic segmentation for cell microscopy.  Instance segmentation requires pixelwise-loss weights.  Currently, model.fit_generator supports only val_sample_weights and class_weight--not pixelwise-loss weights.  

There have been several work-arounds proposed for this issue, and I have implemented almost everyone.  Most generated errors except 

https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/keras-users/ue1S8uAPDKU/PzL7Vb0bBQAJ

It kind of works, but the Jupyter notework keeps issuing warnings like

E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.

When I run it, the results change drastically from run to run.

**Will this change the current api? How?**
It will add more versatility to the api by incorporating an important aspect of computer vision.  

**Who will benefit with this feature?**
Those using keras to implement computer vision programs.

**Any Other info.**
Thank you kindly for your consideration."
38059,"Also show ""Available Since"" for each API in tf docs","It would be useful to show the lifecycle of an API in the docs. That is, show when say, `tf.data.Dataset.take` was added or when a certain is removed / renamed. It could be extended to arguments of each API as well. Or to put it in one line,
``` 
Expose version control information of APIs directly on tf docs
```
It would really help developers keep up with rapid development of TF even better."
38057,MaxPooling1D layer causes ESP32 to crash,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.4
- TensorFlow installed from (source or binary): Installed with pip `pip install --upgrade tensorflow`
- Tensorflow version (commit SHA if source): Version 2.1.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32

**Describe the problem**
Using a MaxPooling1D in my model causes the ESP32 to crash. However the model works fine when I remove the MaxPooling1D layer.

Here is the error from the exception:
```
Didn't find op for builtin opcode 'MAX_POOL_2D' version '2'
Failed to get registration from op code MAX_POOL_2D
AllocateTensors() failed
Guru Meditation Error: Core  1 panic'ed (LoadProhibited). Exception was unhandled.
```

**Please provide the exact sequence of commands/steps when you ran into the problem**

Here is my model:

```python
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_timestep,6)))
model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
model.add(Dropout(0.5))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dense(4, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=1e-3), metrics=['accuracy'])
```

To convert the model:

```python
converter = lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]
converter.optimizations = [lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
tfmodel = converter.convert()
open(PATH+'/model.tflite',""wb"").write(tfmodel)
```

For the ops resolver I am using AllOpsResolver:
`static tflite::ops::micro::AllOpsResolver resolver;`

If I look in the file all_ops_resolver.cc there is no min/max version for MAX_POOL_2D:
`AddBuiltin(BuiltinOperator_MAX_POOL_2D, Register_MAX_POOL_2D());`

Best regards,
Victor Douet"
38056,Error with tensorflow 2: Inputs to eager execution function cannot be Keras symbolic tensors,"Hi everyone,

The tensorflow 2 release notes request that an issue be filed when experiencing problems with the new single path execution code.

I regularly work with custom loss functions that require additional information other than the predictors and observed outcomes. A simple example is estimating a general binomial regression model, where the number of trials are part of the likelihood/loss function, but they are not part of the predictors or observed outcome. Sample R code is provided in https://github.com/rstudio/keras/issues/1008

Would you please make available a permanent option in tensorflow 2 to pass additional inputs layers into custom loss functions so that we can keep using tensorflow to estimate such models? I guess that setting `experimental_run_tf_function=False` is only a temporary fix.

Thank you"
38055,PiecewiseConstantDecay Keras Learning Rate Scheduler not compatible with XLA Compilation,"**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

The Keras PieceWiseConstant Decay scheduler currently causes an 'unsupport op' error when used with XLA compilation (i.e. when used with `@tf.function(experimental_compile=True)`).  This happens because [it calls the control_flow_ops `case` function with `exclusive=True`](https://github.com/tensorflow/tensorflow/blob/e3baa0f5b6cb77cbd312f0ad89e46829c073b1ec/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py#L258), which eventually results in [an assertion that is incompatible with XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L3194).

I would like to add an additional `validate_args=True` parameter to PiecewiseConstantDecay that can be optionally set to False to forgo setting `exclusive=True`, thus making the operation TPU-friendly ([similar to the approach taken elsewhere](https://github.com/tensorflow/probability/pull/771)).

**Will this change the current api? How?**

This will not change beahviour of any existing code, but will provide an additional parameter that can be used to leverage XLA compatibility for running on TPU.

**Who will benefit with this feature?**

All users of Cloud TPUs who would like to make complete use of Keras learning rate schedulers.

**Any Other info.**

N/A"
38054,Unable to access files on S3 with tf.io.gfile.GFile,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): MacOS Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 2.0.0
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Executing the following code: 

```
with tf.io.gfile.GFile(""s3://path/to/my/file"", mode=""r"") as f:
    data = f.read()
```

results in the following error message:

```
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/Users/neelabh/opt/anaconda3/envs/tftrt/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py"", line 124, in read
    length = self.size() - self.tell()
  File ""/Users/neelabh/opt/anaconda3/envs/tftrt/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py"", line 102, in size
    return stat(self.__name).length
  File ""/Users/neelabh/opt/anaconda3/envs/tftrt/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py"", line 727, in stat
    return stat_v2(filename)
  File ""/Users/neelabh/opt/anaconda3/envs/tftrt/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py"", line 744, in stat_v2
    pywrap_tensorflow.Stat(compat.as_bytes(path), file_statistics)
tensorflow.python.framework.errors_impl.NotFoundError: Object s3://[REDACTED]/train.txt does not exist
```


**Describe the expected behavior**
The contents of the file should be written to variable `data`

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

This points to a public s3 file, but still fails:

https://colab.research.google.com/drive/1VSlfzRPdFNSGGI8wd6RdhH9uFj7fkaFG

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
2020-03-30 23:22:36.465054: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /Users/neelabh//.aws/config and using profilePrefix = 1
2020-03-30 23:22:36.465103: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /Users/neelabh//.aws/credentials and using profilePrefix = 0
2020-03-30 23:22:36.465129: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /Users/neelabh//.aws/credentials for credentials file and /Users/neelabh//.aws/config for the config file , for use with profile default
2020-03-30 23:22:36.465207: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating AWSHttpResourceClient with max connections2 and scheme http
2020-03-30 23:22:36.465336: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2
2020-03-30 23:22:36.465391: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 300000
2020-03-30 23:22:36.465427: I tensorflow/core/platform/s3/aws_logging.cc:54] Added EC2 metadata service credentials provider to the provider chain.
2020-03-30 23:22:36.465682: I tensorflow/core/platform/s3/aws_logging.cc:54] Successfully reloaded configuration.
2020-03-30 23:22:36.465889: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25
2020-03-30 23:22:36.466559: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2
2020-03-30 23:22:36.466586: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2020-03-30 23:22:37.536377: E tensorflow/core/platform/s3/aws_logging.cc:60] HTTP response code: 301
Exception name: 
Error message: No response body.
7 response headers:
content-type : application/xml
date : Mon, 30 Mar 2020 17:52:37 GMT
server : AmazonS3
transfer-encoding : chunked
x-amz-bucket-region : eu-north-1
x-amz-id-2 : 021phnKX0e6e9R+N9sMrXZHViGoHdzJrTT5rnyHyWsP8d9ErkPMZT02RbTZcjVeCVrI/3hDkWk8=
x-amz-request-id : 7DAC97A633CA370B
2020-03-30 23:22:37.536443: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2020-03-30 23:22:37.536681: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2020-03-30 23:22:37.818954: W tensorflow/core/platform/s3/aws_logging.cc:57] Encountered Unknown AWSError 'PermanentRedirect': The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2020-03-30 23:22:37.819073: E tensorflow/core/platform/s3/aws_logging.cc:60] HTTP response code: 301
Exception name: PermanentRedirect
Error message: Unable to parse ExceptionName: PermanentRedirect Message: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
7 response headers:
content-type : application/xml
date : Mon, 30 Mar 2020 17:52:37 GMT
server : AmazonS3
transfer-encoding : chunked
x-amz-bucket-region : eu-north-1
x-amz-id-2 : ob9TL15Q4Y7/idUzUTWvurB3Z4nxfVYRV2V+9ly88HrVGuHytuZA1U02rhcL0vFUpv83vUxeO9o=
x-amz-request-id : 51E3695245C81463
2020-03-30 23:22:37.819138: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/Users/neelabh/opt/anaconda3/envs/tftrt/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py"", line 124, in read
    length = self.size() - self.tell()
  File ""/Users/neelabh/opt/anaconda3/envs/tftrt/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py"", line 102, in size
    return stat(self.__name).length
  File ""/Users/neelabh/opt/anaconda3/envs/tftrt/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py"", line 727, in stat
    return stat_v2(filename)
  File ""/Users/neelabh/opt/anaconda3/envs/tftrt/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py"", line 744, in stat_v2
    pywrap_tensorflow.Stat(compat.as_bytes(path), file_statistics)
tensorflow.python.framework.errors_impl.NotFoundError: Object s3://[REDACTED]/train.txt does not exist
```"
38053,high thread count,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information** 
- Custom inference application written in Python.
- ubuntu 18.04.3, latest
- TensorFlow 1.14
- Python version:  3.7
- CUDA/cuDNN version:10.2
 - GPU. Nvidia 5 x T4 16G.

This is more of a question related to performance/effeciency  I'm in charge of setting up Ubuntu. I've been ask by a software developper to increase linux DefaultTaskMax to 20000.  The rationnal is that tensorflow create 2000 threads per GPU, there is about 20  models uploaded on 1 card. Yet 2000 seems like a totaly crazy number of threads.  My gut feeling tells me something must be wrong. That tensor flow is set up or used wrong.  I have been trying to find information about this with no luck. I also tried to find information on the effect of increasing DefaultTaskMax on kernel overhead.

Regards,
- Mario
"
38047,Launch tensorboard in colab,"i try launch tensorboard on colab, my code:

```
LOG_DIR = model_dir
get_ipython().system_raw(
    'tensorboard --logdir {} --host 0.0.0.0 --port 6060 &'
    .format(LOG_DIR)
)

get_ipython().system_raw('./ngrok http 6060 &')

! curl -s http://localhost:4040/api/tunnels | python3 -c \
    ""import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])""
```
two days ago everything worked, but now such an error:

[error](http://puu.sh/FqU8i/62b7eec918.jpg)"
38045,Unable to set top_k,"I downloaded this example file in order to execute detection using tensorflow lite as described in the [Coral edgetpu example](https://coral.ai/docs/edgetpu/tflite-python/#load-tensorflow-lite-and-run-an-inference).
I executed on a lots of image and i get **ALWAYS 10 detections** even if i lower the threshold to 0.
I want to upper this limit (during my research i found something like a top_k variable set somewhere in the modules).
How do i change it from python?

The example file is [here](https://github.com/tensorflow/tensorflow/blob/82ff6702ab409a88bfb070e5f3e8749e5865c303/tensorflow/lite/examples/python/label_image.py#L62)"
38044,Distributed tensorflow：Does every worker also have all tf.Variable parameters?,"Execute the following code on the two workers, and the printed values are different, indicating that each worker has parameters, so is ps sever only used to update parameters?
How should the parameters of the PS server be stored in the specified GPU?
# task0: 140340378338864 140340378339144 140340378449344
# task1: 140099479135792 140099479136072 140099479246272

worker_device = ""/job:worker/task:{i}"".format(i=FLAGS.task_index)
 with tf.device(tf.train.replica_device_setter(
            worker_device=worker_device,
            cluster=cluster
        )):
            x = tf.Variable([[1, 2, 3, 4]])
            w = tf.Variable([[2], [2], [2], [2]])
            mat = tf.matmul(x, w)
            print(id(x), id(w), id(mat))
"
38043,TextVectorizationLayer supporting custom padding token,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): v2.2.0-rc2
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

When using the `tf.keras.layers.experimental.preprocessing.TextVectorization` we are not able to specify a padding token which should map to the value 0. When we have a sequence such as:

```
tf.constant(['<PAD> word1 word2 OOV <EOS>'], dtype=tf.string)
```
in which the `<PAD>` is the padding token. If the TextVectorization  `output_sequence_length` is bigger than the size of the previous sequence (5), for example 10, we would get:

```
vectorize_layer(tf.expand_dims(tf.constant(['<PAD> word1 word2 OOV <EOS>'], dtype=tf.string), axis=0))

`<tf.Tensor: shape=(1, 10), dtype=int64, numpy=array([[5,   2,   3, 1,  6,   0,   0,   0,   0,   0]])>`
```
(the oov token maps to 1, while the `<PAD>` token has a value different than 0 which is the default padding value by the layer). In my preprocessing I am generating sequences of the same size in each batch and the `<PAD>` token only appears at the beginning, so the model is oblivious to the 0 values and only sees the 5's as the padding token ID. However, it would be nice if we only had one token ID for the padding.

I overrided this behavior in tf v2.1.0 by doing the following:

```
vectorize_layer.adapt(tf.data.TextLineDataset('train.txt').batch(32))
pad_token_vocab = vectorize_layer._convert_to_ndarray(['<PAD>'])
pad_token_value = vectorize_layer._convert_to_ndarray([0])
vectorize_layer._insert_table_data(pad_token_vocab, pad_token_value)
vectorize_layer.set_vocabulary[('<EOS>'], append=True)
```
However, there were major changes from v2.1.0 to v.2.2-rc2 so this is no longer possible. 

**Will this change the current api? How?** Adding an optional argument: padding_token.

**Who will benefit with this feature?** I believe it's a nice to have for everyone training language models.

**Any Other info.**
"
38042,saved_model_cli broken on 2.2rc,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): no
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): all platforms (Linux, Win, MacOS)
- TensorFlow installed from binary
- TensorFlow version == 2.2rc[012]
- Python version: 3.6

Hi,

`saved_model_cli` appears to be broken on all platforms and all versions, except for rc0, linux.

Simply trying to use `saved_model_cli` from the command line yields on all versions and all platforms:
```
Traceback (most recent call last):
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\marco\repos\tfutils\venv\Scripts\saved_model_cli.exe\__main__.py"", line 5, in <module>
  File ""c:\users\marco\repos\tfutils\venv\lib\site-packages\tensorflow\python\tools\saved_model_cli.py"", line 51, in <module>
    from tensorflow.python.tools import saved_model_aot_compile
ImportError: cannot import name 'saved_model_aot_compile' from 'tensorflow.python.tools' (c:\users\marco\repos\tfutils\venv\lib\site-packages\tensorflow\python\tools\__init__.py)
```
Linux 2.2rc0 appears to be the only exception:
```
usage: saved_model_cli [-h] [-v] {show,run,scan,convert,aot_compile_cpu} ...
saved_model_cli: error: too few arguments
```

Best Regards,

Marco

"
38041,Using SavedModels with low-level API in TF 2.x,"I am not exactly sure whether this is a bug but it seems so

**System information** 
* I have custom code
* OS: Windows
* Tensorflow 2.1.0, Conda MKL version
* Python 3.7

**Describe the current behavior**
I am developing a system that uses Tensorflow SavedModel format to serve models in a custom embedded environment. So I want to create a basic computational graph and move on from there to test my toolchain and find bugs easily. For that I tried to create a simple graph as below:

**Standalone code to reproduce the issue** 

```python3
import tensorflow as tf
import tensorflow_core as tfcore
import pathlib

graph = tf.Graph()

with graph.as_default():
    A = tf.raw_ops.Placeholder(dtype=tf.dtypes.float32, shape=None, name=""A"")
    B = tf.raw_ops.Placeholder(dtype=tf.dtypes.float32, shape=None, name=""B"")
    Result = tf.raw_ops.Add(x=A, y=B, name=""Result"")


with tfcore.python.Session(graph=graph) as sess:
    script_dir = pathlib.Path(__file__).resolve().parent
    builder = tfcore.python.saved_model.builder.SavedModelBuilder(str(script_dir / ""saved_model""))
    save_signature = tfcore.python.saved_model.signature_def_utils.predict_signature_def(
        inputs={""A"": A, ""B"": B},
        outputs={""Result"": Result})
    builder.add_meta_graph_and_variables(sess=sess,
                                         signature_def_map={""predict"": save_signature},
                                         tags=[""test_tag""],
                                         main_op=Result)
    builder.save(as_text=True) 
```

However this throws an exception when executed:

```
2020-03-30 13:10:56.894798: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-03-30 13:10:56.897115: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:From C:\Users\ongun\miniconda3\envs\npu_tf\lib\site-packages\tensorflow_core\python\saved_model\signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.
Traceback (most recent call last):
  File ""C:/Users/ongun/code/02_npu/simple_tf_model/sum_graph.py"", line 25, in <module>
    main_op=Result)
  File ""C:\Users\ongun\miniconda3\envs\npu_tf\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\ongun\miniconda3\envs\npu_tf\lib\site-packages\tensorflow_core\python\saved_model\builder_impl.py"", line 582, in add_meta_graph_and_variables
    main_op = main_op or legacy_init_op
  File ""C:\Users\ongun\miniconda3\envs\npu_tf\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 757, in __bool__
    self._disallow_bool_casting()
  File ""C:\Users\ongun\miniconda3\envs\npu_tf\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 526, in _disallow_bool_casting
    self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
  File ""C:\Users\ongun\miniconda3\envs\npu_tf\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 515, in _disallow_in_graph_mode
    "" this function with @tf.function."".format(task))
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
```

I traced the bug a little bit and the line https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder_impl.py#L537 seems to cause the error by trying to treat  `main_op` as a boolean.

A simple fix seems to be viable by changing the line to:

```python3
main_op = main_op if main_op is not None else legacy_init_op
```

I can create a PR if that's an appropriate fix."
38039,flatbuffer.shape() might return nullptr,"For certain tensors (e.g. multiply with a scalar (constant) tensor, such as 1/6 at the output of hard sigmoid), the shape attribute might be None/undefined and hence, flatbuffer.shape() returns a nullptr.

In this code
https://github.com/tensorflow/tensorflow/blob/59c06b9016700dbf1ab0cefc062d247345cdd0f0/tensorflow/lite/micro/memory_helpers.cc#L82
this nullptr is dereferenced unconditionally."
38038,Custom Metrics and Losses: AttributeError: 'Tensor' object has no attribute 'numpy' raised during training,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes 
- OS Platform and Distribution: Linux Ubuntu 18.04 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:  No
- TensorFlow installed from (source or
binary): pip
- TensorFlow version (use command below): 2.1.0
- Python version: - Bazel
version (if compiling from source): 3.6.9 64-bit
- GCC/Compiler version (if compiling from
source): Ubuntu 7.5.0-3ubuntu1~18.04
- CUDA/cuDNN version: - GPU model and memory: No

**Describe the current behavior**

I am trying to implement a custom metric function as well as a custom loss function. Both implementations are face the same issue, so I am going to focus this post in just one of them.

As an example, we have the dummy code below. The current behaviour is 
`AttributeError: 'Tensor' object has no attribute 'numpy'`.
The full log is also shown below. 

**Describe the expected behavior**

My goal is to access the value of a tensor during the fit method in order to make calculations based on said values stored in both `y_true `and `y_pred`. **These calculations cannot be done using built-in Keras backend functions.**

I wrote this dummy function test just to illustrate the issue. If only `tf.print` is used, the code runs and the values in the tensors are printed on `stdout` after the fit is done. However, if I try something like `y_true.numpy()` or `print(y_true.numpy())` the code returns

`AttributeError: 'Tensor' object has no attribute 'numpy'`

I have tried several methods from several StackOverflow and Github threads (e.g. #27519, #36979), including combinations of `sess = tf.Session()` with `.eval()`, `tf.GradientTape`, but somehow failed to implement any of them successfully.

Does anyone know how to solve this problem?

**Standalone code to reproduce the issue** 

    import numpy as np
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Input, LSTM, Dense
    from tensorflow.keras.metrics import Metric

    x, y = list(), list()
    for _ in range(10):
        x.append(np.arange(10))
        y.append(np.random.randint(0, 2))

    x = np.reshape(x, (len(x), 1, len(x[0])))
    y = np.asarray(y)

    print(tf.convert_to_tensor(x).numpy())

    class custom_metric(Metric):
        def __init__(self, name = 'custom_metrics', **kwargs):
            super(custom_metric, self).__init__(name = name, **kwargs)
            self.true_positives = self.add_weight(name = 'tp', initializer = 'zeros')

        def update_state(self, y_true, y_pred, sample_weight = None):
            self.test(y_true, y_pred)
            # In a real application, new_metric would be a function that depends on
            # the values stored in both y_true and y_pred 
            new_metric = 0.1 
            self.true_positives.assign_add(tf.reduce_sum(new_metric))

        def result(self):
            return self.true_positives

        def reset_states(self):
            self.true_positives.assign(0.)

        def test(self, y_true, y_pred):
            tf.print(y_true)
            print(y_true.numpy())

    model = Sequential([
        LSTM(5,
             input_shape = (np.asarray(x).shape[1], np.asarray(x).shape[2]),
             return_sequences = True,
             recurrent_initializer = 'glorot_uniform',
             activation = 'tanh',
             recurrent_dropout = 0.2,
             dropout = 0.2
        ),
        Dense(2, activation = 'softmax')
    ])

    model.compile(
        optimizer = 'adam',
        loss = 'sparse_categorical_crossentropy',
        metrics = ['sparse_categorical_accuracy', custom_metric()]
    )

    model.run_eagerly = True

    model.fit(
        x, y,
        epochs = 1,
        batch_size = 1
    )`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

    array([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],

           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],

           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],

           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],

           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],

           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],

           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],

           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],

           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],

           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])
    ---------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)
    ~/path/to/file.py in 
         54     optimizer = 'adam',
         55     loss = 'sparse_categorical_crossentropy',
    ---> 56     metrics = ['sparse_categorical_accuracy', custom_metric()]
         57 )
         58 model.run_eagerly = True

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
        455     self._self_setattr_tracking = False  # pylint: disable=protected-access
        456     try:
    --> 457       result = method(self, *args, **kwargs)
        458     finally:
        459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
        437           targets=self._targets,
        438           skip_target_masks=self._prepare_skip_target_masks(),
    --> 439           masks=self._prepare_output_masks())
        440 
        441       # Prepare sample weight modes. List with the same length as model outputs.

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _handle_metrics(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)
       2002           metric_results.extend(
       2003               self._handle_per_output_metrics(self._per_output_metrics[i],
    -> 2004                                               target, output, output_mask))
       2005         if return_weighted_and_unweighted_metrics or return_weighted_metrics:
       2006           metric_results.extend(

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights)
       1953       with K.name_scope(metric_name):
       1954         metric_result = training_utils.call_metric_function(
    -> 1955             metric_fn, y_true, y_pred, weights=weights, mask=mask)
       1956         metric_results.append(metric_result)
       1957     return metric_results

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in call_metric_function(metric_fn, y_true, y_pred, weights, mask)
       1153 
       1154   if y_pred is not None:
    -> 1155     return metric_fn(y_true, y_pred, sample_weight=weights)
       1156   # `Mean` metric only takes a single value.
       1157   return metric_fn(y_true, sample_weight=weights)

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/metrics.py in __call__(self, *args, **kwargs)
        194     from tensorflow.python.keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top
        195     return distributed_training_utils.call_replica_local_fn(
    --> 196         replica_local_fn, *args, **kwargs)
        197 
        198   @property

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)
       1133     with strategy.scope():
       1134       return strategy.extended.call_for_each_replica(fn, args, kwargs)
    -> 1135   return fn(*args, **kwargs)
       1136 
       1137 

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/metrics.py in replica_local_fn(*args, **kwargs)
        177     def replica_local_fn(*args, **kwargs):
        178       """"""Updates the state of the metric in a replica-local context.""""""
    --> 179       update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable
        180       with ops.control_dependencies([update_op]):
        181         result_t = self.result()  # pylint: disable=not-callable

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)
         74 
         75     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):
    ---> 76       update_op = update_state_fn(*args, **kwargs)
         77     if update_op is not None:  # update_op will be None in eager execution.
         78       metric_obj.add_update(update_op)

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
        566         xla_context.Exit()
        567     else:
    --> 568       result = self._call(*args, **kwds)
        569 
        570     if tracing_count == self._get_tracing_count():

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
        613       # This is the first call of __call__, so we have to initialize.
        614       initializers = []
    --> 615       self._initialize(args, kwds, add_initializers_to=initializers)
        616     finally:
        617       # At this point we know that the initialization is complete (or less

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
        495     self._concrete_stateful_fn = (
        496         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    --> 497             *args, **kwds))
        498 
        499     def invalid_creator_scope(*unused_args, **unused_kwds):

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
       2387       args, kwargs = None, None
       2388     with self._lock:
    -> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)
       2390     return graph_function
       2391 

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
       2701 
       2702       self._function_cache.missed.add(call_context_key)
    -> 2703       graph_function = self._create_graph_function(args, kwargs)
       2704       self._function_cache.primary[cache_key] = graph_function
       2705       return graph_function, args, kwargs

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
       2591             arg_names=arg_names,
       2592             override_flat_arg_shapes=override_flat_arg_shapes,
    -> 2593             capture_by_value=self._capture_by_value),
       2594         self._function_attributes,
       2595         # Tell the ConcreteFunction to clean up its graph once it goes out of

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
        976                                           converted_func)
        977 
    --> 978       func_outputs = python_func(*func_args, **func_kwargs)
        979 
        980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
        437         # __wrapped__ allows AutoGraph to swap in a converted function. We give
        438         # the function a weak reference to itself to avoid a reference cycle.
    --> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)
        440     weak_wrapped_fn = weakref.ref(wrapped_fn)
        441 

    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
        966           except Exception as e:  # pylint:disable=broad-except
        967             if hasattr(e, ""ag_error_metadata""):
    --> 968               raise e.ag_error_metadata.to_exception(e)
        969             else:
        970               raise

    AttributeError: in converted code:

    ~/path/to/file.py:7 update_state  *
        self.test(y_true, y_pred)
    ~/path/to/file.py:20 test  *
        y_true.numpy()

    AttributeError: 'Tensor' object has no attribute 'numpy'
"
38037,Unable to cross compile TFLite for Raspberry Pi Zero  using Docker image nightly-develop,"
**System information**
- OS Platform and Distribution:  Linux Mint 19:03 (based on Ubuntu 18:04)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A ?  Attempting to crosscompile tensorflowlite for Raspbery Pi Zero (Armv6) 
- TensorFlow installed from (source or binary): nightly-develop  docker image
- TensorFlow version: 2.2
- Python version: 2.7 
- Installed using virtualenv? pip? conda?: docker file from latest nightly build
- Bazel version (if compiling from source): bazel 0:15:0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: none

When cross compiling for raspberry pi zero (ARMv6), as per instructions on following page [https://www.tensorflow.org/lite/guide/build_rpi](https://www.tensorflow.org/lite/guide/build_rpi)
and running the command indicated to build for the ARMv6 architecture, 
`./tensorflow/lite/tools/make/build_rpi_lib.sh TARGET_ARCH=armv6`

  there are errors indicating that the compiler options set are incorrect

/usr/arm-linux-gnueabihf/include/bits/stdio.h: In function 'int getchar()':
/usr/arm-linux-gnueabihf/include/bits/stdio.h:44:14: sorry, unimplemented: Thumb-1 hard-float VFP ABI
 getchar (void)

**Any other info / logs**

if I examine the file 

`./tensorflow/lite/tools/make/build_rpi_lib.sh
`
I found that the TARGET_ARCH parameter was hard coded for armv7 in the final line of the file

If I edit the file so that this reads

`CC_PREFIX=arm-linux-gnueabihf- make -j 3 -f tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv6
`
(the hardcoding is not ideal for obvious reasons )
 and then change the armv6 compiler options in

`tensorflow/lite/tools/make/targets/rpi_makefile.inc
`
to the following

```
ifeq ($(TARGET_ARCH), armv6)
CXXFLAGS +=
-marm
-mfpu=vfp
-mlong-calls
-mthumb-interwork
-mfloat-abi=hard

CCFLAGS += \
  -marm \
  -mfpu=vfp \
  -mlong-calls \
  -mthumb-interwork \
  -mfloat-abi=hard

LDFLAGS := \
  -Wl,--no-export-dynamic \
  -Wl,--exclude-libs,ALL \
  -Wl,--gc-sections \
  -Wl,--as-needed \
  -mfpu=vfp \
  -mlong-calls \
  -mfloat-abi=hard \
  -mthumb-interwork 
endif

```
I can compile without errors (and just a couple of warnings).

Given that it seems to be difficult (or just very slow) to compile bazel directly on a raspberry pi zero due to the lack of memory and that increasing the memory available (even via machine virtualisation such as QEMU appears impossible), I think that for deployment on a PI Zero, cross compiling is the only way forward. At the moment I can inference, but only by using a full tensorflow implementation, which is really too slow for my needs) 

Additionally it would be nice to have a way to generate a python wheel as well. The main ci_build process does this for the full fat tensorflow package, (and i can run one of those on the pi zero), but I'd like to just use tensorflow-lite for my inferencing, as i think any performance improvements that I can get would be beneficial. I'm using a Pi Zero for it's low power consumption for energy efficiency reasons, so swapping out to a PI 3 or Pi 4 would not really be viable for my project. 

thanks for your help


"
38036,java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: Nokia 7.1
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): tf-nightly 2.2-dev
- Python version: - Bazel 3.7
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory: Mobile gpu Adreno 506

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I am trying to use the following [model code here](https://stackoverflow.com/a/60869999/8030107) and have made a tensorflow lite model, when we run it on android we get the following error using the GPU delegate:

```
java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
```

And when we switch to the CPU version we get the following error:

```

 Cannot convert between a TensorFlowLite tensor with type FLOAT32 and a Java object of type [I (which is compatible with the TensorFlowLite type INT32).
------------------------------------------------
whole error -> 2020-03-30 12:26:15.613 3471-3471/com.valuepitch.intruderdetector W/System.err: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite tensor with type FLOAT32 and a Java object of type [I (which is compatible with the TensorFlowLite type INT32).
2020-03-30 12:26:15.614 3471-3471/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.Tensor.throwIfTypeIsIncompatible(Tensor.java:316)
2020-03-30 12:26:15.614 3471-3471/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.Tensor.getInputShapeIfDifferent(Tensor.java:218)
2020-03-30 12:26:15.614 3471-3471/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:137)
2020-03-30 12:26:15.614 3471-3471/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:311)
```

**Describe the expected behavior**

I expect the model to take 3 arguments and ouput the model outputs. I've seen similar issue [here](https://github.com/tensorflow/tensorflow/issues/25131) , but couldn't comment there for unknown reasons or might be because the issue was closed? No clue as to that.

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Pls find colab notebook [here](https://drive.google.com/file/d/1MN4-FX_-hz3y-UAuf7OTj_XYuVTlsSTP/view?usp=sharing)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
38035,Couldn't match files for checkpoint,"Tensorflow can't load weight from a folder with a `+` symbol such as `a+b`, but if I simply change the folder name to `a`, it works well. See the [notebook](https://colab.research.google.com/drive/1CrWIySL6kofS7CEJgN72l8yrpgohIavJ) to reproduce this issue yourself.

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): no
- OS Platform and Distribution:  MacOS 10.15.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: no
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below):  v2.2.0-rc0-43-gacf4951a2f 2.2.0-rc1
- Python version: Python 3.7.7 (default, Mar 10 2020, 15:43:33)



**Describe the current behavior**

```
>>> model.load_weights(""data/dev/checkpoint/a+b/cp1.ckpt"")
Traceback (most recent call last):
  File ""/Users/izhangzhihao/Library/Caches/pypoetry/virtualenvs/ve-0MKN22N3-py3.7/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 95, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for data/dev/checkpoint/a+b/cp1.ckpt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/izhangzhihao/Library/Caches/pypoetry/virtualenvs/ve-0MKN22N3-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 249, in load_weights
    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)
  File ""/Users/izhangzhihao/Library/Caches/pypoetry/virtualenvs/ve-0MKN22N3-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1226, in load_weights
    py_checkpoint_reader.NewCheckpointReader(filepath)
  File ""/Users/izhangzhihao/Library/Caches/pypoetry/virtualenvs/ve-0MKN22N3-py3.7/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 99, in NewCheckpointReader
    error_translator(e)
  File ""/Users/izhangzhihao/Library/Caches/pypoetry/virtualenvs/ve-0MKN22N3-py3.7/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator
    raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for data/dev/checkpoint/a+b/cp1.ckpt
```

**Describe the expected behavior**

```
>>> model.load_weights(""data/dev/checkpoint/a/cp1.ckpt"")
<tensorflow.python.training.tracking.util.CheckpointLoadStatus object at 0x148b5c250>
```

**Standalone code to reproduce the issue** 

https://colab.research.google.com/drive/1CrWIySL6kofS7CEJgN72l8yrpgohIavJ
"
38034,Build failed on Windows with -c dbg,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: branch master 2.2
- Python version: 3.7.7 Debug
- Bazel version (if compiling from source): 2.0
- GCC/Compiler version (if compiling from source): MSVC 19 ( 14.25.28610 )
- CUDA/cuDNN version: No
- GPU model and memory: No



**Describe the problem**
```
grpc_python_plugin.exe has stopped can not seek string iterator after end 
protoc-gen-grpc: Plugin failed with status code 3221226505.
```
it failed when try to build
```
ProtoCompile tensorflow/core/profiler/profiler_analysis_pb2.py; 15s local
ProtoCompile tensorflow/core/debug/debug_service_pb2.py; 15s local
ProtoCompile tensorflow/core/profiler/profiler_service_pb2.py; 15s local
```
<img width=""478"" alt=""image"" src=""https://user-images.githubusercontent.com/17511625/77883331-1ebed700-728d-11ea-8e77-8c5f7e7dcfb2.png"">


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel build --config=opt -c dbg --jobs=8 --cxxopt=""/wd4716"" //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
38033,tflite_runtime pip package compiled from source does not run,"**System information** 
- OS Platform and Distribution: 
Ubuntu 18.04 running in Windows 10 WSL
- TensorFlow installed from: 
Binary, however the tflite_runtime installed from source.
- TensorFlow version:  2.1.0
- Python version: 3.6
 - GCC/Compiler version: 7.4.0

**Describe the current behavior**
Compiled the tflite_runtime from master using:
`./tensorflow/lite/tools/pip_package/build_pip_package.sh`
and installed:
`pip install --upgrade tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.1.0-cp36-cp36m-linux_x86_64.whl`
Trying to run load an tflite  file:
`python -c ""from tflite_runtime.interpreter import Interpreter; Interpreter('foo.tflite')""`
gives the following error:
`Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/fresve01/.local/lib/python3.6/site-packages/tflite_runtime/interpreter.py"", line 203, in __init__
    _interpreter_wrapper.CreateWrapperFromFile(
AttributeError: module 'tflite_runtime.interpreter_wrapper' has no attribute 'CreateWrapperFromFile'`

**Describe the expected behavior**
The tflite file should load without error.
Also the README.md file with build instructions should be updated since there is no swig dependency any more.

**Standalone code to reproduce the issue** 
`./tensorflow/lite/tools/pip_package/build_pip_package.sh`
`pip install --upgrade tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.1.0-cp36-cp36m-linux_x86_64.whl`
`python -c ""from tflite_runtime.interpreter import Interpreter; Interpreter('foo.tflite')""`"
38032,"When I replace fit_generator with fit, behavior is inconsistent.","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Windows 10
- TensorFlow installed from (source or
binary): conda - TensorFlow version (use command below): 2.0.0
- Python version: 3.7.6

You can collect some of this information using our environment capture
unknown 2.0.0

**Describe the current behavior**
I train a multi-outputs model with a custom data generator. It runs successfully with the api(model.fit_generator). But when I swap to model.fit, it is broken. 

I found that 'fit' can not handle 'multi outputs in list' (such as `yield x, [y1, y2, ...]`) correctly, but tuple(such as `yield x, (y1, y2, ...)`) is ok.

**Describe the expected behavior**
I think both fit_generator and fit should have a consistent behavior to a same generator.

**Standalone code to reproduce the issue** 

```
import numpy as np
from tensorflow.keras import layers, optimizers, losses, Model, Input

inputs = Input(shape=(10,), name='img_input')

x1 = layers.Dense(5)(inputs)
x2 = layers.Dense(2)(inputs)

model = Model(inputs=inputs,
                    outputs=[x1, x2])

model.compile(
    optimizer=optimizers.Adam(),
    loss=losses.categorical_crossentropy)


img_data = np.random.random_sample(size=(1, 10))
targets_0 = np.random.random_sample(size=(1, 5))
targets_1 = np.random.random_sample(size=(1, 2))

def generator_tuple():

    while True:
        yield img_data, (targets_0, targets_1)


def generator_list():
    while True:
        yield img_data, [targets_0, targets_1]

model.fit_generator(generator_tuple(), steps_per_epoch=1, epochs=3) # ok
model.fit(generator_tuple(), steps_per_epoch=1, epochs=3) # ok
model.fit_generator(generator_list(), steps_per_epoch=1, epochs=3) # ok
model.fit(generator_list(), steps_per_epoch=1, epochs=3) # raise error
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""C:/Users/yuyang/Documents/codehub/yolov3/bug.py"", line 40, in <module>
    model.fit(generator_list(), steps_per_epoch=1, epochs=3) # ok
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 224, in fit
    distribution_strategy=strategy)
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 547, in _process_training_inputs
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 606, in _process_inputs
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\engine\data_adapter.py"", line 566, in __init__
    reassemble, nested_dtypes, output_shapes=nested_shape)
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 540, in from_generator
    output_types, tensor_shape.as_shape, output_shapes)
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\data\util\nest.py"", line 471, in map_structure_up_to
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\data\util\nest.py"", line 471, in <listcomp>
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py"", line 1216, in as_shape
    return TensorShape(shape)
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py"", line 776, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py"", line 776, in <listcomp>
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py"", line 718, in as_dimension
    return Dimension(value)
  File ""C:\Users\yuyang\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py"", line 193, in __init__
    self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'
```
"
38030,Wrong usage of ```tf.keras.layers.Layer._maybe_build```,"The function [```tf.keras.layers.Layer._maybe_build```](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L2357-L2394) is used inappropriately in some functions inside tensorflow implementation.

For example, [```tf.keras.layers.Layer.compute_output_shape```](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L663-L705) calls [```tf.keras.layers.Layer._maybe_build```](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L2357-L2394) with ```input_shapes``` argument([*1](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L687)), where it's supposed to be ```inputs```.

[```tf.keras.layers.Layer._maybe_build```](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L2357-L2394) tries to detect input shape by accessing ```inputs.shape``` attribute([*2](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L2371)), which doesn't exist because ```inputs``` is not a tensor but ```input_shape``` already, hence this function doesn't work as expected. This incident is eventually reported to a user as layer output shape inference failure and suggests overriding ```compute_output_shape``` function.

Although this error can be solved by overriding ```compute_output_shape``` as suggested by the error message, there're many cases where users don't need to do so if the functions are working properly.

Possible ways to solve this circumstance I can think of are

1. Add keyword argument ```input_shape``` to ```_maybe_build``` function so that they can receive either ```inputs``` or ```input_shapes```.

2. Create a dummy tensor inside ```compute_output_shape``` which can be used as an input argument to ```_maybe_build``` function.

I think either way won't take that much time to implement, so I hope this can be fixed soon.
If developers in the TensorFlow team are busy and can't spare time for this issue, I can work on this and issue a pull request. In that case, please give me some suggestions or opinions regarding the implementation or the modification, so that it can be approved and merged smoothly.

Thank you."
38028,[tflite] Build tflite using ./build_lib.sh failed!,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux16.04 
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2
- GCC/Compiler version (if compiling from source):  g++ 5.4

**Describe the problem**

Build tflite using ./build_lib.sh failed!

```sh
tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a(cpu_backend_context.o): In function `tflite::CpuBackendContext::~CpuBackendContext()':
cpu_backend_context.cc:(.text+0x3e8): undefined reference to `ruy::detail::SystemAlignedFree(void*)'
cpu_backend_context.cc:(.text+0x43d): undefined reference to `ruy::detail::SystemAlignedFree(void*)'
cpu_backend_context.cc:(.text+0x445): undefined reference to `ruy::detail::SystemAlignedAlloc(long)'
cpu_backend_context.cc:(.text+0x470): undefined reference to `ruy::detail::SystemAlignedFree(void*)'
cpu_backend_context.cc:(.text+0x48e): undefined reference to `ruy::detail::SystemAlignedFree(void*)'
cpu_backend_context.cc:(.text+0x4f0): undefined reference to `ruy::detail::SystemAlignedFree(void*)'
cpu_backend_context.cc:(.text+0x4f8): undefined reference to `ruy::detail::SystemAlignedAlloc(long)'
cpu_backend_context.cc:(.text+0x520): undefined reference to `ruy::detail::SystemAlignedFree(void*)'
cpu_backend_context.cc:(.text+0x53e): undefined reference to `ruy::detail::SystemAlignedFree(void*)'
cpu_backend_context.cc:(.text+0x57e): undefined reference to `ruy::ThreadPool::~ThreadPool()'
...
```

As the `ruy` subfolder has been moved into `lite/experimental/ruy`, we should update the `Makefile` to correct the depends.

```diff
tensorflow/lite/tools/make$ git diff ./Makefile
diff --git a/tensorflow/lite/tools/make/Makefile b/tensorflow/lite/tools/make/Makefile
index ef265cc..fa2a3f1 100644
--- a/tensorflow/lite/tools/make/Makefile
+++ b/tensorflow/lite/tools/make/Makefile
@@ -119,7 +119,8 @@ $(wildcard tensorflow/lite/c/*.c) \
 $(wildcard tensorflow/lite/core/*.cc) \
 $(wildcard tensorflow/lite/core/api/*.cc) \
 $(wildcard tensorflow/lite/experimental/resource/*.cc) \
-$(wildcard tensorflow/lite/experimental/ruy/*.cc)
+$(wildcard tensorflow/lite/experimental/ruy/*.cc)  \
+$(wildcard tensorflow/lite/experimental/ruy/ruy/*.cc)
 ifneq ($(BUILD_TYPE),micro)
 CORE_CC_ALL_SRCS += \
 $(wildcard tensorflow/lite/kernels/*.cc) \
@@ -142,14 +143,18 @@ CORE_CC_EXCLUDE_SRCS := \
 $(wildcard tensorflow/lite/*test.cc) \
 $(wildcard tensorflow/lite/*/*test.cc) \
 $(wildcard tensorflow/lite/*/*/benchmark.cc) \
+$(wildcard tensorflow/lite/*/*/*/benchmark.cc) \
 $(wildcard tensorflow/lite/*/*/example*.cc) \
+$(wildcard tensorflow/lite/*/*/*/example*.cc) \
 $(wildcard tensorflow/lite/*/*/test*.cc) \
 $(wildcard tensorflow/lite/*/*/*test.cc) \
 $(wildcard tensorflow/lite/*/*/*tool.cc) \
 $(wildcard tensorflow/lite/*/*/*/*test.cc) \
+$(wildcard tensorflow/lite/experimental/ruy/ruy/test*.cc) \
+$(wildcard tensorflow/lite/experimental/ruy/ruy/profiler/test*.cc) \
 $(wildcard tensorflow/lite/kernels/*test_main.cc) \
 $(wildcard tensorflow/lite/kernels/*test_util*.cc) \
-tensorflow/lite/experimental/ruy/tune_tool.cc \
+tensorflow/lite/experimental/ruy/ruy/tune_tool.cc \
 tensorflow/lite/tflite_with_xnnpack.cc \
 $(MINIMAL_SRCS)

@@ -351,3 +356,4 @@ $(DEPDIR)/%.d: ;
 .PRECIOUS: $(DEPDIR)/%.d

 -include $(patsubst %,$(DEPDIR)/%.d,$(basename $(ALL_SRCS)))
```

And I make a pull request : https://github.com/tensorflow/tensorflow/pull/38029
"
38026,How to run .pb model with tensorflow( or tensorflowLite)  using DSP in android,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
38025,segfault in FlexDelegate on android,"I'm hoping to run a custom tensorflow/tflite model (one that uses tflite's select ops) on-device in an android app.  My understanding is that I need to configure the tflite interpreter with a FlexDelegate, but when I try to do this (on the android-studio emulator), the app segfaults, apparently in the FlexDelegate constructor. I've managed to reproduce the crash in a minimal code, which I link to and describe below. 

Thanks in advance for any help on this, and thanks also to all the devs for creating tensorflow!

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Only a little.  I've added a call to the FlexDelegate constructor in the MainActivity of the default flutter app that android studio generates when you tell it to start a new project.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): the crash I'm seeing happens in an android phone emulator, but the box the emulator is running on is running gentoo linux.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: the android emulator that android-studio provides (I've tested a few configurations including api 27, 29, and R as well as x86 and x86_64 abis)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 
in app/build.gradle, 
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'

**Describe the current behavior**
the app crashes while attempting to construct a FlexDelegate instance while running in the emulator.  (I actually don't have a physical device handy, so I can't test to see if it happens on real hardware right now.)

**Describe the expected behavior**
FlexDelegate should be created with no segfault

**Standalone code to reproduce the issue** 
The line that crashes is 

FlexDelegate delegate = new FlexDelegate();

which I've added to the configureFlutterEngine method of the app's MainActivity.  I've put the code for the full example app in this repository:

https://github.com/particlebbq/tflite_bug_report

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The error message in the logcat is:
2020-03-29 16:22:01.392 11042-11042/com.example.tflitebugreport A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xfffffff4 in tid 11042 (tflitebugreport), pid 11042 (tflitebugreport)
"
38024,Unable to build micro_speech for Sparkfun Edge,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 44400dfcde6e39aca68c4bc103c2e4e15b5379c5
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge

**Describe the problem**

I'm trying to build the micro_speech example for Sparkfun Edge from the `master` branch, but getting a few compile errors.

**Please provide the exact sequence of commands/steps when you ran into the problem**

I am following the steps at:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech

The very first step,
`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge TAGS=""cmsis-nn"" micro_speech_bin`
results in the following compile errors and warnings:

```
tensorflow/lite/micro/kernels/cmsis-nn/softmax.cc: In function 'void tflite::ops::micro::activations::SoftmaxQuantized(const TfLiteTensor*, TfLiteTensor*, const tflite::SoftmaxParams&)':
tensorflow/lite/micro/kernels/cmsis-nn/softmax.cc:97:30: error: 'input_shape' was not declared in this scope
     const int trailing_dim = input_shape.DimensionsCount() - 1;
                              ^~~~~~~~~~~
tensorflow/lite/micro/kernels/cmsis-nn/softmax.cc:97:30: note: suggested alternative: 'initstate'
     const int trailing_dim = input_shape.DimensionsCount() - 1;
                              ^~~~~~~~~~~
                              initstate
tensorflow/lite/micro/kernels/cmsis-nn/softmax.cc:99:60: error: 'output_shape' was not declared in this scope
         MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
                                                            ^~~~~~~~~~~~
tensorflow/lite/micro/kernels/cmsis-nn/softmax.cc:99:60: note: suggested alternative: 'outer_size'
         MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);
                                                            ^~~~~~~~~~~~
                                                            outer_size
```

Any help is super appreciated. Cheers!"
38023,tf.sparse.reorder() produces fully-unknown shaped outputs from partially-unknown shaped placeholder inputs,"When reordering placeholder `SparseTensor`s with partially unknown shapes, the output shape is fully unknown. This prevents usage of the `tf.sparse.reorder()` function in keras models because an unknown batch size at model compile time prevents downstream layers from knowing their expected input shape.

**System information** 

- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): **Yes**

- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): **Linux CentOS 7**

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: **N/A**

- TensorFlow installed from (source or
binary): **Binary**

- TensorFlow version (use command below):  **2.2.0-dev20200327**

- Python version: **Python 3.7.6**

- Bazel version (if compiling from source): **N/A**

- GCC/Compiler version (if compiling from source):  **N/A**

- CUDA/cuDNN version: - GPU model and memory: **Unused**

**Describe the current behavior**
When calling `tf.sparse.reorder()` on a placeholder SparseTensor with a partially
unknown shape, the returned SparseTensor has a fully unknown shape

**Describe the expected behavior**
When calling `tf.sparse.reorder()` on a placeholder SparseTensor with a partially
unknown shape, the returned SparseTensor has the same partially-known shape
as the input tensor

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```python
placeholder = tf.keras.backend.placeholder(shape=(None, 32), dtype=tf.float32, sparse=True)
reordered = tf.sparse.reorder(sparse_placeholder)

print(placeholder.shape)
print(reordered.shape)

> (None, 32)
> (None, None)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached."
38022,"Failed to get convolution algorithm. (Checked existing solution, but not working)","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No, using this example. 
- OS Platform and Distribution: Ubuntu 19.10
- TensorFlow installed from (source or
binary): pip 
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.4
- GCC/Compiler version (if compiling from
source): 7.3.0
- CUDA/cuDNN version: 10.1.243 and 7.6.4
- GPU model and memory: RTX2060S 8GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Trainning failed

**Describe the expected behavior**
Trainning is processing. 

**Standalone code to reproduce the issue** 
https://www.tensorflow.org/tutorials/images/cnn
jupyte notebook download locally


**Other info / logs** 
WARNING:tensorflow:From <ipython-input-19-01c6f78f4d4f>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
Please use Model.fit, which supports generators.
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
Train for 500 steps, validate for 250 steps
Epoch 1/15
  1/500 [..............................] - ETA: 9:01
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
<ipython-input-19-01c6f78f4d4f> in <module>
      4     epochs=epochs,
      5     validation_data=val_data_gen,
----> 6     validation_steps=total_val // batch_size
      7 )

~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py in new_func(*args, **kwargs)
    322               'in a future version' if date is None else ('after %s' % date),
    323               instructions)
--> 324       return func(*args, **kwargs)
    325     return tf_decorator.make_decorator(
    326         func, new_func, 'deprecated',

~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1304         use_multiprocessing=use_multiprocessing,
   1305         shuffle=shuffle,
-> 1306         initial_epoch=initial_epoch)
   1307 
   1308   @deprecation.deprecated(

~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    817         max_queue_size=max_queue_size,
    818         workers=workers,
--> 819         use_multiprocessing=use_multiprocessing)
    820 
    821   def evaluate(self,

~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    340                 mode=ModeKeys.TRAIN,
    341                 training_context=training_context,
--> 342                 total_epochs=epochs)
    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    344 

~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    126         step=step, mode=mode, size=current_batch_size) as batch_logs:
    127       try:
--> 128         batch_outs = execution_function(iterator)
    129       except (StopIteration, errors.OutOfRangeError):
    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     96     # `numpy` translates Tensors to values in Eager mode.
     97     return nest.map_structure(_non_none_constant_value,
---> 98                               distributed_function(input_fn))
     99 
    100   return execution_function

~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    566         xla_context.Exit()
    567     else:
--> 568       result = self._call(*args, **kwds)
    569 
    570     if tracing_count == self._get_tracing_count():

~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    630         # Lifting succeeded, so variables are initialized and we can run the
    631         # stateless function.
--> 632         return self._stateless_fn(*args, **kwds)
    633     else:
    634       canon_args, canon_kwds = \

~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   2361     with self._lock:
   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2364 
   2365   @property

~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1609          if isinstance(t, (ops.Tensor,
   1610                            resource_variable_ops.BaseResourceVariable))),
-> 1611         self.captured_inputs)
   1612 
   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1690       # No tape is watching; skip to running the function.
   1691       return self._build_call_outputs(self._inference_function.call(
-> 1692           ctx, args, cancellation_manager=cancellation_manager))
   1693     forward_backward = self._select_forward_and_backward_functions(
   1694         args,

~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    543               inputs=args,
    544               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 545               ctx=ctx)
    546         else:
    547           outputs = execute.execute_with_cancellation(

~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~/.conda/envs/fcgf/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node sequential/conv2d/Conv2D (defined at <ipython-input-19-01c6f78f4d4f>:6) ]] [Op:__inference_distributed_function_1027]

Function call stack:
distributed_function
"
38021,Problem with can bus interface on TM4C123GH6PM,"I tried to make an interface for can bus on TM4C123GH6PM, i used loop back test, i sent data and make a test on these data using loop back test mode to make sure that data is sent successfully by turning on a led connected on PORT F but nothing happened, I enabled interrupts and created a function called ( CAN0_Handler ) this is my ISR, when i received data an interrupt occurred and the program go to ISR and turn on a led depending on the receiving data but nothing happened and i do not know where the problem is, This is my code.


#include <stdbool.h>
#include <stdint.h>

#define RCGC2_R *((volatile unsigned int*) 0x400FE108)
#define GPIODEN_R *((volatile unsigned int*) 0x4002451C)
#define GPIODIR_R *((volatile unsigned int*) 0x40024400)
#define GPIOAFSEL_R *((volatile unsigned int*) 0x40024420)
#define GPIOPCTL_R *((volatile unsigned int*) 0x4002452C)
#define RCGC0_R *((volatile unsigned int*) 0x400FE100)
#define CANCTL_R *((volatile unsigned int*) 0x40040000)
#define CANIF1CRQ_R *((volatile unsigned int*) 0x40040020)
#define CANIF1CMSK_R *((volatile unsigned int*) 0x40040024)
#define CANIF1ARB1_R *((volatile unsigned int*) 0x40040030)
#define CANIF1ARB2_R *((volatile unsigned int*) 0x40040034)
#define CANIF1MCTL_R *((volatile unsigned int*) 0x40040038)
#define CANBIT_R *((volatile unsigned int*) 0x4004000C)
#define CANBRPE_R *((volatile unsigned int*) 0x40040018)
#define CANSTS_R *((volatile unsigned int*) 0x40040004)
#define CANIF1DA1_R *((volatile unsigned int*) 0x4004003C)
#define CANIF1DA2_R *((volatile unsigned int*) 0x40040040)
#define CANIF1DB1_R *((volatile unsigned int*) 0x40040044)
#define CANIF1DB2_R *((volatile unsigned int*) 0x40040048)
#define CANTST_R *((volatile unsigned int*) 0x40040014)
#define CANIF1MSK1_R *((volatile unsigned int*) 0x40040028)
#define CANIF1MSK2_R *((volatile unsigned int*) 0x4004002C)
#define NVIC_EN1_R *((volatile unsigned int*) 0xE000E104)
#define GPIODEN_PORTF_R *((volatile unsigned int*) 0x4002551C)
#define GPIODIR_PORTF_R *((volatile unsigned int*) 0x40025400)
#define GPIODR2R_PORTF_R *((volatile unsigned int*) 0x40025500)
#define GPIODR4R_PORTF_R *((volatile unsigned int*) 0x40025504)
#define GPIODR8R_PORTF_R *((volatile unsigned int*) 0x40025508)
#define GPIODATA_PORTF_R *((volatile unsigned int*) 0x40025008)

char Can_data_buffer[8];


typedef struct
{
    unsigned long MsgID ;       // CAN message ID , 11 or 29 bit
    unsigned long MsgIDMask ;   // CAN message ID mask
    unsigned long MsgLen ;      // CAN message data length field
    unsigned long MsgFlags ;    // CAN message control and status
    const char *MsgData ;    // CAN message data

} CAN_Msg_Object ;



void CAN0_init_mode();
void CAN_Config_Tx_Message ( CAN_Msg_Object can_msg );
void CAN_Config_Tx_Message_test_mode ( CAN_Msg_Object can_msg );
void Delay ( unsigned long counter );
void CAN0_Handler(void);


int main(void)
{
    RCGC2_R |= 0x30;                                                    //Enable clock to port E & F
    GPIODEN_R |= 0x30;                                                  //enable digital function to PE4 , PE5
    GPIODIR_R |= 0x20;                                                  //PE4 as input RX & PE5 as output TX
    GPIOAFSEL_R |= 0x30;                                                //
    GPIOPCTL_R |= 0x880000;                                             //enable CAN0 TX , RX on PE5 , PE4
    RCGC0_R |= 0x1000000;                                               //enable clock to CAN0
    Delay ( 3 );

    GPIODEN_PORTF_R |= 0x02;                                            //
    GPIODIR_PORTF_R |=0xFF;                                             //
    GPIODR2R_PORTF_R = 0;
    GPIODR4R_PORTF_R = 0;
    GPIODR8R_PORTF_R = 0xFF;


    CAN0_init_mode();

    CAN_Msg_Object id;

    id.MsgID = 0x011;
    id.MsgLen = 0x8;
    id.MsgData = ""abcdefgh"";

    CAN_Config_Tx_Message_test_mode ( id );

    /*if(Can_data_buffer[0]=='a')
    {
        GPIODATA_PORTF_R = 2;
    }*/


    return 0;
}


void CAN0_init_mode()
{
    unsigned char msg_num;
    CANCTL_R |= 0x1;                                                     //enable initialization mode on CAN0
    while( CANIF1CRQ_R & 0x8000 );                                       //wait for busy bit to clear
    CANIF1CMSK_R |= 0xB0;                                                //
    CANIF1ARB1_R = 0;                                                    //
    CANIF1ARB2_R = 0;                                                    //
    CANIF1MCTL_R = 0;                                                    //

    // Configure all messages objects in message RAM as invalid
    for(msg_num=1; msg_num<=32; msg_num++)
    {
        while( CANIF1CRQ_R & 0x8000 );                                   //wait for busy bit to clear
        CANIF1CRQ_R = msg_num;

    }

    CANIF1CMSK_R |= 0x0C;                                                // Make sure that the interrupt and new data flags are updated

    for(msg_num=1; msg_num<=32; msg_num++)
    {
        while( CANIF1CRQ_R & 0x8000 );                                   //wait for busy bit to clear
        CANIF1CRQ_R = msg_num;

    }

    CANCTL_R |= 0x40;                                                   //Write accesses to the CANBIT register are allowed

    CANBIT_R = (0x0002<<12) | (0x0003<<8) | (0x0001<<6) | 0x3;          //setting bit timing make baud rate 500kbps and sampling point at 60%
    CANBRPE_R = 0;                                                      //
    CANCTL_R |= 0x0E;                                                   //Enable CAN 0 interrupts for error , status and CAN controller
    NVIC_EN1_R |= 0x80;                                                 //enable global interrupt for CAN0

    CANCTL_R &= ~(0x41);                                                //disable initialization mode

}


//Normal mode
void CAN_Config_Tx_Message ( CAN_Msg_Object can_msg )
{
    CANIF1CMSK_R |= 0xF3;                                               //
    CANIF1ARB1_R = 0;                                                   //
    CANIF1ARB2_R = 0xA000 | (can_msg.MsgID << 2);                       //
    CANIF1MCTL_R = (0x180 | can_msg.MsgLen);                            //

    CANIF1DA1_R = can_msg.MsgData[0] + (can_msg.MsgData[1] << 8);       //
    CANIF1DA2_R = can_msg.MsgData[2] + (can_msg.MsgData[3] << 8);       //
    CANIF1DB1_R = can_msg.MsgData[4] + (can_msg.MsgData[5] << 8);       //
    CANIF1DB2_R = can_msg.MsgData[6] + (can_msg.MsgData[7] << 8);       //

    CANIF1CRQ_R = 2;                                                    //Initiate programming message object to second message buffer
    while( CANIF1CRQ_R & 0x8000 );                                      //wait for busy bit to clear

}


//loop back test mode
void CAN_Config_Tx_Message_test_mode ( CAN_Msg_Object can_msg )
{
    CANCTL_R |= 0x80;                                                   //test mode
    CANTST_R |= 0x10;                                                   //enable loop back test
    CANIF1CMSK_R |= 0xF3;                                               //
    CANIF1MSK1_R = 0;                                                   //
    CANIF1MSK2_R = 0;                                                   //
    CANIF1ARB1_R = 0;                                                   //
    CANIF1ARB2_R = 0xA000 | (can_msg.MsgID << 2);                       //
    CANIF1MCTL_R = (0x180 | can_msg.MsgLen);                            //

    CANIF1DA1_R = can_msg.MsgData[0] + (can_msg.MsgData[1] << 8);       //
    CANIF1DA2_R = can_msg.MsgData[2] + (can_msg.MsgData[3] << 8);       //
    CANIF1DB1_R = can_msg.MsgData[4] + (can_msg.MsgData[5] << 8);       //
    CANIF1DB2_R = can_msg.MsgData[6] + (can_msg.MsgData[7] << 8);       //

    CANIF1CRQ_R = 2;                                                    //Initiate programming message object to second message buffer
    while( CANIF1CRQ_R & 0x8000 );                                      //wait for busy bit to clear

}


void Delay ( unsigned long counter )
{
    unsigned long i = 0;
    for (i =0; i< counter ; i ++) ;
}



//CAN interrupt handler
void CAN0_Handler(void)
{
    unsigned long status;
    status = CANSTS_R;                                                  //Clear interrupt register

    if(status & 0x10)
    {
        CANIF1MCTL_R |= 0x088;                                          //
        CANIF1CRQ_R = 1;                                                //
        while( CANIF1CRQ_R & 0x8000 );                                  //wait for busy bit to clear

        Can_data_buffer[0] = (char)CANIF1DA1_R;                         //
        Can_data_buffer[1] = (char)(CANIF1DA1_R >> 8);                  //
        Can_data_buffer[2] = (char)CANIF1DA2_R;                         //
        Can_data_buffer[3] = (char)(CANIF1DA2_R >> 8);                  //
        Can_data_buffer[4] = (char)CANIF1DB1_R;                         //
        Can_data_buffer[5] = (char)(CANIF1DB1_R >> 8);                  //
        Can_data_buffer[6] = (char)CANIF1DB2_R;                         //
        Can_data_buffer[7] = (char)(CANIF1DB2_R >> 8);                  //

    }

    if(Can_data_buffer[0]=='a')
    {
        GPIODATA_PORTF_R = 2;
    }
}
"
38018,"tf.estimator.add_metrics ends in Shapes (None, 12) and (None,) are incompatible","**System information** 
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit
TensorFlow installed from (source or binary): PyCharm
TensorFlow version (use command below): 2.0.0
Python version: 3.7.6

**Describe the current behavior**
I am using a DNNClassifier as my estimator and wanted to add some additional metrics to the estimator. the code I am using is basically the one from the tf.estimator.add_metrics documentation 
[https://www.tensorflow.org/api_docs/python/tf/estimator/add_metrics](url).
The model works fine without the add_metrics statement. But runs into an ValueError: ""Shapes (None, 12) and (None,) are incompatible"" when including it. The error occures in the line:

`auc_metric.update_state(y_true=labels, y_pred=predictions['logits'])`

The line is called by `est.evaluate(validation_data)`.

It is not clear to me why this happens, but it seems like the y_true parameter is not filled correctly. Hence, the label column is not processed correctly to the function. This seems strange since the model works correctly without the additional metric. The training and validation data is created by the following function:

```
def get_dataset_from_tensor_slices(data_input, label_column, n_epochs=None, shuffle=True):
    def get_dataset():
        dataset = tf.data.Dataset.from_tensor_slices((dict(data_input), label_column))
        if shuffle:
            dataset = dataset.shuffle(len(label_column))

        # For training, cycle through dataset as many times as need (n_epochs=None).
        dataset = dataset.repeat(n_epochs)
        # In memory training doesn't use batching.
        dataset = dataset.batch(len(label_column))
        return dataset
    return get_dataset
```

**Describe the expected behavior**
It should be able to add an additional metric to the estimator.

**Standalone code to reproduce the issue** 

```
def my_auc(labels, predictions):
    auc_metric = tf.keras.metrics.AUC(name=""my_auc"")
    auc_metric.update_state(y_true=labels, y_pred=predictions['logits'])
    return {'auc': auc_metric}


def model_evaluation(features, training_data, validation_data, labels, validation_label_column):
    hidden_layers = len(training_data.__call__().element_spec[0])
    final_layer = len(labels)
    est = tf.estimator.DNNClassifier(feature_columns=features,
                                     hidden_units=[hidden_layers, (hidden_layers / 2), (hidden_layers / 4),
                                                   final_layer],
                                     n_classes=final_layer, label_vocabulary=labels)
    est = tf.estimator.add_metrics(est, my_auc)

    est.train(training_data, max_steps=100)

    result = est.evaluate(validation_data)

```
**Other info / logs** 
As far as I debugged it, the problem goes back to the fact that the labels created from the get_dataset_from_tensor_slices method have the shape (None,). --> Thats maybe the problem...how can I fix this?
Whereas the predictions are generated in shape (None, 12) (where 12 is the number of possible labels).


Does anybody know why this happens? Any help is appreciated!
"
38017,Broken colab button in nn_from_scratch.ipnyb,"# issue
The colab button in https://github.com/tensorflow/examples/blob/master/community/en/nn_from_scratch.ipynb is Broken.
# Pull Request
I've sent the PR regarding this issue."
38015,Adding correct version of tensorflow in udacity deep learning course,"# issue
This https://github.com/tensorflow/examples/tree/master/courses/udacity_deep_learning repo
contains Udacity deep learning course having codes of tensorflow 1.x  . But now colab is having tensorflow 2.x by default.
# Pull Request
Hey, @MarkDaoust will you please tell me the right tensorflow version so that I'll update all the notebooks. "
38014,ImportError: DLL load failed: The specified module could not be found.   Failed to load the native TensorFlow runtime.,"
- windows 10 home 
- TensorFlow installed from source :
- TensorFlow version: 2.0.0
- Python version: 3.6.2
- Installed using  conda:
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda9.0/cudnn-9.0
- GPU model and memory: Nvidea 1050 ti



**Describe the problem**
cant import tensorflow after its installation 
import tensorflow as tf
and give the following error


**Any other info / logs**
Using TensorFlow backend.
Traceback (most recent call last):

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""G:\anaconda\envs\machinelearning\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)

  File ""G:\anaconda\envs\machinelearning\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)

ImportError: DLL load failed: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""D:\programms\teleuniv ques\untitled0.py"", line 1, in <module>
    from keras.preprocessing import image

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""G:\anaconda\envs\machinelearning\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""G:\anaconda\envs\machinelearning\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
38013,ImportError: DLL load failed: The specified module could not be found.,"Traceback (most recent call last):

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""G:\anaconda\envs\machinelearning\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)

  File ""G:\anaconda\envs\machinelearning\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)

ImportError: DLL load failed: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""D:\programms\teleuniv ques\untitled0.py"", line 1, in <module>
    from keras.preprocessing import image

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *

  File ""G:\anaconda\envs\machinelearning\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\sathw\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""G:\anaconda\envs\machinelearning\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""G:\anaconda\envs\machinelearning\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
38012,module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),

the function populate_dict_with_module_objects() is missed in the 2.2.0rc2


**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
38011,Notebook problem while contributing,"# issue
I'm contributing to Tensorflow/lucid notebooks. But while changing anything in the notebook I'm getting message in github "" The diff is too large to display"". And when I clicked to see the differences in the branch I'm seeing there are a lot of things is changed in the notebook automatically
# Problem
It's very hard for the reviewer to see what changes I've made.
* I've tried into vs code having python extension
* Also tried in jupyter notebook
But I'm getting the same problem in all these
## Example notebook
https://github.com/tensorflow/lucid/blob/master/notebooks/activation-atlas/activation-atlas-adversarial.ipynb"
38009,"Keras conversion works in python, but gives strange results in Anroid","**System information**
- Platform (where I'm experiencing issues)
   android

**Command used to run the converter or code if you’re using the Python API**
```tflite_convert --keras_model_file=best-model-03-0.00.h5 --output_file=new_model.tflite```


**The output from the converter invocation**
```
2020-03-28 15:34:27.528903: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-03-28 15:34:27.562893: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb8e8f21fd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-03-28 15:34:27.562928: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-03-28 15:34:28.585694: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-03-28 15:34:28.590807: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-03-28 15:34:28.604941: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-03-28 15:34:28.604967: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.238ms.
2020-03-28 15:34:28.604972: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-03-28 15:34:28.707516: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-03-28 15:34:28.707602: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-03-28 15:34:28.734064: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-03-28 15:34:28.734111: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 71 nodes (-32), 102 edges (-32), time = 15.415ms.
2020-03-28 15:34:28.734122: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 71 nodes (0), 102 edges (0), time = 3.223ms.```

**Also, please include a link to the saved model or GraphDef**

```
Here is the .h5 file: https://drive.google.com/file/d/1Zaj02NDmxG4A7_Mkl81W0puMXAekEEmR/view?usp=sharing

Here is the .tflite file: https://drive.google.com/file/d/1k_RJU7vm54gVtM_S_n7s4UAE3oekL51M/view?usp=sharing
```

**Failure details**
Using the command above and using the .h5 file linked above, I am able to successfully convert to a .tflite file and perform inference on that .tflite file in Python. 

However, when I use this tflite model file in my Android code, I always get outputs that like this: 
```
[{""label"": ""4"", ""prob"": ""0.9999292""}, {""label"": ""2"", ""prob"": ""5.891328E-5""}, {""label"": ""3"", ""prob"": ""9.082261E-6""}, {""label"": ""0"", ""prob"": ""2.7710482E-6""}, {""label"": ""1"", ""prob"": ""3.9977402E-10""}]
```

(this model has 5 outputs, 0-4)

No matter what image I pass, I get that ""4"" is the output with very similar (but not exactly the same) probabilities. These same images in Python are correctly analyzed and give probabilities that are more reasonable.  

**Any other info / logs**

- The input image is a grayscale image
- this is a float32 tflite model
- Here is the relevant android code: 

```
public class TensorFlowModule extends ReactContextBaseJavaModule {

  private static final int DIM_BATCH_SIZE = 1;
  private static final int DIM_PIXEL_SIZE = 1;
  static final int DIM_IMG_SIZE_X = 300;
  static final int DIM_IMG_SIZE_Y = 100;
  private static final int BYTE_SIZE_OF_FLOAT = 4;

  private final String TAG = this.getClass().getSimpleName();

  // THIS IS THE ""JAVA BUFFER""
  protected ByteBuffer imgData = ByteBuffer.allocateDirect(BYTE_SIZE_OF_FLOAT * DIM_BATCH_SIZE * DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y * DIM_PIXEL_SIZE);
  
  private Interpreter tflite;
  private List<String> labelList;
  private ByteBuffer inputBuffer = null;
  private float[][] labelProbArray = null;
  private static final int RESULTS_TO_SHOW = 5;
  private static final float IMAGE_MEAN = 0f;
  private static final float IMAGE_STD = 255f;

  private static final String MODEL_PATH = ""best_model.tflite"";
  private static final String LABEL_PATH = ""best-model-03242020_dict.txt"";
  private int[] intValues = new int[DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y];

  public TensorFlowModule(ReactApplicationContext reactContext) {
    super(reactContext);
  }

  @Override
  public String getName() {
    return ""TensorFlow"";
  }

  @ReactMethod void predictFromExactAssayWindow(String base64Image, final Promise promise) {
    imgData.order(ByteOrder.nativeOrder());
    labelProbArray = new float[1][RESULTS_TO_SHOW];

    try {

      labelList = loadLabelList();
    } catch (Exception ex) {
      ex.printStackTrace();
    }

    byte[] decodedString = Base64.decode(base64Image, Base64.DEFAULT);
    Bitmap old_bitmap = BitmapFactory.decodeByteArray(decodedString, 0, decodedString.length);
    Bitmap bitmap = Bitmap.createScaledBitmap(old_bitmap, DIM_IMG_SIZE_X, DIM_IMG_SIZE_Y, true);

    convertBitmapToByteBuffer(bitmap);

      
    try {
      tflite = new Interpreter(loadModelFile());
    } catch (Exception ex) {
      Log.w(""FIND_exception in loading tflite"", ""1"");      
    }

    tflite.run(imgData, labelProbArray);
    promise.resolve(getResult());
    
  }

  private WritableNativeArray getResult() {
    WritableNativeArray result = new WritableNativeArray();

    //ArrayList<JSONObject> result = new ArrayList<JSONObject>();
    try {

      for (int i = 0; i < RESULTS_TO_SHOW; i++) {
          WritableNativeMap map = new WritableNativeMap();

          map.putString(""label"", labelList.get(i));
          float output = labelProbArray[0][i];
          map.putString(""prob"", String.valueOf(output));
          result.pushMap(map);

          Log.w(""FIND_label "", labelList.get(i));
          Log.w(""FIND_prob "", String.valueOf(labelProbArray[0][i]));
      }
    } catch (Exception ex) {
      ex.printStackTrace();
    }

    return result;
  }

  private List<String> loadLabelList() throws IOException {
    Activity activity = getCurrentActivity();
      List<String> labelList = new ArrayList<String>();
      BufferedReader reader =
              new BufferedReader(new InputStreamReader(activity.getAssets().open(LABEL_PATH)));
      String line;
      while ((line = reader.readLine()) != null) {
          labelList.add(line);
      }
      reader.close();
      return labelList;
  }

  private void convertBitmapToByteBuffer(Bitmap bitmap) {
      if (imgData == null) {
          return;
      }
      imgData.rewind();
      // Log.w(""FIND_bitmap width "", String.valueOf(bitmap.getWidth()));
      // Log.w(""FIND_bitmap height "", String.valueOf(bitmap.getHeight()));

      bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
      // Convert the image to floating point.
      int pixel = 0;
      //long startTime = SystemClock.uptimeMillis();
      for (int i = 0; i < DIM_IMG_SIZE_X; ++i) {
          for (int j = 0; j < DIM_IMG_SIZE_Y; ++j) {
              final int val = intValues[pixel++];
              float rChannel = (val >> 16) & 0xFF;
              float gChannel = (val >> 8) & 0xFF;
              float bChannel = (val) & 0xFF;
              float gray = (rChannel * 0.299f + gChannel * 0.587f + bChannel * 0.114f);

              // float pixelValue = (rChannel + gChannel + bChannel) / 3 ;
              Log.w(""FIND_pixelValue"", ""("" + i + "", "" + j + "") - "" + String.valueOf(gray));

              imgData.putFloat(gray);

          }
      }
  }

  private MappedByteBuffer loadModelFile() throws IOException {
    Activity activity = getCurrentActivity();
    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_PATH);
    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
    FileChannel fileChannel = inputStream.getChannel();
    long startOffset = fileDescriptor.getStartOffset();
    long declaredLength = fileDescriptor.getDeclaredLength();
    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
  }
}
```

Would love some insight into why my outputs are  always a ""4"" and the probabilities for everything except ""4"" are so, so small. Thanks ahead of time"
38008,How to use Google AutoML Tensorflow Container export in Python?,"I've trained a model on Google AutoML and exported it as Container. It gave me a `.pb` file. I want to use it offline with Python. Here is I use in this code:
```
import os
import cv2
import numpy as np
import tensorflow as tf
import sys

sys.path.append("".."")

from utils import label_map_util
from utils import visualization_utils as vis_util

MODEL_NAME = 'inference_graph'
IMAGE_NAME = 'test8.jpg'
CWD_PATH = os.getcwd()

PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')
PATH_TO_LABELS = os.path.join(CWD_PATH,'training','optik.pbtxt')
PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)
NUM_CLASSES = 1

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

    sess = tf.Session(graph=detection_graph)

image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
num_detections = detection_graph.get_tensor_by_name('num_detections:0')

image = cv2.imread(PATH_TO_IMAGE)
image_expanded = np.expand_dims(image, axis=0)

(boxes, scores, classes, num) = sess.run(
    [detection_boxes, detection_scores, detection_classes, num_detections],
    feed_dict={image_tensor: image_expanded})

vis_util.visualize_boxes_and_labels_on_image_array(
    image,
    np.squeeze(boxes),
    np.squeeze(classes).astype(np.int32),
    np.squeeze(scores),
    category_index,
    use_normalized_coordinates=True,
    line_thickness=8,
    min_score_thresh=0.80)

cv2.imshow('Object detector', image)
cv2.imwrite('testres.jpg', image)

cv2.waitKey(0)
cv2.destroyAllWindows()
```
I've put the `.pb` file to inference_graph folder with `frozen_inference_graph.pb` name. I've setted the .pbtxt file on my own. But it gives this error:

```
Traceback (most recent call last):
  File ""Object_detection_image.py"", line 67, in <module>
    od_graph_def.ParseFromString(serialized_graph)
google.protobuf.message.DecodeError: Error parsing message
```"
38007,TF 2 ignores one of 2 GPUs," Dear community,

I have a problem regarding tensorflow calculation on 2 GPUs connected via SLI technology: only one of them is working and second one is not, although both GPUs are recognized by TF.

Setup:
- Ubuntu 18.04
- Python 3
- Tensorflow 2.1
- Cuda 10.1
- Nvidia drivers (officials) 440.64
- AMD Ryzen 2700
- Asus x470 prime
- Two GPUs of GTX 1070 connected via SLI techno.

I have already tested many things that I had found in internet. Concretely:

1. I started with Tensorflow 2.0, it did not work, so I updated it to TF 2.1. The problem remains

2. Purged and reinstalled the Nvidia drivers 430.50. Updated them to 440.64. The problem remains

3. I verified each of my GPUs separately. I removed physically one of them, and launched code on the remaining. It worked and it seems that the GPUs are OK.

4. I verified each of the GPU's ports on my motherboard separately. It worked and it means that each of the ports are fine.

5. I inserted two GPUs with and without hardware SLI connection and launched the following code:

```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import Xception
import numpy as np

num_samples = 100
height = 224
width = 224
num_classes = 50

strategy = tf.distribute.MirroredStrategy(devices=['/GPU:0', '/GPU:1'])
with strategy.scope():
    parallel_model = Xception(weights=None,
                              input_shape=(height, width, 3),
                              classes=num_classes)
    parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

### Works only for the first GPU of the
# parallel_model = Xception(weights=None,
#                           input_shape=(height, width, 3),
#                           classes=num_classes)
# parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))

# Generate dummy data.
x = np.random.random((num_samples, height, width, 3))
y = np.random.random((num_samples, num_classes))

parallel_model.summary()
# This `fit` call will be distributed on 8 GPUs.
# Since the batch size is 256, each GPU will process 32 samples.
parallel_model.fit(x, y, epochs=20, batch_size=16)
```

As a result, when ```strategy = tf.distribute.MirroredStrategy(devices=['/GPU:0'])```, the code is running fine.
However, when ```devices=['/GPU:1']``` or ```devices=['/GPU:0', '/GPU:1']```, the nvidia-smi shows some process on the 2nd GPU, but the code execution is stacked at line

```
2020-03-28 21:51:14.891325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7162 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)
2020-03-28 21:51:14.891805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-28 21:51:14.892399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1),
```
so I have to reboot the computer, because it s dead.

6. Initially, my X11 configuration (xorg.conf) was not configured for SLI:
```
Section ""Device""
    Identifier     ""Device0""
    Driver         ""nvidia""
    VendorName     ""NVIDIA Corporation""
EndSection

Section ""Device""
    Identifier     ""Device1""
    Driver         ""nvidia""
    VendorName     ""NVIDIA Corporation""
EndSection

Section ""Screen""
    Identifier     ""Screen0""
    Device         ""Device0""
    Monitor        ""Monitor0""
    DefaultDepth    24
    SubSection     ""Display""
        Depth       24
    EndSubSection
EndSection
```
After google search, I played with ```sudo nvidia-xconfig -sli=on```; ```sudo nvidia-xconfig -sli=auto```, etc

As a result, after reboot, I obtain a bootloop with 2 lines:
```
recovering journal
/dev/nume0n1p2: clean, XXX/XXX files, XXX/XXX blocks
```
Every ~3 sec the screen becomes black and then these 2 lines show again.
Impossible to access to TTY, because it is in bootloop as well. I looked everything that I could find on this subject, nothing worked. So, I kept the previous X11 config without SLI

If you experienced such type of problem, do not hesitate to share it. Any advice would help.

Thanks! "
38005,Cleaning up old codes in colab notebooks.,"## Cleaning up the codes in the colab notebooks.
From taking the reference from this [commit](https://github.com/tensorflow/examples/commit/5351bd4e1d4067e2a007b410495200c4bcf3fe61). I've see that there is celaning of codes going on.
* from __future__ import absolute_import, division, print_function, unicode_literals these lines are unnecessary. 
*   ``` 
         % try tensorflow 2.x
             except:
                     pass 
     ```
As now colab already supports tensorflow 2.x by default

# Pull Request
I'll love to address this issue. Kindly assign me this task.
## Repositories
1. Tensorflow/community

P.S :- I'll keep on adding repo in this list after discussing with the community."
38004,KerasClassifier.score is ... broken!?,"I am using the scikit_learn wrapper to wrap a keras model and train / evaluate it in scikit learn. Calling `KerasClassifer.score` should return the accuracy of the classifier; however, no matter what I do, it just doesn't.

Looking at the source the code does two things:

1. In case of sparse labels it converts them to a OneHot matrix (lines 296 - 300)
2. It calles `Sequential.evaluate` and then hopes to find a metric called `acc` or `accuracy` which it treats as the accuracy of the model. (lines 302 - 307)

If it doesn't manage to find a named metric with the right name, it raises an exception.

I don't understand how this could possibly work (and it doesn't work for me). Given that the target labels are OneHot encoded the correct metric to use is `CategoricalAccuracy`; however, it is named 

https://github.com/tensorflow/tensorflow/blob/e24331bf116f5efc8d42bc888a0dbd271aa92aab/tensorflow/python/keras/metrics.py#L758

Logically `KerasClassifer.score` raises an exception. Worse, the error message suggests to add the `Accuracy` metric to the model. This can be misleading, as it makes the error disappear and returns a value, but that value is not ... accurate (pun intended).

I suggest renaming `accuracy` in 

https://github.com/tensorflow/tensorflow/blob/e24331bf116f5efc8d42bc888a0dbd271aa92aab/tensorflow/python/keras/wrappers/scikit_learn.py#L306

to `categorical_accuracy`, and, while at it, I suggest to add `_estimator_type = ""classifier""` as a class variable. Scikit learn checks for it to identify `KerasClassifier` as a classifier, and without it a lot of functionality doesn't work as intended.

If there is agreement for this change I can submit a PR.

"
38003,AttributeError: module 'tensorflow' has no attribute 'layers',"   model = tf.layers.Sequential([
        tf.keras.layers.Conv2D(64, (10, 10), activation = 'relu', input_shape = input_shape, kernel_initializer = initialize_weights(), kernel_regularizer = tf.keras.regularizers.l2(2e-4)),
        tf.keras.layers.MaxPool2D(),
        tf.keras.layers.Conv2D(128, (7, 7), activation='relu', kernel_initializer = initialize_weights(), bias_initializer = initialize_bias(), kernel_regularizer=tf.keras.regularizers.l2(2e-4)),
        tf.keras.layers.MaxPool2D(),
        tf.keras.layers.Conv2D(128, (4, 4), activation='relu', kernel_initializer= initialize_weights(), bias_initializer=initialize_bias(), kernel_regularizer=tf.keras.regularizers.l2(2e-4)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Conv2D(256, (4, 4), activation='relu', kernel_initializer=initialize_weights(), bias_initializer=initialize_bias(), kernel_regularizer=tf.keras.regularizers.l2(2e-4)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(4096, activation='sigmoid',kernel_regularizer=l2(1e-3), kernel_initializer=initialize_weights(), bias_initializer=initialize_bias())

    ])


### **Error**
Traceback (most recent call last):
  File ""C:\Users\shane\OneDrive\Desktop\Project new\signature_verification-master 3\core\model.py"", line 61, in <module>
    model = get_model((105, 105, 1))
  File ""C:\Users\shane\OneDrive\Desktop\Project new\signature_verification-master 3\core\model.py"", line 28, in get_model
    model = tf.layers.Sequential([
AttributeError: module 'tensorflow' has no attribute 'layers'
>>> "
38002,Is retracing inevitable when working with variously-sized inputs?,"I'm beginning to develop some models for token classification with varying-length documents, and it's looking like using variable-length inputs with one-document batches is likely going to give better-results than padding, given the extreme heterogeneity in document lengths.

Here's a simplified version of my model:

```
inp = Input(shape=(None, input_dims))
lay = Conv1D(filters=100, kernel_size=5, padding='same', activation='relu')(inp)
outlayers = [Conv1D(filters=3, kernel_size=5, activation=""softmax"", padding='same', name=i)(lay) for i in out_varnames]
model = Model(inp, outlayers)
```

The inputs vary in size from `(100,input_dims)` to `(8000,input_dims)`, hence the choice to use variable-length inputs.  

My training function is fairly generic:
```
@tf.function
def train(x, y):
    with tf.GradientTape() as g:
        pred = model(x)
        loss = loss_object(y, pred)
        gradients = g.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

When I loop the documents through the train function, I get the following warning:

> WARNING:tensorflow:5 out of the last 5 calls to <function train at 0x7f954a804d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.

Reading and github issues elsewhere, I get the impression that retracing is caused by the fact that the varying input lengths require tensorflow to rejigger some stuff under the hood.  

My questions:  
(1) is this accurate?  Is retracing caused by the variably-sized inputs?
(2) is there a way to avoid retracing when working with variably-sized inputs?
(3) what are some ways to optimize performance when working with variably-sized inputs?"
37998,[tf.data.experimental.cardinality] not working on FlatMapDataset ? [2.2.0-rc1],"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.2.0-rc0-43-gacf4951a2f 2.2.0-rc1
- Python version: 3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: - GPU model and memory: NA 

**Describe the current behavior**
I am making some test with 2.2.0-rc1. When using data from tensorflow dataset tf.data.experimental.cardinality is returning the number of even 

```
print(data['train'])
print(tf.data.experimental.cardinality(data['train'])) 
```

```
<DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>
tf.Tensor(67349, shape=(), dtype=int64)
```
In such case I have a `DatasetV1Adapter`

Now when I am using Huggingface transformer that modify  the structure of the data:

```
train_dataset = glue_convert_examples_to_features(data['train'], 
                                                  tokenizer, 
                                                  max_length=128, 
                                                  task='sst-2')
```

```print(train_dataset)
print(tf.data.experimental.cardinality(train_dataset))
```

```
<FlatMapDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>
tf.Tensor(-2, shape=(), dtype=int64)
```

In this case this is a `FlatMapDataset ` and tf.data.experimental.cardinality is not able tor eturn the number of event ? Is this expected ? In which case is tf.data.experimental.cardinality working ?

The documentation for TF 2.1.0 just said:
`dataset: A tf.data.Dataset for which to determine cardinality.`
 https://www.tensorflow.org/api_docs/python/tf/data/experimental/cardinality
For me (as a naive user) FlatMapDataset/DatasetV1Adapter are tf.data.Dataset 

The way the transformer is modifying the data is by using tf.data.Dataset.from_generator

**Describe the expected behavior**
Be able to return the total number of entry after the data being transformer using tf.data.Dataset.from_generator

**Standalone code to reproduce the issue** 

```
data, info = tensorflow_datasets.load(name='glue/sst2',
                                      data_dir=data_dir,
                                      with_info=True)
# recap input dataset
print(data['train'])
print(tf.data.experimental.cardinality(data['train']))
print(len(list(data['train']))

# Prepare data for BERT
train_dataset = glue_convert_examples_to_features(data['train'], 
                                                  tokenizer, 
                                                  max_length=128, 
                                                  task='sst-2')

print(train_dataset)
print(tf.data.experimental.cardinality(train_dataset))
print(len(list(train_dataset)))
```
A full notebook can be found here:
https://github.com/tarrade/proj_multilingual_text_classification/blob/master/notebook/00-Test/08_SST2_Huggingface_model.ipynb

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
37997,1.15.2: _pywrap_tensorflow_internal.so differs between builds,"**System information**
- OS Platform and Distribution: openSUSE-Tumbleweed-20200324
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15.2
- Python version: 3.8
- Installed using virtualenv? pip? conda?: built with rpm [spec](https://github.com/bmwiedemann/openSUSE/blob/master/packages/t/tensorflow/tensorflow.spec#L449)
- Bazel version (if compiling from source): 0.24
- GCC/Compiler version (if compiling from source): gcc9 and gcc10
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**

While working on reproducible builds for openSUSE, I found that
our tensorflow-1.15.2 package varied across builds.

See https://reproducible-builds.org/ for why this matters.

The variations do not occur when disabling ASLR for the build.

The previous 1.13.2 version built with python-3.7 still did build reproducibly.


**Provide the exact sequence of commands / steps that you executed before running into the problem**

build tensorflow twice from scratch:
osc checkout openSUSE:Factory/tensorflow && cd $_
osc build --noservice --keep-pkg=RPMS

and compare resulting _pywrap_tensorflow_internal.so content

**Any other info / logs**

`/usr/lib64/python3.8/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so differs in assembler output`

by building without Link Time Optimization (LTO), I could see that exactly one .o file differed in the build environment
Binary files /var/tmp/build-root.10/.mount/home/abuild/rpmbuild/SOURCES/BAZEL/_bazel_abuild/089fd2236bcbfcbcf994cdf39cd6bcb6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/lite/_objs/tensorflow_lite_legalize_tf/prepare_tf.pic.o and /var/tmp/build-root.10b/.mount/home/abuild/rpmbuild/SOURCES/BAZEL/_bazel_abuild/089fd2236bcbfcbcf994cdf39cd6bcb6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/lite/_objs/tensorflow_lite_legalize_tf/prepare_tf.pic.o differ

also the asm diff contained
+       lea    offset(%rip),%rsi        #   <_ZTSZN4mlir16PassRegistrationINS_3TFL12_GLOBAL__N_129PrepareCompositeFunctionsPassEEC4EN4llvm9StringRefES6_EUlvE_ + ofs>

that comes from
tensorflow-1.15.2/tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc
which is very close to the prepare_tf.cc file used to create the differing .o file

It is possible that the nondeterminism comes from within gcc9 and gcc10 triggered by some special feature used in prepare_tf.cc but to prove that, I would need a preprocessed version of that compilation. Due to the size and complexity of the build process, I did not manage to get that yet.
"
37996,Documentation corresponding to Arguments for Pre-Trained Models inside <tf.keras.applications> is missing,"## URL(s) with the issue: The information related to Arguments corresponding to the Pre-Trained Models defined under  https://www.tensorflow.org/api_docs/python/tf/keras/applications is missing.

Some examples links are shown below:

https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNet

https://www.tensorflow.org/api_docs/python/tf/keras/applications/InceptionV3

https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50

https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16

## Description of issue (what needs changing): The information corresponding to Arguments should be specified like that it is Specified in [Keras Website](https://keras.io/applications/#vgg16).

For example, why should someone use this method? How is it useful? : If someone want to know what Arguments should be passed while trying to use these Pre-Trained Models, information is lacking in TF.Org site and the Developers should go to Keras Website. The information is not available in the Source Code corresponding to those TF Pre-Trained Models.

### Correct links

Is the link to the source code correct? : Yes

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined? : Yes

### Usage example

Is there a usage example? : NO

### Submit a pull request? : No

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
37995,Bazel is not able to detect CUDA 10.2 header files when building from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): MSVC 2019 (Visual Studio 2019)
- CUDA/cuDNN version: CUDA Version: 10.2, cuDNN Version: 7
- GPU model and memory: NVIDIA Geforce GTX 940MX 2GB VRAM with Cuda Compatibility 5.0 (Lenovo IdeaPad Flex 5)



**Describe the problem**
Bazel is not able to detect the CUDA library files while building tensorflow, even though everything is included in the PATH, and the configure script is also detecting all relevant CUDA library files and includes.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. I was following the steps building from source as the official Documentation showed. While it mentioned CUDA 10.1, I wanted to test my installation with CUDA 10.2, since I have issues with working with CUDA 10.1.
2. I ensured that I set all relevant Bazel PATH environment variables before running the configure script `configure.py`.
3. I successfully configured bazel using `./config.py`.
4. Now, I ran the following command,
```
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```
This gave me different errors, which I initially resolved using the following links:
1. I had the same issue as the poster in this link, which is resolved using [this ](https://github.com/tensorflow/tensorflow/issues/35253#issuecomment-570895670)patch.
Now, I'm stuck with the following error. Apparently, bazel isn't able to find the CUDA header files, even though the relevant path is included in the `.tf_configure.bazelrc` file.

For reference, here is the `.tf_configure.bazelrc` file:
```
build --action_env PYTHON_BIN_PATH=""C:/Users/$USER/AppData/Local/Programs/Python/Python36/python.exe""
build --action_env PYTHON_LIB_PATH=""C:/Users/$USER/AppData/Local/Programs/Python/Python36/lib/site-packages""
build --python_path=""C:/Users/$USER/AppData/Local/Programs/Python/Python36/python.exe""
build --config=xla
build --action_env CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""5.0""
build --config=cuda
build:opt --copt=/arch:AVX
build:opt --define with_default_optimizations=true
build --define=override_eigen_strong_inline=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""
```

Here is the bazel command I used for buliding:
*Build Command*:
```
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```

The log is mentioned below.

**Any other info / logs**
```
E:\Tensorflow\tensorflow-master\tensorflow-master>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
WARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from e:\tensorflow\tensorflow-master\tensorflow-master\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/srira/AppData/Local/Programs/Python/Python36/python.exe
INFO: Reading rc options for 'build' from e:\tensorflow\tensorflow-master\tensorflow-master\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from e:\tensorflow\tensorflow-master\tensorflow-master\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/srira/AppData/Local/Programs/Python/Python36/python.exe --action_env PYTHON_LIB_PATH=C:/Users/srira/AppData/Local/Programs/Python/Python36/lib/site-packages --python_path=C:/Users/srira/AppData/Local/Programs/Python/Python36/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file e:\tensorflow\tensorflow-master\tensorflow-master\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file e:\tensorflow\tensorflow-master\tensorflow-master\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file e:\tensorflow\tensorflow-master\tensorflow-master\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file e:\tensorflow\tensorflow-master\tensorflow-master\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:opt in file e:\tensorflow\tensorflow-master\tensorflow-master\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true
INFO: Found applicable config definition build:cuda in file e:\tensorflow\tensorflow-master\tensorflow-master\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file e:\tensorflow\tensorflow-master\tensorflow-master\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:windows in file e:\tensorflow\tensorflow-master\tensorflow-master\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file e:\tensorflow\tensorflow-master\tensorflow-master\.bazelrc: --define framework_shared_object=false
INFO: Call stack for the definition of repository 'local_config_cuda' which is a cuda_configure (rule definition at E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl:1243:18):
 - E:/tensorflow/tensorflow-master/tensorflow-master/tensorflow/workspace.bzl:91:5
 - E:/tensorflow/tensorflow-master/tensorflow-master/WORKSPACE:26:1
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 1213
                _create_local_cuda_repository(<1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 1075, in _create_local_cuda_repository
                to_list_of_strings(<1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 1076, in to_list_of_strings
                _cuda_include_path(<2 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 365, in _cuda_include_path
                inc_entries.append(<1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 365, in inc_entries.append
                realpath(repository_ctx, <1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl"", line 268, in realpath
                execute(repository_ctx, <1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
realpath: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include': No such file or directory
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 1213
                _create_local_cuda_repository(<1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 1075, in _create_local_cuda_repository
                to_list_of_strings(<1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 1076, in to_list_of_strings
                _cuda_include_path(<2 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 365, in _cuda_include_path
                inc_entries.append(<1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 365, in inc_entries.append
                realpath(repository_ctx, <1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl"", line 268, in realpath
                execute(repository_ctx, <1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
realpath: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include': No such file or directory
WARNING: Target pattern parsing failed.
ERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 1213
                _create_local_cuda_repository(<1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 1075, in _create_local_cuda_repository
                to_list_of_strings(<1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 1076, in to_list_of_strings
                _cuda_include_path(<2 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 365, in _cuda_include_path
                inc_entries.append(<1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl"", line 365, in inc_entries.append
                realpath(repository_ctx, <1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl"", line 268, in realpath
                execute(repository_ctx, <1 more arguments>)
        File ""E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
realpath: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include': No such file or directory
INFO: Elapsed time: 2.717s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
```

**Additional Information**
CUDA 10.2 works perfectly fine on my system, and I've tested programs using CUDA. Pytorch 1.4 with CUDA also runs fine on my system. I have Visual Studio 2019 installed, and I've included all relevant paths for both CUDA and Bazel.
"
37993,can't install tensorflow-gpu==2.0.0b1 in Google Colaboratory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Google Colaboratory



**Describe the problem**

ImportError                               Traceback (most recent call last)
<ipython-input-2-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

11 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/__init__.py in <module>()
     31 from tensorflow.python.keras.saving.save import load_model
     32 from tensorflow.python.keras.saving.save import save_model
---> 33 from tensorflow.python.keras.saving.saved_model import export_saved_model
     34 from tensorflow.python.keras.saving.saved_model import load_from_saved_model
     35 from tensorflow.python.keras.saving.saving_utils import trace_model_call

ImportError: cannot import name 'export_saved_model'

**Provide the exact sequence of commands / steps that you executed before running into the problem**

!pip install tensorflow-gpu==2.0.0-beta1

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

tensorflow==2.0.0b1 does work. I've also tried to uninstall any other version of tensorflow, but it doesn't solve the problem
"
37991,[tf.estimator] - `tf.estimator.Head` do not work with TPUs,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Google Collab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: Nont issue
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): Google Collab
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): None
- CUDA/cuDNN version: - GPU model and memory: None, TPU

**Describe the current behavior**

`tf.estimator.Heads` do not work with TPUs; they are scoped to the CPU, and are seen to belong to a different graph than the expected.

**Describe the expected behavior**

`tf.estimator.Heads` work across CPUs, GPUs, and TPUs.

**Standalone code to reproduce the issue** 

Please run the below Collab notebook. You will need to create, provide, and authenticate to a GCS bucket.

https://colab.research.google.com/gist/nmatare/3e08383751184bf10cc2a1ef0a922b97/untitled6.ipynb

"
37990,[tf.keras] Model.metrics_names bug in TensorFlow 2.2.0,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Mac OS
- TensorFlow version (use command below): 2.2.0rc0
- Python version: 3.7

**Describe the current behavior**

Model.metrics_names returns an empty list (see below example) for a compiled model. This is new unexpected behavior as of TensorFlow 2.2.0 (not the case in TensorFlow 2.1.0)

These metrics names are important at compile time because they can be used to check against monitored quantities in callback. E.g. if a `ModelCheckpoint` callback is trying to monitor `val_lss` we can easily catch the typo before calling `model.fit` or finishing the first epoch of training.

**Describe the expected behavior**

Model.metric returns metrics and loss names (see below example).

**Standalone code to reproduce the issue** 
```python
import tensorflow as tf


inputs = tf.keras.layers.Input(shape=(3,))
outputs = tf.keras.layers.Dense(2, name=""out_1"")(inputs)
net = tf.keras.models.Model(inputs=inputs, outputs=outputs)
net.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae""])

net.metrics_names = []  # TensorFlow 2.2.0rc2
net.metrics_names = [""loss"", ""mae""] # TensorFlow 2.1.0
```
"
37989,TFLite slower than Tensorflow on Desktop?,"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Unbutu 18.04, Windows 10
- TensorFlow installed from (source or binary): binary (tf-nightly)
- TensorFlow version (use command below): v1.12.1-28195-g6609461732 2.2.0-dev20200327
- Python version: 3.6.9

**Describe the current behavior**
On desktop, TFLite infers 2x slower than the regular tensorflow in python (running on unbutu, installed from pip tf-nightly), and 5x slower in C (running on Windows10, with DLLs compiled from source)
I tested with the UNet model found here: https://github.com/deezer/spleeter/blob/master/spleeter/model/functions/unet.py
Without GPU enabled.

**Describe the expected behavior**
TFLite is supposed to be optimized to be more efficient for prediction-only, but with this model it's not the case.
Is this because TFLite is only optimized for mobile?
Or is it a specific operator in the model that slows down the current build of TFLite?

PS: I didn't have access to test on mobile, I only tested on desktop.
PS2: I did try to play with SetNumThread, but it didn't change the results.

**Standalone code to reproduce the issue** 
```
#!/usr/bin/env python
# coding: utf8

import time
import tensorflow as tf
import numpy as np

interpreter = tf.lite.Interpreter(model_path='model.tflite')
interpreter.allocate_tensors()
data = np.zeros((1, 512, 1024, 2), dtype=np.float32)
interpreter.set_tensor(0, data)
for i in range(1, 10):
    start = time.time()
    interpreter.invoke()
    end = time.time()
    print(""tflite:"", end-start)

loaded = tf.saved_model.load('saved_model')
infer = loaded.signatures[""serving_default""]
input = tf.convert_to_tensor(data, np.float32)
for i in range(1, 10):
    start = time.time()
    infer(input)
    end = time.time()
    print(""tensorflow:"", end-start)
```

**Other info / logs**
```
> python3 profile.py
tflite: 3.574683427810669
tflite: 1.5097410678863525
tflite: 1.486119031906128
tflite: 1.5448968410491943
tflite: 1.4875497817993164
tflite: 1.4804818630218506
tflite: 1.543769121170044
tflite: 1.5214331150054932
tflite: 1.4932630062103271
tensorflow: 1.282404899597168
tensorflow: 0.6814086437225342
tensorflow: 0.7910218238830566
tensorflow: 0.6581625938415527
tensorflow: 0.620100736618042
tensorflow: 0.7165799140930176
tensorflow: 0.6456854343414307
tensorflow: 0.6069018840789795
```"
37988,AttributeError with TF2.2.0-rc1 using `keras.model.train_on_batch` inside `tf.function`,"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab (both CPU and GPU runtime 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): From default Colab TF2.x version
- TensorFlow version (use command below): v2.2.0-rc1-0-gacf4951a2f (2.2.0-rc1)
- Python version: 3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When trying to train a keras model using `train_on_batch` inside a `tf.function`:
```
@tf.function(input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32),
                              tf.TensorSpec(shape=(None, 10), dtype=tf.int32)
                              ])
def train(inp, extra):
  expected = calc_expected(inp, extra)
  return model_1.train_on_batch((inp, extra), expected)
```
TensorFlow raises an `AttributeError`:
```
AttributeError: in user code:

    <ipython-input-6-49c8760b225c>:6 train  *
        return model_1.train_on_batch((inp, extra), expected)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1287 train_on_batch  **
        logs = tf_utils.to_numpy_or_python_type(logs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:523 to_numpy_or_python_type
        return nest.map_structure(_to_single_numpy_or_python_type, tensors)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 map_structure
        structure[0], [func(*x) for x in entries],
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 <listcomp>
        structure[0], [func(*x) for x in entries],
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:519 _to_single_numpy_or_python_type
        x = t.numpy()

    AttributeError: 'Tensor' object has no attribute 'numpy'
```

**Describe the expected behavior**
The code works fine in eager execution, and should not raise an error when run in a `tf.function`.
This also works fine with TensorFlow 2.1, so it looks to be a recent regression.

**Standalone code to reproduce the issue** 
https://colab.research.google.com/drive/1uy1awEQE4_t_0uZ4MzGXvfeeRnG_EJVQ

**Other info / logs**
<details>
<summary>Full backtrace copied from Colab</summary>
<p>

```
---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

<ipython-input-28-c04cb8faf387> in <module>()
      2 b = tf.random.uniform(shape=(2, 10), maxval=10, dtype=tf.int32)
      3 
----> 4 train(a, b)

8 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--> 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    625       # This is the first call of __call__, so we have to initialize.
    626       initializers = []
--> 627       self._initialize(args, kwds, add_initializers_to=initializers)
    628     finally:
    629       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    504     self._concrete_stateful_fn = (
    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 506             *args, **kwds))
    507 
    508     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444       args, kwargs = None, None
   2445     with self._lock:
-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2447     return graph_function
   2448 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-> 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--> 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--> 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

AttributeError: in user code:

    <ipython-input-6-49c8760b225c>:6 train  *
        return model_1.train_on_batch((inp, extra), expected)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1287 train_on_batch  **
        logs = tf_utils.to_numpy_or_python_type(logs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:523 to_numpy_or_python_type
        return nest.map_structure(_to_single_numpy_or_python_type, tensors)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 map_structure
        structure[0], [func(*x) for x in entries],
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 <listcomp>
        structure[0], [func(*x) for x in entries],
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:519 _to_single_numpy_or_python_type
        x = t.numpy()

    AttributeError: 'Tensor' object has no attribute 'numpy'
```

</p>
</details>"
37986,[Bug] DLPack tensor seg faults when calling `tensor.device`,"After https://github.com/tensorflow/tensorflow/pull/37944, the bug below happens. As I tested, if remove the added `TFE_DeleteContext(ctx);`, the codes below work fine. Do you have any idea what's the proper way to use TFE_Context here? @sanjoy @alextp 

```python
import tensorflow as tf
a = tf.constant([1])
dlpack_arr = tf.experimental.dlpack.to_dlpack(a)
tf_tensor = tf.experimental.dlpack.from_dlpack(dlpack_arr)
print(tf_tensor) # This works fine
print(tf_tensor.device) # Segment fault here. Works well without #37944 
```


**System information** 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 16.04
- TF Version: latest nightly build or installed from source"
37985,Unable to successfully build a devel-cpu.Dockerfile for arm32v7/arm64v8,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 and 19.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 B (4 GB)
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.1
- Python version: 3.6 (Ubuntu 18.04) and 3.7 (Ubuntu 19.10)
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.2.1-9ubuntu2) 9.2.1 20191008
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**
There are no crossbuild compile instructions for Raspberry Pi 4 armhf or aarch64--the latter being of much more interest--so I figured I'd follow the devel-cpu.Dockerfile (in tensorflow/tools/dockerfiles/dockerfiles) and devel-cpu-ppc64le.Dockerfile (in tensorflow/tools/dockerfiles/dockerfiles/ppc64le), and attempt to create both a devel-cpu-arm32v7.Dockerfile and devel-cpu-arm32v7.Dockerfile.  However, both fail to build.  Specifically, they fail at the building Bazel step.  I have attempted this from docker on an amd64 host using qemu, as well as from docker on arm32v7 and arm64v8, and finally directly on arm32v7 and arm64v8 without docker.  I have tried installing potential dependancies, i.e., pip install zypper, but still have had no success.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

See attachment for devel-cpu-arm64v8.Dockerfile.  Simply replace ""arm64v8"" with ""arm32v7"" to create the devel-cpu-arm32v7.Dockerfile.
[devel-cpu-arm64v8.Dockerfile.txt](https://github.com/tensorflow/tensorflow/files/4395428/devel-cpu-arm64v8.Dockerfile.txt)

docker build --build-arg UBUNTU_VERSION=19.10 --build-arg CHECKOUT_TF_SRC=1 --build-arg USE_PYTHON_3_NOT_2=1 -f ./devel-cpu-arm32v7.Dockerfile -t arm32v7/tensorflow:latest-devel-py3 .
docker build --build-arg UBUNTU_VERSION=19.10 --build-arg CHECKOUT_TF_SRC=1 --build-arg USE_PYTHON_3_NOT_2=1 -f ./devel-cpu-arm64v8.Dockerfile -t arm64v8/tensorflow:latest-devel-py3 .

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

ERROR: /bazel/src/BUILD:105:1: PythonZipper src/create_embedded_tools.zip failed (Exit 255)
Target //src:bazel_nojdk failed to build
INFO: Elapsed time: 292.985s, Critical Path: 199.34s
INFO: 438 processes: 438 local.
FAILED: Build did NOT complete successfully

ERROR: Could not build Bazel
The command '/bin/sh -c cd /bazel &&     env BAZEL_JAVAC_OPTS=""-J-Xmx2g -J-Xms200m"" EXTRA_BAZEL_ARGS=""--host_javabase=@local_jdk//:jdk"" bash ./compile.sh' returned a non-zero code: 1
"
37984,no detection on custom trained model,I have made a custom trained model using yolov2 and after 1500 iteration i tried to test it but no detection boxes the output image as the input any help please
37983,Calling next with a default value on an exhausted Dataset iterator raises an OutOfRangeError in graph mode,"
**System information** 
- Have I written custom code: Yes
- OS Platform and Distribution:  Windows 10
- TensorFlow installed from binary: 2.1.0

**Describe the current behavior**
`next(iterator, default)` is supposed to give the next element in the iterator or the value given as _default_ if the iterator is at the end.
However, when using the above construction in a function with @tf.function, the default value is not returned and an error (tensorflow.python.framework.errors_impl.OutOfRangeError) is produced when trying to call _next_ on an iterator that is at the end.
When running this code in eager mode, the default value is returned as expected.

**Describe the expected behavior**
In graph mode the default value should be returned when at the end of an iterator.

**Standalone code to reproduce the issue** 
```
import tensorflow as tf

x = tf.convert_to_tensor([[1], [2], [3]])
ds = tf.data.Dataset.from_tensor_slices(x)
dsi = iter(ds)


@tf.function # remove this to get the expected behaviour
def func():
    for _ in range(4):
        tf.print(next(dsi, -1))


func()
```
Output (see below for a full stacktrace):
```
[1]
[2]
[3]
2020-03-27 18:56:09.523946: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_3}}]]
```

Expected output:
```
[1]
[2]
[3]
-1

```

**Other info / logs**
**Colab link:** https://colab.research.google.com/drive/1PBxoXiE48aC-bo-aY-Bau1Igt4Aj6OFy

[stacktrace.txt](https://github.com/tensorflow/tensorflow/files/4395116/stacktrace.txt)
"
37982,Large Model Support,"**System information**
- TensorFlow version (you are using): 2.2 nightly
- Are you willing to contribute it (Yes/No): Code exists

code is here:
https://github.com/IBM/tensorflow-large-model-support

**Describe the feature and the current behavior/state.**
TensorFlow Large Model Support (TFLMS) is a feature in the TensorFlow provided by IBM Watson Machine Learning Community Edition (WML CE) that allows the successful training of deep learning models that would otherwise exhaust GPU memory and abort with ""out-of-memory"" errors. LMS manages this oversubscription of GPU memory by temporarily swapping tensors to host memory when they are not needed.

**Will this change the current api? How?**
will add:
`tf.config.experimental.set_lms_enabled(True)`

**Who will benefit with this feature?**
everyone who trains models
"
37981,I get cultural errors when I import the Tensorflow library. Can you help me?,"
Traceback (most recent call last):
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Muhammet/PycharmProjects/Tez/Sentiment_Analysis.py"", line 3, in <module>
    from tensorflow.python.keras.models import Sequential
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Muhammet\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
### "
37980,Kernel crashes with 'Check failed: work_element_count > 0 (0 vs. 0)' after first model has finished training.,"**System information**  
- Have I written custom code (as opposed to using example directory):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- TensorFlow backend (yes / no):  Yes
- TensorFlow version:  2.1
- Keras version:  2.2.4-tf
- Python version:  3.7.6
- CUDA/cuDNN version:  CUDA 10.1, cuDNN: 7.5.6
- GPU model and memory:  Nvidia GTX1070, 8GB

**Describe the current behavior**  

I'm currently trying to train multiple permutations of the same model using the ""Keras Tuner"" with the following setup:

```
from kerastuner.tuners import BayesianOptimization
from kerastuner.engine.hyperparameters import HyperParameters

tuner = BayesianOptimization(build_model,
                             objective='val_loss',
                             max_trials=1000,
                             executions_per_trial=3,
                             directory=LOG_DIR)

tuner.search(train_data_single,
             verbose=0,
             epochs=EPOCHS,
             steps_per_epoch=EVALUATION_INTERVAL,
             validation_data=val_data_single,
             validation_steps=EVALUATION_INTERVAL // 4)
```

Where my dynamic model is defined as:

```
def build_model(hp):
    model = tf.keras.models.Sequential()

    model.add(tf.keras.layers.LSTM(units=hp.Int(f'LSTM_0_Units', min_value=8, max_value=128, step=8),
                                   dropout=hp.Float(f'LSTM_0_Dropout_Rate', min_value=0, max_value=0.5, step=0.1),
                                   batch_input_shape=(BATCH_SIZE, x_train_single.shape[1], x_train_single.shape[2]),
                                   return_sequences = True))

    for i in range(hp.Int('n_Extra_Layers', 0, 3)):
        model.add(tf.keras.layers.LSTM(units=hp.Int(f'LSTM_{i + 1}_Units', min_value=8, max_value=128, step=8),
                                       dropout=hp.Float(f'LSTM_{i + 1}_Dropout_Rate', min_value=0, max_value=0.5, step=0.1),
                                       return_sequences = True))
    
    model.add(tf.keras.layers.LSTM(units=hp.Int(f'LSTM_Closing_Units', min_value=8, max_value=128, step=8),
                                   dropout=hp.Float(f'LSTM_Closing_Dropout_Rate', min_value=0, max_value=0.5, step=0.1),
                                   return_sequences = False))
    
    if hp.Boolean(""Extra_Dense""):
        model.add(tf.keras.layers.Dense(units=hp.Int(f'Extra_Dense_Units', min_value=8, max_value=128, step=8)))
    
    if hp.Boolean(""Extra_Dropout""):
        model.add(tf.keras.layers.Dense(units=hp.Float(f'Extra_Dropout_Rate', min_value=0.1, max_value=0.5, step=0.1)))
    
    model.add(tf.keras.layers.Dense(1))

    model.compile(optimizer=tf.keras.optimizers.Adam(), 
                  loss='mae')
    
    return model
```

I'm faced with the following after the first successfully trained model:

```
2020-03-27 17:21:28.275596: F .\tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
[I 17:21:29.843 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports
kernel 2bac517a-c195-47ca-952b-c25881cf0757 restarted
```

And the Jupyter Kernel crashes. Any ideas? :) 

**Describe the expected behavior**  

I would expect the session to NOT crash during hyperparameter optimization after the first model has completed since I would ideally want to train hundreds if not thousands of permutations.

**Other info**  
Prior to this issue I was faced with the following: https://github.com/tensorflow/tensorflow/issues/37932"
37979,Error when tf2 model is ask to output gradients with prediction,"-------------------------------------
System information

Tensorflow==2.1.0 in a conda env

-------------------------------------
Test Goal 
The goal was to get the gradients as an output of the model to get it during inference

------------------------------------
Test Code

import tensorflow as tf
import numpy as np

tf.config.experimental_run_functions_eagerly(True)

def get_model():
    
    input_sequences = tf.keras.layers.Input(shape=(10,10))
    
    output = tf.keras.layers.Flatten()(input_sequences)
    output = tf.keras.layers.Dense(10*10)(output)
    output = tf.keras.layers.Dense(10)(output)
    output = tf.keras.layers.Dense(1)(output)
    model = tf.keras.models.Model(inputs=[input_sequences], outputs=[output])
    
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.mae])
    model.build((10,10))
    return model

def get_model_grad():
    
    input_sequences = tf.keras.layers.Input(shape=(10,10))
    
    m = get_model()    
    t = m(input_sequences)
    
    loss = tf.keras.losses.mean_squared_error(t, 1.)
    gradients = tf.keras.layers.Lambda(lambda x : tf.keras.backend.gradients(loss, input_sequences))(t)
    
    model = tf.keras.models.Model(inputs=[input_sequences], outputs=[t, gradients])
    
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.mae])
    model.build((10,10))
    print(model.summary())
    return model

model = get_model_grad()

random_input = np.random.rand(10,10)
print(random_input.shape)
random_batch = np.expand_dims(random_input, 0)
print(random_batch.shape)
print(model.predict(random_batch))

-------------------------------------
ERROR 

RuntimeError                              Traceback (most recent call last)
<ipython-input-13-dc70cf4714b3> in <module>
      5 random_batch = np.expand_dims(random_input, 0)
      6 print(random_batch.shape)
----> 7 print(model.predict(random_batch))

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1011         max_queue_size=max_queue_size,
   1012         workers=workers,
-> 1013         use_multiprocessing=use_multiprocessing)
   1014 
   1015   def reset_metrics(self):

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,
    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,
--> 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
    499 
    500 

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    473               mode=mode,
    474               training_context=training_context,
--> 475               total_epochs=1)
    476           cbks.make_logs(model, epoch_logs, result, mode)
    477 

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    126         step=step, mode=mode, size=current_batch_size) as batch_logs:
    127       try:
--> 128         batch_outs = execution_function(iterator)
    129       except (StopIteration, errors.OutOfRangeError):
    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     96     # `numpy` translates Tensors to values in Eager mode.
     97     return nest.map_structure(_non_none_constant_value,
---> 98                               distributed_function(input_fn))
     99 
    100   return execution_function

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    553     context.ensure_initialized()
    554     if RUN_FUNCTIONS_EAGERLY:
--> 555       return self._python_function(*args, **kwds)
    556 
    557     tracing_count = self._get_tracing_count()

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)
     83     args = _prepare_feed_values(model, input_iterator, mode, strategy)
     84     outputs = strategy.experimental_run_v2(
---> 85         per_replica_function, args=args)
     86     # Out of PerReplica outputs reduce or pick values to return.
     87     all_outputs = dist_utils.unwrap_output_dict(

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)
    761       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),
    762                                 convert_by_default=False)
--> 763       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    764 
    765   def reduce(self, reduce_op, value, axis):

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
   1817       kwargs = {}
   1818     with self._container_strategy().scope():
-> 1819       return self._call_for_each_replica(fn, args, kwargs)
   1820 
   1821   def _call_for_each_replica(self, fn, args, kwargs):

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)
   2162         self._container_strategy(),
   2163         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):
-> 2164       return fn(*args, **kwargs)
   2165 
   2166   def _reduce_to(self, reduce_op, value, destinations):

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    256   def wrapper(*args, **kwargs):
    257     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):
--> 258       return func(*args, **kwargs)
    259 
    260   if inspect.isfunction(func) or inspect.ismethod(func):

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in _predict_on_batch(***failed resolving arguments***)
    210       del y, sample_weights
    211       # Note that the x and batch_index is already per-replica value.
--> 212       result = predict_on_batch(model, x)
    213       if batch_index is None:
    214         return result

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in predict_on_batch(model, x, standalone)
    554 
    555   with backend.eager_learning_phase_scope(0):
--> 556     return predict_on_batch_fn(inputs)  # pylint: disable=not-callable

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--> 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)
    715     return self._run_internal_graph(
    716         inputs, training=training, mask=mask,
--> 717         convert_kwargs_to_constants=base_layer_utils.call_context().saving)
    718 
    719   def compute_output_shape(self, input_shape):

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask, convert_kwargs_to_constants)
    889 
    890           # Compute outputs.
--> 891           output_tensors = layer(computed_tensors, **kwargs)
    892 
    893           # Update tensor_dict.

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--> 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py in call(self, inputs, mask, training)
    844     with backprop.GradientTape(watch_accessed_variables=True) as tape,\
    845         variable_scope.variable_creator_scope(_variable_creator):
--> 846       result = self.function(inputs, **kwargs)
    847     self._check_variables(created_variables, tape.watched_variables())
    848     return result

<ipython-input-10-fe47f043d606> in <lambda>(x)
      7 
      8     loss = tf.keras.losses.mean_squared_error(t, 1.)
----> 9     gradients = tf.keras.layers.Lambda(lambda x : tf.keras.backend.gradients(loss, input_sequences))(t)
     10 
     11     model = tf.keras.models.Model(inputs=[input_sequences], outputs=[t, gradients])

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in gradients(loss, variables)
   3782   """"""
   3783   return gradients_module.gradients(
-> 3784       loss, variables, colocate_gradients_with_ops=True)
   3785 
   3786 

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)
    156         ys, xs, grad_ys, name, colocate_gradients_with_ops,
    157         gate_gradients, aggregation_method, stop_gradients,
--> 158         unconnected_gradients)
    159   # pylint: enable=protected-access
    160 

~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    489   """"""Implementation of gradients().""""""
    490   if context.executing_eagerly():
--> 491     raise RuntimeError(""tf.gradients is not supported when eager execution ""
    492                        ""is enabled. Use tf.GradientTape instead."")
    493   if src_graph is None:

RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.
"
37978, cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version,"I installed Anaconda 2020(64 bit) after uninstalling every installation and files related to python before that and then used these instructions:
conda install tensorflow
conda install tensorflow-gpu
now I have 
cudatoolkit               10.0.130                      0
cudnn                     7.6.5                cuda10.0_0
 and NVIDIA Graphics Driver 388.73

Then I run these instructions in spyder:

checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)
class_weights = {0:1, 1:1, 2:1, 3:1}
fittedModel = model.fit(X_train, Y_train, batch_size = 16, epochs = 10, 
                        verbose = 1, validation_data=(X_validate, Y_validate),
                        callbacks=[checkpointer], class_weight = class_weights)
 
and I have this error:
Train on 144 samples, validate on 72 samples
Traceback (most recent call last):

  File ""<ipython-input-2-4eb902219300>"", line 3, in <module>
    callbacks=[checkpointer],class_weight = class_weights)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 780, in fit
    steps_name='steps_per_epoch')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 250, in model_iteration
    model.reset_metrics()

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1084, in reset_metrics
    m.reset_states()

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\metrics.py"", line 199, in reset_states
    K.batch_set_value([(v, 0) for v in self.variables])

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"", line 3071, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"", line 459, in get_session
    session = _get_session(op_input_list)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"", line 431, in _get_session
    config=get_default_session_config())

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1570, in __init__
    super(Session, self).__init__(target, graph, config=config)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 693, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)

InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version

 what should I do?
Please help me. thanks"
37977,Micro -- Crash in ErrorReporter::Report,"**System information** 
TensorFlow Lite for Microcontrollers
running greedy_memory_planner_test from master
building with a Clang based compiler where va_list has the type void*

**Describe the current behavior**
Test crashes on 
`TF_LITE_REPORT_ERROR(error_reporter, ""%s"", line);`
Most likely because the string arg matches two prototypes in this configuration of Clang, and its picking the wrong one. Ultimately it tries do dereference the literal string leading to a crash.

**Describe the expected behavior**
No crash, print some ASCII art.

**Standalone code to reproduce the issue** 
If you have Docker up and running, then call this from the tensorflow folder:
`rm -rf tensorflow/lite/micro/tools/make/downloads/*`
`make -f tensorflow/lite/micro/tools/make/Makefile clean`
`docker run -it -v$(pwd):/home/builder:z --rm xcoreai/build-tools:latest make -f tensorflow/lite/micro/tools/make/Makefile TARGET=""xcore"" test_greedy_memory_planner_test`

**Other info / logs** Include any logs or source code that would be helpful to
PR #37976
Fixes the immediate issue in greedy_memory_planner by adding an explicit typecast:
`TF_LITE_REPORT_ERROR(error_reporter, ""%s"", (const char *)line);`"
42796,[zh-cn] Notebooks failing,"### site/zh-cn/tutorials/distribute/multi_worker_with_keras.ipynb

```
nbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:
------------------
options = tf.data.Options()
options.experimental_distribute.auto_shard = False
train_datasets_no_auto_shard = train_datasets.with_options(options)
------------------

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-9-540e26d88b40> in <module>
      1 options = tf.data.Options()
----> 2 options.experimental_distribute.auto_shard = False
      3 train_datasets_no_auto_shard = train_datasets.with_options(options)

/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow_core/python/data/util/options.py in __setattr__(self, name, value)
     54     else:
     55       raise AttributeError(
---> 56           ""Cannot set the property %s on %s."" % (name, type(self).__name__))
     57 
     58 

AttributeError: Cannot set the property auto_shard on DistributeOptions.
```

###  site/zh-cn/tutorials/generative/style_transfer.ipynb

```
nbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:
------------------
file_name = 'kadinsky-turtle.png'
mpl.image.imsave(file_name, image[0])

try:
  from google.colab import files
except ImportError:
   pass
else:
  files.download(file_name)
------------------

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-35-aea7d0ffa719> in <module>
      1 file_name = 'kadinsky-turtle.png'
----> 2 mpl.image.imsave(file_name, image[0])
      3 
      4 try:
      5   from google.colab import files

~/.local/lib/python3.6/site-packages/matplotlib/image.py in imsave(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)
   1548         if origin == ""lower"":
   1549             arr = arr[::-1]
-> 1550         rgba = sm.to_rgba(arr, bytes=True)
   1551         if format == ""png"" and pil_kwargs is None:
   1552             with cbook.open_file_cm(fname, ""wb"") as file:

~/.local/lib/python3.6/site-packages/matplotlib/cm.py in to_rgba(self, x, alpha, bytes, norm)
    215                         alpha = np.uint8(alpha * 255)
    216                     m, n = x.shape[:2]
--> 217                     xx = np.empty(shape=(m, n, 4), dtype=x.dtype)
    218                     xx[:, :, :3] = x
    219                     xx[:, :, 3] = alpha

TypeError: data type not understood
```

### site/zh-cn/tutorials/keras/text_classification_with_hub.ipynb

```
nbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:
------------------
# 将训练集按照 6:4 的比例进行切割，从而最终我们将得到 15,000
# 个训练样本, 10,000 个验证样本以及 25,000 个测试样本
train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])

(train_data, validation_data), test_data = tfds.load(
    name=""imdb_reviews"", 
    split=(train_validation_split, tfds.Split.TEST),
    as_supervised=True)
------------------

---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-3-fd9cf994df99> in <module>
      6     name=""imdb_reviews"",
      7     split=(train_validation_split, tfds.Split.TEST),
----> 8     as_supervised=True)

~/.local/lib/python3.6/site-packages/tensorflow_datasets/core/api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     50     _check_no_positional(fn, args, ismethod, allowed=allowed)
     51     _check_required(fn, kwargs)
---> 52     return fn(*args, **kwargs)
     53 
     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

...

~/.local/lib/python3.6/site-packages/tensorflow_datasets/core/tfrecords_reader.py in _str_to_relative_instruction(spec)
    354   res = _SUB_SPEC_RE.match(spec)
    355   if not res:
--> 356     raise AssertionError('Unrecognized instruction format: %s' % spec)
    357   unit = '%' if res.group('from_pct') or res.group('to_pct') else 'abs'
    358   return ReadInstruction(

AssertionError: Unrecognized instruction format: NamedSplit('train')(tfds.percent[0:60])
```
"
37975,Binary add op BF16 has lower performance than FP32,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Centos 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below):  binary pip install tensorflow==2.1.0
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

**Describe the current behavior**
Running the binary op of two tensor, change the tensor type to Bfloat16 increase  the running time of 2x

**Describe the expected behavior**
Bfloat16 has lower memory consumption should be a little bit quick.

**Standalone code to reproduce the issue** 
pip install tensorflow==2.1.0
```
import tensorflow as tf
tf.compat.v1.disable_eager_execution()

if __name__ == ""__main__"":
  input_shape = [1, 224, 224, 1024]
  images = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.float32, name='images')
  images2 = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.float32, name='images2')
  images_bf16 = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.bfloat16, name='images_bf16')
  images_bf16_2 = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.bfloat16, name='images_bf16_2')
  mysum = tf.add(images, images2)
  mysum_bf16 = tf.add(images_bf16, images_bf16_2)
  with tf.compat.v1.Session() as sess:
    def run():
      res = sess.run(mysum)
      #print(res)
    def run2():
      res = sess.run(mysum_bf16)
    import timeit
    cost = timeit.timeit(stmt=run, number=200)
    print(""fp32 cost time: {}"".format(cost))
    cost_bf16 = timeit.timeit(stmt=run2, number=200)
    print(""bf16 cost time: {}"".format(cost_bf16))

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Currently result:
fp32 cost time: 15.485223675030284
bf16 cost time: 30.486599242896773

"
37974,Graph Transform Tool remove_nodes is unable to remove Switch nodes but Identity nodes,"I can remove all the Identity nodes from my .pb model with the commands:

```
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=../m.pb --out_graph=../new.pb --inputs='batch_size,phase_train' --outputs='label_batch,embeddings' --transforms='strip_unused_nodes(type=float, shape=""1,299,299,3"") remove_nodes(op=Identity) fold_old_batch_norms fold_constants(ignore_errors=true)'
```

However, I cannot do the same things if change Identity to Switch. It means the below command does not remove any nodes.

```
bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=../m.pb --out_graph=../new.pb --inputs='batch_size,phase_train' --outputs='label_batch,embeddings' --transforms='strip_unused_nodes(type=float, shape=""1,299,299,3"") remove_nodes(op=Switch) fold_old_batch_norms fold_constants(ignore_errors=true)'
```

This is how I check the model nodes:

```
bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=../new.pb
```

**The result as below:**

Found 2 possible inputs: (name=phase_train, type=bool(10), shape=<unknown>) (name=batch_size, type=int32(3), shape=<unknown>) 
No variables spotted.
Found 2 possible outputs: (name=label_batch, op=Identity) (name=embeddings, op=Mul) 
Found 23512506 (23.51M) const parameters, 0 (0) variable parameters, and 676 control_edges
Op types used: 2019 Switch, 1105 Const, 566 Identity, 449 Merge, 448 Sub, 249 Mul, 224 FusedBatchNormV3, 132 Conv2D, 131 Relu, 23 ConcatV2, 21 BiasAdd, 21 AddV2, 3 Shape, 3 MaxPool, 3 Reshape, 2 Placeholder, 1 Maximum, 1 Pack, 1 MatMul, 1 QueueDequeueUpToV2, 1 RandomUniform, 1 GreaterEqual, 1 FIFOQueueV2, 1 Rsqrt, 1 AvgPool, 1 Square, 1 StridedSlice, 1 Cast, 1 Sum, 1 Add
To use with tensorflow/tools/benchmark:benchmark_model try these arguments:
bazel run tensorflow/tools/benchmark:benchmark_model -- --graph=../new.pb --show_flops --input_layer=phase_train,batch_size --input_layer_type=bool,int32 --input_layer_shape=: --output_layer=label_batch,embeddings


**My question is how can I remove the Switch nodes?**
 "
37973,Keras loading a saved model - ValueError: Could not find matching function to call loaded from the SavedModel,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information** 
- Have written custom code: Yes
- OS Platform and Distribution: Windows 10 
- Mobile device: No 
- TensorFlow installed from: conda install tensorflow
- TensorFlow version: 2.1.0 (CPU only)
- Python version: 3

**Describe the current behavior**
I train a TF Keras model (summary below) and use it for slot classification and it works fine.
I then save it using: `tf.saved_model.save(joint_model, 'BERT2.tf')`
I then load the model: bertmodel = `tf.keras.models.load_model('BERT2.tf', compile=False)`
It is compiled using the same arguments as saved model.
And then try predictions using the same function as before but I getting the following error: ValueError: Could not find matching function to call loaded from the SavedModel

**Describe the expected behavior**
I run prediction using the same function but the loaded model gives the error.
I tried installing the nightly tensorflow build but was not successful.

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (1 total):
    * Tensor(""inputs:0"", shape=(1, 8), dtype=int32)
  Keyword arguments: {'training': False}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (1 total):
    * {'attention_masks': TensorSpec(shape=(None, 25), dtype=tf.int32, name='inputs/attention_masks'), 'input_ids': TensorSpec(shape=(None, 25), dtype=tf.int32, name='inputs/input_ids')}
  Keyword arguments: {'training': False}

Option 2:
  Positional arguments (1 total):
    * {'attention_masks': TensorSpec(shape=(None, 25), dtype=tf.int32, name='attention_masks'), 'input_ids': TensorSpec(shape=(None, 25), dtype=tf.int32, name='input_ids')}
  Keyword arguments: {'training': False}

Option 3:
  Positional arguments (1 total):
    * {'attention_masks': TensorSpec(shape=(None, 25), dtype=tf.int32, name='inputs/attention_masks'), 'input_ids': TensorSpec(shape=(None, 25), dtype=tf.int32, name='inputs/input_ids')}
  Keyword arguments: {'training': True}

Option 4:
  Positional arguments (1 total):
    * {'attention_masks': TensorSpec(shape=(None, 25), dtype=tf.int32, name='attention_masks'), 'input_ids': TensorSpec(shape=(None, 25), dtype=tf.int32, name='input_ids')}
  Keyword arguments: {'training': True}

```
```"
37972,Tensorflow 2.x version is used in 1.x Tutorials,"## URL(s) with the issue: https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/keras/save_and_restore_models.ipynb

## Description of issue (what needs changing): The Tutorials corresponding to 1.x Version in Github has the version 2.x used inside it, thus leaving no Tutorials corresponding to 1.x (at least for Save and Restore)

### Clear description: Please find the screenshot in [this link](https://screenshot.googleplex.com/gT6SwTNeY3E).

### Correct links

Is the link to the source code correct? : Yes

### Parameters defined

Are all parameters defined and formatted correctly? : N/A

### Returns defined

Are return values defined? : N/A

### Usage example

Is there a usage example? : No, usage example for Save and Restore is not present for Tensorflow 1.x version

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content? : N/A

### Submit a pull request?: No

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
37971,Broken link for tf.debugging.assert_same_float_dtype,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/index.md

## Description of issue (what needs changing):
The link for `tf.debugging.assert_same_float_dtype` is dead

### Clear description

The previous link leads to: https://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tf/debugging/assert_same_float_dtype 
The new link must be:
https://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tf/debugging/assert_same_float_dtype.md
(The .md file extension is missing in the link)


### Submit a pull request?

Yes, will be updating the issue soon enough
"
37969,Memory leak in TensorFlow 2.0 DataSet when using group_by_window.,"**System information**
- OS Platform: - Google Cloud Linux Ubuntu 16.04 
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0.0
- Python version: 3.7
- CUDA/cuDNN version: 10.01
- GPU model and memory: Tesla P100, 16GB
- Running on GRAPH MODE:- YES

This is creating memory leak.
```
def pairwise_batch_iterator(tf_records,
                           no_threads=14,
                           batch_size=64,
                           num_epochs=50):
    
    dataset = make_dataset(tf_records, no_threads)
    dataset = dataset.repeat(num_epochs)
    
    dataset = dataset.apply(tf.data.experimental.group_by_window(
        key_func=lambda elem, *args: elem,
        reduce_func=lambda _, window: window.batch(batch_size),
        window_size=batch_size))
    
    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    return dataset
```

This works fine
```
def pairwise_batch_iterator(tf_records,
                           no_threads=14,
                           batch_size=64,
                           num_epochs=50):
    
    dataset = make_dataset(tf_records, no_threads)
    dataset = dataset.repeat(num_epochs)
    
    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    return dataset
```
***
I am training a pairwise ranking model, where I have 100k TF Records each file has all pairs belongs to one query id, While training I have to group by query id that's why I am using tf.data.experimental.group_by_window and this is creating a memory leak. (**This only happens when I have a huge number of TF records** ) If I use the second version of code I don't face any issue but I have to group by query id.
***

"
37968,tf.keras.models.Model.fit strange behaviour after upgrading from 2.1 to 2.2-rc1,"I was experimenting with `tf.keras.applications.inception_v3.InceptionV3` for classifying skin cancer lesions. It was going smooth since when Colaboratory decided to upgrade its VM's TF version from 2.1 to 2.2-rc1.

Now when loading the model from disk it says:
```text
WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.
```
But most importantly the `Model.fit` is not able anymore to properly process a `keras.utils.Sequence`! Indeed during training the steps per epoch are no more inferred from the `Sequence` object showing `1/Unknown`, also it does not actually terminate the epoch!

Snippet to reproduce the issue:
Colab code: https://colab.research.google.com/drive/1wdlWES83ibvLCwzpHrhJsQBNep-Aycqj
HAM1000 dataset: https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000
ISIC dataset: https://www.isic-archive.com/#!/topWithHeader/wideContentTop/main

The code stopped working overnight after Colab updated their VM images.

In the code the HAM dataset is automatically downloaded and rearranged provided you have `kaggle.json` file with the API Token. The ISIC dataset needs to be downlaoded manually from their official website or from my GDrive [here](https://drive.google.com/drive/folders/1E_xIwuA3HkovP4F_cDDzD1K-nEl0TNUC?usp=sharing)

**NB**: Also even if I create a new network and start training it with a `Sequence` the steps per epochs are not inferred as well. "
37967,Using experimental_new_converter generates opcode that is not found in kernel,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): tf-nightly on colab
- Tensorflow version (commit SHA if source): Version 2.2
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ARM Mbed OS (Cortex M K64F)

**Describe the problem**
I am trying the hello_world (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world).
Model : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb (with unroll set)

The model has operations SHAPE, GATHER, REDUCE_PROD etc which is not present in all_ops_resolver.cc. The Keras_lstm experimental example uses version tensorflowlite v2. I believe these operations are not supported in v1 . Any pointers on how to proceed. 

**Please provide the exact sequence of commands/steps when you ran into the problem**
Step 1: Model generation using https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb with tf.keras.layers.LSTM(20, unroll=True). Without setting unroll, I get more than 1 subgraph error.

Step 2: Use the example https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world to generate binary for Cortex-M 
gives error ""Didn't find op for builtin opcode 'SHAPE' version '1'""

My requirement is a working example of LSTM for Cortex-M (K64F)."
37966,Guide for Distributed TF with c++,"I see the core here https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime

But there is no instruction about how to run TF with distributed system (C++ language). 
Please share with us some documents about distributed TF c++.

Thanks,"
37964,Warning caused by local-scope dataset.map() function,"Just a quick observation (**tf = 2.1.0**)

Using `tf.data.Dataset.from_tensor_slices((filenames, labels)).map(FUNC)` when FUNC is in a local scope causes the warning:

**No warning:**

```
def mapper_func(f, l):
    return foo(f, l)

def create_dataset(input):
    return tf.data.Dataset.from_tensor_slices((input.filenames, input.labels)).map(mapper_func)
```

**Warning:**

```
def create_dataset(input):

    def mapper_func(f, l):
        return foo(f, l)

    return tf.data.Dataset.from_tensor_slices((input.filenames, input.labels)).map(mapper_func)
```


```
W0327 05:10:18.360908  4452 ag_logging.py:146] AutoGraph could not transform <function create_datasets.<locals>.parse_img at 0x0000020603A95268> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 
WARNING: AutoGraph could not transform <function create_datasets.<locals>.parse_img at 0x0000020603A95268> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 
```"
37963,tf dataset cache behave differently with filter for TF 1.x,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: NA
- TensorFlow installed from (source or
binary): binary (pip)
- TensorFlow version (use command below): v1.15.0-rc3-22-g590d6eef7e 1.15.0
- Python version: - Bazel
version (if compiling from source): NA
- GCC/Compiler version (if compiling from
source): NA
- CUDA/cuDNN version: - GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When applying filter before cache on `tf.data.Dataset`, the outputs when caching to file compared to caching to memory are different.

**Describe the expected behavior**

They should be the same.

TF 2x is working as expected.

**Standalone code to reproduce the issue** 

Cache to memory
```python
import tensorflow as tf

tf.enable_eager_execution()

data = tf.data.Dataset.from_tensor_slices(list(range(50)))
data = data.filter(lambda x: tf.random.uniform([]) < 0.5)
data = data.cache()

outputs = [x for x in data]
outputs_1 = [x for x in data]

assert len(outputs) == len(outputs_1)

for x, y in zip(outputs, outputs_1):
    assert x.numpy() == y.numpy()
```

Cache to file
```python
import tensorflow as tf

tf.enable_eager_execution()

data = tf.data.Dataset.from_tensor_slices(list(range(50)))
data = data.filter(lambda x: tf.random.uniform([]) < 0.5)
data = data.cache('/tmp/dummy_data')

outputs = [x for x in data]
outputs_1 = [x for x in data]

assert len(outputs) == len(outputs_1)

for x, y in zip(outputs, outputs_1):
    assert x.numpy() == y.numpy()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
37961,I meet a problem with HDFSWritableFile::Append,"**System information** 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux centos 7
- TensorFlow installed from (source or
binary): - find in tf_1.10, and recurrent in master.
- Python version: find in python2.7, recurrent in python3.6.5 
- Bazel version :find in 0.15.2, recurrent:2.0.0,
- GCC/Compiler version find in 4.8.5, recurrent:7.3.0

I meet a problem with HDFSWritableFile::Append
Background 1：     I save model and checkpoint in HDFS.
Background 2: 	My users want to add a big dict(30millon data, above 3GB) to graph.
The Problem:		HDFS abort quit when TF saves graph.txt to HDFS.
Part of logs:
```
File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 450, in after_create_session
    ""graph.pbtxt"")
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/graph_io.py"", line 71, in write_graph
    text_format.MessageToString(graph_def))
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 434, in atomic_write_string_to_file
    write_string_to_file(temp_pathname, contents)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 314, in write_string_to_file
    f.write(file_content)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 111, in write
    compat.as_bytes(file_content), self._writable_file, status)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
InvalidArgumentError: viewfs://hadoop-meituan/xxxx01/user/hadoop-waimai/xxxx/model/model//date/graph.pbtxt.tmp04d0a32366f548ec9f3aa629600fa19f; Invalid argument
```

I deal with this question by logs, then I get a result that the graph is too big to save. 
problem code:
```
Status HDFSWritableFile::Append(StringPiece data) {
    if (libhdfs()->hdfsWrite(fs_, file_, data.data(),
                             static_cast<tSize>(data.size())) == -1) {
       return IOError(filename_, errno);
}
```
data.size() return uint64_t, but hdfsWrite only accept int, so there are some questions when append a big string(len > INT_MAX)

So I change HDFSWritableFile::Append function to solve my question, and successfully solve it.
so I want to make a pull request ."
37960,TF 1.15.2 Memory Leak ,"**System information** 

- OS Platform and Distribution : macOS Catalina 10.15.3

- TensorFlow installed from : binary

- TensorFlow version : 1.15.2
- Python version: 3.7.3


**Describe the current behavior**

I am running simple tensorflow  sess.run as below -

`from tensorflow.python.framework import ops`
`import gc`
`import psutil`
`process = psutil.Process(os.getpid())`


`N_REPS = 10000`
`sess = tf.Session(graph=tf.Graph())`
`tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)`
`for i in range(N_REPS):`
        &nbsp;&nbsp;&nbsp;&nbsp;`sess.graph.finalize()`
	&nbsp;&nbsp;&nbsp;&nbsp;`tf.reset_default_graph()`
        &nbsp;&nbsp;&nbsp;&nbsp;`with tf.Graph().as_default() as graph:`
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`tf.get_default_graph().finalize()`
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`scores = self.sess.run(['<loss>'], feed_dict={'k':v})`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`memoryUsed = process.memory_info().rss`
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`del v`
	&nbsp;&nbsp;&nbsp;&nbsp;`gc.collect()`

The memory is increasing from 0th iteration till i reach 10000 iteration.

I also tried to see if any operation is added to graph by using `self.sess.graph.finalize()` and it run perfectly well without throwing error which means no new operation is added to graph .

**Describe the expected behavior**

I expect memory to be constant. 

How to solve this memory issue ? "
37959,ImportError: cannot import name 'load_from_saved_model',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): na
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): colab/ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): pip
- Python version: - Bazel
version (if compiling from source):1.4.0
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Error raised once ""import tensorflow as tf""
from tensorflow.python.keras.saving.saved_model import load_from_saved_model
ImportError: cannot import name 'load_from_saved_model'

It seems like it happens today, which ""import tensorflow"" will raise this error once I ""pip install tensorflow-gpu==1.14.0"". 

**Describe the expected behavior**

Try to run ""from tensorflow.python.keras.saving.saved_model import load_from_saved_model
""
It should import correctly without any error.

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
37958,Can I get a output shape( or size) of each node from NodeDef ?,"TF version: 1.13.1 (so I am using not eager execution for my project.)

I want to know the output tensor shape or size of each node before actually executing graph.
(meaning, Graph building part in tensorflow before going into executor part.)

I guess I can do it by extracting proper information from NodeDef protobuf. There is ""attr"" and I guess it possibly contains the output tensor shape information. However, I don't see it. I printed ""SummarizeAttrs(node_def)"" which is returning ""attr"" in string format. But I can't find the output size info from it. 

There is some way to get the output shape in python api level. (ref: [)](https://stackoverflow.com/questions/46127471/how-to-get-weights-from-pb-model-in-tensorflow)

But I can't do the same in TF source code (c++ core part of tf) level.

I am pretty sure there is a way. Could anybody know how to do it?

Thank you in advance.

P.S I looked up other documents and articles, but couldn't find the answer."
37957,segmentation fault,"/home/tf_mtrl# python tf_train.py
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
Retrying in 1 seconds
Loading configuration... done.
/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
~~~ Well done ~~~
~~~ Well done ~~~
~~~ Well done ~~~
~~~ Well done ~~~
~~~ Well done ~~~
~~~ Well done ~~~
!!! Collision !!!
~~~ Well done ~~~
~~~ Well done ~~~
Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown
Quitting (on error).
Segmentation fault (core dumped)
"
37956,Severe performance drop after updating,"**System information** 
- Fairly basic Keras/python script
- OS: Manjaro, up to date (as of 3/26)
- Device: Running on CPU (Intel 8th gen)
- TensorFlow (opt-cuda) installed from PacMan
-TF version: 2.1.0-4
- Python version: 3.8.2-1

**Explanation of issue**
I updated my system earlier today. A few minutes ago I started training a small feed-forward network using a python/Keras script. I had run this script before on a 95% identical training dataset, each epoch took about five seconds according to logs I saved. This time they took about four times as long. No other settings were changed. I double checked with a non-modified dataset and the results were the same.

According to logs, TensorFlow was previously on version 2.1.0-2 (now 2.1.0-4) The performance delta isn't caused by having other programs competing for CPU/RAM, or frequency/thermal throttling. I suppose minor updates to the kernel could cause some performance changes, but I don't see these causing such a profound difference.

I'd normally not care about a small performance drop, but since this was quite sizable I decided it was worth reporting."
37951,AttributeError: 'ObjectDetectionResult' object has no attribute 'image_id',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15.2
- Python version: 3.6.2
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source) : 0.24.1
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I try to evaluate my TensorFlow Lite model using the tool in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection: But when I try to process the dataset, I met some problem. Firstly, I use t the large dataset and limited RAM of my computer. Then I used the 2017 val COCO dataset and not met the memory problem now. But I meet the problem:  AttributeError: 'ObjectDetectionResult' object has no attribute 'image_id'.

My code for running :
bazel run //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival -- \
  --images_folder=/home/sicong/Documents/datasets/COCO/for_tflite_evaluation/val2014 \
  --instances_file=/home/sicong/Documents/datasets/COCO/annotations_trainval2017/annotations/instances_val2017.json \
  --whitelist_file=/home/sicong/Documents/datasets/udacity/Udacity_coco/image_id.txt \
  --output_folder=/home/sicong/Documents/datasets/COCO/for_tflite_evaluation \
  --num_images=1000

Error Information:
INFO: Build completed successfully, 305 total actions
Traceback (most recent call last):
  File ""/home/sicong/.cache/bazel/_bazel_sicong/7b2e833de2ecb2ea31568e76c69b68a8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py"", line 211, in <module>
    _dump_data(ground_truths, args.images_folder, args.output_folder)
  File ""/home/sicong/.cache/bazel/_bazel_sicong/7b2e833de2ecb2ea31568e76c69b68a8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py"", line 152, in _dump_data
    detection_result.image_id = image_dict['id']
AttributeError: 'ObjectDetectionResult' object has no attribute 'image_id'

Is there any solution? Thank you!
"
37950,"UnicodeDecodeError: ""utf-8"" codec can't decode byte in position : invalid start byte","**System information** 

- OS Platform and Distribution : ubuntu 18.04
the issue happens on mobile device: 
- TensorFlowinstalled with pip3  : v1.14.0

I generate my own dataset .tfrecord with that modify code:

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import hashlib
import io
import logging
import os

from lxml import etree
import PIL.Image
import tensorflow as tf

from object_detection.utils import dataset_util
from object_detection.utils import label_map_util


flags = tf.app.flags
flags.DEFINE_string('data_dir', '', 'Root directory to raw PASCAL VOC dataset.')
flags.DEFINE_string('set', 'train', 'Convert training set, validation set or '
                    'merged set.')
flags.DEFINE_string('annotations_dir', 'Annotations',
                    '(Relative) path to annotations directory.')
flags.DEFINE_string('year', 'VOC2007', 'Desired challenge year.')
flags.DEFINE_string('output_path', '', 'Path to output TFRecord')
flags.DEFINE_string('label_map_path', 'data/pascal_label_map.pbtxt',
                    'Path to label map proto')
flags.DEFINE_boolean('ignore_difficult_instances', False, 'Whether to ignore '
                     'difficult instances')
FLAGS = flags.FLAGS

SETS = ['train', 'val', 'trainval', 'test']
YEARS = ['VOC2007', 'VOC2012', 'merged']


def dict_to_tf_example(data,
                       dataset_directory,
                       label_map_dict,
                       ignore_difficult_instances=False,
                       image_subdirectory='JPEGImages'):
  """"""Convert XML derived dict to tf.Example proto.

  Notice that this function normalizes the bounding box coordinates provided
  by the raw data.

  Args:
    data: dict holding PASCAL XML fields for a single image (obtained by
      running dataset_util.recursive_parse_xml_to_dict)
    dataset_directory: Path to root directory holding PASCAL dataset
    label_map_dict: A map from string label names to integers ids.
    ignore_difficult_instances: Whether to skip difficult instances in the
      dataset  (default: False).
    image_subdirectory: String specifying subdirectory within the
      PASCAL dataset directory holding the actual image data.

  Returns:
    example: The converted tf.Example.

  Raises:
    ValueError: if the image pointed to by data['filename'] is not a valid JPEG
  """"""
  print("" ---- {0} \n {1} \n {2}"".format(data['folder'], image_subdirectory, data['filename']))
  img_path = os.path.join(data['folder'], image_subdirectory, data['filename'])
  full_path = os.path.join(dataset_directory, img_path)
  with tf.gfile.GFile(full_path, 'rb') as fid:
    encoded_jpg = fid.read()
  encoded_jpg_io = io.BytesIO(encoded_jpg)
  image = PIL.Image.open(encoded_jpg_io)
  if image.format != 'JPEG':
    raise ValueError('Image format not JPEG')
  key = hashlib.sha256(encoded_jpg).hexdigest()

  width = int(data['size']['width'])
  height = int(data['size']['height'])

  xmin = []
  ymin = []
  xmax = []
  ymax = []
  classes = []
  classes_text = []
  truncated = []
  poses = []
  difficult_obj = []
  if 'object' in data:
    for obj in data['object']:
      difficult = bool(int(obj['difficult']))
      if ignore_difficult_instances and difficult:
        continue

      difficult_obj.append(int(difficult))

      xmin.append(float(obj['bndbox']['xmin']) / width)
      ymin.append(float(obj['bndbox']['ymin']) / height)
      xmax.append(float(obj['bndbox']['xmax']) / width)
      ymax.append(float(obj['bndbox']['ymax']) / height)
      classes_text.append(obj['name'].encode('utf8'))
      classes.append(label_map_dict[obj['name']])
      truncated.append(int(obj['truncated']))
      poses.append(obj['pose'].encode('utf8'))

  example = tf.train.Example(features=tf.train.Features(feature={
      'image/height': dataset_util.int64_feature(height),
      'image/width': dataset_util.int64_feature(width),
      'image/filename': dataset_util.bytes_feature(
          data['filename'].encode('utf8')),
      'image/source_id': dataset_util.bytes_feature(
          data['filename'].encode('utf8')),
      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),
      'image/encoded': dataset_util.bytes_feature(encoded_jpg),
      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),
      'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),
      'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),
      'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),
      'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),
      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
      'image/object/class/label': dataset_util.int64_list_feature(classes),
      'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),
      'image/object/truncated': dataset_util.int64_list_feature(truncated),
      'image/object/view': dataset_util.bytes_list_feature(poses),
  }))
  return example


def main(_):
  if FLAGS.set not in SETS:
    raise ValueError('set must be in : {}'.format(SETS))
  if FLAGS.year not in YEARS:
    raise ValueError('year must be in : {}'.format(YEARS))

  data_dir = FLAGS.data_dir
  years = ['VOC2007', 'VOC2012']
  if FLAGS.year != 'merged':
    years = [FLAGS.year]

  writer = tf.python_io.TFRecordWriter(FLAGS.output_path)

  label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)

  for year in years:
    logging.info('Reading from PASCAL %s dataset.', year)
    examples_path = os.path.join(data_dir, 'ImageSets', 'Main',
                                 'default.txt')
    annotations_dir = os.path.join(data_dir, FLAGS.annotations_dir)
    examples_list = dataset_util.read_examples_list(examples_path)
    for idx, example in enumerate(examples_list):
      if idx % 100 == 0:
        logging.info('On image %d of %d', idx, len(examples_list))
      path = os.path.join(annotations_dir, example + '.xml')
      if not os.path.exists(path) : 
        continue
      with tf.gfile.GFile(path, 'r') as fid:
        xml_str = fid.read()
      xml = etree.fromstring(xml_str)
      data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']

      tf_example = dict_to_tf_example(data, FLAGS.data_dir, label_map_dict,
                                      FLAGS.ignore_difficult_instances)
      writer.write(tf_example.SerializeToString())

  writer.close()


if __name__ == '__main__':
  tf.app.run()`

```

I use that command to train my model:
```
`
export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
PIPELINE_CONFIG_PATH=/data/ssd_mobilenet_v1_coco_2018_01_28/ssd_mobilenet_v1_coco_2018_01_28/pipeline.config
MODEL_DIR=/data/training
NUM_TRAIN_STEPS=1500
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python3 object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr
`
```

After few iterations I get that error:

```
`  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py"", line 519, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py"", line 539, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py"", line 920, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 477, in evaluate
    name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 519, in _actual_eval
    return _evaluate()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 501, in _evaluate
    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1501, in _evaluate_build_graph
    self._call_model_fn_eval(input_fn, self.config))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1534, in _call_model_fn_eval
    input_fn, ModeKeys.EVAL)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1022, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1113, in _call_input_fn
    return input_fn(**kwargs)
  File ""/home/label/models/research/object_detection/inputs.py"", line 625, in _eval_input_fn
    params=params)
  File ""/home/label/models/research/object_detection/inputs.py"", line 725, in eval_input
    transform_input_data_fn=transform_and_pad_input_data_fn)
  File ""/home/label/models/research/object_detection/builders/dataset_builder.py"", line 130, in build
    num_additional_channels=input_reader_config.num_additional_channels)
  File ""/home/label/models/research/object_detection/data_decoders/tf_example_decoder.py"", line 319, in __init__
    default_value=''),
  File ""/home/label/models/research/object_detection/data_decoders/tf_example_decoder.py"", line 64, in __init__
    label_map_proto_file, use_display_name=False)
  File ""/home/label/models/research/object_detection/utils/label_map_util.py"", line 172, in get_label_map_dict
    label_map = load_labelmap(label_map_path_or_proto)
  File ""/home/label/models/research/object_detection/utils/label_map_util.py"", line 139, in load_labelmap
    label_map_string = fid.read()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 128, in read
    pywrap_tensorflow.ReadFromStream(self._read_buf, length))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 98, in _prepare_value
    return compat.as_str_any(val)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/compat.py"", line 117, in as_str_any
    return as_str(value)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/compat.py"", line 87, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb4 in position 10: invalid start byte
`
```

My tfrecord seems to be corrupted.
"
37949,tensorflow Docker behaves different to host (produces nan),"
**System information** 
Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Mint 19.3 (ubuntu)
TensorFlow installed from (source or
binary): binary
TensorFlow version (use command below): tf-nightly
Python version: 3.8 and 3.6
 
two systems:
Host: NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1 TF: 2.2.0-dev20200325
Docker: NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1 TF: 2.2.0-dev20200325

**Describe the current behavior**
In Docker the gradient calculation produces only NANs
On the host the same model produces normal gradients

**Describe the expected behavior**
in docker the same gradients should be produced

**Standalone code to reproduce the issue** 
can not be provided, sorry, very big model, and problem is hard to locate.
if i can locate the problem i will provide a Standalone code

**Other info / logs** Include any logs or source code that would be helpful to
Gradient magnitude on host:
```
...
    [3.31210213e-05 -2.52332793e-05 -2.79605388e-06 ... -1.31225511e-06 1.79109138e-05 3.6471597e-06]
    [-4.21071627e-06 5.53799646e-06 4.40907201e-07 ... -7.86542842e-07 -6.31262401e-06 -7.09808887e-07]
    [-2.62982558e-05 -6.01945476e-06 -3.60058812e-06 ... 4.37716608e-06 5.87437571e-05 4.91119135e-06]]
```"
37945,tensorflow v2  save and load model,"I got a problem for saving model with tensorflow 2.0 ,it's awful,I can not know how to save model with
that way,it wants me have a signatures for saving my model ,but I can not save my model like :

class Model(tf.keras.Model):

  @tf.function
  def call(self, x):
    ...

m = Model()
tf.saved_model.save(
    m, '/tmp/saved_model/',
    signatures=m.call.get_concrete_function(
        tf.TensorSpec(shape=[None, 3], dtype=tf.float32, name=""inp"")))

I don't know what ""get_concrete_function"" function is;
Somebody can help me?"
37943,"when using tensorflow inside flask server, I am getting localhost container not found error.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15.0
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: pip in a virtual environment

**Describe the problem**

I am trying to make a flask server and deploy my model there and connect that server to my android application. Without deploying the model in the server, the flask server and android app are connected successfully. But, after writing my model code in the flask server it gives the following error:

-----------------------
2020-03-26 17:52:10.868137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-26 17:52:10.868137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      
2020-03-26 17:52:11.075640: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at resource_variable_ops.cc:660 : Not found: Container localhost does not exist. (Could not find resource: localhost/encoder_embedding/embeddings)
[2020-03-26 17:52:11,075] ERROR in app: Exception on / [GET]
Traceback (most recent call last):
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\flask\app.py"", line 2446, in wsgi_app
    response = self.full_dispatch_request()
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\flask\app.py"", line 1951, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\flask\app.py"", line 1820, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\flask\_compat.py"", line 39, in reraise
    raise value
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\flask\app.py"", line 1949, in full_dispatch_request
    rv = self.dispatch_request()
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\flask\app.py"", line 1935, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""H:\PYTHON\PyCharm Projects\FlaskServer\app.py"", line 302, in hello_world
    reply = convert(""Hi"")
  File ""H:\PYTHON\PyCharm Projects\FlaskServer\app.py"", line 267, in convert
    initial_state = model_encoder.predict(input_tokens)
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 908, in predict
    use_multiprocessing=use_multiprocessing)
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\tensorflow_core\python\keras\engine\training_arrays.py"", line 723, in predict
    callbacks=callbacks)
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\tensorflow_core\python\keras\engine\training_arrays.py"", line 394, in model_iteration
    batch_outs = f(ins_batch)
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\tensorflow_core\python\keras\backend.py"", line 3476, in __call__
    run_metadata=self.run_metadata)
  File ""H:\PYTHON\Environments\Flask_env_PyCharm\lib\site-packages\tensorflow_core\python\client\session.py"", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.NotFoundError: Container localhost does not exist. (Could not find resource: localhost/encoder_embedding/embeddings)
	 [[{{node encoder_embedding/embedding_lookup}}]]
127.0.0.1 - - [26/Mar/2020 17:52:11] ""GET / HTTP/1.1"" 500 -

-------------------------------


I opened an issue previously for "".tflite"" conversion. But, as an alternative, I created a flask server. And now this is the only error I am facing. Before I tried with TensorFlow 2.0.0. But, as I had to use 3 or 4 lines of version 1 code, I installed 1.15.0.

P.S: This issue is related to this [issue](https://github.com/tensorflow/tensorflow/issues/30352). I've tried it in many ways. But, ultimately I face failure. Please, help me fix this error so that I could close both the issues. You need not worry about the responses or results of the trained model. 

--------

The code and dataset are in this file:
[code.zip](https://github.com/tensorflow/tensorflow/files/4387137/code.zip)

I am also placing my drive link for checkpoint files so that you need not train the model.
https://drive.google.com/open?id=1wdHQRGMMealZv2ygrSK5LKWGyNgxVNPT
"
37942,GPU-accelerated LSTMs crash randomly with: [ InternalError:  [_Derived_] Failed to call ThenRnnBackward with model config ],"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro N, Build 17763
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Pypi
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version: 3.7.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: CUDA 10.1, cudnn-10.1-windows10-x64-v7.6.5.32
- GPU model and memory: GTX 1060, 6 GB

**Describe the current behavior**

Dear Tensorflow-Developers,

my jupyter notebook that is training some LSTMs on the GPU crashes after some time with the following traceback:

```
InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 100, 100, 1, 249, 32, 100] 
	 [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
	 [[StatefulPartitionedCall_1]] [Op:__inference_distributed_function_7604]

Function call stack:
distributed_function -> distributed_function -> distributed_function
```

This crash happens after a random amount of epochs (sometimes 6, sometimes 130+ sometimes 300+). It also crashes on different Windows machines with different GPUs.

Please see this minimal notebook to reproduce the behaviour that also includes the whole stacktrace: https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5

In the stacktrace I can find the following line: 
`130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?`

I wonder if this is connected to this issue? 🙂 

On a CPU-training everything works well and stable.

Thank you kindly in advance for your consideration and great work. 🚀 

**Describe the expected behavior**

The GPU-accelerated LSTM should not crash randomly.

**Standalone code to reproduce the issue** 

https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5

**Other info / logs** 

For the full traceback, please check the gist from above.
"
37941,Update vocabulary and add a new embeddings for the new token as you see them in training time,"I am aware i can use `x = tf.feature_column.categorical_column_with_vocabulary_list(cat_col_name,cat_col_unique_values)`. Where `cat_col_name` is the categorical column that i need to convert to consumable format by a dense layer `cat_col_unique_values` are the unique values present in this column. Then i use something like `feature_column.embedding_column(x, dimension=4)` to map each unique value in `cat_col_name` to an embedding of dimension 4. 

What if i have a really large dataset and it would not be feasable to know `cat_col_unique_values` beforehand. I mean to say, During training time:

1. keep reading batches and mapping the unique values seen to their corresponding embedding
2. In case a new value is seen in the columns cat_col_name, add another embedding to correspond to this newfound value

I dont suppose there is a way to do this currently, or is there?
"
37938,Anconda Tensorflow-GPU 2.1.0 cudart64_101.dll not found,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Conda
- TensorFlow version: 2.1.0
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: Conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA GTX 1060 6GB

**Describe the problem**
Can't use tensorflow with GPU.
This warning always show up : 
2020-03-26 14:16:44.969758: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-03-26 14:16:44.975297: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Installed CUDA on Windows
2. Setting the Path as the tutorial said
3. Installed tensorflow with ""conda install -c anaconda tensorflow-gpu""

**Any other info / logs**
Cupy works (need cuda also) on my current environment. So I wonder what's wrong.
I found multiple cudart64_101.dll in these directories : 
1. On Cuda installation : C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin
2. On Anaconda environment directory : C:\Users\user\Anaconda3\Library\bin
"
37935,Compile error on Windows,"**System information** 
- OS Platform and Distribution: Windows 10
- TensorFlow installed from source
- TensorFlow version 2.1
- Python version: 3.7.1
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): Visual Studio 2019
- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5
- GPU model and memory: NVidia GForce GTX 1060 3GB

**Describe the current behavior**

I am trying to build a Windows DLL exporting custom code dependent on Tensorflow:

load(""//tensorflow:tensorflow.bzl"", ""tf_cc_binary"")

tf_cc_binary(
    name = ""Inference_GPU.dll"",
    linkshared = True,
    srcs = glob([""*.cpp"", ""*.h""]),
    defines	= [""_WINDOWS"", ""INFERENCE_NET_EXPORTS"", ""TF_GPU""],
    deps = [
	""//tensorflow/cc:cc_ops"",
        ""//tensorflow/cc:client_session"",
        ""//tensorflow/core:tensorflow"",	
    ],
	copts = [""/O2"", ""/Gm-"", ""/Zi"", ""/FS""],
    linkopts = [""/DEBUG""]
)

Run the following bazel command:
bazel --output_base=F:\Bazel_cache build --config=opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/cc/Inference:Inference_GPU.dll --jobs 2

Bazel stopped with the following error:

F:\bazel_cache\execroot\org_tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src/Tensor/TensorExecutor.h(381): error: identifier ""std::conj<double> "" is undefined in device code

4 errors detected in the compilation of ""F:/bazel_temp/nvcc_inter_files_tmp_dir/tmpc6wr1kw5/cwise_op_gpu_conj.cu.compute_75.cpp1.ii"".

Thank you,
George Scortaru
"
37934,roadmap link broken,"in [tensorflow.org](https://www.tensorflow.org/community) website the below roadmap link is  not working
"
37933,TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Windows 8.1
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below):  Tensorflow 2.0
- Python version: - Bazel
version (if compiling from source): Python 3.5

**Describe the current behavior**
I am trying to migrate TF1.14 code to TF 2.0. However, some functions are disappeared in TF2.0. 
I changed the codes like this:
loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=[
                self.winner_loc[0] + self.winner_loc[1]])
            optimizer = tf.keras.optimizers.SGD(learning_rate=alpha_op)
            optimizer.minimize(loss, self.weightage_vects)
but when I run the code, the error happen:
TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable in the line:
optimizer.minimize(loss, self.weightage_vects)

I tried to change my code like:
with tf.GradientTape() as tape:
             loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=[
                   self.winner_loc[0] + self.winner_loc[1]])
            gradients = tape.gradient(target=loss, sources=self.weightage_vects)
            optimizer.apply_gradients(zip(gradients, self.weightage_vects))
but other errors happened: TypeError: zip argument #1 must support iteration

I don't know how to correct these codes. T.T Please Help!
"
37932,CUDNN_STATUS_INTERNAL_ERROR after hours of HyperParameter optimization.,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): **I'm using completely standard Tensorflow / Keras implementations, see code below.**
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): **Windows 10**
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): **tensorflow==2.1.0**
- Python version: - Bazel
version (if compiling from source): **Python 3.7**
- CUDA/cuDNN version: - GPU model and memory: **Nvidia GTX 1070 8GB, Cuda compilation tools release 10.1, V10.1.243**

**Describe the current behavior**

I was training and tuning HyperParameters using ""Keras Tuner"" for about ~2 hours until I noticed that no new trials where created in the ""Tuner project"" folder, this is my training setup:
![image](https://user-images.githubusercontent.com/17565925/77623713-e17ee000-6f40-11ea-9d7b-bc9e2fad9b5f.png)

And this is my dynamic model:
![image](https://user-images.githubusercontent.com/17565925/77623844-24d94e80-6f41-11ea-831c-55d1eb62ee31.png)

After I noticed the stand-still I checked my terminal and got the following error:
![Fejlmeddelse](https://user-images.githubusercontent.com/17565925/77624487-3111db80-6f42-11ea-84e4-70a111ff2478.png)


**Describe the expected behavior**

I would expect the training to keep running. It completed 48 permutations training each one 3 times, i.e. 144 models without error, so the sudden interruption and kernel restart is weird to me!

**Standalone code to reproduce the issue** 

In addition to the code referenced above, I've pretty much followed the setup in this Tensorflow tutorial: [Time series forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series#single_step_model)
"
37931,How can I get all tensor names in tflite model? Not only input_details and output_details,
37928,Building TensorFlow Lite NNAPI with C/C++ for Android: runtime link error libnnapi_delegate.so not found,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy Fold, Samsung Galaxy S10, Oppo ... etc Android Devices
- TensorFlow installed from (source or binary): source
- TensorFlow version: latest (cloned 3 days ago)
- Python version: 2.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): Apple clang version 11.0.0 (clang-1100.0.33.17)
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I'm building TensorFlow Lite with C/C++ to run on Android devices. Building C, GPU delegate works well, but when I tried to link NNAPI delegate(```libnnapi_delegate.so```), library open fail on runtime.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Got ```nnapi_delegate.so``` from
```
bazel build -c opt --config=android_arm64 //tensorflow/lite/delegates/nnapi:nnapi_delegate
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

### Error Log
```
2020-03-26 15:37:26.311 20452-20452/com.example.hellolibs E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.example.hellolibs, PID: 20452
    java.lang.UnsatisfiedLinkError: dlopen failed: library ""/Users/yonggyulee/Documents/GitHub/AI-Kit/tflite/c/hello-libs/app/src/main/cpp/../../../../distribution/tflite/lib/arm64-v8a/libnnapi_delegate.so"" not found
        at java.lang.Runtime.loadLibrary0(Runtime.java:1016)
        at java.lang.System.loadLibrary(System.java:1669)
        at com.example.hellolibs.MainActivity.<clinit>(MainActivity.java:32)
        at java.lang.Class.newInstance(Native Method)
        at android.app.AppComponentFactory.instantiateActivity(AppComponentFactory.java:69)
        at androidx.core.app.CoreComponentFactory.instantiateActivity(CoreComponentFactory.java:41)
        at android.app.Instrumentation.newActivity(Instrumentation.java:1219)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3040)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3292)
        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)
        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1980)
        at android.os.Handler.dispatchMessage(Handler.java:106)
        at android.os.Looper.loop(Looper.java:214)
        at android.app.ActivityThread.main(ActivityThread.java:7168)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:494)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:975)
```

### CMakeLists.txt
```
cmake_minimum_required(VERSION 3.4.1)

# configure import libs
set(distribution_DIR ${CMAKE_CURRENT_SOURCE_DIR}/../../../../distribution)

add_subdirectory(""${CMAKE_CURRENT_SOURCE_DIR}/opencv"")

add_library(lib_tflite SHARED IMPORTED)
set_target_properties(lib_tflite PROPERTIES IMPORTED_LOCATION
        ${distribution_DIR}/tflite/lib/arm64-v8a/libtensorflowlite_c.so)

add_library(lib_tflite_gpu SHARED IMPORTED)
set_target_properties(lib_tflite_gpu PROPERTIES IMPORTED_LOCATION
        ${distribution_DIR}/tflite/lib/arm64-v8a/libtensorflowlite_gpu_delegate.so)

add_library(lib_nnapi SHARED IMPORTED)
set_target_properties(lib_nnapi PROPERTIES IMPORTED_LOCATION
        ${distribution_DIR}/tflite/lib/arm64-v8a/libnnapi_delegate.so)

# build application's shared lib
set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -std=gnu++11"")

add_library(hello-libs SHARED
        hello-libs.cpp)

target_include_directories(hello-libs PRIVATE
        ${distribution_DIR}/tflite/include
        ${distribution_DIR}/tflite/include/tensorflow/lite/tools/make/downloads
        ${distribution_DIR}/tflite/include/tensorflow/lite/tools/make/downloads/flatbuffers/include
)

target_link_libraries(hello-libs
        android
        lib_tflite
        lib_tflite_gpu
        neuralnetworks
        lib_nnapi  # If i remove this line, there is no error, and everything including inference works well.
        opencv
        log)
```
"
37926,ValueError: Unknown floatx type: bfloat16 tf.keras.backend.set_floatx('bfloat16'),"**System information** 
Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Mint 19.3 (ubuntu)
TensorFlow installed from (source or
binary): binary
TensorFlow version (use command below): tf-nightly
Python version: 3.8 and 3.6

**Describe the current behavior**
when using layers with bfloat16 they are autocast to float32, and a warning/info gives a hint to use
`tf.keras.backend.set_floatx('bfloat16')`
when using the function a exception is raised
`ValueError: Unknown floatx type: bfloat16`


**Describe the expected behavior**
data type should be recognised

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
Info from Tensorflow:
```
WARNING:tensorflow:Layer compress is casting an input tensor from dtype bfloat16 to the layer's dtype of float16, which is new behavior in TensorFlow 2.  The layer has dtype float16 because its dtype defaults to floatx.

If you intended to run this layer in float16, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype bfloat16 by default, call `tf.keras.backend.set_floatx('bfloat16')`. To change just this layer, pass dtype='bfloat16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.
```
Error if `tf.keras.backend.set_floatx('bfloat16')` is used:
```
2.2.0-dev20200325
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home//.vscode-server/extensions/ms-python.python-2020.3.69010/pythonFiles/lib/python/debugpy/no_wheels/debugpy/__main__.py"", line 45, in <module>
    cli.main()
  File ""/home//.vscode-server/extensions/ms-python.python-2020.3.69010/pythonFiles/lib/python/debugpy/no_wheels/debugpy/../debugpy/server/cli.py"", line 427, in main
    run()
  File ""/home//.vscode-server/extensions/ms-python.python-2020.3.69010/pythonFiles/lib/python/debugpy/no_wheels/debugpy/../debugpy/server/cli.py"", line 264, in run_file
    runpy.run_path(options.target, run_name=""__main__"")
  File ""/usr/lib/python3.6/runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""/usr/lib/python3.6/runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home//Docker/volumes/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/ShAReD_Net/training/run_train_train_model.py"", line 206, in <module>
    main()
  File ""/home//Docker/volumes/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/ShAReD_Net/training/run_train_train_model.py"", line 50, in main
    tf.keras.backend.set_floatx(dtype.name)
  File ""/home//Docker/volumes/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/keras/backend_config.py"", line 108, in set_floatx
    raise ValueError('Unknown floatx type: ' + str(value))
ValueError: Unknown floatx type: bfloat16```"
37924,_pywrap_file_io.BufferedInputStream raises UnicodeDecodeError,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows10 Home Edition
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): 2.2.0rc1
- Python version: 3.7.7 
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source):  None
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: GeForce GTX 980Ti 38845MB

**Describe the current behavior**
In jupyter notebook, I have tried TF Models tutorial(research/object_detection/object_detection_tutorial.ipynb) and got UnicodeDecodeError at following section.

List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'
category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)

error log:
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8e in position 90: invalid start byte

**Describe the expected behavior**
UnicodeDecodeError don't happen. 

**Standalone code to reproduce the issue** 
from tensorflow.python import _pywrap_file_io
_read_buf=_pywrap_file_io.BufferedInputStream('test', 1024*512)

**Other info / logs** 
My system language is Japanese and this may affect encode behavior."
37923,How to get a flowchart for fireworks cnn optimization algorithm.,"def fire_incept(x, fire=16, intercept=64):
    x = Conv2D(fire, (5,5), strides=(2,2))(x)
    x = LeakyReLU(alpha=0.15)(x)
    
    left = Conv2D(intercept, (3,3), padding='same')(x)
    left = LeakyReLU(alpha=0.15)(left)
    
    right = Conv2D(intercept, (5,5), padding='same')(x)
    right = LeakyReLU(alpha=0.15)(right)
    
    x = concatenate([left, right], axis=3)
    return x

def fire_squeeze(x, fire=16, intercept=64):
    x = Conv2D(fire, (1,1))(x)
    x = LeakyReLU(alpha=0.15)(x)
    
    left = Conv2D(intercept, (1,1))(x)
    left = LeakyReLU(alpha=0.15)(left)
    
    right = Conv2D(intercept, (3,3), padding='same')(x)
    right = LeakyReLU(alpha=0.15)(right)
    
    x = concatenate([left, right], axis=3)
    return x

image_input=Input(shape=input_shape)

x = fire_incept((image_input), fire=16, intercept=16)

x = fire_incept(x, fire=32, intercept=32)
x = fire_squeeze(x, fire=32, intercept=32)

x = fire_incept(x, fire=64, intercept=64)
x = fire_squeeze(x, fire=64, intercept=64)

x = fire_incept(x, fire=64, intercept=64)
x = fire_squeeze(x, fire=64, intercept=64)

x = Conv2D(64, (3,3))(x)
x = LeakyReLU(alpha=0.1)(x)

x = Flatten()(x)

x = Dense(512)(x)
x = LeakyReLU(alpha=0.1)(x)
x = Dropout(0.1)(x)

out = Dense(len(SPECIES), activation='softmax')(x)

model_new = Model(image_input, out)
model_new.summary()"
37922,Create a Sequential model in tensorflow,"Hi All,
         I have two models say M1, M2, I want to combine these models in such a way that the output of M1 is given as input to M2 and the gradient should be backpropagated till M1's input."
37920,Cannot import ImageDataGenerator from tensorflow.keras GPU version,"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.6 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.5
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: 10.1/7.3.1
- GPU model and memory: GeForce RTX 2080Ti 10989MiB

**Describe the current behavior**
I am transfering from TensorFlow 1 to 2 and my codes are all running correct in TensorFlow 1.13 & 1.15. I met the problem when I try to import the ImageDataGenerator: no matter to use `tf.keras.processing.image` or `tf.python.keras.processing.image`.

**Describe the expected behavior**
In TensorFlow 1.13 & 1.15 and TensorFlow 2.0.0 CPU version, using `from tensorflow.keras.preprocessing.image import ImageDataGenerator` can import the ImageDataGenerator normally. But for TensorFlow 2.1.0 GPU version, it shows error even I add `.python` right after `tensorflow`. Here ""CPU version"" or ""GPU version"" means the hardware status of the PC I use. 

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

`from tensorflow.keras.preprocessing.image import ImageDataGenerator # option 1`
`from tensorflow.python.keras.preprocessing.image import ImageDataGenerator # option 2`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

For now the only way I think it can work is to revise the content in ""__init__.py"" in somewhere within the TensorFlow package. But it will not be a good solution so I wonder if anyone can help me with this. Thanks. 

`Traceback (most recent call last):`
`  File ""<stdin>"", line 1, in <module>`
`  File ""/home/daiwei/.conda/envs/daiwei_tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/preprocessing/__init__.py"", line 23, in <module>`
`    import keras_preprocessing`
`ModuleNotFoundError: No module named 'keras_preprocessing'`
"
37913,Performance of Tensorflow distributed training parameter server strategy is much slower for multi gpu multi worker environment,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): source
- TensorFlow version (use command below): 1.14 
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: Cuda 10.1 cuDNN 7.6.5
- GPU model and memory: V100

**Describe the current behavior**
I'm doing a performance test on my object detection model on multiple workers, multiple gpus on each worker environment. I'm testing the parameter server strategy and mirror strategy.

strategy | workers | gpus per worker | time per 100 iters
-- | -- | -- | --
no distributed training | 1 | 1 | 53s
mirror strategy + NCCL | 1 | 8 | 78~79s
parameter server strategy | 8 | 1 | 65~70s per 800 iters
parameter server strategy | 8 | 8 | 260s per 800 iters

I noticed that the time spend on the 8 worker * 8 gpus per worker is much slower than the time spend on 8 worker * 1 gpus per worker + the overhead of 1 worker * 8 gpus per worker.

After I profile the code, I noticed that although we aggregate gradient on each worker and send to the parameter server. Each gpu is reading variable from the parameter server directly. Therefore, the parameter server is sending variables 8*8=64 times.

**Describe the expected behavior**

I'm wondering if there's a way to change the strategy so that the parameter server will send the variable to each worker and all gpus on the worker load variables from the worker local memory instead.

If possible, could you point me to a few code lines that I can modify."
37912,@tf.Module.with_name_scope AttributeError: __enter__,"**System information** 
Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Mint 19.3 (ubuntu)
TensorFlow installed from (source or
binary): binary
TensorFlow version (use command below): tf-nightly
Python version: 3.8 and 3.6

**Describe the current behavior**
Exeption is raised, when @tf.Module.with_name_scope is used. The context manager is not implemented proper.

**Describe the expected behavior**
work without error

**Standalone code to reproduce the issue** 
```
import tensorflow as tf

class DummyModel(tf.keras.layers.Layer):
    
    def __init__(self, name = ""DummyModel"", **kwargs):  
        super().__init__(name = name, **kwargs)

    @tf.Module.with_name_scope
    def build(self, inputs_shape):
        super().build(inputs_shape)
    
    @tf.Module.with_name_scope
    def call(self, inputs):
        return inputs
    
dm = DummyModel()


print(dm([1]))
```

**Other info / logs** 
```
AttributeErrorTraceback (most recent call last)
<ipython-input-7-2dd7f6e61139> in <module>
----> 1 import src.test_use_namescope

~tf/pose3D/src/test_use_namescope.py in <module>
     17 
     18 
---> 19 print(dm([1]))

~tf/pose3D/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    967         # Eager execution on data tensors.
    968         with backend.name_scope(self._name_scope()):
--> 969           self._maybe_build(inputs)
    970           cast_inputs = self._maybe_cast_inputs(inputs)
    971           with base_layer_utils.autocast_context_manager(

~tf/pose3D/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)
   2364         # operations.
   2365         with tf_utils.maybe_init_scope(self):
-> 2366           self.build(input_shapes)  # pylint:disable=not-callable
   2367       # We must set also ensure that the layer is marked as built, and the build
   2368       # shape is stored since user defined build functions may not be calling

~tf/pose3D/venv/lib/python3.6/site-packages/tensorflow/python/module/module.py in method_with_name_scope(self, *args, **kwargs)
    286     """"""
    287     def method_with_name_scope(self, *args, **kwargs):
--> 288       with self.name_scope:
    289         return method(self, *args, **kwargs)
    290 

AttributeError: __enter__
```
"
37910,"MicroInterpreter error: Seg Fault, *** stack smashing detected *** inside MicroInterpreter::Invoke() method","@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 64-bit, 
- TensorFlow installed from (source or binary): Source 
- Tensorflow version (commit SHA if source): 0c487d64172c64d60a93bc98cf5ea07f1a8e95ba
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Ubuntu 18.04.4 64-bit, Linux raspberrypi 4.19.97-v7
- Compiler: g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0 and arm-linux-gnueabihf-g++ 8.4.0
- Python: Python 3.7.6

**Describe the problem**
I am trying to use the quantized model with Tensorflow Lite Micro, and got a segmentation error inside interpreter->Invoke() call.

Debugger showed that segmentation error occurred on returning from Eval() in conv.cc on Node 28 of CONV_2D, and stack was corrupted. Error message is *** stack smashing detected ***: <unknown> terminated with compiler flags ""-fstack-protector-all -Wstack-protector"".

My test was simply from the [person detection example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection) with model replaced with Mobilenet_V1_0.25_224_quant at the [Tensorflow lite pre-trained models site](https://www.tensorflow.org/lite/guide/hosted_models) with increased enough kTensorArenaSize and model input/output size changed to 224x224x3 and 1x1001, amd pulled additional required operators.

Also tried a few different models, at another quantified mode Mobilenet_V1_0.25_192_quant is showing the same segfault problem, But the regular floating point modes Mobilenet_V1_0.25_192, and Mobilenet_V1_0.25_224 run OK with many loops.

Have anyone seen similar problem ? Or is some limitations on Tensorflow Lite Micro that I should be aware of ?

**Please provide the exact sequence of commands/steps when you ran into the problem**

This problem can be reproduced at [this commit ](https://github.com/Jeff-Zhu/tensorflow/tree/bf2fef29281108c553c16e14dd4632b9a629de3c) of forked tensorflow repo.

Commands to reproduce this problem:
Build command:
```
$ git clone https://github.com/Jeff-Zhu/tensorflow
$ cd tensorflow
$ git checkout bf2fef29281108c553c16e14dd4632b9a629de3c
$ bazel build //tensorflow/lite/micro/examples/person_detection:person_detection       -c dbg --copt=-fstack-protector-all --copt=-Wstack-protector --copt=-fno-omit-frame-pointer
$ ./bazel-bin/tensorflow/lite/micro/examples/person_detection/person_detection
*** stack smashing detected ***: <unknown> terminated
Aborted (core dumped)
```
coredump stack shows corrupted:
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007fd4e9c5b801 in __GI_abort () at abort.c:79
#2  0x00007fd4e9ca4897 in __libc_message (action=action@entry=do_abort, 
    fmt=fmt@entry=0x7fd4e9dd1988 ""*** %s ***: %s terminated\n"") at ../sysdeps/posix/libc_fatal.c:181
#3  0x00007fd4e9d4fcd1 in __GI___fortify_fail_abort (need_backtrace=need_backtrace@entry=false, 
    msg=msg@entry=0x7fd4e9dd1966 ""stack smashing detected"") at fortify_fail.c:33
#4  0x00007fd4e9d4fc92 in __stack_chk_fail () at stack_chk_fail.c:29
#5  0x0000559e534cab55 in tflite::ops::micro::conv::Eval (context=0x559e536c9280 <setup::static_interpreter+32>, 
    node=0x559e536c5b18 <(anonymous namespace)::tensor_arena+1419832>) at tensorflow/lite/micro/kernels/conv.cc:269
#6  0x42f6730442f67304 in ?? ()
#7  0x42f6730442f67304 in ?? ()
#8  0x42f6730442f67304 in ?? ()
#9  0x42f6730442f67304 in ?? ()
#10 0x42f6730442f67304 in ?? ()

***Files changed from the original person_detection example***
```
tensorflow/lite/micro/examples/person_detection/main_functions.cc
tensorflow/lite/micro/examples/person_detection/model_settings.h 
tensorflow/lite/micro/examples/person_detection/person_detect_model_data.cc
```

Changes in [main_functions.cc](https://github.com/Jeff-Zhu/tensorflow/blob/bf2fef29281108c553c16e14dd4632b9a629de3c/tensorflow/lite/micro/examples/person_detection/main_functions.cc):

```
constexpr int kTensorArenaSize = 1400 * 1024;
static tflite::MicroOpResolver<5> micro_op_resolver;
micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_RESHAPE,
                             tflite::ops::micro::Register_RESHAPE());
micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_SOFTMAX,
                             tflite::ops::micro::Register_SOFTMAX(), 1, 2);
```

Changes in [model_settings.h](https://github.com/Jeff-Zhu/tensorflow/blob/bf2fef29281108c553c16e14dd4632b9a629de3c/tensorflow/lite/micro/examples/person_detection/model_settings.h)

```
constexpr int kNumCols = 224;
constexpr int kNumRows = 224;
constexpr int kNumChannels = 3;
constexpr int kCategoryCount = 1001;
```

Changes in [person_detect_model_data.cc](https://github.com/Jeff-Zhu/tensorflow/blob/bf2fef29281108c553c16e14dd4632b9a629de3c/tensorflow/lite/micro/examples/person_detection/person_detect_model_data.cc):

This file is modified with data from Mobilenet_V1_0.25_224_quant.tflite from [this pre-trained quantized mobilenet v1 model](https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_224_quant.tgz) on [Hosted models site](https://www.tensorflow.org/lite/guide/hosted_models).
```
$ xxd -i -c 12 mobilenet_v1_0.25_224_quantized.tflite  > model_data.cc
```
then replacing model data in the new model_data.cc in the tensorflow/lite/micro/examples/person_detection/person_detect_model_data.cc, update the array name and size.

Thanks for your help.

"
37909,Direct feeding from GPU memory for Java,"**System information**
- TensorFlow version: 1.15.2
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Such feature is currently available in C++ API. It is mentioned as experimental but seems to be working. See [#5902](https://github.com/tensorflow/tensorflow/issues/5902) and particularly [direct session test](https://github.com/tensorflow/tensorflow/issues/5902#issuecomment-410792030).

This feature is for Java. It adds 2 things: 
* allocate tensor in GPU (GPUBFCAllocator)
* directly feed tensors allocated in GPU (makeCallable/runCallable with CallableOptions)

The use case is when one does pre-processing of the data (like image rectification and aberration correction) in GPU and then would like to feed the network without copying data back and forth. This might save quite some time for high resolution imagery.

**Will this change the current api? How?**
Add methods to Java API and new JNI calls.

**Who will benefit with this feature?**
One who uses Java API. This will increase performance of Java applications that employ data preprocessing before feeding graph.

**Any Other info.**
* My initial approach: https://github.com/okdzhimiev/tensorflow/tree/r1.15
* Small Java Maven project for testing: [tfhello](https://git.elphel.com/oleg/tfhello)

Intended use:
* Allocate Tensor in GPU memory
* Get the pointer to that memory
* Run a custom CUDA kernel, writing results to the Tensor memory (JCuda)
* Run inference

Currently doing testing.
As I don't casually do C++, please, have a look and let me know if I'm at least on the right track. Any advice will be greatly appreciated.
Thanks.
"
37906,No module named '_pywrap_tensorflow_internal',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

_**System information**_
**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Windows 10
**- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:** No
**- TensorFlow installed from (source or binary):** python -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl
(doing a regular pip installation does not work for me)
- **TensorFlow version:** 1.12.0
- **Python** version: 3.6.0
- **Installed using virtualenv? pip? conda?**: pip
- **Bazel version (if compiling from source):** ?? I dont think I have this
- **GCC/Compiler version (if compiling from source):** ?? I dont think I have this
- **CUDA/cuDNN version: ??** I dont think I have this
- **GPU model and memory:** None. I was told I could do this without a gpu. 

**Describe the problem**

I want to run DarkFlow. Specifically I want to run this: 
----------------------------------------------
from darkflow.net.build import TFNet

options = {""model"": ""cfg/yolo.cfg"", 
           ""load"": ""bin/yolo.weights"", 
           ""threshold"": 0.1, 
           ""gpu"": 0.0}

tfnet = TFNet(options)
----------------------------------------------

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. I installed tensorflow in the aforementioned manner

2. I Tried to run the aforementioned code:

C:\Users\gcovillo\darkflow-master>python darkflowEx.py
Traceback (most recent call last):
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""darkflowEx.py"", line 2, in <module>
    from darkflow.net.build import TFNet
  File ""C:\Users\gcovillo\darkflow-master\darkflow\net\build.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\gcovillo\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errorshefor some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Any Help would be appreciated. I know there is already a closed case with this but I really have no clue what that guy was saying. "
37904,"**debug feature** for debugging numeric instability, like nan or infinity","**System information**
- TensorFlow version (you are using): nightly
- Are you willing to contribute it (Yes/No): cant do it alone, but willing to help



**Describe the feature and the current behaviour/state.**
I propose a **debug feature** for debugging numeric instability. The functionality should be similar to `tf.debugging.enable_check_numerics()` but more granular, but still more high level then `tf.debugging.check_numerics`.

Implementation Idea:
A `tf.debugging.check_func_numerics` annotation is added. It can be applied to ops warped in a python eager function and to tf.functons.
It will add numerics checks to all ops used in this function and, this is IMPORTANT, to all corresponding gradient ops.

The part of a model which courses numeric instability can be identified with this.

**Will this change the current api? How?**
it will add another debugging function/annotation, so the standard api is not changed.

**Who will benefit with this feature?**
Everyone with problems with nan and infinity in there models.
Especially for big models where `tf.debugging.check_func_numerics` can not be applied to the hole model."
37902,Cannot get TensorFlow to work on Mac,"Hello everybody. I have done some programming in the past in C++, HTML, CSS and PHP but I wanted to get into AI. I had done a bit of python in the past (not very much though) and I thought that TensorFlow would be a good place to start. I am on a Mac (something I plan to change in the summer) and I was trying to follow this tutorial to get set up: https://www.youtube.com/watch?v=gWfVwnOyG78

To sum up, this is what I did:
1. I installed Anaconda-Navigator.
2. I made a new environment and added TensorFlow and Keras packages (I might need Keras in the future but it's not relevant for now).
3. I opened PyCharm (I already had it on my computer).

At this point I ran into issues. In the video linked above at 8:30 he makes a new project. However, my screen options are far different:

<img width=""400"" alt=""Screen Shot 2020-03-25 at 7 05 21 PM"" src=""https://user-images.githubusercontent.com/47663532/77558085-98407900-6ecb-11ea-823b-c018b2b8ced8.png"">
 
So, I setup the project as shown in the video above. The Project Interpreter is set up as so:
<img width=""572"" alt=""Screen Shot 2020-03-25 at 7 08 04 PM"" src=""https://user-images.githubusercontent.com/47663532/77558481-06853b80-6ecc-11ea-8921-950c36b98c7d.png"">

I added the TensorFlow packages:
<img width=""600"" alt=""Screen Shot 2020-03-25 at 7 09 17 PM"" src=""https://user-images.githubusercontent.com/47663532/77558589-287ebe00-6ecc-11ea-8f39-2ea5367b4b87.png"">

It said that the packages were installed:
<img width=""374"" alt=""Screen Shot 2020-03-25 at 7 09 46 PM"" src=""https://user-images.githubusercontent.com/47663532/77558679-44825f80-6ecc-11ea-95c2-c5195c36f2b7.png"">

Also, when I import TensorFlow it looks like it is recognized:
<img width=""425"" alt=""Screen Shot 2020-03-25 at 7 12 37 PM"" src=""https://user-images.githubusercontent.com/47663532/77558937-9dea8e80-6ecc-11ea-9e0c-8fd2cf8e2ea6.png"">

However, when I run the code I get this error:

<img width=""616"" alt=""Screen Shot 2020-03-25 at 7 13 39 PM"" src=""https://user-images.githubusercontent.com/47663532/77559067-c2466b00-6ecc-11ea-8b17-f46fe498dbb2.png"">


I can't seem to understand what I did wrong. I have spent hours at this point and I am so desperate that I would probably pay someone to help me set it up. I would appreciate any help more that you know. Thanks."
37900,Copying tensors to GPU fails non-deterministically,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-174-generic x86_64)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: No
- TensorFlow installed from (source or
binary): `pip install tensorflow==2.2.0rc1`
- TensorFlow version (use command below): v2.2.0-rc0-43-gacf4951a2f 2.2.0-rc1
- Python version: Python 3.7.3
- CUDA/cuDNN version: 10.1, V10.1.243
- GPU model and memory: 2x GeForce GTX 1080 8GB 

**Describe the current behavior**
I have a system with two GPUs. Since I need the fix of https://github.com/tensorflow/tensorflow/issues/33929 I have upgraded from 2.1 to 2.2.0rc0 and 2.2.0rc1. In both cases, I sometimes get the following exception when trying to train on a GPU:
```
Traceback (most recent call last):
  File ""/data/personal/username/deployed/project/project/bin/train.py"", line 86, in <module>
    main()
  File ""/data/personal/username/deployed/project/project/bin/train.py"", line 82, in main
    train()  # pragma: no cover
  File ""/data/personal/username/deployed/project/project/bin/train.py"", line 75, in train
    callbacks=callbacks,
  File ""/data/personal/username/deployed/project/project/objects/models/KerasModel.py"", line 190, in train_on_generator
    callbacks=callbacks,
  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 65, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 783, in fit
    tmp_logs = train_function(iterator)
  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 644, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1665, in _filtered_call
    self.captured_inputs)
  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1746, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 598, in call
    ctx=ctx)
  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown:  InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run AddV2: Attempted to set tensor for existing mirror. [Op:AddV2]
Traceback (most recent call last):

  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 241, in __call__
    return func(device, token, args)

  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 130, in __call__
    ret = self._func(*args)

  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 309, in wrapper
    return func(*args, **kwargs)

  File ""/data/personal/username/deployed/project/project/objects/generators/GeneratorRetinaNet.py"", line 85, in _getitem_pre_anchors
    image, node = self.get_prepped_node(idx=idx)

  File ""/data/personal/username/deployed/project/project/objects/generators/Generator.py"", line 252, in get_prepped_node
    node = self.get_node(idx=idx)

  File ""/data/personal/username/deployed/project/project/objects/generators/GeneratorObjectDetection.py"", line 97, in get_node
    node = super(GeneratorObjectDetection, self).get_node(idx=idx)

  File ""/data/personal/username/deployed/project/project/objects/generators/Generator.py"", line 376, in get_node
    return self._dataFetcher.get_node(idx)

  File ""/data/personal/username/deployed/project/project/objects/generators/DataFetchers/DataFetcher.py"", line 117, in get_node
    nodes = self.get_nodes(node_index, node_index + 1)

  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py"", line 997, in binary_op_wrapper
    return func(x, y, name=name)

  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py"", line 1276, in _add_dispatch
    return gen_math_ops.add_v2(x, y, name=name)

  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 480, in add_v2
    _ops.raise_from_not_ok_status(e, name)

  File ""/home/username/.local/share/virtualenvs/project-1BQtdZDZ/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6653, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)

  File ""<string>"", line 3, in raise_from

tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run AddV2: Attempted to set tensor for existing mirror. [Op:AddV2]
```

The odd thing about this is, that is is not deterministic behavior. At one moment it may not work. When I kill the process, wait a minute and try again, it sometimes works. 
In general, retrying one or two times will fix the issue, but currently I'm only successful to train about 10% of the time. Is this a known issue?

Note that in (TF1.13,) TF2.0 and TF2.1, this seemed to work fine for me.

Before training, I always use `os.environ['CUDA_VISIBLE_DEVICES'] = '0` or `'1'`, depending on the GPU I want to use. I have tried replacing this with `tf.config.set_visible_devices` to no avail.

For completeness, I do not use TPUs or distributed strategies."
37899,tflite | Build using Intel C++ compiler (Windows),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.2.0-rc1

Is it possible to build the TensorFlow Lite DLL for Windows 10 using the Intel C++ compiler? Would I need to create a new crosstool? Most info I found online is about Linux or building the pip wheel.

Linux: https://groups.google.com/forum/#!searchin/bazel-discuss/Use$20or$20support$20Intel$20compiler$20in$20Bazel/bazel-discuss/t-ruQRMis8A/gzF9PWY_AQAJ
Pip wheel: https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide

It would be great if someone could point me in the right direction. Thanks!"
37898,TensorFlow Lite vs TensorFlow-TRT vs TesnorRT,"Hello


I am doing a benchmark between the Jetson Family (TensorRT, TensorFlow-TRT) and Coral (TensorFlow Lite).

1.	Can TensorFlow Lite work with any type of GPU? Or ONLY with GPUs of mobile and embedded devices?
For example, the NVIDIA Tesla T4 has a precision of INT8 like the Coral TPU.





















2.	How does TensorFlow Lite optimize? Does it work like Tensorflow-TRT? 
https://medium.com/tensorflow/high-performance-inference-with-tensorrt-integration-c4d78795fbfe
•	Elimination of layers whose outputs are not used
•	Elimination of operations which are equivalent to no-op
•	The fusion of convolution, bias and ReLU operations
•	Aggregation of operations with sufficiently similar parameters and the same source tensor (for example, the 1x1 convolutions in GoogleNet v5’s inception module)
•	Merging of concatenation layers by directing layer outputs to the correct eventual destination.


 



Thank you
"
37897,ERROR: An error occurred during the fetch of repository 'com_google_protobuf' ,"**The following is my configuration:**

**System information** : Windows 10 Professional
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  No
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10 Professional
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: N/A
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below):  pip install tensorflow
- Python version:  N/A
- Bazel version (if compiling from source): 0.27.1
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version:  V10.0/v7.6.1
- GPU model and memory: GeForce 1050 Ti

While running the code cloneed from [URL](https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/gans/experiments/emnist) with 
`bazel run run_experiments`
, I encountered the following issue:

![image](https://user-images.githubusercontent.com/14258909/77538982-91b9fd80-6edb-11ea-8345-1ef2b5f91f1d.png)

The masked information is my user name.
Could yout please give me some adcvices?




"
37896,tf.keras.losses.cosine_similarity() is not a negative quantity,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/losses/cosine_similarity

## Description of issue (what needs changing):
Documentation states that tf.keras.losses.cosine_similarity() ""is a negative quantity between -1 and 0, where 0 indicates orthogonality and values closer to -1 indicate greater similarity.""
But it is actually not true. tf.keras.losses.cosine_similarity() can return positive values.

### Usage example
```
>>> import tensorflow as tf
>>> tf.keras.losses.cosine_similarity([[1., 1.]], [[-1., -1.]])
<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.99999994], dtype=float32)>
```
"
37895,run_eagerly argument bug in tf.Keras.Model compile method,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10, x64
- TensorFlow installed from (source or
binary): Binary (pip)
- TensorFlow version (use command below): TF 2.2.0 (2.2.0.dev20200324)
- Python version: 3.7
- CUDA/cuDNN version: 10.1 / 7.6
- GPU model and memory: Bug appears on several computers with different GPU

**Describe the current behavior**

The run_eagerly argument of the compile method of tf.Keras models does not actually set the run_eagerly attribute of the model. This causes the model to not be run eagerly even if run_eagerly=True was given as argument to its compile method.

This seems to be caused by a recent modification of the arguments of the compile method, as detailed bellow.

**Describe the expected behavior**

Calling compile on a Keras model with the argument run_eagerly=True should set the run_eagerly attribute of the model to True, which causes the model to be run eagerly when calling methods such as fit on it.

**Standalone code to reproduce the issue** 

```
import numpy as np
import tensorflow as tf

class CustomLayer(tf.keras.layers.Layer):
	def __init__(self):
		super().__init__()
		
	def call(self, inputs):
		tf.print('Running eagerly: ', tf.executing_eagerly())
		return inputs

if __name__ == ""__main__"" :
	data = np.random.random((16, 3)).astype(np.float32)

	inputs = tf.keras.Input(shape=(3,))
	outputs = tf.keras.layers.Dense(3)(inputs)
	outputs = CustomLayer()(outputs)
	model = tf.keras.Model(inputs=inputs, outputs=outputs)
	
	model.compile(loss='mse', run_eagerly=True) # does not set model.run_eagerly to True, and model does not run eagerly
	# model.run_eagerly = True # sets model.run_eagerly to True, and model runs eagerly

	print('run_eagerly: ', model.run_eagerly)

	model.fit(x=data, y=data)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Logs corresponding to the standalone code: [log.txt](https://github.com/tensorflow/tensorflow/files/4380372/log.txt)

**Supplementary info regarding the issue**

compile's signature is now
```
def compile(self,
              optimizer='rmsprop',
              loss=None,
              metrics=None,
              loss_weights=None,
              sample_weight_mode=None,
              weighted_metrics=None,
              run_eagerly=None,
              **kwargs):
```
when it used to be (notice that run_eagerly=None has been added to its arguments)
```
def compile(self,
              optimizer='rmsprop',
              loss=None,
              metrics=None,
              loss_weights=None,
              sample_weight_mode=None,
              weighted_metrics=None,
              **kwargs):
```

However, it still sets _run_eagerly as follows `self._run_eagerly = kwargs.pop('run_eagerly', None)` which can no longer work as run_eagerly is no longer in kwargs.

"
37894,How to save/load the partial model for fine-tuning/transfer-learning in TF2.1?,"I would like to build my own base model and train it with big dataset. After training, I save the base model. I have another customized model and I want to load the weights of first two layers from the base model. How should I achieve it in Tensorflow 2.1.0, thanks.

Sample codes:

    import os
    os.environ[""CUDA_VISIBLE_DEVICES""]="""" 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers

    class BaseModel():
        def __init__(self):
            inputs = keras.Input(shape=(32, 32, 3))
            x = inputs
            x = layers.Conv2D(32, 3, padding='same', activation=tf.nn.relu)(x)
            x = layers.MaxPool2D()(x)
            x = layers.Conv2D(64, 3, padding='same', activation=tf.nn.relu)(x)

            x = layers.Flatten()(x)

            x = layers.Dense(500, activation=tf.nn.relu)(x)

            outputs = layers.Dense(1000, activation=tf.nn.softmax)(x)

            self.model = keras.Model(inputs=inputs, outputs=outputs)

        def __call__(self, inputs):
            return self.model(inputs)

    bm = BaseModel()  # the model for pretraining
    bm.model.save_weights('base_model') # save the pretrained model


    class MyModel():
        def __init__(self):
            inputs = keras.Input(shape=(32, 32, 3))
            x = inputs
            x = layers.Conv2D(32, 3, padding='same', activation=tf.nn.relu)(x)
            x = layers.MaxPool2D()(x)
            x = layers.Conv2D(64, 3, padding='same', activation=tf.nn.relu)(x)
            
            x = layers.Conv2D(128, 3, padding='same', activation=tf.nn.relu)(x)

            x = layers.Flatten()(x)

            x = layers.Dense(1000, activation=tf.nn.relu)(x)

            outputs = layers.Dense(10, activation=tf.nn.softmax)(x)

            self.model = keras.Model(inputs=inputs, outputs=outputs)

        def __call__(self, inputs):
            return self.model(inputs)


    mm = MyModel()  # the model for my customized applications
    mm.model.load_weights('base_model')  # load the pretrained model with the first two conv layers

    # further fine-tuning or transfer learning

"
37892,Converting unsupported operation: PyFunc,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
pycharm
- TensorFlow version (or github SHA if from source):
tensorflow 1.15.0

**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.
This is my code.
import tensorflow as tf

path = 'vgg_freeze_model.pb'
inputs = [""Placeholder""]
input_shapes = {""Placeholder"": [1, 224, 224, 3]}
outputs = [""vgg_16/cls_prob""]

converter = tf.lite.TFLiteConverter.from_frozen_graph(path, inputs, outputs, input_shapes=input_shapes)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
converter.post_training_quantize = True
tflite_model = converter.convert()
open(""vgg16.tflite"", ""wb"").write(tflite_model)

```
# Copy and paste here the exact command
```

**The output from the converter invocation**

```
# Copy and paste the output here.
```
This is my issue
2020-03-25 03:36:10.513606: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: PyFunc
2020-03-25 03:36:10.513648: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
37891,help me with tensorflow 2.0 installation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 2.0
- Python version:3.7
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory: nvidia geforce gtx 1050ti



i tried a lots of tutorials to install tensorflow gpu but failed facing different issues mainly while running the object_detection_tutorial.ipynb is not ruuning..
![Screenshot (22)](https://user-images.githubusercontent.com/62600923/77514129-446a6b80-6e9c-11ea-9bed-f807ac43556d.png)
 
i already installed the tensorflow gpu but still it is again installing in jupyter notebook..
iam fully confused what to do...
please anyone help me out.
"
37889,Bazel Build Error : crosstool_wrapper_driver_is_not_gcc failed(Install Tensorflow1.14 using Bazel),"I NEED YOUR HEEEEEEEEEEEEEEEEEEEEEEEEEEEELP!!!

According to Nvidia TensorRT page
(https://docs.nvidia.com/deeplearning/sdk/tensorrt-release-notes/tensorrt-6.html), 
![Screenshot from 2020-03-25 10-30-32](https://user-images.githubusercontent.com/60951742/77500185-a8d6fc00-6e97-11ea-92a3-ec03f9614c27.png)
so install like this...

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **18.04(llinux-x86-64)**
- TensorFlow installed from : **git clone https://github.com/tensorflow/tensorflow.git**
- TensorFlow version: **1.14.0**
- Python version: **3.6**
- Bazel version (if compiling from source):**0.26.1**
- GCC/Compiler version (if compiling from source):**7.5.0**
- CUDA/cuDNN version: **CUDA : 10.0, cuDNN : 7.6.5**
- GPU model and memory: **GeForce GTX 1070**
- TensorRT : **6.0.1.5**


**Describe the problem**
After `./configure`,
run 
`bazel build --config=cuda --config=opt //tensorflow/tools/pip_package:build_pip_package  --verbose_failures`.
or 
`bazel build --config=opt --config=cuda --config=nonccl //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --verbose_failures`

And encountered problem like this log...

```
ERROR: /home/gogotis/tensorflow/tensorflow/compiler/tf2tensorrt/BUILD:330:1: C++ compilation of rule '//tensorflow/compiler/tf2tensorrt:trt_conversion' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/gogotis/.cache/bazel/_bazel_gogotis/a070fbb10dc532167de59febc30d733d/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/home/gogotis/TensorRT-6.0.1.5/lib \
    PATH=/usr/local/cuda-10.0/bin:/home/gogotis/.local/bin:/home/gogotis/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/gogotis/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/compiler/tf2tensorrt/_objs/trt_conversion/convert_nodes.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/compiler/tf2tensorrt/_objs/trt_conversion/convert_nodes.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -msse3 -pthread '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -c tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc -o bazel-out/host/bin/tensorflow/compiler/tf2tensorrt/_objs/trt_conversion/convert_nodes.pic.o)
```
I read same problem issue...But it can't solve my problem...


"
37888,windows build with r2.2 failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 x64 1909
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0rc2
- Python version: 3.8.1
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): Visual Studio 2019
- CUDA/cuDNN version: 10.2/7.6.5
- GPU model and memory:
RTX2080Ti GDDR6 11GB


**Describe the problem**
build failed
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
INFO: Call stack for the definition of repository 'local_config_python' which is a python_configure (rule definition at D:/repo/tensorflow/third_party/py/python_configure.bzl:280:20):
 - D:/repo/tensorflow/tensorflow/workspace.bzl:97:5
 - D:/repo/tensorflow/WORKSPACE:19:1
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 263
                _create_local_python_repository(<1 more arguments>)
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 209, in _create_local_python_repository
                _check_python_bin(<2 more arguments>)
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 143, in _check_python_bin
                raw_exec(repository_ctx, <1 more arguments>)
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 143, in raw_exec
                get_bash_bin(repository_ctx)
        File ""D:/repo/tensorflow/third_party/remote_config/common.bzl"", line 66, in get_bash_bin
                which(repository_ctx, ""bash"")
        File ""D:/repo/tensorflow/third_party/remote_config/common.bzl"", line 27, in which
                execute(repository_ctx, <1 more arguments>)
        File ""D:/repo/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
INFO: Could not find files for the given pattern(s).
ERROR: While resolving toolchains for target //third_party/eigen3:eigen3: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 263
                _create_local_python_repository(<1 more arguments>)
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 209, in _create_local_python_repository
                _check_python_bin(<2 more arguments>)
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 143, in _check_python_bin
                raw_exec(repository_ctx, <1 more arguments>)
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 143, in raw_exec
                get_bash_bin(repository_ctx)
        File ""D:/repo/tensorflow/third_party/remote_config/common.bzl"", line 66, in get_bash_bin
                which(repository_ctx, ""bash"")
        File ""D:/repo/tensorflow/third_party/remote_config/common.bzl"", line 27, in which
                execute(repository_ctx, <1 more arguments>)
        File ""D:/repo/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
INFO: Could not find files for the given pattern(s).
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 263
                _create_local_python_repository(<1 more arguments>)
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 209, in _create_local_python_repository
                _check_python_bin(<2 more arguments>)
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 143, in _check_python_bin
                raw_exec(repository_ctx, <1 more arguments>)
        File ""D:/repo/tensorflow/third_party/py/python_configure.bzl"", line 143, in raw_exec
                get_bash_bin(repository_ctx)
        File ""D:/repo/tensorflow/third_party/remote_config/common.bzl"", line 66, in get_bash_bin
                which(repository_ctx, ""bash"")
        File ""D:/repo/tensorflow/third_party/remote_config/common.bzl"", line 27, in which
                execute(repository_ctx, <1 more arguments>)
        File ""D:/repo/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
INFO: Could not find files for the given pattern(s).
INFO: Elapsed time: 6.220s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (101 packages loaded, 322 targets configured)
```"
37887,Wrong loss calculation with multi-output models when using model.fit.,"**System information** 
- Have I written custom code: Yes, though the code sample below is adapted from an official tensorflow tutorial with minimal changes.
- OS Platform and Distribution: Ubuntu 18.04 
- TensorFlow installed from: official pip package, version 2.1.0
- Python version: 3.6.9
- CUDA/cuDNN version: CUDA 10.1, GTX 1070


**Describe the current behavior**
Consider the following mnist example
```python
import numpy as np
import tensorflow as tf

inputs = tf.keras.Input(shape=(784,), name='digits')
x = tf.keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)
x = tf.keras.layers.Dense(64, activation='relu', name='dense_2')(x)
outputs = tf.keras.layers.Dense(10, name='predictions', activation='softmax')(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)

model.compile(
    optimizer=tf.keras.optimizers.SGD(),
    loss={""predictions"": tf.keras.losses.SparseCategoricalCrossentropy()},
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=""accuracy"")],
)

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype('float32') / 255
x_test = x_test.reshape(10000, 784).astype('float32') / 255
y_train = y_train.astype('float32')
y_test = y_test.astype('float32')

x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).repeat().batch(64)

val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.batch(64, drop_remainder=True)

test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_dataset = test_dataset.batch(64)

model.fit(train_dataset, epochs=3, steps_per_epoch=1000, validation_data=val_dataset)
```
The output is 
```bash
Epoch 1/3
1000/1000 [==============================] - 4s 4ms/step - loss: 0.9192 - accuracy: 0.7615 - val_loss: 0.4060 - val_accuracy: 0.8914
Epoch 2/3
1000/1000 [==============================] - 3s 3ms/step - loss: 0.3786 - accuracy: 0.8939 - val_loss: 0.3147 - val_accuracy: 0.9127
Epoch 3/3
1000/1000 [==============================] - 3s 3ms/step - loss: 0.3211 - accuracy: 0.9080 - val_loss: 0.2796 - val_accuracy: 0.9196
```
This works just fine as the loss decreases and the accuracy increases.

However, if I simply add another dummy output to the model, the training will become erroneous. Let's just change the model definition line to `model = tf.keras.Model(inputs=inputs, outputs=[x, outputs])`

Now the output is:
```bash
WARNING:tensorflow:Output dense_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_2.
Epoch 1/3
WARNING:tensorflow:Gradients do not exist for variables ['predictions/kernel:0', 'predictions/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['predictions/kernel:0', 'predictions/bias:0'] when minimizing the loss.
1000/1000 [==============================] - 4s 4ms/step - loss: 5.7781 - predictions_loss: 5.7781 - predictions_accuracy: 0.1048 - val_loss: 5.5196 - val_predictions_loss: 5.5196 - val_predictions_accuracy: 0.1243
Epoch 2/3
1000/1000 [==============================] - 4s 4ms/step - loss: 5.3866 - predictions_loss: 5.3866 - predictions_accuracy: 0.1202 - val_loss: 5.2255 - val_predictions_loss: 5.2255 - val_predictions_accuracy: 0.0967
Epoch 3/3
1000/1000 [==============================] - 4s 4ms/step - loss: 5.1167 - predictions_loss: 5.1167 - predictions_accuracy: 0.0996 - val_loss: 5.1479 - val_predictions_loss: 5.1479 - val_predictions_accuracy: 0.0992
```
Basically the network is not trained at all because as the warning says: *Gradients do not exist for variables ['predictions/kernel:0', 'predictions/bias:0'] when minimizing the loss.*



**Describe the expected behavior**
The expect behavior is that by adding additional outputs to the model should not affect the training process in anyway. Note that in `model.compile`, I specified the loss function using a dictionary, so I don't need to modify the rest of the code after adding a dummy output.


"
37886,"After training the model using keras, tensorflow1.15 performs Post-training integer quantization, but the MCU prediction results are inconsistent with tflite output","@tensorflow/micro
**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):  conda install tensorflow=1.15
- Tensorflow version (commit SHA if source): 1.15.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm-cortex M4

**Describe the problem**
My English level is not very good, thank you very much for your help.
I want to complete a voice wake-up MCU, after training the model using keras, tensorflow1.15 performs Post-training integer quantization, but the MCU prediction results are inconsistent with tflite output.
When I run tflite tests using tensorflow, the output is correct, but the MCU output is wrong.
Same model input, tflite goes through the first layer depth_wise output：
![41585108178_ pic](https://user-images.githubusercontent.com/27952292/77500064-e3886680-6e8e-11ea-8b75-72218b61b898.jpg)
Same model input, mcu goes through the first layer depth_wise output：
![51585108202_ pic](https://user-images.githubusercontent.com/27952292/77500142-221e2100-6e8f-11ea-8c65-e9685d505ec4.jpg)


**Please provide the exact sequence of commands/steps when you ran into the problem**
1. Convert to a TensorFlow Lite model
```
def conver_tflite_int8():
    converter = tf.lite.TFLiteConverter.from_keras_model_file(""./dscnn_dense_model_depthwise/checkpoint-03e-loss_0.029.hdf5"")
    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.int8
    converter.inference_output_type = tf.int8
    train_data = read_data()
    def representative_dataset_gen():
        for input_value in train_data:
            yield [input_value]
    converter.representative_dataset = tf.lite.RepresentativeDataset(representative_dataset_gen)
    tflite_model_quant = converter.convert()
    with open(""./converted_model_float_dense_depthwise.tflite"", ""wb"") as wb:
        wb.write(tflite_model_quant)
```
2. Load tflite prediction
```
def load_int_perlayer():
    txts = glob.glob(""./first_test*.txt"")

    interpreter_quant = tf.lite.Interpreter(model_path=str(""./converted_model_int8_dense_depthwise.tflite""))
    interpreter_quant.allocate_tensors()
    tensor_list = interpreter_quant.get_tensor_details()
    for i in tensor_list:
        print(i)

    datas = []
    for i in txts:
        data = []
        with open(i, 'r') as tr:
            lines = tr.readlines()
            for j in lines:
                data.append([float(z) for z in j[:-1].split(' ')])
        datas.append(data)
    datas = np.array(datas)
    data_min = -248.
    data_max = 38.

    for i in tqdm(range(len(datas))):
        with open('./layer_out/mfcc_float.csv', 'w') as wc:
            for j in datas[i].reshape((74, 10)):
                wc.write(str(j) + '\n')
        data = (((datas[i].reshape((1, 74, 10, 1)) + 105) / 143) * 127.).astype(np.int8)
        with open('./layer_out/mfcc_int.csv', 'w') as wc:
            for j in data.reshape((74, 10)):
                wc.write(str(j) + '\n')

        input_index = interpreter_quant.get_input_details()[0][""index""]
        output_index = interpreter_quant.get_output_details()[0][""index""]
        interpreter_quant.set_tensor(input_index, data.reshape((1, 74, 10, 1)))
        interpreter_quant.invoke()
        predictions = interpreter_quant.get_tensor(output_index)
        print(predictions[0])
```
3. Convert to a C array
```
xxd -i ./converted_model_int8_dense_depthwise.tflite > model.cc
```
model.cc screenshot
![61585108480_ pic](https://user-images.githubusercontent.com/27952292/77500247-77f2c900-6e8f-11ea-8e53-185d971527b3.jpg)
"
37885,Error when deserialising BatchNormalization layers from yaml,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes (see example below)
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10, Anaconda3

- TensorFlow installed from: binary (pip)
- TensorFlow version (use command below): tensorflow-gpu 2.1.0 (from pip)
- Python version: 3.7.7
- CUDA/cuDNN version: 10.2
- GPU model and memory: GTX 1080

**Describe the current behavior**
When deserialising from yaml a model that contains a BatchNormalization layer, I get the exception `yaml.constructor.ConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:tensorflow.python.training.tracking.data_structures.ListWrapper'`. Indeed, inspecting the yaml shows that this tag is in the BN layer, as follows:
```
[... other yaml]
  - class_name: BatchNormalization
    config:
      axis: !!python/object/apply:tensorflow.python.training.tracking.data_structures.ListWrapper
      - - 3
[... other yaml]
``` 

I have included a script below that seems able to reproduce this. Oddly, I then tried the same script on a Ubuntu system with `tensorflow==2.1.0` and that appeared ok, so this might only apply to tensorflow-gpu.

**Describe the expected behavior**
The script below serialises and deserialises to/from yaml without errors.

**Standalone code to reproduce the issue** 
```
import tensorflow as tf

input_layer = tf.keras.layers.Input(shape=(32,32,3))
output = tf.keras.layers.BatchNormalization()(input_layer)
model = tf.keras.Model(inputs=input_layer, outputs=output)

yaml_out = model.to_yaml()
model2 = tf.keras.models.model_from_yaml(yaml_out)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Full traceback:
```
Traceback (most recent call last):
  File "".\bin\reproduce-bug.py"", line 13, in <module>
    model2 = tf.keras.models.model_from_yaml(yaml_out)
  File ""C:\Users\nitbi_000\anaconda3\envs\tf2-gpu\lib\site-packages\tensorflow_core\python\keras\saving\model_config.py"", line 76, in model_from_yaml
    config = yaml.load(yaml_string)
  File ""C:\Users\nitbi_000\anaconda3\envs\tf2-gpu\lib\site-packages\yaml\__init__.py"", line 114, in load
    return loader.get_single_data()
  File ""C:\Users\nitbi_000\anaconda3\envs\tf2-gpu\lib\site-packages\yaml\constructor.py"", line 51, in get_single_data
    return self.construct_document(node)
  File ""C:\Users\nitbi_000\anaconda3\envs\tf2-gpu\lib\site-packages\yaml\constructor.py"", line 60, in construct_document
    for dummy in generator:
  File ""C:\Users\nitbi_000\anaconda3\envs\tf2-gpu\lib\site-packages\yaml\constructor.py"", line 413, in construct_yaml_map
    value = self.construct_mapping(node)
  File ""C:\Users\nitbi_000\anaconda3\envs\tf2-gpu\lib\site-packages\yaml\constructor.py"", line 218, in construct_mapping
    return super().construct_mapping(node, deep=deep)
  File ""C:\Users\nitbi_000\anaconda3\envs\tf2-gpu\lib\site-packages\yaml\constructor.py"", line 143, in construct_mapping
    value = self.construct_object(value_node, deep=deep)
  File ""C:\Users\nitbi_000\anaconda3\envs\tf2-gpu\lib\site-packages\yaml\constructor.py"", line 100, in construct_object
    data = constructor(self, node)
  File ""C:\Users\nitbi_000\anaconda3\envs\tf2-gpu\lib\site-packages\yaml\constructor.py"", line 429, in construct_undefined
    node.start_mark)
yaml.constructor.ConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:tensorflow.python.training.tracking.data_structures.ListWrapper'
  in ""<unicode string>"", line 24, column 13:
          axis: !!python/object/apply:tensorflow ...
```"
37884,"Try to convert a custom SSD_MobileNet model to tflite, but get the error: Check failed: dim_size >= 1 (0 vs. 1)","System information:

- Windows 10

- Python 3.6.10

- Tensorflow 2.0.0

```
import tensorflow as tf
# pb path
path = ""C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/frozen_inference_graph-SteelRoll.pb""
# input and output
inputs = [""image_tensor""]
outputs = [""detection_boxes"", ""detection_classes"", ""detection_scores"", ""num_detections""]
# convert pb to tflite
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(path, inputs, outputs, input_shapes={""image_tensor"":[1,640,360,3]})
converter.post_training_quantize = True
tflite_model = converter.convert()
# save
open(""frozen_inference_graph-SteelRoll.tflite"", ""wb"").write(tflite_model)
```
However, when I run the above code, error messages will show up as the following:
```
2020-03-25 09:48:40.298641: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
C:\Users\LAWSSSS\.conda\envs\tfcpu\lib\site-packages\tensorflow_core\lite\python\lite.py:846: UserWarning: Property post_training_quantize is deprecated, please use optimizations=[Optimize.DEFAULT] instead.
  "" instead."" % name)
Traceback (most recent call last):
  File ""C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/test.py"", line 18, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\LAWSSSS\.conda\envs\tfcpu\lib\site-packages\tensorflow_core\lite\python\lite.py"", line 983, in convert
    **converter_kwargs)
  File ""C:\Users\LAWSSSS\.conda\envs\tfcpu\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""C:\Users\LAWSSSS\.conda\envs\tfcpu\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-03-25 09:48:42.916392: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.916652: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.916927: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-25 09:48:42.917171: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.917396: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.917617: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.917915: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.918138: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.918357: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.918575: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.918843: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.919088: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.919315: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-03-25 09:48:42.919565: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.919837: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.920075: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.920288: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-25 09:48:42.920582: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.920799: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.921020: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-25 09:48:42.921263: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.921479: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.921704: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-25 09:48:42.922236: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-25 09:48:42.922514: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-25 09:48:42.922751: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-25 09:48:42.923035: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-25 09:48:42.923274: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-25 09:48:42.923526: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-25 09:48:42.947959: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.948212: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.948444: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.948713: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.949022: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.949258: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.949485: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.949774: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.950029: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-25 09:48:42.950303: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-25 09:48:42.950636: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-25 09:48:42.950916: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-03-25 09:48:42.951170: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.951404: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.951638: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.951923: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.952150: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.952382: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.952614: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-03-25 09:48:42.952849: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.953080: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.953305: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.953526: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.953802: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.954023: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.954241: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.954527: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.954812: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-03-25 09:48:42.955065: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.955311: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.955544: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.955805: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-25 09:48:42.956042: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.956253: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.956469: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.956688: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-25 09:48:42.956921: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.957130: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.957348: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.957562: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-25 09:48:42.957856: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.958068: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.958285: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.958497: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-03-25 09:48:42.958953: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3
2020-03-25 09:48:42.959338: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3
2020-03-25 09:48:42.959836: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2020-03-25 09:48:42.960451: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2020-03-25 09:48:42.961178: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.961393: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.961612: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-25 09:48:42.961940: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.962246: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.962459: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-25 09:48:42.962731: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.962943: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.963160: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-25 09:48:42.963396: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-03-25 09:48:42.963608: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-03-25 09:48:42.963905: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-03-25 09:48:42.964158: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-25 09:48:42.964369: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-25 09:48:42.964578: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-25 09:48:42.964863: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-03-25 09:48:42.965125: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-25 09:48:42.965416: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-25 09:48:42.965733: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-25 09:48:42.965983: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-25 09:48:42.966222: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-25 09:48:42.966470: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-25 09:48:42.966816: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-03-25 09:48:42.967131: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-03-25 09:48:43.049924: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1791 operators, 3037 arrays (0 quantized)
2020-03-25 09:48:43.216798: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1770 operators, 2983 arrays (0 quantized)
2020-03-25 09:48:43.404253: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1770 operators, 2983 arrays (0 quantized)
2020-03-25 09:48:43.509305: F tensorflow/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)
Fatal Python error: Aborted
```
How do I solve `Check failed: dim_size >= 1 (0 vs. 1)`.?
Thanks in advance."
37883,tf.keras.layers.conv*d is slow compared to tf.nn.conv*d,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution : Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: N/A
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: - Bazel
version (if compiling from source): 3.8.0
- GCC/Compiler version (if compiling from
source): 9.3.0
- CUDA/cuDNN version: - GPU model and memory:
10.2/7.6.5

**Describe the current behavior**
I initially discovered this issue last year. I switched to pytorch as a solution for that project. However, it seems the issue still exists on the master branch of tensorflow as of today. I will try to remember as much detail as I can based on my investigation last year.

tf.keras.layers.conv\*d calls nn_ops.Convolution to perform the convolution. However, nn_ops.Convolution is inefficient for dilated convolution, since it uses _WithSpaceToBatch to handle dilation. space_to_batch allocates and copies the memory. (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/spacetobatch_functor.cc https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/spacetobatch_functor_gpu.cu.cc)  The performance issue is more severe for conv3d, since 3d volumes can be huge.  

After space_to_batch, nn_ops.Convolution then calls _NonAtrousConvolution, which calls gen_nn_ops.conv\*d for the actual convolution. Then batch_to_space is called to restore shape.

On the contrary, tf.nn.conv\*d directly calls gen_nn_ops.conv\*d for dilated convolution, which I guess calls the corresponding cudnn dilated convolution routines. It's efficient and fast.

**Describe the expected behavior**
nn_ops.Convolution should directly call gen_nn_ops.conv\*d for dilated convolution. It seems _WithSpaceToBatch is not necessary. cudnn supports dilated convolutions. Switching to gen_nn_ops.conv\*d can improve the performance with little efforts.

Maybe this change can also potentially fix #23101?

**Standalone code to reproduce the issue** 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Thank you so much for the help!"
37882,How to apply cosine similarity loss function in unsupervised training?,"In the Figure below is showed a simple deep learning architecture capable of learning embeddings for sentences. The training set is composed by sentence pairs `[[sentence_a],[sentence_b]]` that have the same semantics.

[![enter image description here][1]][1]

The objective is to fine-tune the embeddings of the sentences to be similar (since sentences in the pair have the same semantics). Consequently, a possible loss function would be `CosineSimilarity` loss.



`Encoder 1` and `Encoder 2` have the same definition:

```python
from tensorflow.keras import layers


class LSTMEncoder(layers.Layer):

    def __init__(self,
                 units,  # dimensionality of the output space
                 input_dim,  # vocab of size
                 output_dim,  # embedding dimension
                 name='encoder',
                 **kwargs):
        super(Encoder, self).__init__(name=name, **kwargs)
        self.embedding = layers.Embedding(input_dim=input_dim, output_dim=output_dim)
        self.lstm = layers.LSTM(units=units)

    def call(self, inputs):
        emb = self.embedding(inputs)
        return self.lstm(emb)
```

And the SEntence Representations model `(SERModel)` looks like:

```python
import tensorflow as tf

from source.layer.rnn_encoder import LSTMEncoder


class SERModel(tf.keras.Model):
    """"""
    Defines a SEntence Representations Model
    """"""

    def __init__(self,
                 units,  # dimensionality of the output space
                 input_dim,  # vocab of size
                 output_dim,  # embedding dimension
                 name='SEntence Representations Model',
                 **kwargs):
        super(SERModel, self).__init__(name=name, **kwargs)
        self.encoder = LSTMEncoder(units=units, input_dim=input_dim, output_dim=output_dim)

    def call(self, inputs):
        sentence_a, sentence_b = inputs
        return self.encoder(sentence_a), self.encoder(sentence_b)

```

To train the model one can simply do as shown next:


```python
from source.model.SERModel import SERModel
import tensorflow as tf


def run_test():
    
    # load train data
    train_data = load_data()

    ser = SERModel(units=128, input_dim=10000, output_dim=64)

    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

    ser.compile(optimizer, loss=tf.keras.losses.CosineSimilarity())
    ser.fit(train_data, epochs=3, batch_size=64)


if __name__ == '__main__':
    run_test()

```

At this point is where I need help. The loss requires `y_true` and `y_pred`, but as can be seen, this is unsupervised training and there is no `y_true`. So, would you have an approach that would allow me to train the above architecture using `CosineSimilarity` loss?


Thank you very much in advance.


  [1]: https://i.stack.imgur.com/Px8hL.png"
37881,tf.tpu.experimental.initialize_tpu_system raises GRPC error,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: NA
- TensorFlow installed from (source or
binary): using google colab
- TensorFlow version (use command below): 
- Python version: - Bazel
version (if compiling from source): 3.6

**Describe the current behavior**
When I try to setting TPU system in google colab in tensorflow 2.0.0, tf.tpu.experimental.initialize_tpu_system raises GRPC error. 
It worked normally until about 12 hours ago and I don't change any codes.
What I try to do to solve this problem?

**Standalone code to reproduce the issue** 
https://gist.github.com/fwatty-1218/48d4f1be7b66245321376c4e2af3352a

**Any other info / logs**
If I use tensorflow 2.1.0, it works fine, but I want to use tensorflow 2.0.0 because I want to use TPU."
37880,"Build Fail Due to ""ImportError: No module named enum"" (python 3.7)","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (macOS Mojave, version 10.14.6):
- TensorFlow installed from (source, https://github.com/tensorflow/tensorflow.git):
- TensorFlow version: 2
- Python version: 3.7
- Installed using pip
- Bazel version (if compiling from source): 2.2.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Following the extract instruction here: https://www.tensorflow.org/install/source 
Specified python 3.7 in ./configure

Run:  **bazel build //tensorflow/tools/pip_package:build_pip_package --host_force_python=PY3**

The build failed, saying ""ImportError: No module named enum"". See the detail below. 
**Already googled and tried  pip install --upgrade pip enum34, but still not working.**



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
ERROR: /Users/leiwang/GoogleDrive/models/tensorflow/tensorflow/python/keras/api/BUILD:133:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)
Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_leiwang/db84a35c42355241097f16166c7580af/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/private/var/tmp/_bazel_leiwang/db84a35c42355241097f16166c7580af/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 64, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""/private/var/tmp/_bazel_leiwang/db84a35c42355241097f16166c7580af/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py"", line 52, in <module>
    from tensorflow.python.framework.importer import import_graph_def
  File ""/private/var/tmp/_bazel_leiwang/db84a35c42355241097f16166c7580af/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 28, in <module>
    from tensorflow.python.framework import function
  File ""/private/var/tmp/_bazel_leiwang/db84a35c42355241097f16166c7580af/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/framework/function.py"", line 37, in <module>
    from tensorflow.python.ops import resource_variable_ops
  File ""/private/var/tmp/_bazel_leiwang/db84a35c42355241097f16166c7580af/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 46, in <module>
    from tensorflow.python.ops import variables
  File ""/private/var/tmp/_bazel_leiwang/db84a35c42355241097f16166c7580af/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/ops/variables.py"", line 20, in <module>
    import enum  # pylint: disable=g-bad-import-order
ImportError: No module named enum
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /Users/leiwang/GoogleDrive/models/tensorflow/tensorflow/tools/pip_package/BUILD:65:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1)
INFO: Elapsed time: 1.875s, Critical Path: 1.19s
INFO: 0 processes.
FAILED: Build did NOT complete successfully

"
37876,WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.112000). Check your callbacks.,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10 Pro x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: No
- TensorFlow installed from (source or
binary): source
- TensorFlow version (use command below): master branch, commit 99e754b3a189eefab15fdbf326115d312e44fc7b
- Python version: 3.7
- Bazel
version (if compiling from source): the default downloaded by Bazelisk
- GCC/Compiler version (if compiling from
source): MSVS2019 v16.5.0
- CUDA/cuDNN version: CUDA 10.2, cuDNN 7.6.5
- GPU model and memory: Geforce GTX 1080, 8GB or Geforce RTX 2080, 8GB (same problem)

**Describe the current behavior**
![on_train_batch_end](https://user-images.githubusercontent.com/5251612/77467782-a59f2a00-6e1d-11ea-82e1-b45377442294.png)
2/3 of time is spent in method `on_train_batch_end` without any user callbacks (i.e. it seems it spends time in TensorFlow internals)

**Describe the expected behavior**
`on_train_batch_end` doesn't take a majority of training time if no user callbacks that do heavy ops in it are specified.

**Standalone code to reproduce the issue** 
You need at least 64GB of RAM to execute this:
```
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

num_inputs=1440*3
num_outputs=2

N = 1000*1000
train_X = np.random.randn(N, num_inputs)
train_y = np.random.randn(N, num_outputs)

tf.keras.backend.set_floatx('float32')
model = keras.Sequential()
n_hidden = 2*num_inputs

activation1 = tf.nn.relu
model.add(layers.Dense(n_hidden, activation=activation1, input_shape=(num_inputs,)))
for i in range(4):
    model.add(layers.Dense(n_hidden, activation=activation1))
model.add(layers.Dense(num_outputs))

model.compile(optimizer=tf.keras.optimizers.RMSprop(1e-3),
              loss=tf.keras.losses.mse,
              metrics=(tf.keras.metrics.mse,))

model.fit(train_X, train_y, epochs=1000, batch_size=512, verbose=2)
```

**Other info / logs**
```
D:\Programs\Anaconda\python.exe ""C:\Program Files\JetBrains\PyCharm 2019.3.3\plugins\python\helpers\pydev\pydevconsole.py"" --mode=client --port=1305
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['D:\\Dev\\Views\\Trading-git\\CudaNN\\src\\Anaconda', 'D:/Dev/Views/Trading-git/CudaNN/src/Anaconda'])
Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 7.8.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.8.0
Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] on win32
runfile('D:/Dev/Views/Trading-git/CudaNN/src/Anaconda/Reproduction01.py', wdir='D:/Dev/Views/Trading-git/CudaNN/src/Anaconda')
2020-03-24 22:40:00.449504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_102.dll
2020-03-24 22:41:58.323992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-03-24 22:41:58.347918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties: 
pciBusID: 0000:0b:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.8855GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2020-03-24 22:41:58.348257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_102.dll
2020-03-24 22:41:58.352623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-03-24 22:41:58.356230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-03-24 22:41:58.357648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-03-24 22:41:58.361747: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-03-24 22:41:58.364018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-03-24 22:41:58.372291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-03-24 22:41:58.373318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0
2020-03-24 22:41:58.373720: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-03-24 22:41:58.382851: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d1a0667d60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-03-24 22:41:58.383116: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-03-24 22:41:58.383724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties: 
pciBusID: 0000:0b:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.8855GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2020-03-24 22:41:58.384041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_102.dll
2020-03-24 22:41:58.384254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-03-24 22:41:58.384441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-03-24 22:41:58.384612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-03-24 22:41:58.384788: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-03-24 22:41:58.384962: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-03-24 22:41:58.385136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-03-24 22:41:58.385759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0
2020-03-24 22:41:58.938950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-24 22:41:58.939152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 
2020-03-24 22:41:58.939263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N 
2020-03-24 22:41:58.939991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6278 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:0b:00.0, compute capability: 6.1)
2020-03-24 22:41:58.943050: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d1c48bade0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-03-24 22:41:58.943286: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1
Epoch 1/1000
2020-03-24 22:42:13.278214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188999). Check your callbacks.
WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.198997). Check your callbacks.
WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.199499). Check your callbacks.
WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.200001). Check your callbacks.
WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.199999). Check your callbacks.
WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.200000). Check your callbacks.
WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.200000). Check your callbacks.
WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.200000). Check your callbacks.
WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.200000). Check your callbacks.
```
"
37874,Allow tf.image.random_crop to return multiple crops,"It would be nice if there was an option for `tf.image.random_crop` to return more than a single crop at a time.

It's possible to reproduce this logic through something like the code below, but I find that it is bottlenecking my tf.data pipeline, and I have to use `tf.image.extract_patches` instead.

```
@tf.function
def get_patches(image, num_patches=100, patch_size=16):
    patches = []
    for i in range(num_patches):
        patch = tf.image.random_crop(image, [patch_size, patch_size, 3])
        patches.append(patch)

    return tf.stack(patches)
```
"
37872,Keras model for sentiment analysis is not learning anything!,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 

- Google Colab

**Describe the current behavior**

I have tried to build a Keras model for sentiment analysis using ``tf.GradientTape`` and ``tf.function`` instead of using keras's ``model.compile`` and ``model.fit``. With the latter, the accuracy starts off at around ``0.50`` and raises to ``0.95`` on the training set and ``0.86`` on the test set. While the former just wanders around ``0.50`` accuracy.

**Describe the expected behavior**

It is expected that the accuracy goes up and not just oscillate around ``0.5``.

**Standalone code to reproduce the issue** 

The full notebook: https://colab.research.google.com/drive/1DESzPRktTkYzZ0nnOo9ofNMPaVg-0UHd

The Model:

```python
class ANNForSentimentAnalysis(tf.keras.Model):
    def __init__(self, embedding = ""https://tfhub.dev/google/nnlm-en-dim128/1"", name=""ANNForSentimentAnalysis"", **kwargs):
        super(ANNForSentimentAnalysis, self).__init__(name=name, **kwargs)
        self._layers = [
            hub.KerasLayer(embedding, trainable=True, dtype=tf.string),
            Dense(16, activation='relu'),
            Dense(1, activation='sigmoid')
        ]
        # self._model = Sequential(self._layers)

    @tf.function
    def call(self, inputs):
        # return self._model(inputs)
        for layer in self._layers:
            inputs = layer(inputs)
        return inputs

def train_step(train_data, model, optimizer, loss_func, metric):
    for i, batch in enumerate(train_data):
        x_train, y_train = batch
        with tf.GradientTape() as tape:
            preds = model(x_train)
            loss = loss_func(y_train, preds)
        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        metric_eval = metric(y_train, preds)
        sys.stdout.write(f""\rStep {i}: ["" + i*""*"" + f""]\tloss: {loss:.4f}\taccuracy: {metric_eval:.4f}"")
    print("""")

def accuracy(y_train, y_pred):
    return tf.reduce_mean(tf.cast(tf.cast(y_train, tf.float32) == tf.cast(y_pred > 0.5, tf.float32), tf.float32))
```

**Training Log**

```none
Step 29: [*****************************]	loss: 0.6937	accuracy: 0.5776
Step 29: [*****************************]	loss: 0.6667	accuracy: 0.4945
Step 29: [*****************************]	loss: 0.6421	accuracy: 0.4564
Step 29: [*****************************]	loss: 0.6559	accuracy: 0.5151
Step 29: [*****************************]	loss: 0.6455	accuracy: 0.5210
Step 29: [*****************************]	loss: 0.6347	accuracy: 0.5201
Step 29: [*****************************]	loss: 0.6236	accuracy: 0.5031
Step 29: [*****************************]	loss: 0.6119	accuracy: 0.5059
Step 29: [*****************************]	loss: 0.5719	accuracy: 0.5007
Step 29: [*****************************]	loss: 0.5685	accuracy: 0.4990
Step 29: [*****************************]	loss: 0.5626	accuracy: 0.4990
Step 29: [*****************************]	loss: 0.5764	accuracy: 0.5074
Step 29: [*****************************]	loss: 0.5735	accuracy: 0.4994
Step 29: [*****************************]	loss: 0.5386	accuracy: 0.4995
Step 29: [*****************************]	loss: 0.5384	accuracy: 0.4995
Step 29: [*****************************]	loss: 0.5517	accuracy: 0.5000
Step 29: [*****************************]	loss: 0.5328	accuracy: 0.4997
Step 29: [*****************************]	loss: 0.5351	accuracy: 0.4979
Step 29: [*****************************]	loss: 0.5574	accuracy: 0.5052
Step 29: [*****************************]	loss: 0.5166	accuracy: 0.5010
```
"
37871,Argmax error on deeplab quantized model with tflite hexagon delegate,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS
- TensorFlow installed from (source or binary): Source
- TensorFlow version:  Build from: git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git
- Python version: 3.6.9
- Bazel version (if compiling from source): 2.0.0, 1.14
- GCC/Compiler version (if compiling from source):gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- Device: Redmi Note 7 Pro (Hexagon 685 DSP),  Android 10.0; MIUI 11

**Describe the current behavior**
When i try to run the official **deeplab model** with quantization aware training in tflite benchmark  using **hexagon** delegate, it fails with the following error:-

`adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/frozen_inference_graph_dm05_5.tflite --use_hexagon=true hexagon_profiling=true
adb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)
STARTING!
Min num runs: [50]
Min runs duration (seconds): [1]
Max runs duration (seconds): [150]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [/data/local/tmp/frozen_inference_graph_dm05_5.tflite]
Input layers: []
Input shapes: []
Input value ranges: []
Input layer values files: []
Use legacy nnapi : [0]
Allow fp16 : [0]
Require full delegation : [0]
Enable op profiling: [0]
Max profiling buffer entries: [1024]
CSV File to export profiling data to: []
Max number of delegated partitions : [0]
Enable platform-wide tracing: [0]
Use gpu : [0]
Allow lower precision in gpu : [1]
Use Hexagon : [1]
Hexagon lib path : [/data/local/tmp]
Hexagon Profiling : [0]
External delegate path : []
External delegate options : []
Use nnapi : [0]
Use xnnpack : [0]
Loaded model /data/local/tmp/frozen_inference_graph_dm05_5.tflite
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so
INFO: Created TensorFlow Lite delegate for Hexagon.
INFO: Hexagon delegate: 71 nodes delegated out of 71 nodes.

Applied Hexagon delegate, and the model graph will be completely executed w/ the delegate.
The input model file size (MB): 0.746232
Initialized session in 315.521ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.

----------------
Timestamp: Tue Mar 24 18:13:02 2020


Log
hexagon/ops/src/op_argminmax_8_d32.c:119:argminmax_8_d32 out too small
hexagon/src/execute.c:142:execute() failed on node id=37b err=-1
hexagon/src/interface.c:1174:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph.. STATE: FAILED_TO_EXECUTE_GRAPH
ERROR: Node number 71 (TfLiteHexagonDelegate) failed to invoke.

----------------
Timestamp: Tue Mar 24 18:13:02 2020


Log
hexagon/ops/src/op_argminmax_8_d32.c:119:argminmax_8_d32 out too small
hexagon/src/execute.c:142:execute() failed on node id=37b err=-1
hexagon/src/interface.c:1174:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph.. STATE: FAILED_TO_EXECUTE_GRAPH
ERROR: Node number 71 (TfLiteHexagonDelegate) failed to invoke.

----------------
Timestamp: Tue Mar 24 18:13:02 2020


Log
hexagon/ops/src/op_argminmax_8_d32.c:119:argminmax_8_d32 out too small
hexagon/src/execute.c:142:execute() failed on node id=37b err=-1
hexagon/src/interface.c:1174:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph.. STATE: FAILED_TO_EXECUTE_GRAPH
ERROR: Node number 71 (TfLiteHexagonDelegate) failed to invoke.

----------------
Timestamp: Tue Mar 24 18:13:02 2020


Log
hexagon/ops/src/op_argminmax_8_d32.c:119:argminmax_8_d32 out too small
hexagon/src/execute.c:142:execute() failed on node id=37b err=-1
hexagon/src/interface.c:1174:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph.. STATE: FAILED_TO_EXECUTE_GRAPH
ERROR: Node number 71 (TfLiteHexagonDelegate) failed to invoke.

----------------
Timestamp: Tue Mar 24 18:13:03 2020


Log
hexagon/ops/src/op_argminmax_8_d32.c:119:argminmax_8_d32 out too small
hexagon/src/execute.c:142:execute() failed on node id=37b err=-1
hexagon/src/interface.c:1174:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph.. STATE: FAILED_TO_EXECUTE_GRAPH
ERROR: Node number 71 (TfLiteHexagonDelegate) failed to invoke.

----------------
Timestamp: Tue Mar 24 18:13:03 2020


Log
hexagon/ops/src/op_argminmax_8_d32.c:119:argminmax_8_d32 out too small
hexagon/src/execute.c:142:execute() failed on node id=37b err=-1
hexagon/src/interface.c:1174:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph.. STATE: FAILED_TO_EXECUTE_GRAPH
ERROR: Node number 71 (TfLiteHexagonDelegate) failed to invoke.

----------------
Timestamp: Tue Mar 24 18:13:03 2020


Log
hexagon/ops/src/op_argminmax_8_d32.c:119:argminmax_8_d32 out too small
hexagon/src/execute.c:142:execute() failed on node id=37b err=-1
hexagon/src/interface.c:1174:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph.. STATE: FAILED_TO_EXECUTE_GRAPH
ERROR: Node number 71 (TfLiteHexagonDelegate) failed to invoke.

count=7 first=84641 curr=81095 min=79276 max=84641 avg=80591 std=1766

Benchmarking failed.`

I removed argmax and model worked with nearest neighbor resizing.The model does not seem to work with int32/int64 argmax as the final layer; even though the operator is supported by [hexagon delegate](https://www.tensorflow.org/lite/performance/hexagon_delegate?hl=fr#faq). The final resize has output shape [513,513], so i tried replacing bilinear resize with  nearest neighbor resizing; but the benchmark still failed . I also verified the setup by  correctly running mobienet_v2 quantized model. Both models with **int32 and int64 argmax** seems to give same error: **argminmax_8_d32 out too small**

Hexagon library: [v1.14](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.14.run)

**Describe the expected behavior**
The benchmark model should run the quantized deeplab model without any problems.

**Other info / logs**
Android NDK: 20, Benchmark tool built from latest source with bazel 2.0

Here are the three models that i've tried :-
[quant_aware_deeplab_dm05_513.zip](https://github.com/tensorflow/tensorflow/files/4378780/quant_aware_deeplab_dm05_513.zip)


"
37870,run_eagerly model option doesn't work in Keras loss,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10, x64
- TensorFlow installed from (source or
binary): Binary (pip)
- TensorFlow version (use command below): TF 2.2.0 (2.2.0.dev20200323)
- Python version: 3.7
- CUDA/cuDNN version: 10.1 / 7.6
- GPU model and memory: Bug appears on several computers with different GPU

**Describe the current behavior**

When fitting a Keras model in eager mode, by compiling it with the option run_eagerly=True, the loss is not run eagerly, even though the rest of the model is.

This bug does not appear when using  tf.config.experimental_run_functions_eagerly(True) instead of the run_eagerly option to run the model eagerly, in which case the loss is run eagerly.

**Describe the expected behavior**

Both tf.config.experimental_run_functions_eagerly(True) and run_eagerly=True should have the same behaviour when fitting a model: run all of the model, including its loss, eagerly.

**Standalone code to reproduce the issue** 

```
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Custom loss
class CustomLoss(keras.losses.Loss):
	def call(self, y_true, y_pred):
		print(tf.executing_eagerly())
		
		x = y_true + y_pred
		return tf.reduce_mean(x)

if __name__ == ""__main__"" :
	data = np.random.random((16, 1000, 3)).astype(np.float32)
	
	inputs = tf.keras.Input(shape=(1000,3))
	outputs = tf.keras.layers.Dense(3)(inputs)
	model = tf.keras.Model(inputs=inputs, outputs=outputs)
	
	# tf.config.experimental_run_functions_eagerly(True) # runs custom loss eagerly
	# model.compile(loss=CustomLoss())
	
	model.compile(loss=CustomLoss(), run_eagerly = True) # does not run custom loss eagerly
	
	model.fit(x=data, y=data)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

[logCompileEagerly.txt](https://github.com/tensorflow/tensorflow/files/4375802/logCompileEagerly.txt)
[logExperimentalRunEagerly.txt](https://github.com/tensorflow/tensorflow/files/4375804/logExperimentalRunEagerly.txt)

"
37869,Build tensorflow for  c++,"Hello,

I try to build tensorflow for cpp API. I use a docker to compile and generate a package.

Here is the docker : 
```
`
FROM tensorflow/tensorflow

RUN apt-get install -y git
RUN DEBIAN_FRONTEND=noninteractive  apt-get install -y tzdata 

# Install object detection api dependencies
RUN apt-get install -y protobuf-compiler python-pil python-lxml python-tk zip unzip && \
    pip install Cython && \
    pip install contextlib2 && \
    pip install jupyter && \
    pip install matplotlib 

# Get tensorflow source
RUN cd /home/xxxx && git clone https://github.com/tensorflow/tensorflow

# Install bazel
RUN apt install curl 
RUN curl https://bazel.build/bazel-release.pub.gpg | apt-key add -
RUN echo ""deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8"" |  tee /etc/apt/sources.list.d/bazel.list
RUN apt update && apt install -y  bazel && apt install -y  bazel-1.0.0
RUN apt install bazel-2.0.0
RUN pip install future

RUN cd /home/xxxx/tensorflow/ && /home/xxxx/tensorflow/ && bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both //tensorflow:libtensorflow_cc.so \
                //tensorflow:libtensorflow_framework.so \
                //tensorflow:install_headers

`

```
I try with and without HW optimisation:
Without:
```
bazel build -c opt //tensorflow:libtensorflow_cc.so \
                //tensorflow:libtensorflow_framework.so \
                //tensorflow:install_headers

```

With:
```
bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both //tensorflow:libtensorflow_cc.so \
                //tensorflow:libtensorflow_framework.so \
                //tensorflow:install_headers""
```

When I use the optimize or the standard package the execution time is the same. 
When I use the optimize version the following message dissapear:   

```
2020-03-24 `14:48:16.411418: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA

```
The message dissapears but the excecution time is equivalent ~200ms. 



"
37868,Train time multi crop augmentation,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
37867,tables_initializer,"`sess_id.run(graph_id.get_operation_by_name(""MyGraphID/init_all_tables"")) `

I did not find a way to achieve this in C ++. what should I do？"
37866,unable to generate train.record file,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
37865,Migrate TF1 to TF2,"Hi, Im newbie on tensorflow. 
How to use embedding_rnn_seq2seq on tensorflow v2? On tensorflow v1, it is in contrib.legacy_seq2seq class."
37864,protobuf.bzl outdated making build with TF_SYSTEM_LIBS=com_google_protobuf fail,"**Describe the current behavior**

When using `TF_SYSTEM_LIBS=com_google_protobuf` to configure TensorFlow 2.1 the build fails with 

```
ERROR: /dev/shm/s3248973-EasyBuild/TensorFlow/2.1.0/foss-2019b-Python-3.7.4/TensorFlow/tensorflow-2.1.0/tensorflow/core/profiler/BUILD:51:1: in proto_gen rule //tensorflow/core/profiler:profiler_service_proto_py_genproto: 
Traceback (most recent call last):
        File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.1.0/foss-2019b-Python-3.7.4/TensorFlow/tensorflow-2.1.0/tensorflow/core/profiler/BUILD"", line 51
                proto_gen(name = 'profiler_service_proto_py_genproto')
        File ""/tmp/easybuild-tmp/eb-NQjKjm/tmp8LJdGE-bazel-build/external/com_google_protobuf/protobuf.bzl"", line 110, in _proto_gen_impl
                ctx.actions.run(inputs = inputs, outputs = ctx.out..., <4 more arguments>)
Found tool(s) 'bazel-out/k8-opt/bin/external/grpc/grpc_python_plugin' in inputs. A tool is an input with executable=True set. All tools should be passed using the 'tools' argument instead of 'inputs' in order to make their runfiles availa
ble to the action. This safety check will not be performed once the action is modified to take a 'tools' argument. To temporarily disable this check, set --incompatible_no_support_tools_in_action_inputs=false.
```

Reason is seemingly a very outdated protobuf.bzl which is used with system_libs. It might be enough to update it from the current protobuf repo. Maybe even download that single file only when using system libs"
37863,Win10: ImportError: DLL load failed: The specified module could not be found.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.1.0
- Python version: 3.7 (Anaconda 2019.10, conda 4.8.3)
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA, not NVIDIA
- GPU model and memory:Intel(R) HD Graphics 620



**Describe the problem**
Cannot import tensorflow

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I created Anaconda environment using Anaconda Navigator. Environment name is tensorflow_env. In Anaconda Prompt, I first checked pip version
>> pip -V

which is 20 (greater than requirement for tensorflow), and version for Python, which falls in the required versions. I then activated the environment
>> activate tensorflow_env

which worked: the environment name appeared in the command line. Then, I did 
>> pip install tensorflow

Installation seemed to work just fine. Then I checked if the package was added to the environment 
>> conda list

Tensorflow is visible in this Anaconda environment. Then, I switched to python 
>> python

and try to import tensorflow
>> import tensorflow as tf

I get error, copypasted below.


**Any other info / logs**
(tensorflow_env) C:\DA_AI\3_ML>python

Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf

Traceback (most recent call last):
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\jugi\AppData\Local\Continuum\anaconda3\envs\tensorflow_env\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
37862,"Learning Rate scheduler with custom training using ""tf.GradientTape""","Hello, I am using a learning rate scheduler to reduce the learning rate which is defined in the function as follows:

```
def scheduler(epoch):
    learning_rate = 0.0012
    if epoch >= 9 and epoch == 19:
        return learning_rate / 10
    elif epoch > 19:
        return learning_rate / 100
    else:
        return learning_rate
```

How can I use this defined scheduler() function with custom training loop using ""tf.GradientTape""?

The way I know is to use the scheduler as a callback:

```
callback = tf.keras.callbacks.LearningRateScheduler(scheduler)

model.fit(
	x = data, y = labels,
	epochs=100, callbacks=[callback],
	validation_data=(val_data, val_labels))

```

Thanks"
37861,Debundled protobuf broken,"
**System information** 
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0 but most likely all affected
- Bazel version (if compiling from source): 0.26.1

**Describe the current behavior**

Using TF_SYSTEM_LIBS=com_google_protobuf leads to 

> In file included from bazel-out/k8-opt/bin/tensorflow/compiler/xla/xla_data.pb.cc:4:
bazel-out/k8-opt/bin/tensorflow/compiler/xla/xla_data.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory
 #include <google/protobuf/port_def.inc>

Full compiler invocation:

> /sw/installed/OpenMPI/3.1.3-GCC-8.2.0-2.31.1/bin/mpicc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/xla/_objs/xla_data_proto/xla_data.pb.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/compiler/xla/_objs/xla_data_proto/xla_data.pb.pic.o' -fPIC -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -O2 -ftree-vectorize '-march=native' -fno-math-errno -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-opt/bin/tensorflow/compiler/xla/xla_data.pb.cc -o bazel-out/k8-opt/bin/tensorflow/compiler/xla/_objs/xla_data_proto/xla_data.pb.pic.o


**Describe the expected behavior**

Using TF_SYSTEM_LIBS=com_google_protobuf works

**Standalone code to reproduce the issue** 
Configure with  TF_SYSTEM_LIBS=com_google_protobuf then build

**Other info / logs**
There are 2 issues with that:

1. E.g. for 2.0.0 the directory containing the symlinked headers is passed as `-iquote bazel-out/k8-opt/bin/external/com_google_protobuf` but incuded with ` #include <google/protobuf/port_def.inc>` which by definition of `-iquote` will not be found
2. After manually changing the `-iquote` to `-I` it shows that not all headers are copied. The list for 2.0.0 is at https://github.com/tensorflow/tensorflow/blob/v2.0.0/third_party/systemlibs/protobuf.BUILD#L15-L43 and is missing `port_undef.inc` hence the command fails with 
> In file included from bazel-out/k8-opt/bin/tensorflow/compiler/xla/xla_data.pb.cc:4:
bazel-out/k8-opt/bin/tensorflow/compiler/xla/xla_data.pb.h:22:10: fatal error: google/protobuf/port_undef.inc: No such file or directory
 #include <google/protobuf/port_undef.inc>

A possible solution was suggested in https://github.com/tensorflow/tensorflow/issues/37835: Allow usage of explicit prefixes and do NOT try to copy/symlink headers (or at least test that)"
37860,freezing tensorflow model,"The model I use is meta graph and win 10 ver 1903
My model clone :
https://github.com/ZJULearning/pixel_link

I am trying to freeze a flow pattern

In tenorflow, training from scratch is created after 4 files:

1. model.ckpt-38055.data-00000-of-00001
2. model.ckpt-38055.index
3. model.ckpt-38055.meta
4. checkpoint

I would like to convert them (or only the needed ones) into one file  graph.pb

I use src :

> `import tensorflow as tf
> 
> meta_path = 'model.ckpt-38055.meta'  # Your .meta file
> 
> 
> config = tf.ConfigProto()
> 
> 
> with tf.Session(config=config) as sess:
> 
>     with tf.device(""/cpu:0""): 
> 
>     # Restore the graph
>         saver = tf.train.import_meta_graph(meta_path)
> 
>         # Load weights
>         saver.restore(sess, tf.train.latest_checkpoint('./'))
>         # saver.restore(sess, ""model.ckpt-38055"")
> 
>         output_node_names = [n.name for n in tf.get_default_graph().as_graph_def().node]
>         # output_node_names = ""reconstruction_layer""
> 
>         # Freeze the graph
>         frozen_graph_def = tf.graph_util.convert_variables_to_constants(
>             sess,
>             sess.graph_def,
>             output_node_names)
> 
>         # Save the frozen graph
>         with open('output_graph.pb', 'wb') as f:
>             f.write(frozen_graph_def.SerializeToString())`

 I encountered an error :

> 2020-03-24 15:00:30.107728: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
Traceback (most recent call last):
  File ""freeze_graph.py"", line 5, in <module>
    sess, sess.graph_def, [""out""])
  File ""C:\Users\Lerror\Anaconda3\lib\site-packages\tensorflow\python\framework\graph_util_impl.py"", line 227, in convert_variables_to_constants
    inference_graph = extract_sub_graph(input_graph_def, output_node_names)
  File ""C:\Users\Lerror\Anaconda3\lib\site-packages\tensorflow\python\framework\graph_util_impl.py"", line 171, in extract_sub_graph
    _assert_nodes_are_present(name_to_node, dest_nodes)
  File ""C:\Users\Lerror\Anaconda3\lib\site-packages\tensorflow\python\framework\graph_util_impl.py"", line 131, in _assert_nodes_are_present
    assert d in name_to_node, ""%s is not in graph"" % d
AssertionError: out is not in graph


Hope you can help me

thank you"
37859,tf.keras.losses.SparseCategoricalCrossentropy  Documentation Issue,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy

## Description of issue (what needs changing):

### Clear description

On the third line of the second code box, origin probability is [.9, .05, .05], [.5, .89, .6], [.05, .01, .94], it should be [.9, .05, .05], [.05, .89, .06], [.05, .01, .94]. "
37858,tf.function retracing even with input_signature,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.3 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.1
- **Python version**: 3.7
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: 9.2
- **GPU model and memory**: GTX 1080 Ti
- **Exact command to reproduce**: Run the attached script

### Describe the problem
I made & saved a Keras model 
But it brought tf.function retracing even with input_signature when it was loaded

### Source code / logs

* log
WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7ff01c64e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.

* Source code
I removed the code one by one to narrow down the root-cause, still having the retracing issue
I think that this should work.
To reproduce the issue, we need to do 2 steps as follows

1) Save a model
```
import os
import numpy as np
import tensorflow as tf

@tf.function(input_signature=[
    tf.TensorSpec([None, 15, 15], tf.uint16, name='curr'),
    tf.TensorSpec([None, 2, 15, 15], tf.uint16, name='refs'),
    tf.TensorSpec([None, 15, 15], tf.uint8, name='grades_image'),
    tf.TensorSpec([None, 15, 15], tf.uint8, name='alarms_image'),
])
def all_attributes(curr, refs, grades_image, alarms_image):
    return tf.ones(tf.shape(curr)[0], tf.float32)

def create_layer(tf_func, name=None, trainable=False):
    Return keras layer instance for given tensorflow function""""""
    if name is None:
        name = getattr(tf_func, '__name__', 'unnamed') + '_layer'
    class NewLayer(tf.keras.layers.Layer):
        def call(self, inputs):
            return tf_func(*inputs)
    NewLayer.__name__ = name  
    layer_instance = NewLayer()
    layer_instance.trainable = trainable
    return layer_instance

class TestModel:
    Class for Selfi model for production""""""
    def build(self, name='unnamed_model'):
        curr = tf.keras.Input(shape=(15, 15), dtype=tf.uint16, name='curr')
        refs = tf.keras.Input(shape=(2, 15, 15), dtype=tf.uint16, name='refs')
        grades_image = tf.keras.Input(shape=(15, 15), dtype=tf.uint8, name='grades_image')
        alarms_image = tf.keras.Input(shape=(15, 15), dtype=tf.uint8, name='alarms_image')
        inputs = [curr, refs, grades_image, alarms_image]
        all_atts_stacked = create_layer(all_attributes)(inputs)
        outputs = [all_atts_stacked]        
        m = tf.keras.Model(inputs=inputs, outputs=outputs, name=name)
        self._keras_model = m
        return self
    def save(self, out_path):
        model = self._keras_model
        model.save(out_path)
        return self

if __name__ == ""__main__"":
    m = TestModel().build(name='Attribute_Model')
    save_root_dir = './Models'
    save_path = os.path.join(save_root_dir, m.name)
    m.save(save_path) 
```

2) Load & try the saved model. Here the retracing issue happens
```
import numpy as np
from numpy import ndarray
from typing import Tuple, List
import tensorflow as tf
import time

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")
  except RuntimeError as e:
    print(e)

model_path = ""./Models""

def copy_to_aligned_data(input_arr: ndarray, align: int) -> ndarray:
    pixels = input_arr.size
    alignment = align
    dtype = input_arr.dtype
    nbytes = pixels * dtype.itemsize
    buf = np.empty(nbytes + alignment, dtype=np.uint8)
    start_index = -buf.ctypes.data % alignment
    out_array = buf[start_index:start_index + nbytes].view(dtype).reshape(input_arr.shape)
    out_array[:] = input_arr[:]
    return out_array

def create_inputs_for_supervised_model(img_shape, refs_shape) -> List:
    curr = np.random.randint(max_value_14bit, size=img_shape, dtype=np.uint16)
    refs = np.random.randint(max_value_14bit, size=refs_shape, dtype=np.uint16)
    grades_image = np.random.randint(max_value_8bit, size=img_shape, dtype=np.uint8)
    alarms_image = np.random.randint(2, size=img_shape, dtype=np.uint8)
    curr_align = copy_to_aligned_data(curr, 64)
    refs_align = copy_to_aligned_data(refs, 64)
    grades_image_align = copy_to_aligned_data(grades_image, 64)
    alarms_image_align = copy_to_aligned_data(alarms_image, 64)
    curr_tf = tf.convert_to_tensor(curr_align, curr_align.dtype)
    refs_tf = tf.convert_to_tensor(refs_align, refs_align.dtype)
    grades_tf = tf.convert_to_tensor(grades_image_align, grades_image_align.dtype)
    alarms_tf = tf.convert_to_tensor(alarms_image_align, alarms_image_align.dtype)
    return [curr_tf, refs_tf, grades_tf, alarms_tf]

model_orig = tf.keras.models.load_model(model_path, compile=False)

max_value_14bit = np.iinfo(np.uint16).max // 4
max_value_8bit = np.iinfo(np.uint8).max

defects_count = 20
img_shapee = (defects_count, 15, 15)
refs_shapee = (defects_count, 2, 15, 15)
inputs = create_inputs_for_supervised_model(img_shapee, refs_shapee)
output = model_orig(inputs)

loop = 10
for i in range(loop):
    defects_count = 10000 + i*100
    img_shapee = (defects_count, 15, 15)
    refs_shapee = (defects_count, 2, 15, 15)
    inputs = create_inputs_for_supervised_model(img_shapee, refs_shapee)
    start = time.time()
    output = model_orig(inputs)
    output.numpy()
    duration_ms = (time.time() - start)*1000  # / loop
```


"
37857,tflite_with_xnnpack.cc: undefined reference to `TfLiteXNNPackDelegateOptionsDefault',"**System information**

```
Linux ubuntu 5.3.0-1019-raspi2 #21-Ubuntu SMP Mon Feb 17 14:05:03 UTC 2020 aarch64 aarch64 aarch64 GNU/Linux
```

- TensorFlow installed from source 
- TensorFlow version: commit e9db6486b37a5aa4ed25aa3e661d64cc9e0fdd2
- GCC/Compiler version (if compiling from source):

```
ubuntu@ubuntu:~$ aarch64-linux-gnu-g++ --version
aarch64-linux-gnu-g++ (Ubuntu 9.2.1-9ubuntu2) 9.2.1 20191008
```
- GPU model and memory: Raspberry Pi 4 4GB

**Describe the problem**

Problem building TFLite library with make in arm64
tflite_with_xnnpack.cc: undefined reference to `TfLiteXNNPackDelegateOptionsDefault'

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
ubuntu@ubuntu:~/tensorflow$` git checkout e9db6486b37a5aa4ed25aa3e661d64cc9e0fdd2e
...
ubuntu@ubuntu:~/tensorflow$ ./tensorflow/lite/tools/make/download_dependencies.sh
...
ubuntu@ubuntu:~/tensorflow$ ./tensorflow/lite/tools/make/build_aarch64_lib.sh 
...
aarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC --std=c++11  -DTFLITE_WITH_RUY -DTFLITE_WITHOUT_XNNPACK -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/ubuntu/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/ubuntu/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/ubuntu/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/ubuntu/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/ubuntu/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/ubuntu/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/ubuntu/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/ubuntu/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/ubuntu/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/ubuntu/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include \
-o /home/ubuntu/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/bin/benchmark_model /home/ubuntu/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/benchmark/benchmark_main.o \
 -Wl,--whole-archive /home/ubuntu/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/lib/benchmark-lib.a -Wl,--no-whole-archive -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lstdc++ -lpthread -lm -ldl
/usr/bin/ld: /home/ubuntu/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/lib/benchmark-lib.a(tflite_with_xnnpack.o): in function `tflite::AcquireXNNPACKDelegate(int)':
tflite_with_xnnpack.cc:(.text+0x2c): undefined reference to `TfLiteXNNPackDelegateOptionsDefault'
/usr/bin/ld: tflite_with_xnnpack.cc:(.text+0x40): undefined reference to `TfLiteXNNPackDelegateCreate'
/usr/bin/ld: tflite_with_xnnpack.cc:(.text+0x48): undefined reference to `TfLiteXNNPackDelegateDelete'
/usr/bin/ld: tflite_with_xnnpack.cc:(.text+0x58): undefined reference to `TfLiteXNNPackDelegateDelete'
collect2: error: ld returned 1 exit status
make: *** [tensorflow/lite/tools/make/Makefile:321: /home/ubuntu/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/bin/benchmark_model] Error 1
make: *** Waiting for unfinished jobs....
```

**Any other info / logs**

**Workarround**: revert commit 7247fefebbbae4ba02a4aa01d18c503719eb72c8 fix the problem


"
37854,Tenserflow error,"Using TensorFlow backend.
Using TensorFlow backend.
Exception in thread django-main-thread:
Traceback (most recent call last):
  File ""C:\Users\munis\Python36\lib\threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""C:\Users\munis\Python36\lib\threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Users\munis\Python36\lib\site-packages\django\utils\autoreload.py"", line 54, in wrapper
    fn(*args, **kwargs)
  File ""C:\Users\munis\Python36\lib\site-packages\django\core\management\commands\runserver.py"", line 117, in inner_run
    self.check(display_num_errors=True)
  File ""C:\Users\munis\Python36\lib\site-packages\django\core\management\base.py"", line 390, in check
    include_deployment_checks=include_deployment_checks,
  File ""C:\Users\munis\Python36\lib\site-packages\django\core\management\base.py"", line 377, in _run_checks
    return checks.run_checks(**kwargs)
  File ""C:\Users\munis\Python36\lib\site-packages\django\core\checks\registry.py"", line 72, in run_checks
    new_errors = check(app_configs=app_configs)
  File ""C:\Users\munis\Python36\lib\site-packages\django\core\checks\urls.py"", line 40, in check_url_namespaces_unique
    all_namespaces = _load_all_namespaces(resolver)
  File ""C:\Users\munis\Python36\lib\site-packages\django\core\checks\urls.py"", line 57, in _load_all_namespaces
    url_patterns = getattr(resolver, 'url_patterns', [])
  File ""C:\Users\munis\Python36\lib\site-packages\django\utils\functional.py"", line 80, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""C:\Users\munis\Python36\lib\site-packages\django\urls\resolvers.py"", line 579, in url_patterns
    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)
  File ""C:\Users\munis\Python36\lib\site-packages\django\utils\functional.py"", line 80, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""C:\Users\munis\Python36\lib\site-packages\django\urls\resolvers.py"", line 572, in urlconf_module
    return import_module(self.urlconf_name)
  File ""C:\Users\munis\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\Users\munis\Python36\miniproject\miniproject\urls.py"", line 3, in <module>
    from bankloan import views
  File ""C:\Users\munis\Python36\miniproject\bankloan\views.py"", line 5, in <module>
    from keras import backend as K
  File ""C:\Users\munis\Python36\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\munis\Python36\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\munis\Python36\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\munis\Python36\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\munis\Python36\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\munis\Python36\lib\site-packages\tensorflow\__init__.py"", line 99, in <module>
    from tensorflow_core import *
  File ""C:\Users\munis\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\munis\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\munis\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 10, in <module>
    from tensorflow.python.eager import context as _context
  File ""C:\Users\munis\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\eager\context.py"", line 29, in <module>
    from tensorflow.core.protobuf import config_pb2
  File ""C:\Users\munis\Python36\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\munis\Python36\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\munis\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named 'tensorflow_core.core'"
37848,Cannot import Tensorflow on Ubuntu VPS,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda install -c conda-forge tensorflow=1.10.0
- TensorFlow version: 1.10.0
- Python version: 3.5.4
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
When I try to import tensorflow on my vps, I get an undefined symbol error. I have tried reinstalling it, but that did not help. I am not sure what else to try or do. Does it maybe have something to do with a wheel?

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```$ python app.py
Traceback (most recent call last):
  File ""app.py"", line 7, in <module>
    from tensorflow import * # pylint: disable=unused-import
  File ""/home/me/anaconda3/envs/audiogen/lib/python3.5/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/me/anaconda3/envs/audiogen/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/me/anaconda3/envs/audiogen/lib/python3.5/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/home/me/anaconda3/envs/audiogen/lib/python3.5/site-packages/google/protobuf/descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: /home/iman/anaconda3/envs/audiogen/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so: undefined symbol: _ZNK6google8protobuf10TextFormat17FieldValuePrinter9PrintBoolEb```"
37847,TypeError: save() got an unexpected keyword argument 'save_format',"

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): I followed a tutorial of Adrian Rosebrock ( Pyimagesearch )
- OS Platform and Distribution : Windows 10 x64
- TensorFlow installed from (
binary): - TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- CUDA/cuDNN version: - GPU model and memory:9/7

**Standalone code to reproduce the issue** 
I'm trying to train a dataset of COVID-19 virus to detect the normal person and the no normal person ( he have not the virus) .
when I execute the script `train_covid19.py` that I have got from ( https://www.pyimagesearch.com/2020/03/16/detecting-covid-19-in-x-ray-images-with-keras-tensorflow-and-deep-learning/?fbclid=IwAR2P7bkAaIf9F_I1dk2xyLRpQB2mfAtc3ov3fpsls8B-wNEeCVPT3cizCWY), I got an error after I have completed the 25 epochs of training.
>[INFO] saving COVID-19 detector model...
Traceback (most recent call last):
  File ""train_covid19.py"", line 164, in <module>
    model.save(args[""model""], save_format=""h5"")
TypeError: save() got an unexpected keyword argument 'save_format'

I'm sure that the problem is a bug in the version of the tensorflow or some changes ( because Adrain is used Tensorflow 2.0 and the version that I used is 1.13.1)
So I need to know how can I change this line to make the complete code works?
Thanks :)

"
37846,How to solve this problem?(download_dependencies.sh)(c++ tensorflow),"downloading https://github.com/google/googletest/archive/release-1.8.0.tar.gz
downloading https://mirror.bazel.build/github.com/google/nsync/archive/0559ce013feac8db639ee1bf776aca0325d28777.tar.gz

gzip: stdin: unexpected end of file
tar: Child returned status 1
tar: Error is not recoverable: exiting now
"
37845,Calling tf.keras.model.save twice fails,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Debian 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: n/a
- TensorFlow installed from (source or
binary): binary
- TensorFlow version (use command below): `v2.1.0-rc2-17-ge5bf8de 2.1.0`
- Python version: 3.5.3
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from
source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

As demonstrated in this [gist](https://gist.github.com/zmjjmz/ade6ecb5eaf011788b1da62db4413cad), calling `tf.keras.Model.save` twice on a Model object fails on the second call.

**Describe the expected behavior**

It uh, would work.

**Standalone code to reproduce the issue** 

This [gist](https://gist.github.com/zmjjmz/ade6ecb5eaf011788b1da62db4413cad)

Related to (I'm fairly certain) [this issue](https://github.com/tensorflow/tensorflow/issues/33085)."
37844,Cannot convert model containing categorical_column_with_vocabulary_list op,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.2.0rc0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf
import os

model_dir = ""models/feature_column_example""
category = tf.constant([""A"", ""B"", ""A"", ""C"", ""C"", ""A""])
label = tf.constant([1, 0, 1, 0, 0, 0])

ds = tf.data.Dataset.from_tensor_slices(({""category"": category}, label))
ds = ds.batch(2)

fc_category = tf.feature_column.indicator_column(
    tf.feature_column.categorical_column_with_vocabulary_list(
        ""category"", vocabulary_list=[""A"", ""B"", ""C""]
    )
)
feature_layer = tf.keras.layers.DenseFeatures([fc_category])

model = tf.keras.Sequential(
    [
        feature_layer,
        tf.keras.layers.Dense(10, activation=""relu""),
        tf.keras.layers.Dense(1, activation=""sigmoid""),
    ]
)
model.compile(
    optimizer=""adam"", loss=""binary_crossentropy"", metrics=[""accuracy""]
)

model.fit(ds, epochs=2)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.allow_custom_ops = True
# converter.experimental_new_converter = True
# converter.experimental_new_quantizer = True

# Convert the model.
tflite_model = converter.convert()
open(os.path.join(model_dir, ""output.tflite""), ""wb"").write(tflite_model)

```

**The output from the converter invocation**

```
Cannot convert a Tensor of dtype resource to a NumPy array.
```

**Also, please include a link to the saved model or GraphDef**
```
saved_model_cli show --dir models/feature_column_example/ --tag_set serve --signature_def serving_default

The given SavedModel SignatureDef contains the following input(s):
  inputs['category'] tensor_info:
      dtype: DT_STRING
      shape: (-1, 1)
      name: serving_default_category:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['output_1'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 1)
      name: StatefulPartitionedCall_7:0
Method name is: tensorflow/serving/predict

```

**Failure details**
Cannot convert a Tensor of dtype `resource` to a NumPy array.

According to my analysis, this might be caused by some HashTable Ops, which create table handles. And my additional question is: whether tfliteconverter could convert model contains ops of initialize hashtableV2 and LookupTableImportV2? Thank you.


**Any other info / logs**: Full logs
```
Traceback (most recent call last):
  File ""feature_column_example.py"", line 62, in <module>
    export_keras_hashtable_model()
  File ""feature_column_example.py"", line 58, in export_keras_hashtable_model
    tflite_model = converter.convert()
  File ""/root/tf2.2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 464, in convert
    self._funcs[0], lower_control_flow=False))
  File ""/root/tf2.2/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 706, in convert_variables_to_constants_v2_as_graph
    func, lower_control_flow, aggressive_inlining)
  File ""/root/tf2.2/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 457, in _convert_variables_to_constants_v2_impl
    tensor_data = _get_tensor_data(func)
  File ""/root/tf2.2/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 217, in _get_tensor_data
    data = val_tensor.numpy()
  File ""/root/tf2.2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 961, in numpy
    maybe_arr = self._numpy()  # pylint: disable=protected-access
  File ""/root/tf2.2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 929, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.

```
"
37841,iOS example projects don't build with Swift file and bridging header,"Problem description
The iOS example apps don't build if you add a single Swift file and bridging header to the project. The error is ""error: 'vector' file not found"" in tensorflow/examples/ios/camera/ios_image_load.h:18:10.

Steps to reproduce
1. Download the TensorFlow repo and follow instructions to set up any/all of the iOS example projects.
2. Run an example app on your phone or simulator to verify that it works.
3. Add a single empty Swift file, File.swift, to the project and include a bridging header.
4. #import ""ios_image_load.h"" in the bridging header.
5. Build and observe build error 'vector' file not found.

**System information**
- MacOS Mojave
- Xcode 11.3.1
- TensorFlow installed from latest on GitHub

This is similar to https://github.com/tensorflow/tensorflow/issues/22051 but that issue was closed with a solution that does not apply to the iOS example apps."
37840,validation_split-Parameter in model.fit() leads to severe crashes due to enormous stacktrace,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Installed from Pypi (Python Packaging Index), Version 2.1.0
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version: 3.7.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: nvcc -V: release 10.1, V10.1.105 
- GPU model and memory: GTX 1060, 6 GB 

**Describe the current behavior**

I created the following model and I am using it in a jupyter notebook to learn some data:

```
model = Sequential()
model.add(LSTM(100, input_shape=input_shape, return_sequences=True))
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(number_of_classes, activation='softmax'))

model.compile(optimizer=Adam(lr=0.001),
              loss='categorical_crossentropy', 
              metrics=['acc'])

history = model.fit(x=[MODEL_DATA['DATA_TRAIN_PADDED_NORMALIZED']],
                    y=[MODEL_DATA['DATA_LABELS_OHC']],
                    validation_split=0.2,
                    epochs=500,
                    verbose=1)
```

When I imported all layers from the `keras`-package, the code works successfully on the CPU. Yet I would like to use the GPU-accelerated LSTMs (formerly known as CuDNNLSTM, but they are merged in TF 2.1 into LSTM).

To train my model, I am using a jupyter notebook. Sadly, once I changed my imports from the old `keras`-packages to `tensorflow.keras` and thus to the GPU-accelerated LSTMs, I suffered from sudden major crashes. And by ""major"" I mean a crashing jupyter-server that crashes my Firefox as well. When using pycharm with the jupyter notebook plugin, pycharm crashes too.

So I wondered why I had those massive problems across different processes on my machines that occured when I executed tensorflow and my notebook.

As it turns out, I could trace the problem to the following line [0]. It appears that as part of raising the ValueError, `x` is returned in its constructor in the `str.format()`-call. Here, `x` is my complete training data set. This resulted in a dump of 262 MB of text into stderr as part of the stacktrace that then lead to the consequences (crashing jupyter server, crashing browser via websocket-connection to localhost). I found that out by piping the stream to a file, as the exception's message has a len() of 275168064 characters!

Next, I wondered if the bug is still present in the newest version of tensorflow. I could see in git that the corresponding lines of code do not exist anymore when installing `tf-nightly` (version: v1.12.1-27849-g9278bbfc24 2.2.0-dev20200323 ) as the whole file, `training_v2.py`, does not exist anymore. Subsequently, I re-ran my code with the current tf-nightly from PyPI. First, I got this error [1], as I did not provide a numpy array.

After changing my model.fit() to the following:
```
history = model.fit(x=np.array([MODEL_DATA['DATA_TRAIN_PADDED_NORMALIZED']]),
                    y=np.array([MODEL_DATA['DATA_LABELS_OHC']]),
                    validation_split=0.2,
                    epochs=500,
                    verbose=1)
```

... I ran into the following error [2]:

```
Traceback (most recent call last):
  File ""C:\Users\Joo\AppData\Local\Programs\Python\Python37\lib\contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""C:\[...]\venv\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2805, in variable_creator_scope
    yield
  File ""C:\[...]\venv\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 854, in fit
    epoch_logs = copy.copy(logs)
UnboundLocalError: local variable 'logs' referenced before assignment
```
It appears, that `logs` does not exist here [2].

Not using `validation_split` as part of model.fit() solved all my problems for tf 2.1.0, but I had to find it out the hard way. 🙁  Moreover, I could not resolve the problem for tf-nightly at all, even when omitting the parameter as other exceptions popped up.

Thanks in advance for checking the code and improving tensorflow.

[0] https://github.com/tensorflow/tensorflow/blob/f270180a6caa8693f2b2888ac7e6b8e69c4feaa8/tensorflow/python/keras/engine/training_v2.py#L539

[1] https://github.com/tensorflow/tensorflow/blob/e7bbb424808eb7ebbeb959b993496adafd024609/tensorflow/python/keras/engine/data_adapter.py#L1386

[2] https://github.com/tensorflow/tensorflow/blob/c45d13f92b921dae044a5639f8b1bc560fe9b71c/tensorflow/python/keras/engine/training.py#L857

**Describe the expected behavior**

I would expect an exception that does not return over 260 MB of text as part of the stack trace. The important part of the stack trace (in the beginning of the message) is also not shown (i. e., that validation_split cannot be used), as the 260 MB of `x` overfill the non-infinite buffer of my IDE in a split second, thus omitting the whole stack trace with my training data."
37839,ModelCheckpoint callback can't save entire subclassed models,"The keras callback `ModelCheckpoint` won't save a subclassed model in SavedModel format. It only ever saves the checkpoints.

```python
import tensorflow as tf
import numpy as np

class TestModel(tf.keras.Model):

    def __init__(self):
        super().__init__()
        self.dense1 = tf.keras.layers.Dense(64, activation='relu')
        self.dense2 = tf.keras.layers.Dense(64, activation='relu')
        self.dense3 = tf.keras.layers.Dense(10)

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        return self.dense3(x)


model = TestModel()
model.compile(optimizer=tf.keras.optimizers.Adam(0.01),
              loss='mse',      
              metrics=['mae'])  

data = np.random.random((1000, 32))
labels = np.random.random((1000, 10))

model.fit(
    data, labels, epochs=10, batch_size=32,
    callbacks=[tf.keras.callbacks.ModelCheckpoint('model_{epoch:02d}.ckpt',
                                                  save_weights_only=False)])
```

It seems as if the callback forces weights_only saving [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L1148). While this makes sense if the model is in `.h5` format, it doesn't make sense for `SavedModel` format."
42795,[ko] Remove references to tf-nightly-2.0-preview package,"The `tf-nightly-2.0-preview` package does not exist anymore. Please upgrade all notebooks to use `%tensorflow_version 2.x` like:

```
try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass
```

Affects:

* [/site/ko/tutorials/keras/text_classification.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/text_classification.ipynb)
* [/site/ko/tutorials/keras/overfit_and_underfit.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/overfit_and_underfit.ipynb)
* [/site/ko/tutorials/structured_data/feature_columns.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/structured_data/feature_columns.ipynb)
"
42794,[zh-cn] Remove references to tf-nightly-2.0-preview package,"The `tf-nightly-2.0-preview` package does not exist anymore. Please upgrade all notebooks to use `%tensorflow_version 2.x` like:

```
try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass
```

Affects:

* [/site/zh-cn/lite/convert/python_api.md](https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/lite/convert/python_api.md)
* [/site/zh-cn/tutorials/estimator/boosted_trees.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/estimator/boosted_trees.ipynb)
"
42793,[ja] Remove references to tf-nightly-2.0-preview package,"The `tf-nightly-2.0-preview` package does not exist anymore. Please upgrade all notebooks to use `%tensorflow_version 2.x` like:

```
try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass
```

Affects:

* [/site/ja/lite/convert/python_api.md](https://github.com/tensorflow/docs-l10n/blob/master/site/ja/lite/convert/python_api.md)
* [/site/ja/guide/function.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/ja/guide/function.ipynb)
* [/site/ja/tutorials/load_data/csv.ipynb](https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/load_data/csv.ipynb)
"
37838,Can't restore weights to fine-tune SSD model,"I am trying to fine-tune the model `ssd_mobilenet_coco_v1` but, when I execute the training script I do not obtain **restoring checkpoints**. Instead, I obtain **saving checkpoints** (as described below), so I think I am not using the pre-trained model correctly.
**My question is: am I loading the pre-trained weights or the weights are being randomly initialized?**
I provide the obtained output, the expected output, and my `pipeline.config` file.
Thanks in advance.

**System information** 
- OS Platform and Distribution: Ubuntu 18.04 
- TensorFlow installed from binary - 1.15
- Python version: 3 
- CUDA/cuDNN version: 10.0/7.6.5
- GPU model and memory: 12GB and Tesla P100-PCIE-16GB

**Describe the current behavior**
```
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /content/models/research/object_detection/model_main.py:109: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

WARNING:tensorflow:From /content/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

W0323 17:45:14.716734 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:628: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.

W0323 17:45:14.720826 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:628: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.

WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
W0323 17:45:14.720992 140406623115136 model_lib.py:629] Forced number of epochs for all eval validations to be 1.
WARNING:tensorflow:From /content/models/research/object_detection/utils/config_util.py:488: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

W0323 17:45:14.721235 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/utils/config_util.py:488: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

INFO:tensorflow:Maybe overwriting train_steps: 50000
I0323 17:45:14.721395 140406623115136 config_util.py:488] Maybe overwriting train_steps: 50000
INFO:tensorflow:Maybe overwriting use_bfloat16: False
I0323 17:45:14.721562 140406623115136 config_util.py:488] Maybe overwriting use_bfloat16: False
INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1
I0323 17:45:14.721702 140406623115136 config_util.py:488] Maybe overwriting sample_1_of_n_eval_examples: 1
INFO:tensorflow:Maybe overwriting eval_num_epochs: 1
I0323 17:45:14.721852 140406623115136 config_util.py:488] Maybe overwriting eval_num_epochs: 1
INFO:tensorflow:Maybe overwriting load_pretrained: True
I0323 17:45:14.722014 140406623115136 config_util.py:488] Maybe overwriting load_pretrained: True
INFO:tensorflow:Ignoring config override key: load_pretrained
I0323 17:45:14.722141 140406623115136 config_util.py:498] Ignoring config override key: load_pretrained
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
W0323 17:45:14.722986 140406623115136 model_lib.py:645] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False
I0323 17:45:14.723167 140406623115136 model_lib.py:680] create_estimator_and_inputs: use_tpu False, export_to_tpu False
INFO:tensorflow:Using config: {'_model_dir': '/content/data_trunks/train_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb28ee6dd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0323 17:45:14.723658 140406623115136 estimator.py:212] Using config: {'_model_dir': '/content/data_trunks/train_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb28ee6dd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7fb28ee132f0>) includes params argument, but params are not passed to Estimator.
W0323 17:45:14.723914 140406623115136 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7fb28ee132f0>) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:Not using Distribute Coordinator.
I0323 17:45:14.724645 140406623115136 estimator_training.py:186] Not using Distribute Coordinator.
INFO:tensorflow:Running training and evaluation locally (non-distributed).
I0323 17:45:14.724889 140406623115136 training.py:612] Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.
I0323 17:45:14.725183 140406623115136 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.
WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0323 17:45:14.731667 140406623115136 deprecation.py:323] From /tensorflow-1.15.0/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.

W0323 17:45:14.744504 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.

WARNING:tensorflow:From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.

W0323 17:45:14.744798 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.

WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.

W0323 17:45:14.759091 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.

WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W0323 17:45:14.759980 140406623115136 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
W0323 17:45:14.767862 140406623115136 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0323 17:45:14.768066 140406623115136 deprecation.py:323] From /tensorflow-1.15.0/python3.6/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W0323 17:45:14.794663 140406623115136 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.

W0323 17:45:16.303668 140406623115136 module_wrapper.py:139] From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.

WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.

W0323 17:45:24.681373 140406623115136 module_wrapper.py:139] From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.

WARNING:tensorflow:From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0323 17:45:24.773309 140406623115136 deprecation.py:323] From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

W0323 17:45:27.258008 140406623115136 module_wrapper.py:139] From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/operators/control_flow.py:1004: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0323 17:45:31.055150 140406623115136 api.py:332] From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/operators/control_flow.py:1004: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

W0323 17:45:35.093766 140406623115136 module_wrapper.py:139] From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

W0323 17:45:35.095073 140406623115136 module_wrapper.py:139] From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:166: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0323 17:45:35.583931 140406623115136 deprecation.py:323] From /content/models/research/object_detection/inputs.py:166: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.

W0323 17:45:37.435810 140406623115136 module_wrapper.py:139] From /tensorflow-1.15.0/python3.6/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.

WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
W0323 17:45:38.169332 140406623115136 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
INFO:tensorflow:Calling model_fn.
I0323 17:45:38.185292 140406623115136 estimator.py:1148] Calling model_fn.
WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0323 17:45:38.194174 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0323 17:45:38.196500 140406623115136 deprecation.py:323] From /tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /content/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.

W0323 17:45:40.323006 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.

INFO:tensorflow:depth of additional conv before box predictor: 0
I0323 17:45:40.337304 140406623115136 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0323 17:45:40.377328 140406623115136 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0323 17:45:40.414876 140406623115136 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0323 17:45:40.454860 140406623115136 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0323 17:45:40.493021 140406623115136 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0323 17:45:40.533962 140406623115136 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
WARNING:tensorflow:From /content/models/research/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W0323 17:45:40.580100 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /content/models/research/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.

W0323 17:45:40.581026 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.

WARNING:tensorflow:From /content/models/research/object_detection/utils/variables_helper.py:142: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

W0323 17:45:40.583110 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/utils/variables_helper.py:142: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:353: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.

W0323 17:45:40.585104 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:353: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.

WARNING:tensorflow:From /content/models/research/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.

W0323 17:45:41.311894 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1163: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0323 17:45:44.619992 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1163: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.

W0323 17:45:44.627572 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.

WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.

W0323 17:45:44.629132 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.

WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1275: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

W0323 17:45:44.672892 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1275: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From /content/models/research/object_detection/builders/graph_rewriter_builder.py:36: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0323 17:45:44.675707 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/builders/graph_rewriter_builder.py:36: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/add_fold
I0323 17:45:50.407618 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/add_fold
I0323 17:45:50.408249 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/add_fold
I0323 17:45:50.408696 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/add_fold
I0323 17:45:50.409127 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/add_fold
I0323 17:45:50.409549 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/add_fold
I0323 17:45:50.410022 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/add_fold
I0323 17:45:50.410556 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/add_fold
I0323 17:45:50.411051 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/add_fold
I0323 17:45:50.411566 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/add_fold
I0323 17:45:50.412048 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/add_fold
I0323 17:45:50.412456 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/add_fold
I0323 17:45:50.412898 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/add_fold
I0323 17:45:50.413309 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/add_fold
I0323 17:45:50.413769 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/add_fold
I0323 17:45:50.414207 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/add_fold
I0323 17:45:50.414690 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/add_fold
I0323 17:45:50.415128 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/add_fold
I0323 17:45:50.415588 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/add_fold
I0323 17:45:50.415981 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/add_fold
I0323 17:45:50.416409 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/add_fold
I0323 17:45:50.416898 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/add_fold
I0323 17:45:50.417309 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/add_fold
I0323 17:45:50.417730 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/add_fold
I0323 17:45:50.418143 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/add_fold
I0323 17:45:50.418587 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/add_fold
I0323 17:45:50.419028 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/add_fold
I0323 17:45:50.419415 140406623115136 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/add_fold
WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:380: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

W0323 17:45:50.432834 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:380: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

WARNING:tensorflow:From /content/models/research/object_detection/builders/optimizer_builder.py:58: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

W0323 17:45:50.448522 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/builders/optimizer_builder.py:58: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:398: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

W0323 17:45:50.448780 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:398: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:515: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

W0323 17:45:57.322220 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:515: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:519: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

W0323 17:45:57.992308 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:519: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:520: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.

W0323 17:45:57.992649 140406623115136 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:520: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.

INFO:tensorflow:Done calling model_fn.
I0323 17:45:57.993029 140406623115136 estimator.py:1150] Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
I0323 17:45:57.994582 140406623115136 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
I0323 17:46:01.981108 140406623115136 monitored_session.py:240] Graph was finalized.
2020-03-23 17:46:01.986871: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-03-23 17:46:01.987126: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x172ef100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-03-23 17:46:01.987164: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-03-23 17:46:01.989628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-03-23 17:46:02.107870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-23 17:46:02.108885: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x172eef40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-03-23 17:46:02.108919: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-03-23 17:46:02.109195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-23 17:46:02.110107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2020-03-23 17:46:02.110456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-23 17:46:02.112221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-23 17:46:02.115141: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-03-23 17:46:02.115636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-03-23 17:46:02.117511: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-03-23 17:46:02.118515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-03-23 17:46:02.122292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-23 17:46:02.122478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-23 17:46:02.123576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-23 17:46:02.124399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-03-23 17:46:02.124482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-23 17:46:02.126257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-23 17:46:02.126306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-03-23 17:46:02.126323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-03-23 17:46:02.126518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-23 17:46:02.127473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-23 17:46:02.128351: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2020-03-23 17:46:02.128408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:Running local_init_op.
I0323 17:46:05.840927 140406623115136 session_manager.py:500] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I0323 17:46:06.250185 140406623115136 session_manager.py:502] Done running local_init_op.
INFO:tensorflow:
Saving checkpoints for 0 into /content/data_trunks/train_dir/model.ckpt
```

**Describe the expected behavior**
I was expecting to see something like
```
(...)
I0323 17:46:17.423462 140406623115136 basic_session_run_hooks.py:606] Restoring checkpoints for 0 into /content/data_trunks/train_dir/model.ckpt.
```

**Standalone code to reproduce the issue** 
```
python model_main.py \
    --pipeline_config_path=models/ssd_mobilenet_v1_coco/pipeline.config \
    --model_dir=train_dir \
    --alsologtostderr \
    --num_train_steps=50000 \
    --num_eval_steps=500
```

**Other info / logs**

My `pipeline.config` file:
```
model {
  ssd {
    num_classes: 1
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    feature_extractor {
      type: ""ssd_mobilenet_v1""
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.99999989895e-05
          }
        }
        initializer {
          random_normal_initializer {
            mean: 0.0
            stddev: 0.00999999977648
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.97000002861
          center: true
          scale: true
          epsilon: 0.0010000000475
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.99999989895e-05
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0
              stddev: 0.00999999977648
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.97000002861
            center: true
            scale: true
            epsilon: 0.0010000000475
          }
        }
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.800000011921
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        class_prediction_bias_init: -4.59999990463
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.20000000298
        max_scale: 0.949999988079
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.333299994469
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 0.300000011921
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 2.0
          alpha: 0.75
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    encode_background_as_zeros: true
    normalize_loc_loss_by_codesize: true
    inplace_batchnorm_update: true
    freeze_batchnorm: false
  }
}
train_config {
  batch_size: 18
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
  sync_replicas: true
  optimizer {
    momentum_optimizer {
      learning_rate {
        cosine_decay_learning_rate {
          learning_rate_base: 0.20000000298
          total_steps: 50000
          warmup_learning_rate: 0.0599999986589
          warmup_steps: 2000
        }
      }
      momentum_optimizer_value: 0.899999976158
    }
    use_moving_average: false
  }
  fine_tune_checkpoint: ""/content/data_trunks/ssd_mobilenet_v1_coco/model.ckpt""
  fine_tune_checkpoint_type: ""detection""
  num_steps: 50000
  startup_delay_steps: 0.0
  replicas_to_aggregate: 8
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}
train_input_reader {
  label_map_path: ""/content/data_trunks/label_map.pbtxt""
  tf_record_input_reader {
    input_path: ""/content/data_trunks/train_8869_v4.record""
  }
}
eval_config {
  num_examples: 640
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""/content/data_trunks/label_map.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""/content/data_trunks/eval_640_v4.record""
  }
}
graph_rewriter {
  quantization {
    delay: 48000
    weight_bits: 8
    activation_bits: 8
  }
}
```"
37835,Allow different prefixes for TF_SYSTEM_LIBS libraries,"**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Using TF_SYSTEM_LIBS to compile TensorFlow requires to have the libraries installed into a common prefix. It is however not uncommon to have different prefixes for different software (think: /opt/foo-1.x.y, /opt/bar-w.e.r...)

However TF currently only supports a single prefix via `PREFIX, INCLUDEDIR and LIBDIR`.

So what would be great is having one environment variable per dependency, e.g. `TF_PROTOBUF_PREFIX` to set this to the folder containing the `include`, `bin` etc folders.

Alternatively: As all the stuff (headers, binaries) must already be able to be found: Why not use the environment variables PATH, CPATH, (LD_)LIBRARY_PATH? Or a similar (maybe new) variable supporting multiple paths?

**Will this change the current api? How?**

No

**Who will benefit with this feature?**

Package maintainers"
37834,keras conv layer weight cannot be updated during distribution training mode,"OS: Ubuntu 18.04
Tensorflow: 2.1.0
Python: 3.7
GPUs: 4 gpus

trying to train a gan network on multiple GPUs. The network has a spectral normalization layer to update convolution layer weights. The sample code and error are provided as follows:

```python
import tensorflow as tf
strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    conv = tf.keras.layers.Conv2D(5, 3, 1)
    x = tf.random.normal((1,5, 5, 3))
    conv(x)

@tf.function
def update_w(w, nw):
    w.assign(nw)

x2 = tf.random.normal((3,3, 3, 5))
strategy.experimental_run_v2(update_w, args=(conv.kernel, x2))
```
![image](https://user-images.githubusercontent.com/731496/77329724-5a234800-6d59-11ea-9ee4-a3b976178147.png)
"
37833,session->Run,"```
/home/sy/tf_test/mnist/tf.cpp: In function ‘int main(int, char**)’:
/home/sy/tf_test/mnist/tf.cpp:219:142: error: no matching function for call to ‘tensorflow::Session::Run(<brace-enclosed initializer list>, <brace-enclosed initializer list>, <brace-enclosed initializer list>, std::vector<tensorflow::Tensor>*)’
 n = session->Run({{inputt, inputm}}, {""code2str_conversion/chars_conversion/cond/Merge:0"",""init_all_tables""}, {}, &outputs);
                                                                                                                           ^
In file included from /home/sy/tf_test/mnist/tf.cpp:25:0:
/home/sy/tensorflow/tensorflow/core/public/session.h:121:18: note: candidate: virtual tensorflow::Status tensorflow::Session::Run(const std::vector<std::pair<std::__cxx11::basic_string<char>, tensorflow::Tensor> >&, const std::vector<std::__cxx11::basic_string<char> >&, const std::vector<std::__cxx11::basic_string<char> >&, std::vector<tensorflow::Tensor>*)
   virtual Status Run(const std::vector<std::pair<string, Tensor> >& inputs,
                  ^~~
/home/sy/tensorflow/tensorflow/core/public/session.h:121:18: note:   no known conversion for argument 1 from ‘<brace-enclosed initializer list>’ to ‘const std::vector<std::pair<std::__cxx11::basic_string<char>, tensorflow::Tensor> >&’
/home/sy/tensorflow/tensorflow/core/public/session.h:150:18: note: candidate: virtual tensorflow::Status tensorflow::Session::Run(const tensorflow::RunOptions&, const std::vector<std::pair<std::__cxx11::basic_string<char>, tensorflow::Tensor> >&, const std::vector<std::__cxx11::basic_string<char> >&, const std::vector<std::__cxx11::basic_string<char> >&, std::vector<tensorflow::Tensor>*, tensorflow::RunMetadata*)
   virtual Status Run(const RunOptions& run_options,
                  ^~~
/home/sy/tensorflow/tensorflow/core/public/session.h:150:18: note:   candidate expects 6 arguments, 4 provided
CMakeFiles/tf_test.dir/build.make:62: recipe for target 'CMakeFiles/tf_test.dir/tf.cpp.o' failed
make[2]: *** [CMakeFiles/tf_test.dir/tf.cpp.o] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/tf_test.dir/all' failed
make[1]: *** [CMakeFiles/tf_test.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2
```


when I run a model of EAST,the program run successfully。
But,when I run the second model, the program fails.(python ok )
Can the C ++ API initialize the model?

（ubuntu18.04,tensorflow c++ 1.13）"
37830,Replace VarHandleOp in Keras with VariableV2,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue: https://www.tensorflow.org/api_docs/python/tf/Variable

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing): 

### Clear description

TF 1.13.1
I can't find anything detail about differences between VarHandlOp and VariableV2. 
As far as I know it seems that VarHandleOp implement in Keras and VariableV2 in TF 1.X. 
How can I convert VarHandleOp to VariableV2 in tf.keras?
I'm using some model processing tool that can only run under VariableV2 ops.

### Correct links

https://git.codingcafe.org/Mirrors/tensorflow/tensorflow/commit/e4a5c5356063d7f7b324a5771fe296bb199b532c
Somelink above is all I could find.

### Parameters defined

---

### Returns defined

---

### Raises listed and defined

---

### Usage example

Is this a usage example?
https://www.tensorflow.org/api_docs/python/tf/raw_ops/VariableV2?hl=pl
Is that a kind of example?

### Request visuals, if applicable

---

### Submit a pull request?
---
"
42792,[ja] Automatic proofreading on GitHub Actions,"We, Japanese community members, are using a [proofreading tool](https://github.com/tfug/proofreading).

We want to apply the proofreading tool on CI and make Japanese contributors to pass it.
Thus, our questions are below:

**1. Could we create `docs-l10n/.github/workflows/ja.yml`?**

We want to define a GitHub Actions's workflow for Japanese documents proofreading.
The setting below will not affect non-japanese documents:

```yaml
on:
  push:
    paths:
      - 'site/ja/**'
```

**2. Could we use `docs-l10n/tools` directory to develop proofreading tool?**

I guess it's natural that the proofreading tool is in `docs-l10n/tools`.

If it's ""no"", however, we are planning to checkout [tfug/proofreading](https://github.com/tfug/proofreading) in GitHub Actions and apply it.
Actually, [current workflow](https://github.com/tensorflow/docs-l10n/blob/6cd120c79237361422f83df4f22c2f7760f5fa87/.github/workflows/ci.yaml) for tensorflow/docs-l10n checkout [tensorflow/docs](https://github.com/tensorflow/docs) repository and use `docs/tools/nbfmt.py`.

"
37829,Input tensor has different type from described in the comments.,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started.md#validate-input-shape

## Description of issue (what needs changing):

### Clear description

At following lines of the second code of [this section](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started.md#validate-input-shape), it is claimed that input is a 2D tensor.

```c++
// The property ""dims"" tells us the tensor's shape. It has one element for
// each dimension. Our input is a 2D tensor containing 1 element, so ""dims""
// should have size 2.
TF_LITE_MICRO_EXPECT_EQ(2, input->dims->size);
```

However, based on [the notebook where the model defined](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/create_sine_model.ipynb), it should have 1D tensor for input.

### Submit a pull request?

No."
37827,tf.metrics.mean_cosine_distance fails during distributed evaluation,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): `No`
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): `Linux Debian GNU/Linux 9`
- TensorFlow installed from (source or
binary): `binary`
- TensorFlow version (use command below): `v1.15.2-1-g61ff2cb 1.15.2`
- Python version: `3.7.6`
- CUDA/cuDNN version: `CUDA 10, cuDNN 7.6.5`
- GPU model and memory: `2x Tesla K80`

**Describe the current behavior**

`tf.metrics.mean_cosine_distance` fails at the end of distributed evaluation with `MirroredStrategy`:

```
TypeError: Fetch argument PerReplica:{
  0 /replica:0/task:0/device:GPU:0: <tf.Tensor 'Sub:0' shape=() dtype=float32>,
  1 /replica:0/task:0/device:GPU:1: <tf.Tensor 'replica_1/Sub:0' shape=() dtype=float32>
} has invalid type <class 'tensorflow.python.distribute.values.PerReplica'>, must be a string or Tensor. (Can not convert a PerReplica into a Tensor or Operation.)
```

Non-distributed evaluation (that is, with `RunConfig.eval_distribute=None` or with a single GPU only) finishes without errors.

**Standalone code to reproduce the issue** 
```python
import numpy as np
import tensorflow as tf

def model_fn(features, labels, mode):
    predictions = tf.layers.dense(features, 2)
    metrics = {'cos': tf.metrics.mean_cosine_distance(labels, predictions, 1)}
    return tf.estimator.EstimatorSpec(
        mode=mode,
        predictions=predictions,
        loss=tf.constant(0.1),
        train_op=None,
        eval_metric_ops=metrics)


def input_fn():
    dataset = tf.data.Dataset.from_tensor_slices(
        (np.array([[1., 1.]]), np.array([[2., 2.]])))
    dataset = dataset.repeat()
    dataset = dataset.batch(1, drop_remainder=True)
    return dataset


if __name__ == '__main__':
    gpus = tf.config.experimental.list_physical_devices('GPU')
    assert len(gpus) > 1, 'Need >1 GPUs to run'
    strategy = tf.distribute.MirroredStrategy()
    run_config = tf.estimator.RunConfig(train_distribute=strategy,
                                        eval_distribute=strategy)

    estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)
    print(estimator.evaluate(input_fn, steps=5))
```

**Other info / logs**: 
[logs_1_15.txt](https://github.com/tensorflow/tensorflow/files/4368761/logs_1_15.txt)
"
37826,Op type not registered 'HashTableV2' in binary running on 5f00e250dfa5,"when I load a model of CRNN,I find this peoblem.
ERROR: Creating graph in session failed...Not found: Op type not registered 'HashTableV2' in binary running on 5f00e250dfa5. Make sure the Op and Kernel are registered in the binary running in this process.


I use this code:
` 
#include <fstream>
#include <utility>
#include <vector>
#include <Eigen/Core>
#include <Eigen/Dense>

#include ""tensorflow/core/framework/graph.pb.h""
#include ""tensorflow/core/framework/tensor.h""
#include ""tensorflow/core/graph/default_device.h""
#include ""tensorflow/core/graph/graph_def_builder.h""
#include ""tensorflow/core/lib/core/errors.h""
#include ""tensorflow/core/lib/core/stringpiece.h""
#include ""tensorflow/core/lib/core/threadpool.h""
#include ""tensorflow/core/lib/io/path.h""
#include ""tensorflow/core/lib/strings/stringprintf.h""
#include ""tensorflow/core/platform/env.h""
#include ""tensorflow/core/platform/init_main.h""
#include ""tensorflow/core/platform/logging.h""
#include ""tensorflow/core/platform/types.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/util/command_line_flags.h""

#include <iostream>

using namespace std;
using namespace tensorflow;

int main()
{
    Session* session;
    Status status = NewSession(SessionOptions(), &session);

    if (!status.ok()) {
        cout << status.ToString() << ""\n"";
        return 1;
    }
    cout << ""Session successfully created.\n"";
    string model_path=""/tensorflow_cc/mnist/crnn_frozen_model.pb"";
    GraphDef graphdef; //Graph Definition for current model


    Status status_load = ReadBinaryProto(Env::Default(), model_path, &graphdef); / 
    if (!status_load.ok()) {
        std::cout << ""ERROR: Loading model failed..."" << model_path << std::endl;
        std::cout << status_load.ToString() << ""\n"";
        return -1;
    }
    Status status_create = session->Create(graphdef);  
    if (!status_create.ok()) {
        std::cout << ""ERROR: Creating graph in session failed..."" << status_create.ToString() << std::endl;
        return -1;
    }
    cout << ""Session successfully created.""<< endl;
}
`"
37825,Issue with installation of Tensorflow in Windows10,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution:Windows
- TensorFlow installed from (source or binary): https://www.tensorflow.org/install
- TensorFlow version:2.0
- Python version:3.6
- Installed using virtualenv? pip? conda?: Created virtualenv using conda. used pip to install tensorflow
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory: GTX 1650

**Describe the problem**
Tensorflow is getting installed in virtual environment but cannot import it.When I try to import, I get the following error

Traceback (most recent call last):
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors

TensorFlowTensorF
*Steps followed**
1. Uninstall all NVIDIA Drivers/Software and delete all NVIDIA files from program files (x86 aswell)

2. Install Visual Studio 2017 (from link above)

3. Install CUDA (first check compatible versions using link above)

4. Download cuDNN (again check version is same as CUDA version)

5. Extract the cuDNN zip folder to your desktop. Open a new windows explorer and navigate to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\Vx.x\ . Now copy the contents of the downloaded and extracted cuDNN folder into the appropriate folders (files from cuDNN bin go into the new windows explorer bin folder etc.).

6. Navigate to your system enviorment variables and edit the path. Add the following two directories into your path:
-  C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin
- C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp

7. Download and Install Anaconda (from link above)

8. Set up a virtual enviorment using python 3.5
- conda create -n [name] python=3.5

9. Activate the virtual enviorment
- activate [name]

10. Install packages
- pip install --ignore-installed --upgrade tensorflow-gpu
- pip install keras

**Any other info / logs**
Traceback (most recent call last):
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\indra\anaconda3\envs\imgreco\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors
"
37824,out of memory issue in running face-api.js,"hii 
while running face-api.js node examples (node faceRecognition.js) am getting the OOM error while running 2 hour video face recognition. how can solve this issue.
 
2020-03-20 16:12:39.728434: W tensorflow/core/framework/op_kernel.cc:1651] OP_RE
QUIRES failed at cwise_ops_common.cc:82 : Resource exhausted: OOM when allocatin
g tensor with shape[1,32,32,512] and type float on /job:localhost/replica:0/task
:0/device:CPU:0 by allocator cpu
Exception code=0xc0000005 flags=0x0 at 0x00007FF957AEA277. Access violation - at
tempting to read data at address 0x0000000000000010"
37823,How to solve this problem？（c++ tensorflow+opencv ）,"/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4350:30: note: suggested alternative: ‘GOOGLE_DLOG’
   operation_timeout_in_ms_ = GOOGLE_LONGLONG(0);
                              ^~~~~~~~~~~~~~~
                              GOOGLE_DLOG
/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h: In member function ‘void tensorflow::RunOptions_Experimental::clear_collective_graph_key()’:
/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4571:27: error: ‘GOOGLE_LONGLONG’ was not declared in this scope
   collective_graph_key_ = GOOGLE_LONGLONG(0);
                           ^~~~~~~~~~~~~~~
/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4571:27: note: suggested alternative: ‘GOOGLE_DLOG’
   collective_graph_key_ = GOOGLE_LONGLONG(0);
                           ^~~~~~~~~~~~~~~
                           GOOGLE_DLOG
/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h: In member function ‘void tensorflow::RunOptions::clear_timeout_in_ms()’:
/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4617:20: error: ‘GOOGLE_LONGLONG’ was not declared in this scope
   timeout_in_ms_ = GOOGLE_LONGLONG(0);
                    ^~~~~~~~~~~~~~~
/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4617:20: note: suggested alternative: ‘GOOGLE_DLOG’
   timeout_in_ms_ = GOOGLE_LONGLONG(0);
                    ^~~~~~~~~~~~~~~
                    GOOGLE_DLOG
"
37822,"tf.fill(), This function does not handle the case of the path where all inputs are not already EagerTensors","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 2.20rc0
- Python version: 3.7.6
- CUDA/cuDNN version: - GPU model and memory:CPU only

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
# I run this in eager mode
# input: [batch_size, features]
def get_indices(self, input, batch_size, max_len):
	num_phones = input.get_shape().as_list()[1]

	out = []
	for i in range(batch_size):
		cur_len = tf.reduce_sum(input[i])
		indices = tf.concat([tf.fill([input[i][idx]], idx)
							 for idx in range(num_phones)], axis=-1)
							 
	return out
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3307, in fill
    dims, value)
tensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/mnt/c/SourceCode/tacotron-tf2/train.py"", line 168, in <module>
    main()
  File ""/mnt/c/SourceCode/tacotron-tf2/train.py"", line 164, in main
    train(log_dir, args)
  File ""/mnt/c/SourceCode/tacotron-tf2/train.py"", line 98, in train
    mel_targets, linear_targets)
  File ""/mnt/c/SourceCode/tacotron-tf2/train.py"", line 30, in train_step
    mel_outputs, linear_outputs = model((sequences, durations), training=True)
  File ""/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 967, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/mnt/c/SourceCode/tacotron-tf2/models/tacotron.py"", line 567, in call
    training=training)
  File ""/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 967, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/mnt/c/SourceCode/tacotron-tf2/models/tacotron.py"", line 229, in call
    indices = self.get_indices(durations, batch_size, max_len)
  File ""/mnt/c/SourceCode/tacotron-tf2/models/tacotron.py"", line 245, in get_indices
    for idx in range(num_phones)], axis=-1)
  File ""/mnt/c/SourceCode/tacotron-tf2/models/tacotron.py"", line 245, in <listcomp>
    for idx in range(num_phones)], axis=-1)
  File ""/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 234, in fill
    result = gen_array_ops.fill(dims, value, name=name)
  File ""/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3312, in fill
    dims, value, name=name, ctx=_ctx)
  File ""/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3339, in fill_eager_fallback
    ctx=ctx, name=name)
  File ""/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension -14 must be >= 0 [Op:Fill]
```
Process finished with exit code 1

"
37821,Error converting Mobilenet model using TFLiteConverter,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source): 2.1.1


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
 model._set_inputs(x)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
```

**Failure details**

When I convert a model using TFLiteConverter with the settings above, it seems to create a uint8 node. However, when I try to run the converted model on my microcontroller using the Tensorflow Lite Micro the input node of the graph still seems to be the float version. Is there any way I can access the graph being created by Tensorflow Lite Converter or possibly set the input to the uint8 node?

"
37818,[Feature Request][Keras] Allow loss function using multiple tensors as input,"Some loss functions not only depend on `y_pred` and `y_true` but also depend on other tensors, such as a CTC loss (related to 
* https://github.com/ysoullard/CTCModel/blob/d9e87fb24a6f7446457de5c89004404a900ce00b/CTCModel.py#L108
* https://github.com/robmsmt/KerasDeepSpeech/blob/553638821c996cb6049e34185babacdb8f00c215/model.py#L145
* https://github.com/ypwhs/captcha_break/blob/master/ctc_2019.ipynb

), 

Yolo3 loss (related to 
* https://github.com/qqwweee/keras-yolo3/blob/e6598d13c703029b2686bc2eb8d5c09badf42992/train.py#L131
* https://github.com/experiencor/keras-yolo3/blob/768c524f277adbfd26c2f44d73cb1826bbaf2d10/yolo.py#L361

)

 and CRF loss function (relate to 

* tensorflow/addons#1363
* tensorflow/addons#377

). 

Maybe we can provide a container named `GroupedTensorContainer` to make it works like below:

```python
x_np, y_np = get_test_data()

x_input = tf.keras.layers.Input(shape=x_np.shape[1:])

# NOTE: tensors can from different layers
pred_y, loss_required_1, loss_required_2 = SomeLayer(5)(x_input)

canned_tensors = GroupedTensorContainer(visiable=pred_y, hidden=(loss_required_1, loss_required_2))

model = tf.keras.Model(x_input, canned_tensors)

model.compile(""adam"", loss=SomeLoss())
model.fit(x_np, y_np)
model.evaluate(x_np, y_np)

container = model.predict(x_np)
container  # this is the visible tensor
```

while the `SomeLoss` has such a prototype:
```python
class SomeLoss:
    def __call__(self, y_true, y_pred, sample_weight=None):
        pred_y = y_pred.visiable_tensor
        loss_required_1, loss_required_2 = y_pred.hidden_tensors

       loss = compute_loss(
            y_true, pred_y, loss_required_1, loss_required_2
        )

        return loss
```


**System information**
- TensorFlow version (you are using): 2.1
- Are you willing to contribute it (Yes/No): Yes"
37817,Can't convert .pb model file to .tflite,"When I ran the following code:
```
import tensorflow as tf

path = ""C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/frozen_inference_graph-SteelRoll.pb""

inputs = [""image_tensor""]
outputs = [""detection_boxes""]

converter = tf.lite.TFLiteConverter.from_frozen_graph(path, inputs, outputs, input_shapes={""image_tensor"":[1,640,360,3]})
converter.post_training_quantize = True
tflite_model = converter.convert()
open(""frozen_inference_graph-SteelRoll.tflite"", ""wb"").write(tflite_model)
```
I met a fatal error which said:
`F tensorflow/lite/toco/tooling_util.cc:2258] Check failed: array.data_type == array.final_data_type Array ""image_tensor"" has mis-matching actual and final data types (data_type=uint8, final_data_type=float).
Fatal Python error: Aborted`
How do I solve this problem?
"
37815,Error when train a model on Colab,"**Code**

```
history = model.fit(train_dataset,
                    epochs=EPOCHS,
                    callbacks=[lr_schedule],
                    steps_per_epoch=STEPS_PER_EPOCH,
                    )
final_stats = model.evaluate(validation_dataset, steps=1)
print(""Validation accuracy: "", final_stats[1])
```


**Describe the problem**
When I tried to train my model on **Colab** using above code
I got the error like below mentioned:

```
Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.
Epoch `1/20`
UnimplementedError                        Traceback (most recent call last)
/tensorflow-1.15.0/python3.6/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
   1364     try:
-> 1365       return fn(*args)
   1366     except errors.OpError as e:

10 frames
UnimplementedError: From /job:worker/replica:0/task:0:
{{function_node __inference_Dataset_map_decode_image_22}} File system scheme '[local]' not implemented (file: '/gdrive/My Drive/plant-pathology-2020-fgvc7/images/Train_187.jpg')
	 [[{{node ReadFile}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional_9]]

During handling of the above exception, another exception occurred:

UnimplementedError                        Traceback (most recent call last)
/tensorflow-1.15.0/python3.6/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
   1382                     '\nsession_config.graph_options.rewrite_options.'
   1383                     'disable_meta_optimizer = True')
-> 1384       raise type(e)(node_def, op, message)
   1385 
   1386   def _extend_graph(self):

UnimplementedError: From /job:worker/replica:0/task:0:
 File system scheme '[local]' not implemented (file: '/gdrive/My Drive/plant-pathology-2020-fgvc7/images/Train_187.jpg')
	 [[{{node ReadFile}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional_9]]

```"
37814,import_pb_to_tensorboard.py OSError,"`python import_pb_to_tensorboard.py --model_dir C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/mnist_model_graph.pb --log_dir /tmp/tensorflow_logdir
OSError: SavedModel file does not exist at: C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/mnist_model_graph.pb`
But I'm pretty sure that I put it in this path, how can I solve this problem?"
37812,redefinition of cusolverStatus_t in tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc at lines 192 and 202,"I tried building TF on my system (ubuntu x86_64, cuda 10.2). I came across this error after about 15 mins of compiling. 

<em>In file included from tensorflow/stream_executor/cuda/cusolver_stub.cc:61:0:
./tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc: In function 'cusolverStatus_t cusolverDnIRSInfosGetNiters(cusolverDnIRSParams_t, cusolverDnIRSInfos_t, cusolver_int_t*)':
./tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc:202:30: error: redefinition of 'cusolverStatus_t cusolverDnIRSInfosGetNiters(cusolverDnIRSParams_t, cusolverDnIRSInfos_t, cusolver_int_t*)'
 cusolverStatus_t CUSOLVERAPI cusolverDnIRSInfosGetNiters(
                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc:192:30: note: 'cusolverStatus_t cusolverDnIRSInfosGetNiters(cusolverDnIRSParams_t, cusolverDnIRSInfos_t, cusolver_int_t*)' previously defined here
 cusolverStatus_t CUSOLVERAPI cusolverDnIRSInfosGetNiters(
                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~
</em>
Two definitions are right one after the other. Both functions seem to be identical. I am going to delete one of the definitions, and restart the build. Keeping my fingers crossed."
37811,GET returned 401 Unauthorized for  repository 'upb,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
macox 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source (github head)
- TensorFlow version:
master
- Python version:
2..7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
2.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**


```
INFO: Call stack for the definition of repository 'upb' which is a http_archive (rule definition at /private/var/tmp/_bazel_etsang/4c08b338ce3f5893c94fb26f4e062082/external/bazel_tools/tools/build_defs/repo/http.bzl:292:16):
 - /private/var/tmp/_bazel_etsang/4c08b338ce3f5893c94fb26f4e062082/external/com_github_grpc_grpc/bazel/grpc_deps.bzl:228:9
 - /Users/etsang/dev/src/github.com/tensorflow/tensorflow/WORKSPACE:121:1
WARNING: Download from https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 401 Unauthorized
ERROR: An error occurred during the fetch of repository 'upb':
   java.io.IOException: Error downloading [https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz] to /private/var/tmp/_bazel_etsang/4c08b338ce3f5893c94fb26f4e062082/external/upb/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz: GET returned 401 Unauthorized
ERROR: no such package '@upb//bazel': java.io.IOException: Error downloading [https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz] to /private/var/tmp/_bazel_etsang/4c08b338ce3f5893c94fb26f4e062082/external/upb/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz: GET returned 401 Unauthorized
ERROR: no such package '@upb//bazel': java.io.IOException: Error downloading [https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz] to /private/var/tmp/_bazel_etsang/4c08b338ce3f5893c94fb26f4e062082/external/upb/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz: GET returned 401 Unauthorized
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure
answer the questions interactively prompted form ./configure
bazel build (no target to see what happens)
Errorred out
I already have .netrc setup under my home folder with the correct credentials

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
37809,import error,"DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.
"
37808,Code improvements results from Cppcheck,"I ran [cppcheck](https://github.com/danmar/cppcheck) on the Tensorflow codebase which resulted in the some code improvements. 

Download the results here: [tensorflow_cppcheck.zip](https://github.com/tensorflow/tensorflow/files/4365647/tensorflow_cppcheck.zip)
"
37807,Broken link,"## URL(S) with issue:
https://github.com/tensorflow/federated/blob/master/docs/federated_core.md
In the above readme file [MapsReduce](https://research.google/pubs/pub62.pdf/)

### Pull Request
I've corrected the link and by successfully merging [#813](https://github.com/tensorflow/federated/pull/813) it'll resolve this issue.
Hey, @mihaimaruseac Kindly review it."
37806,tf.Module does not update state when a trackable attribute is reassigned a nontrackable,"
**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Sid and Linux Arch
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): PyPi binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: - Bazel version (if compiling from source): 3.7.6
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When subclassing tf.Module and assigning a tf.Variable to an attribute, then later replacing it with a constant, tensorflow does not delete the Variable from its internal list of tracked objects. This leads to e.g. the Variable being saved and loaded with tf.saved_model, even though it's no longer referenced by any user-visible attribute.

**Describe the expected behavior**
Tensorflow correctly deletes the tf.Variable from its internal state.

**Standalone code to reproduce the issue** 
```python
class test(tf.Module):
    pass

a = test()
a.var = tf.Variable(1)
a.const = tf.constant(2)
a.var = tf.constant(a.var.numpy())

tf.saved_model.save(a, ""a"")
al = tf.saved_model.load(""a"")
```
Trying to access `al.const` throws an exception:
```python
al.const
```
```python
AttributeError: '_UserObject' object has no attribute 'const'
```
However, `al.var` still contains the tf.Variable:
```python
al.var
```
```python
<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>
```

**Other info / logs**

As far as I can tell, the problem is that `AutoTrackable.__setattr__` does not delete its internal tracking state if a Trackable is replaced by something non-trackable, it only handles cases where a non-trackable is replaced by a Trackable or a Trackable is replaced by another Trackable."
37805,How to force TF Eigen to use openBLAS library for GEMM calls?,"Hi,
I am trying to link OpenBLAS to Tensorflow Eigen, Basically I want to override Eigen gemm call to OpenBLAS gemm call using below command which is not working.

Command to building TF.
bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --copt=-DEIGEN_USE_BLAS --linkopt=-L<path to BLAS> --linkopt=-lopenblas //tensorflow/tools/pip_package:build_pip_package   

I did't find anything with TF documentation. Is there any way to achieve the same?
"
37804,Failed to load the native TensorFlow runtime,"
**System information**
- OS Platform and Distribution MAC version 10.10.5 
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.1
- Python version: 3.7
- Installed using  conda :
- CUDA/cuDNN version: NVIDIA GeForce GT 330M 256 Mo
- GPU model and memory: mi-2010, 

**Describe the problem**

When i try to execut : import tensorflow as tf

i have :

ImportError: Traceback (most recent call last):
  File ""/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

Can you help me.

please?
"
37802,Cannot build TensorFlow 2.2 branch on Windows 10,"**System information**
- OS Platform and Distribution: Windows 10 Pro x64
- Mobile device: None
- TensorFlow installed from (source or binary): sources
- TensorFlow version: branch r2.2, commit acf4951a2f5fdc181ed14c163381c0cf135d9ee6
- Python version: 3.7.1
- Installed using virtualenv? pip? conda?: virtualenv with pip, without conda
- Bazel version (if compiling from source): the default version downloaded by Bazelisk
- GCC/Compiler version (if compiling from source): MSVS 2019 v16.5.0
- CUDA/cuDNN version: 10.2 / 7.6.5.32
- GPU model and memory: Geforce GTX 1080

**Describe the problem**

Compilation stops with the following error:
```
ERROR: D:/libs/ai_ml/tensorflow/tensorflow/lite/tools/optimize/BUILD:115:1: C++ compilation of rule '//tensorflow/lite/tools/optimize:operator_property' failed (Exit 2)
tensorflow/lite/tools/optimize/operator_property.cc(207): error C2679: binary '=': no operator found which takes a right-hand operand of type 'initializer list' (or there is no acceptable conversion)
.\tensorflow/lite/tools/optimize/operator_property.h(34): note: could be 'tflite::optimize::operator_property::DerivedScale &tflite::optimize::operator_property::DerivedScale::operator =(tflite::optimize::operator_property::DerivedScale &&) noexcept(<expr>)'
.\tensorflow/lite/tools/optimize/operator_property.h(34): note: or       'tflite::optimize::operator_property::DerivedScale &tflite::optimize::operator_property::DerivedScale::operator =(const tflite::optimize::operator_property::DerivedScale &)'
tensorflow/lite/tools/optimize/operator_property.cc(207): note: while trying to match the argument list '(tflite::optimize::operator_property::DerivedScale, initializer list)'
```


**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
python ./configure.py
set path=C:\Programs\Build;C:\msys64\usr\bin;%PATH%
""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build\vcvars64.bat""
set BAZEL_VC=""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC""
bazel build --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
None
"
37800,typo in https://www.tensorflow.org/guide/upgrade,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/guide/upgrade#recommended_upgrade_process


## Description of issue (what needs changing):
there is two consecutive ""will"" in the point `Run the upgrade script`. 

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? 
Yes. I'll be opening a PR soon."
37799,Progress bar with tensorflow.keras.load_model,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):1.15
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
When loading larger multiple models it is taking too much time. It will be better to have a progress bar to know current status of model loading.
**Will this change the current api? How?**
I do not know.
**Who will benefit with this feature?**
Those working with models trained using transfer learning or using very learge models on a CPU.
**Any Other info.**
I checked on progressbar2 and tqdm. But both requires an iterable to work with."
37797,ImportError: DLL load failed: The specified module could not be found.,"**

> SOLVED ===>  I installed Visual Studio 2009.  The root of the problem is that I need clean C++ redistributable files available.  WHEW!

**

Thanks Sreerag-ibtl for jumping into my issue.  I tried reinstall using pip, conda then use anaconda interface several times.  "
37796,Problem with Custom Metrics Even for H5 models,"These are how I defined and saved the custom metric fbeta and vgg model on colab:

```
from sklearn.model_selection import train_test_split
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential, Model
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten
from keras.layers import Dropout
from keras.optimizers import SGD
from keras import backend
```
```
def fbeta(y_true, y_pred, beta=2):
    y_pred = backend.clip(y_pred, 0, 1)
    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)
    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)
    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)
    p = tp / (tp + fp + backend.epsilon())
    r = tp / (tp + fn + backend.epsilon())
    # calculate fbeta, averaged across each class
    fbeta_score = backend.mean((1 + beta ** 2) * (p * r) / ((beta ** 2) * p + r + backend.epsilon()))
    return fbeta_score
```

 
```
def vgg16_model(in_shape=(128, 128, 3), out_shape=17):
    # load model
    model = VGG16(include_top=False, input_shape=in_shape)
    # mark loaded layers as not trainable
    for layer in model.layers:
        layer.trainable = False
    # allow last vgg block to be trainable
    model.get_layer('block5_conv1').trainable = True
    model.get_layer('block5_conv2').trainable = True
    model.get_layer('block5_conv3').trainable = True
    model.get_layer('block5_pool').trainable = True
    # add new classifier layers
    flat1 = Flatten()(model.layers[-1].output)
    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)
    output = Dense(out_shape, activation='sigmoid')(class1)
    # define new model
    model = Model(inputs=model.inputs, outputs=output)
    # compile model
    opt = SGD(lr=0.01, momentum=0.9)
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])
    return model
```


Create model and save in google drive:
```
model.save('model.h5')
model_file = drive.CreateFile({'title' : 'model.h5'})
model_file.SetContentFile('model.h5')
model_file.Upload()
print(""Model is saved."")
drive.CreateFile({'id': model_file.get('id')})
print(""Model downloaded to google drive."")
```

The error happened when I load the model with the following:

```
from pandas import read_csv
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.models import load_model
model = load_model('path/')  # i also tried ""model = load_model('path/', {'F-Beta': fbeta})""
result = model.predict(img)
print(result[0])
```

Got this error:
ValueError: Unknown metric function:fbeta

I believe all packages and paths are imported and defined correctly. Models were saved correctly on google drive as well. I tried all 3 other posts (#33646, #33648, #36390 ) regarding similar customized metric issues, and tried all the potential fixes and get-arounds, unfortunately none of them worked. I am surprised that some community members claimed that .h5 models should not have this issue though. "
37795,GradientTape.jacobian works for batch shape 0 when `experimental_use_pfor=True` but not when `experimental_use_pfor=False`,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  No
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  `macOS Catalina 10.15.2 (19C57)`
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): `v2.2.0-rc0-43-gacf4951a2f 2.2.0-rc1 0.10.0-dev20200321`
- Python version: 3.7.6
- CUDA/cuDNN version: - GPU model and memory: n/a


**Describe the current behavior**
Using `experimental_use_pfor=False` in `GradientTape.jacobian` behaves otherwise similarly as using `experimental_use_pfor=True`, but when running with tensors of batch shape 0, the former fails due to a type error, whereas the latter one works fine.

**Describe the expected behavior**
I would expect the behavior to be the same in both cases when `experimental_use_pfor=False` and `experimental_use_pfor=True`.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf


def main():
    variable = tf.Variable(1.0)
    inputs = (
        tf.constant(tf.random.uniform((0, 4))),
        tf.constant(tf.random.uniform((0, 3))),
    )

    with tf.GradientTape(persistent=True) as tape:
        outputs = variable * tf.pow(tf.concat(inputs, axis=-1), 2.0)

    jacobians_1 = tape.jacobian(
        outputs,
        variable,
        experimental_use_pfor=True,
    )
    print(jacobians_1)
    print(""tape.jacobians(..., experimental_use_pfor=True) works!"")

    try:
        jacobians_2 = tape.jacobian(
            outputs,
            variable,
            experimental_use_pfor=False,
        )
        print(jacobians_2)
        print(""tape.jacobians(..., experimental_use_pfor=False) works!"")
    except TypeError:
        print(""tape.jacobians(..., experimental_use_pfor=False) doesn't work!"")
        raise


if __name__ == '__main__':
    main()
```

**Other info / logs**
Originally posted here: https://github.com/tensorflow/tensorflow/issues/32460#issuecomment-572545864.

```
tape.jacobians(..., experimental_use_pfor=True) works!
tape.jacobians(..., experimental_use_pfor=False) doesn't work!
Traceback (most recent call last):
  File ""./tests/test_tape_jacobian.py"", line 36, in <module>
    main()
  File ""./tests/test_tape_jacobian.py"", line 26, in main
    experimental_use_pfor=False,
  File ""/Users/hartikainen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py"", line 1151, in jacobian
    for i, out in enumerate(output):
TypeError: 'NoneType' object is not iterable
```
"
37794,Problem with tf.train.FloatList,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:  /
- TensorFlow installed from (source or
binary): Source, 2.1 - TensorFlow version (use command below): 
- Python version: 3.6.9 - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory: 10.2

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

I have been trying to convert my Pascal VOC annotations (xml files + jpg) to TFRecords for training and validation datasets but I seem to be running into problems.
My bounding boxes are correct in the xml files (as in they don't go out of bounds) but whenever I start converting them to TFRecords they suddenly change value mid through. I have narrowed the specific code down to one line:
float_list=tf.train.FloatList(value=ymax)
After this line (the same for xmin, xmax and ymin) the values that I've calculated beforehand suddenly change. For example when I have 2 values in my ymax list and then generate the float_list I get the following:
[0.7670886848546281, 0.8025317614591573]
value: 0.7670887112617493
value: 0.8025317788124084

I don't really understand why they change but in some circumstances it makes the value go to 1.0 which makes my loss keep going to nan. 

My question is: is there any way to fix this or am i somehow doing this all wrong? I have provided the conversion script below.

Colab link:
https://colab.research.google.com/drive/1hKXjsgWdKUIAAeQHM5KiRL2iRhxKWSRu


```Python
'''
This program is meant for transforming .xml files to TFRecords.
In the program itself there is no need for changes. The only things
one has to change are the AIBASEMAP and IMAGEMAP locations when 
necessary!

If needed one could also change some settings in the FLAGS, although
this is not deemed necessary if the folder structure stays the same.
'''
import time
import os
from os import listdir
from os.path import isdir, isfile, join

import hashlib

from absl import app, flags, logging
from absl.flags import FLAGS
import tensorflow as tf
import lxml.etree
import tqdm
import time

# Set the basemap from which we're working 
# This is different for each device! Change accordingly!
AIBASEMAP = '/Code/yolov3-tf2-master/'                         #XXX JupyterLab

# Set the image map from which we're working
IMAGEBASEMAP = '/Images/Trash2_updated'


flags.DEFINE_string('data_dir', 'Directory with images and annotations',
                    'Directory with images and annotations')
flags.DEFINE_enum('split', 'train', [
                  'train', 'val'], 'train') #'specify train or val spit')
flags.DEFINE_string('output_file_train', 'Path to output train.tfrecord', 'Path to output train.tfrecord')
flags.DEFINE_string('output_file_val', 'Path to output val.tfrecord', 'Path to output val.tfrecord')
flags.DEFINE_string('classes', 'Path to .names file with labels', 'Class names record')

def build_example(annotation, class_map, path, i):
    check = True
    img_path = path + '.jpg'
    img_raw = open(img_path, 'rb').read()
    key = hashlib.sha256(img_raw).hexdigest()

    width = int(annotation['size']['width'])
    height = int(annotation['size']['height'])

    xmin = []
    ymin = []
    xmax = []
    ymax = []
    classes = []
    classes_text = []
    truncated = []
    views = []
    difficult_obj = []
    if 'object' in annotation:
        for obj in annotation['object']:
            difficult = bool(int(obj['difficult']))
            difficult_obj.append(int(difficult))

            xmin.append(float(obj['bndbox']['xmin']) / width)
            ymin.append(float(obj['bndbox']['ymin']) / height)
            xmax.append(float(obj['bndbox']['xmax']) / width)
            ymax.append(float(obj['bndbox']['ymax']) / height)
            classes_text.append(obj['name'].encode('utf8'))
            classes.append(class_map[obj['name']])
            truncated.append(int(obj['truncated']))
            views.append(obj['pose'].encode('utf8'))

    example = tf.train.Example(features=tf.train.Features(feature={
        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),
        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),
        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[
            annotation['filename'].encode('utf8')])),
        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[
            annotation['filename'].encode('utf8')])),
        'image/key/sha256': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode('utf8')])),
        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),
        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=['jpeg'.encode('utf8')])),
        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),
        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),
        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),
        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),
        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),
        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),
        'image/object/difficult': tf.train.Feature(int64_list=tf.train.Int64List(value=difficult_obj)),
        'image/object/truncated': tf.train.Feature(int64_list=tf.train.Int64List(value=truncated)),
        'image/object/view': tf.train.Feature(bytes_list=tf.train.BytesList(value=views)),
    }))
    return example


def parse_xml(xml):
    if not len(xml):
        return {xml.tag: xml.text}
    result = {}
    for child in xml:
        child_result = parse_xml(child)
        if child.tag != 'object':
            result[child.tag] = child_result[child.tag]
        else:
            if child.tag not in result:
                result[child.tag] = []
            result[child.tag].append(child_result[child.tag])
    return {xml.tag: result}

def conversion(image_list, writer, class_map):
    i = 0
    for image in tqdm.tqdm(image_list):
        i += 1
        temp = image
        try:
            image = image.split('.')[0]
            path_name = image.replace('.', '')
            annotation_xml = path_name + '.xml'
            annotation_xml = lxml.etree.fromstring(open(annotation_xml, 'rb').read())
            annotation = parse_xml(annotation_xml)['annotation']
            tf_example = build_example(annotation, class_map, path_name, i)
            writer.write(tf_example.SerializeToString())
        except Exception as exception:
            print('Er is een fout bij: ', path_name)
            with open('exception.log', '+w') as error_log_file:
                error_log_file.write(str(exception))
            quit()
            # check for XML syntax errors
        except etree.XMLSyntaxError as err:
            print('XML Syntax Error, see error_syntax.log, Error in: ', path_name)
            with open('error_syntax.log', '+a') as error_log_file:
                error_log_file.write('\nEr is een Fout in: ', path_name)
                error_log_file.write(str(err.error_log))
            quit()


def main(_argv):
    class_map = {name: idx for idx, name in enumerate(
        open(FLAGS.classes).read().splitlines())}
    logging.info(""Class mapping loaded: %s"", class_map)

    directories = [f for f in listdir(FLAGS.data_dir) if isdir(join(FLAGS.data_dir, f))]
    print('Start trainingsdata conversion.')
    writer = tf.io.TFRecordWriter(FLAGS.output_file_train)
    image_list = open(os.path.join(
        FLAGS.data_dir, 'Trash', 'solutions_%s.txt' % FLAGS.split)).read().splitlines()
    logging.info(""Image list loaded: %d"", len(image_list))
    conversion(image_list, writer, class_map)
    writer.close()
    print('Done')
    print('Start validationdata conversion.')
    writer = tf.io.TFRecordWriter(FLAGS.output_file_val)
    image_list = open(os.path.join(
        FLAGS.data_dir, 'Trash', 'validation_data.txt' )).read().splitlines()
    logging.info(""Image list loaded: %d"", len(image_list))
    conversion(image_list, writer, class_map)
    writer.close()
    logging.info(""Done"")


if __name__ == '__main__':
    app.run(main)
```"
37793,how to use tf.train.Checkpoint to restore only part of the model,"https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/training/tracking/util.py#L1692-L1989

I found it a little confusing when I use tf 2.x to restore part of the model (like decoder) to perform transfer learning. I found the code styles like this:
`checkpoint = tf.train.Checkpoint(model=autoencoder, optimizer=optimizer)`
if I want to use the pretrained decoder and write a new encoder different from the old_model, I know how to load the pretrained decoder's weights by explicitly using the decoder's name. But now it seems the restoration operation is through tf.train.Checkpoint. I don't know how to load part of the model's weight using tf.train.Checkpoint or change the model's encoder after restore the model. Is it correct to add a new line after `checkpoint = tf.train.Checkpoint(model=old_model, optimizer=optimizer)` like `autoencoder.encoder = new_encoder`?
"
37792,"Does Keras disable regularization (dropout, noise) layers when evaluating the validation data during Model.fit()?","## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit
https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict

## Description of issue (what needs changing):

### Clear description

I can't find a clear statement whether or not the regularization layers (noise, dropout) are bypassed when the validation data is processed (to calculate the validation loss) when calling `Model.fit()` provided with validation data. I can see that in the source code of the Dropout Layer that it is branched based on the `training` argument of `call()`. https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/layers/core.py#L181

But it is completely unclear to me whether or not the validation pass is considered ""training"" or not. After all, I'm calling the ""fit"" function."
37791,[Feature Request] Keras backend support jax,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
Let ```tensorflow.keras``` support ```jax``` so developers could use ```keras``` as ```jax``` high level api.
Tensorflow keras only support tensorflow backend now.
**Will this change the current api? How?**
Maybe
**Who will benefit with this feature?**
Developer who is interested in ```keras``` and ```jax```
**Any Other info.**
Keras could support numpy backend so it's easy to support jax as backend."
37787,Tensorflow Java API in Scala with scala-reflect runtime compilation,"**System information** 

I am using the [HelloWorld](https://www.tensorflow.org/install/lang_java) example from Java API. I am using Scala to execute it without any problems if I compile the code at ""compile time. Instead with I run that using the scala-reflect and therefore runtime compilation, the Session.Runner is not found.


- OS Platform and Distribution: Linux Ubuntu 18.04
- JDK: 
```
openjdk version ""1.8.0_242""
OpenJDK Runtime Environment (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08)
OpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode)
```
- TensorFlow: from `build.sbt`

```
lazy val commonSettings = Seq(
    scalaVersion := ""2.12.10"",

    libraryDependencies ++= {
      Seq(
                  // To support runtime compilation
        ""org.scala-lang"" % ""scala-reflect"" % scalaVersion.value,
        ""org.scala-lang"" % ""scala-compiler"" % scalaVersion.value,

        // for tensorflow4java
        ""org.tensorflow"" % ""tensorflow"" % ""1.15.0"",
        ""org.tensorflow"" % ""proto"" % ""1.15.0"",
        ""org.tensorflow"" % ""libtensorflow_jni"" % ""1.15.0""

      )
    }
)

lazy val `test-proj` = project
  .in(file("".""))
  .settings(commonSettings)
```

**Describe the current behavior**
Basically, when compiling at runtime the Runner does not gets executed. It compiles properly, but when executing, the runner does not exists. Here is the short stacktrace.

java.lang.NoSuchMethodError: org.tensorflow.Session.runner()Lorg/tensorflow/Session$$Runner;
  at __wrapper$1$f093d26a3c504d4381a37ef78b6c3d54.__wrapper$1$f093d26a3c504d4381a37ef78b6c3d54$.$anonfun$wrapper$1(<no source file>:15)

**Describe the expected behavior**

I expect that both pre-compiled or runtime compilation code will have the same behavior.

**Standalone code to reproduce the issue** 

Here is what WORKS:
```
import org.tensorflow.Graph
import org.tensorflow.Session
import org.tensorflow.Tensor
import org.tensorflow.TensorFlow


val g = new Graph()
val value = ""Hello from "" + TensorFlow.version()
val t = Tensor.create(value.getBytes(""UTF-8""))
// The Java API doesn't yet include convenience functions for adding operations.
g.opBuilder(""Const"", ""MyConst"").setAttr(""dtype"", t.dataType()).setAttr(""value"", t).build();

val s = new Session(g)
val output = s.runner().fetch(""MyConst"").run().get(0)
```

And here, what DOES NOT work:
```
import scala.reflect.runtime.{universe => ru}
import scala.tools.reflect.ToolBox
val fnStr = """"""
    {() =>
      import org.tensorflow.Graph
      import org.tensorflow.Session
      import org.tensorflow.Tensor
      import org.tensorflow.TensorFlow

      val g = new Graph()
      val value = ""Hello from "" + TensorFlow.version()
      val t = Tensor.create(value.getBytes(""UTF-8""))
      g.opBuilder(""Const"", ""MyConst"").setAttr(""dtype"", t.dataType()).setAttr(""value"", t).build();

      val s = new Session(g)

      s.runner().fetch(""MyConst"").run().get(0)
    }
    """"""
val mirror = ru.runtimeMirror(getClass.getClassLoader)
val tb = mirror.mkToolBox()
var t = tb.parse(fnStr)
val fn = tb.eval(t).asInstanceOf[() => Any]
// and finally, executing the function
fn()
```

Before, submitting this issue I posted a Question on stackoverflow: https://stackoverflow.com/questions/60783153/tensorflow-in-scala-reflection

I will try the Scala API provided by https://github.com/eaplatanios/tensorflow_scala"
37786,Missing link.,"
## URL(s) with the issue:
https://github.com/tensorflow/federated/blob/master/docs/deployment.md
```tf.backends``` link is missing .
##Pull request
[#812](https://github.com/tensorflow/federated/pull/812) by sucessfully merging this PR will closw this issue.


"
37783,Broken links,"
## URL(s) with the issue:
In this repo the ```tf.framework.Executor``` link is broken.
### Pull Request
Successfully merging of [#811](https://github.com/tensorflow/federated/pull/811) will close this issue.
Hey, @MarkDaoust , @lamberta, and @mihaimaruseac would you please review this pull request."
37782,Broken link in community/forums.md,"
## URL(s) with the issue:
https://github.com/tensorflow/docs/blob/master/site/en/community/forums.md

## Description of the issue (what needs changing):
In forums.ms  [how to get help](https://github.com/tensorflow/docs/blob/master/community/#get_help) is broken and while clicking it's showing 404 error.

### submit a pull request
Hey, @MarkDaoust and @lamberta would you please assign me this issue and please provide details to fix this issue I'll love to fix this issue."
37780,Missing interpreter initialization code in the codelab,"The codelab in question is [Build a handwritten digit classifier app with TensorFlow Lite
](https://codelabs.developers.google.com/codelabs/digit-classifier-tflite/index.html?index=..%2F..index#3).

Step 4.6 introduces the following snippet of code:

```kotlin
// Read input shape from model file
val inputShape = interpreter.getInputTensor(0).shape()
inputImageWidth = inputShape[1]
inputImageHeight = inputShape[2]
modelInputSize = FLOAT_TYPE_SIZE * inputImageWidth * inputImageHeight * PIXEL_SIZE

// Finish interpreter initialization
this.interpreter = interpreter
```

`interpreter` is a class field that has not been initialized in any previous code snippets and the code fails to compile.

Looking at the finalized code in the `finish` directory of the downloadable sample, 4.6 should've probably included this code snippet to initialize `interpreter`:

```kotlin
// Initialize TF Lite Interpreter with NNAPI enabled.
val options = Interpreter.Options()
options.setUseNNAPI(true)
val interpreter = Interpreter(model, options)
```"
37779,UnavailableError: Socket closed while using custom training loop,"**System information**
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- Tensorflow version: `2.1.0`

About hardware and software system information I'm using Kaggle kernels so more information can be found here: https://github.com/Kaggle/docker-python

**Describe the current behavior**
I'm getting errors like the one below, this happens during the training loop.

![Screenshot from 2020-03-21 09-41-52](https://user-images.githubusercontent.com/16668746/77226592-4b217780-6b58-11ea-8d38-eedea01ca32a.png)

**Describe the expected behavior**
The model was supposed to train normally.

**Standalone code to reproduce the issue** 
Link for the Kaggle kernel: https://www.kaggle.com/dimitreoliveira/bug-report-unavailableerror-socket-closed

**Other info / logs** 
As described on the notebook linked above usually happens when using some combination of the below:
- Long epochs
- Heavy models
- Some loops using too much memory
"
37778,Missing arguments when the inputs of concrete function contain list,"**System information** 
- TensorFlow version: v2.1.0

**Describe the current behavior**
```python
@tf.function
def test(a, b, c):
    pass


c_test = test.get_concrete_function(a=[tf.TensorSpec((None, 1)),
                                       tf.TensorSpec((None, 1))],
                                    b=tf.TensorSpec((None, 2)),
                                    c=[tf.TensorSpec((None, 3)),
                                       tf.TensorSpec((None, 3))])

c_test(a=[tf.random.normal([2, 1]), tf.random.normal([2, 1])],
       b=tf.random.normal([2, 2]),
       c=[tf.random.normal([2, 3]), tf.random.normal([2, 3])])
```
The function `test` has `a` and `c`  two arguments that should be as the type list. I create a concrete function `c_test` from `test`. But when `c_test` is called like the code above, it will throw
```
Expected argument names ['a', 'a_1', 'b', 'c', 'c_1'] but got values for ['a', 'b', 'c']. Missing: ['c_1', 'a_1'].
```
It seems that the function only received the first element of the list arguments."
37777,It seems that Tensorflow needs a check for the unreasonable parameter `input_dim=0` in the layer `Embedding`.,"## System information

- Have I written custom code (as opposed to using example directory):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 & Linux Ubuntu 18.04
- Tensorflow version：2.1.0-cpu (using 'pip install TensorFlow' to download directly)
- Python version: 3.6.9
- CUDA/cuDNN version: -
- GPU model and memory: -

## Describe the current behavior
When I build the model with an illogical parameter `input_dim = 0`in the layer `Embedding`, **Tensorflow uses this unreasonably parameter to build and even save the model**. The detailed performance in building the model is shown in the following picture。

![image](https://user-images.githubusercontent.com/46860123/77224293-27355480-6b9f-11ea-9d96-df3e852ca4b3.png)

## Key insights
To sum up, `input_dim` or `output_dim = 0` are unreasonable corner cases. ** Tensorflow seems to lack in checking this corner case**. This may lead Tensorflow users to create and even save a wrong model, which will bring potential risks in the subsequent usage.

## Code to reproduce the issue

``` python
import numpy as np
import tensorflow.keras.layers as L 
from tensorflow.keras import Model, Input
import tensorflow
import os

print(tensorflow.__version__) 

kwargs = {
    'input_dim': 0, # you can also set input_dim to 0 to test
	'output_dim': 18,
	'mask_zero': True
}
input = (10 * np.random.random((32,10)))
layer = L.Embedding(**kwargs)
x = Input(batch_shape=input.shape)
y = layer(x)
bk_model = Model(x, y)
model_path = os.path.join('./', 'mode.h5')
bk_model.save(model_path, bk_model)
print('finish')
```"
37776,Potential bug in Tensorflow 1 integration with Jupyter notebook,"When running the attached jupyter notebook example from [Manning book ""Tensorflow in action""](https://www.manning.com/books/machine-learning-with-tensorflow) - I know there are some typos in the code but I correct them to make the code works - , I meet the following condition

- When I run all steps for the first time, everything works fine 

- When I run the tf placeholder declaration twice

Code[
alpha = tf.constant(0.05)
curr_value = tf.placeholder(tf.float32)
prev_avg = tf.Variable(0.)

update_avg = alpha * curr_value + (1 - alpha) * prev_avg
]

the session crashes with an error :

Code[
init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    writer.add_graph(sess.graph)
    for i in range(len(raw_data)):
        summary_str, curr_avg = sess.run([merged, update_avg], feed_dict={curr_value: raw_data[i]})
        sess.run(tf.assign(prev_avg, curr_avg))
        print(raw_data[i], curr_avg)
        writer.add_summary(summary_str, i)
]

Error message[
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-13-81676f6fb632> in <module>()
      5     writer.add_graph(sess.graph)
      6     for i in range(len(raw_data)):
----> 7         summary_str, curr_avg = sess.run([merged, update_avg], feed_dict={curr_value: raw_data[i]})
      8         sess.run(tf.assign(prev_avg, curr_avg))
      9         print(raw_data[i], curr_avg)

/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    958     try:
    959       result = self._run(None, fetches, feed_dict, options_ptr,
--> 960                          run_metadata_ptr)
    961       if run_metadata:
    962         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1181     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1182       results = self._do_run(handle, final_targets, final_fetches,
-> 1183                              feed_dict_tensor, options, run_metadata)
   1184     else:
   1185       results = []
...]

**Workaround** : I have to restart kernel and clear all outputs to run the session again. 

**Possible cause** : either tensorflow or jupyter doesn't detect that placeholders have already been instantiated and instantiates them twice.

**Environment** : docker TF container tensorflow/tensorflow:latest-gpu-jupyter

[ch2Tensorboard.ipynb.txt](https://github.com/tensorflow/tensorflow/files/4362615/ch2Tensorboard.ipynb.txt)

"
37770,MSE loss computes different values depending on shape,"**System information** 
- Have I written custom code:  yes
- OS Platform and Distribution: Linux Ubuntu 19.04
 - TensorFlow version: 2.1.0
- Python version: 3.7.5
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX Titan X

**Describe the current behavior**
When MSE is calculated on two vectors, if one has an additional dimension, the result is different. For instance if one is [128] and the other is [128,1] the final value is different than what is calculated with vectors containing the same values but both [128.].
Moreover the score computed for the 4 combinations I tried ( `mse([128], [128[)`, `mse([128, 1], [128[)`, `mse([128], [128, 1])`, `mse([128, 1], [128, 1]`)  )are all different, so I don't really trust the computation at all, so there's likely a bug somewhere.

**Describe the expected behavior**
Either that the computation is performed correctly or that an error is raised about the inouts having different shapes. Silent errors like this are difficult to debug.

**Standalone code to reproduce the issue** 
```
import numpy as np
import tensorflow as tf

targets = np.array([180.858, -53.000732, 161.95107, 135.10085, -5.4330907, -86.42976, -4.4581985, -32.90381, -153.1223, -78.94036, 190.12129, -157.32057, -8.215049, -17.959564, -21.816954, 40.21217, -50.351727, 38.70819, 52.955853, 213.77878, -142.41376, 127.22313, 164.2927, -74.497314, -74.87329, 14.303827, 164.1599, 190.37862, -63.337723, 74.058975, -70.482285, -40.203323, -47.59432, -17.782892, 70.3338, -127.87029, -12.542, -31.236902, 70.227974, -81.60634, -186.79362, -176.01068, -118.73177, -74.14537, -56.437016, 98.60682, -3.1523242, 9.694114, -11.809049, -16.225067, -4.6299715, -194.44075, -138.53864, -118.06511, -201.88641, -85.310356, 91.92171, 107.94937, -44.26706, -93.79351, -9.981134, 40.544876, 131.26842, 7.305799, -97.13315, 94.43779, 146.48007, -24.092182, 32.081444, 32.98506, 93.73731, 65.58341, 36.74394, 57.02824, -78.452866, -6.0548353, -11.639992, 114.853455, -15.473761, -24.454018, -127.82523, 68.350876, -41.449036, 39.643234, -45.420197, -0.9474962, -111.20463, -10.079266, -79.32773, -93.07437, -111.04116, -47.006187, -68.18549, 36.195724, 100.86029, -74.86413, -13.0117655, 293.18875, 39.411587, 121.270706, -142.66888, 23.961506, 81.58176, -137.42886, 31.068184, 73.448364, -90.646164, 133.64107, 88.79693, -117.37866, 54.3003, -181.60715, 100.147194, 179.99359, 24.455635, 59.38088, 135.56046, 67.400925, 151.78516, 212.14339, -202.64584, 66.06116, 1.9135226, -244.05527, -70.778275, -50.001457, -194.73297, 33.012333])
predictions = np.array([0.12198464, 0.09282801, 0.09430753, 0.06222287, 0.07448876, 0.03799684, 0.02936049, 0.03837839, 0.04432172, 0.01919999, 0.07735521, 0.04389271, 0.09087924, 0.05364547, 0.01343504, 0.04935993, 0.02090639, 0.04636865, 0.06702548, 0.09186736, 0.11273132, 0.0611049 , 0.06820674, 0.07969542, 0.02481739, 0.04868209, 0.08474196, 0.0776654 , 0.03664336, 0.04501346, 0.06626878, 0.03605408, 0.02785883, 0.01698643, 0.09615672, 0.07914701, 0.02611066, 0.0447035 , 0.08619086, 0.04838634, 0.07977191, 0.06319098, 0.04025086, 0.05129454, 0.02673621, 0.05525842, 0.0054835 , 0.04647385, 0.02476176, 0.02783814, 0.11566448, 0.08409265, 0.03792451, 0.03227585, 0.0632838 , 0.08329175, 0.04616582, 0.06513302, 0.07169756, 0.05911999, 0.05913429, 0.01704707, 0.06693612, 0.04886937, 0.02549478, 0.04468452, 0.07630262, 0.05455045, 0.06637821, 0.01789702, 0.11108026, 0.03976684, 0.0171865 , 0.13416564, 0.02845822, 0.05074854, 0.04896633, 0.05221288, 0.03563176, 0.05014472, 0.05413034, 0.0347496 , 0.0645119 , 0.04159546, 0.01868404, 0.0582131 , 0.0336203 , 0.04432501, 0.03495208, 0.02673723, 0.09592278, 0.02579375, 0.01584711, 0.02812203, 0.03840974, 0.02530819, 0.08957738, 0.14304015, 0.03345468, 0.06080145, 0.09284427, 0.04770067, 0.07064755, 0.04004309, 0.02097335, 0.08742893, 0.04389744, 0.0479476 , 0.05911161, 0.0748862 , 0.06840549, 0.0580482 , 0.05427855, 0.10075781, 0.01691986, 0.04473659, 0.0634447 , 0.03176469, 0.05624699, 0.12614223, 0.08688905, 0.02355402, 0.0871409 , 0.0734048 , 0.02676748, 0.02766727, 0.08999605, 0.03465028])

mse = tf.keras.losses.MeanSquaredError()

tf.config.experimental_run_functions_eagerly(True)

print(mse(targets, predictions))
print(mse(targets[:, tf.newaxis], predictions))
print(mse(targets, predictions[:, tf.newaxis]))
print(mse(targets[:, tf.newaxis], predictions[:, tf.newaxis]))
```
Output:
```
tf.Tensor(10867.5537109375, shape=(), dtype=float64)
tf.Tensor(10868.94140625, shape=(), dtype=float64)
tf.Tensor(10868.9404296875, shape=(), dtype=float64)
tf.Tensor(10867.552734375, shape=(), dtype=float64)
```"
37769,Distributed Tensorflow : Issue while starting/connecting to parameter server,"I am trying to run a distributed tensorflow program with one parameter server and one worker over 2 vm each having gpu card but  it gives issues while connecting to parameter server with following 2 issues in the log:
1.tensorflow/core/distributed_runtime/master.cc:268] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0
2.2020-03-20 17:34:11.395256: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
error: error running IJS server: ""could not get remote execution state""

System Info : 
Tensorflow-gpu = 1.14.0
gpu - Tesla V100-SXM2
Ubuntu 18.04.4

With grpc debug enabled got the following logs:
Using TensorFlow backend.
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
WARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-03-20 17:34:00.582262: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-03-20 17:34:00.595624: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2020-03-20 17:34:01.159488: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x43f71d0 executing computations on platform CUDA. Devices:
2020-03-20 17:34:01.159525: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-03-20 17:34:01.190018: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194895000 Hz
2020-03-20 17:34:01.195769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3bd9420 executing computations on platform Host. Devices:
2020-03-20 17:34:01.195833: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-03-20 17:34:01.198050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:0b:00.0
2020-03-20 17:34:01.199864: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-03-20 17:34:01.202665: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-03-20 17:34:01.204955: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-03-20 17:34:01.206197: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-03-20 17:34:01.208933: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-03-20 17:34:01.211107: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-03-20 17:34:01.216280: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-03-20 17:34:01.220159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-03-20 17:34:01.220215: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-03-20 17:34:01.222970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-20 17:34:01.222996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-03-20 17:34:01.223010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-03-20 17:34:01.227719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13690 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0b:00.0, compute capability: 7.0)
WARNING:tensorflow:From /dev/local/store_keras_graph.py:32: The name tf.train.write_graph is deprecated. Please use tf.io.write_graph instead.

WARNING:tensorflow:From /dev/local/store_keras_graph.py:33: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
WARNING:tensorflow:From /dev/local/tensorflow_backend.py:17: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

WARNING:tensorflow:From /dev/local/tensorflow_backend.py:31: The name tf.train.Server is deprecated. Please use tf.distribute.Server instead.

2020-03-20 17:34:11.395256: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
error: error running IJS server: ""could not get remote execution state""
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
WARNING:tensorflow:From /dev/local/tensorflow_backend.py:17: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

WARNING:tensorflow:From /dev/local/tensorflow_backend.py:31: The name tf.train.Server is deprecated. Please use tf.distribute.Server instead.

2020-03-20 17:34:12.093571: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-03-20 17:34:12.130334: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2020-03-20 17:34:12.362334: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3b74510 executing computations on platform CUDA. Devices:
2020-03-20 17:34:12.362373: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-03-20 17:34:12.390397: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194855000 Hz
2020-03-20 17:34:12.395027: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x40bcb60 executing computations on platform Host. Devices:
2020-03-20 17:34:12.395049: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-03-20 17:34:12.396859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:0b:00.0
2020-03-20 17:34:12.398523: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-03-20 17:34:12.400933: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-03-20 17:34:12.403149: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-03-20 17:34:12.404455: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-03-20 17:34:12.406953: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-03-20 17:34:12.409163: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-03-20 17:34:12.413758: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-03-20 17:34:12.416923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-03-20 17:34:12.416983: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-03-20 17:34:12.419143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-20 17:34:12.419161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-03-20 17:34:12.419185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-03-20 17:34:12.422384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 15023 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0b:00.0, compute capability: 7.0)
D0320 17:34:12.423871920   55784 ev_posix.cc:170]            Using polling engine: poll
D0320 17:34:12.423907616   55784 dns_resolver.cc:325]        Using native dns resolver
I0320 17:34:12.425125694   55784 socket_utils_common_posix.cc:346] Disabling AF_INET6 sockets because ::1 is not available.
I0320 17:34:12.425176554   55784 socket_utils_common_posix.cc:292] Enabling TCP_USER_TIMEOUT with a timeout of 20000 ms
I0320 17:34:12.425247870   55784 tcp_server_posix.cc:308]    Failed to add :: listener, the environment may not support IPv6: {""created"":""@1584750852.425163790"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":379,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:2225""}
2020-03-20 17:34:12.425340: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job ps -> {0 -> lgpbddgx03.gso.aexp.com:2224}
2020-03-20 17:34:12.425355: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2225}
2020-03-20 17:34:12.432092: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2225
WARNING:tensorflow:From /dev/local/tensorflow_backend.py:49: The name tf.train.replica_device_setter is deprecated. Please use tf.compat.v1.train.replica_device_setter instead.

WARNING:tensorflow:From /dev/local/tensorflow_backend.py:52: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.

WARNING:tensorflow:From /dev/local/tensorflow_backend.py:53: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /dev/local/tensorflow_backend.py:188: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /dev/local/tensorflow_backend.py:70: The name tf.train.create_global_step is deprecated. Please use tf.compat.v1.train.create_global_step instead.

WARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1308: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /dev/local/tensorflow_backend.py:86: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From /dev/local/tensorflow_backend.py:89: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
WARNING:tensorflow:From /dev/local/tensorflow_backend.py:94: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

D0320 17:34:15.655343415   56157 dns_resolver.cc:275]        Start resolving.
I0320 17:34:15.658418088   56042 tcp_client_posix.cc:332]    CLIENT_CONNECT: ipv4:10.16.53.197:2224: asynchronously connecting fd 0x154c04009a30
I0320 17:34:15.658539193   56045 tcp_client_posix.cc:150]    CLIENT_CONNECT: ipv4:10.16.53.197:2224: on_writable: error=""No Error""
I0320 17:34:15.658636266   56045 tcp_client_posix.cc:113]    CLIENT_CONNECT: ipv4:10.16.53.197:2224: on_alarm: error=""Cancelled""
I0320 17:34:15.658682435   56045 subchannel.cc:973]          Connect failed: {""created"":""@1584750855.658611519"",""description"":""Failed to connect to remote host: Connection refused"",""errno"":111,""file"":""external/grpc/src/core/lib/iomgr/tcp_client_posix.cc"",""file_line"":207,""os_error"":""Connection refused"",""syscall"":""connect"",""target_address"":""ipv4:10.16.53.197:2224""}
I0320 17:34:15.658714952   56045 subchannel.cc:897]          Subchannel 0x154c04008290: Retry in 1000 milliseconds
D0320 17:34:15.658725238   56045 dns_resolver.cc:255]        In cooldown from last resolution (from 3 ms ago). Will resolve again in 997 ms
D0320 17:34:15.658734069   56045 dns_resolver.cc:275]        Start resolving.
 "
37768,min_epsilon of fused_batch_norm is incorrect,"**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): n/a
- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): n/a
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source orbinary): n/a
- TensorFlow version (use command below):  n/a
- Python version: n/a
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

The source code in current master branch has this:
https://github.com/tensorflow/tensorflow/blob/0c853d6cadf055be10562cfec92a93776fafd555/tensorflow/python/ops/nn_impl.py#L1516-L1519

However, the statement in the comment is no longer correct, according to https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_750.html:
> the value of epsilon is required to be greater or equal to CUDNN_BN_MIN_EPSILON which was defined in the cudnn.h file to the value 1e-5. This threshold value is now lowered to 0.0 to allow a wider range of epsilon value.

If compatibility with earlier version of cudnn is needed, the minimum should be obtained by using the `CUDNN_BN_MIN_EPSILON` constant defined by `cudnn.h`. 
Or, maybe it's better to just throw an error when a wrong epsilon is provided by the user.  Personally I don't like my parameter to be silently changed -- this can cause issues like `nn.fused_batch_norm` and `nn.batch_normalization` producing different results."
37765,[Bug] TF 2.2.0rc0 fails with AMP and Horovod 0.19.1 in Keras compile & fit,"With the recent changes in the Tensorflow Keras Optimizer API and Horovod. We did some testing and found that the following configuration was now broken:
- Tensorflow 2.2.0rc0
- Horovod 0.19.1
- AMP + Keras Model Compile & Fit 

@sanjoy @pkanwar23 could we make sure to fix this one before TF 2.2.0 gets officially published ? It's still an RC release for now :)

If needed you can use this docker container which contains the right set of dependency and based on the public TF2.2.0rc0 container:
```bash
docker pull born2data/tensorflow:hvd-0.19.1_tf_2.2.0rc0
```

**Code to reproduce:**
```bash
mpirun \
    -np 2 \
    -H localhost:2 \
    -bind-to none \
    -map-by slot \
    -x NCCL_DEBUG=VERSION \
    -x LD_LIBRARY_PATH \
    -x PATH \
    -mca pml ob1 -mca btl ^openib \
    --allow-run-as-root \
    python main.py
```

```python
import tensorflow as tf
import horovod.tensorflow.keras as hvd

# Horovod: initialize Horovod.
hvd.init()

# Horovod: pin GPU to be used to process local rank (one GPU per process)
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
if gpus:
    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')

(mnist_images, mnist_labels), _ = \
    tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())

dataset = tf.data.Dataset.from_tensor_slices(
    (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),
             tf.cast(mnist_labels, tf.int64))
)
dataset = dataset.repeat().shuffle(10000).batch(128)

policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16', 128)
tf.keras.mixed_precision.experimental.set_policy(policy)

mnist_model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),
    tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Horovod: adjust learning rate based on number of GPUs.
opt = tf.optimizers.Adam(0.001)

# Horovod: add Horovod DistributedOptimizer.
opt = hvd.DistributedOptimizer(opt)

# Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow
# uses hvd.DistributedOptimizer() to compute gradients.
mnist_model.compile(loss=tf.losses.SparseCategoricalCrossentropy(),
                    optimizer=opt,
                    metrics=['accuracy'],
                    experimental_run_tf_function=False)

callbacks = [
    # Horovod: broadcast initial variable states from rank 0 to all other processes.
    # This is necessary to ensure consistent initialization of all workers when
    # training is started with random weights or restored from a checkpoint.
    hvd.callbacks.BroadcastGlobalVariablesCallback(0),
]

# Train the model.
# Horovod: adjust number of steps based on number of GPUs.
mnist_model.fit(
    dataset,
    steps_per_epoch=500 // hvd.size(),
    callbacks=callbacks,
    epochs=24,
    verbose=1 if hvd.rank() == 0 else 0
)
```

**Error:**
```python
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:503 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:473 train_step  **
        _minimize(tape, self.optimizer, loss, self.trainable_variables)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1739 _minimize
        optimizer.apply_gradients(zip(gradients, trainable_variables))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:232 apply_gradients
        args=(grads_and_vars, name, all_reduce_sum_gradients))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2420 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2427 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:256 _apply_gradients_cross_replica  **
        control_flow_ops.no_op)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/smart_cond.py:54 smart_cond
        return true_fn()
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:248 apply_fn
        all_reduce_sum_gradients))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:262 _apply_gradients
        name, all_reduce_sum_gradients)
    /usr/local/lib/python3.6/dist-packages/horovod/_keras/__init__.py:73 apply_gradients
        raise Exception('`apply_gradients()` was called without a call to '

    Exception: `apply_gradients()` was called without a call to `get_gradients()` or `_aggregate_gradients`. If you're using TensorFlow 2.0, please specify `experimental_run_tf_function=False` in `compile()`.
```

Please let me know how I can help

CC: @nluehr @reedwm @tgaddair @cliffwoolley @omalleyt12 @houtoms"
37764,Tensorflow can build and even run a model with `tf.nn.conv2d('filter_width=0' and 'filter_height=0')`,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 2.1.0
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Tensorflow crashed when I run code with filter width and filter height is 0 
**Describe the expected behavior**
Check the bad situation and throw an error.
**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

[colab example](https://colab.research.google.com/drive/1_dD7KBsr0H08tqNpDTNh5ztGoHQpVKAz)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The bug seems to happen at tensorflow backend.
Related #37334"
37763,There is a bug that strings are not displayed in iOS darkmode.,"**System information** 

- Mobile device : iPhone 8 , iOS 13

**Describe the current behavior**
url : [image_classification/ios](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/ios)
There is a bug that strings are not displayed in iOS darkmode.
<img src=""https://user-images.githubusercontent.com/55723667/77207772-9da16a80-6b3d-11ea-8e27-b9e0593600e4.PNG"" width=""300""> 

There is not a bug in light mode.
<img src=""https://user-images.githubusercontent.com/55723667/77211304-ead60a00-6b46-11ea-8a1b-0d049318db84.PNG"" width=""300""> 



**Other info / logs** 
The reason for this is that the background and text are the same color.
"
42791,"[ja] load_data/csv.ipynb using non-existant ""Dataset.output_shapes"".","This tutorials is currently broken (also needs a `%tensorflow_version 2.x`):

https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/load_data/csv.ipynb"
42790,Some notebooks using: `tf-nightly-2.0-preview`,"This package does not exist anymore. Please upgrade all notebooks to use `%tensorflow_version 2x`.

https://github.com/tensorflow/docs-l10n/search?q=tf-nightly-2.0-preview&unscoped_q=tf-nightly-2.0-preview"
42789,Several notebooks failing on `keras.experimental.export_saved_model`,"https://github.com/tensorflow/docs-l10n/search?q=%22keras.experimental.export_saved_model%22&unscoped_q=%22keras.experimental.export_saved_model%22

```
# Exportar el modelo a 'SavedModel'
keras.experimental.export_saved_model(model, 'path_to_saved_model')

# Recrea exactamente el mismo modelo
new_model = keras.experimental.load_from_saved_model('path_to_saved_model')
```

These should just be `model.save('path_to_saved_model')`, and `keras.models.load_model('path_to_saved_model')`."
37761,Failed to convert object detection model to tensorflow lite,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: from binary 
- **TensorFlow version (use command below)**: 2.0
- **Python version**: 3.69
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:7.4.0
- **CUDA/cuDNN version**: 10.2
- **GPU model and memory**:
- **Exact command to reproduce**:
frozen_model_path = '/home/tflite/tflite_graph.pb'
input_arrays=[""image_tensor""]
output_arrays=[""detection_boxes"",""detection_scores"",""num_detections"",""detection_classes""]
input_tensor={""image_tensor"":[1,300,300,3]}
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_model_path, input_arrays, output_arrays, input_tensor)
tflite_model = converter.convert()

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I am using object detection API. I have a pretrained ssd_mobilenet_v2_coco model and I want to convert it to tensorflow lite(and do float16 quantization). In the folder of the model, I have checkpoint, frozen_inference_graph.pb, model.ckpt.data-00000-of-00001, model.ckpt.index, model.ckpt.meta, pipeline.config, and saved_model folder.

I used the following command to get the tflite.pb
python3 export_tflite_ssd_graph.py --output_directory=/home/tflite --pipeline_config_path=/home/exportedLP1/pipeline.config --trained_checkpoint_prefix=/home/exportedLP1/model.ckpt

Then I used the method posted online to convert tflite.pb. The last step  tflite = converter.convert() caused ""abort(core dumped)'. According to posts I read online, I should be able to convertt to tflite without any issue.

### Source code / logs
Current thread 0x00007efbff6f1740 (most recent call first):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56 in execute
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250 in _run_main
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299 in run
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93 in main
  File ""/usr/local/bin/toco_from_protos"", line 8 in <module>
Aborted (core dumped)

"
37760,Modifying an existing Tensorflow.js tutorial. Trying to modify 'part_3' of tutorial with code from 'part_4 handsign example'. Images not being read correctly.,"[Part 3: MNIST Classification]: https://medium.com/ailab-telu/learn-and-play-with-tensorflow-js-part-3-dd31fcab4c4b

[Part 4: Application Example]: https://medium.com/ailab-telu/learn-and-play-with-tensorflow-js-part-4-f011eb7bf4dc

Github link for tutorial: https://github.com/adf-telkomuniv/TensorFlowJS_Tutorial

After completing the the tutorial for '[Part 3: MNIST Classification]', I tried to apply code to the script to allow the option to import an image of a handwritten digit from the PC and have the trained model predict the value of the digit in the image.

I tried to use the code in the 'handsign.js' and 'handsign.html' files given in the 'part_4' github folder, but the images I'm using all give the same result despite all being different digits.

I think it might be reading the data from the canvas incorrectly?

This is the code I added to the 'index.js' file of Part 3 from the 'handsign.js' file of Part 4:
`initCanvas('predict-canvas1')
$('#upload-btn').click(async() => {
$('#img-upload').click()
})

var imgTensor
$('#img-upload').change(async(evt) => {
var canvas = $('#predict-canvas1')[0]
var context = canvas.getContext(""2d"")

var img = new Image() 

var file = evt.target.files[0]
if(file.type.match('image.*')) {
    var reader = new FileReader()
    reader.readAsDataURL(file)
    reader.onload = function(evt){
        if( evt.target.readyState == FileReader.DONE) {
            img.src = evt.target.result
            img.onload = () => {                    
                context.clearRect(0, 0, canvas.width, canvas.height)
                context.drawImage(img, 0, 0)
                

                
            }
        }
    }        
    $('#predict-btn1').prop('disabled', false)
    // $('#prediction1').text( 'Predicted: ')    
} else {
    alert(""not an image"")
}
})

$('#predict-btn1').click(async() => {
// try{
// $('#predict-btn1').prop('disabled', true)
// const img = imgTensor.div(tf.scalar(255))

//     var x_data = tf.cast(resized.reshape([1, 28, 28, 3]), 'float32')
//     var y_pred = await model.predict(x_data)
//     var predictions = Array.from(y_pred.argMax(1).dataSync())

//     $('#prediction1').text( 'Predicted: '+ predictions)    
// } catch (e) {
//     console.log(e)
//     alert('failed to predict image')
// }
var canvas = $('#predict-canvas1')[0]
var preview = $('#preview-canvas1')[0]

var img1 = tf.browser.fromPixels(imgTensor, 4)
var resized = util.cropImage(img1, canvas.width)        
tf.browser.toPixels(resized, preview)    
var x_data = tf.cast(resized.reshape([1, 28, 28, 1]), 'float32')
var y_pred = model.predict(x_data)

var prediction = Array.from(y_pred.argMax(1).dataSync())    
$('#prediction1').text( 'Predicted: '+ prediction)    

const barchartData = Array.from(y_pred.dataSync()).map((d, i) => {
    return { index: i, value: d }
})
tfvis.render.barchart($('#predict-graph1')[0], barchartData,  { width: 400, height: 140 })    
})`

Is there a way for this part of 'handsign.js' to work with the 'index.js' file?

If not, is there an alternative way for the trained model to predict the digit contained in an image?

All the code used in this project, including the images I'm trying to use, can be found at this repository: https://github.com/Miller144/verbose-octo-carnival"
37759,ImportError: DLL load failed: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.,"ImportError: DLL load failed: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.
Traceback (most recent call last):
  File ""C:\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.

During handling of the above exception, another exception occurred:"
37758,tf.io.gfile.glob missing some patterns. Using tf-nightly,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): tf-nightly
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:


**Describe the current behavior**
cc @Conchylicultor,
Please have a look on issue from `TFDS` tensorflow/datasets#1670, tests are failing for `PlantVillage `and `The300wLp` datasets because in `_generate_example ` function of both [`plant_village.py`](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image/plant_village.py#L142) and [the300w_lp.py](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image/the300w_lp.py#L116) `tf.io.gfile.glob()` does not correctly matches all examples patterns. However python [`glob`](https://docs.python.org/3/library/glob.html) solves issue see PR tensorflow/datasets#1684
**Describe the expected behavior**
tf.io.gfile.glob() must matches all patterns provided so that all required examples are generated.

**Standalone code to reproduce the issue** 
Please have a look on this [`colab`](https://colab.research.google.com/drive/1tfLOQubRWd6Dc9mPTtAwca-eAzctGLId) notebook, it contains all tracebacks as well as problem with ` tf.io.gfile.glob()` and how python `glob `solves this issue.

**As glob fix this issue but we have to use `tf.io.gfile` because we need to support GCS and other distributed files systems.**
"
37757,"TFLite - Experimental New Converter, incorrect model for Bidirectional GRU","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
Yes Provided Below.
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): 
Mac OS 10.12.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: Samsung Note9
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
2.1.0
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
TFLite produces a graph with 1 output of size 1(*4 bytes)
**Describe the expected behavior**
TFLite produces a graph with 1 output of size 64(*4 bytes)
**Standalone code to reproduce the issue** 
import tensorflow as tf
word = tf.keras.Input(shape=(1,), name='word', dtype=tf.int32)
embedding_output = tf.keras.layers.Embedding(8000,100,input_length=1)(word)
modified_embedding = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32))(embedding_output)
model = tf.keras.Model(inputs=[word], outputs=[modified_embedding])
model._make_predict_function()
model.summary()
model.save(""testmodel"")
converter = tf.lite.TFLiteConverter.from_saved_model(""testmodel"")
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
converter.experimental_new_converter = True
tflite_model = converter.convert()
open(""testmodel.tflite"", ""wb"").write(tflite_model)

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
37756,AttributeError when attempting to import Tensorflow on Windows 10,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.1.0
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None just CPU
- GPU model and memory: Just CPU



**Describe the problem**
When importing tensorflow, there seems to be a problem with compat not existing for datasetinitializerhook...
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Python 3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

```
>>> import tensorflow
2020-03-20 16:51:15.881191: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-03-20 16:51:15.884479: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\__init__.py"", line 46, in <module>
    from . _api.v2 import compat
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\_api\v2\compat\__init__.py"", line 39, in <module>
    from . import v1
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\_api\v2\compat\v1\__init__.py"", line 32, in <module>
    from . import compat
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\_api\v2\compat\v1\compat\__init__.py"", line 39, in <module>
    from . import v1
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\_api\v2\compat\v1\compat\v1\__init__.py"", line 29, in <module>
    from tensorflow._api.v2.compat.v1 import app
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\_api\v2\compat\__init__.py"", line 39, in <module>
    from . import v1
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\_api\v2\compat\v1\__init__.py"", line 32, in <module>
    from . import compat
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\_api\v2\compat\v1\compat\__init__.py"", line 39, in <module>
    from . import v1
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\_api\v2\compat\v1\compat\v1\__init__.py"", line 667, in <module>
    from tensorflow_estimator.python.estimator.api._v1 import estimator
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_estimator\__init__.py"", line 10, in <module>
    from tensorflow_estimator._api.v1 import estimator
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_estimator\_api\v1\estimator\__init__.py"", line 10, in <module>
    from tensorflow_estimator._api.v1.estimator import experimental
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_estimator\_api\v1\estimator\experimental\__init__.py"", line 10, in <module>
    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_estimator\python\estimator\canned\dnn.py"", line 33, in <module>
    from tensorflow_estimator.python.estimator import estimator
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 53, in <module>
    from tensorflow_estimator.python.estimator import util as estimator_util
  File ""C:\Users\gragundier\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_estimator\python\estimator\util.py"", line 75, in <module>
    class _DatasetInitializerHook(tf.compat.v1.train.SessionRunHook):
AttributeError: module 'tensorflow' has no attribute 'compat'
>>>
```"
37755,sparse_placeholder missing tensor name in SavedModel,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): mac-osx
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: n/a
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 1.14
- Python version: - Bazel
version (if compiling from source): n/a
- GCC/Compiler version (if compiling from
source): n/a
- CUDA/cuDNN version: - GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
If a sparse_placeholder is used in ServingInputReceiver, the SavedModel does not contain the name of the sparse_placeholder. This causes issues when the SavedModel is loaded by SavedModelEstimator, as shown below.
**Describe the expected behavior**
The tensor represented by sparse_placeholder should be assigned a name in the SavedModel.
**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Save the code below as a file, test.py. 
then run `python test.py`

```python
import tensorflow as tf
from pathlib import Path

tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)

DIM = 4
FEATURE_NAME = ""myfeature""
LABEL_NAME = ""mylabel""


def train_input_fn():
    dataset = tf.data.Dataset.from_tensors((
        {FEATURE_NAME: tf.SparseTensor(indices=[[0], [2]],
                        values=[1.0, 2.0],
                        dense_shape=[DIM])},[1.0, 2.3]))
    dataset = dataset.repeat(9).batch(3)
    return dataset


def serving_input_receiver_fn():
    """"""Serving input_fn that builds features from placeholders

    Returns
    -------
    tf.estimator.export.ServingInputReceiver
    """"""
    feature = tf.compat.v1.sparse_placeholder(dtype=tf.float32, shape=[None, DIM], name=FEATURE_NAME)
    receiver_tensors = {FEATURE_NAME: feature}
    return tf.estimator.export.ServingInputReceiver(receiver_tensors, receiver_tensors)


def model_fn(features, labels, mode):
    w = tf.compat.v1.get_variable(
        name='w',
        initializer=tf.compat.v1.random.truncated_normal((DIM,1), stddev=1))
    predictions = tf.compat.v1.sparse.sparse_dense_matmul(features[FEATURE_NAME], w)

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)
    else:
        loss = tf.nn.l2_loss(predictions - labels)
        if mode == tf.estimator.ModeKeys.EVAL:
            return tf.estimator.EstimatorSpec(
                mode, loss=loss)

        elif mode == tf.estimator.ModeKeys.TRAIN:
            train_op = tf.compat.v1.train.AdamOptimizer(learning_rate=0.5).minimize(
                loss, global_step=tf.compat.v1.train.get_global_step())
            return tf.estimator.EstimatorSpec(
                mode, loss=loss, train_op=train_op)
        else:
            raise NotImplementedError()


def find_latest(export_dir):
    subdirs = [x for x in Path(export_dir).iterdir()
               if x.is_dir() and 'temp' not in str(x)]
    latest = str(sorted(subdirs)[-1])
    return latest


if __name__ == '__main__':
    config = tf.estimator.RunConfig(save_summary_steps=1,
                                    save_checkpoints_steps=5,
                                    log_step_count_steps=1)
    export_dir = '/tmp/saved_model'
    checkpoint_dir = '/tmp/checkpoint'

    # train
    estimator = tf.estimator.Estimator(model_fn, checkpoint_dir, config=config)
    estimator.train(train_input_fn)

    # export
    estimator.export_saved_model(export_dir, serving_input_receiver_fn)

    # load
    latest_model = find_latest(export_dir)
    print(latest_model)

    # predict, tf 1.x
    estimator = tf.contrib.estimator.SavedModelEstimator(latest_model)
    predictions = estimator.predict(train_input_fn)
    for x in predictions:
        print(x)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Traceback
```
INFO:tensorflow:Using config: {'_model_dir': '/tmp/checkpoint', '_tf_random_seed': None, '_save_summary_steps': 1, '_save_checkpoints_steps': 5, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12caa1eb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
WARNING:tensorflow:From /envs/py3.6/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
WARNING:tensorflow:From /envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
INFO:tensorflow:Graph was finalized.
2020-03-20 08:09:40.379515: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /tmp/checkpoint/model.ckpt.
INFO:tensorflow:loss = 5.3372154, step = 1
INFO:tensorflow:global_step/sec: 732.118
INFO:tensorflow:loss = 1.6047388, step = 2 (0.002 sec)
INFO:tensorflow:global_step/sec: 713.317
INFO:tensorflow:loss = 4.220809, step = 3 (0.001 sec)
INFO:tensorflow:Saving checkpoints for 3 into /tmp/checkpoint/model.ckpt.
INFO:tensorflow:Loss for final step: 4.220809.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
WARNING:tensorflow:From /envs/py3.6/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.
INFO:tensorflow:Signatures INCLUDED in export for Classify: None
INFO:tensorflow:Signatures INCLUDED in export for Regress: None
INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']
INFO:tensorflow:Signatures INCLUDED in export for Train: None
INFO:tensorflow:Signatures INCLUDED in export for Eval: None
WARNING:tensorflow:From /envs/py3.6/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from /tmp/checkpoint/model.ckpt-3
INFO:tensorflow:Assets added to graph.
INFO:tensorflow:No assets to write.
INFO:tensorflow:SavedModel written to: /tmp/saved_model/temp-b'1584716980'/saved_model.pb
/tmp/saved_model/1584716980
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/vr/_8gkdt4s3wb88ncn_jmhk8hc000wcr/T/tmp1criy16p
INFO:tensorflow:Using config: {'_model_dir': '/var/folders/vr/_8gkdt4s3wb88ncn_jmhk8hc000wcr/T/tmp1criy16p', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x130e7eeb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
INFO:tensorflow:Checking available modes for SavedModelEstimator.
WARNING:tensorflow:train mode not found in SavedModel.
WARNING:tensorflow:eval mode not found in SavedModel.
INFO:tensorflow:Available modes for Estimator: ['infer']
INFO:tensorflow:Could not find trained model in model_dir: /var/folders/vr/_8gkdt4s3wb88ncn_jmhk8hc000wcr/T/tmp1criy16p, running initialization to predict.
WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.
INFO:tensorflow:Calling model_fn.
Traceback (most recent call last):
  File ""test_savedmodel.py"", line 83, in <module>
    for x in predictions:
  File ""/envs/py3.6/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 619, in predict
    features, None, ModeKeys.PREDICT, self.config)
  File ""/envs/py3.6/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1146, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/envs/py3.6/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/saved_model_estimator.py"", line 243, in _model_fn_from_saved_model
    g, tags, input_map=input_map, return_elements=output_tensor_names)
  File ""/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 352, in load_graph
    meta_graph_def, import_scope=import_scope, **saver_kwargs)
  File ""/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1473, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py"", line 857, in import_scoped_meta_graph_with_return_elements
    return_elements=return_elements)
  File ""/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 412, in import_graph_def
    input_map = _ConvertInputMapValues(name, input_map)
  File ""/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 193, in _ConvertInputMapValues
    'tf.import_graph_def() requires a non-empty `name` if `input_map` '
ValueError: tf.import_graph_def() requires a non-empty `name` if `input_map` contains non-Tensor values. Try calling tf.convert_to_tensor() on `input_map` values before calling tf.import_graph_def().
```

Further examine the savedmodel with cli `saved_model_cli show --dir /tmp/saved_model/1584716980 --all`

```
MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['myfeature'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, -1)
        name:
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: SparseTensorDenseMatMul/SparseTensorDenseMatMul:0
  Method name is: tensorflow/serving/predict
```
Note: `name` is missing from inputs['myfeature'] tensor_info"
37752,./build_all_linux.sh,"```
/home/light/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/util/test_log.pb.o:(.data.rel.ro._ZTVN6google8protobuf8internal12MapEntryImplIN10tensorflow35BenchmarkEntry_ExtrasEntry_DoNotUseENS0_7MessageENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_10EntryValueELNS1_14WireFormatLite9FieldTypeE9ELSE_11ELi0EEE[_ZTVN6google8protobuf8internal12MapEntryImplIN10tensorflow35BenchmarkEntry_ExtrasEntry_DoNotUseENS0_7MessageENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_10EntryValueELNS1_14WireFormatLite9FieldTypeE9ELSE_11ELi0EEE]+0x58)：对‘google::protobuf::Message::InitializationErrorString[abi:cxx11]() const’未定义的引用
/usr/bin/ld: 最后的链结失败: 符号需要不存在的调试节
collect2: error: ld returned 1 exit status
tensorflow/contrib/makefile/Makefile:899: recipe for target '/home/light/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text' failed
make: *** [/home/light/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1
```"
37751,corrupted tfrecord error should also have the info of file name,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

Could TensorFlow 2.0 add the name of tfrecord file  when the file is corrupted in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/record_reader.cc#L105 ? It  is very helpful for investigating  tfrecord file issue or storage issue.

we tried to add  like https://github.com/cheyang/tensorflow/commit/54015629282b35d848d3c64550d15c3804e05c8d

But it has segment fault issue. 

**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
37750,run label_image at linux-armv7l wrong,"
i follow the [guide](https://www.tensorflow.org/lite/guide/build_rpi) to build `minimal` and `label_image` for linux-armv7l, but when i run `label_image`, i got this error:
`Illegal instruction (core dumped) `
i build it at ubuntu18.04 with arm-linux-gnueabihf-gcc-4.8, and the ARM is cortexe-A9 with linux12.04.
what should i do?"
37748,TF 2.2.0 Windows Bazel 2.0.0 '@local_config_cuda//cuda'  (Python Path),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Winndows 10
- TensorFlow installed from (source or binary): source 
- TensorFlow version: 2.2.0
- Python version: 3.7
- Bazel version (if compiling from source):  2.0.0
- GCC/Compiler version (if compiling from source):  Visual Studio 2019 14.25.28610
- CUDA/cuDNN version:  CUDA 10.2 cuDNN 7.5
- GPU model and memory: RTX 2060 6Go

the problem is a bazel error when, I start building with bazel
The problem comes from \tensorflow\third_party\gpus\cuda_configure.bzl in the line 608:
`return execute(repository_ctx, [python_bin, ""-c"", decompress_and_execute_cmd])`

the real problem comes from the python_bin Path when it executes that line:
`system(""C:/Program Files/Python37/python.exe script.py cuda cudnn"")`
the space between Program and Files make an error in execution.

The current solution that i have found is to move the python folder too : 
`C:/Python37/python.exe`

That move resolves that problem for now.
I can't help more.
thanks



"
37747,[TF 2.2] keras fit class weights are being cast to int,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No
- TensorFlow version : introduced since 2.2.0-rc0
- Python version: - Bazel
version (if compiling from source): Python 3 (Colab)

Google Colaboratory
Tensorflow 2.2.0-rc0 with Python 3


**Describe the current behavior**
The class weights passed to keras model.fit are converted to int64, which leads to all class weights that are below 1 being set to 0.

**Describe the expected behavior**
That the class weights are kept as float values, so that results can be more precise based on the class weights

**Other info / logs**
I tracked the piece of code that is responsible for this behavior: 
[https://github.com/tensorflow/tensorflow/blob/c0306ef626b02ab5ab10aac2cec6d08f56136a5c/tensorflow/python/keras/engine/data_adapter.py#L1258](url)
`class_weight_tensor = ops.convert_to_tensor_v2(
      [class_weight[c] for c in class_ids])	      [int(class_weight[c]) for c in class_ids], dtype=""int64"")`

and it seems that this commit for TF 2.2 introduced it, but I cannot figure out why it was changed like this:
[https://github.com/tensorflow/tensorflow/commit/10666c59dd4858645d1b03ce01f4450da80710ec#diff-f8dd40712ac721c1b363e1a1ec44c1a3](url)

As a temporary workaround, I tried to make my lowest weight normalized to 1, but then I still get a floating poant value for the other weight, so even if I round it, it will still not be as precise as I need it.
Since I use tf.dataset loaded from GCS, I cannot simply pass the sample_weights directly myself either as a workaround.


So I am not sure whether is was a bug, or an intended change. In case it was an intended change, I would like to know what the alternative would be.
"
37746,Build TensorFlow library iOS from source,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iOS
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: Conda
- Bazel version (if compiling from source): 0.26.1


**Describe the problem**
I need to build Tensorflow 2.0.0 from source for iOS devices, to get a library libtensorflow-core.a, I found this link https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios where I found another link that seems to have the answer, but it redirect me to a missing page.

Has anyone an idea or a simple guide on how I can build 2.0.0 library for iOS?

Thanks!

"
37745,[EfficientDet] ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04 x86_64, CUDA10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): tf_nightly-2.2.0.dev20200319
- Python version: 3.6

**Command used to run the converter or code if you’re using the Python API**

I am trying to perform Weight Quantization using converter from saved_model, but I am suffering from the problem that the INPUT OP of saved_model is not recognized correctly by converter. You can confirm that the INPUT OP is included in the saved_model in the following steps, but an error will occur when the conversion is performed.
1. Download the **`EfficientDet`** **[google/automl](https://github.com/google/automl)** trained checkpoint from the link **[here](https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d0.tar.gz)**. This is the Google official release EfficientDet model. Then unzip the downloaded tar.gz file to any location.
2. Clone the google / automl repository.
```console:
$ sudo pip3 install tf-nightly
$ git clone https://github.com/google/automl.git
$ cd automl/efficientdet
```
3. The following script freezes the downloaded checkpoint.
```console
$ python3 model_inspect.py \
  --model_name=efficientdet-d0 \
  --delete_logdir=False \
  --freeze=True \
  --runmode=freeze \
  --input_image_size=512 \
  --ckpt_path=${HOME}/Downloads/efficientdet-d0 \
  --logdir=${HOME}/Downloads/efficientdet-d0/log \
  --export_ckpt=${HOME}/Downloads/efficientdet-d0/export \
  --threads=4
```
4. Use the following Python script to extract the layer name of the INPUT layer. This indicates that the INPUT layer name of the model is ""input"".
```python
### tf-nightly 2.2.0-dev20200319

import tensorflow as tf
import os
from tensorflow.python import ops

def get_graph_def_from_file(graph_filepath):
  tf.compat.v1.reset_default_graph()
  with ops.Graph().as_default():
    with tf.compat.v1.gfile.GFile(graph_filepath, 'rb') as f:
      graph_def = tf.compat.v1.GraphDef()
      graph_def.ParseFromString(f.read())
      return graph_def

# Look up the name of the placeholder for the input node
graph_def=get_graph_def_from_file('./efficientdet-d0_train.pb')
input_name=""""
for node in graph_def.node:
    if node.op=='Placeholder':
        print(""##### efficientdet-d0_train - Input Node Name #####"", node.name) # this will be the input node
        input_name=node.name
```
5. The following command converts **`efficientdet-d0_train.pb`** and save_model is created in **` saved_model`** folder.
```python
### tf-nightly 2.2.0-dev20200319

import tensorflow as tf
import os
import shutil
from tensorflow.python.saved_model import tag_constants
from tensorflow.python import ops

def get_graph_def_from_file(graph_filepath):
  tf.compat.v1.reset_default_graph()
  with ops.Graph().as_default():
    with tf.compat.v1.gfile.GFile(graph_filepath, 'rb') as f:
      graph_def = tf.compat.v1.GraphDef()
      graph_def.ParseFromString(f.read())
      return graph_def

def convert_graph_def_to_saved_model(export_dir, graph_filepath, input_name, outputs):
  graph_def = get_graph_def_from_file(graph_filepath)
  with tf.compat.v1.Session(graph=tf.Graph()) as session:
    tf.import_graph_def(graph_def, name='')
    tf.compat.v1.saved_model.simple_save(
        session,
        export_dir,# change input_image to node.name if you know the name
        inputs={input_name: session.graph.get_tensor_by_name('{}:0'.format(node.name))
            for node in graph_def.node if node.op=='Placeholder'},
        outputs={t.rstrip("":0""):session.graph.get_tensor_by_name(t) for t in outputs}
    )
    print('Optimized graph converted to SavedModel!')

# Look up the name of the placeholder for the input node
graph_def=get_graph_def_from_file('./efficientdet-d0_train.pb')
input_name=""input""
outputs = ['class_net/class-predict/BiasAdd:0',
           'class_net/class-predict_1/BiasAdd:0',
           'class_net/class-predict_2/BiasAdd:0',
           'class_net/class-predict_3/BiasAdd:0',
           'class_net/class-predict_4/BiasAdd:0',
           'box_net/box-predict/BiasAdd:0',
           'box_net/box-predict_1/BiasAdd:0',
           'box_net/box-predict_2/BiasAdd:0',
           'box_net/box-predict_3/BiasAdd:0',
           'box_net/box-predict_4/BiasAdd:0']
# convert this to a TF Serving compatible mode
shutil.rmtree('./saved_model', ignore_errors=True)
convert_graph_def_to_saved_model('./saved_model', './efficientdet-d0_train.pb', input_name, outputs)
```
6. Confirm that saved_model is generated correctly by the following command.
```console:
$ saved_model_cli show --dir ./saved_model --all

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['input'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 512, 512, 3)
        name: input:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['box_net/box-predict/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 64, 64, 36)
        name: box_net/box-predict/BiasAdd:0
    outputs['box_net/box-predict_1/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 32, 32, 36)
        name: box_net/box-predict_1/BiasAdd:0
    outputs['box_net/box-predict_2/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 16, 16, 36)
        name: box_net/box-predict_2/BiasAdd:0
    outputs['box_net/box-predict_3/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 8, 8, 36)
        name: box_net/box-predict_3/BiasAdd:0
    outputs['box_net/box-predict_4/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 4, 4, 36)
        name: box_net/box-predict_4/BiasAdd:0
    outputs['class_net/class-predict/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 64, 64, 810)
        name: class_net/class-predict/BiasAdd:0
    outputs['class_net/class-predict_1/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 32, 32, 810)
        name: class_net/class-predict_1/BiasAdd:0
    outputs['class_net/class-predict_2/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 16, 16, 810)
        name: class_net/class-predict_2/BiasAdd:0
    outputs['class_net/class-predict_3/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 8, 8, 810)
        name: class_net/class-predict_3/BiasAdd:0
    outputs['class_net/class-predict_4/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 4, 4, 810)
        name: class_net/class-predict_4/BiasAdd:0
  Method name is: tensorflow/serving/predict
```
7. Finally, an error occurs when performing Weight Quantization with the following Python script.
```python
### tf_nightly-2.2.0.dev20200319
import tensorflow as tf

# Weight Quantization - Input/Output=float32
converter = tf.lite.TFLiteConverter.from_saved_model('./saved_model')
#converter.experimental_new_converter = True
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.allow_custom_ops = True
tflite_quant_model = converter.convert()
with open('./efficientdet-d0_train.tflite', 'wb') as w:
    w.write(tflite_quant_model)
print(""Weight Quantization complete! - efficientdet-d0_train.tflite"")
```
The following error occurs even though the ""input"" Placeholder does exist in the last check procedure.
```
ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.
```

**The output from the converter invocation**

```console
2020-03-20 17:36:18.835931: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:18.836031: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:18.836056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
2020-03-20 17:36:20.375954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-03-20 17:36:20.388477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-20 17:36:20.388790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 with Max-Q Design computeCapability: 6.1
coreClock: 1.2655GHz coreCount: 16 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s
2020-03-20 17:36:20.388916: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.389000: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.389063: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.389126: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.389185: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.389246: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.391780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-20 17:36:20.391813: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-03-20 17:36:20.392010: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-03-20 17:36:20.414641: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
2020-03-20 17:36:20.415705: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xc1805f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-03-20 17:36:20.415744: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-03-20 17:36:20.460041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-20 17:36:20.460435: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xc216420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-03-20 17:36:20.460451: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070 with Max-Q Design, Compute Capability 6.1
2020-03-20 17:36:20.460530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-20 17:36:20.460538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      
2020-03-20 17:36:21.550114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-20 17:36:21.550409: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-03-20 17:36:21.550490: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-03-20 17:36:21.551093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-20 17:36:21.551353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 with Max-Q Design computeCapability: 6.1
coreClock: 1.2655GHz coreCount: 16 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s
2020-03-20 17:36:21.551456: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551525: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551587: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551648: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551710: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551772: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-20 17:36:21.551796: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-03-20 17:36:21.551808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-20 17:36:21.551812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-03-20 17:36:21.551818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-03-20 17:36:21.582079: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-03-20 17:36:21.582108: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-03-20 17:36:21.582113: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
Traceback (most recent call last):
  File ""03_weight_quantization.py"", line 12, in <module>
    tflite_quant_model = converter.convert()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py"", line 423, in convert
    self._funcs[0], lower_control_flow=False)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py"", line 506, in convert_variables_to_constants_v2
    raise ValueError(""Cannot find the Placeholder op that is an input ""
ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.
```

**Also, please include a link to the saved model or GraphDef**

```
https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d0.tar.gz
```

**Failure details**
The error occurs even though the ""input"" Placeholder does exist in the last check procedure.


**Any other info / logs**

The .pb, checkpoint, and script I used to check are committed below.
**https://github.com/PINTO0309/PINTO_model_zoo/tree/master/18_EfficientDet/01_float32**
"
37744,tflite | -O3 warning during tflite dll build in windows,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.2.0rc1
- Bazel version (if compiling from source): 2.0.0

**Describe the problem**
I'm getting a long list of -O3 warnings (below is only a small selection) during the tflite dll build in windows. The option -O3 should be changed to /O3 (or removed) for all windows builds.
```
INFO: From Compiling tensorflow/lite/core/subgraph.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/model.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/shape.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/skip_gram.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/segment_sum.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/if.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/floor_div.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/round.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/resize_nearest_neighbor.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/basic_rnn.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/slice.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/arg_min_max.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/local_response_norm.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/squared_difference.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling tensorflow/lite/kernels/squeeze.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`bazel build -c opt //tensorflow/lite:tensorflowlite`"
37743,Predictions are not accurate after loading the saved model - Resume classification,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: NA
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 2.1.0
- Python version: - Bazel
version (if compiling from source):3.7
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
We have created a sample resume classification model and saved the model with morethan 95% accuracy on train and test data set.After we have loaded the model and trying to make the predictions by defining prediction_classes using tf.argmax and we have observed some ambuiguity in predictions(Not predicting the correct label)

**Describe the expected behavior**

The predictions should be accurate after loading the model

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

[Resume_classification_V_1_5.zip](https://github.com/tensorflow/tensorflow/files/4358924/Resume_classification_V_1_5.zip)


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


"
37742,Add more ops in tflite,"**System information**
- Windows 10
- TensorFlow 2.0



Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MUL, SUB. Here is a list of operators for which you will need custom implementations: IdentityN.


"
37741,load_model error when model has multiple inputs,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

This is the last bit of the error

> ValueError: Layer lstm expects 1 inputs, but it received 3 input tensors. Inputs received: [<tf.Tensor 'input_1_2:0' shape=(None, 368, 32) dtype=float32>, <tf.Tensor 'input_2_2:0' shape=(None, 72) dtype=float32>, <tf.Tensor 'input_3_2:0' shape=(None, 72) dtype=float32>]

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  no
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): W10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if 
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: - Bazel 3.7 for Python and latest 2.1 for TF-gpu
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory: 10.1 -- 7.6 -- TitanRTX

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Created a SIMPLE model with multiple inputs as shown below, saved it with model.save method but failed to load it.

**Describe the expected behavior**

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
def create_model():
    ip1 = tf.keras.layers.Input(shape=(seq_len, ip_size))
    ip2 = tf.keras.layers.Input(shape=(hidden_size,))
    ip3 = tf.keras.layers.Input(shape=(hidden_size,))
    y0 = tf.keras.layers.LSTM(units=hidden_size)(inputs=ip1, initial_state=[ip2, ip3])
    y1 = tf.keras.layers.Dense(units=num_points, activation=tf.keras.activations.tanh)(y0)
    y = tf.keras.layers.Dense(units=num_points)(y1)
    model = tf.keras.models.Model(inputs=[ip1, ip2, ip3], outputs=y, name='cheater_model')
    return model
```
model = create_model()
model.save(path)
model = tf.keras.models.load(path)

PS: fill up the variables with any numbers, it doesn't matter
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
37740,AlreadyExistsError when using large tensors,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution: Ubuntu 18.04.3 LTS and Ubuntu 16.04.6 LTS
- TensorFlow installed from: pip  
- Python version: 3.6.8 and 3.7.5 
- CUDA/cuDNN version: Did not use GPU

Tensorflow version: 2.1.0

**Describe the current behavior**

The code casts 

tensorflow.python.framework.errors_impl.AlreadyExistsError

**Describe the expected behavior**

It does not cast that error

**Standalone code to reproduce the issue** 
Here's a minimal example I managed to make. The error only occurs if I use gradient-tape, if i remove the gradient tape the code runs perfectly. But I would like to be able to take the gradient.

```python
import tensorflow as tf
import numpy as np


class C:
    def __init__(self):
        n = 2000
        self.ae = tf.Variable(np.eye(n), trainable=True, dtype=tf.float32)
        self.aa = tf.Variable(np.eye(n), trainable=True, dtype=tf.float32)
        self.fr = tf.Variable(0.5, trainable=True, dtype=tf.float32)

        self.kp = tf.Variable(np.zeros(n), trainable=False, dtype=tf.float32)

    @tf.function
    def loss_op(self, k: tf.Tensor, a: tf.Tensor, s: tf.Tensor):
        l = tf.constant(0.0)

        def loop_fn(i, k, l):
            p = tf.clip_by_value(k[a[i]], 0.01, 0.99)
            l = l - (s[i] * tf.math.log(p) + (1 - s[i]) * tf.math.log(1 - p))

            at = self.aa[a[i]]

            gk = (self.ae[a[i]] - k) * at
            lk = k * at

            k = tf.clip_by_value(
                k + s[i] * gk - (1 - s[i]) * self.fr * lk, 0.0, 1.0)
            return i + 1, k, l

        def loop_cond(i: tf.Tensor, _, __):
            return tf.logical_and(tf.greater_equal(s[i], 0), tf.less(i, 199))

        _, _, l = tf.while_loop(loop_cond, loop_fn, (0, k, l), back_prop=True)
        return l

    @tf.function
    def regularizer(self, tensor: tf.Tensor):
        return tf.reduce_sum(tf.math.log(tf.abs(tensor) + 1))

    @tf.function
    def train_op(self, a, s, opt):
        with tf.GradientTape() as tape:
            loss = self.loss_op(self.kp, a, s)

            aal = self.regularizer(self.aa)
            al = self.regularizer(self.ae)

            o = loss + 0.5 * (aal + al)

        train_vars = [self.ae, self.aa, self.fr]

        gradient = tape.gradient(o, train_vars)
        opt.apply_gradients(zip(gradient, train_vars))

        return loss

c = C()

o = tf.optimizers.Adam(learning_rate=1e-3)

a = np.arange(200, dtype=np.int32)
s = np.ones(200, dtype=np.float32)

c.train_op(a, s, o)
```

Here's a stack-trace if that is helpful

```bash
2020-03-20 09:56:30.208767: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/StatefulPartitionedCall_3/gradient
s/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE                                                                                            
2020-03-20 09:56:30.209510: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Already exists: Resource __per_step_0/StatefulPartitionedCall_
3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE                                                                                  
         [[{{node StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var}}]]
2020-03-20 09:56:30.209650: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
2020-03-20 09:56:30.209813: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
2020-03-20 09:56:30.219319: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
Traceback (most recent call last):
  File ""error.py"", line 69, in <module>
    c.train_op(a, s, o)
  File ""/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 632, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2363, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1611, in _filtered_call
    self.captured_inputs)
  File ""/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1692, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 545, in call
    ctx=ctx)
  File ""/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.AlreadyExistsError:  Resource __per_step_0/StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
         [[{{node StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var}}]] [Op:__inference_train_op_806]

Function call stack:
train_op
```
"
37739,"kernel_concatenation_test fails with ""Aborted (Core dumped)""","@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 765ddddb22
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Linux x86_64

**Describe the problem**
Running
`$ make -f tensorflow/lite/micro/tools/make/Makefile test_kernel_concatenation_test`

results in the following output
`tensorflow/lite/micro/testing/test_linux_binary.sh: line 46:  4201 Aborted                 (core dumped) $1 > ${MICRO_LOG_FILENAME} 2>&1
tensorflow/lite/micro/tools/make/Makefile:321: recipe for target 'test_kernel_concatenation_test' failed
make: *** [test_kernel_concatenation_test] Error 134
`

**Please provide the exact sequence of commands/steps when you ran into the problem**
`$ make -f tensorflow/lite/micro/tools/make/Makefile test_kernel_concatenation_test`
"
37738,Support SparseTensor in tf.keras.Discretization layer.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF2.1 and TF2.2-rc0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Feature: [tf.keras.preprocessing.Discretization](https://github.com/tensorflow/tensorflow/blob/765ddddb2278551441ecaf45528898cd839df5c9/tensorflow/python/keras/layers/preprocessing/discretization.py#L33) can process the SparseTensor inputs.
Current State: This layer cannot process SparseTensor.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
The model developer who want to discretize the value from Sparse Input. It's a common for recommendation & ranking scenario.

**Any Other info.**
"
37737,Training Keras models with moving averages on Estimator,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: - Bazel
version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I am using code snippets like this to define a Keras model with moving averages of its weights:

``` Python
model = ...
ema = tf.train.ExponentialMovingAverage(0.995)
model.add_update(ema.apply())
```

However, the update op is lost after the model is converted into an estimator:

``` Python
model.compile(...)
estimator = tf.keras.estimator.model_to_estimator(model)  # The above update op is lost here
```

Investigation shows that it is lost during the cloning of the model.

**Describe the expected behavior**

The update op should be kept as is, and an estimator should be able to train such a model.

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
37736,questions about transformer.ipynb tutorial,"Hi. I am new to transformer and I'm trying to understand this transformer tutorial (https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb).

In this tutorial, a transformer has encoders, decoders, and a final linear layer.
But in the paper, a transformer has a softmax layer after the final linear layer.
I think that `predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)` line correctly returns the  expected output anyway, but I just want to know why the softmax layer is not implemented in this tutorial. If I add a softmax layer after the final linear layer and train the model, will the prediction result be different?

And why this tutorial used two separate vocabs instead of a shared subwords vocabulary? (maybe to keep the example simple?) If I want to use a shared vocabulary, should I implement the `shared_embedding_and_softmax_weights` part in tensor2tensor?

Thanks."
42788,Generate unexpected notebook cell in Japanese Documentation,"I found issues about after convert to docs that generated extra notebook cell.

- https://www.tensorflow.org/tutorials/keras/classification in Japanesse version

![image](https://user-images.githubusercontent.com/2786333/77142134-896c5780-6ac2-11ea-94f8-8e97e35c7521.png)

- Original: notebook 
- https://github.com/masa-ita/tf-docs/blob/b8cd622e042ce322345d5fa30858087ee1abcab1/site/ja/tutorials/keras/basic_classification.ipynb

"
37735,SavedModelEstimator not found in TF 2,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): mac-os
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: n/a
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 2.1.0
- Python version: 3.7 - Bazel
version (if compiling from source): n/a
- GCC/Compiler version (if compiling from
source): n/a
- CUDA/cuDNN version: - GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
tf.estimator.experimental.SavedModelEstimator not found
**Describe the expected behavior**
according to https://github.com/tensorflow/estimator/blob/r2.1/tensorflow_estimator/python/estimator/canned/saved_model_estimator.py#L112 it should be found at ""tf.estimator.experimental.SavedModelEstimator""
**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

>>> import tensorflow as tf
>>> tf.estimator.experimental.SavedModelEstimator
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow_estimator.python.estimator.api._v2.estimator.experimental' has no attribute 'SavedModelEstimator'

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
37733,TensorFlow Lite Error: tensorflow/lite/kernels/concatenation.cc:52 axis >= 0 was not true.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MACOS Catilina 10.15.3
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source):1.15.2


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.
git clone https://github.com/yuanlida/nc.git
Firstly run this file first and you can get saved_model in one minute.
https://github.com/yuanlida/nc/blob/master/bilstm_onefile/bi_lstm_mini_net.py
Then run this file and you can get a tflite file named bi-lstm.tflite in the path ../model/Bi-LSTM
https://github.com/yuanlida/nc/blob/master/tf_converter.py
The copy this files to the Xcode project, I can't allocateTensors.
```
# Copy and paste here the exact command
I found this problem is coused by this python method:

                _output = bidirectional_dynamic_rnn(
                    cell_fw, cell_bw, char_embeddings,
                    # sequence_length=word_lengths,
                    dtype=tf.float32,
                    time_major=True
                )

                _, ((_, output_fw), (_, output_bw)) = _output
                output = tf.concat([output_fw, output_bw], axis=-1)
The Swift code is :
      guard let modelPath = Bundle.main.path(forResource: ""bi-lstm"", ofType: ""tflite"") else {
            print(""Failed to load the model file."")
            return
        }

        var options = Interpreter.Options()
        options.threadCount = 4
            let interpreter = try Interpreter(modelPath: modelPath, options: options)
            try interpreter.allocateTensors()
            print(interpreter.inputTensorCount)
```

**The output from the converter invocation**

```
2020-03-20 07:49:14.148393+0800 SignatureTextClassfication[25938:3910176] Initialized TensorFlow Lite runtime.
TensorFlow Lite Error: tensorflow/lite/kernels/concatenation.cc:52 axis >= 0 was not true.
TensorFlow Lite Error: Node number 1 (CONCATENATION) failed to prepare.

Failed to create the interpreter with error: Failed to allocate memory for input tensors.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
[bi-lstm.tflite.zip](https://github.com/tensorflow/tensorflow/files/4357639/bi-lstm.tflite.zip)

```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
37732,TFLite Micro Speech not detecting audio on ESP-EYE,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Cross compile OSX 10.15.3 to esp xstensa
- TensorFlow installed from (source or binary):
Using Tensorflow source master branch
- Tensorflow version (commit SHA if source):
b2ea4cc49d7ff501ec7bdbfcee7bfdaa3e3befd2 and 5c4931bbf69e0f006f210c6382a234e83dd4dc8e
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):
ESP32 using esp-idf

**Describe the problem**
I'm trying to use the `micro_speech` demo for TFLite following the instructions [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech) for the `ESP-EYE`. The exact board I have can be found at [mouser](https://www.mouser.com/ProductDetail/Espressif-Systems/ESP-EYE?qs=sGAEpiMZZMu3sxpa5v1qrjQmuz3X2%252B1RFNXM50fXoAA%3D). After following the build steps I noticed that no audio input was being detected. I updated `command_responder.cc` to use the ESP logger with each call:

```cc
#include ""esp_log.h""
#include ""command_responder.h""

static const char *TAG = ""TF_LITE_COMMAND_RESPONDER"";
// The default implementation writes out the name of the recognized command
// to the error console. Real applications will want to take some custom
// action instead, and should implement their own versions of this function.
void RespondToCommand(tflite::ErrorReporter* error_reporter,
                      int32_t current_time, const char* found_command,
                      uint8_t score, bool is_new_command) {
  if (is_new_command) {
    TF_LITE_REPORT_ERROR(error_reporter, ""Heard %s (%d) @%dms"", found_command,
                         score, current_time);
    ESP_LOGI(TAG, ""Heard %s (%d) @%dms"", found_command, score, current_time);
  }
  ESP_LOGI(TAG, ""Heard %s (%d) @%dms"", found_command, score, current_time);
}
```

`build`, `flash`, `monitor` and the board doesn't appear to recognize any audio:

```bash
(37) tron:esp-idf n0mn0m$ idf.py --port /dev/cu.SLAB_USBtoUART flash monitor
Checking Python dependencies...
Python requirements from /Users/n0mn0m/projects/esp/esp-idf/requirements.txt are satisfied.
Adding flash's dependency ""all"" to list of actions
Executing action: all (aliases: build)
Running ninja in directory /Users/n0mn0m/projects/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build
Executing ""ninja all""...
[1/3] Performing build step for 'bootloader'
ninja: no work to do.
Executing action: flash
Running esptool.py in directory /Users/n0mn0m/projects/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build
Executing ""/Users/n0mn0m/.virtualenvs/37/bin/python /Users/n0mn0m/projects/esp/esp-idf/components/esptool_py/esptool/esptool.py -p /dev/cu.SLAB_USBtoUART -b 460800 --before default_reset --after hard_reset write_flash @flash_project_args""...
esptool.py -p /dev/cu.SLAB_USBtoUART -b 460800 --before default_reset --after hard_reset write_flash --flash_mode dio --flash_freq 80m --flash_size 2MB 0x8000 partition_table/partition-table.bin 0x1000 bootloader/bootloader.bin 0x10000 micro_speech.bin
esptool.py v2.8
Serial port /dev/cu.SLAB_USBtoUART
Connecting........__
Detecting chip type... ESP32
Chip is ESP32D0WDQ5 (revision 1)
Features: WiFi, BT, Dual Core, 240MHz, VRef calibration in efuse, Coding Scheme None
Crystal is 40MHz
MAC: bc:dd:c2:d0:23:4c
Uploading stub...
Running stub...
Stub running...
Changing baud rate to 460800
Changed.
Configuring flash size...
Compressed 3072 bytes to 103...
Wrote 3072 bytes (103 compressed) at 0x00008000 in 0.0 seconds (effective 2555.0 kbit/s)...
Hash of data verified.
Compressed 27296 bytes to 16010...
Wrote 27296 bytes (16010 compressed) at 0x00001000 in 0.4 seconds (effective 595.8 kbit/s)...
Hash of data verified.
Compressed 252256 bytes to 149646...
Wrote 252256 bytes (149646 compressed) at 0x00010000 in 3.7 seconds (effective 543.5 kbit/s)...
Hash of data verified.

Leaving...
Hard resetting via RTS pin...
Executing action: monitor
Running idf_monitor in directory /Users/n0mn0m/projects/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf
Executing ""/Users/n0mn0m/.virtualenvs/37/bin/python /Users/n0mn0m/projects/esp/esp-idf/tools/idf_monitor.py -p /dev/cu.SLAB_USBtoUART -b 115200 /Users/n0mn0m/projects/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/micro_speech.elf -m '/Users/n0mn0m/.virtualenvs/37/bin/python' '/Users/n0mn0m/projects/esp/esp-idf/tools/idf.py' '--port' '/dev/cu.SLAB_USBtoUART'""...
--- idf_monitor on /dev/cu.SLAB_USBtoUART 115200 ---
--- Quit: Ctrl+] | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---
ets Jun  8 2016 00:22:57

rst:0x1 (POWERON_RESET),boot:0x13 (SPI_FAST_FLASH_BOOT)
configsip: 0, SPIWP:0xee
clk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00
mode:DIO, clock div:1
load:0x3fff0018,len:4
load:0x3fff001c,len:7304
load:0x40078000,len:14944
ho 0 tail 12 room 4
load:0x40080400,len:4940
entry 0x40080704
I (64) boot: Chip Revision: 1
I (70) boot_comm: chip revision: 1, min. bootloader chip revision: 0
I (41) boot: ESP-IDF v4.0-223-gfdbdf9a0e-dirty 2nd stage bootloader
I (41) boot: compile time 15:49:55
I (42) boot: Enabling RNG early entropy source...
I (48) qio_mode: Enabling default flash chip QIO
I (53) boot: SPI Speed      : 80MHz
I (57) boot: SPI Mode       : QIO
I (61) boot: SPI Flash Size : 2MB
I (65) boot: Partition Table:
I (69) boot: ## Label            Usage          Type ST Offset   Length
I (76) boot:  0 nvs              WiFi data        01 02 00009000 00006000
I (83) boot:  1 phy_init         RF data          01 01 0000f000 00001000
I (91) boot:  2 factory          factory app      00 00 00010000 00100000
I (98) boot: End of partition table
I (103) boot_comm: chip revision: 1, min. application chip revision: 0
I (110) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x0d5c0 ( 54720) map
I (134) esp_image: segment 1: paddr=0x0001d5e8 vaddr=0x3ffb0000 size=0x020e8 (  8424) load
I (137) esp_image: segment 2: paddr=0x0001f6d8 vaddr=0x40080000 size=0x00400 (  1024) load
0x40080000: _WindowOverflow4 at /Users/n0mn0m/projects/esp/esp-idf/components/freertos/xtensa_vectors.S:1778

I (141) esp_image: segment 3: paddr=0x0001fae0 vaddr=0x40080400 size=0x00530 (  1328) load
I (150) esp_image: segment 4: paddr=0x00020018 vaddr=0x400d0018 size=0x245b8 (148920) map
0x400d0018: _stext at ??:?

I (199) esp_image: segment 5: paddr=0x000445d8 vaddr=0x40080930 size=0x09358 ( 37720) load
I (218) boot: Loaded app from partition at offset 0x10000
I (218) boot: Disabling RNG early entropy source...
I (218) cpu_start: Pro cpu up.
I (222) cpu_start: Application information:
I (227) cpu_start: Project name:     micro_speech
I (232) cpu_start: App version:      v1.12.1-25881-gb2ea4cc49d
I (239) cpu_start: Compile time:     Mar 18 2020 15:49:49
I (245) cpu_start: ELF file SHA256:  22cc8962718b499b...
I (251) cpu_start: ESP-IDF:          v4.0-223-gfdbdf9a0e-dirty
I (257) cpu_start: Starting app cpu, entry point is 0x40080fe8
0x40080fe8: call_start_cpu1 at /Users/n0mn0m/projects/esp/esp-idf/components/esp32/cpu_start.c:271

I (0) cpu_start: App cpu up.
I (268) heap_init: Initializing. RAM available for dynamic allocation:
I (274) heap_init: At 3FFAE6E0 len 00001920 (6 KiB): DRAM
I (281) heap_init: At 3FFB69C8 len 00029638 (165 KiB): DRAM
I (287) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM
I (293) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM
I (300) heap_init: At 40089C88 len 00016378 (88 KiB): IRAM
I (306) cpu_start: Pro cpu start user code
I (323) spi_flash: detected chip: generic
I (323) spi_flash: flash io: qio
W (323) spi_flash: Detected size(4096k) larger than the size in the binary image header(2048k). Using the size in the binary image header.
I (334) cpu_start: Starting scheduler on PRO CPU.
I (0) cpu_start: Starting scheduler on APP CPU.
I (428) I2S: DMA Malloc info, datalen=blocksize=600, dma_buf_count=3
I (428) I2S: DMA Malloc info, datalen=blocksize=600, dma_buf_count=3
I (438) I2S: PLL_D2: Req RATE: 16000, real rate: 16025.000, BITS: 16, CLKM: 39, BCK: 8, MCLK: 4096000.000, SCLK: 512800.000000, diva: 64, divb: 4
I (558) TF_LITE_AUDIO_PROVIDER: Audio Recording started
I (1578) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @0ms
I (2478) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @1100ms
I (3268) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @2000ms
I (4068) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @2800ms
I (4868) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @3600ms
I (5568) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @4400ms
I (6258) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @5100ms
```

I will mention that the board worked with the default `Lexin` keyword demo application so the microphone is functional.

**Please provide the exact sequence of commands/steps when you ran into the problem**

`git clone` Tensorflow
Follow the instructions [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-esp32) creating the `micro_speech` project and setting the `IDF_PATH`.
Update `command_responder.cc` as mentioned above to see what is heard in the ESP monitor.
Run `idf.py build` follow by `idf.py flash monitor`.
Observe the monitor output which is showing silence on each pass while I speak loudly, softly and with environment noise.



Just to make sure, I cloned master and did this clean from the top and replicated the issue.

```bash
Last login: Wed Mar 18 20:35:13 on ttys003
(38) tron:esp-idf n0mn0m$ cd
(38) tron:~ n0mn0m$ esp
Adding ESP-IDF tools to PATH...
Checking if Python packages are up to date...
Python requirements from /Users/n0mn0m/projects/esp/esp-idf/requirements.txt are satisfied.
Added the following directories to PATH:
  /Users/n0mn0m/projects/esp/esp-idf/components/esptool_py/esptool
  /Users/n0mn0m/projects/esp/esp-idf/components/espcoredump
  /Users/n0mn0m/projects/esp/esp-idf/components/partition_table/
  /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin
  /Users/n0mn0m/.espressif/tools/esp32ulp-elf/2.28.51.20170517/esp32ulp-elf-binutils/bin
  /Users/n0mn0m/.espressif/tools/openocd-esp32/v0.10.0-esp32-20190313/openocd-esp32/bin
  /Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin
  /Users/n0mn0m/projects/esp/esp-idf/tools
Done! You can now compile ESP-IDF projects.
Go to the project directory and run:

  idf.py build

(38) tron:~ n0mn0m$ git clone https://github.com/tensorflow/tensorflow
Cloning into 'tensorflow'...
remote: Enumerating objects: 191, done.
remote: Counting objects: 100% (191/191), done.
remote: Compressing objects: 100% (109/109), done.
remote: Total 855064 (delta 91), reused 127 (delta 82), pack-reused 854873
Receiving objects: 100% (855064/855064), 495.33 MiB | 12.16 MiB/s, done.
Resolving deltas: 100% (692955/692955), done.
Checking out files: 100% (19882/19882), done.
(38) tron:~ n0mn0m$ gmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_micro_speech_esp_project
gmake: tensorflow/lite/micro/tools/make/Makefile: No such file or directory
gmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/Makefile'.  Stop.
(38) tron:~ n0mn0m$ cd tensorflow/
(38) tron:tensorflow n0mn0m$ gmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_micro_speech_esp_project
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip"" ""7e8191b24853d75de2af87622ad293ba"" tensorflow/lite/micro/tools/make/downloads/gemmlowp  
downloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://github.com/google/flatbuffers/archive/v1.12.0.tar.gz"" ""c62ffefb3d4548b127cca14ce047f16c"" tensorflow/lite/micro/tools/make/downloads/flatbuffers  
downloading https://github.com/google/flatbuffers/archive/v1.12.0.tar.gz
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://github.com/mborgerding/kissfft/archive/v130.zip"" ""438ba1fef5783cc5f5f201395cc477ca"" tensorflow/lite/micro/tools/make/downloads/kissfft patch_kissfft 
downloading https://github.com/mborgerding/kissfft/archive/v130.zip
Finished patching kissfft
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip"" ""fe2934bd0788f1dcc7af3f0a954542ab"" tensorflow/lite/micro/tools/make/downloads/person_model_grayscale  
downloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip"" ""8a7d2c70325f53136faea6dde517b8cc"" tensorflow/lite/micro/tools/make/downloads/person_model_int8  
downloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip
(38) tron:tensorflow n0mn0m$ cd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf
(38) tron:esp-idf n0mn0m$ idf.py build
Checking Python dependencies...
Python requirements from /Users/n0mn0m/projects/esp/esp-idf/requirements.txt are satisfied.
Executing action: all (aliases: build)
Running cmake in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build
Executing ""cmake -G Ninja -DPYTHON_DEPS_CHECKED=1 -DESP_PLATFORM=1 --warn-uninitialized -DWARN_UNINITIALIZED=1 -DCCACHE_ENABLE=0 /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf""...
Warn about uninitialized values.
-- Found Git: /usr/bin/git (found version ""2.21.1 (Apple Git-122.3)"") 
CMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):
  Git submodule components/bt/controller/lib is out of date.  Run 'git
  submodule update --init --recursive' to fix.
Call Stack (most recent call first):
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)
  CMakeLists.txt:2 (include)


CMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):
  Git submodule components/esp_wifi/lib_esp32 is out of date.  Run 'git
  submodule update --init --recursive' to fix.
Call Stack (most recent call first):
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)
  CMakeLists.txt:2 (include)


CMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):
  Git submodule components/lwip/lwip is out of date.  Run 'git submodule
  update --init --recursive' to fix.
Call Stack (most recent call first):
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)
  CMakeLists.txt:2 (include)


-- IDF_TARGET not set, using default target: esp32
-- The C compiler identification is GNU 8.2.0
-- The CXX compiler identification is GNU 8.2.0
-- The ASM compiler identification is GNU
-- Found assembler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc
-- Check for working C compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc
-- Check for working C compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++
-- Check for working CXX compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Project version: v1.12.1-27700-g5c4931bbf6
-- Building ESP-IDF components for target esp32
Loading defaults file /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/sdkconfig.defaults...
/Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/sdkconfig.defaults:16 CONFIG_FLASHMODE_QIO was replaced with CONFIG_ESPTOOLPY_FLASHMODE_QIO
/Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/sdkconfig.defaults:18 CONFIG_INT_WDT was replaced with CONFIG_ESP_INT_WDT
/Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/sdkconfig.defaults:19 CONFIG_TASK_WDT was replaced with CONFIG_ESP_TASK_WDT
-- Found PythonInterp: /Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python (found version ""3.8.2"") 
-- Found Perl: /opt/local/bin/perl (found version ""5.28.2"") 
-- Adding linker script /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/esp-idf/esp32/esp32_out.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp32/ld/esp32.project.ld.in
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp32/ld/esp32.peripherals.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.libgcc.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.syscalls.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.newlib-data.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.newlib-funcs.ld
-- Components: app_trace app_update asio bootloader bootloader_support bt coap console cxx driver efuse esp-tls esp32 esp_adc_cal esp_common esp_eth esp_event esp_gdbstub esp_http_client esp_http_server esp_https_ota esp_https_server esp_local_ctrl esp_ringbuf esp_rom esp_websocket_client esp_wifi espcoredump esptool_py expat fatfs freemodbus freertos heap idf_test jsmn json libsodium log lwip main mbedtls mdns mqtt newlib nghttp nvs_flash openssl partition_table protobuf-c protocomm pthread sdmmc soc spi_flash spiffs tcp_transport tcpip_adapter tfmicro ulp unity vfs wear_levelling wifi_provisioning wpa_supplicant xtensa
-- Component paths: /Users/n0mn0m/projects/esp/esp-idf/components/app_trace /Users/n0mn0m/projects/esp/esp-idf/components/app_update /Users/n0mn0m/projects/esp/esp-idf/components/asio /Users/n0mn0m/projects/esp/esp-idf/components/bootloader /Users/n0mn0m/projects/esp/esp-idf/components/bootloader_support /Users/n0mn0m/projects/esp/esp-idf/components/bt /Users/n0mn0m/projects/esp/esp-idf/components/coap /Users/n0mn0m/projects/esp/esp-idf/components/console /Users/n0mn0m/projects/esp/esp-idf/components/cxx /Users/n0mn0m/projects/esp/esp-idf/components/driver /Users/n0mn0m/projects/esp/esp-idf/components/efuse /Users/n0mn0m/projects/esp/esp-idf/components/esp-tls /Users/n0mn0m/projects/esp/esp-idf/components/esp32 /Users/n0mn0m/projects/esp/esp-idf/components/esp_adc_cal /Users/n0mn0m/projects/esp/esp-idf/components/esp_common /Users/n0mn0m/projects/esp/esp-idf/components/esp_eth /Users/n0mn0m/projects/esp/esp-idf/components/esp_event /Users/n0mn0m/projects/esp/esp-idf/components/esp_gdbstub /Users/n0mn0m/projects/esp/esp-idf/components/esp_http_client /Users/n0mn0m/projects/esp/esp-idf/components/esp_http_server /Users/n0mn0m/projects/esp/esp-idf/components/esp_https_ota /Users/n0mn0m/projects/esp/esp-idf/components/esp_https_server /Users/n0mn0m/projects/esp/esp-idf/components/esp_local_ctrl /Users/n0mn0m/projects/esp/esp-idf/components/esp_ringbuf /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom /Users/n0mn0m/projects/esp/esp-idf/components/esp_websocket_client /Users/n0mn0m/projects/esp/esp-idf/components/esp_wifi /Users/n0mn0m/projects/esp/esp-idf/components/espcoredump /Users/n0mn0m/projects/esp/esp-idf/components/esptool_py /Users/n0mn0m/projects/esp/esp-idf/components/expat /Users/n0mn0m/projects/esp/esp-idf/components/fatfs /Users/n0mn0m/projects/esp/esp-idf/components/freemodbus /Users/n0mn0m/projects/esp/esp-idf/components/freertos /Users/n0mn0m/projects/esp/esp-idf/components/heap /Users/n0mn0m/projects/esp/esp-idf/components/idf_test /Users/n0mn0m/projects/esp/esp-idf/components/jsmn /Users/n0mn0m/projects/esp/esp-idf/components/json /Users/n0mn0m/projects/esp/esp-idf/components/libsodium /Users/n0mn0m/projects/esp/esp-idf/components/log /Users/n0mn0m/projects/esp/esp-idf/components/lwip /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/main /Users/n0mn0m/projects/esp/esp-idf/components/mbedtls /Users/n0mn0m/projects/esp/esp-idf/components/mdns /Users/n0mn0m/projects/esp/esp-idf/components/mqtt /Users/n0mn0m/projects/esp/esp-idf/components/newlib /Users/n0mn0m/projects/esp/esp-idf/components/nghttp /Users/n0mn0m/projects/esp/esp-idf/components/nvs_flash /Users/n0mn0m/projects/esp/esp-idf/components/openssl /Users/n0mn0m/projects/esp/esp-idf/components/partition_table /Users/n0mn0m/projects/esp/esp-idf/components/protobuf-c /Users/n0mn0m/projects/esp/esp-idf/components/protocomm /Users/n0mn0m/projects/esp/esp-idf/components/pthread /Users/n0mn0m/projects/esp/esp-idf/components/sdmmc /Users/n0mn0m/projects/esp/esp-idf/components/soc /Users/n0mn0m/projects/esp/esp-idf/components/spi_flash /Users/n0mn0m/projects/esp/esp-idf/components/spiffs /Users/n0mn0m/projects/esp/esp-idf/components/tcp_transport /Users/n0mn0m/projects/esp/esp-idf/components/tcpip_adapter /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/components/tfmicro /Users/n0mn0m/projects/esp/esp-idf/components/ulp /Users/n0mn0m/projects/esp/esp-idf/components/unity /Users/n0mn0m/projects/esp/esp-idf/components/vfs /Users/n0mn0m/projects/esp/esp-idf/components/wear_levelling /Users/n0mn0m/projects/esp/esp-idf/components/wifi_provisioning /Users/n0mn0m/projects/esp/esp-idf/components/wpa_supplicant /Users/n0mn0m/projects/esp/esp-idf/components/xtensa
-- Configuring done
-- Generating done
-- Build files have been written to: /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build
Running ninja in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build
Executing ""ninja all""...
[425/902] Performing configure step for 'bootloader'
Warn about uninitialized values.
-- Found Git: /usr/bin/git (found version ""2.21.1 (Apple Git-122.3)"") 
CMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):
  Git submodule components/bt/controller/lib is out of date.  Run 'git
  submodule update --init --recursive' to fix.
Call Stack (most recent call first):
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)
  CMakeLists.txt:20 (include)


CMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):
  Git submodule components/esp_wifi/lib_esp32 is out of date.  Run 'git
  submodule update --init --recursive' to fix.
Call Stack (most recent call first):
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)
  CMakeLists.txt:20 (include)


CMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):
  Git submodule components/lwip/lwip is out of date.  Run 'git submodule
  update --init --recursive' to fix.
Call Stack (most recent call first):
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)
  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)
  CMakeLists.txt:20 (include)


-- The C compiler identification is GNU 8.2.0
-- The CXX compiler identification is GNU 8.2.0
-- The ASM compiler identification is GNU
-- Found assembler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc
-- Check for working C compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc
-- Check for working C compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++
-- Check for working CXX compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Project version: v4.0-223-gfdbdf9a0e-dirty
-- Building ESP-IDF components for target esp32
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp32/ld/esp32.peripherals.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.newlib-funcs.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.libgcc.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/bootloader/subproject/main/esp32.bootloader.ld
-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/bootloader/subproject/main/esp32.bootloader.rom.ld
-- Components: bootloader bootloader_support efuse esp32 esp_common esp_rom esptool_py log main micro-ecc partition_table soc spi_flash xtensa
-- Component paths: /Users/n0mn0m/projects/esp/esp-idf/components/bootloader /Users/n0mn0m/projects/esp/esp-idf/components/bootloader_support /Users/n0mn0m/projects/esp/esp-idf/components/efuse /Users/n0mn0m/projects/esp/esp-idf/components/esp32 /Users/n0mn0m/projects/esp/esp-idf/components/esp_common /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom /Users/n0mn0m/projects/esp/esp-idf/components/esptool_py /Users/n0mn0m/projects/esp/esp-idf/components/log /Users/n0mn0m/projects/esp/esp-idf/components/bootloader/subproject/main /Users/n0mn0m/projects/esp/esp-idf/components/bootloader/subproject/components/micro-ecc /Users/n0mn0m/projects/esp/esp-idf/components/partition_table /Users/n0mn0m/projects/esp/esp-idf/components/soc /Users/n0mn0m/projects/esp/esp-idf/components/spi_flash /Users/n0mn0m/projects/esp/esp-idf/components/xtensa
-- Configuring done
-- Generating done
-- Build files have been written to: /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/bootloader
[489/902] Performing build step for 'bootloader'
[1/62] Generating project_elf_src.c
[2/62] Building C object CMakeFiles/bootloader.elf.dir/project_elf_src.c.obj
[3/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/gpio_periph.c.obj
[4/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_periph.c.obj
[5/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_pm.c.obj
[6/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/cpu_util.c.obj
[7/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/sdmmc_periph.c.obj
[8/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/soc_memory_layout.c.obj
[9/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/sdio_slave_periph.c.obj
[10/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/spi_periph.c.obj
[11/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/lldesc.c.obj
[12/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_time.c.obj
[13/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_init.c.obj
[14/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_clk_init.c.obj
[15/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/soc_include_legacy_warn.c.obj
[16/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/memory_layout_utils.c.obj
[17/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_wdt.c.obj
[18/62] Building ASM object esp-idf/xtensa/CMakeFiles/__idf_xtensa.dir/debug_helpers_asm.S.obj
[19/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_slave_hal.c.obj
[20/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_sleep.c.obj
[21/62] Building C object esp-idf/xtensa/CMakeFiles/__idf_xtensa.dir/eri.c.obj
[22/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_slave_hal_iram.c.obj
[23/62] Building C object esp-idf/xtensa/CMakeFiles/__idf_xtensa.dir/debug_helpers.c.obj
[24/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_flash_hal.c.obj
[25/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_hal.c.obj
[26/62] Building C object esp-idf/xtensa/CMakeFiles/__idf_xtensa.dir/trax.c.obj
[27/62] Building C object esp-idf/efuse/CMakeFiles/__idf_efuse.dir/esp32/esp_efuse_table.c.obj
[28/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_clk.c.obj
[29/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_hal_iram.c.obj
[30/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_flash_hal_iram.c.obj
[31/62] Building C object esp-idf/log/CMakeFiles/__idf_log.dir/log.c.obj
[32/62] Building C object esp-idf/efuse/CMakeFiles/__idf_efuse.dir/src/esp_efuse_fields.c.obj
[33/62] Linking C static library esp-idf/log/liblog.a
[34/62] Building C object esp-idf/efuse/CMakeFiles/__idf_efuse.dir/src/esp_efuse_api.c.obj
[35/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_clock.c.obj
[36/62] Linking C static library esp-idf/xtensa/libxtensa.a
[37/62] Linking C static library esp-idf/soc/libsoc.a
[38/62] Building C object esp-idf/spi_flash/CMakeFiles/__idf_spi_flash.dir/spi_flash_rom_patch.c.obj
[39/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_flash.c.obj
[40/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_flash_config.c.obj
[41/62] Building C object esp-idf/efuse/CMakeFiles/__idf_efuse.dir/src/esp_efuse_utility.c.obj
[42/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_random.c.obj
[43/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/flash_partitions.c.obj
[44/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/flash_encrypt.c.obj
[45/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_common.c.obj
[46/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/esp32/secure_boot_signatures.c.obj
[47/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/esp32/bootloader_sha.c.obj
[48/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/flash_qio_mode.c.obj
[49/62] Building C object esp-idf/main/CMakeFiles/__idf_main.dir/bootloader_start.c.obj
[50/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/esp_image_format.c.obj
[51/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/esp32/flash_encrypt.c.obj
[52/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/esp32/secure_boot.c.obj
[53/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_utility.c.obj
[54/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_init.c.obj
[55/62] Building C object esp-idf/micro-ecc/CMakeFiles/__idf_micro-ecc.dir/micro-ecc/uECC.c.obj
[56/62] Linking C static library esp-idf/micro-ecc/libmicro-ecc.a
[57/62] Linking C static library esp-idf/bootloader_support/libbootloader_support.a
[58/62] Linking C static library esp-idf/efuse/libefuse.a
[59/62] Linking C static library esp-idf/spi_flash/libspi_flash.a
[60/62] Linking C static library esp-idf/main/libmain.a
[61/62] Linking C executable bootloader.elf
[62/62] Generating binary image from built executable
esptool.py v2.8
Generated /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/bootloader/bootloader.bin
[807/902] Building CXX object esp-idf/main/CMakeFiles/__idf_main.dir/esp/main.cc.obj
../main/esp/main.cc: In function 'void app_main()':
../main/esp/main.cc:34:32: warning: cast between incompatible function types from 'int (*)(int, char**)' to 'TaskFunction_t' {aka 'void (*)(void*)'} [-Wcast-function-type]
   xTaskCreate((TaskFunction_t)&tf_main, ""tensorflow"", 32 * 1024, NULL, 8, NULL);
                                ^~~~~~~
[818/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...ir/tensorflow/lite/experimental/microfrontend/lib/fft.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[819/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...nsorflow/lite/experimental/microfrontend/lib/fft_util.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[834/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/debug_log.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[836/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_error_reporter.cc.ob
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[840/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_time.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[842/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_string.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[843/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_utils.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[845/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/activations.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[848/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/memory_helpers.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[850/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...cro.dir/tensorflow/lite/micro/simple_memory_allocator.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[851/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_interpreter.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[852/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i....dir/tensorflow/lite/micro/micro_optional_debug_tools.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[853/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_allocator.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[854/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/ceil.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[855/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...cro.dir/tensorflow/lite/micro/kernels/circular_buffer.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[856/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/arg_min_max.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[857/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...ro.dir/tensorflow/lite/micro/kernels/all_ops_resolver.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[858/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/floor.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[859/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/add.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[860/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/dequantize.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[861/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/elementwise.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[862/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...micro.dir/tensorflow/lite/micro/kernels/concatenation.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[863/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...icro.dir/tensorflow/lite/micro/kernels/depthwise_conv.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[864/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/conv.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[865/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/neg.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[866/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/logistic.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[867/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/logical.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[868/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...cro.dir/tensorflow/lite/micro/kernels/fully_connected.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[869/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/pack.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[870/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/test_helpers.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[871/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/quantize.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[872/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/reduce.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[873/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/prelu.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[874/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/reshape.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[875/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/pooling.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[876/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/round.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[877/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/pad.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[878/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...rflow/lite/micro/memory_planner/greedy_memory_planner.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[879/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...rflow/lite/micro/memory_planner/linear_memory_planner.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[881/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/core/api/error_reporter.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[882/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/split.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[883/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/unpack.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[884/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/mul.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[885/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/core/api/tensor_utils.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[886/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/svdf.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[887/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...cro.dir/tensorflow/lite/micro/kernels/maximum_minimum.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[888/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...ir/tensorflow/lite/kernels/internal/quantization_util.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[889/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...micro.dir/tensorflow/lite/micro/kernels/strided_slice.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[890/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/kernels/kernel_util.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[891/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/softmax.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[892/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/core/api/op_resolver.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[893/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/testing/test_utils.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[894/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/sub.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[895/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/comparisons.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[896/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...o.dir/tensorflow/lite/core/api/flatbuffer_conversions.cc.obj
cc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++
[902/902] Generating binary image from built executable
esptool.py v2.8
Generated /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/micro_speech.bin

Project build complete. To flash, run this command:
/Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python ../../../../../../../../../../../projects/esp/esp-idf/components/esptool_py/esptool/esptool.py -p (PORT) -b 460800 --before default_reset --after hard_reset write_flash --flash_mode dio --flash_size detect --flash_freq 80m 0x1000 build/bootloader/bootloader.bin 0x8000 build/partition_table/partition-table.bin 0x10000 build/micro_speech.bin
or run 'idf.py -p (PORT) flash'
(38) tron:esp-idf n0mn0m$ ls
CMakeLists.txt		README_ESP.md		components		sdkconfig
LICENSE			build			main			sdkconfig.defaults
(38) tron:esp-idf n0mn0m$ vim main/command_responder.cc 
(38) tron:esp-idf n0mn0m$ idf.py build
Checking Python dependencies...
Python requirements from /Users/n0mn0m/projects/esp/esp-idf/requirements.txt are satisfied.
Executing action: all (aliases: build)
Running ninja in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build
Executing ""ninja all""...
[1/8] Performing build step for 'bootloader'
ninja: no work to do.
[6/6] Generating binary image from built executable
esptool.py v2.8
Generated /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/micro_speech.bin

Project build complete. To flash, run this command:
/Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python ../../../../../../../../../../../projects/esp/esp-idf/components/esptool_py/esptool/esptool.py -p (PORT) -b 460800 --before default_reset --after hard_reset write_flash --flash_mode dio --flash_size detect --flash_freq 80m 0x1000 build/bootloader/bootloader.bin 0x8000 build/partition_table/partition-table.bin 0x10000 build/micro_speech.bin
or run 'idf.py -p (PORT) flash'
(38) tron:esp-idf n0mn0m$ idf.py --port /dev/cu.SLAB_USBtoUART flash monitor
Checking Python dependencies...
Python requirements from /Users/n0mn0m/projects/esp/esp-idf/requirements.txt are satisfied.
Adding flash's dependency ""all"" to list of actions
Executing action: all (aliases: build)
Running ninja in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build
Executing ""ninja all""...
[1/3] Performing build step for 'bootloader'
ninja: no work to do.
Executing action: flash
Running esptool.py in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build
Executing ""/Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python /Users/n0mn0m/projects/esp/esp-idf/components/esptool_py/esptool/esptool.py -p /dev/cu.SLAB_USBtoUART -b 460800 --before default_reset --after hard_reset write_flash @flash_project_args""...
esptool.py -p /dev/cu.SLAB_USBtoUART -b 460800 --before default_reset --after hard_reset write_flash --flash_mode dio --flash_freq 80m --flash_size 2MB 0x8000 partition_table/partition-table.bin 0x1000 bootloader/bootloader.bin 0x10000 micro_speech.bin
esptool.py v2.8
Serial port /dev/cu.SLAB_USBtoUART
Connecting........__
Detecting chip type... ESP32
Chip is ESP32D0WDQ5 (revision 1)
Features: WiFi, BT, Dual Core, 240MHz, VRef calibration in efuse, Coding Scheme None
Crystal is 40MHz
MAC: bc:dd:c2:d0:23:4c
Uploading stub...
Running stub...
Stub running...
Changing baud rate to 460800
Changed.
Configuring flash size...
Compressed 3072 bytes to 103...
Wrote 3072 bytes (103 compressed) at 0x00008000 in 0.0 seconds (effective 2196.8 kbit/s)...
Hash of data verified.
Compressed 27296 bytes to 16009...
Wrote 27296 bytes (16009 compressed) at 0x00001000 in 0.4 seconds (effective 599.2 kbit/s)...
Hash of data verified.
Compressed 256864 bytes to 153590...
Wrote 256864 bytes (153590 compressed) at 0x00010000 in 3.7 seconds (effective 552.3 kbit/s)...
Hash of data verified.

Leaving...
Hard resetting via RTS pin...
Executing action: monitor
Running idf_monitor in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf
Executing ""/Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python /Users/n0mn0m/projects/esp/esp-idf/tools/idf_monitor.py -p /dev/cu.SLAB_USBtoUART -b 115200 /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/micro_speech.elf -m '/Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python' '/Users/n0mn0m/projects/esp/esp-idf/tools/idf.py' '--port' '/dev/cu.SLAB_USBtoUART'""...
--- idf_monitor on /dev/cu.SLAB_USBtoUART 115200 ---
--- Quit: Ctrl+] | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---
ets Jun  8 2016 00:22:57

rst:0x1 (POWERON_RESET),boot:0x13 (SPI_FAST_FLASH_BOOT)
configsip: 0, SPIWP:0xee
clk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00
mode:DIO, clock div:1
load:0x3fff0018,len:4
load:0x3fff001c,len:7304
load:0x40078000,len:14944
ho 0 tail 12 room 4
load:0x40080400,len:4940
entry 0x40080704
I (64) boot: Chip Revision: 1
I (70) boot_comm: chip revision: 1, min. bootloader chip revision: 0
I (41) boot: ESP-IDF v4.0-223-gfdbdf9a0e-dirty 2nd stage bootloader
I (41) boot: compile time 20:36:40
I (41) boot: Enabling RNG early entropy source...
I (47) qio_mode: Enabling default flash chip QIO
I (53) boot: SPI Speed      : 80MHz
I (57) boot: SPI Mode       : QIO
I (61) boot: SPI Flash Size : 2MB
I (65) boot: Partition Table:
I (69) boot: ## Label            Usage          Type ST Offset   Length
I (76) boot:  0 nvs              WiFi data        01 02 00009000 00006000
I (83) boot:  1 phy_init         RF data          01 01 0000f000 00001000
I (91) boot:  2 factory          factory app      00 00 00010000 00100000
I (98) boot: End of partition table
I (102) boot_comm: chip revision: 1, min. application chip revision: 0
I (110) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x0d740 ( 55104) map
I (134) esp_image: segment 1: paddr=0x0001d768 vaddr=0x3ffb0000 size=0x020e8 (  8424) load
I (137) esp_image: segment 2: paddr=0x0001f858 vaddr=0x40080000 size=0x00400 (  1024) load
0x40080000: _WindowOverflow4 at /Users/n0mn0m/projects/esp/esp-idf/components/freertos/xtensa_vectors.S:1778

I (141) esp_image: segment 3: paddr=0x0001fc60 vaddr=0x40080400 size=0x003b0 (   944) load
I (150) esp_image: segment 4: paddr=0x00020018 vaddr=0x400d0018 size=0x25640 (153152) map
0x400d0018: _stext at ??:?

I (200) esp_image: segment 5: paddr=0x00045660 vaddr=0x400807b0 size=0x094d8 ( 38104) load
I (219) boot: Loaded app from partition at offset 0x10000
I (219) boot: Disabling RNG early entropy source...
I (220) cpu_start: Pro cpu up.
I (223) cpu_start: Application information:
I (228) cpu_start: Project name:     micro_speech
I (233) cpu_start: App version:      v1.12.1-27700-g5c4931bbf6
I (240) cpu_start: Compile time:     Mar 19 2020 20:36:26
I (246) cpu_start: ELF file SHA256:  de88378406963121...
I (252) cpu_start: ESP-IDF:          v4.0-223-gfdbdf9a0e-dirty
I (258) cpu_start: Starting app cpu, entry point is 0x40080fe8
0x40080fe8: call_start_cpu1 at /Users/n0mn0m/projects/esp/esp-idf/components/esp32/cpu_start.c:271

I (0) cpu_start: App cpu up.
I (269) heap_init: Initializing. RAM available for dynamic allocation:
I (276) heap_init: At 3FFAE6E0 len 00001920 (6 KiB): DRAM
I (282) heap_init: At 3FFB69D8 len 00029628 (165 KiB): DRAM
I (288) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM
I (294) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM
I (301) heap_init: At 40089C88 len 00016378 (88 KiB): IRAM
I (307) cpu_start: Pro cpu start user code
I (324) spi_flash: detected chip: generic
I (324) spi_flash: flash io: qio
W (324) spi_flash: Detected size(4096k) larger than the size in the binary image header(2048k). Using the size in the binary image header.
I (336) cpu_start: Starting scheduler on PRO CPU.
I (0) cpu_start: Starting scheduler on APP CPU.
I (429) I2S: DMA Malloc info, datalen=blocksize=600, dma_buf_count=3
I (429) I2S: DMA Malloc info, datalen=blocksize=600, dma_buf_count=3
I (439) I2S: PLL_D2: Req RATE: 16000, real rate: 16025.000, BITS: 16, CLKM: 39, BCK: 8, MCLK: 4096000.000, SCLK: 512800.000000, diva: 64, divb: 4
I (559) TF_LITE_AUDIO_PROVIDER: Audio Recording started
I (1569) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @0ms
I (2469) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @1100ms
I (3259) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @2000ms
I (4069) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @2800ms
I (4869) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @3600ms
I (5569) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @4400ms
I (6259) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @5100ms
I (6969) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @5800ms
I (7579) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @6500ms
I (8169) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @7100ms
I (8759) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @7700ms
I (9259) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @8300ms
I (9769) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @8800ms
I (10269) TF_LITE_COMMAND_RESPONDER: Heard silence (64) @9300ms
I (10759) TF_LITE_COMMAND_RESPONDER: Heard silence (64) @9800ms
```
"
37731,Single-precision `tf.math.erf` with TF2 produces (slightly) different results to TF1 and scipy,"**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): macOS 10.15.2d and Colab
- TensorFlow installed from (source or
binary): binary
- TensorFlow version (use command below): v2.1.0-0-ge5bf8de410 2.1.0
- Python version: 3.7.3

**Describe the current behavior**
`tf.math.erf` with `float32` can produce slightly different results between TF1 and TF2 (scipy is consistent with TF1).

**Describe the expected behavior**
I wouldn't expect any difference between the two (although I confess I'm not sure this is actually a bug, maybe it's an expected side-effect of some other change).

**Standalone code to reproduce the issue** 
With TF2 (https://colab.research.google.com/drive/1K3QB6bmu1GmAVepPMpTNgLkIKXKKB7eX):
```
import tensorflow as tf
import scipy
import numpy as np
print(tf.math.erf(-1.0606601).numpy())
print(scipy.special.erf(np.array(-1.0606601, dtype=np.float32)))
```
produces
```
-0.86638564
-0.8663856
```

With TF1 (https://colab.research.google.com/drive/1dQZOXSX8t4Jy1ZIk9Xzs9gxpbUn8acws):
```
import tensorflow as tf
import scipy
import numpy as np
print(tf.Session().run(tf.math.erf(-1.0606601)))
print(scipy.special.erf(np.array(-1.0606601, dtype=np.float32)))
```
produces
```
-0.8663856
-0.8663856
```

With double precision the result is consistently
```
-0.8663855711671024
```"
37730,STM32F746NG Hello World example fails in /tensorflow/lite/micro/kernels/cmsis-nn/conv.cc,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=19.10
DISTRIB_CODENAME=eoan
DISTRIB_DESCRIPTION=""Pop!_OS 19.10""
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 
commit bfafc1acef59ff5a7ba2bf2675350812e552d5ad
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):
arm mbed

mbed config --list
[mbed] Global config:
GCC_ARM_PATH=/opt/gcc-arm-none-eabi-9-2019-q4-major/bin/
ARM_PATH=/opt/gcc-arm-none-eabi-9-2019-q4-major/bin/arm-none-eabi-gcc

**Describe the problem**
when building tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed$
mbed compile -m DISCO_F746NG -t GCC_ARM

the following build time error is returned : 
Compile [ 79.9%]: conv.cc
[Error] arm_nnsupportfunctions.h@505,39: 'Q31_MIN' was not declared in this scope
[Error] arm_nnsupportfunctions.h@507,18: 'Q31_MAX' was not declared in this scope
[Error] arm_nnsupportfunctions.h@660,36: 'Q31_MAX' was not declared in this scope
[Error] arm_nnsupportfunctions.h@667,64: 'Q31_MAX' was not declared in this scope
[Error] arm_nnsupportfunctions.h@668,65: 'Q31_MIN' was not declared in this scope
[Error] arm_nnsupportfunctions.h@674,49: 'Q31_MAX' was not declared in this scope
[ERROR] '_queue.SimpleQueue' object has no attribute 'queue'

and with -v added 

Compile [ 74.9%]: conv.cc
Compile: /opt/gcc-arm-none-eabi-9-2019-q4-major/bin/arm-none-eabi-g++ -std=gnu++14 -fno-rtti -Wvla -c -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -fmessage-length=0 -fno-exceptions -ffunction-sections -fdata-sections -funsigned-char -MMD -fno-delete-null-pointer-checks -fomit-frame-pointer -Os -g -DMBED_TRAP_ERRORS_ENABLED=1 -mcpu=cortex-m7 -mthumb -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -DMBED_ROM1_START=0x200000 -DMBED_ROM1_SIZE=0x100000 -DMBED_ROM_START=0x8000000 -DMBED_ROM_SIZE=0x100000 -DMBED_RAM_START=0x20010000 -DMBED_RAM_SIZE=0x40000 -DMBED_RAM1_START=0x20000000 -DMBED_RAM1_SIZE=0x10000 -D__MBED__=1 -DDEVICE_ANALOGOUT=1 -DDEVICE_STDIO_MESSAGES=1 -D__CMSIS_RTOS -DTARGET_NAME=DISCO_F746NG -DTARGET_RTOS_M4_M7 -DCOMPONENT_PSA_SRV_IMPL=1 -DARM_MATH_CM7 -DTARGET_M7 -DTARGET_STM32F7 -DDEVICE_CAN=1 -DTARGET_CORTEX -DDEVICE_PWMOUT=1 -DCOMPONENT_QSPIF=1 -D__FPU_PRESENT=1 -DTARGET_STM32F746 -D__MBED_CMSIS_RTOS_CM -DDEVICE_PORTOUT=1 -DMBED_TICKLESS -DDEVICE_ANALOGIN=1 -DDEVICE_I2C_ASYNCH=1 -DTARGET_STM -DDEVICE_CRC=1 -DTARGET_CORTEX_M -DUSE_HAL_DRIVER -DDEVICE_I2CSLAVE=1 -DDEVICE_SERIAL_FC=1 -DCOMPONENT_FLASHIAP=1 -DTARGET_STM32F746NG -DTARGET_LIKE_CORTEX_M7 -D__CORTEX_M7 -DDEVICE_USBDEVICE=1 -DTARGET_LIKE_MBED -DDEVICE_PORTIN=1 -DDEVICE_WATCHDOG=1 -DDEVICE_SERIAL_ASYNCH=1 -DDEVICE_PORTINOUT=1 -DDEVICE_SPI=1 -DTOOLCHAIN_GCC_ARM -DDEVICE_SLEEP=1 -DUSBHOST_OTHER -DDEVICE_TRNG=1 -DTARGET_FAMILY_STM32 -DDEVICE_LPTICKER=1 -DTARGET_FF_ARDUINO -DDEVICE_RTC=1 -DEXTRA_IDLE_STACK_REQUIRED -DTRANSACTION_QUEUE_SIZE_SPI=2 -DDEVICE_FLASH=1 -DDEVICE_SPI_ASYNCH=1 -DCOMPONENT_PSA_SRV_EMUL=1 -DDEVICE_INTERRUPTIN=1 -DUSE_FULL_LL_DRIVER -DDEVICE_USTICKER=1 -DTOOLCHAIN_GCC -DDEVICE_MPU=1 -DTARGET_STM_EMAC -DTARGET_RELEASE -DMBED_BUILD_TIMESTAMP=1584658515.2890425 -DTARGET_STM32F746xG -DDEVICE_RESET_REASON=1 -DDEVICE_EMAC=1 -DDEVICE_SERIAL=1 -DDEVICE_QSPI=1 -DUSB_STM_HAL -DCOMPONENT_NSPE=1 -DDEVICE_I2C=1 -DDEVICE_SPISLAVE=1 -DTARGET_DISCO_F746NG @./BUILD/DISCO_F746NG/GCC_ARM/.includes_12aa10db82ce4f9a526c01c2017b7c4b.txt -include ./BUILD/DISCO_F746NG/GCC_ARM/mbed_config.h -MD -MF BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/kernels/cmsis-nn/conv.d -o BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/kernels/cmsis-nn/conv.o ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc
[Error] arm_nnsupportfunctions.h@505,39: 'Q31_MIN' was not declared in this scope
[Error] arm_nnsupportfunctions.h@507,18: 'Q31_MAX' was not declared in this scope
[Error] arm_nnsupportfunctions.h@660,36: 'Q31_MAX' was not declared in this scope
[Error] arm_nnsupportfunctions.h@667,64: 'Q31_MAX' was not declared in this scope
[Error] arm_nnsupportfunctions.h@668,65: 'Q31_MIN' was not declared in this scope
[Error] arm_nnsupportfunctions.h@674,49: 'Q31_MAX' was not declared in this scope
[DEBUG] Return: 1
[DEBUG] Output: In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,
[DEBUG] Output:                  from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'q31_t arm_nn_sat_doubling_high_mult(q31_t, q31_t)':
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:505:39: error: 'Q31_MIN' was not declared in this scope
[DEBUG] Output:   505 |     if ((m1 == m2) && (m1 == (int32_t)Q31_MIN))
[DEBUG] Output:       |                                       ^~~~~~~
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:507:18: error: 'Q31_MAX' was not declared in this scope
[DEBUG] Output:   507 |         result = Q31_MAX;
[DEBUG] Output:       |                  ^~~~~~~
[DEBUG] Output: In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,
[DEBUG] Output:                  from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'int32_t arm_nn_exp_on_negative_values(int32_t)':
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:660:36: error: 'Q31_MAX' was not declared in this scope
[DEBUG] Output:   660 |     return SELECT_USING_MASK(mask, Q31_MAX, result);
[DEBUG] Output:       |                                    ^~~~~~~
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'
[DEBUG] Output:    45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))
[DEBUG] Output:       |                                                  ^
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'q31_t arm_nn_mult_by_power_of_two(int32_t, int32_t)':
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:667:64: error: 'Q31_MAX' was not declared in this scope
[DEBUG] Output:   667 |     result = SELECT_USING_MASK(MASK_IF_NON_ZERO(val > thresh), Q31_MAX, result);
[DEBUG] Output:       |                                                                ^~~~~~~
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'
[DEBUG] Output:    45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))
[DEBUG] Output:       |                                                  ^
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:668:65: error: 'Q31_MIN' was not declared in this scope
[DEBUG] Output:   668 |     result = SELECT_USING_MASK(MASK_IF_NON_ZERO(val < -thresh), Q31_MIN, result);
[DEBUG] Output:       |                                                                 ^~~~~~~
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'
[DEBUG] Output:    45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))
[DEBUG] Output:       |                                                  ^
[DEBUG] Output: In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,
[DEBUG] Output:                  from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'int32_t arm_nn_one_over_one_plus_x_for_x_in_0_1(int32_t)':
[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:674:49: error: 'Q31_MAX' was not declared in this scope
[DEBUG] Output:   674 |     const int64_t sum = (int64_t)val + (int64_t)Q31_MAX;
[DEBUG] Output:       |                                                 ^~~~~~~
Traceback (most recent call last):
  File ""/home/tgall/tinyml/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py"", line 555, in compile_queue
    res['command']
  File ""/home/tgall/tinyml/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py"", line 682, in compile_output
    raise ToolException(stderr)
tools.utils.ToolException: In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,
                 from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'q31_t arm_nn_sat_doubling_high_mult(q31_t, q31_t)':
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:505:39: error: 'Q31_MIN' was not declared in this scope
  505 |     if ((m1 == m2) && (m1 == (int32_t)Q31_MIN))
      |                                       ^~~~~~~
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:507:18: error: 'Q31_MAX' was not declared in this scope
  507 |         result = Q31_MAX;
      |                  ^~~~~~~
In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,
                 from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'int32_t arm_nn_exp_on_negative_values(int32_t)':
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:660:36: error: 'Q31_MAX' was not declared in this scope
  660 |     return SELECT_USING_MASK(mask, Q31_MAX, result);
      |                                    ^~~~~~~
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'
   45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))
      |                                                  ^
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'q31_t arm_nn_mult_by_power_of_two(int32_t, int32_t)':
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:667:64: error: 'Q31_MAX' was not declared in this scope
  667 |     result = SELECT_USING_MASK(MASK_IF_NON_ZERO(val > thresh), Q31_MAX, result);
      |                                                                ^~~~~~~
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'
   45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))
      |                                                  ^
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:668:65: error: 'Q31_MIN' was not declared in this scope
  668 |     result = SELECT_USING_MASK(MASK_IF_NON_ZERO(val < -thresh), Q31_MIN, result);
      |                                                                 ^~~~~~~
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'
   45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))
      |                                                  ^
In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,
                 from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'int32_t arm_nn_one_over_one_plus_x_for_x_in_0_1(int32_t)':
./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:674:49: error: 'Q31_MAX' was not declared in this scope
  674 |     const int64_t sum = (int64_t)val + (int64_t)Q31_MAX;
      |                                                 ^~~~~~~


Looking at it the declarations for Q31_MAX etc seem to be in arm_math.h which seems to be found in : 
tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/he
llo_world/mbed/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include

~/tinyml/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/he
llo_world/mbed/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include$ ls
arm_common_tables.h  arm_helium_utils.h  arm_mve_tables.h
arm_const_structs.h  arm_math.h          arm_vec_math.h

 


**Please provide the exact sequence of commands/steps when you ran into the problem**

"
37729,import tensorflow is very slow,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Ubuntu 18.04.3 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: 
- TensorFlow installed from (source or
binary): binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: Python 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from
source): 
- CUDA/cuDNN version: 
CUDA Version: 10.1
cudnn-10.1
- GPU model and memory:
TITAN RTX
24190MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I'm thinking about using tensorflow 2.x for research. However, every time I import tf, it takes about 5 seconds.  `import tensorflow` is 10 times slower than`import torch`. 

**Describe the expected behavior**
I was expecting the two to be roughly the same level for a fresh import. I know these two libraries have their own design choices. But I'm just wondering is there any chance to speed up the import statement. As a researcher, I need to constantly debug and 5s is just not good.

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```python
from timeit import default_timer as timer
import os

print('import pytorch')
start = timer()
import torch

end = timer()
print('Elapsed time: ' + str(end - start))

print('import tensorflow')
start = timer()
import tensorflow

end = timer()
print('Elapsed time: ' + str(end - start))
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
import pytorch
Elapsed time: 0.46338812075555325
import tensorflow
Elapsed time: 4.396180961281061
```
"
37728,I get completely Wrong Results/Prediction(No result or wrong predictions) when I use Tflite format.,"**System information**
- OS Platform and Distribution : Linux Ubuntu 18.04.4 LTS
- TensorFlow installed from (source or binary): pip3 install --user --upgrade tensorflow
- TensorFlow version (or github SHA if from source): 1.14.0


I get good results on frozen_inference_graph.pb but I get completely Wrong Results/Prediction(No result or wrong predictions) when I convert it to Tflite format.
I trained two models with different datasets(road signs and fruits) and got wrong results in both.

Initially I followed [this Blog](https://towardsdatascience.com/deeppicar-part-6-963334b2abe0) for road sign detection.
When I use my Tflite file in android app, it always returns 0 results . So I thought,Maybe there is something wrong with my App code. But then I created an under trained model on same data set(100 steps) and then used that in my App. This time,results were returned but obviously they were completely wrong. However,It helped me understand that there was something wrong with my Tflite file.
Note: I used Android app from official repo of Tensorflow object detection.

Then I followed [ this Medium Blog](https://medium.com/datadriveninvestor/how-to-train-your-own-custom-model-with-tensorflow-object-detection-api-and-deploy-it-into-android-aeacab7fa76f) for fruits detection.
Here also I got good results on frozen_inference_graph.pb but my tflite was giving completely wrong result by detecting any objects and background randomly. 
Here is the [notebook](https://colab.research.google.com/drive/1khXaKmKUb6OE7OAYgkj0KKM6lZ7DtaUG) for fruit detection.

I will be explaining steps I followed to generate Tflite file for road sign detection as per https://towardsdatascience.com/deeppicar-part-6-963334b2abe0

Please refer  [my Github Repo](https://github.com/omkardhawal21/my_tflite) for some essential files like my Mobilenet -SSD mOdel ,  model.ckpt-2000 , frozen_inference_graph.pb , tflite_graph.pb and sign.tflite files 

I have used a non-Quantized model for training on the same dataset and trained for 2000 steps.
My tensorflow version is 1.14

I follwed the below steps to obtained Tflite file

**1)Train Model**

python3 model_main.py \
--pipeline_config_path=/machine-learning/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \
--model_dir=/home/omkar/machine-learning/road_sign_detection/ \
--alsologtostderr \
--num_train_steps=2000 \
--num_eval_steps=50

**2) Export tflite_graph.pb using model.ckpt-2000 obtained after Training.** 

python3 export_tflite_ssd_graph.py \
--pipeline_config_path='/machine-learning/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config' \
--trained_checkpoint_prefix='/machine-learning/road_sign_detection/model.ckpt-2000' \
--output_directory='/home/omkar/machine-learning/road_sign_detection/tflite' \
--add_postprocessing_op=true

**3) Generate Tflite file from tflite_graph.pb**

tflite_convert \
--output_file='/machine-learning/road_sign_detection/tflite/mysign.tflite' \
--graph_def_file='/machine-learning/road_sign_detection/tflite/tflite_graph.pb' \ --input_arrays='normalized_input_image_tensor' \
--inference_type=FLOAT \
--output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \
--mean_values=128 \
--std_dev_values=128 \
--input_shapes=1,300,300,3 \
--change_concat_input_ranges=false \
--allow_nudging_weights_to_use_fast_gemm_kernel=true 
--allow_custom_ops

Note: I've also tried using TOCO to convert to Tflite but achieved same results.

**Failure details**
 Tflite produces completely wrong Results .
On road sign detection model, there were no results/ zero predicted objects.
On fruit detection model there were too many wrong predictions/results.

Am I doing something wrong ? 
Is there something wrong with the process,commands ?
"
37727,Issue with calling tf.device,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Ubuntu19.10
- From pip
- TensorFlow version:2.1.0
- Python version:3.7.4
- Installed using virtualenv? pip? conda?:pip
- GCC/Compiler version (if compiling from source):7.3.0
- CUDA/cuDNN version:10.1
- GPU model and memory: RTX2060 Super 8G



**Describe the problem**

When calling with
                    tf.device(tf.compat.v1.train.replica_device_setter(
                    worker_device=worker, ps_device='/cpu:0', ps_tasks=1)):
It gives error of not supporting functions. 


**Any other info / logs**
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Traceback (most recent call last):
  File ""export_detections.py"", line 38, in <module>
    with experiment._init_graph(config, with_dataset=True) as (net, dataset):
  File ""/home/hashswan/anaconda3/lib/python3.7/contextlib.py"", line 112, in __enter__
    return next(self.gen)
  File ""/home/hashswan/Desktop/SuperPoint/superpoint/experiment.py"", line 73, in _init_graph
    n_gpus=n_gpus, **config['model'])
  File ""/home/hashswan/Desktop/SuperPoint/superpoint/models/base_model.py"", line 122, in __init__
    self._build_graph()
  File ""/home/hashswan/Desktop/SuperPoint/superpoint/models/base_model.py"", line 275, in _build_graph
    self._pred_graph(self.pred_in)
  File ""/home/hashswan/Desktop/SuperPoint/superpoint/models/base_model.py"", line 223, in _pred_graph
    pred_out = self._gpu_tower(data, Mode.PRED, self.config['pred_batch_size'])
  File ""/home/hashswan/Desktop/SuperPoint/superpoint/models/base_model.py"", line 157, in _gpu_tower
    worker_device=worker, ps_device='/cpu:0', ps_tasks=1)):
  File ""/home/hashswan/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 5078, in device_v2
    raise RuntimeError(""tf.device does not support functions."")
RuntimeError: tf.device does not support functions.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

"
37726,Gradients do not exist for variables after tf.concat().,"<em>Tensorflow is unable to compute gradients after merging two variables with `tf.concat()`.</em> The issue is demonstrated in the following colab: https://colab.research.google.com/drive/1dkCcL5jfBmo47EsvmhNumjIkCGIdeFd5

**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have based my issue on codes composed from oficial tutorials.
- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): Google Colab 
- TensorFlow installed from (source or binary): Binary
 - TensorFlow version (use command below): v2.1.0-0-ge5bf8de410 2.1.0
I have tested the code also on TF 2.2 rc1.
- Python version: 3.6.9

**Describe the current behavior**

Tensorflow is unable to compute gradients after merging two variables with `tf.concat()`. The variables are not modified during training despite setting them as trainable.

**Describe the expected behavior**

Tensorflow should be able to compute gradients also for concatenated variables.

**Standalone code to reproduce the issue** 

The full test is available in Colab: [colab](https://colab.research.google.com/drive/1dkCcL5jfBmo47EsvmhNumjIkCGIdeFd5)

The most important part is the following:


```
class CustomEmbedding(tf.keras.layers.Layer):
  
  def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):
    super(CustomEmbedding, self).__init__(**kwargs)
    self.input_dim = input_dim
    self.output_dim = output_dim
    self.mask_zero = mask_zero
    self.embeddings = None
   
  def build(self, input_shape):
    e1 = self.add_weight(
      shape=(int(self.input_dim/2), self.output_dim),
      dtype=""float32"", trainable=True,
      name=""e1"")

    e2 = self.add_weight(
      shape=(self.input_dim-int(self.input_dim/2), self.output_dim),
      dtype=""float32"", trainable=True,
      name=""e2"")
    
    self.embeddings = tf.concat((e1, e2), 0)

    tf.print(self.embeddings)
    
  def call(self, inputs):
    return tf.nn.embedding_lookup(self.embeddings, inputs)
  
  def compute_mask(self, inputs, mask=None):
    if not self.mask_zero:
      return None
    return tf.not_equal(inputs, 0)

model = tf.keras.Sequential()
model.add(layers.Embedding(3,32,trainable=False))
model.add(layers.LSTM(32))
model.add(layers.Dense(16, ""relu""))
model.add(layers.Dense(2, ""softmax""))

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(data)
```


**Other info / logs** 

```
[[-0.042568922 0.302248985 0.401204079 ... -0.204555377 0.235091716 0.257138401]
 [-0.372319102 -0.415126026 0.340110391 ... -0.386968911 -0.410127133 -0.135176718]
 [0.341201216 0.208624214 0.357687324 ... 0.0621320605 0.0829377472 0.119318634]
 [0.380090982 0.0431897044 -0.2187078 ... -0.246274695 0.0664974749 0.223051161]]
Train for 400 steps
WARNING:tensorflow:Gradients do not exist for variables ['sequential_4/custom_embedding_4/e1:0', 'sequential_4/custom_embedding_4/e2:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['sequential_4/custom_embedding_4/e1:0', 'sequential_4/custom_embedding_4/e2:0'] when minimizing the loss.
400/400 [==============================] - 4s 11ms/step - loss: 0.3301 - accuracy: 1.0000
[[-0.042568922 0.302248985 0.401204079 ... -0.204555377 0.235091716 0.257138401]
 [-0.372319102 -0.415126026 0.340110391 ... -0.386968911 -0.410127133 -0.135176718]
 [0.341201216 0.208624214 0.357687324 ... 0.0621320605 0.0829377472 0.119318634]
 [0.380090982 0.0431897044 -0.2187078 ... -0.246274695 0.0664974749 0.223051161]]
```"
37725,"UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.","**System information** 
- OS Platform and Distribution: Ubuntu 18.04.
- LInux Kernel: 
> 5.3.0-40-generic #32~18.04.1-Ubuntu SMP Mon Feb 3 14:05:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux


- TensorFlow installed from source as written [here](https://www.tensorflow.org/install/source#gpu_support_2) (tried from binary also): 
- Build was with all flags OFF except cuda-Y
- TensorFlow version: 2.1.0
- Python version: 3.6.9 
- Bazel: 0.29.1
- GCC/Compiler version: 7.5.0 
- CUDA/cuDNN version: - GPU model and memory:
CUDA: 10.1
CuDNN: 7.6.5 ( tried 7.6.1 also)
GPU model: Nvidia Geforce 1660 Ti (6 GB)
------------------------------------------------------------------------------------------------------------------------------

Tried to execute simple MNIST CNN [example](https://keras.io/examples/mnist_cnn/) but with **tensorflow.keras** importing NOT keras!

**Error**
> UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[node sequential/conv2d/Conv2D (defined at <ipython-input-4-288e0dab5d94>:70) ]] [Op:__inference_distributed_function_1004]
> 
> Function call stack:
> distributed_function

------------------------------------------------------------------------------------------------------------------------------
Firstly, I got this error when installed tensorflow-gpu from binary via pip.

Then, I thought that against CUDA-10.1 I have to manually build tensorflow via bazel but after successful built - **I got the same error**. I have also tried change CuDNN version from 7.6.5 to 7.6.1 but again it did not help. 

**CUDA** downloaded **from nvidia website** and works perfectly
**CuDNN** is rightly installed by downloading it **from nvidia website** and copying into `/usr/loca/cuda-*`
```
cp -P cuda/include/cudnn.h /usr/local/cuda-${CUDA_VERSION}/include
cp -P cuda/lib64/libcudnn* /usr/local/cuda-${CUDA_VERSION}/lib64/
chmod a+r /usr/local/cuda-${CUDA_VERSION}/lib64/libcudnn*
```

If I try to train model without **Convolution layers** all works great with GPU calculations:
for example
```
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])
```

If you need some additional information, I will add it.

Please, help me solve this issue, I truly don't know what else try using. BTW. As I am concerned, I need CUDA-10.1  because only this CUDA supports my Nvidia Geforce 1660 Ti.
"
37724,[TF 2.2] TFLite Android ARM64 benchmark tool fails when compiling XNNPACK,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.3
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.2.0-rc1 and master
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): Apple clang version 11.0.0

**Describe the problem**

Compiling the TFLite benchmarking tool for Android ARM64 following the steps described [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#to-buildinstallrun) fails due to NEON float16 errors in XNNPACK with the error message posted below. This happens for both v2.2.0-rc1 and current master, but worked correctly on 2.1.0.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel build -c opt \
  --config=android_arm64 \
  tensorflow/lite/tools/benchmark:benchmark_model
```
However `--config=android_arm` compiles without issues.

**Any other info / logs**
```
ERROR: /private/var/tmp/_bazel_lukasgeiger/b38acd93c344ce8dbb5b0de713e0e337/external/XNNPACK/BUILD.bazel:1687:1: C++ compilation of rule '@XNNPACK//:neonfp16arith_ukernels' failed (Exit 1)
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:76:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc0x01234567 = vfmaq_lane_f16(vacc0x01234567, vb01234567c0, va0, 0);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:77:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc1x01234567 = vfmaq_lane_f16(vacc1x01234567, vb01234567c0, va1, 0);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:78:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc2x01234567 = vfmaq_lane_f16(vacc2x01234567, vb01234567c0, va2, 0);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:79:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc3x01234567 = vfmaq_lane_f16(vacc3x01234567, vb01234567c0, va3, 0);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:94:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc0x01234567 = vfmaq_lane_f16(vacc0x01234567, vb01234567c1, va0, 1);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:95:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc1x01234567 = vfmaq_lane_f16(vacc1x01234567, vb01234567c1, va1, 1);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:96:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc2x01234567 = vfmaq_lane_f16(vacc2x01234567, vb01234567c1, va2, 1);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:97:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc3x01234567 = vfmaq_lane_f16(vacc3x01234567, vb01234567c1, va3, 1);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:112:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc0x01234567 = vfmaq_lane_f16(vacc0x01234567, vb01234567c2, va0, 2);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:113:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc1x01234567 = vfmaq_lane_f16(vacc1x01234567, vb01234567c2, va1, 2);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:114:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc2x01234567 = vfmaq_lane_f16(vacc2x01234567, vb01234567c2, va2, 2);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:115:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc3x01234567 = vfmaq_lane_f16(vacc3x01234567, vb01234567c2, va3, 2);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:130:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc0x01234567 = vfmaq_lane_f16(vacc0x01234567, vb01234567c3, va0, 3);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:131:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc1x01234567 = vfmaq_lane_f16(vacc1x01234567, vb01234567c3, va1, 3);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:132:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc2x01234567 = vfmaq_lane_f16(vacc2x01234567, vb01234567c3, va2, 3);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:133:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
          vacc3x01234567 = vfmaq_lane_f16(vacc3x01234567, vb01234567c3, va3, 3);
                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:157:24: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
        vacc0x01234567 = vfmaq_f16(vacc0x01234567, va0, vb01234567);
                       ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:158:24: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
        vacc1x01234567 = vfmaq_f16(vacc1x01234567, va1, vb01234567);
                       ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:159:24: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'
        vacc2x01234567 = vfmaq_f16(vacc2x01234567, va2, vb01234567);
                       ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
Target //tensorflow/lite/tools/benchmark:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 473.607s, Critical Path: 33.08s
INFO: 1229 processes: 1229 local.
FAILED: Build did NOT complete successfully
```"
37723,Keras Unable to clone_model a load_model result saved in TF format,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes, see attached example
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: N/A
- TensorFlow installed from (source or
binary): Binary
- TensorFlow version (use command below): 2.0.1
- Python version: 3.6.8
- Bazel
version (if compiling from source): N/A
- GCC/Compiler version (if compiling from
source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Attempting to clone a loaded model that had been saved in TF format (i.e. a directory rather than `.h5`) creates an error.  This appears to be because the loaded model is of a slightly malformed type.

**Describe the expected behavior**

`clone_model` should be able to clone derived types.  `load_model` should return a true model instance even when loading a TF format model.

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf

# make dummy functional model
x = tf.keras.layers.Input(10)
y = tf.keras.layers.Dense(10)(x)

a = tf.keras.models.Model(x,[y])

# save model with tf format
a.save('foo_model')

print('Type of a is {0}'.format(type(a)))
print('a._is_graph_network = {0}'.format(a._is_graph_network))


# read in the model
b = tf.keras.models.load_model('foo_model')

print('Type of b is {0}'.format(type(b)))
print('b._is_graph_network = {0}'.format(b._is_graph_network))

# Try to clone b, errors because b is not of correct type.
c = tf.keras.models.clone_model(b)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Output
```
Type of a is <class 'tensorflow.python.keras.engine.training.Model'>
a._is_graph_network = True
Type of b is <class 'tensorflow.python.keras.saving.saved_model.load.Model'>
b._is_graph_network = False
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/proj/keras_load_clone.py in <module>()
     21 
     22 # Try to clone b, errors because b is not of correct type.
---> 23 c = tf.keras.models.clone_model(b)

~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/models.py in clone_model(model, input_tensors, clone_function)
    420   else:
    421     return _clone_functional_model(
--> 422         model, input_tensors=input_tensors, layer_fn=clone_function)
    423 
    424 

~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/models.py in _clone_functional_model(model, input_tensors, layer_fn)
    163                      'got a `Sequential` instance instead:', model)
    164   if not model._is_graph_network:
--> 165     raise ValueError('Expected `model` argument '
    166                      'to be a functional `Model` instance, '
    167                      'but got a subclass model instead.')

ValueError: Expected `model` argument to be a functional `Model` instance, but got a subclass model instead.
```

I've tried overwriting the `_is_graph_network` attribute, but that just pushes off the problem to another line.

The reason I have to load and then clone is so I can recreate old models with specific datatypes (because TF 2.0.1 can't save mixed_float16 models, I had to save as float32 and fix after loading).  Normally, one would just rebuild the model from the actual commands, but I've made backward-compatibility breaking changes since then.  I'd like to still be able to use my old models.  I'm looking at just updating to TF2.2, but I don't control my environment so that may not be an option."
37721,Partial shapes for keras.InputLayer with sparse=True ,"**System information**
- TensorFlow version (you are using): 2.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
It is currently not possible to use input `SparseTensor` into e.g. `keras.Sequential` with partially defined shapes. This is because `SparseTensors` need to be declared specifically, thus necessitating the use of `keras.InputLayer` as the fiirst layer in the model. 

If the shape of `keras.InputLayer` for sparse inputs is not fully defined (including the batch size), the
produced `SparseTensor` will have `dense_shape` where only the rank is defined.
This happens because unless the `dense_shape`is fully defined, a `placeholder` will be used for feeding in  the `dense_shape`, removing the partial shape information.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
This will make models of the form
```
Sequenctial([
   InputLayer(shape=(num_features,), sparse=True), 
  Dense(num_units)
])
```
work, as they require partial shape for the input of the `Dense` layer.
"
37720,tf.contrib Module not found in tensorflow. Training and graph export not possible.,"Since tuesday  around 9 AM CEST I was not able to proceed training from checkpoint or even export current checkpoint via export_inference_graph.py. Tested on both **TF 1.15 and 2.1.0** and it worked flawlessly up until yesterday. Is there any workaround currently?

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ( Google **Colab**)
Mobile device (e.g., Pixel 4, Samsung Galaxy 10) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): **1.15** and **2.1.0**
Python version: 3.6.9
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory: depends on which one Colab assigns to me
Please provide the entire URL of the model you are using?
https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config

Describe the current behavior
Using /usr/local/lib/python3.6/dist-packages
Finished processing dependencies for object-detection==0.1
env: PYTHONPATH=/content/models/research:/content/models/research/slim
Traceback (most recent call last):
File ""object_detection/builders/model_builder_test.py"", line 23, in
from object_detection.builders import model_builder
File ""/content/models/research/object_detection/builders/model_builder.py"", line 22, in
from object_detection.builders import box_predictor_builder
File ""/content/models/research/object_detection/builders/box_predictor_builder.py"", line 20, in
from object_detection.predictors import convolutional_box_predictor
File ""/content/models/research/object_detection/predictors/convolutional_box_predictor.py"", line 23, in
slim = tf.contrib.slim
AttributeError: module 'tensorflow' has no attribute 'contrib'

same with train.py:

Traceback (most recent call last):
File ""object_detection/legacy/train.py"", line 48, in
from tensorflow.contrib import framework as contrib_framework
ModuleNotFoundError: No module named 'tensorflow.contrib'

and model_main.py:
Traceback (most recent call last):
File ""object_detection/model_main.py"", line 26, in
from object_detection import model_lib
File ""/content/models/research/object_detection/model_lib.py"", line 27, in
from object_detection import eval_util
File ""/content/models/research/object_detection/eval_util.py"", line 40, in
slim = tf.contrib.slim
AttributeError: module 'tensorflow' has no attribute 'contrib'

and even export_inference_graph.py:

Traceback (most recent call last):
File ""object_detection/export_inference_graph.py"", line 108, in
from object_detection import exporter
File ""/content/models/research/object_detection/exporter.py"", line 20, in
from tensorflow.contrib.quantize.python import graph_matcher
ModuleNotFoundError: No module named 'tensorflow.contrib'

Describe the expected behavior

test the model builder:

object_detection/builders/model_builder_test.py outputs 17 in TF1(10 in TF2) successful tests

Code to reproduce the issue

!python object_detection/builders/model_builder_test.py

or just run **this notebook** in **Colab**:

https://colab.research.google.com/drive/1F4ggfg9_xnCcGSBKb5-ovYi_DQFZp-uV

** UPD: 1.14 version works partly. Training fails after the first evaluation due to typeerror in numpy with float64 to int conversion."
37718,Draw boundary box after classification,"Hi, 
My problem is: I would to draw a boundary around the object was predicted by a neural network. Here I use a vgg16 model (you can see the code below, it's very simple). But after the last line of code, where I predict the classes of the object. I would to draw a box but I don't how I can do that.
Someone have a tutorial for me ?
Thanks
[pre_entrained_model_vgg_16.py.tar.gz](https://github.com/tensorflow/tensorflow/files/4355415/pre_entrained_model_vgg_16.py.tar.gz)
"
37717,Segmentation fault with nightly-gpu docker,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: No
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): Nightly-gpu (2.2)
- Python version: - Bazel
version (if compiling from source): Python 3.6.9
- GCC/Compiler version (if compiling from
source): N/A
- CUDA/cuDNN version: - GPU model and memory: RTX 2080 Ti 11GB CUDA 10.1

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Segmentation fault (core dumped) mid-train after calling model.fit()

I use a callback to check memory and it seems like may be there is a memory leak.

**Describe the expected behavior**

Not have this happen

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Cannot provide, i have a large dataset (10gb for training inputs).

I can train fine on my local 1060 6gb card with same amount of CPU memory (32GB). But it seems like this docker image has a problem with a memory leak.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

No logs, just this error.
"
37716,get_static_value does not support tf.identity,"**System information**
- TensorFlow version (you are using): 2.1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
`get_static_value` currently cannot see through a `tf.identity`op, see the following minimal example
```
>>> @tf.function
... def test():
...     a = tf.constant([5, 4])
...     print(tf.get_static_value(a))
...     b = tf.identity(a)
...     print(tf.get_static_value(b))
...     
>>> test()
[5 4]
None
```

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
I noticed this problem while investigating the reason for `tf.sparse.retain` losing shape information, even though by definition it should return the same shape as its input. Turns out it constructs a new `SparseTensor` with `dense_shape` given by `tf.identity(input.dense_shape)`. There might be other sparse functions with similar problems, but I have not investigated.

I think improving `get_static_shape` is preferable to providing better shape info for `sparse.retain`, so for now I am not going to submit a feature request for that.

**Any Other info.**
"
37715,ImportError: DLL load failed: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 
- TensorFlow installed from (source or binary):  ?
- TensorFlow version: 2.1.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): ?
- GCC/Compiler version (if compiling from source): ?
- CUDA/cuDNN version: 10.2
- GPU model and memory: NVIDA Geforce GTX 950, 2Go



**  the problem**
Hello I installed tensorflow this morning, but the import does not work. Can anyone help me here? Thx

```
import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\tgalt\.p2\pool\plugins\org.python.pydev.core_7.5.0.202001101138\pysrc\_pydev_bundle\pydev_import_hook.py"", line 24, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\tgalt\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\tgalt\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""C:\Users\tgalt\.p2\pool\plugins\org.python.pydev.core_7.5.0.202001101138\pysrc\_pydev_bundle\pydev_import_hook.py"", line 24, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\tgalt\.p2\pool\plugins\org.python.pydev.core_7.5.0.202001101138\pysrc\_pydev_bundle\pydev_import_hook.py"", line 24, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\tgalt\.p2\pool\plugins\org.python.pydev.core_7.5.0.202001101138\pysrc\_pydev_bundle\pydev_import_hook.py"", line 24, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\tgalt\.p2\pool\plugins\org.python.pydev.core_7.5.0.202001101138\pysrc\_pydev_bundle\pydev_import_hook.py"", line 24, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\tgalt\.p2\pool\plugins\org.python.pydev.core_7.5.0.202001101138\pysrc\_pydev_bundle\pydev_import_hook.py"", line 24, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\tgalt\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\tgalt\.p2\pool\plugins\org.python.pydev.core_7.5.0.202001101138\pysrc\_pydev_bundle\pydev_import_hook.py"", line 24, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\tgalt\.p2\pool\plugins\org.python.pydev.core_7.5.0.202001101138\pysrc\_pydev_bundle\pydev_import_hook.py"", line 24, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\tgalt\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\tgalt\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\tgalt\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.


Failed to load the native TensorFlow runtime.
```


Previous steps :
I had a problem of dll before this bug so I followed the log instrusction and I installed some stuff to have msvcp140_1.dll 
then i notice I did not have CUDA installed so I installed cuda, then i got this bug.
"
37714,Tensorflow 2.2.0 does not save metrics using model.compile(metrics=['accuracy']),"Minimal example for bug:

```
import tensorflow as tf
from tensorflow.keras import layers

model = tf.keras.Sequential([
layers.Dense(64, activation='relu', input_shape=(32,)),
layers.Dense(10)])

model.compile(optimizer=tf.keras.optimizers.Adam(0.01),
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

print(model.metrics)
print(model.metrics_names)
```

output with tensorflow 2.1.0:
[<tensorflow.python.keras.metrics.MeanMetricWrapper object at 0x1497f1400>]
['loss', 'accuracy']

output with tensorflow 2.2.0rc0:
[]
[]
"
37713,Problem with documentation tutorial,"Hello tf team. I was trying the embedding tutorial as mentioned on the tf docs webpage.
[https://www.tensorflow.org/tutorials/text/word_embeddings](https://www.tensorflow.org/tutorials/text/word_embeddings)

The below two lines do give me error while I am following the same documentation as mentioned in the webpage.

train_batches = train_data.shuffle(1000).padded_batch(10)
test_batches = test_data.shuffle(1000).padded_batch(10)

![image](https://user-images.githubusercontent.com/47158509/77058288-3e3b4180-69fb-11ea-932a-df913d43c385.png)

I have manually tried to fix the error by putting padded_shapes as [None] or [None, None] but both of them have thrown error.

"
37712,make command fails due to outdated flatbuffers,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 2decf5694a

**Describe the problem**
When running
$ make -f tensorflow/lite/micro/tools/make/Makefile test
the following error occurs:

`In file included from ./tensorflow/lite/core/api/op_resolver.h:20,                                                                                                                                                                                                                         
                 from ./tensorflow/lite/micro/micro_interpreter.h:20,                                                                                                                                                                                                                      
                 from tensorflow/lite/micro/examples/person_detection_experimental/person_detection_test.cc:23:                                                                                                                                                                            
./tensorflow/lite/schema/schema_generated.h: In function ‘const char* tflite::EnumNameTensorType(tflite::TensorType)’:                                                                                                                                                                     
./tensorflow/lite/schema/schema_generated.h:416:20: error: ‘IsOutRange’ is not a member of ‘flatbuffers’                                                                                                                                                                                           `

This is because IsOutRange is not available in flatbuffer v1.11.0, which is currently downloaded by the make system. If I replace the flatbuffers directory in **tensorflow/lite/micro/tools/make/downloads** with version 1.12.0 of flatbuffer source from [here](https://github.com/google/flatbuffers/releases/tag/v1.12.0), it compiles fine.

Except for the test_micro_features_generator_test which fails due to not being able to find kiss_fft.h.

**Please provide the exact sequence of commands/steps when you ran into the problem**
$ make -f tensorflow/lite/micro/tools/make/Makefile test
"
37711,Add interface function to get the needed arena size,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Binary
- Tensorflow version (commit SHA if source): 2.1
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS

**Describe the problem**
The greedy memory planner places activation buffers in a smart way to re-use memory between layers. It would be nice to have a function that the application can call to find out what the actual memory usage turned out to be, to be able to dimension the arena buffer properly. One suggestion in to have add a method to the interpreter class, 'get_total_arena_size()', that can be called after all tensors has been allocated. Let me know if there has been some thoughts around this.

This issue is similar to #35070.

Thanks!

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
