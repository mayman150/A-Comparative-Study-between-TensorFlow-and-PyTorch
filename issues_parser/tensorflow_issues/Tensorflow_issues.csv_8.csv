Issue Number,Issue Title,Issue Body
54856,`tf.experimental.numpy.floor_divide` is inconsistent with numpy version when divide by zero,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
print(np.floor_divide(0,0)) # Outputs 0
print(tf.experimental.numpy.floor_divide(0,0 )) # InvalidArgumentError: Integer division by zero [Op:FloorDiv]
```

**Describe the current behavior**
[`tf.experimental.numpy.floor_divide`](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/floor_divide)  is TF's version of [numpy.floor_div](https://numpy.org/doc/stable/reference/generated/numpy.floor_divide.html?highlight=floor_div#numpy.floor_divide), and users would expect them to have the same behavior. In the special case of divide by zero, `numpy.floor_div` would throw `RuntimeWarning` but still give `0` as output, but `tf.experimental.numpy.floor_divide` would throw an error. 


**Describe the expected behavior**
Expect  `tf.experimental.numpy.floor_divide`  to output `0`, consistent with `numpy.floor_divide`"
54855,`tf.raw_ops.RGBToHSV` lack support for bfloat16,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
images = tf.random.uniform([1, 1, 3], dtype=tf.bfloat16)
tf.raw_ops.RGBToHSV(images=images)
```
throws error:
```
NotFoundError: Could not find device for node: {{node RGBToHSV}} = RGBToHSV[T=DT_BFLOAT16]
All kernels registered for op RGBToHSV:
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
 [Op:RGBToHSV]
```
**Describe the current behavior**
[`tf.raw_ops.RGBToHSV`](https://www.tensorflow.org/api_docs/python/tf/raw_ops/RGBToHSV) should support half, bfloat16, float32, float64 according to the document."
54851,`tf.ragged.segment_ids_to_row_splits` lack check for `out_type`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
segment_ids = [0,0,0,2,2,3,4,4,4]
res=tf.ragged.segment_ids_to_row_splits(segment_ids, out_type=1) # pass
print(res)
```

**Describe the current behavior**
[`tf.ragged.segment_ids_to_row_splits`](https://www.tensorflow.org/api_docs/python/tf/ragged/segment_ids_to_row_splits?hl=en) should check `out_type` is a valid DType. In the example code, `out_typt` is `1`, so it should raise an error instead of silently pass."
54849,`tf.ragged.row_splits_to_segment_ids` lack input validation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
splits = [-16, 4, 2, 5, 5, 7]
result = tf.ragged.row_splits_to_segment_ids(splits) # pass, but it should throw ValueError as splits starts with -16
print(result) 
```

**Describe the current behavior**
[`tf.ragged.row_splits_to_segment_ids`](https://www.tensorflow.org/api_docs/python/tf/ragged/row_splits_to_segment_ids?hl=en) should check `splits` starts with `0`, and throw `ValueError` if invalid."
54848,`tf.linalg.tensor_diag_part` lack input dimension checking,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
input = 6.0
tf.linalg.tensor_diag_part(input) # pass, but it should fail instead
```

**Describe the current behavior**
[`tf.linalg.tensor_diag_part`](https://www.tensorflow.org/api_docs/python/tf/linalg/tensor_diag_part) should accept `input` with rank `2k`, and at least dim 2. It should throw an error in the example code where the shape in `[]`."
54841,Unexpected outputs in MaxPooling layer,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 2.8.0
- Python version: 3.7.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: using CPU
- GPU model and memory: using CPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When I passes NaN inputs to the MaxPooling2D layer, it returns a large negative values -3.4028235e+38. 

**Describe the expected behavior**
Normally, it should return NaN values in the corresponding location, just like the other frameworks, i.e. Theano.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
https://colab.research.google.com/drive/13SEcY5Lzj9fLQTv-JSNWA7cgnirNWnh6?usp=sharing#scrollTo=gZUgM5985nJs

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54835,IndexError: tuple index out of range in tf.Model.fit(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **I am using a tutorial script, adapted for a custom dataset** (Tutorial: Training and evaluation with the built-in methods)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows11 Home (10.0.22000 Build 22000)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n.a**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **v1.12.1-71979-ge2609918a1c 2.9.0-dev20220301**
- Python version: **Python 3.10.2**
- Bazel version (if compiling from source): **n.a.**
- GCC/Compiler version (if compiling from source): **n.a.**
- CUDA/cuDNN version: **n.a.**
- GPU model and memory: **n.a.**

**Describe the current behavior**
tf.model.fit() does not show any output and eventually generates an error:

**Traceback (most recent call last):
  File ""g:\My Drive\Personal\Spider\Datasets\ModelEGame.py"", line 92, in <module>
    history = model.fit(
  File ""C:\Users\jordi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\jordi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\tensorflow\python\framework\tensor_shape.py"", line 908, in __getitem__
    return self._dims[key]
IndexError: tuple index out of range**

**Describe the expected behavior**
The model should get trained with the data provided.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.python.ops.numpy_ops import np_config

# Create a dictionary describing the features.
observ = {
        'data' : tf.io.FixedLenFeature([104], tf.int64),
        'label' : tf.io.FixedLenFeature([], tf.int64) }

# Parse the input tf.train.Example proto using the dictionary above.
def _map_function_1(example_proto):
  return tf.io.parse_single_example(example_proto, observ)

def split_data_label(sample):
    return sample['data'].astype('int16'), sample['label'].astype('int16')

# Import End game data
def load_data() :
    dataset = tf.data.TFRecordDataset('EndGsh.tfrecords')
    parsed_dataset = dataset.map(_map_function_1)
    parsed_and_split_datased = parsed_dataset.map(split_data_label)

    DATASET_SIZE = 10000 # 2836995
    train_size = int(0.7 * DATASET_SIZE)
    val_size = int(0.10 * DATASET_SIZE)
    test_size = int(0.20 * DATASET_SIZE)

    parsed_and_split_datased = parsed_and_split_datased.shuffle(DATASET_SIZE)

    train_dataset = parsed_and_split_datased.take(train_size)
    tAv_dataset = parsed_and_split_datased.skip(train_size)
    val_dataset = tAv_dataset.take(val_size)
    test_dataset = tAv_dataset.skip(val_size)

    return train_dataset, test_dataset, val_dataset

# Define model
np_config.enable_numpy_behavior()

model = keras.Sequential()
model.add(keras.Input(shape=(104,)))
model.add(layers.Dense(64, activation=""relu"", name=""end_game_in""))
model.add(layers.Dense(32, activation=""relu"", name=""end_game_h1""))
model.add(layers.Dense(16, activation=""relu"", name=""end_game_h2""))
model.add(layers.Dense(1, activation=""relu"", name=""end_game_out""))
model.summary()

train_dataset , test_dataset, val_dataset = load_data()

model.compile(
    optimizer=keras.optimizers.RMSprop(),  # Optimizer
    # Loss function to minimize
    loss=keras.losses.SparseCategoricalCrossentropy(),
    # List of metrics to monitor
    metrics=[keras.metrics.SparseCategoricalAccuracy()],
)

print(""Fit model on training data"")
x_train =[]
y_train = []
count = 0
for it in train_dataset :
    x , y = it
    x_train.append(x.astype(""float32""))
    y_train.append(y.astype(""float32""))
    count += 1
    if count % 5000 == 0:
        print(""Train: "", count)

x_val =[]
y_val = []
count = 0
for it in val_dataset :
    x , y = it
    x_val.append(x.astype(""float32""))
    y_val.append(y.astype(""float32""))
    count += 1
    if count % 5000 == 0:
        print(""Val: "", count)

history = model.fit(
    x_train,
    y_train,
    batch_size=64,
    epochs=5,
    verbose=2,
    # We pass some validation for
    # monitoring validation loss and metrics
    # at the end of each epoch
    validation_data=(x_val, y_val),
)

# rest of script happens after the error
history.history

# Evaluate the model on the test data using `evaluate`
print(""Evaluate on test data"")

x_test =[]
y_test = []
count = 0
for it in val_dataset :
    x , y = it
    x_test.append(x.astype(""float32""))
    y_test.append(y.astype(""float32""))
    count += 1
    if count % 5000 == 0:
        print(""Test: "", count)

results = model.evaluate(x_test, y_test, batch_size=128)
print(""test loss, test acc:"", results)

# Generate predictions (probabilities -- the output of the last layer)
# on new data using predict
print(""Generate predictions for 3 samples"")
predictions = model.predict(x_test[:3])
print(""predictions shape:"", predictions.shape)
```

Any data set of x_train.shape() = (104,) y_train.shape() = () produces the same behavior. Script provided uses 10,000 examples, but lower and higher numbers produce same result.

**Other info / logs**

I have tested it in a different computer and got a similar (yet different) error:

**Traceback (most recent call last):
  File ""C:\Users\jordi\Google Drive\Personal\Spider\Datasets\modelegame.py"", line 92, in <module>
    history = model.fit(
  File ""C:\Users\jordi\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\jordi\AppData\Roaming\Python\Python310\site-packages\tensorflow\python\framework\tensor_shape.py"", line 902, in __getitem__
    return self._dims[key].value
IndexError: list index out of range**"
54832,Not able to cluster Conv1DTranspose layer,"Hi, 

I am using this code for clustering the conv1d layers.

```
`# Create a base model
base_model = setup_model()
base_model.load_weights(pretrained_weights)


def apply_clustering_to_dense(layer):
  if isinstance(layer, tf.keras.layers.Conv1D):
    return cluster_weights(layer, **clustering_params)
  return layer

clustered_model = tf.keras.models.clone_model(
    base_model,
    clone_function=apply_clustering_to_dense,
)`
```

I am getting below error. Is Conv1DTranspose clustering is possible or not? Can anyone suggest how to solve this.
 
**`Please initialize `Cluster` with a supported layer. Layers should either be a `ClusterableLayer` instance, or should be supported by the ClusteringRegistry. You passed: <class 'keras.layers.convolutional.Conv1DTranspose'>`**


--------------------------------------------------------------------------------------
<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` : v2.6.0-rc2-32-g919f693420e 2.6.0

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54828,How to arrange tensorboard's graphs horizontally in tensorflow 2.x?,"I use the following code, the drawing is arranged vertically, how to change it to horizontal arrangement?

```python
with self.summary_writer.as_default():
    tf.summary.scalar(""loss"", self.loss[-1], step=self.steps)
    tf.summary.scalar(""reward"", self.rewards[-1], step=self.steps)
    tf.summary.scalar(""average_rewards"", np.nanmean(self.rewards[-1000:]), step=self.steps)
```
I didn't find it in the homepage of tensorflow.
[https://www.tensorflow.org/api_docs/python/tf/summary/scalar](https://www.tensorflow.org/api_docs/python/tf/summary/scalar)

![1](https://user-images.githubusercontent.com/26672425/156324352-bc39cf12-fd50-4d16-881a-c92721920ffa.png)

"
54822,cmake error: when build libtensorflow-lite.a for arm64.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.6.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: latest version 2.8.0 [master branch]
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
git clone latest master source, and use this CLI command in /tensorflow/lite dir:
`cmake ../ -DCMAKE_TOOLCHAIN_FILE=${ANDROID_NDK}/build/cmake/android.toolchain.cmake \
        -DTFLITE_ENABLE_GPU=ON  \
        -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=21 -DABSL_PROPAGATE_CXX_STD=ON
`

it's OK to build static library libtensorflow-lite.a 
BUT, when i include ""c_api.h"" and related header(e.g: c_api_types.h) and link libtensorflow-lite.a in my project.
it comes compiling err.
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

cmake err log: (so many same style errs. seems like absl lib is not linked in libtensorflow-lite.a？？)
`/TFLite/TF_master/tensorflow/tensorflow/lite/delegates/gpu/common/model_builder_helper.cc:129:  undefined reference to absl::lts_20211102::OutOfRangeError(absl::lts_20211102::string_view)
/TFLite/TF_master/tensorflow/tensorflow/lite/delegates/gpu/common/model_builder_helper.cc:150: undefined reference to absl::lts_20211102::UnavailableError(absl::lts_20211102::string_view)
`

and then i add this code in /tensorflow/lite/CMakeList.txt:

`
find_package(absl REQUIRED)
if(absl_FOUND)
  message(""absl found!"")
endif()
`
BUT there is no ""absl found!"" log. so there is no absl lib found??? but it's really confused..."
54819,nn.PixelShuffle的pytorch到tflite,"How to convert nn.PixelShuffle from pytorch to tflite？
"
54818,`tf.experimental.numpy.sqrt` lack support for bfloat16,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
x = tf.random.uniform([2,1], dtype=tf.bfloat16) 
print(tf.math.sqrt(x)) # pass
print(tf.experimental.numpy.sqrt(x)) # AttributeError
```

**Describe the current behavior**
`tf.experimental.numpy.sqrt` cannot accept a tensor of type `bfloat16`. 
For the above code snippet, the error message is:
```
AttributeError: 
        'EagerTensor' object has no attribute 'astype'.
        If you are looking for numpy-related methods, please run the following:
        from tensorflow.python.ops.numpy_ops import np_config
        np_config.enable_numpy_behavior()
```

"
54817,Custom LSTMCell has cell state gradients being zeros,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.3.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.6.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Let c_t be the cell state at step t, c_0 the initial cell state of the same shape. I am trying to compute dc_t/dc_0 for every step as an indicative statistic for the extent of vanishing gradients in LSTM. On a text comment dataset, where each text sequence is labeled with a binary index as target, I have constructed an LSTM and computed and plotted the gradients over steps. 

For the first 20 steps the gradients are decreasing drastically, demonstrating behaviour of vanishing gradients. For the last 20 steps they are completely zero. 

In the reproducible example below I have used random uniform inputs. The same LSTM model and gradient computation process return me the exact SAME behaviour as it behaves on the text data. There is probably something wrong with my code that the LSTM is missing something when computing the gradients.

**Describe the expected behavior**
The above-described gradients dc_t/dc_0 should stay away from zero, as theory of LSTM suggests that it is resistant against vanishing gradients. i.e. LSTM can capture (moderately) long-term dependencies, so dc_t/dc_0 with t being a few hundred steps should not vanish to zero so drastically. Of course longer temporal-dependency e.g. t = 1000 will cause this gradient to vanish. Models like transformers will do better.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import numpy as np
from numpy import array, asarray, zeros
import pandas as pd 
from tqdm import tqdm
from sklearn.model_selection import train_test_split

import tensorflow as tf
from keras import Input, Model
from keras.models import Sequential
from keras.layers.recurrent import LSTM, GRU, SimpleRNN
from keras.layers.core import Dense, Activation, Dropout, Flatten
from keras.layers.embeddings import Embedding
from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline
from keras.layers import RNN, LSTMCell, Flatten, Bidirectional, SpatialDropout1D
from keras.preprocessing import sequence, text
from keras.callbacks import EarlyStopping, LambdaCallback, TensorBoard
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from keras import backend as k
import os 


# For each c_t, compute gradient dc_t/dc_0:
batch_size = 100; input_length_m = 20
xtr_pad = tf.random.uniform((batch_size*2, input_length_m), maxval = 500, dtype=tf.int32)
ytr = tf.random.normal((batch_size*2, input_length_m, 200))

inp= Input(batch_shape= (batch_size, input_length_m), name= 'input') 
emb_out= Embedding(500, 100, input_length= input_length_m, trainable= False, name= 'embedding')(inp)

class LSTMCellwithStates(LSTMCell):
    def call(self, inputs, states, training=None):
        real_inputs = inputs[:,:self.units] # decouple [h, c]
        outputs, [h,c] = super().call(real_inputs, states, training=training)
        return tf.concat([h, c], axis=1), [h,c]
    
rnn = RNN(LSTMCellwithStates(200), return_sequences= True, return_state= False, name= 'LSTM') 

h0 = tf.Variable(tf.random.uniform((batch_size, 200)))
c0 = tf.Variable(tf.random.uniform((batch_size, 200)))
rnn_allstates= rnn(emb_out, initial_state=[h0, c0])  

model_lstm_mod = Model(inputs=inp, outputs= rnn_allstates[:, :, 200:], name= 'model_LSTMCell')
model_lstm_mod.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

### Compute gradients: 

ds = tf.data.Dataset.from_tensor_slices((xtr_pad, ytr)).batch(100)

@tf.function
# Compute gradients
def compute_dct_dc0(t, x, c0):
    return tf.gradients(model_lstm_mod(x)[:,t,:], c0)

n_b = int(xtr_pad.shape[0]/ 100)  # 200 batches
n_steps = 20   # look up only the first and last 20 steps

dctdc0_all= tf.zeros([n_b, n_steps])
for b, (x_batch_train, y_batch_train) in enumerate(ds):  # batches 0,1
    grad_batch= []   # a list of 1403 scalar gradients on the current batch
    for t in range(n_steps):  
        # steps 0,...,19
        dctdc0_b_t = compute_dct_dc0(t, x_batch_train, c0)  # (batch_size, n_units)
        grad_t = tf.reduce_mean(abs(dctdc0_b_t[0]), [0,1]) # Scalar dctdc0 at the current batch and step
        print('step', t+1, 'of batch' ,b+1, 'done')
        grad_batch.append(grad_t)
    
    dctdc0_all= tf.concat([dctdc0_all, [grad_batch]], axis = 0)   

dctdc0_agg= tf.reduce_mean(dctdc0_all, 0)  # take rowmean to obtain a vector of shape (20,)
print(dctdc0_agg.shape)

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The LSTM gradient plot for the reproducible eg:

<img width=""647"" alt=""Screen Shot 2022-03-02 at 4 31 31 pm"" src=""https://user-images.githubusercontent.com/96099256/156301257-7df3a396-fe2a-4122-be3a-51f90a4a1774.png"">

Plots for first and last 20 steps on text data:
<img width=""620"" alt=""Screen Shot 2022-03-02 at 4 29 00 pm"" src=""https://user-images.githubusercontent.com/96099256/156301342-370fb553-40c8-425a-9963-fd5da2d53199.png"">


<img width=""617"" alt=""Screen Shot 2022-03-02 at 4 33 00 pm"" src=""https://user-images.githubusercontent.com/96099256/156301388-7119d512-3333-4a84-b0a5-70a3710fbd97.png"">



"
54790,tf.rank returns incorrect value in trace-compiled conditional when a SparseTensor is involved,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.9
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.7.7
- CUDA/cuDNN version: CUDA 11.2.152 / cuDNN 8.1.1
- GPU model and memory: GeForce RTX 2080 Ti 11GBytes

**Describe the current behavior**
`tf.rank()` returns the wrong value (at runtime) when used for conditional branching in trace-compiled code and when one of branches involves a `SparseTensor`.
Specifically, `tf.rank(v)` returns `2` for a tensor `v` of shape `(2,10,3)` when called as part of a conditional test, when one of the execution branches runs `tf.sparse.sparse_dense_matmul(L,v)` where `L` is a sparse matrix and `v` is a dense tensor.

**Describe the expected behavior**
In this scenario, I would expect `tf.rank(v)` to return `3` for a tensor `v` of shape `(2,10,3)`.

**Standalone code to reproduce the issue**
```
import tensorflow as tf

@tf.function(input_signature=[
    tf.SparseTensorSpec(shape=(None,None), dtype=tf.float32),
    tf.TensorSpec(shape=None, dtype=tf.float32)
])
def f(L,v):
    tf.print(""runtime: tf.rank(v) = "", tf.rank(v))
    tf.print(""runtime: tf.shape(v) = "", tf.shape(v))
    if (tf.rank(v) == 2):
        return tf.sparse.sparse_dense_matmul(L, v)
    else:
        return v[0,0,0] * v
        
L = tf.sparse.from_dense(tf.eye(10,dtype=tf.float32))
f(L,tf.ones((2,10,3), dtype=tf.float32))
```
The code above prints:
```
runtime: tf.rank(v) =  2
runtime: tf.shape(v) =  [2 10 3]
```
Because `tf.rank` returns the wrong value, the wrong branch is executed and results in the Traceback below.
Note that if `L` is a dense matrix then `tf.rank` returns the correct result and the correct branch is executed -- here is a version of the code above with this modification:
```
import tensorflow as tf

@tf.function(input_signature=[
    tf.TensorSpec(shape=(None,None), dtype=tf.float32),
    tf.TensorSpec(shape=None, dtype=tf.float32)
])
def f(L,v):
    tf.print(""runtime: tf.rank(v) = "", tf.rank(v))
    tf.print(""runtime: tf.shape(v) = "", tf.shape(v))
    if (tf.rank(v) == 2):
        return tf.matmul(L, v)
    else:
        return v[0,0,0] * v
        
L = tf.eye(10,dtype=tf.float32)
f(L,tf.ones((2,10,3), dtype=tf.float32))
```

**Other info / logs**
Traceback obtained when running the first code example above:
```
runtime: tf.rank(v) =  2
runtime: tf.shape(v) =  [2 10 3]
Traceback (most recent call last):
  File ""tmp/tf_cond.v3.py"", line 16, in <module>
    f(L,tf.ones((2,10,3), dtype=tf.float32))
  File ""/lucas/ilm/dept/rnd/home/sgrabli/src/fluxml/venv/lib64/python3.7/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/lucas/ilm/dept/rnd/home/sgrabli/src/fluxml/venv/lib64/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Tensor 'b' is not a matrix
         [[node cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul
 (defined at tmp/tf_cond.v3.py:11)
]] [Op:__inference_f_61]

Errors may have originated from an input operation.
Input Source operations connected to node cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul:
In[0] cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul/L:
In[1] cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul/L_1:
In[2] cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul/L_2:
In[3] cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul/v:

Operation defined at: (most recent call last)
>>>   File ""tmp/tf_cond.v3.py"", line 16, in <module>
>>>     f(L,tf.ones((2,10,3), dtype=tf.float32))
>>> 
>>>   File ""tmp/tf_cond.v3.py"", line 10, in f
>>>     if (tf.rank(v) == 2):
>>> 
>>>   File ""tmp/tf_cond.v3.py"", line 11, in f
>>>     return tf.sparse.sparse_dense_matmul(L, v)
>>> 

Function call stack:
f -> cond_true_30
```
"
54784,Could not load library cudnn_ops_infer64_8.dll. Error code 126,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform : Windows 10 Pro

- TensorFlow installed from (source or binary): Installed via pip
- TensorFlow version: 2.6.2
- Python version:: 3.6.3
- Installed using pip
- CUDA/cuDNN version: Cuda 11.6
- GPU 1080

Below are the  Environment Variables
![image](https://user-images.githubusercontent.com/37058769/156252235-8d51862f-0fd7-4ae2-a169-3df8302d33df.png)

Variable Name   Variable Value
CUDA_PATH   C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6
CUDA_PATH_V10_1    C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1
CUDA_PATH_V11_6    C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6
CuDnn   C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\include
cudnn_ops_infer64_8.dll    C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin
CUPTI   C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\extras\CUPTI\lib64

**Describe the problem**

Tensor flow produces the messages below when beginning the training of the model
2022-03-01 15:02:55.368641: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/15
Could not load library cudnn_ops_infer64_8.dll. Error code 126
Please make sure cudnn_ops_infer64_8.dll is in your library path!

**There is no cudnn_ops_infer64_8.dll** 


**Provide the exact sequence of commands / steps that you executed before running into the problem**

The python code is below.

model = LeNet.build(width=28, height=28, depth=1, classes=2)
model.compile(loss=""binary_crossentropy"", optimizer=""adam"",
	metrics=[""accuracy""])

# train the network
print(""[INFO] training network..."")
H = model.fit(trainX, trainY, validation_data=(testX, testY),
	class_weight=classWeight, batch_size=64, epochs=15, verbose=1)


I'm totally clueless as to what is going on the all was working well until I installed  v11.6
Please advise
Thanks"
54763,Square root symbol Sqrt not displayed correctly,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/sqrt
https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/rsqrt

## Description of the issue (what needs changing):
The square root symbol is not displayed in the rendered HTML documentation. Although, the source code includes the appropriate code `\sqrt{x}`. One would expect a square symbol around the x in the following image:

![image](https://user-images.githubusercontent.com/12571895/156189226-cc3865fe-cc0e-416d-a3d7-1086c776fdff.png)


- https://github.com/tensorflow/tensorflow/blob/e88293b0d59a237af1e2a299048b783f5331b7fb/tensorflow/core/api_def/base_api/api_def_Rsqrt.pbtxt#L5
- https://github.com/tensorflow/tensorflow/blob/e88293b0d59a237af1e2a299048b783f5331b7fb/tensorflow/core/api_def/base_api/api_def_Sqrt.pbtxt#L5

### Clear description

The missing square symbol is confusing for someone reading the documentation.

### Fix

One could change every occurrence of the sqrt symbol to `x^{1/2}`. However, this is not a good solution and we should rather fix the display issue itself. "
54753,model.save fails with ValueError __inference_conv2d_transpose_layer_call_fn_4530 when Conv2DTranspose is quantization aware,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary):  pip install
- TensorFlow version: 
        tf=2.7.0, tensorflow_model_optimization=0.7.1
	tf=2.8.0, tensorflow_model_optimization=0.7.1
	tf-nightly=2.9.0dev20211222, tensorflow_model_optimization=0.7.1
- Python version: 3.7.12

**Describe the current behavior**
We save a quantization-aware keras-model in a .pb model format using `model.save()`. This operation fails with `ValueError: __inference_conv2d_transpose_layer_call_fn_4530` when our model contains a `Conv2DTranspose` layer. 
- The error is reproducible when we quantize the entire model using `tfmot.quantization.keras.quantize_model()`
- The error is also reproducible when we annotate layers using `tf.keras.models.clone_model()` apply quantization using `tfmot.quantization.keras.quantize_apply()`. Our current workaround is to not annotate `Conv2DTranspose` but this prevents us from having a fully quantization-aware model. 
 - The error is reproducible in  tf2.7.0, tf2.8.0 and tf-nightly

Saving the same model as .h5 works (unfortunately this workaround this is not suitable for us because our technical requirement is that we save a .pb-model)

**Describe the expected behavior**
`model.save()` saves a QAT  model with a  Conv2DTranspose layer in a .pb-format successfully. 

**Standalone code to reproduce the issue**
Here are collabs to reproduce the issue using a very simple model with a Conv2DTranspose layer and two ways to make a model quantization aware mentioned above:
     - [ Collab with tf2.7.0](https://colab.research.google.com/drive/1BHQeFMd0Pc8qtfjY3FBBZ2rM1m2tqs23?usp=sharing)
     - [ Collab with tf2.8.0](https://colab.research.google.com/drive/1zTzM9nJnY0NagzAcnZSlUgKW-vTUsL4F?usp=sharing)

	
**Other info / logs** 
Similar issue [#868](https://github.com/tensorflow/model-optimization/issues/868 )

```
Traceback
ValueError                                Traceback (most recent call last)
<ipython-input-7-dc1f93a93afb> in <module>()
      2 annotated_model = tf.keras.models.clone_model(base_model, clone_function=apply_quantization)
      3 q_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)
----> 4 q_aware_model.save('D:/output_folder/q_aware_model') # save keras model as .pb, fails

1 frames
/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py in map_resources(self)
    402           if capture_constant_value is None:
    403             raise ValueError(
--> 404                 f""Unable to save function {concrete_function.name} because it ""
    405                 f""captures graph tensor {capture} from a parent function which ""
    406                 ""cannot be converted to a constant with `tf.get_static_value`."")

ValueError: Unable to save function b'__inference_conv2d_transpose_layer_call_fn_4530' because it captures graph tensor 
Tensor(""model/quant_conv2d_transpose/transpose_1:0"", shape=(3, 3, 16, 16), dtype=float32) from a parent function which 
cannot be converted to a constant with `tf.get_static_value`.
```
"
54751,Inconsistent behaviour between CPU and GPU using TFLite model on iOS,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  iOS 14.7.1 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 11 Pro
- TensorFlow installed from (source or binary): binary using cocoapods
- TensorFlow version (use command below): 2.6.0
- Python version: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: Apple A13 GPU

**Describe the current behavior**
I'm running a TFLite [object detection model](https://github.com/google/mediapipe/blob/master/mediapipe/modules/objectron/object_detection_3d_sneakers_1stage.tflite) with 2 main outputs and an auxiliary output that provides a segmentation map. The model is used in iOS application via Swift TFLite API where I'm using a Metal Delegate to utilise the GPU. While running on GPU, I'm constantly receiving a tensor of zeros from the auxiliary output. On CPU however, I'm getting non-zero values (which is a desired behaviour). Switching between GPU and CPU does not effect the two main outputs. I tried manipulating different options of the MetalDelegate, but to no success.  Can you point me towards reasons for such behaviour and any possible solutions? 

**Describe the expected behavior**
Receiving non-zero values from the segmentation output, consistent with CPU performance, while running on GPU. 

**Other info / logs**
Below I'm posting a snippet from method responsible for configuration of an Interpreter. `InferenceConfig.accelerate` is a boolean flag - if true, the model runs on GPU.
```
if InferenceConfig.accelerate {
    var options = MetalDelegate.Options()

    options.isPrecisionLossAllowed = false
    options.isQuantizationEnabled = false
    options.waitType = .active
    let gpuDelegate = MetalDelegate(options: options)

    interpreter = try Interpreter(
        modelPath: InferenceConfig.modelPath!,
        delegates: [gpuDelegate]
    )
} else {
    var interpreterOptions = Interpreter.Options()
    interpreterOptions.isXNNPackEnabled = true

    interpreter = try Interpreter(
        modelPath: InferenceConfig.modelPath!,
        options: interpreterOptions
    )
}
```"
54750,Tensorflow Lite iOS framework- info.plist file missing,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Iphone 13 mini
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7r
- Python version: 3.9.5
- Installed using virtualenv? pip? conda?: venv
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): compilation is fine.
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Problem description**
Building an iOS framework does not generate info.plist which is needed when the library is used as an embed framework. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure -> use iOS
bazel build --config=ios_arm64 -c opt //tensorflow/lite/ios:dvdnr_framework

**Any other info / logs**
the framework generates fine, but info.plist should be included in the framework.
"
54749,LLVM component error.,"**System information**

OS Platform : Windows 11
TensorFlow version: 2.8
Python version: 3.9.9
Bazel version : 421
GPU model and memory: GTX1650
CUDA/CUdnn: 11.5/8.3.1

**Describe the problem**
Problem when build from source. LLVM component.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=156
INFO: Reading rc options for 'build' from c:\users\dubli\downloads\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=E:/Python3/python.exe
INFO: Reading rc options for 'build' from c:\users\dubli\downloads\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from c:\users\dubli\downloads\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=E:/Python3/python.exe --action_env PYTHON_LIB_PATH=E:/Python3/lib/site-packages --python_path=E:/Python3/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.5 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from c:\users\dubli\downloads\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file c:\users\dubli\downloads\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\dubli\downloads\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file c:\users\dubli\downloads\tensorflow\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file c:\users\dubli\downloads\tensorflow\.tf_configure.bazelrc: --copt=/arch:FMA3 --host_copt=/arch:FMA3
INFO: Found applicable config definition build:windows in file c:\users\dubli\downloads\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\dubli\downloads\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Repository llvm-project instantiated at:
  C:/users/dubli/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/dubli/downloads/tensorflow/tensorflow/workspace2.bzl:888:21: in workspace
  C:/users/dubli/downloads/tensorflow/tensorflow/workspace2.bzl:526:15: in _tf_repositories
  C:/users/dubli/downloads/tensorflow/third_party/llvm/setup.bzl:22:19: in llvm_setup
Repository rule llvm_configure defined at:
  C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/configure.bzl:83:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'llvm-project':
   Traceback (most recent call last):
        File ""C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/configure.bzl"", line 73, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/configure.bzl"", line 62, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'E:/Python3/python.exe C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/overlay_directories.py --src C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw --overlay C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""C:\users\dubli\_bazel_dubli\mnmgdhl7\external\llvm-raw\utils\bazel\overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""C:\users\dubli\_bazel_dubli\mnmgdhl7\external\llvm-raw\utils\bazel\overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""C:\users\dubli\_bazel_dubli\mnmgdhl7\external\llvm-raw\utils\bazel\overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] ╩ышхэЄ эх юсырфрхЄ ЄЁхсєхь√ьш яЁртрьш: 'C:\\users\\dubli\\_bazel_dubli\\mnmgdhl7\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'C:\\users\\dubli\\_bazel_dubli\\mnmgdhl7\\external\\llvm-project\\.bazelignore'

ERROR: Error fetching repository: Traceback (most recent call last):
        File ""C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/configure.bzl"", line 73, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/configure.bzl"", line 62, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'E:/Python3/python.exe C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/overlay_directories.py --src C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw --overlay C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""C:\users\dubli\_bazel_dubli\mnmgdhl7\external\llvm-raw\utils\bazel\overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""C:\users\dubli\_bazel_dubli\mnmgdhl7\external\llvm-raw\utils\bazel\overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""C:\users\dubli\_bazel_dubli\mnmgdhl7\external\llvm-raw\utils\bazel\overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] ╩ышхэЄ эх юсырфрхЄ ЄЁхсєхь√ьш яЁртрьш: 'C:\\users\\dubli\\_bazel_dubli\\mnmgdhl7\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'C:\\users\\dubli\\_bazel_dubli\\mnmgdhl7\\external\\llvm-project\\.bazelignore'

ERROR: C:/users/dubli/downloads/tensorflow/tensorflow/tools/pip_package/BUILD:273:10: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/compiler/mlir/tensorflow:gen_mlir_passthrough_op_py in repository @ which failed to fetch. no such package '@llvm-project//mlir': Failed to execute overlay script: 'E:/Python3/python.exe C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/overlay_directories.py --src C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw --overlay C:/users/dubli/_bazel_dubli/mnmgdhl7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""C:\users\dubli\_bazel_dubli\mnmgdhl7\external\llvm-raw\utils\bazel\overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""C:\users\dubli\_bazel_dubli\mnmgdhl7\external\llvm-raw\utils\bazel\overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""C:\users\dubli\_bazel_dubli\mnmgdhl7\external\llvm-raw\utils\bazel\overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] ╩ышхэЄ эх юсырфрхЄ ЄЁхсєхь√ьш яЁртрьш: 'C:\\users\\dubli\\_bazel_dubli\\mnmgdhl7\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'C:\\users\\dubli\\_bazel_dubli\\mnmgdhl7\\external\\llvm-project\\.bazelignore'

ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 2.208s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
    currently loading: tensorflow/compiler/mlir/tensorflow
"
54748,AttributeError: module 'tensorflow' has no attribute 'reduce_sum',"Hi while running 

python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""

To verify my Tensorflow installation I get AttributeError;

AttributeError: module 'tensorflow' has no attribute 'reduce_sum'

tensorflow version: Version: 2.5.0"
54747,Segmentation Fault (core dumped) Issue,"**System information**
- I have written custom code
- OS Platform and Distribution: CentOS Linux release 7.7.1908 (AltArch), Power9 PC with 4 GPU nodes, 1 login node and 1 compute node and it has little endian architecture
- TensorFlow installed from (source or binary): https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda/
- TensorFlow version (use command below): 2.1.3 (tensorflow-gpu)
- Python version:3.6.13
- Bazel version (if compiling from source):
-  python compiler version: GCC 7.3.0
- CUDA/cuDNN version:10.2
- GPU model and memory: NVIDIA Tesla V100, Total memory: 379140608 kB =379.140608 GB (in one of the node)



I use Supercomputer at our organization for training. I am facing the Segmentation Fault (core dumped) Issue, when I attempt to train a CNN model. The model gets trained for a smaller number of Epochs. Eg if epochs=50 the model runs fine and outputs the trained model file (using callbacks in model.fit). But when epochs are increased, to 75 then segmentation fault occurs at 54th epoch for the same dataset. I ran .py file by submitting it as a job through slurm script. Dataset size is 80,000*5600, around 9GB.

**Code**
```
import os
import os.path
import sys

import h5py
import numpy as np
import tensorflow as tf
from tensorflow import keras

def load_file(database_file, load_data=False):
    in_file  = h5py.File(database_file, ""r"")
    X_train = np.array(in_file['training/data'],dtype=np.int8)
    Y_train = np.array(in_file['training/label'])
    X_test = np.array(in_file['testing/data'], dtype=np.int8)
    Y_test = np.array(in_file['testing/label'])
    if load_data == False:
        return (X_train, Y_train), (X_test, Y_test)
    else:
        return (X_train, Y_train), (X_test, Y_test), (in_file['train/data'], in_file['test/data'])

database = ""/home/..../dataset.h5""
trained_model = ""/home..../trained_epoch_test.h5""
  
(X_train, Y_train), (X_test, Y_test) = load_file(database_file)

X_train_scaled = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_scaled = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

y_train_categorical = keras.utils.to_categorical(Y_train, num_classes=256)
y_test_categorical    = keras.utils.to_categorical(Y_test, num_classes=256)

batch_size = 300 
epochs = 100
   
classes=256
input_shape = (5600,1)     
img_input = keras.layers.Input(shape=input_shape)

x = keras.layers.Conv1D(64, 11, activation='relu', padding='same', name='block1_conv1')(img_input)
x = keras.layers.AveragePooling1D(2, strides=2, name='block1_pool')(x)

x = keras.layers.Conv1D(128, 11, activation='relu', padding='same', name='block2_conv1')(x)
x = keras.layers.AveragePooling1D(2, strides=2, name='block2_pool')(x)

x = keras.layers.Conv1D(256, 11, activation='relu', padding='same', name='block3_conv1')(x)
x = keras.layers.AveragePooling1D(2, strides=2, name='block3_pool')(x)

x = keras.layers.Conv1D(512, 11, activation='relu', padding='same', name='block4_conv1')(x)
x = keras.layers.AveragePooling1D(2, strides=2, name='block4_pool')(x)

x = keras.layers.Conv1D(512, 11, activation='relu', padding='same', name='block5_conv1')(x)
x = keras.layers.AveragePooling1D(2, strides=2, name='block5_pool')(x)

x = keras.layers.Flatten(name='flatten')(x)
x = keras.layers.Dense(4096, activation='relu', name='fc1')(x)
x = keras.layers.Dense(4096, activation='relu', name='fc2')(x)
x = keras.layers.Dense(classes, activation='softmax', name='predictions')(x)
  
model = keras.models.Model(img_input, x, name='cnn_best')
optimizer = keras.optimizers.RMSprop(lr=0.00001)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    
save_model = keras.callbacks.ModelCheckpoint(trained_model)
callbacks=[save_model]
    
model.fit(x=X_train_scaled, y=y_train_categorical, batch_size=batch_size, verbose = 1, epochs=epochs, callbacks=callbacks)
```
**Following is the logs generated after enabling fault handler() function:**
![fault handler](https://user-images.githubusercontent.com/70361953/156146446-230da029-000b-4c0f-9911-90ff9c51276e.png)

**Slurm Script used to submit the job**
#!/bin/bash
#SBATCH --partition=GPU_three
#SBATCH --nodelist=Node_03
#SBATCH --output=output
python3 train.py

"
54745,Drive ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54714,Linking process takes forever,"**System information**
- OS Platform and Distribution: Ubuntu18.04 (x86_64)
- TensorFlow installed from (source or binary): https://github.com/tensorflow/tensorflow.git
- TensorFlow version: latest
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip3
- Bazel version (if compiling from source): bazel-5.0.0-linux-arm64
- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04)
- CUDA/cuDNN version: 8.2.1.32-1+cuda10.2
- CPU model and memory: CPU:       32 core Intel Xeon E5-2667 v2 (-MT-MCP-) speed: 3292 MHz (max) / 40GB RAM



I'm trying to cross-compile Tensorflow for my NVIDIA Jetson Nano device, from a source code, on an Intel platform server machine. Got by the git clone. It seems like compilation process is finished, or the part of is finished, but the linking process stuck. It gives a tiny load on 1 of 32 cpu threads, but it already took almost a day, but nothing happens. Linking.... Also i checked, IO load, it's minimal. Is it ok, and it takes so much time, or i do something wrong ?

> 
![image](https://user-images.githubusercontent.com/4686690/156057119-5578e436-e2e3-4dd4-abbd-d9fe58465562.png)

![image](https://user-images.githubusercontent.com/4686690/156057201-384425a1-032c-4692-a719-f33192d97f24.png)

![image](https://user-images.githubusercontent.com/4686690/156057731-d5c55dc8-30cd-4306-b56e-98abbf1c1f37.png)
"
54697, multi label `class_weight` ,"hi 
i was trying to deal with multi-output classification problem on an imbalanced dataset using class_weight parameter of fit method  ; but i faced this issue; i am looking for an alternative or solution 
`class_weight` is only supported for Models with a single output.
thanks "
54694,TypeError: can't pickle _thread.RLock objects when using KerasClassifier with Keras and RandomizedSearchCV,"I created a simple neural network for binary spam/ham text classification using pretrained BERT transformer. Now I want to apply randomized search for tuning the hyperparameters. For now the only hyperparameter I would like to tune is the dropout regularization probability.

I set up a sklearn pipeline with scikeras's KerasClassifier that contains my custom `build_sequential_nn()` method. When merely fitting  the pipeline, everything is fine; however, when I pass the pipeline into a sklearn `RandomizedSearchCV`, the following error message pops up:

`TypeError: can't pickle _thread.RLock objects`

Full error message will be detailed at the end of the post.

A full reproducible code is as follows, including sample data:
```
import numpy as np
import pandas as pd 

import sklearn 
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV

from scikeras.wrappers import KerasClassifier

import tensorflow_hub as hub 
import tensorflow as tf 
import tensorflow_text
from tensorflow.keras.layers import Input, Dropout, Dense


class BertPreprocessor(TransformerMixin, BaseEstimator):
    def __init__(self, preprocessor_model):
        super().__init__()
        self.preprocessor_model = preprocessor_model
    def fit(self, X=None, y=None):
        return self 
    def transform(self, X, y=None):
        return self.preprocessor_model(X)

class BertEncoder(TransformerMixin, BaseEstimator):
    def __init__(self, encoder_model):
        super().__init__()
        self.encoder_model = encoder_model
    def fit(self, X=None, y=None):
        return self
    def transform(self, X, y=None):
        return pd.DataFrame(self.encoder_model(X)['pooled_output'])

def build_nn_sequential(dropout_prob=0.1, epochs=10): 
    model = Sequential()
    model.add(Input(shape=768, name='bert_pooled_output'))
    model.add(Dropout(dropout_prob, name='dropout'))
    model.add(Dense(1, activation='sigmoid', name='classification_output'))

    metrics_list = [
        tf.keras.metrics.AUC(name='auc'),
        tf.keras.metrics.BinaryAccuracy(name='accuracy')
    ]
    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = metrics_list)
    return model

# Automatically fetch BERT preprocessor and encoder
bert_encoder_url = ""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4""
bert_preprocessor_url = ""https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3""
bert_preprocessor_model = hub.KerasLayer(bert_preprocessor_url)
bert_encoder_model = hub.KerasLayer(bert_encoder_url)

# Get train data 
df = pd.read_json(...)   # see data also below

X_train_ = df['comment_text']
y_train_ = df['label']

pipe = Pipeline([
    ('preprocess', BertPreprocessor(bert_preprocessor_model)),
    ('encode', BertEncoder(bert_encoder_model)),
    ('model', KerasClassifier(build_fn=build_nn_sequential)),
])

# pipe.fit(X_train_, y_train_)    # This evaluates fine! 

search_spaces = ({
    'model__dropout_prob': np.logspace(np.log10(0.05), np.log10(0.67), 10),
    'model__epochs': [10, 20, 30],
})

search = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=search_spaces, 
    scoring='roc_auc',
    cv=3, 
    verbose=0
).fit(X_train_, y_train_)   # <--- row where error pops up
```

### Sample data:
```
df = pd.read_json('{""comment_text"":{""0"":""problem wanted say problem trying redirect event schedule pakistan NUMBERTAG NUMBERTAG pakistan mother fucker boy want married sister ohhhh love sister boob hmmmmm yummyy"",""1"":""get life fucking loser question ask ask katie goulet picture"",""2"":""cum drinker hey wat nigga thought u could ban took long cuz wa busy az hell recently ill keep cumming back take word cumdrinker"",""3"":""liar liar pant fire seriously looked contribution tennis portal page tennis page ha descussion ever please lie NUMBERTAG NUMBERTAG NUMBERTAG NUMBERTAG"",""4"":""stop writing p nothing discus given lack bsinc education diplomacy"",""5"":""wa fucking page one edit page"",""6"":""question mad gay"",""7"":""warning page nerd please leave one stay girl though pleeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaassssssssssssssssseeeeeeeeeeeee NUMBERTAG oneoneoneoneoneoneoneoneoneoneoenone"",""8"":""full shit"",""9"":""go fuck conrad black cheated thousand people pension anyone defends hm asshole apologist evil"",""10"":""list office bearer national union student australia wp userfy userfied page located"",""11"":""talk history scottish national party claim spying hi sentence someone belief npov claim mean someone belief npov claim"",""12"":""section meant vice review btw magazine website writer name attached also like richardwilson NUMBERTAG even know question ninjarobotpirate wa responding happy criticise answer \\u2026 \\u2026 btw NUMBERTAG far know none editor either albanian croatian maybe airplane vision quite good think take care"",""13"":""next time subtweet"",""14"":""physicsyo yo yo dog"",""15"":""self censorship tv show might might notable tv pre empted breaking news notable happens time"",""16"":""article contains information soursed huddersfield aa street street"",""17"":""utc onto something centrifugal force experienced mass exhibiting inertia result tiny little bullet hitting side ride merry go round rueda puthoff haisch described zero point field electronic lorenz equation coupling inertial frame reference give mass inertial reluctance rather resistance enable describe change velocity direction compare ac v dc tesla v edison NUMBERTAG NUMBERTAG NUMBERTAG june NUMBERTAG"",""18"":""meant wa meant state either unblock create new account rendering block useless simple"",""19"":""NUMBERTAG utc hi NUMBERTAG must mistakenly thought ian wa original member b c always viewed band definitive axeman NUMBERTAG yeah almost bought akai headrush looper year ago notorious role cab one guitarist recording settled bos loop station instead rather headrush boomerang due two reliability price issue respectively check hovercraft southpacific auburn lull kind hallucinitory guitar looping thought cab new lineup wa incredible saw NUMBERTAG skipped classic lineup NUMBERTAG compare two performance wise best NUMBERTAG NUMBERTAG NUMBERTAG may""},""label"":{""0"":1,""1"":1,""2"":1,""3"":1,""4"":1,""5"":1,""6"":1,""7"":1,""8"":1,""9"":1,""10"":0,""11"":0,""12"":0,""13"":0,""14"":0,""15"":0,""16"":0,""17"":0,""18"":0,""19"":0}}')
```

### Full error message:
```
TypeError                                 Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_18240\1200168424.py in <module>
     63     cv=3,
     64     verbose=0
---> 65 ).fit(X_train_, y_train_)
     66 
     67 # print(f""Best model parameters: {search.best_params_}, best score {search.best_score_}."")

~\AppData\Local\Programs\Python\Python37\lib\site-packages\skopt\searchcv.py in fit(self, X, y, groups, callback, **fit_params)
    464             self.optimizer_kwargs_ = dict(self.optimizer_kwargs)
    465 
--> 466         super().fit(X=X, y=y, groups=groups, **fit_params)
    467 
    468         # BaseSearchCV never ranked train scores,

~\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\model_selection\_search.py in fit(self, X, y, groups, **fit_params)
    803         n_splits = cv_orig.get_n_splits(X, y, groups)
    804 
--> 805         base_estimator = clone(self.estimator)
    806 
    807         parallel = Parallel(n_jobs=self.n_jobs, pre_dispatch=self.pre_dispatch)

~\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\base.py in clone(estimator, safe)
     84     new_object_params = estimator.get_params(deep=False)
     85     for name, param in new_object_params.items():
---> 86         new_object_params[name] = clone(param, safe=False)
     87     new_object = klass(**new_object_params)
     88     params_set = new_object.get_params(deep=False)

~\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\base.py in clone(estimator, safe)
     62     # XXX: not handling dictionaries
     63     if estimator_type in (list, tuple, set, frozenset):
---> 64         return estimator_type([clone(e, safe=safe) for e in estimator])
     65     elif not hasattr(estimator, ""get_params"") or isinstance(estimator, type):
     66         if not safe:

~\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\base.py in <listcomp>(.0)
     62     # XXX: not handling dictionaries
     63     if estimator_type in (list, tuple, set, frozenset):
---> 64         return estimator_type([clone(e, safe=safe) for e in estimator])
     65     elif not hasattr(estimator, ""get_params"") or isinstance(estimator, type):
     66         if not safe:

~\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\base.py in clone(estimator, safe)
     62     # XXX: not handling dictionaries
     63     if estimator_type in (list, tuple, set, frozenset):
---> 64         return estimator_type([clone(e, safe=safe) for e in estimator])
     65     elif not hasattr(estimator, ""get_params"") or isinstance(estimator, type):
     66         if not safe:

~\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\base.py in <listcomp>(.0)
     62     # XXX: not handling dictionaries
     63     if estimator_type in (list, tuple, set, frozenset):
---> 64         return estimator_type([clone(e, safe=safe) for e in estimator])
     65     elif not hasattr(estimator, ""get_params"") or isinstance(estimator, type):
     66         if not safe:

~\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\base.py in clone(estimator, safe)
     84     new_object_params = estimator.get_params(deep=False)
     85     for name, param in new_object_params.items():
---> 86         new_object_params[name] = clone(param, safe=False)
     87     new_object = klass(**new_object_params)
     88     params_set = new_object.get_params(deep=False)

~\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\base.py in clone(estimator, safe)
     65     elif not hasattr(estimator, ""get_params"") or isinstance(estimator, type):
     66         if not safe:
---> 67             return copy.deepcopy(estimator)
     68         else:
     69             if isinstance(estimator, type):

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_tuple(x, memo, deepcopy)
    219 
    220 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
--> 221     y = [deepcopy(a, memo) for a in x]
    222     # We're not going to put the tuple in the memo, but it's still important we
    223     # check for it, in case the tuple contains recursive mutable structures.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in <listcomp>(.0)
    219 
    220 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
--> 221     y = [deepcopy(a, memo) for a in x]
    222     # We're not going to put the tuple in the memo, but it's still important we
    223     # check for it, in case the tuple contains recursive mutable structures.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_tuple(x, memo, deepcopy)
    219 
    220 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
--> 221     y = [deepcopy(a, memo) for a in x]
    222     # We're not going to put the tuple in the memo, but it's still important we
    223     # check for it, in case the tuple contains recursive mutable structures.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in <listcomp>(.0)
    219 
    220 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
--> 221     y = [deepcopy(a, memo) for a in x]
    222     # We're not going to put the tuple in the memo, but it's still important we
    223     # check for it, in case the tuple contains recursive mutable structures.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_list(x, memo, deepcopy)
    214     append = y.append
    215     for a in x:
--> 216         append(deepcopy(a, memo))
    217     return y
    218 d[list] = _deepcopy_list

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_tuple(x, memo, deepcopy)
    219 
    220 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
--> 221     y = [deepcopy(a, memo) for a in x]
    222     # We're not going to put the tuple in the memo, but it's still important we
    223     # check for it, in case the tuple contains recursive mutable structures.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in <listcomp>(.0)
    219 
    220 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
--> 221     y = [deepcopy(a, memo) for a in x]
    222     # We're not going to put the tuple in the memo, but it's still important we
    223     # check for it, in case the tuple contains recursive mutable structures.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_list(x, memo, deepcopy)
    214     append = y.append
    215     for a in x:
--> 216         append(deepcopy(a, memo))
    217     return y
    218 d[list] = _deepcopy_list

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_tuple(x, memo, deepcopy)
    219 
    220 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
--> 221     y = [deepcopy(a, memo) for a in x]
    222     # We're not going to put the tuple in the memo, but it's still important we
    223     # check for it, in case the tuple contains recursive mutable structures.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in <listcomp>(.0)
    219 
    220 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
--> 221     y = [deepcopy(a, memo) for a in x]
    222     # We're not going to put the tuple in the memo, but it's still important we
    223     # check for it, in case the tuple contains recursive mutable structures.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_tuple(x, memo, deepcopy)
    219 
    220 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
--> 221     y = [deepcopy(a, memo) for a in x]
    222     # We're not going to put the tuple in the memo, but it's still important we
    223     # check for it, in case the tuple contains recursive mutable structures.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in <listcomp>(.0)
    219 
    220 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
--> 221     y = [deepcopy(a, memo) for a in x]
    222     # We're not going to put the tuple in the memo, but it's still important we
    223     # check for it, in case the tuple contains recursive mutable structures.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    305             for key, value in dictiter:
    306                 key = deepcopy(key, memo)
--> 307                 value = deepcopy(value, memo)
    308                 y[key] = value
    309         else:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    279     if state is not None:
    280         if deep:
--> 281             state = deepcopy(state, memo)
    282         if hasattr(y, '__setstate__'):
    283             y.__setstate__(state)

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\AppData\Local\Programs\Python\Python37\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    239     memo[id(x)] = y
    240     for key, value in x.items():
--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)
    242     return y
    243 d[dict] = _deepcopy_dict

~\AppData\Local\Programs\Python\Python37\lib\copy.py in deepcopy(x, memo, _nil)
    167                     reductor = getattr(x, ""__reduce_ex__"", None)
    168                     if reductor:
--> 169                         rv = reductor(4)
    170                     else:
    171                         reductor = getattr(x, ""__reduce__"", None)

TypeError: can't pickle _thread.RLock objects
```

### Versions:
```
numpy 1.21.5
pandas 1.3.5.
sklearn 1.0.2.
scikeras 0.6.1.
tensorflow 2.8.0.
tensorflow_text 2.8.1.
tensorflow_hub 0.12.0
```

### Note:
- A [very similar issue was reported in 2020](https://github.com/tensorflow/tensorflow/issues/42641) where the issue was reproducible on TFv2.3 as mentioned [in this comment](https://github.com/tensorflow/tensorflow/issues/42641#issuecomment-684982906) in that thread. However there was no clear solution/workaround provided there, rather [this link](https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/). Having compared the code snippets provided in that blog post with my code above, I don't see any obvious inconsistencies or mistakes in my solution. Note that the linked blog post is actually only about incorporating a KerasClassifier into a grid/randomized search logic, and not about incorporating an entire sklearn pipeline. This might be important. 
- Another time the same error message was reported here on github was [this one](https://github.com/tensorflow/tensorflow/issues/47324). Yet, in this case the reason is seemingly unrelated. 
- [In this thread on StackOverflow](https://stackoverflow.com/a/48720211/5123111) Marcin Możejko mentions that `keras` doesn't support parallelization via pickle which would be performed by default by grid/randomized search, and proposed explicitly setting `n_jobs=1` to prevent multiprocessing. Makes total sense, I tried, yet I am still getting the same error message.   
- [Another bug report from 2020](https://github.com/keras-team/keras/issues/14194) also mentioned the same issue. Just like before, this was unfortunately also closed without solution. Note however, that in this case too, a pure `KerasClassifier` model was used and not a `sklearn` pipeline. Might be relevant."
54693,Validation not executed,"Why is the validation not executed, when the Flatten layer is used?

```python 
 model = Sequential()
  for i in range(hp.Int('n_layers', 1, 6)):
    model.add(Conv1D(filters=hp.Int(f'conv_{i}_filter',min_value=16,max_value=256,step=16), 
                     kernel_size=hp.Int(f'conv_{i}_kernel',min_value=1,max_value=20,step=1), 
                     activation='relu', 
                     input_shape=(n_steps_in, n_features)))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Flatten())
  model.add(Dense(50, activation='relu'))
  model.add(Dense(n_output))
  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
  model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=hp_learning_rate),metrics = ['mae', 'msle'])
```
"
54687,"After finetuning a BERT model max_position_embeddings=128, my saved model is not reflecting the smaller sequence length","I used to fine-tune BERT with tf1 with original BERT code, now that I am using tf-models-official, although I change max_position_embedding, my saved model file doesn't change. Previously when I limit max_seq_length and saved model with the associated signature (with the same max_seq_length) my saved model would be smaller. Here in tf-models-official I don't need to explicitly pass a signature, so the only place that l put limitation is in max_position_embedding. The problem is my saved model is bigger than I expected.
   "
54681,tf.saved_model.save is very slow if a tf.function contains an onnx model,"I can successfully convert a pretrained Pytorch model into the onnx format and export to a Tensorflow proto file. I can load this file in Tensorflow, add some preprocessing/postprocessing steps and pack them into a `tf.function`. Everything works fine. Now it is very slow if I want to use `tf.saved_model.save` to save this new tf.function.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.19
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.8.0
- Python version: 3.7.11
- CUDA/cuDNN version: 11.3
- GPU model and memory: 2 NV TESLA K80 GPUs, 2x12GB 

**Describe the current behavior**
It took 500+ sec to save a simple resnet18, 7000+ sec to save a resnet34, and more than 40 hours to save a resnet50 in the following toy example.
Not sure why this process is so slow or I did something wrong.

**Describe the expected behavior**
Hopefully it could be faster.

**Standalone code to reproduce the issue**
```python
import torch
from torchvision.models import resnet18
import torch.onnx

import tensorflow as tf

import onnx
from onnx_tf.backend import prepare

# load a pytorch model
torch_model = resnet18(pretrained=True, progress=False)
# save to onnx
save_dst = './torch_resnet18.onnx'
x = torch.rand(1, 3, 224, 224)
torch.onnx.export(
    torch_model, 
    x, 
    save_dst, 
    verbose=True,
    opset_version=12,
    export_params=True, 
    input_names = ['input'], 
    output_names = ['output'], 
    dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}},
    )
# load and save to tf graph
onnx_model = onnx.load(save_dst)
tf_model = prepare(onnx_model)
tf_model.export_graph('tf_resnet18')
#%% modify
class TestModule(tf.Module):
    def __init__(self):
        super().__init__()
        self.t_model = tf.saved_model.load('tf_resnet18')
        self.f = self.t_model.signatures['serving_default']

    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None, 256, 256, 3], dtype=tf.float32),
    ])
    def __call__(self, x):
        x = tf.image.resize(x, (224, 224))
        x = tf.transpose(x, [0, 3, 1, 2])
        return self.f(x)

m = TestModule()
x = tf.random.uniform((1, 256, 256, 3))
y = m(x)
print(y)
# it works fine above
tf.saved_model.save(m, 'tf_new')
# the last line takes a lot of time...
```
**Note**
It seems the program spent a lot of time on:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L1959
which leads to a lower level implementation:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L1084"
54671,Build failure with TF 2.7.1 on Linux ppc64le with high memory consumption,"**System information**
- Red Hat Enterprise Linux CoreOS 4.6:
- Desktop ppc64le 
- TensorFlow installed from (source or binary): Source 
- TensorFlow version: 2.7.1
- Python version: 3.9
- Installed using conda:
- Bazel version : 3.7.2
- GCC/Compiler version (if compiling from source): 8.2 from Anaconda Compiler Toolchain
- CUDA/cuDNN version: 11-2
- GPU model and memory: 511GB RAM
![image](https://user-images.githubusercontent.com/18348768/155977636-dc529d72-ae51-4493-8803-8d6310cb7cc6.png)



**Describe the problem**:
Our Team has been building Tensorflow source code on ppc64le(511GB RAM total) Openshift CI/CD pipeline
We have observing that while Tensorflow builds it tends to consume huge chunk of RAM memory like 110GB and then  the build fails .. on multiple retries like in the number of 17-18 and sometimes even more the builds randomly pass

In comparison to this same version of Tensorflow on x86(61GB RAM total) environment wherein the RAM is much less as compared to ppc64le the build completes successfully must faster and consistent with max reties of 1 as compared to ppc64le

The above mentioned scenario is also been tested for Tensorflow v 2.4.4 on ppc64le and its performance has been much much better as compared to Tensorflow v 2.7.1.

We are trying to understand what is it with Tensorflow v 2.7.1 that occupies so much memory on ppc64le and refuses to release memory which causes repeated build failures .. is there something we can do in code to correct this behavior on ppc64le


**Provide the exact sequence of commands / steps that you executed before running into the problem**
https://github.com/open-ce/tensorflow-feedstock/blob/main/recipe/meta.yaml

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

![image](https://user-images.githubusercontent.com/18348768/155978048-9950a81d-96ea-4f80-a6fd-facc750e109d.png)

"
54660,Please remove invalid hyperlinks.,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/guide/migrate/upgrade

## Description of issue (what needs changing):
There is a link but the domain is for sell.

### Clear description
In the very end.
Check out [tf2up.ml](http://tf2up.ml/) for a convenient tool to upgrade Jupyter notebooks and Python files in a GitHub repository.
This link is not working anymore.


### Correct links
https://github.com/lc0/tf2up

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
54659,"TFLite conversion works in TF 2.8 / fails in 2.7.1, both produce large file sizes and incorrect input size of 1x1x1x3","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installation (pip package or built from source): pip install 
- TensorFlow library (version, if pip package or github SHA, if built from source): test 2.7.1 and 2.8

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

TensorFlow 2.8
https://colab.research.google.com/drive/1uYZOrPJWbSGfhf-DYEgMtCVmy5xLBE5G?usp=sharing
TensorFlow 2.7.1
https://colab.research.google.com/drive/15kzXMSF2olfzNskYN99WKq7DarUy-3tj?usp=sharing

### 3. Failure after conversion

Used ssd_resnet101_v1_fpn_640x640_coco17_tpu-8 from model zoo to train a custom model since SSD models are supported in TF lite conversion. Exported the model using 'exporter_main_v2.py' from model zoo repo and the saved model was 16 Mb.

All TFLite models incorrectly have an input size of 1x1x1x3

TF 2.8 converts all 3 different quantizations tested (see relevant colab)
TF Lite Dynamic Range file size is 73.5 Mb
TF Lite Int with float fallback file size is 267.7 Mb
TF lite Full integer quantization file size is 271.2 Mb

TF 2.7.1 integer quantization fails with
Failed to quantize: <unknown>:0: error: loc(""Postprocessor/Slice;Postprocessor/Slice""): 'tfl.slice' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 2.5953195290639997E-4:-128> vs. !quant.uniform<i8:f32, 0.0011857558274641633:-128>

### 5. (optional) Any other info / logs

For the large file size generation I have also commented over at #54405 as well but I created new issue for the incorrect input size and failure of conversion for 2.7.1

I also tried using the 'export_tflite_graph_tf2.py' (not sure if that is what I should be using) to export my saved model which seems to correctly generate the input signature of 1,640,640,3 though I am not sure about the generated tflite yet but the converted tflite files are still 70 Mb.
"
54658,"tfa.seq2seq - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling layer ""embedding"" (type Embedding).","I am following closely the Seq2seq for translation tutorial here https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt#define_the_optimizer_and_the_loss_function. I meet an error when instantiating the Encoder which is defined as

```
class Encoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    super(Encoder, self).__init__()
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)

    ##-------- LSTM layer in Encoder ------- ##
    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')

  def call(self, x, hidden):
    x = self.embedding(x)
    output, h, c = self.lstm_layer(x, initial_state = hidden)
    return output, h, c

  def initialize_hidden_state(self):
    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]
```

It is falling when testing here

```
# Test Encoder Stack
encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)

# sample input
sample_hidden = encoder.initialize_hidden_state()
sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)
```
The error is the following
```
Traceback (most recent call last):
  File ""C:/Users/Seq2seq/Seq2seq-V3.py"", line 132, in <module>
    sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)
  File ""C:\Users\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:/Users/Seq2seq/Seq2seq-V3.py"", line 119, in call
    x = self.embedding(x)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling layer ""embedding"" (type Embedding).

indices[12,148] = 106 is not in [0, 106) [Op:ResourceGather]

Call arguments received:
  • inputs=tf.Tensor(shape=(64, 200), dtype=int64)
```
TF 2.0

"
54655,BERT guide in tensorflow.org works differently(not converging),"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/

https://www.tensorflow.org/text/tutorials/classify_text_with_bert

## Description of issue (what needs changing):

not converging

### Clear description

For example, why should someone use this method? How is it useful?

works differentlly

### Correct links

https://www.tensorflow.org/text/tutorials/classify_text_with_bert

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

yes

### Returns defined

Are return values defined?

yes

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

N/A

### Usage example

Is there a usage example?

![image](https://user-images.githubusercontent.com/4515120/155879988-294e8af7-9022-4187-b748-5f72a8bc5c7f.png)


![image](https://user-images.githubusercontent.com/4515120/155880012-ffac1736-032e-4f79-aa37-2f3c0ae38308.png)



See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
54654,Autograph ,"AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7fb3dc34d050> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert"
54648,expand support for image file types (primarily TIFF),"This issue requests support for TIFF images when using TensorFlow for image processing research.

**System information**
- TensorFlow version (you are using): 2.8.0 available via Colab: Python 3 Google Compute Engine backend (GPU)
- Are you willing to contribute it (Yes/No): I'm willing to help but looking for high-level suggestions on how to implement this. Feel free to assign me with a mentor.


**Describe the feature and the current behavior/state.**
Currently, TensorFlow only supports a limited number of image filetypes. This should be expanded for both convenience and data uses of users who require other image filetypes.

**Will this change the current api? How?**
Maybe? The current image decoder would have to be updated to read TIFF filetypes. ~No commands would have to change.~ A `tf.io.decode_jpeg(sample_image)` equivalent would have to be added.

**Who will benefit with this feature?**
This feature would benefit domain science users who rely on TIFF image types for microscopy images, among other uses.

**Any Other info.**
Traceback on Error if running TIFF image filetype is shown below. Even if I change the filetype to JPEG, if the image was originally TIFF I get this error.

```python
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
[<ipython-input-4-940ee08e0488>](https://localhost:8080/#) in <module>()
      2 
      3 
----> 4 sample_image = tf.io.decode_jpeg(sample_image)
      5 print(sample_image.shape)

2 frames
[/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py](https://localhost:8080/#) in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

InvalidArgumentError: Unknown image file format. One of JPEG, PNG, GIF, BMP required. [Op:DecodeJpeg]
```"
54612,TensorFlow doesn't recognize RTX 3080 ti Laptop GPU,"**System information**
- OS Linux Ubuntu 21.10
- TensorFlow installed with pip install tensorflow==2.7.0
- Python 3.9.7:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version 9.4:
- CUDA 11.2 / cuDNN 8.1:
- GPU RXT 3080 ti Laptop

**I installed CUDA and Cudnn but TensorFlow doesn't recognize GPU**

**I have 510 Nvidia driver but saw that TensorFlow support  CUDA 11.2 and Cudnn 8.1. That mean I have to download 460 Nvidia driver. Unfortunately, my GPU does not work with 460 Nvidia driver. I tried to continue with CUDA 11.2 and CUDNN 8.1 but I got mistake as below. Can anyone help, please?**


**Python Code**
```
import tensorflow as tf
from tensorflow.python.client import device_lib

print(tf.__version__)
print(device_lib.list_local_devices())
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```

**OUTPUT**
2022-02-25 21:03:35.125701: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-02-25 21:03:36.870592: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination
2022-02-25 21:03:36.870651: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ak-pc
2022-02-25 21:03:36.870654: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ak-pc
2022-02-25 21:03:36.870702: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.103.1
2022-02-25 21:03:36.870721: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.47.3
2022-02-25 21:03:36.870724: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 510.47.3 does not match DSO version 470.103.1 -- cannot find working devices in this configuration
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 16144767737604269050
xla_global_id: -1
]
Num GPUs Available:  0

"
54582,huggingface pre_trained model loading takes full gpu memory,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  2.8.0
- Python version: 3.9.7
- Bazel version (if compiling from source): 4.2.1
- GCC/Compiler version (if compiling from source): visual studio 2019
- CUDA/cuDNN version: 11.6/8.3.2
- GPU model and memory: RTX3090 24GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
load from pre-trained transformer takes full gpu memory, so oom happens after training started
**Describe the expected behavior**
appropriate memory allocation

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): add options for loading pre-trained model

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

with kaggle feedback-prize data
https://www.kaggle.com/c/feedback-prize-2021/data
this code
```
import os
from tqdm import tqdm,trange
import tensorflow as tf
from tensorflow.keras import layers,Model,Input,Sequential,metrics,regularizers,optimizers,losses,activations,callbacks
from transformers import AutoTokenizer,AutoConfig,TFAutoModel
import pandas as pd
import numpy as np
from iterstrat.ml_stratifiers import MultilabelStratifiedKFold
from copy import copy,deepcopy

VER=12
EPOCHS=5
N_SPLITS=5
MAX_LEN=1024
NUM_ATTENTION_HEADS=16
ROBERTA_LARGE_PATH='../../pretrained/transformers/roberta-large/'
LRS = [1e-4, 1e-4, 1e-4, 1e-5, 1e-5,1e-5]
RUN_TRAINING=False

tokenizer=AutoTokenizer.from_pretrained(ROBERTA_LARGE_PATH+'auto_tokenizer')
config=AutoConfig.from_pretrained(ROBERTA_LARGE_PATH+'auto_config')

backbone=TFAutoModel.from_pretrained('roberta-large',config=config)

FEEDBACK_DATA_PATH='../../mldata/Feedback_Prize/'
train_df=pd.read_csv(FEEDBACK_DATA_PATH+'train.csv')

N_ID=train_df.id.nunique()
IDS=train_df.id.unique()

discourse_types=['Lead','Position','Evidence','Claim','Concluding Statement','Counterclaim','Rebuttal']
target_map = {k:i for i,k in enumerate(discourse_types)}

targets=np.load(f'../kaggle/encoded/targets_{MAX_LEN}.npy',allow_pickle=False)
train_tokens=np.load(f'../kaggle/encoded/tokens_{MAX_LEN}.npy',allow_pickle=False)
train_attention=np.load(f'../kaggle/encoded/attention_{MAX_LEN}.npy',allow_pickle=False)

test_files=os.listdir('../../mldata/Feedback_Prize/test/')
TEST_IDS=[f.replace('.txt','') for f in test_files if 'txt' in f]
print('There are',len(TEST_IDS),'test texts.')
test_tokens=np.zeros((len(TEST_IDS),MAX_LEN),dtype=np.int32)
test_attention=np.zeros((len(TEST_IDS),MAX_LEN),dtype=np.int32)
for i,id in enumerate(TEST_IDS):
    name=f'../../mldata/Feedback_Prize/test/{id}.txt'
    txt=open(name,'r').read()
    tokens=tokenizer.encode_plus(txt,max_length=MAX_LEN,padding='max_length',truncation=True,return_offsets_mapping=True)
    test_tokens[i]=tokens['input_ids']
    test_attention[i]=tokens['attention_mask']

def build_model():
    tokens=Input((MAX_LEN,),name='tokens',dtype=tf.int32)
    attention=Input((MAX_LEN,),name='attention',dtype=tf.int32)
    x=backbone(tokens,attention_mask=attention)
    x1=layers.Dropout(0.1)(x[0])
    x=layers.Dense(15,activation='softmax',dtype=tf.float32)(x1)
    model=Model(inputs=[tokens,attention],outputs=x)
    model.compile(optimizers.Adam(1e-4),losses.CategoricalCrossentropy(),[metrics.CategoricalAccuracy()])
    return model

discourse_types_ext=copy(discourse_types)
discourse_types_ext.append('blank')

def get_preds(dataset = 'train', verbose = True, text_ids = None, preds = None):
    all_predictions = []
    for id_num in range(len(preds)):
        if (id_num % 100 == 0) & (verbose): print(id_num, ', ', end = '')
        n = text_ids[id_num]
        name = f'../input/feedback-prize-2021/{dataset}/{n}.txt'
        txt = open(name, 'r').read()
        tokens = tokenizer.encode_plus(txt, max_length = MAX_LEN, padding = 'max_length', truncation = True, return_offsets_mapping = True)
        off = tokens['offset_mapping']
        w = []
        blank = True
        for i in range(len(txt)):
            if (txt[i] != ' ') & (txt[i] != '\n') & (blank == True):
                w.append(i)
                blank = False
            elif (txt[i] == ' ') | (txt[i] == '\n'):
                blank = True
        w.append(1e6)
        word_map = -1 * np.ones(MAX_LEN, dtype = 'int32')
        w_i = 0
        for i in range(len(off)):
            if off[i][1] == 0: continue
            while off[i][0] >= w[w_i + 1]: w_i += 1
            word_map[i] = int(w_i)
        pred = preds[id_num,] / 2.0
        i = 0
        while i < MAX_LEN:
            prediction = []
            start = pred[i]
            if start in [0, 1, 2, 3, 4, 5, 6, 7]:
                prediction.append(word_map[i])
                i += 1
                if i >= MAX_LEN: break
                while pred[i] == start + 0.5:
                    if not word_map[i] in prediction: prediction.append(word_map[i])
                    i += 1
                    if i >= MAX_LEN: break
            else: i += 1
            prediction = [x for x in prediction if x != -1]
            if len(prediction) > 4: all_predictions.append((n, discourse_types_ext[int(start)], ' '.join([str(x) for x in prediction])))

    # MAKE DATAFRAME
    df = pd.DataFrame(all_predictions)
    df.columns = ['id', 'class', 'predictionstring']
    return df

def calc_overlap(row):
    set_pred = set(row.predictionstring_pred.split(' '))
    set_gt = set(row.predictionstring_gt.split(' '))
    len_gt = len(set_gt)
    len_pred = len(set_pred)
    inter = len(set_gt.intersection(set_pred))
    overlap_1 = inter / len_gt
    overlap_2 = inter / len_pred
    return [overlap_1, overlap_2]

def score_feedback_comp(pred_df, gt_df):
    gt_df = gt_df[['id', 'discourse_type', 'predictionstring']].reset_index(drop = True).copy()
    pred_df = pred_df[['id', 'class', 'predictionstring']].reset_index(drop = True).copy()
    pred_df['pred_id'] = pred_df.index
    gt_df['gt_id'] = gt_df.index
    joined = pred_df.merge(gt_df, left_on = ['id', 'class'], right_on = ['id', 'discourse_type'], how = 'outer', suffixes = ('_pred', '_gt'))
    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')
    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')
    joined['overlaps'] = joined.apply(calc_overlap, axis=1)
    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])
    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])
    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)
    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)
    tp_pred_ids = joined.query('potential_TP').sort_values('max_overlap', ascending=False).groupby(['id','predictionstring_gt']).first()['pred_id'].values
    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]
    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()
    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]
    TP = len(tp_pred_ids)
    FP = len(fp_pred_ids)
    FN = len(unmatched_gt_ids)
    my_f1_score = TP / (TP + 0.5*(FP+FN))
    return my_f1_score

EPOCHS=6

all_scores=[]
oof_preds=np.zeros((N_ID,15))
test_preds=np.zeros((len(TEST_IDS),MAX_LEN,15))
for fold,(train_idx,valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=N_SPLITS,shuffle=True,random_state=777).split(train_tokens,targets[:,0,:])):
    print('#'*25)
    print('### FOLD %i' % (fold + 1))
    print('#'*25)
    
    model=build_model()
    
    def lrfn(epoch):
        return LRS[epoch]
    
    model.fit([train_tokens[train_idx],train_attention[train_idx]],targets[train_idx],validation_data=([train_tokens[valid_idx],train_attention[valid_idx]],targets[valid_idx]),
                callbacks=[callbacks.LearningRateScheduler(lrfn,verbose=True),callbacks.ModelCheckpoint('models/%s-roberta-%i.tf'%(VER,fold),monitor='val_loss',
                                                                                                        save_best_only=True,save_weights_only=True)],
                epochs=EPOCHS,batch_size=8,verbose=1)
    pred=model.predict([train_tokens[valid_idx],train_attention[valid_idx]],batch_size=4,verbose=1)
    print(pred.shape)
    print('predicting OOF...')
    oof=get_preds(dataset='train',verbose=True,text_ids=IDS[valid_idx],preds=np.argmax(pred,axis=-1))
    f1s=[]
    CLASSES=oof['class'].unique()
    for c in CLASSES:
        pred_df = oof.loc[oof['class'] == c].copy()
        valid = train_df.loc[train_df.id.isin(IDS[valid_idx])]
        gt_df = valid.loc[valid['discourse_type'] == c].copy()
        f1 = score_feedback_comp(pred_df, gt_df)
        print(c, f1)
        f1s.append(f1)
    print()
    print('Fold score: ', np.mean(f1s))
    
    test_preds+=model.predict([test_tokens,test_attention],batch_size=4,verbose=1)/N_SPLITS
```


error
```
2022-02-25 20:42:31.859917: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size:
2022-02-25 20:42:31.859950: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 131 Chunks of size 256 totalling 32.8KiB
2022-02-25 20:42:31.859978: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB
2022-02-25 20:42:31.860006: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3072 totalling 3.0KiB
2022-02-25 20:42:31.860033: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3840 totalling 3.8KiB
2022-02-25 20:42:31.860061: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 655 Chunks of size 4096 totalling 2.56MiB
2022-02-25 20:42:31.860089: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4352 totalling 4.2KiB
2022-02-25 20:42:31.860117: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4864 totalling 4.8KiB
2022-02-25 20:42:31.860145: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 7168 totalling 7.0KiB
2022-02-25 20:42:31.860173: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 66 Chunks of size 16384 totalling 1.03MiB
2022-02-25 20:42:31.860201: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 19200 totalling 18.8KiB
2022-02-25 20:42:31.860228: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 20480 totalling 40.0KiB
2022-02-25 20:42:31.860256: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 24576 totalling 24.0KiB
2022-02-25 20:42:31.860284: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 28672 totalling 28.0KiB
2022-02-25 20:42:31.860311: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 32256 totalling 31.5KiB
2022-02-25 20:42:31.860339: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 32 Chunks of size 32768 totalling 1.00MiB
2022-02-25 20:42:31.860367: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 40960 totalling 40.0KiB
2022-02-25 20:42:31.860394: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 48896 totalling 47.8KiB
2022-02-25 20:42:31.860422: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 61440 totalling 180.0KiB
2022-02-25 20:42:31.860450: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 65536 totalling 64.0KiB
2022-02-25 20:42:31.860478: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 491520 totalling 480.0KiB
2022-02-25 20:42:31.860506: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 2105344 totalling 4.02MiB
2022-02-25 20:42:31.860534: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 289 Chunks of size 4194304 totalling 1.13GiB
2022-02-25 20:42:31.860562: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4578560 totalling 4.37MiB
2022-02-25 20:42:31.860590: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 139 Chunks of size 16777216 totalling 2.17GiB
2022-02-25 20:42:31.860618: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 20971520 totalling 20.00MiB
2022-02-25 20:42:31.860646: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 21258240 totalling 20.27MiB
2022-02-25 20:42:31.860674: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 25530368 totalling 24.35MiB
2022-02-25 20:42:31.860702: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 27955200 totalling 26.66MiB
2022-02-25 20:42:31.860730: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 29360128 totalling 28.00MiB
2022-02-25 20:42:31.860758: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 89 Chunks of size 33554432 totalling 2.78GiB
2022-02-25 20:42:31.860787: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 39534592 totalling 37.70MiB
2022-02-25 20:42:31.860815: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 40222720 totalling 38.36MiB
2022-02-25 20:42:31.860843: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 28 Chunks of size 134217728 totalling 3.50GiB
2022-02-25 20:42:31.860872: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 205885440 totalling 589.04MiB
2022-02-25 20:42:31.860901: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 21 Chunks of size 536870912 totalling 10.50GiB
2022-02-25 20:42:31.860928: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 20.86GiB2022-02-25 20:42:31.860953: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 22729785344 memory_limit_: 22729785344 available bytes: 0 curr_region_allocation_bytes_: 45459570688
2022-02-25 20:42:31.860988: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats:
Limit:                     22729785344
InUse:                     22400043008
MaxInUse:                  22433597440
NumAllocs:                        5211
MaxAllocSize:                536870912
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-02-25 20:42:31.861075: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ***************************************************************************************************_
2022-02-25 20:42:31.861123: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:681 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8,16,1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""D:\repo\robertatraining\robertatraining.py"", line 157, in <module>
    model.fit([train_tokens[train_idx],train_attention[train_idx]],targets[train_idx],validation_data=([train_tokens[valid_idx],train_attention[valid_idx]],targets[valid_idx]),
  File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\alanp\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node 'model/tf_roberta_model/roberta/encoder/layer_._7/attention/self/MatMul' defined at (most recent call last):
    File ""D:\repo\robertatraining\robertatraining.py"", line 157, in <module>
      model.fit([train_tokens[train_idx],train_attention[train_idx]],targets[train_idx],validation_data=([train_tokens[valid_idx],train_attention[valid_idx]],targets[valid_idx]),
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\training.py"", line 1384, in fit
      tmp_logs = self.train_function(iterator)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\training.py"", line 1021, in train_function
      return step_function(self, iterator)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\training.py"", line 1010, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\training.py"", line 1000, in run_step
      outputs = model.train_step(data)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\training.py"", line 859, in train_step
      y_pred = self(x, training=True)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\functional.py"", line 451, in call
      return self._run_internal_graph(
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\functional.py"", line 589, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 996, in call
      outputs = self.roberta(
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 755, in call
      encoder_outputs = self.encoder(
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 528, in call
      for i, layer_module in enumerate(self.layer):
    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 534, in call
      layer_outputs = layer_module(
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 443, in call
      self_attention_outputs = self.attention(
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 356, in call
      self_outputs = self.self_attention(
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\alanp\anaconda3\lib\site-packages\transformers\models\roberta\modeling_tf_roberta.py"", line 284, in call
      attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
Node: 'model/tf_roberta_model/roberta/encoder/layer_._7/attention/self/MatMul'
OOM when allocating tensor with shape[8,16,1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node model/tf_roberta_model/roberta/encoder/layer_._7/attention/self/MatMul}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_train_function_53102]
```

[8,16,1024,1024] with tf.fload32 is just 512MB total, it doesn't make sense


just with ""from_pretrained"", gpu allocate almost all of its memory
![image](https://user-images.githubusercontent.com/4515120/155709476-168583f7-e1cc-495b-8ab3-2a218934d7ac.png)


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54578,"failed: (Exit 2): cl.exe failed: error executing command，Bazel=5.0.0, tf=2.8.0,MSVC2019 ","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):build it from the source
- TensorFlow version:2.8.0
- Python version:3.10 && 3.8.8
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):5.0.0 && 4.2.1
- GCC/Compiler version (if compiling from source):MSVC2019
- CUDA/cuDNN version:No
- GPU model and memory:No



**Describe the problem**
when I use bazel to build the tf2.8, the blow error happened.
`failed: (Exit 2): cl.exe failed: error executing command`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`F:\tensorflow>python ./configure.py`
You have bazel 5.0.0 installed.
Please specify the location of python. [Default is D:\JetBrain\python\python.exe]:

Found possible Python library paths:
D:\JetBrain\python\lib\site-packages
Please input the desired Python library path to use. Default is [D:\JetBrain\python\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:

Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
--config=mkl # Build with MKL support.
--config=mkl_aarch64 # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
--config=monolithic # Config for mostly static monolithic build.
--config=numa # Build with NUMA support.
--config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.
--config=v1 # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
--config=nogcp # Disable GCP support.
--config=nonccl # Disable NVIDIA NCCL support.


`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`
Extracting Bazel installation...
Starting local Bazel server and connecting to it...


**Any other info / logs**
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: F:/tensorflow/tensorflow/core/framework/BUILD:1400:31 Middleman _middlemen/_S_Stensorflow_Score_Sframework_Cattr_Uvalue_Uproto_Utext-BazelCppSemantics_build_arch_x64_windows-opt failed: (Exit 2): cl.exe failed: error executing command
cd /d C:/users/manyuan/_bazel_manyuan/2qttxlm7/execroot/org_tensorflow
SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\cppwinrt
SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.16299.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools;;C:\Windows\system32
SET PWD=/proc/self/cwd
SET PYTHON_BIN_PATH=D:/JetBrain/python/python.exe
SET PYTHON_LIB_PATH=D:/JetBrain/python/lib/site-packages
SET RUNFILES_MANIFEST_ONLY=1
SET TEMP=C:\Users\manyuan\AppData\Local\Temp
SET TF2_BEHAVIOR=1
SET TMP=C:\Users\manyuan\AppData\Local\Temp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/core/platform/windows/_objs/platform_port/port.obj.params"
54573,"while do evaluation in the runhook, the eval_metrics calculate each batch and the average the result or calculate after all evaluation dataset?", like auc in eval_metrics. Anybody knows this?Please help
54568,How to propagate gradients after a assign op?,"Hello,
I got the no gradient error if I changed the update of self.key_memory and self.long_memory to . assign. It seems that assign op would break gradient. If I used ""="", then it will create a new tensor.

WARNING:tensorflow:Gradients do not exist for variables ['memory_layer_write_controller/hidden_layer_0/kernel:0', 'memory_layer_write_controller/hidden_layer_0/bias:0', 'memory_layer_write_controller/hidden_layer_1/kernel:0', 'memory_layer_write_controller/hidden_layer_1/bias:0', 'memory_layer_key_erase_layer/hidden_layer_0/kernel:0', 'memory_layer_key_erase_layer/hidden_layer_0/bias:0', 'memory_layer_key_add_layer/hidden_layer_0/kernel:0', 'memory_layer_key_add_layer/hidden_layer_0/bias:0', 'memory_layer_long_erase_layer/hidden_layer_0/kernel:0', 'memory_layer_long_erase_layer/hidden_layer_0/bias:0', 'memory_layer_long_add_layer/hidden_layer_0/kernel:0', 'memory_layer_long_add_layer/hidden_layer_0/bias:0'] when minimizing the loss.

class MemoryLayer(tf.keras.layers.Layer):
def init(self, controller_layers, controller_hidden_act='relu', controller_output_act=None, n_clusters=100, key_dims=8, long_dims=8, short_dims=8, temperature=0.1, alpha=0.1, is_short=False, name=""memory_layer"", **kwargs):
super(MemoryLayer, self).init(name=name, trainable=True, **kwargs)
self.controller_layers = controller_layers
self.controller_hidden_act = controller_hidden_act
self.controller_output_act = controller_output_act
self.n_clusters = n_clusters
self.key_dims = key_dims
self.long_dims = long_dims
self.short_dims = short_dims
self.temperature = temperature
self.alpha = alpha
self.is_short = is_short

    # self.key_memory = tf.compat.v1.get_variable(
    #     f""{self.name}_key_memory"",
    #     shape=[self.n_clusters, self.key_dims],
    #     initializer=tf.compat.v1.glorot_normal_initializer())
    self.key_memory = self.add_weight(f""{self.name}_key_memory"",
                          shape=(self.n_clusters, self.key_dims),
                          initializer=tf.keras.initializers.GlorotNormal(),
                          trainable=True)


    self.long_memory = self.add_weight(f""{self.name}_long_memory"",
                          shape=(self.n_clusters, self.long_dims),
                          initializer=tf.keras.initializers.GlorotNormal(),
                          trainable=True)

    # self.long_memory = tf.compat.v1.get_variable(
    #     f""{self.name}_long_memory"",
    #     shape=[self.n_clusters, self.long_dims],
    #     initializer=tf.compat.v1.glorot_normal_initializer())

    if is_short:
        self.short_memory = self.add_weight(f""{self.name}_short_memory"",
                          shape=(self.n_clusters, self.short_dims),
                          initializer=tf.keras.initializers.GlorotNormal(),
                          trainable=False)

        self.short_erase_layer = DNNLayer([self.short_dims], 'sigmoid',
                                         name=f""{self.name}_short_erase_layer"")  # short的删除层
        self.short_add_layer = DNNLayer([self.short_dims], 'tanh', name=f""{self.name}_short_add_layer"")  #short的更新层

    self.write_controller = DNNLayer(controller_layers, controller_hidden_act, controller_output_act, name=f""{self.name}_write_controller"") #write控制器

    self.read_controller = DNNLayer(controller_layers, controller_hidden_act, controller_output_act, name=f""{self.name}_read_controller"") #read控制器

    self.key_erase_layer = DNNLayer([self.key_dims], 'sigmoid', name=f""{self.name}_key_erase_layer"") #key的删除层
    self.key_add_layer = DNNLayer([self.key_dims], 'tanh', name=f""{self.name}_key_add_layer"") #key的更新层

    self.long_erase_layer = DNNLayer([self.long_dims], 'sigmoid', name=f""{self.name}_long_erase_layer"")  #long的删除层
    self.long_add_layer = DNNLayer([self.long_dims], 'tanh', name=f""{self.name}_long_add_layer"")  # long的更新层


@tf.function
def call(self, inputs, training=None, **kwargs):
    attr_emb = inputs[0]  # [batch_size, attr_emb]
    long_emb = inputs[1]  # [batch, long_dims] 长期兴趣表征
    isActive = inputs[2]  # [batch_size, 1] 是否需要更新long、short memory

    active_status = tf.cast(tf.transpose(isActive, perm=[1, 0]), dtype='float32')

    denominator = tf.math.count_nonzero(isActive)  # 统计非0的个数

    # ＞0才更新
    if denominator > 0 and training:
        # write memory
        # [batch_size, key_dims]
        write_query = self.write_controller(tf.stop_gradient(attr_emb))  # 获取写控制器的表征

        # [batch, n_clusters]
        write_attention = compute_cosine_similarity(write_query, self.key_memory, temperature=self.temperature)
        #
        # #[1, n_clusters]
        write_memory_attention = tf.matmul(active_status, write_attention) / tf.cast(denominator, dtype=tf.float32)

        # [n_clusters, 1]
        write_memory_attention = tf.transpose(write_memory_attention, perm=[1, 0])

        # write key memory
        key_erase_vector = self.key_erase_layer(write_query)  # [batch, key_dims]
        key_erase_vector = tf.matmul(active_status, key_erase_vector) / tf.cast(denominator,
                                                                                dtype=tf.float32)  # [1,key_dims]
        #
        key_add_vector = self.key_add_layer(write_query)  # [batch, key_dims]
        key_add_vector = tf.matmul(active_status, key_add_vector) / tf.cast(denominator,
                                                                            dtype=tf.float32)  # [1,key_dims]

        key_val = tf.stop_gradient(self.key_memory) \
                          * (1. - self.alpha * tf.matmul(write_memory_attention, key_erase_vector)) \
                          + self.alpha * tf.matmul(write_memory_attention, key_add_vector)

        # self.key_memory.assign(key_val)
        tf.compat.v1.assign(self.key_memory, key_val)

        # write long_memory
        long_erase_vector = self.long_erase_layer(long_emb)  # [batch, long_dims]
        long_erase_vector = tf.matmul(active_status, long_erase_vector) / tf.cast(denominator,
                                                                                  dtype=tf.float32)  # [1,long_dims]
        #
        long_add_vector = self.long_add_layer(long_emb)  # [batch, long_dims]
        long_add_vector = tf.matmul(active_status, long_add_vector) / tf.cast(denominator,
                                                                              dtype=tf.float32)  # [1,long_dims]

        long_val = tf.stop_gradient(self.long_memory) \
                           * (1. - self.alpha * tf.matmul(write_memory_attention, long_erase_vector)) \
                           + self.alpha * tf.matmul(write_memory_attention, long_add_vector)

        # self.long_memory.assign(long_val)
        tf.compat.v1.assign(self.long_memory, long_val)

        # read memory
    # [batch, key_dims]
    read_query = self.read_controller(tf.stop_gradient(attr_emb))

    # [batch, n_clusters]
    read_attention = compute_cosine_similarity(read_query, self.key_memory, temperature=self.temperature)

    # read_attention = self.read_sim_layer([read_query, self.key_memory], temperature=self.temperature)

    long_cluster_emb = tf.matmul(read_attention, self.long_memory)  # [batch, long_dims] 长期兴趣表征

    return long_cluster_emb"
54559,TFLite Converter: how to skip Dequantize node in FP16 models for GPU Delegate?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yocto dunfell 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: aarch64
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.6.1
- Python version: 3.9.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Mali G72


**Describe the current behavior**
I converted mobilenet_v2_1.0_224 [https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz](url) to fp16 tflite model using the steps mentioned here [https://www.tensorflow.org/lite/performance/post_training_quantization#float16_quantization](url)

the converted mobilenet_v2_fp16.tflite model has multiple Dequantize steps inserted within which seem to convert float16 to float32, even though the model is supposed to be quantized to float16.
Below is a snippet of the model visualization from netron. The inputs to the converted model are float32 and its outputs are also float32. 
 
![test](https://user-images.githubusercontent.com/78979784/155630251-9e58343d-95a9-420c-8101-86fed9165372.PNG)


All the dequantize nodes have float16 input and float32 output. Node 0 dequantize node properties are visualized below:

![image](https://user-images.githubusercontent.com/78979784/155630719-1c989e54-22d8-4423-846f-93eb911c22ff.png)

all dequantize nodes have similar properties where the inputs to the layer comprise of only weights and/or bias. 

now according to the official documentation, _""By default, a float16 quantized model will ""dequantize"" the weights values to float32 when run on the CPU. (Note that the GPU delegate will not perform this dequantization, since it can operate on float16 data.)""_ This means dequantize step is added just for cpu execution and gpu will not perform or override this node. But the question I have is, the dequantize node is already present, can a delegate simply skip a node in a model architecture during execution runtime, since the node is a part of the model graph and every node is executed with no op delegated to CPU? 


What is the main use of this dequantize node? And why is a dequantization step inserted during fp16 quantization? Is there a way to skip these dequantize nodes in the GPU delegate during post-training float16 quantization itself, for I want to run the model only on GPU and not CPU? The node seems to be executed by GPU delegate, as none of the actions are delegated to the CPU at runtime. Or are they being simply ignored by the GPU delegate?

mobilenet_v2_fp16.tflite model is attached for your reference.
[mobilenet_v2_fp16.zip](https://github.com/tensorflow/tensorflow/files/8137542/mobilenet_v2_fp16.zip)
 
thanks



"
54545,Bug in `tf.math.acos`: `string` input is not supported,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/math/acos?hl=en

## Description of issue (what needs changing):
### Parameters defined
Doc says `x` must be one of the following types: bfloat16, half, ..., **string**, but obviously the input to `tf.math.acos` cannot be a `string` Tensor.

"
54540,InaccessibleTensorError when running model as a .expand_dims call in the main model code,"**System information** 
I have written a custom model in TensorFlow as a modification of the transformer model.
M1 Pro 32 GB Macbook Pro 12.1 Monterey(also ran on colab T4 GPU 16 GB)
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.7.1 also tried 2.8
- Python version: 3.7.1.2
- CUDA/cuDNN version: N/A
- GPU model and memory: 32 GB M1 PRO,  Tesla T4 16 GB

**Describe the current behavior**
Current when I run my model in eager execution model it is fine, but when I run it with @tf.function wrapper I get an inaccessible tensor error when building the AutoGraph. This is coming in the model call method, when I .expand_dims on the call of another layer class. The error is:

```
TypeError: <tf.Tensor 'hi_d_transformer/ExpandDims:0' shape=(64, 1, 64) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.
Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.

<tf.Tensor 'hi_d_transformer/ExpandDims:0' shape=(64, 1, 64) dtype=float32> was defined here:
    File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
      ""__main__"", mod_spec)
    File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py"", line 846, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py"", line 499, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py"", line 132, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.7/asyncio/base_events.py"", line 541, in run_forever
      self._run_once()
    File ""/usr/lib/python3.7/asyncio/base_events.py"", line 1786, in _run_once
      handle._run()
    File ""/usr/lib/python3.7/asyncio/events.py"", line 88, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py"", line 122, in _handle_events
      handler_func(fileobj, events)
    File ""/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py"", line 452, in _handle_events
      self._handle_recv()
    File ""/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py"", line 481, in _handle_recv
      self._run_callback(callback, msg)
    File ""/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py"", line 431, in _run_callback
      callback(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
      return self.dispatch_shell(stream, msg)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
      handler(stream, idents, msg)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
      user_expressions, allow_stdin)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py"", line 208, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py"", line 537, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
      interactivity=interactivity, compiler=compiler, result=result)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes
      if self.run_code(code, result):
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-32-1bb5ee661811>"", line 13, in <module>
      train_step(inp, tar)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py"", line 150, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 915, in __call__
      result = self._call(*args, **kwds)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 963, in _call
      self._initialize(args, kwds, add_initializers_to=initializers)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 786, in _initialize
      *args, **kwds))
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 2983, in _get_concrete_function_internal_garbage_collected
      graph_function, _ = self._maybe_define_function(args, kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 3292, in _maybe_define_function
      graph_function = self._create_graph_function(args, kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 3140, in _create_graph_function
      capture_by_value=self._capture_by_value),
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 1161, in func_graph_from_py_func
      func_outputs = python_func(*func_args, **func_kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 677, in wrapped_fn
      out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 1143, in autograph_handler
      user_requested=True,
    File ""<ipython-input-20-949802b44a4a>"", line 5, in train_step
      predictions,_ = transformer([inp, tar],
    File ""/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py"", line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""<ipython-input-14-ac60aac10b0e>"", line 131, in call
      if self.scvs == None:
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1321, in if_stmt
      _py_if_stmt(cond, body, orelse)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1374, in _py_if_stmt
      return body() if cond else orelse()
    File ""<ipython-input-14-ac60aac10b0e>"", line 133, in call
      scvs = tf.expand_dims(scvs, axis=1)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py"", line 150, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py"", line 1082, in op_dispatch_handler
      return dispatch_target(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 435, in expand_dims_v2
      return gen_array_ops.expand_dims(input, axis, name)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 2354, in expand_dims
      ""ExpandDims"", input=input, dim=axis, name=name)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 742, in _apply_op_helper
      attrs=attr_protos, op_def=op_def)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 695, in _create_op_internal
      compute_device)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 3784, in _create_op_internal
      op_def=op_def)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 2175, in __init__
      self._traceback = tf_stack.extract_stack_for_node(self._c_op)

The tensor <tf.Tensor 'hi_d_transformer/ExpandDims:0' shape=(64, 1, 64) dtype=float32> cannot be accessed from here, because it was defined in FuncGraph(name=train_step, id=140104354578128), which is out of scope.
```

I expect the model to compile, I have tried to move the tf.expand_dims to other layers/subclasses and still get the same bug no matter where. I guess this could be rooted from somewhere else as well but I cannot understand the error.


**Standalone code to reproduce the issue**

Reproducible test case can be found here:  https://colab.research.google.com/gist/ae20cg/fe1577f53f6e68db1e3429643f1e957a/tft2tf2-graph.ipynb

"
54532,tape.gradient returns None when assign is used in a model for computation of a function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Springdale
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.8.9
- Python version: 3.7.10+


**Describe the current behavior**
When a function is calculated based on a weight of a model the gradient can't trace the connection and returns None. More precisely I am trying to define a log_posterior from a model that has multiple weights (a shallow neural network, if you will). But Hamiltonian Monte Carlo does not work because my defined log_posterior does not have a gradient.


**Describe the expected behavior**
TF should return the gradient of the log_posterior function defined.


**Standalone code to reproduce the issue**
 In the example below tape.gradient should return the value of w as the gradient. Here DummyModel's call supposed to replicate a log_postrior function, where I reassign its parameter ""w"" with ""x"". 

```
>>> import tensorflow as tf
>>> class DummyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        pass
    def build(self, input_shape):
        self.w = self.add_weight('w', shape=(), dtype=tf.float64, initializer=lambda x, dtype:tf.constant(10., dtype=tf.float64))
    def call(self, X):
        return tf.cast(X, tf.float64) * self.w
... 
>>> a = DummyModel()
>>> print(""the output of DummyModel for X=1:"", a(1.))
the output of DummyModel for X=1: tf.Tensor(10.0, shape=(), dtype=float64)
>>> x = tf.Variable(3., dtype=tf.float64)
>>> with tf.GradientTape() as tape:
    tape.watch([x, a.w])
    a.w.assign(x)
    out = a(15.)
... 
<tf.Variable 'UnreadVariable' shape=() dtype=float64, numpy=3.0>
>>> print(tape.gradient(out, x))
None

```

"
54518,Unit tests broken by commit of quantization passes,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.8.10
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 5.0.0
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

Attempting to run unit tests results in build failure

# Execution platform: @local_execution_config_platform//:platform
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc: In function 'bool mlir::quant::{anonymous}::IsExported(mlir::FuncOp&)':
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:66:71: error: no matching function for call to 'mlir::Operation::getAttrOfType<mlir::ArrayAttr>(const absl::lts_20211102::string_view&)'
   66 |   auto exported_names = op->getAttrOfType<ArrayAttr>(kExportedNameAttr);
      |                                                                       ^
In file included from external/llvm-project/mlir/include/mlir/IR/OpDefinition.h:23,
                 from external/llvm-project/mlir/include/mlir/Dialect/Arithmetic/IR/Arithmetic.h:12,
                 from external/llvm-project/mlir/include/mlir/Dialect/ControlFlow/IR/ControlFlow.h:16,
                 from external/llvm-project/mlir/include/mlir/Dialect/StandardOps/IR/Ops.h:17,
                 from tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:21:
external/llvm-project/mlir/include/mlir/IR/Operation.h:326:13: note: candidate: 'AttrClass mlir::Operation::getAttrOfType(mlir::StringAttr) [with AttrClass = mlir::ArrayAttr]'
  326 |   AttrClass getAttrOfType(StringAttr name) {
      |             ^~~~~~~~~~~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:326:38: note:   no known conversion for argument 1 from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'
  326 |   AttrClass getAttrOfType(StringAttr name) {
      |                           ~~~~~~~~~~~^~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:330:13: note: candidate: 'AttrClass mlir::Operation::getAttrOfType(llvm::StringRef) [with AttrClass = mlir::ArrayAttr]'
  330 |   AttrClass getAttrOfType(StringRef name) {
      |             ^~~~~~~~~~~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:330:37: note:   no known conversion for argument 1 from 'const absl::lts_20211102::string_view' to 'llvm::StringRef'
  330 |   AttrClass getAttrOfType(StringRef name) {
      |                           ~~~~~~~~~~^~~~
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc: In function 'bool mlir::quant::{anonymous}::IsEntryFunction(mlir::FuncOp&)':
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:71:73: error: no matching function for call to 'mlir::Operation::hasAttr(const absl::lts_20211102::string_view&)'
   71 | bool IsEntryFunction(FuncOp& op) { return op->hasAttr(kEntryFunctionAttr); }
      |                                                                         ^
In file included from external/llvm-project/mlir/include/mlir/IR/OpDefinition.h:23,
                 from external/llvm-project/mlir/include/mlir/Dialect/Arithmetic/IR/Arithmetic.h:12,
                 from external/llvm-project/mlir/include/mlir/Dialect/ControlFlow/IR/ControlFlow.h:16,
                 from external/llvm-project/mlir/include/mlir/Dialect/StandardOps/IR/Ops.h:17,
                 from tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:21:
external/llvm-project/mlir/include/mlir/IR/Operation.h:336:8: note: candidate: 'bool mlir::Operation::hasAttr(mlir::StringAttr)'
  336 |   bool hasAttr(StringAttr name) { return attrs.contains(name); }
      |        ^~~~~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:336:27: note:   no known conversion for argument 1 from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'
  336 |   bool hasAttr(StringAttr name) { return attrs.contains(name); }
      |                ~~~~~~~~~~~^~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:337:8: note: candidate: 'bool mlir::Operation::hasAttr(llvm::StringRef)'
  337 |   bool hasAttr(StringRef name) { return attrs.contains(name); }
      |        ^~~~~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:337:26: note:   no known conversion for argument 1 from 'const absl::lts_20211102::string_view' to 'llvm::StringRef'
  337 |   bool hasAttr(StringRef name) { return attrs.contains(name); }
      |                ~~~~~~~~~~^~~~
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc: In function 'bool mlir::quant::{anonymous}::CreateMainFunction(mlir::ModuleOp&)':
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:127:71: error: no matching function for call to 'mlir::Operation::getAttrOfType<mlir::DictionaryAttr>(const absl::lts_20211102::string_view&)'
  127 |             function->getAttrOfType<DictionaryAttr>(kEntryFunctionAttr)) {
      |                                                                       ^
In file included from external/llvm-project/mlir/include/mlir/IR/OpDefinition.h:23,
                 from external/llvm-project/mlir/include/mlir/Dialect/Arithmetic/IR/Arithmetic.h:12,
                 from external/llvm-project/mlir/include/mlir/Dialect/ControlFlow/IR/ControlFlow.h:16,
                 from external/llvm-project/mlir/include/mlir/Dialect/StandardOps/IR/Ops.h:17,
                 from tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:21:
external/llvm-project/mlir/include/mlir/IR/Operation.h:326:13: note: candidate: 'AttrClass mlir::Operation::getAttrOfType(mlir::StringAttr) [with AttrClass = mlir::DictionaryAttr]'
  326 |   AttrClass getAttrOfType(StringAttr name) {
      |             ^~~~~~~~~~~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:326:38: note:   no known conversion for argument 1 from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'
  326 |   AttrClass getAttrOfType(StringAttr name) {
      |                           ~~~~~~~~~~~^~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:330:13: note: candidate: 'AttrClass mlir::Operation::getAttrOfType(llvm::StringRef) [with AttrClass = mlir::DictionaryAttr]'
  330 |   AttrClass getAttrOfType(StringRef name) {
      |             ^~~~~~~~~~~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:330:37: note:   no known conversion for argument 1 from 'const absl::lts_20211102::string_view' to 'llvm::StringRef'
  330 |   AttrClass getAttrOfType(StringRef name) {
      |                           ~~~~~~~~~~^~~~
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:130:40: error: expected primary-expression before '>' token
  130 |             inputs_attr.cast<StringAttr>().getValue().str();
      |                                        ^
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:130:42: error: expected primary-expression before ')' token
  130 |             inputs_attr.cast<StringAttr>().getValue().str();
      |                                          ^
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:138:41: error: expected primary-expression before '>' token
  138 |             outputs_attr.cast<StringAttr>().getValue().str();
      |                                         ^
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:138:43: error: expected primary-expression before ')' token
  138 |             outputs_attr.cast<StringAttr>().getValue().str();
      |                                           ^
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:160:65: error: no matching function for call to 'mlir::StringAttr::get(mlir::MLIRContext*&, const absl::lts_20211102::string_view&)'
  160 |   main_func->setAttr(StringAttr::get(context, kEntryFunctionAttr), dictAttr);
      |                                                                 ^
In file included from external/llvm-project/mlir/include/mlir/IR/BuiltinAttributes.h:744,
                 from external/llvm-project/mlir/include/mlir/IR/OperationSupport.h:18,
                 from external/llvm-project/mlir/include/mlir/IR/Dialect.h:16,
                 from external/llvm-project/mlir/include/mlir/Dialect/Arithmetic/IR/Arithmetic.h:11,
                 from external/llvm-project/mlir/include/mlir/Dialect/ControlFlow/IR/ControlFlow.h:16,
                 from external/llvm-project/mlir/include/mlir/Dialect/StandardOps/IR/Ops.h:17,
                 from tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:21:
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen/mlir/IR/BuiltinAttributes.h.inc:572:21: note: candidate: 'static mlir::StringAttr mlir::StringAttr::get(const llvm::Twine&, mlir::Type)'
  572 |   static StringAttr get(const Twine &bytes, Type type);
      |                     ^~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen/mlir/IR/BuiltinAttributes.h.inc:572:38: note:   no known conversion for argument 1 from 'mlir::MLIRContext*' to 'const llvm::Twine&'
  572 |   static StringAttr get(const Twine &bytes, Type type);
      |                         ~~~~~~~~~~~~~^~~~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen/mlir/IR/BuiltinAttributes.h.inc:573:21: note: candidate: 'static mlir::StringAttr mlir::StringAttr::get(mlir::MLIRContext*, const llvm::Twine&)'
  573 |   static StringAttr get(::mlir::MLIRContext *context, const Twine &bytes);
      |                     ^~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen/mlir/IR/BuiltinAttributes.h.inc:573:68: note:   no known conversion for argument 2 from 'const absl::lts_20211102::string_view' to 'const llvm::Twine&'
  573 |   static StringAttr get(::mlir::MLIRContext *context, const Twine &bytes);
      |                                                       ~~~~~~~~~~~~~^~~~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen/mlir/IR/BuiltinAttributes.h.inc:574:21: note: candidate: 'static mlir::StringAttr mlir::StringAttr::get(mlir::MLIRContext*)'
  574 |   static StringAttr get(::mlir::MLIRContext *context);
      |                     ^~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen/mlir/IR/BuiltinAttributes.h.inc:574:21: note:   candidate expects 1 argument, 2 provided
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:161:74: error: no matching function for call to 'mlir::Operation::setAttr(const absl::lts_20211102::string_view&, mlir::ArrayAttr)'
  161 |   main_func->setAttr(kExportedNameAttr, builder.getStrArrayAttr({""main""}));
      |                                                                          ^
In file included from external/llvm-project/mlir/include/mlir/IR/OpDefinition.h:23,
                 from external/llvm-project/mlir/include/mlir/Dialect/Arithmetic/IR/Arithmetic.h:12,
                 from external/llvm-project/mlir/include/mlir/Dialect/ControlFlow/IR/ControlFlow.h:16,
                 from external/llvm-project/mlir/include/mlir/Dialect/StandardOps/IR/Ops.h:17,
                 from tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:21:
external/llvm-project/mlir/include/mlir/IR/Operation.h:346:8: note: candidate: 'void mlir::Operation::setAttr(mlir::StringAttr, mlir::Attribute)'
  346 |   void setAttr(StringAttr name, Attribute value) {
      |        ^~~~~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:346:27: note:   no known conversion for argument 1 from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'
  346 |   void setAttr(StringAttr name, Attribute value) {
      |                ~~~~~~~~~~~^~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:351:8: note: candidate: 'void mlir::Operation::setAttr(llvm::StringRef, mlir::Attribute)'
  351 |   void setAttr(StringRef name, Attribute value) {
      |        ^~~~~~~
external/llvm-project/mlir/include/mlir/IR/Operation.h:351:26: note:   no known conversion for argument 1 from 'const absl::lts_20211102::string_view' to 'llvm::StringRef'
  351 |   void setAttr(StringRef name, Attribute value) {
      |                ~~~~~~~~~~^~~~
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:175:79: error: no matching function for call to 'mlir::FuncOp::setArgAttr(int&, const absl::lts_20211102::string_view&, mlir::ArrayAttr)'
  175 |                              {mlir::StringAttr::get(context, input_names[i])}));
      |                                                                               ^
In file included from external/llvm-project/mlir/include/mlir/IR/FunctionInterfaces.h:294,
                 from external/llvm-project/mlir/include/mlir/IR/BuiltinOps.h:16,
                 from external/llvm-project/mlir/include/mlir/Pass/Pass.h:12,
                 from tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:24:
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen/mlir/IR/FunctionOpInterfaces.h.inc:844:10: note: candidate: 'void mlir::detail::FunctionOpInterfaceTrait<ConcreteOp>::setArgAttr(unsigned int, mlir::StringAttr, mlir::Attribute) [with ConcreteOp = mlir::FuncOp]'
  844 |     void setArgAttr(unsigned index, StringAttr name, Attribute value) {
      |          ^~~~~~~~~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen/mlir/IR/FunctionOpInterfaces.h.inc:844:48: note:   no known conversion for argument 2 from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'
  844 |     void setArgAttr(unsigned index, StringAttr name, Attribute value) {
      |                                     ~~~~~~~~~~~^~~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen/mlir/IR/FunctionOpInterfaces.h.inc:847:10: note: candidate: 'void mlir::detail::FunctionOpInterfaceTrait<ConcreteOp>::setArgAttr(unsigned int, llvm::StringRef, mlir::Attribute) [with ConcreteOp = mlir::FuncOp]'
  847 |     void setArgAttr(unsigned index, StringRef name, Attribute value) {
      |          ^~~~~~~~~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen/mlir/IR/FunctionOpInterfaces.h.inc:847:47: note:   no known conversion for argument 2 from 'const absl::lts_20211102::string_view' to 'llvm::StringRef'
  847 |     void setArgAttr(unsigned index, StringRef name, Attribute value) {
      |                                     ~~~~~~~~~~^~~~
tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:183:72: error: no matching function for call to 'mlir::FuncOp::setResultAttr(int&, const absl::lts_20211102::string_view&, mlir::ArrayAttr)'
  183 |             context, {mlir::StringAttr::get(context, output_names[i])}));
      |                                                                        ^
In file included from external/llvm-project/mlir/include/mlir/IR/FunctionInterfaces.h:294,
                 from external/llvm-project/mlir/include/mlir/IR/BuiltinOps.h:16,
                 from external/llvm-project/mlir/include/mlir/Pass/Pass.h:12,
                 from tensorflow/compiler/mlir/quantization/tensorflow/passes/insert_main_function.cc:24:
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen/mlir/IR/FunctionOpInterfaces.h.inc:937:10: note: candidate: 'void mlir::detail::FunctionOpInterfaceTrait<ConcreteOp>::setResultAttr(unsigned int, mlir::StringAttr, mlir::Attribute) [with ConcreteOp = mlir::FuncOp]'
  937 |     void setResultAttr(unsigned index, StringAttr name, Attribute value) {
      |          ^~~~~~~~~~~~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen/mlir/IR/FunctionOpInterfaces.h.inc:937:51: note:   no known conversion for argument 2 from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'
  937 |     void setResultAttr(unsigned index, StringAttr name, Attribute value) {
      |                                        ~~~~~~~~~~~^~~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen/mlir/IR/FunctionOpInterfaces.h.inc:940:10: note: candidate: 'void mlir::detail::FunctionOpInterfaceTrait<ConcreteOp>::setResultAttr(unsigned int, llvm::StringRef, mlir::Attribute) [with ConcreteOp = mlir::FuncOp]'
  940 |     void setResultAttr(unsigned index, StringRef name, Attribute value) {
      |          ^~~~~~~~~~~~~
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen/mlir/IR/FunctionOpInterfaces.h.inc:940:50: note:   no known conversion for argument 2 from 'const absl::lts_20211102::string_view' to 'llvm::StringRef'
  940 |     void setResultAttr(unsigned index, StringRef name, Attribute value) {
      |                                        ~~~~~~~~~~^~~~
INFO: Elapsed time: 3462.723s, Critical Path: 583.59s
INFO: 8852 processes: 2320 internal, 6532 local.
FAILED: Build did NOT complete successfully


**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --config=nonccl --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu -- //tensorflow/compiler/mlir/... -//tensorflow/compiler/mlir/lite/tests:const-fold.mlir.test -//tensorflow/compiler/mlir/lite/tests:prepare-tf.mlir.test

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Started with commit https://github.com/tensorflow/tensorflow/commit/f7f5f451ddac92076a54978ce861a74529fb2e86
"
54517,Tensorflow-lite with Flex Delegates build fails for aarch64 source architecture using TF2.7,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 583dc6ac55ff 5.4.144+
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.7.0 
- Python version: 3.7.12
- Bazel version (if compiling from source): 3.7.2

**Describe the current behavior**
I'm trying to build a shared library for tensorflow-lite including the Flex Delegate by Modifying the BUILD to add following dependency:
```
""//tensorflow/lite/delegates/flex:delegate"",
```
Since I'm targeting a Linux system running on my aarch64 board, the command I ran is the below:
```
bazel build --config=monolithic --config=noaws --config=nogcp --config=nohdfs \
 --config=nonccl --config=elinux_aarch64 \
 --experimental_ui_max_stdouterr_bytes=1073741819 -c opt --cxxopt=--std=c++14 \
 //tensorflow/lite:libtensorflowlite.so
```
I also made sure to hide my OpenSSL from my /usr/include as suggested in the following issue https://github.com/tensorflow/tensorflow/issues/48401. I noticed that the build fails at compiling **icu** external source code.

**Describe the expected behavior**
The expected behavior is a build completed successfully and a shared library libtensorflow-lite.so generated and supporting Flex Delegates with TF selected Ops.

**Standalone code to reproduce the issue**
[Here](https://colab.research.google.com/drive/1Re6cVzm0mZBrbNYsH2o9WEtin1YTMnsE?usp=sharing) is a Google Colab gist to reproduce the Build issue. All you need to do is to modify the tensorflow/lite/BUILD by adding the following line to the libtensorflow-lite.so deps. 
```
""//tensorflow/lite/delegates/flex:delegate"",
```

**Other info / logs** 
Here is the full traceback of the error that shows in the middle of the build and makes it fail.
```
ERROR: /root/.cache/bazel/_bazel_root/889612a75a81b3d8b4ed860522ba4e34/external/icu/BUILD.bazel:33:11: C++ compilation of rule '@icu//:icuuc' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command /root/.cache/bazel/_bazel_root/889612a75a81b3d8b4ed860522ba4e34/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections ... (remaining 41 argument(s) skipped)
external/icu/icu4c/source/common/localebuilder.cpp: In member function 'icu_60::LocaleBuilder& icu_60::LocaleBuilder::setLanguageTag(icu_60::StringPiece)':
external/icu/icu4c/source/common/localebuilder.cpp:63:24: error: 'forLanguageTag' is not a member of 'icu_60::Locale'
     Locale l = Locale::forLanguageTag(tag, status_);
                        ^~~~~~~~~~~~~~
external/icu/icu4c/source/common/localebuilder.cpp: In function 'void icu_60::_copyExtensions(const icu_60::Locale&, icu_60::Locale*, bool, UErrorCode&)':
external/icu/icu4c/source/common/localebuilder.cpp:166:23: error: invalid use of incomplete type 'class icu_60::StringEnumeration'
     while ((key = iter->next(nullptr, errorCode)) != nullptr) {
                       ^~
In file included from external/icu/icu4c/source/common/unicode/uloc.h:27,
                 from external/icu/icu4c/source/common/ulocimp.h:14,
                 from external/icu/icu4c/source/common/localebuilder.cpp:9:
/usr/include/unicode/uenum.h:27:7: note: forward declaration of 'class icu_60::StringEnumeration'
 class StringEnumeration;
       ^~~~~~~~~~~~~~~~~
external/icu/icu4c/source/common/localebuilder.cpp:169:50: error: no matching function for call to 'icu_60::Locale::getKeywordValue(const char*&, icu_60::CharStringByteSink&, UErrorCode&) const'
         from.getKeywordValue(key, sink, errorCode);
                                                  ^
In file included from external/icu/icu4c/source/common/unicode/localebuilder.h:6,
                 from external/icu/icu4c/source/common/localebuilder.cpp:10:
/usr/include/unicode/locid.h:457:13: note: candidate: 'int32_t icu_60::Locale::getKeywordValue(const char*, char*, int32_t, UErrorCode&) const'
     int32_t getKeywordValue(const char* keywordName, char *buffer, int32_t bufferCapacity, UErrorCode &status) const;
             ^~~~~~~~~~~~~~~
/usr/include/unicode/locid.h:457:13: note:   candidate expects 4 arguments, 3 provided
external/icu/icu4c/source/common/localebuilder.cpp: In function 'void icu_60::_clearUAttributesAndKeyType(icu_60::Locale*, UErrorCode&)':
external/icu/icu4c/source/common/localebuilder.cpp:191:55: error: 'class icu_60::Locale' has no member named 'createUnicodeKeywords'; did you mean 'createKeywords'?
     LocalPointer<icu::StringEnumeration> iter(locale->createUnicodeKeywords(errorCode));
                                                       ^~~~~~~~~~~~~~~~~~~~~
                                                       createKeywords
external/icu/icu4c/source/common/localebuilder.cpp:194:23: error: invalid use of incomplete type 'class icu_60::StringEnumeration'
     while ((key = iter->next(nullptr, errorCode)) != nullptr) {
                       ^~
In file included from external/icu/icu4c/source/common/unicode/uloc.h:27,
                 from external/icu/icu4c/source/common/ulocimp.h:14,
                 from external/icu/icu4c/source/common/localebuilder.cpp:9:
/usr/include/unicode/uenum.h:27:7: note: forward declaration of 'class icu_60::StringEnumeration'
 class StringEnumeration;
       ^~~~~~~~~~~~~~~~~
external/icu/icu4c/source/common/localebuilder.cpp:195:17: error: 'class icu_60::Locale' has no member named 'setUnicodeKeywordValue'; did you mean 'setKeywordValue'?
         locale->setUnicodeKeywordValue(key, nullptr, errorCode);
                 ^~~~~~~~~~~~~~~~~~~~~~
                 setKeywordValue
external/icu/icu4c/source/common/localebuilder.cpp: In function 'void icu_60::_setUnicodeExtensions(icu_60::Locale*, const icu_60::CharString&, UErrorCode&)':
external/icu/icu4c/source/common/localebuilder.cpp:206:17: error: 'forLanguageTag' is not a member of 'icu_60::Locale'
         Locale::forLanguageTag(locale_str.data(), errorCode),
                 ^~~~~~~~~~~~~~
external/icu/icu4c/source/common/localebuilder.cpp: In member function 'icu_60::LocaleBuilder& icu_60::LocaleBuilder::setExtension(char, icu_60::StringPiece)':
external/icu/icu4c/source/common/localebuilder.cpp:235:45: error: no matching function for call to 'icu_60::Locale::setKeywordValue(icu_60::StringPiece, char*, UErrorCode&)'
                                      status_);
                                             ^
In file included from external/icu/icu4c/source/common/unicode/localebuilder.h:6,
                 from external/icu/icu4c/source/common/localebuilder.cpp:10:
/usr/include/unicode/locid.h:473:10: note: candidate: 'void icu_60::Locale::setKeywordValue(const char*, const char*, UErrorCode&)'
     void setKeywordValue(const char* keywordName, const char* keywordValue, UErrorCode &status);
          ^~~~~~~~~~~~~~~
/usr/include/unicode/locid.h:473:10: note:   no known conversion for argument 1 from 'icu_60::StringPiece' to 'const char*'
external/icu/icu4c/source/common/localebuilder.cpp: In member function 'icu_60::LocaleBuilder& icu_60::LocaleBuilder::setUnicodeLocaleKeyword(icu_60::StringPiece, icu_60::StringPiece)':
external/icu/icu4c/source/common/localebuilder.cpp:263:18: error: 'class icu_60::Locale' has no member named 'setUnicodeKeywordValue'; did you mean 'setKeywordValue'?
     extensions_->setUnicodeKeywordValue(key, type, status_);
                  ^~~~~~~~~~~~~~~~~~~~~~
                  setKeywordValue
external/icu/icu4c/source/common/localebuilder.cpp: In member function 'icu_60::LocaleBuilder& icu_60::LocaleBuilder::addUnicodeLocaleAttribute(icu_60::StringPiece)':
external/icu/icu4c/source/common/localebuilder.cpp:290:69: error: no matching function for call to 'icu_60::Locale::getKeywordValue(const char*&, icu_60::CharStringByteSink&, UErrorCode&)'
     extensions_->getKeywordValue(kAttributeKey, sink, localErrorCode);
                                                                     ^
In file included from external/icu/icu4c/source/common/unicode/localebuilder.h:6,
                 from external/icu/icu4c/source/common/localebuilder.cpp:10:
/usr/include/unicode/locid.h:457:13: note: candidate: 'int32_t icu_60::Locale::getKeywordValue(const char*, char*, int32_t, UErrorCode&) const'
     int32_t getKeywordValue(const char* keywordName, char *buffer, int32_t bufferCapacity, UErrorCode &status) const;
             ^~~~~~~~~~~~~~~
/usr/include/unicode/locid.h:457:13: note:   candidate expects 4 arguments, 3 provided
external/icu/icu4c/source/common/localebuilder.cpp: In member function 'icu_60::LocaleBuilder& icu_60::LocaleBuilder::removeUnicodeLocaleAttribute(icu_60::StringPiece)':
external/icu/icu4c/source/common/localebuilder.cpp:344:69: error: no matching function for call to 'icu_60::Locale::getKeywordValue(const char*&, icu_60::CharStringByteSink&, UErrorCode&)'
     extensions_->getKeywordValue(kAttributeKey, sink, localErrorCode);
                                                                     ^
In file included from external/icu/icu4c/source/common/unicode/localebuilder.h:6,
                 from external/icu/icu4c/source/common/localebuilder.cpp:10:
/usr/include/unicode/locid.h:457:13: note: candidate: 'int32_t icu_60::Locale::getKeywordValue(const char*, char*, int32_t, UErrorCode&) const'
     int32_t getKeywordValue(const char* keywordName, char *buffer, int32_t bufferCapacity, UErrorCode &status) const;
             ^~~~~~~~~~~~~~~
/usr/include/unicode/locid.h:457:13: note:   candidate expects 4 arguments, 3 provided
Target //tensorflow/lite:libtensorflowlite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 11557.297s, Critical Path: 288.22s
INFO: 5169 processes: 1657 internal, 3512 local.
FAILED: Build did NOT complete successfully
```
"
54516,"failed: (Exit 2): cl.exe failed: error executing command，Bazel=5.0.0, tf=2.8.0,MSVC2019","Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: F:/tensorflow/tensorflow/core/framework/BUILD:1400:31 Middleman _middlemen/_S_Stensorflow_Score_Sframework_Cattr_Uvalue_Uproto_Utext-BazelCppSemantics_build_arch_x64_windows-opt failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/manyuan/_bazel_manyuan/2qttxlm7/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\cppwinrt
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.16299.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/JetBrain/python/python.exe
    SET PYTHON_LIB_PATH=D:/JetBrain/python/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\manyuan\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\manyuan\AppData\Local\Temp
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/core/platform/windows/_objs/platform_port/port.obj.params
# Configuration: 93a4c43a10f53f2edc71827668aaf369d1f071b1a7a163cf0701c8d4b7b35e12
# Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 17.366s, Critical Path: 8.04s
INFO: 65 processes: 26 internal, 39 local.
FAILED: Build did NOT complete successfully"
54513,Links to some of the guides are broken,"Links to [guide/basics](https://www.tensorflow.org/guide/basics), [guide/autodiff](https://www.tensorflow.org/guide/autodiff), [guide/basic_training_loops](https://www.tensorflow.org/guide/basic_training_loops), [guide/keras/functional](https://www.tensorflow.org/guide/keras/functional), [guide/keras/train_and_evaluate](https://www.tensorflow.org/guide/keras/train_and_evaluate), [guide/keras/rnn](https://www.tensorflow.org/guide/keras/rnn), [guide/keras/transfer_learning](https://www.tensorflow.org/guide/keras/transfer_learning), [guide/advanced_autodiff](https://www.tensorflow.org/guide/advanced_autodiff), [guide/tf_numpy](https://www.tensorflow.org/guide/tf_numpy), [guide/data](https://www.tensorflow.org/guide/data), [guide/data_performance](https://www.tensorflow.org/guide/data_performance) and [guide/saved_model](https://www.tensorflow.org/guide/saved_model) are broken. They are responding with .ipynb files.

Please look into it."
54512,SavedModel guide shows raw Jupyter code instead of HTML,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/guide/saved_model

## Description of issue (what needs changing):

### Clear description

The above URL shows raw Jupyter notebook code, instead of HTML (as in other pages in the documentation)

### Correct links

-

### Parameters defined

-

### Returns defined

-

### Raises listed and defined

-

### Usage example

-

### Request visuals, if applicable

-

### Submit a pull request?

-"
54511,"""std::string is no longer a scalar type, use tensorflow::tstring""","I use python 3.9 and have successfully built tensorflow 2.7 on Windows, But when I build my own code with tensorflow, I get the error as below, please help, thanks!
![image](https://user-images.githubusercontent.com/49086407/155453100-f6558009-306d-4aa7-a0b6-e2f6644717f9.png)
"
54509,AttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'EqualGraphDefWrapper',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): The code is from a conda module which is not continually developed.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WSL Ubuntu 20.04.3 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from: conda package 
- TensorFlow version (use command below): 2.4.1
- TensorFlow-GPU: 2.4.1
- Python version: 3.7.11
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 7.6.5
- GPU model and memory: NVIDIA-SMI 510.00 (4GB)

** Description:
This code is developed in Python 3.6 and TF1. Now I want to upgrade it to run in Python 3.7 and TF2. I followed the instructions to migrate TF1 to TF2 (https://www.tensorflow.org/guide/migrate/migrate_tf2) and fix almost all errors except this AttributeError. I am wondering how to fix it or do we have similar functions to compare graphs in TF2. Thank you. 

**Code:
@pytest.mark.parametrize('conv_patch', (2, 5, 10), ids=lambda x: 'c=%s' % x)
@pytest.mark.parametrize('pool_patch', (2, 3), ids=lambda x: 'p=%s' % x)
def test_convolve3D(conv_patch, pool_patch):
    from tfbio.net import hidden_conv3D, convolve3D

    out_chnls = [8, 16]

    g1 = tf.Graph()
    with g1.as_default():
        x = tf.compat.v1.placeholder(tf.float32, shape=(None, 21, 21, 21, 19))
        h11 = hidden_conv3D(x, out_chnls[0], conv_patch=conv_patch,
                            pool_patch=pool_patch, name='conv0')
        h12 = hidden_conv3D(h11, out_chnls[1], conv_patch=conv_patch,
                            pool_patch=pool_patch, name='conv1')
    def1 = g1.as_graph_def().SerializeToString()

    g2 = tf.Graph()
    with g2.as_default():
        x = tf.compat.v1.placeholder(tf.float32, shape=(None, 21, 21, 21, 19))
        h2 = convolve3D(x, out_chnls, conv_patch=conv_patch,
                        pool_patch=pool_patch)
    def2 = g2.as_graph_def().SerializeToString()

    # graphs should be identical
    assert not pywrap_tensorflow.EqualGraphDefWrapper(def1, def2)


**Describe the current behavior**
FAILED net_test.py::test_convolve3D[p=2-c=2] - AttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'EqualGraphDefWrapper'

**Describe the expected behavior**
Pass the pytest

"
54507,keras GRU RNN tflite conversion wrong model ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  
- TensorFlow version (use command below): tensorflow v2.8.0 lite converter 
- Python version: python v3.8
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: not sure
- GPU model and memory:

Find its' readme there is a guideline to recreate model 
(https://github.com/ARM-software/ML-zoo/blob/master/models/noise_suppression/RNNoise/tflite_int8/recreate_model/README.md)

**Describe the current behavior**

TFLite converter convert model within GRU model , an example can found here:
https://github.com/ARM-software/ML-zoo/blob/master/models/noise_suppression/RNNoise/tflite_int8/rnnoise_INT8.tflite

I found that GRU kernel process r\*wr+i\*wi+bias convert to tflite is wrong, it's convert to
two fc ops and ADD (r\*wr+bias) + (i\*wi+bias) and each biases are the same, so the result bias will be duplicated.
BTW I found LSTM has no such problem, it only convert without bias, and add one bias later.
![image](https://user-images.githubusercontent.com/25763259/155436120-e4838408-0642-4530-a220-621594f82aab.png)

**Describe the expected behavior**
single FC bias only 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54506,tf.argmax() function and documentation inconsistent with numpy.argmax() implementation,"--Issue. 
`tensorflow` documentation and implementation of `argmax()`  threw me off.  
`tensorflow` starts with the same definition as `numpy` when it comes to the  'axis' parameter.

#src: https://numpy.org/doc/stable/reference/generated/numpy.argmax.html
numpy.argmax(a, **axis=None**, out=None, *, keepdims=<no value>)

#src: https://www.tensorflow.org/api_docs/python/tf/math/argmax
tf.math.argmax( input, **axis=None**, output_type=tf.dtypes.int64, name=None)
However, the documentation down the page shows that `axis` really defaults to 0.
<html>
<body>
<!--StartFragment-->

axis | An integer, the axis to reduce across. Default to 0.
-- | --



<!--EndFragment-->
</body>
</html>

The code results differ between `numpy` and `tensorflow`.


--Proposition
If this is a feature and not a bug in `tensorflow`, let's have a consistent documentation.  If it should behave like `numpy`, let's correct the code and by default return a scalar.  (or let me know if this has been already addressed in a later version). 



--Reproducible code samples:

a) with `numpy`
A = np.array([[2, 20, 30, 3, 6], 
                 [3, 11, 16, 1, 8], 
                 [14, 45, 23, 5, 27]])
print(np.argmax(A, 0))
print(np.argmax(A, 1))
print(np.argmax(A, None))

Returns:
[2 2 0 2 2]
[2 2 1]
11

b) with `tensorflow`
B = tf.constant([[2, 20, 30, 3, 6], 
                 [3, 11, 16, 1, 8], 
                 [14, 45, 23, 5, 27]])
print(tf.math.argmax(B, 0))
print(tf.math.argmax(B, 1))
print(tf.math.argmax(B, None))

Returns:
tf.Tensor([2 2 0 2 2], shape=(5,), dtype=int64)
tf.Tensor([2 2 1], shape=(3,), dtype=int64)
tf.Tensor([2 2 0 2 2], shape=(5,), dtype=int64)"
54502,Cannot build in macOS M1 arq,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12.0.1 (21A559)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): None
- TensorFlow version: [1814bd1](https://github.com/tensorflow/tensorflow/commit/1814bd1192336064343361b23b4bb60af738a844)
- Python version: Python 3.8.12
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):5.0.0
- GCC/Compiler version (if compiling from source): Apple clang version 13.0.0
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**
```

➜  tensorflow git:(master) ✗ bazel build --jobs=10 --compilation_mode=opt --copt=-march=native tensorflow --cpu=darwin_arm64 --host_cpu=darwin_arm64
WARNING: Option 'java_toolchain' is deprecated
WARNING: Option 'host_java_toolchain' is deprecated
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=123
INFO: Reading rc options for 'build' from /Users/juan.crescente/Documents/lib/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/juan.crescente/Documents/lib/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --host_java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from /Users/juan.crescente/Documents/lib/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/juan.crescente/mambaforge/envs/mlp/bin/python3 --action_env PYTHON_LIB_PATH=/Users/juan.crescente/mambaforge/envs/mlp/lib/python3.8/site-packages --python_path=/Users/juan.crescente/mambaforge/envs/mlp/bin/python3
INFO: Reading rc options for 'build' from /Users/juan.crescente/Documents/lib/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /Users/juan.crescente/Documents/lib/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/juan.crescente/Documents/lib/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:macos in file /Users/juan.crescente/Documents/lib/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14
INFO: Build option --cpu has changed, discarding analysis cache.
ERROR: /private/var/tmp/_bazel_juan.crescente/7d4fd1924864d2d6fce922e4404d7479/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'
ERROR: /private/var/tmp/_bazel_juan.crescente/7d4fd1924864d2d6fce922e4404d7479/external/local_config_cc/BUILD:48:19: Analysis of target '@local_config_cc//:toolchain' failed
ERROR: Analysis of target '//tensorflow:tensorflow' failed; build aborted: 
INFO: Elapsed time: 0.694s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (126 packages loaded, 488 targets configured)
    currently loading: tensorflow/compiler/mlir/tensorflow ... (2 packages)
    Fetching @com_google_absl; fetching
    Fetching @nsync; fetching
    Fetching @llvm-project; fetching
    Fetching @eigen_archive; fetching
    Fetching ...e_absl; Extracting /private/var/tmp/_bazel_juan.crescente/7d4fd1924864d2d6fce922e4404d7479/external/com_g\
oogle_absl/temp9202958781266814507/215105818dfde3174fe799600bb0f3cae233d0bf.tar.gz
    Fetching .../nsync; Extracting /private/var/tmp/_bazel_juan.crescente/7d4fd1924864d2d6fce922e4404d7479/external/nsync\
/temp14220370431948591900/1.22.0.tar.gz
    Fetching ...rchive; Extracting /private/var/tmp/_bazel_juan.crescente/7d4fd1924864d2d6fce922e4404d7479/external/eigen\
_archive/temp7238515792960443584/eigen-7db0ac977acf276fb0817cfb89e490cdbae0ab56.tar.gz

```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`bazel build --jobs=10 --compilation_mode=opt --copt=-march=native tensorflow`"
54501,Native irfft (and inverse_stft) ops in TFLite,"**System information**
- TensorFlow version (you are using): 2.8.0 (Python 3.9.10)
- Are you willing to contribute it (Yes/No): No (or rather yes, but probably with a lot of help to find my way around)

**Describe the feature and the current behavior/state.**
Now that TFLite supports natively ops for tf.signal.rfft and tf.signal.stft (see [#27030](https://github.com/tensorflow/tensorflow/issues/27303)), it would be really great to close the loop and add support for their inverse functions tf.signal.irfft and tf.signal.inverse_stft to support more broadly 1-dimensional data models.

Right now, here is what happens with this MWE:
```import os
import tensorflow as tf
import tensorflow.keras as tfk
import tensorflow.keras.layers as tfkl

N = 128
fft_length = 1024

input = tfk.Input((N, fft_length // 2 + 1), dtype=tf.dtypes.complex64)
output = tfkl.Lambda(lambda x: tf.signal.irfft(x), output_shape=(N, fft_length))(input)

model = tfk.Model(inputs=input, outputs=output)

tf.get_logger().warning(""Begin Tensorflow test..."")
test_input = tf.complex(
    tf.random.normal((1, N, fft_length // 2 + 1), dtype=tf.dtypes.float32),
    tf.random.normal((1, N, fft_length // 2 + 1), dtype=tf.dtypes.float32))    
test_output = model(test_input)
tf.get_logger().warning(""Tensorflow test finished..."")

tf.get_logger().warning(""Begin TFLite conversion..."")
model.save('./tflite')
converter = tf.lite.TFLiteConverter.from_saved_model('./tflite')
tflite_model = converter.convert()
with open(os.path.join('./tflite', 'model.tflite'), ""wb"") as f:
    f.write(tflite_model)
tf.get_logger().warning(""TFLite conversion finished..."") 
```

Output:
```
2022-02-23 17:05:01.840430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-23 17:05:02.447990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-23 17:05:02.448941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-23 17:05:02.450203: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-02-23 17:05:02.450930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-23 17:05:02.451762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-23 17:05:02.452562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-23 17:05:03.149397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-23 17:05:03.150348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-23 17:05:03.151175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-23 17:05:03.152004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10794 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7
WARNING:tensorflow:Begin Tensorflow test...
WARNING:tensorflow:Tensorflow test finished...
WARNING:tensorflow:Begin TFLite conversion...
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
2022-02-23 17:05:08.281960: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-23 17:05:08.542623: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.
2022-02-23 17:05:08.542684: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.
2022-02-23 17:05:08.544198: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: ./tflite
2022-02-23 17:05:08.546087: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }
2022-02-23 17:05:08.546141: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: ./tflite
2022-02-23 17:05:08.549053: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.
2022-02-23 17:05:08.587791: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: ./tflite
2022-02-23 17:05:08.594120: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 49927 microseconds.
2022-02-23 17:05:08.602500: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(callsite(callsite(fused[""IRFFT:"", ""model/lambda/irfft@__inference__wrapped_model_39""] at fused[""PartitionedCall:"", ""PartitionedCall@__inference_signature_wrapper_138""]) at fused[""PartitionedCall:"", ""PartitionedCall""])): error: 'tf.IRFFT' op is neither a custom op nor a flex op
error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: IRFFT
Details:
	tf.IRFFT(tensor<?x128x513xcomplex<f32>>, tensor<1xi32>) -> (tensor<?x128x1024xf32>) : {device = """"}

Traceback (most recent call last):
  File ""/fsx/findsimilar-experiments/tflite_irfft_test.py"", line 24, in <module>
    tflite_model = converter.convert()
  File ""/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 803, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 789, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1084, in convert
    return self._convert_from_saved_model(graph_def)
  File ""/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 967, in _convert_from_saved_model
    result = _convert_saved_model(**converter_kwargs)
  File ""/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py"", line 213, in wrapper
    raise converter_error from None  # Re-throws the exception.
  File ""/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py"", line 206, in wrapper
    return func(*args, **kwargs)
  File ""/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 789, in convert_saved_model
    data = convert(
  File ""/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 306, in convert
    raise converter_error
tensorflow.lite.python.convert_phase.ConverterError: <unknown>:0: error: loc(callsite(callsite(fused[""IRFFT:"", ""model/lambda/irfft@__inference__wrapped_model_39""] at fused[""PartitionedCall:"", ""PartitionedCall@__inference_signature_wrapper_138""]) at fused[""PartitionedCall:"", ""PartitionedCall""])): 'tf.IRFFT' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""PartitionedCall:"", ""PartitionedCall""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""IRFFT:"", ""model/lambda/irfft@__inference__wrapped_model_39""] at fused[""PartitionedCall:"", ""PartitionedCall@__inference_signature_wrapper_138""]) at fused[""PartitionedCall:"", ""PartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: IRFFT
Details:
	tf.IRFFT(tensor<?x128x513xcomplex<f32>>, tensor<1xi32>) -> (tensor<?x128x1024xf32>) : {device = """"}
```

The TF Select workaround mentioned in the error message does seem to work, so the community may have a workaround in the meantime (with all the caveats mentioned in [https://www.tensorflow.org/lite/guide/ops_select](https://www.tensorflow.org/lite/guide/ops_select)), but I think it'd be in the best interest of the community to have the inverse operators supported natively whenever possible, for a much facilitated deployment of models.

**Will this change the current api? How?**

Not that I can think of, but maybe I don't fully understand the question.

**Who will benefit with this feature?**

Every TensorFlow user needing to deploy a TFLite model that relies on STFT processing, so that could cover many people working on projects involving audio data, financial data, or any other form of 1-dimensional data series.

**Any Other info**

Nothing. Just thanks to the community for keeping TensorFlow running and growing 👍 "
54500,Libtensorflowlite.so build failed for aarch64 with Flex Delegate included in dependencies,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux 0be7898f8b6f 5.4.144+ from Google Colab**
- Mobile device: **Raspberry Pi Aarch64 target architecture**
- TensorFlow installed from (source or binary): **Source**
- TensorFlow version (use command below): **2.7.0**
- Python version: **3.7.12**
- Bazel version (if compiling from source): **3.7.2**

**Describe the current behavior**
I'm trying to build a shared library for tensorflow-lite including the Flex Delegate by Modifying the BUILD to add following dependency:
```
""//tensorflow/lite/delegates/flex:delegate"",
```
Since I'm targeting a Linux system running on my aarch64 board, the command I ran is the below: 
```
bazel build --config=monolithic --config=elinux_aarch64 --define=with_select_tf_ops=true -c opt //tensorflow/lite:libtensorflowlite.so
```
**Describe the expected behavior**
The expected behavior is a build completing successfully. 

**Standalone code to reproduce the issue**
I provide a gist from google Colab to make you able to reproduce the issue. 

**Other info / logs**
```
ERROR: /root/.cache/bazel/_bazel_root/889612a75a81b3d8b4ed860522ba4e34/external/com_github_grpc_grpc/BUILD:1897:16: C++ compilation of rule '@com_github_grpc_grpc//:grpc_transport_chttp2_server_secure' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command /root/.cache/bazel/_bazel_root/889612a75a81b3d8b4ed860522ba4e34/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections ... (remaining 67 argument(s) skipped)
In file included from /usr/include/openssl/evp.h:16,
                 from /usr/include/openssl/x509.h:18,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,
                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,
                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,
                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,
                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:
/usr/include/openssl/bio.h:690:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'
 DEPRECATEDIN_1_1_0(int BIO_get_port(const char *str, unsigned short *port_ptr))
 ^~~~~~~~~~~~~~~~~~
In file included from /usr/include/openssl/asn1.h:23,
                 from /usr/include/openssl/objects.h:15,
                 from /usr/include/openssl/evp.h:28,
                 from /usr/include/openssl/x509.h:18,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,
                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,
                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,
                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,
                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:
/usr/include/openssl/bn.h:183:43: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?
 int BN_abs_is_word(const BIGNUM *a, const BN_ULONG w);
                                           ^~~~~~~~
                                           SHA_LONG
/usr/include/openssl/bn.h:186:39: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?
 int BN_is_word(const BIGNUM *a, const BN_ULONG w);
                                       ^~~~~~~~
                                       SHA_LONG
/usr/include/openssl/bn.h:214:22: error: 'BN_ULONG' was not declared in this scope
 int BN_num_bits_word(BN_ULONG l);
                      ^~~~~~~~
/usr/include/openssl/bn.h:214:22: note: suggested alternative: 'SHA_LONG'
 int BN_num_bits_word(BN_ULONG l);
                      ^~~~~~~~
                      SHA_LONG
/usr/include/openssl/bn.h:266:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?
 BN_ULONG BN_mod_word(const BIGNUM *a, BN_ULONG w);
 ^~~~~~~~
 SHA_LONG
/usr/include/openssl/bn.h:267:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?
 BN_ULONG BN_div_word(BIGNUM *a, BN_ULONG w);
 ^~~~~~~~
 SHA_LONG
/usr/include/openssl/bn.h:268:28: error: 'BN_ULONG' has not been declared
 int BN_mul_word(BIGNUM *a, BN_ULONG w);
                            ^~~~~~~~
/usr/include/openssl/bn.h:269:28: error: 'BN_ULONG' has not been declared
 int BN_add_word(BIGNUM *a, BN_ULONG w);
                            ^~~~~~~~
/usr/include/openssl/bn.h:270:28: error: 'BN_ULONG' has not been declared
 int BN_sub_word(BIGNUM *a, BN_ULONG w);
                            ^~~~~~~~
/usr/include/openssl/bn.h:271:28: error: 'BN_ULONG' has not been declared
 int BN_set_word(BIGNUM *a, BN_ULONG w);
                            ^~~~~~~~
/usr/include/openssl/bn.h:272:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?
 BN_ULONG BN_get_word(const BIGNUM *a);
 ^~~~~~~~
 SHA_LONG
/usr/include/openssl/bn.h:288:37: error: 'BN_ULONG' has not been declared
 int BN_mod_exp_mont_word(BIGNUM *r, BN_ULONG a, const BIGNUM *p,
                                     ^~~~~~~~
/usr/include/openssl/bn.h:323:24: error: variable or field 'BN_consttime_swap' declared void
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                        ^~~~~~~~
/usr/include/openssl/bn.h:323:24: error: 'BN_ULONG' was not declared in this scope
/usr/include/openssl/bn.h:323:24: note: suggested alternative: 'SHA_LONG'
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                        ^~~~~~~~
                        SHA_LONG
/usr/include/openssl/bn.h:323:46: error: expected primary-expression before '*' token
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                                              ^
/usr/include/openssl/bn.h:323:47: error: 'a' was not declared in this scope
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                                               ^
/usr/include/openssl/bn.h:323:57: error: expected primary-expression before '*' token
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                                                         ^
/usr/include/openssl/bn.h:323:58: error: 'b' was not declared in this scope
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                                                          ^
/usr/include/openssl/bn.h:323:61: error: expected primary-expression before 'int'
 void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);
                                                             ^~~
/usr/include/openssl/bn.h:332:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_0_9_8'
 DEPRECATEDIN_0_9_8(int
 ^~~~~~~~~~~~~~~~~~
/usr/include/openssl/bn.h:403:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_0_9_8'
 DEPRECATEDIN_0_9_8(int BN_get_params(int which)) /* 0, mul, 1 high, 2 low, 3
 ^~~~~~~~~~~~~~~~~~
In file included from /usr/include/openssl/objects.h:15,
                 from /usr/include/openssl/evp.h:28,
                 from /usr/include/openssl/x509.h:18,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,
                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,
                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,
                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,
                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:
/usr/include/openssl/asn1.h:555:7: error: expected constructor, destructor, or type conversion before 'unsigned'
 const unsigned char *ASN1_STRING_get0_data(const ASN1_STRING *x);
       ^~~~~~~~
In file included from /usr/include/openssl/x509.h:22,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,
                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,
                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,
                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,
                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:
/usr/include/openssl/ec.h:274:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_2_0'
 DEPRECATEDIN_1_2_0(int EC_GROUP_get_curve_GFp(const EC_GROUP *group, BIGNUM *p,
 ^~~~~~~~~~~~~~~~~~
/usr/include/openssl/ec.h:543:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_2_0'
 DEPRECATEDIN_1_2_0(int EC_POINT_get_affine_coordinates_GFp(const EC_GROUP *group,
 ^~~~~~~~~~~~~~~~~~
/usr/include/openssl/ec.h:631:1: error: expected constructor, destructor, or type conversion before 'size_t'
 size_t EC_POINT_point2oct(const EC_GROUP *group, const EC_POINT *p,
 ^~~~~~
In file included from /usr/include/openssl/x509.h:25,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,
                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,
                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,
                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,
                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:
/usr/include/openssl/rsa.h:239:1: error: expected constructor, destructor, or type conversion before 'int'
 int RSA_generate_key_ex(RSA *rsa, int bits, BIGNUM *e, BN_GENCB *cb);
 ^~~
In file included from /usr/include/openssl/dsa.h:25,
                 from /usr/include/openssl/x509.h:26,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,
                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,
                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,
                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,
                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:
/usr/include/openssl/dh.h:142:1: error: expected constructor, destructor, or type conversion before 'int'
 int DH_generate_parameters_ex(DH *dh, int prime_len, int generator,
 ^~~
In file included from /usr/include/openssl/x509.h:26,
                 from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,
                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,
                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,
                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,
                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:
/usr/include/openssl/dsa.h:103:1: error: expected constructor, destructor, or type conversion before 'int'
 int DSA_sign(int type, const unsigned char *dgst, int dlen,
 ^~~
/usr/include/openssl/dsa.h:127:1: error: expected constructor, destructor, or type conversion before 'int'
 int DSA_generate_parameters_ex(DSA *dsa, int bits,
 ^~~
In file included from external/com_github_grpc_grpc/src/core/tsi/ssl_transport_security.h:28,
                 from external/com_github_grpc_grpc/src/core/lib/security/security_connector/security_connector.h:33,
                 from external/com_github_grpc_grpc/src/core/lib/security/credentials/credentials.h:35,
                 from external/com_github_grpc_grpc/src/core/lib/security/context/security_context.h:28,
                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc:35:
/usr/include/openssl/x509.h:728:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'
 DEPRECATEDIN_1_1_0(ASN1_TIME *X509_CRL_get_nextUpdate(X509_CRL *crl))
 ^~~~~~~~~~~~~~~~~~
Target //tensorflow/lite:libtensorflowlite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 916.628s, Critical Path: 16.49s
INFO: 2033 processes: 1075 internal, 958 local.
FAILED: Build did NOT complete successfully
```
Thank you in advance for your support."
54499,How to remove TensorFlow CUDA messages? TensorFlow was installed for CPU only,"I installed TensorFlow 2.8.0 in my Windows 10 PC with CPU only. No Nvidia/CUDA hardware/software installed at all. I run a simple Python image processing project with CNN and I got bunch of no sense messages:

2022-02-23 07:31:55.511878: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found 
2022-02-23 07:31:55.512119: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 
2022-02-23 07:32:16.820716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found 
2022-02-23 07:32:16.820934: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303) 2022-02-23 07:32:16.823066: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-7OCT2M4 
2022-02-23 07:32:16.823285: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-7OCT2M4 
2022-02-23 07:32:16.823612: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

Why I'm getting these CUDA messages? I never installed anything for CUDA. I did try the following like everyone recommended in Google:

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

And still getting these messages. WHY? I really don't want to see any CUDA message because I did not install nothing for it. This is very bad for the TensorFlow deployment team. Why no one test this before in Windows?

I'm waiting for any suggestion to fix this TensorFlow installation issues.
"
54498,Build failing from tensorflow r2.7,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **centos 7**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): **source** 
- TensorFlow version: **r2.7**
- Python version: **3.6.13**
- Installed using virtualenv? pip? conda?: **conda**
- Bazel version (if compiling from source): **3.7.2**
- GCC/Compiler version (if compiling from source): **8.3.0**
- CUDA/cuDNN version: **CUDA 10.1.243, cuDNN 7.6.4**
- GPU model and memory: **RTX 2080 * 4, 67 GB (not sure)**



**Describe the problem**
Build keep failing with branch r2.7.
The build emits ` error: incomplete type is not allowed` message, yet I have not faced this issue before, nor from issues to best of my knowledge.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I have done configuration with `./configure`, selecting the options described above.
After that, I used the following instrcution to build TensorFlow.
``` bash
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```
The build ended with the following error, and I found that there was no similar issue with this error.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
``` text
WARNING: Output base '/home/jueonpark/.cache/bazel/_bazel_jueonpark/39636ca59c1e25526db6a6ad2e035e5e' is on NFS. This may lead to surprising failures and undetermined behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=209
INFO: Reading rc options for 'build' from /home/jueonpark/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/jueonpark/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /home/jueonpark/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/jueonpark/anaconda3/envs/timeloop/bin/python3 --action_env PYTHON_LIB_PATH=/home/jueonpark/anaconda3/envs/timeloop/lib/python3.6/site-packages --python_path=/home/jueonpark/anaconda3/envs/timeloop/bin/python3 --action_env TF_CUDA_PATHS=/home/jueonpark/cuda --action_env CUDA_TOOLKIT_PATH=/home/jueonpark/cuda-10.1 --action_env CUDNN_INSTALL_PATH=/home/jueonpark/cuda --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --action_env LD_LIBRARY_PATH=/home/jueonpark/cuda/extras/CUPTI/lib64:/home/jueonpark/cuda/lib64:/opt/ohpc/pub/compiler/gcc/8.3.0/lib64:/home/jueonpark/cuda/extras/CUPTI/lib64:/home/jueonpark/cuda/lib64 --action_env GCC_HOST_COMPILER_PATH=/opt/ohpc/pub/compiler/gcc/8.3.0/bin/gcc --config=cuda
INFO: Reading rc options for 'build' from /home/jueonpark/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/jueonpark/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/jueonpark/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/jueonpark/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /home/jueonpark/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /home/jueonpark/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/jueonpark/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/jueonpark/tensorflow/tensorflow/core/kernels/image/BUILD:308:18: C++ compilation of rule '//tensorflow/core/kernels/image:resize_nearest_neighbor_op_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF ... (remaining 216 argument(s) skipped)
external/com_google_absl/absl/types/internal/span.h(38): error: incomplete type is not allowed
          detected during instantiation of ""absl::lts_20210324::span_internal::GetDataImpl"" based on template argument <tensorflow::TensorShapeProto>
./tensorflow/core/framework/tensor_shape.h(376): here

1 error detected in the compilation of ""/tmp/tmpxft_0000b538_00000000-7_resize_nearest_neighbor_op_gpu.cu.compute_70.cpp1.ii"".
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/jueonpark/tensorflow/tensorflow/lite/toco/python/BUILD:92:10 C++ compilation of rule '//tensorflow/core/kernels/image:resize_nearest_neighbor_op_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF ... (remaining 216 argument(s) skipped)
INFO: Elapsed time: 37.049s, Critical Path: 22.44s
INFO: 123 processes: 64 internal, 59 local.
FAILED: Build did NOT complete successfully
```"
54497,How to Edit the c++ files in TensorFlow？,"I need to change the c++ files in tensorflow, but if I got it by using pip, there has no c++ file in the directory. So how can I edit these files? Or can I just pull tensorflow from github? I tried the command `pip install git+https://github.com/tensorflow/tensorflow.git` , it failed."
54496,[TFLite] memory cost is much higher when enable xnnpack delegate,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OPPO Find X3Pro

Dear
I build TFLite benchmark_model from source code, based on commitID: 965c39fdf304a80eacd6cdca43241956084301c3 (CommitDate: Mon Feb 21 15:37:46 2022 -0800)

build command: 
```
bazel build -c opt --config=android_arm64 --define tflite_with_xnnpack=true --define tflite_with_xnnpack_qs8=true --define tflite_with_xnnpack_qu8=true tensorflow/lite/tools/benchmark:benchmark_model
```

then I run benchmark_model on my Android Phone, I test with model  https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip

when disable xnnpack, the memory cost is 8.8MB, see below:
build command: 
```
OP:/data/local/tmp # ./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite --num_threads=4 --use_xnnpack=false
STARTING!
Log parameter values verbosely: [0]
Num threads: [4]
Graph: [mobilenet_v1_1.0_224_quant.tflite]
#threads used for CPU inference: [4]
Use xnnpack: [0]
Loaded model mobilenet_v1_1.0_224_quant.tflite
INFO: Initialized TensorFlow Lite runtime.
The input model file size (MB): 4.27635
Initialized session in 1.004ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=43 first=22701 curr=10983 min=10906 max=22701 avg=11638.5 std=1768

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=91 first=11004 curr=10970 min=10864 max=11745 avg=11008.7 std=105

Inference timings in us: Init: 1004, First inference: 22701, Warmup (avg): 11638.5, Inference (avg): 11008.7
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=3.32422 overall=8.83984
```

When enable xnnpack, the memory cost is 13.8MB, see below:
```
OP:/data/local/tmp # ./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite --num_threads=4 --use_xnnpack=true
STARTING!
Log parameter values verbosely: [0]
Num threads: [4]
Graph: [mobilenet_v1_1.0_224_quant.tflite]
#threads used for CPU inference: [4]
Use xnnpack: [1]
Loaded model mobilenet_v1_1.0_224_quant.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
XNNPACK delegate created.
INFO: Replacing 28 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 4 partitions.
Explicitly applied XNNPACK delegate, and the model graph will be partially executed by the delegate w/ 2 delegate kernels.
The input model file size (MB): 4.27635
Initialized session in 14.79ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=66 first=13307 curr=7452 min=7431 max=13307 avg=7638.41 std=719

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=133 first=7588 curr=7541 min=7445 max=8710 avg=7556.99 std=159

Inference timings in us: Init: 14790, First inference: 13307, Warmup (avg): 7638.41, Inference (avg): 7556.99
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=11.2891 overall=13.8906
```

Could you please help check this ?"
54495,capture_tpu_profile on Cloud TPU VM,"Hello

It seems that capture_tpu_profile only works for legacy TPU devices, but not for Cloud TPU VM.

I was wondering if and when we can expect to have support for Cloud TPU VM. 
At the moment, its is impossible to monitor Jax/Flax TPU workloads, as tensorboard instrumentation does not exhibit the right metrics (eg % utilization of TPU Matrix Units).

Other related thread for legacy TPU devices: https://stackoverflow.com/questions/52427141/check-tpu-workload-utilization

Thanks!
"
54492,keras layer GRU tflite conversion wrong model,"
### System information


-   **TensorFlow version (use command below)**:v2.8.0
-   **Python version**: python 3.8


### Describe the problem
To conversion lite model with in GRU model you can found here for example
https://github.com/ARM-software/ML-zoo/blob/master/models/noise_suppression/RNNoise/tflite_int8/rnnoise_INT8.tflite

I found that GRU kernel process r\*wr+i\*wi+bias convert to  tflite is wrong, it's convert to 
two fc ops and ADD  (r\*wr+bias) + (i\*wi+bias) and each biases are the same, so the result bias will be double added.
LSTM has no such problem, it only convert without bias, and add single bias later.

### Source code / logs
N/A
"
54489,Can't find tensorflow version on python 3.10.2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
macOS Monerey Version 12.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version:
2.8
- Python version:
3.10.2
- Installed using virtualenv? pip? conda?:
pip version 22.0.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I changed from python 3.8 to 3.10 as I saw from the official website that tensorflow 2.8 was supported by python 3.10 however it couldn't find a version that satisfied requirements.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
![imageedit_3_9779348486](https://user-images.githubusercontent.com/52308419/155244353-63d4c411-f866-4ef4-addc-baaab14b4420.jpg)
I tried:
""python3 -m pip install tensorflow"" and when that didn't work ""python3 -m pip install tensorflow==2.8"" but that didn't work either.



"
54488,Read only tensor is mapped as input to IF operation,"### 1. System information

- OS Platform and Distribution: Google Colab Ubuntu 18.04
- TensorFlow installed from binary
- TensorFlow version: 2.8.0

### 2. Code

Colab Notebook: [link](https://colab.research.google.com/drive/16guYzn6go7Sps_5XyVQSELMn5Euwlwpb?usp=sharing)

### 3. Failure after conversion
The read-only tensors containing the weights are mapped as inputs for the `IF` operation in Subgraph `#0`. I agree on the fact that these are indeed inputs for the subgraph but this behavior causes some difficulties in combination with [TFLM](https://github.com/tensorflow/tflite-micro). More specifically, every input tensor is copied over to the ""subgraph's execution environment"". Especially for these (large) read-only tensors, it's a huge unnecessary overhead because these could be linked to the OPs that use the tensors directly. 

```md
Subgraph#0 main(T#0, T#1) -> [T#4]
  Op#0 IF(T#1, T#3, T#0, T#2, Then: Subgraph#2, Else: Subgraph#1) -> [T#4]

...
T#2(cond_1/input_2) shape:[10, 10], type:FLOAT32 RO 400 bytes
T#3(cond_1/input_0) shape:[10, 10], type:FLOAT32 RO 400 bytes
```
I already posted this issue in the TensorFlow lite micro repository over here https://github.com/tensorflow/tflite-micro/issues/903.


Is there any option besides modifying the .tflite file to get these tensors out of the input tensor list of the `IF` operation?


"
54487,ConnectionError when downloading deep_weeds dataset,"I'm unable to use the [deep_weeds dataset](https://www.tensorflow.org/datasets/catalog/deep_weeds) from TensorFlow Datasets. Seems like this has happened in the past [#44053](https://github.com/tensorflow/tensorflow/issues/44053)

```
import tensorflow as tf
import tensorflow_datasets as tfds

data, info = tfds.load(name='deep_weeds')
```

Results in `ConnectionError`

[See gist here](https://colab.research.google.com/gist/nikitamaia/fdd8654acd6d6d889585f60895f158f5/untitled4.ipynb)"
54485,XLA `LuDecomposition` support for CPU/GPU,"**System information**
- TensorFlow version (you are using): 2.8
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

`xla::LuDecomposition` in tensorflow/compiler/xla/client/lib/lu_decomposition.h is currently only implemented for TPU. I'd like to be able to use it on CPU and/or GPU.

**Will this change the current api? How?**

Yes, but in a backwards compatible way.

**Who will benefit with this feature?**

Anyone wanting to build on XLA and do LU decomposition for e.g. calculating determinants. I want this as I'm building an API for XLA in Idris."
54483,Shall we put this into a templated function and use it for the few cases?,"Shall we put this into a templated function and use it for the few cases?

_Originally posted by @bixia1 in https://github.com/tensorflow/tensorflow/pull/54452#discussion_r811363136_"
54481,TFLite with Hexagon delegate produces wrong results for a particular model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (host), Android 11 (target device)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: device with Snapdragon 660
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): 4.2.1
- GCC/Compiler version (if compiling from source): 8.4.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
For a custom model I am trying to use, when I enable the Hexagon delegate, the output contains incorrect values. The Hexagon delegate seems to work fine with other models I have tried.

**Describe the expected behavior**
I expect the Hexagon delegate to produce the same results as CPU.

**Standalone code to reproduce the issue**
The difference between CPU and Hexagon delegate can be observed with your [Inference Diff tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff#inference-diff-tool):
```
INFO: TfLiteHexagonDelegate delegate: 270 nodes delegated out of 270 nodes with 1 partitions.
INFO: Replacing 270 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 1 partitions.
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:77 Test interpreter has been initialized.
native : lite/tools/evaluation/stages/tflite_inference_stage.cc:128 
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:91 Reference interpreter (1 thread on CPU) has been initialized.
Num evaluation runs: 50
Reference run latency: avg=863334(us), std_dev=25697(us)
Test run latency: avg=93164.5(us), std_dev=675(us)
OutputDiff[0]: avg_error=86.0362, std_dev=0.0619768
OutputDiff[1]: avg_error=48.6195, std_dev=0.168915
OutputDiff[2]: avg_error=52.9178, std_dev=0.252582
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Model used: [test_model.zip](https://github.com/tensorflow/tensorflow/files/8118439/test_model.zip)
"
54480,cant open pages,the website downloads  pages instead of opening them 
54479,XLA tf.bincount support,"**System information**
- TensorFlow version (you are using): master
- Are you willing to contribute it (Yes/No): only if we have a clear path and a reviewer on how to contribute this
**Describe the feature and the current behavior/state.**
`tf.bincount` isn't supported by XLA
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
Speedup functions/loops that rely on `tf.bincount`
**Any Other info.**
How to reproduce it:
```python
import tensorflow as tf
@tf.function(jit_compile=True)
def compiled_bincount(values):
  return tf.math.bincount(values)

values = tf.constant([1,1,2,3,2,4,4,5])
print(compiled_bincount(values)) #[0 2 2 1 2 1]
```

```python
InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_compiled_bincount_296[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263] on XLA_CPU_JIT: Bincount (No registered 'Bincount' OpKernel for XLA_CPU_JIT devices compatible with node {{node bincount/Bincount}}){{node bincount/Bincount}}
The op is created at: 
```

Without this we had problems to compile intermediate Ms Coco recall function in keras-cv:
https://github.com/keras-team/keras-cv/issues/141#issuecomment-1043692891

Extra: 
Please also note that the CPU/GPU TF2XLA supported ops tables are probably outdated (2018):
https://github.com/tensorflow/tensorflow/issues/14798#issuecomment-1047796247

/cc @joker-eph @LukeWood "
54478,Web Page problem,"When trying to access some pages of tensorflow it downloads an xml instead of showing the webpage
![image](https://user-images.githubusercontent.com/19721178/155134500-416d0005-29e5-404f-bc8b-0bfb6358a2ab.png)
"
54477,SavedModel way slower than h5 in loading,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.8.0
- Python version: 3.7.7
- Bazel version (if compiling from source): 5.0.0
- GCC/Compiler version (if compiling from source): 7.3
- CUDA/cuDNN version: 11.2/8.1
- GPU model and memory: T4


I have found some issues about this, but I didn't find any solution/explanation about it. Loading a SavedModel is way slower than loading the same model in h5df. Is there any way to speed this up? I find hard to justiy the use SavedModel (even if it is the preferred formar) if it takes 4x-5x longer to load in my code.

![image](https://user-images.githubusercontent.com/61322372/155090488-110b9206-7b85-4b95-8df1-586bf17de3d5.png)

"
54476,What operations are supported in tf.lite.OpsSet.TFLITE_BUILTINS_INT8,"To convert a Keras model to tflite format using post-training quantization, we use `tf.lite.OpsSet.TFLITE_BUILTINS_INT8`. Is there any list of operations that are supported when this flag is set? "
54475,Error message of `tf.nn.gelu` with `uint16` input is misleading ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
features = tf.zeros([3, 4], dtype=tf.uint16)
tf.nn.gelu(features)
```
Throws `TypeError`
```
TypeError: Cannot convert 0.5 to EagerTensor of dtype uint16
```
**Describe the current behavior**
The current message is misleading, as it seems to be some computation error. If `tf.nn.gelu` does not accept `uint16` inputs, the message should be a standard message like
```
Value for attr 'T' of uint16 is not in the list of allowed values:
```
Similar to `tf.nn.crelu`:
```
import tensorflow as tf
features = tf.zeros([3, 4], dtype=tf.uint16)
tf.nn.crelu(features)
# InvalidArgumentError: Value for attr 'T' of uint16 is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, complex64, complex128; NodeDef: {{node Neg}}; Op<name=Neg; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]> [Op:Neg]
```


**Describe the expected behavior**
`tf.nn.gelu` should have better error message in this case."
54474,Several `tf.math` APIs lack input checking for `bool` arguments,"Similar to [issue #54410](https://github.com/tensorflow/tensorflow/issues/54410), the following APIs also lack input validity checking for `bool` arguments.
```
tf.math.cumulative_logsumexp
tf.math.reduce_mean
tf.math.cumprod
```
Example code:
```
import tensorflow as tf
tf.math.cumulative_logsumexp([0.5, 1.0, 2.0, 4.0], axis=-1, exclusive=-1, reverse=0)
tf.math.cumprod([0.5, 1.0, 2.0, 4.0], axis=-1, exclusive=-1, reverse=0)
tf.math.reduce_mean(tf.random.uniform([1,2,2]), axis=[1,], keepdims=-1)
# Pass without exceptions
```"
54473,Building tensorflowlite.so for aarch_64 failed ,"

**System information**
- OS Platform and Distribution (Windows 10):
- Mobile device ( for RPI4) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source (cloned the repo)
- TensorFlow version: latest
- Python version: 3.7.9
- Installed using virtualenv? pip? conda?: none
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): g++
- CUDA/cuDNN version: 
- GPU model and memory: gtx 1060 6gb



**Describe the problem**
I followed this tutorial for cross compilation of aarch64 
https://www.tensorflow.org/lite/guide/build_arm
1- cloned tensorflow repo 
2- ran `bazel build --config=elinux_aarch64 -c opt //tensorflow/lite:libtensorflowlite.so`


**Any other info / logs**
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/Mohamed_Gamal/AppData/Local/Microsoft/WindowsApps/python.exe
INFO: Reading rc options for 'build' from c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --host_java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:elinux_aarch64 in file c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc: --config=elinux --cpu=aarch64 --distinct_host_configuration=true
INFO: Found applicable config definition build:elinux in file c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
INFO: Found applicable config definition build:windows in file c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc: --define framework_shared_object=false
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/9df1c45bd751eeff53f6811501e015c7eba4b536.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Repository local_config_embedded_arm instantiated at:
  C:/users/mohamed_gamal/downloads/tensorflow_src/WORKSPACE:15:14: in <toplevel>
  C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/workspace2.bzl:868:19: in workspace
  C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/workspace2.bzl:118:34: in _tf_toolchains
Repository rule arm_linux_toolchain_configure defined at:
  C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/tools/toolchains/embedded/arm-linux/arm_linux_toolchain_configure.bzl:34:48: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_embedded_arm':
   Traceback (most recent call last):
        File ""C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/tools/toolchains/embedded/arm-linux/arm_linux_toolchain_configure.bzl"", line 23, column 9, in _arm_linux_toolchain_configure_impl
                _tpl(repository_ctx, ""cc_config.bzl"", {
        File ""C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/tools/toolchains/embedded/arm-linux/arm_linux_toolchain_configure.bzl"", line 6, column 28, in _tpl
                repository_ctx.template(
Error in template: Unable to load package for //tensorflow/tools/toolchains/embedded/arm-linux:cc_config.bzl.tpl: BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.
 - C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/tools/toolchains/embedded/arm-linux
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  C:/users/mohamed_gamal/downloads/tensorflow_src/WORKSPACE:23:14: in <toplevel>
  C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/workspace0.bzl:110:34: in workspace
  C:/users/mohamed_gamal/_bazel_mohamed_gamal/wmmhak3q/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  C:/users/mohamed_gamal/_bazel_mohamed_gamal/wmmhak3q/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
ERROR: C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/lite/BUILD:1158:24: //tensorflow/lite:libtensorflowlite.so depends on @local_config_embedded_arm//:toolchain in repository @local_config_embedded_arm which failed to fetch. no such package '@local_config_embedded_arm//': Unable to load package for //tensorflow/tools/toolchains/embedded/arm-linux:cc_config.bzl.tpl: BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.
 - C:/users/mohamed_gamal/downloads/tensorflow_src/tensorflow/tools/toolchains/embedded/arm-linux
ERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: Analysis failed
INFO: Elapsed time: 0.724s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
"
54472,No module named 'keras.optimizer',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
54471,TF1.15 configuartion with CUDA 11.4 and cuDNN 8,"<em>Hi everybody, I am wondering if it's possible to use TF1.15 with CUDA 11.4 and cuDNN 8. The problem that I have is that the migration of the code to TF2 is painful and I have no root privileges on this machine to modify the CUDA drivers.</em>

**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from pip: version 1.15
- Python version: python 3.7.12 in virtaulenv
- CUDA/cuDNN version: CUDA 11.4 and cuDNN 8
- GPU model and memory: NVIDIA A100-SMX-80GB


When I run my script TF looks for libraries of CUDA 10 and cuDNN 7, such as libcudart.10.so, libcublas.10.so, etc...

Thank you!"
54470,`parallel_iterations` in `tf.map_fn` has no effect,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0
- Python version: Python 3.9.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 11.2
- GPU model and memory: RTX A6000, CPU: Intel(R) Xeon(R) W-2255 CPU @ 3.70GHz

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`parallel_iterations` in `tf.map_fn` has no effect
**Describe the expected behavior**
setting higher values in `parallel_iterations` should improve running speed
**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): on
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf
import time
import numpy as np

def some_fn(x):
    return tf.math.exp(x)

@tf.function
def map_fn_test(samples):
    some_fn_vals = tf.map_fn(some_fn, samples, parallel_iterations=1)
    return some_fn_vals

samples = tf.constant(np.linspace(0,10,100000))
start_time = time.time()
values = map_fn_test(samples)
print(""--- %s seconds ---"" % (time.time() - start_time))
```

Changing `parallel_iterations` to any other number gives approximately the same amount of time. It looks like there ain't any parallelism happening.

This bug was also reported [here](https://github.com/tensorflow/tensorflow/issues/34716#issuecomment-560936125) and also [here](https://github.com/tensorflow/tensorflow/issues/24774) but remain stalled.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54469,"Tensorflow import error, undefined symbol","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Arch
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Arch community repository
- TensorFlow version: 2.8.0
- Python version: 3.10
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 11.1.0
- CUDA/cuDNN version: 11.5
- GPU model and memory: Nvidia GTX GeForce 1050ti 4GB



**Describe the problem**
import tensorflow leads to the following import error:

```
>>> import tensorflw
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflw'
>>> import tensorflow
Traceback (most recent call last):
  File ""/home/beany/.local/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 60, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/beany/.local/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/beany/.local/lib/python3.10/site-packages/tensorflow/__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/beany/.local/lib/python3.10/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/beany/.local/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 75, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/home/beany/.local/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 60, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/beany/.local/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb


Failed to load the native TensorFlow runtime.

```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

>>> import tensorflow


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
54467,[TFlite] Different shared library size between bazel and CMake on Android64.,"Hello, 
I want to compile android_arm64 shared library and I compile **library1** using
 **bazel build -c opt --config=android_arm64 tensorflow/lite/libtensorflowlite.so  --verbose_failures**
in docker which is built by tflite-android.Dockerfile. 

Meanwhile I compile **library2** using CMake command **cmake -DCMAKE_TOOLCHAIN_FILE=/opt/android-ndk-r20b/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DBUILD_SHARED_LIBS=ON -DCMAKE_SYSTEM_NAME=Android ../tensorflow/lite && make -j4**, CMake version is 3.16.

What bothers me is why library1's size is 3.9M and library2's size is 60M."
54465,"An op outside of the function building code is being passed a ""Graph"" tensor","I am using tf 2.3.0, and did not explicitly  set the eager mode。

I defined a memory network layer, and call it from another layer。 It succeeded during the training step. However, I got the following error during the evaluation step。

Traceback (most recent call last):
  File ""/Users/fabriszhou/PycharmProjects/tf-model/test/msmt/test_memory.py"", line 94, in <module>
    mymodel = model.get_train_model()
  File ""/Users/fabriszhou/PycharmProjects/tf-model/models/msmt.py"", line 374, in get_train_model
    long_cluster = self.memory.predict_process([query_emb, long_emb, isActive_vals])  # 拿到长期的兴趣表征
  File ""/Users/fabriszhou/PycharmProjects/tf-model/pkgs/tf/extend_layers.py"", line 2499, in predict_process
    read_attention = compute_cosine_similarity(read_query, self.key_memory, temperature=self.temperature)
  File ""/Users/fabriszhou/PycharmProjects/tf-model/pkgs/tf/extend_layers.py"", line 2322, in compute_cosine_similarity
    normed_memory = tf.nn.l2_normalize(memory, axis=1)  # [n_memory, dim]
  File ""/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py"", line 666, in l2_normalize_v2
    square_sum = math_ops.reduce_sum(math_ops.square(x), axis, keepdims=True)
  File ""/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 10309, in square
    x, name=name, ctx=_ctx)
  File ""/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 10347, in square_eager_fallback
    ctx=ctx, name=name)
  File ""/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 75, in quick_execute
    raise e
  File ""/opt/anaconda3/envs/tf23/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: add:0


Here is my source code。

def compute_cosine_similarity(query, memory, temperature=0.1):
    """"""
    Compute cosine similarity between query and memory.

    Args:
        query: (batch_size, embedding_dim)
        memory: (n_memory, embedding_dim)
        temperature: hyper-parameter to control the final probabilities of softmax.

    Returns:
        Attention in the shape of (batch_size, n_memory)
    """"""

    normed_query = tf.nn.l2_normalize(query, axis=1)  # [batch, dim]
    query = tf.expand_dims(normed_query, axis=1)  # [batch, 1, dim]

    normed_memory = tf.nn.l2_normalize(memory, axis=1)  # [n_memory, dim]
    memory = tf.expand_dims(normed_memory, axis=0)  # [1, n_memory, dim]

    similarity = tf.reduce_sum(tf.multiply(query, memory), axis=2)  # [batch, n_memory]
    similarity = tf.nn.softmax(similarity / temperature, axis=1)  # [batch, n_memory]

    return similarity

class MemoryLayer(tf.keras.layers.Layer):
    def __init__(self, controller_layers, controller_hidden_act='relu', controller_output_act=None, n_clusters=100, key_dims=8, long_dims=8, short_dims=8, temperature=0.1, alpha=0.1, is_short=False, name=""memory_layer"", **kwargs):
        super(MemoryLayer, self).__init__(name=name, trainable=True, **kwargs)
        self.controller_layers = controller_layers
        self.controller_hidden_act = controller_hidden_act
        self.controller_output_act = controller_output_act
        self.n_clusters = n_clusters
        self.key_dims = key_dims
        self.long_dims = long_dims
        self.short_dims = short_dims
        self.temperature = temperature
        self.alpha = alpha
        self.is_short = is_short

        self.key_memory = tf.compat.v1.get_variable(
            f""{self.name}_key_memory"",
            shape=[self.n_clusters, self.key_dims],
            initializer=tf.compat.v1.glorot_normal_initializer())
        # self.key_memory = self.add_weight(f""{self.name}_key_memory"",
        #                       shape=(self.n_clusters, self.key_dims),
        #                       initializer=tf.keras.initializers.GlorotNormal(),
        #                       trainable=False)

        #
        # self.long_memory = self.add_weight(f""{self.name}_long_memory"",
        #                       shape=(self.n_clusters, self.long_dims),
        #                       initializer=tf.keras.initializers.GlorotNormal(),
        #                       trainable=False)

        self.long_memory = tf.compat.v1.get_variable(
            f""{self.name}_long_memory"",
            shape=[self.n_clusters, self.key_dims],
            initializer=tf.compat.v1.glorot_normal_initializer())

        if is_short:
            self.short_memory =  self.add_weight(f""{self.name}_short_memory"",
                              shape=(self.n_clusters, self.long_dims),
                              initializer=tf.keras.initializers.GlorotNormal(),
                              trainable=False)

            self.short_erase_layer = DNNLayer([self.short_dims], 'sigmoid',
                                             name=f""{self.name}_short_erase_layer"")  # short earse layer
            self.short_add_layer = DNNLayer([self.short_dims], 'tanh', name=f""{self.name}_short_add_layer"")  #short add layer

        self.write_controller = DNNLayer(controller_layers, controller_hidden_act, controller_output_act) #write controller

        self.read_controller = DNNLayer(controller_layers, controller_hidden_act, controller_output_act) #read controller

        self.key_erase_layer = DNNLayer([self.key_dims], 'sigmoid', name=f""{self.name}_key_erase_layer"") #key erase layer
        self.key_add_layer = DNNLayer([self.key_dims], 'tanh', name=f""{self.name}_key_add_layer"") #key add layer

        self.long_erase_layer = DNNLayer([self.long_dims], 'sigmoid', name=f""{self.name}_long_erase_layer"")  #long erase layer
        self.long_add_layer = DNNLayer([self.long_dims], 'tanh', name=f""{self.name}_long_add_layer"")  # long add layer

        # self.read_sim_layer = SimLayer()
        # self.write_sim_layer = SimLayer()


    @tf.function
    def call(self, inputs, training=None, **kwargs):
        attr_emb = inputs[0]  # [batch_size, attr_emb]
        long_emb = inputs[1] #[batch, long_dims] embedding
        isActive = inputs[2]  # [batch_size, 1]

        active_status = tf.cast(tf.transpose(isActive, perm=[1,0]), dtype='float32')

        # denominator = tf.math.count_nonzero(isActive) 

        #＞0才更新
        if training:
            #write memory
            #[batch_size, key_dims]
            write_query = self.write_controller(attr_emb) 

            write_query_add = tf.reduce_mean(write_query, axis=0, keepdims=True)

            #[batch, n_clusters]
            # write_attention = compute_cosine_similarity(write_query, self.key_memory, temperature=self.temperature)

            # write_attention = self.write_sim_layer([write_query, self.key_memory], temperature=self.temperature)
            #
            # #[1, n_clusters]
            # write_memory_attention = tf.matmul(active_status, write_attention) / tf.cast(1024, dtype=tf.float32)
            # write_memory_attention = tf.matmul(active_status, write_attention) / tf.cast(1024, dtype=tf.float32)

            #[n_clusters, 1]
            # write_memory_attention = tf.transpose(write_memory_attention, perm=[1, 0])

            #write key memory
            # key_erase_vector = self.key_erase_layer(write_query) #[batch, key_dims]
            # key_erase_vector = tf.matmul(active_status, key_erase_vector) / tf.cast(denominator, dtype=tf.float32) #[1,key_dims]
            #
            # key_add_vector = tf.reduce_mean(self.key_add_layer(write_query), axis=0, keepdims=True)#[batch, key_dims]
            # key_add_vector = tf.matmul(active_status, key_add_vector) / tf.cast(denominator, dtype=tf.float32)  # [1,key_dims]

            self.key_memory = self.key_memory \
                               + write_query_add

            # * (1. - self.alpha * tf.matmul(write_memory_attention, key_erase_vector)) \

            #write long_memory
            # long_erase_vector = self.long_erase_layer(long_emb)  # [batch, long_dims]
            # long_erase_vector = tf.matmul(active_status, long_erase_vector) / tf.cast(denominator, dtype=tf.float32)  # [1,long_dims]
            #
            # long_add_vector = self.long_add_layer(long_emb)  # [batch, long_dims]
            # long_add_vector = tf.matmul(active_status, long_add_vector) / tf.cast(denominator, dtype=tf.float32)  # [1,key_dims]

            self.long_memory = self.long_memory
                              # * (1. - self.alpha * tf.matmul(write_memory_attention, long_erase_vector)) \
                              # + self.alpha * tf.matmul(write_memory_attention, long_add_vector)

        #read memory
        #[batch, key_dims]
        read_query = self.read_controller(tf.stop_gradient(attr_emb))

         #[batch, n_clusters]
        read_attention = compute_cosine_similarity(read_query, self.key_memory, temperature=self.temperature)

        # read_attention = self.read_sim_layer([read_query, self.key_memory], temperature=self.temperature)

        long_cluster_emb = tf.matmul(read_attention, self.long_memory) #[batch, long_dims] 长期兴趣表征

        return long_cluster_emb

    def predict_process(self, inputs):
        read_query = self.read_controller(inputs[0])
        # read_attention = self.read_sim_layer([read_query, self.key_memory], temperature=self.temperature)
        read_attention = compute_cosine_similarity(read_query, self.key_memory, temperature=self.temperature)

        long_cluster_emb = tf.matmul(read_attention, self.long_memory)  # [batch, long_dims] 长期兴趣表征

        return long_cluster_emb

    def get_config(self):
        config = super(MemoryLayer, self).get_config()
        config.update({
            'controller_layers': self.controller_layers,
            'controller_hidden_act': self.controller_hidden_act,
            'controller_output_act': self.controller_output_act,
            'n_clusters': self.n_clusters,
            'key_dims': self.key_dims,
            'long_dims': self.long_dims,
            'short_dims': self.short_dims,
            'temperature': self.temperature,
            'alpha': self.alpha,
            'is_short': self.is_short,
            'name': self.name
        })
        return config

    @classmethod
    def from_config(cls, config):
        return cls(**config)"
54464,RNN with nested inputs,"I would like to implement a recurrent GAT model which takes two inputs of shape: [(batch_size, time, N, F), [batch_size,  time, N, N)].  The hidden size should have shape (batch_size, N, hidden_feature).  I've structured the class as follows:
```python
class NestedCell(keras.layers.Layer):
    def __init__(self, nodes, features, channels, attention_heads, **kwargs):
        self.tot_nodes = nodes
        self.nodes_features = features
        self.hidden_size = channels * attention_heads
        self.state_size = [tf.TensorShape([self.tot_nodes, self.hidden_size])]
        self.output_size = [tf.TensorShape([self.tot_nodes, self.tot_nodes]),
                            tf.TensorShape([self.tot_nodes, self.hidden_size])]
        super(NestedCell, self).__init__(**kwargs)
```
The problem is that when I feed a list of two inputs:
```python
N = 10
f = 5
cell = NestedCell(N, f, 10,10)
i1 = tf.keras.Input((None, N, f))
i2 = tf.keras.Input((None, N, N))
rnn = tf.keras.layers.RNN(cell)
o = rnn([i1, i2])
```
I get the error that the initial state is not compatible with the cell.state_size:
```
ValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=ListWrapper([InputSpec(shape=(None, None, 10, 10), ndim=4)]); however `cell.state_size` is [TensorShape([10, 100])]
```
How should I structure the class to accept such input shape?"
54463,Internal error `Blas xGEMV launch failed` on Tensorflow v2.8.0 for the same block of codes that runs perfectly well on Tensorflow v2.4.1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Unknown
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0
- Python version: 3.8
- Bazel version (if compiling from source): N.A.
- GCC/Compiler version (if compiling from source): N.A.
- CUDA/cuDNN version: CUDA 11.2.1
- GPU model and memory: Tesla T4 / 16 GB

**Describe the current behavior**
Running a block of code with Tensorflow v2.8.0 / Cuda 11.2 / CuDNN 8.1 returns an internal error `Blas xGEMV launch failed` when it runs perfectly well with Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0.

**Describe the expected behavior**
Return the same output as Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): N.A.

**Standalone code to reproduce the issue**

The following block of code works perfectly well with Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0, but not with Tensorflow v2.8.0 / Cuda 11.2 / CuDNN 8.1.
```
import tensorflow as tf
empty_image = tf.zeros(shape=[1280, 1280, 3], dtype=tf.float32)
gray_image = tf.image.rgb_to_grayscale(empty_image)
```

An important point to note is that when I reduce the `shape` of `empty_image` to `[512, 512, 3]`, there is no issue. However, I believe this is not a device memory issue as I can reproduce this with GeForce RTX 2080 Ti 11 GB as well as Tesla T4 16 GB.

**Other info / logs** 

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/miniconda3/envs/docrec/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/ubuntu/miniconda3/envs/docrec/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 7186, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InternalError: Blas xGEMV launch failed : a.shape=[1,1638400,3], b.shape=[1,3,1], m=1638400, n=1, k=3 [Op:MatMul]
```

"
54462,C++ prebuilt libs,Are there any plans to release prebuilt C++ libs like the ones for C here https://www.tensorflow.org/install/lang_c? 
54461,`in` operation in TF takes incredibly long time,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.9.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 11.2
- GPU model and memory: RTX A6000

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`in` operation in tensors took an unreasonably amount of time

**Describe the expected behavior**
should be very fast to execute

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf
import numpy as np
import time

a = tf.range(256)
start = time.time()
res = tf.constant([i in a for i in range(256)])
print(""Using tf: took"", time.time()-start, ""seconds."")

a_ = a.numpy()
start = time.time()
res = tf.constant([i in a_ for i in range(256)])
print(""Using numpy: took"", time.time()-start, ""seconds."")
```

In my own machine, this gives
```
Using tf: took 3.605224847793579 seconds.
Using numpy: took 0.0006427764892578125 seconds
```

Well, it's pretty obvious something is wrong when using `in` operations with tensors.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54460,gradient_function/gradient_tapes with device annotations,"**System information**
- TensorFlow version (you are using): 2.8
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Feature request: adding `colocate_gradients_with_ops` option to `tf.gradients` in 1.x to `tf.GradientTape` in 2.x.

This feature request/issue was 1st mentioned in https://github.com/tensorflow/tensorflow/issues/33688 almost two years ago by @olesalscheider . Allow me to quickly review the op in which the current behavior (still hold in tf 2.8) is described.

> Currently GradientTape.gradient() is executed on the device of the scope it is called in. Have a look at the following code:
> 
```
with tf.GradientTape() as tape:
    with tf.device('/gpu:1'):
        x = f1(input)
    with tf.device('/gpu:2'):
        x = f2(x)
    with tf.device('/gpu:0'):
        g = tape.gradient(x, f_vars)
```
> Here all gradient calculations will be carried out by GPU:0 and all variables needed for the gradient calculation will also be allocated on GPU:0. This is a problem if these temporary variables are too large to fit into the VRAM of GPU:0.
> 
> Please provide a way to execute the backward functions on the device of the corresponding forward function and allocate temporary variables for gradient calculation there. This allows to split a large model and distribute it among as many GPUs as necessary.
> 

Also, @olesalscheider provided a pr https://github.com/tensorflow/tensorflow/commit/a64ff0f2cda9d4e35ea450d4e945009a90ddee9a to achieved such feature and it get merged at the beginning, but shortly get rollbacked due to certain performance issues. 

I opened another pr https://github.com/tensorflow/tensorflow/pull/54510/commits/c292044a64700ecc3fa419b8247dfb1b61cf3ce7 to fit the current master (tf2.8) and build and test it. It looks to me the gradient can now be split correctly according to the device annotation.

**Will this change the current api? How?**
It will allow tf.GradientTape to do gradient in the device of the corresponding forward function. Hence, it would be possible to split the training of large models into as many possible GPUs as necessary

**Who will benefit with this feature?**
Anyone who wants to train large models that do not fit into the VRAM of a single GPU.

**Any Other info.**

It looks to me that the original post has gone silence... So I raise the issue again here to draw more attentions. It is a very import feature for peoples working in large image segmentation tasks. Some times the input tensor is so large and the model can not even be fitted into a single A100 card. 

Moreover, I noticed that in the discussion flow in the original post that there are some other work around like split into different tapes. I tried but with no luck. all gradient calculation still be allocated on gpu:0, and moreover, splitting tape would be very different for model like u-net which has a lot of skip-connections.

Last but not the least, I know for a fact that there is the Mash-Tensorflow which propose to do such job, but a native tensorflow support would be also very useful for people work on large models."
54459,StringLookup fails as first layer in a Sequential model in TF 2.8.0,"**System information**

<details>

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04 (Google Colab)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary (preinstalled on Google Colab)**
- TensorFlow version (use command below): **v2.8.0-0-g3f878cff5b6 2.8.0**
- Python version: **3.7.12**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

</details>

**Describe the current behavior**

Using a `tf.keras.layers.StringLookup` layer as the first layer in a `Sequential` model raises an exception when calling the model: `UnimplementedError: Exception encountered when calling layer ""sequential"" (type Sequential). Cast string to int64 is not supported [Op:Cast]` (see full stacktrace below).

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

cat = [""Paris"", ""Singapore"", ""Auckland""]
str_lookup_layer = tf.keras.layers.StringLookup()
str_lookup_layer.adapt(cat)
lookup_and_embed = tf.keras.Sequential([
    str_lookup_layer,
    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),
                              output_dim=2)
])
lookup_and_embed(tf.constant([[""Paris""], [""Singapore""], [""Auckland""]]))  # ERROR!
```

This code is available in this [gist](https://colab.research.google.com/gist/ageron/4ba943dac246ade26699b7fe8fa38fae/tensorflow-issue-54459.ipynb).

**Describe the expected behavior**
This should work like it did in TensorFlow 2.7.1, I believe it's a regression. It should output something like this:

```
<tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[-0.02887753, -0.01268407],
       [ 0.04601531, -0.02668235],
       [ 0.03409723, -0.03205377]], dtype=float32)>
```

**Other info / logs**

Instead, it raises this exception. Full stacktrace:

<details>

```stacktrace
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
[<ipython-input-2-ee4b4b94a15e>](https://localhost:8080/#) in <module>()
      7                               output_dim=2)
      8 ])
----> 9 lookup_and_embed(tf.constant([[""Paris""], [""Singapore""], [""Auckland""]]))

1 frames
[/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py](https://localhost:8080/#) in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

[/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py](https://localhost:8080/#) in raise_from_not_ok_status(e, name)
   7184 def raise_from_not_ok_status(e, name):
   7185   e.message += ("" name: "" + name if name is not None else """")
-> 7186   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   7187 
   7188 

UnimplementedError: Exception encountered when calling layer ""sequential"" (type Sequential).

Cast string to int64 is not supported [Op:Cast]

Call arguments received:
  • inputs=tf.Tensor(shape=(3, 1), dtype=string)
  • training=None
  • mask=None
```

</details>


"
54458,InvalidArgumentError: Node '.../while_grad': Connecting to invalid output 4 of source node while which has 4 outputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): 2.8.0 (Colab)
- Python version: 3.7.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

I get the exception
```
Node 'gradients/while_grad/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).
```
when executing a `NoOp` without any control dependencies.

But also when executing my real code, I get the same exception. I'm not sure if these are two separate issues or the same.

**Describe the expected behavior**

A NoOp without control dependencies should not depend on anything, so I would never expect such exception.

For my real code, I also would not expect such exception. 

**Standalone code to reproduce the issue**

Code (to be executed with disabled eager mode):
```
v = tf.Variable(1.)

def cond(i, x):
  return tf.less(i, 10)

def body(i, x):
  return i + 1, x * 1.

j, y = tf.while_loop(cond, body, [0., v])
loss = tf.reduce_sum(y ** 2)

session.run(tf.compat.v1.global_variables_initializer())

opt = tf.compat.v1.train.GradientDescentOptimizer(0.1)
opt_op = opt.minimize(loss)

no_op = tf.no_op()
session.run(no_op)  # here it crashes already!

print(session.run((j, y, opt_op)))
```

Colab: https://colab.research.google.com/drive/1jjXpz8SAeU-8Cg6nuWu7tjxK_sAU5lVc?usp=sharing

**Other info / logs**

When executing locally, I additionally see this log output:
```
...
2022-02-19 23:07:57.636413: W tensorflow/c/c_api.cc:349] Operation '{name:'while' id:11 op device:{requested: '', assigned: ''} def:{{{node while}} = StatelessWhile[T=[DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=5, _read_only_resource_inputs=[], _stateful_parallelism=false, body=while_body_8_rewritten[], cond=while_cond_7_rewritten[], output_shapes=[[], [], [], [], []], parallel_iterations=10](while/loop_counter, while/maximum_iterations, Const, ReadVariableOp, gradients/while_grad/Placeholder_1_0/accumulator:0)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Traceback (most recent call last):
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1380, in _do_call
    return fn(*args)
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1362, in _run_fn
    self._extend_graph()
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1403, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'gradients/while_grad/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/az/Programmierung/playground/tf-while-v2.py"", line 37, in <module>
    main()
  File ""/Users/az/Programmierung/playground/tf-while-v2.py"", line 31, in main
    session.run(no_op)
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 970, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1193, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1373, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1399, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'gradients/while_grad/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).
```

My hypothesis is that the first session call `session.run(tf.compat.v1.global_variables_initializer())` will somehow freeze the while loop function graph (which is strange though as it would not depend on it), and then the further code which adds the gradients causes this warning `Operation ... StatelessWhile ... was changed by setting attribute after it was run by a session`.

I don't really understand why the first session call does that even though it is independent from the loop.

I don't really understand why it causes the error for the NoOp execution.

Is there any workaround?
"
54457,Install latest version but not find GPU,"laptop: acer nitro
GPU: RTX3070
windows: 10
python: 3.9.10
CUDA: cuda_11.6.0_511.23_windows
cnnCUDA: cudnn_8.3.2.44_windows

I try to install tensorflow but its not work
I try to install from https://www.tensorflow.org/install/source_windows but its a lot complicated

so I use pip and install
`pip install tensorflow`
`pip install tensorflow-gpu`
`pip install tf-nightly-gpu --uesr`

but it has this problem
```
  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\Users\snipe\AppData\Roaming\Python\Python39\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, but you have tf-estimator-nightly 2.9.0.dev2022021909 which is incompatible.
tensorflow-gpu 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, but you have tf-estimator-nightly 2.9.0.dev2022021909 which is incompatible.
Successfully installed absl-py-1.0.0 keras-nightly-2.9.0.dev2022021908 libclang-13.0.0 packaging-21.3 pyparsing-3.0.7 tb-nightly-2.9.0a20220218 tf-estimator-nightly-2.9.0.dev2022021909 tf-nightly-gpu-2.9.0.dev20220218
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -f-nightly-gpu (c:\users\snipe\appdata\roaming\python\python39\site-packages)
WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.
You should consider upgrading via the 'C:\Python39\python.exe -m pip install --upgrade pip' command.
```

I install Python39 in `c:\Python39` but it use `C:\Users\snipe\AppData\Roaming\Python\Python39\Scripts` don't know why.

after run this code I get this error:
https://gist.github.com/lepotatoguy/df494922d0aaff48e62e0797589513ca
```
TensorFlow Version: 2.9.0-dev20220218
Num GPUs Available:  0
[C:\Users\snipe\AppData\Local\Temp\ipykernel_6776\3718260442.py:6](): DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)
[C:\Users\snipe\AppData\Local\Temp\ipykernel_6776\3718260442.py:11](): UserWarning: No GPU found. Please ensure you have installed TensorFlow correctly
  warnings.warn('No GPU found. Please ensure you have installed TensorFlow correctly')
```

**I think is better if we have a cli install with specific version for windows, linux, and mac**

![image](https://user-images.githubusercontent.com/35699848/154803720-b7dca916-42a0-4bed-ab1c-27fd30597280.png)
"
54456,Performance of nn.conv1d and keras.layers.Conv1D is low the first time any given input size is processed even if retracing is prevented! ,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

Linux Ubuntu 20.04

- TensorFlow installed from (source or binary):

pip

- TensorFlow version (use command below):

tested with TF 2.4, 2.6, 2.8

- Python version:

3.7

- CUDA/cuDNN version:

Cuda Toolkit 11.3.1
CUDNN 8.3.1 

- GPU model and memory:

Tested with GeForce GTX 1050 Ti  and GeForce GTX 1080 Ti

**Describe the current behavior**

In both cases using eager mode and using a `tf.function` with `experimental_relax_shapes=True`
running tf.nn.conv1d is slow the first time a tensor of any given size is processed and the processing of a new tensor of the same size then becomes 4 times (on GPU 1050 TI) or 10 times (on GPU 1080 Ti) faster the second or further times
.
The observed behavior is a severe problem for running inference with audio signals because audio signals generally have very different sizes and therefore in a production environment the code will run only with 10% of maximum performance (on a GPU 1080 Ti) 
for the first few 100k examples until the model has seen sufficiently many lengths to achieve peak performance.

**Describe the expected behavior**

conv1d processing time should depend on the size of the input vector and not on the number of times the same size has been seen.

**Standalone code to reproduce the issue**

colab notebook is 
[here](https://colab.research.google.com/gist/roebel/5cc15bb93566d374e87da9d2c9e885f2/performance_issue.ipynb)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem.

Result running on colab with GPU

```
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
=== run 1 ===
len 10000 time: 10.187 speed 9.82kHz su (10, 9991, 100)
len 10001 time: 0.603 speed 165.93kHz su (10, 9992, 100)
len 10002 time: 0.613 speed 163.08kHz su (10, 9993, 100)
len 10003 time: 0.628 speed 159.26kHz su (10, 9994, 100)
len 10004 time: 0.587 speed 170.35kHz su (10, 9995, 100)
=== run 2 ===
len 10000 time: 0.051 speed 1946.11kHz su (10, 9991, 100)
len 10001 time: 0.050 speed 2000.30kHz su (10, 9992, 100)
len 10002 time: 0.066 speed 1518.87kHz su (10, 9993, 100)
len 10003 time: 0.056 speed 1788.66kHz su (10, 9994, 100)
len 10004 time: 0.059 speed 1705.02kHz su (10, 9995, 100)
```

*Notes:*
- You notice 10 times increase the second time the inner loop is run. If you run the same notebook again the first run will be fast as well, which indicates the GPU is caching something. 
- to have the effect again your need to restart the notebook
- the behavior is not CUDA imposed because an equivalent PyTorch script runs the first and second pass of the inner loop without speed difference."
54451,Issue created for Rollback of PR #54426: Add complex support for tf.math.atan,"Merged PR #54426 is rolled back in ef14b5b7f51357afc8fb131c3c35f90137269160.
    Please follow up with the reviewer and close this issue once its resolved."
54450,Improved MPI support for TensorFlow,"
**System information**
- TensorFlow version (you are using):  I am not a current TensorFlow user
- Are you willing to contribute it (Yes/No): I am willing to help develop it

**Describe the feature and the current behavior/state.**
I lead the PMIx initiative and have been the runtime lead on OpenMPI since its inception. We have been receiving a steady stream of problem complaints about MPI use from within TensorFlow, mostly due to problems with MPI wireup. We have developed methods for providing a more robust mechanism for MPI support of programming models that do not follow the traditional MPI ""bulk synchronous"" architecture. However, it has become apparent that TensorFlow is not using those methods.

I would like to initiate a collaboration to resolve these problems by helping TensorFlow developers utilize the dynamic programming model support for MPI. I have given many presentations on this subject, some of which are available as online videos, and am willing to give an in-person version to help kick things off. I can also help a bit with coding and am always available for advice and guidance. I acknowledge that we sometimes have to ""tweak"" our support to better fit a particular programming model, and I am willing to make such adjustments as they are uncovered.

**Will this change the current api? How?**
I don't believe there will be a need for such changes, but I defer that to the TensorFlow community once we identify what needs to be done.

**Who will benefit with this feature?**
All TensorFlow users should benefit as it will make MPI operations more robust

**Any Other info.**
"
54449,"Unrecognized term used by the documentation of the class ""tensorflow.compat.v1.variable_scope"".","The paragraph following:

                      ""A note about using variable scopes in multi-threaded environment: Variable
                        scopes are thread local, so one thread will not see another thread's current
                        scope. Also, when using `default_name`, unique scopes names are also generated
                        only on a per thread basis. If the same name was used within a different
                        thread, that doesn't prevent a new thread from creating the same scope.
                        However, the underlying variable store is shared across threads (within the
                        same graph). As such, if another thread tries to create a new variable with
                        the same name as a variable created by a previous thread, it will fail unless
                        reuse is True.""

which is found in both the documentation (https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) and the source code (https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/ops/variable_scope.py#L2144-L2579).

My question(s):

For the last sentence of this paragraph, what is the point it talks about? 
It seems to talk about a condition when two threads cause an error since one tries to have a variable of the same name as the other, previous one, although I am not sure what is the mentioned 'underlying variable store' referring to.
But after some tests, I haven't found the condition yet, while it is always the case that each thread has a variable scope separated from others.
Does anyone have learned about the documented condition or the term 'underlying variable store'?


Additional Information:

Here are codes used for two tests, all run without an error found. (With Python 3.7.12, Tensorflow 1.15.0 as the environment)

1. Test One

```
      import tensorflow as tf
      
      def F():
        T = tf.get_variable( 'Mark' , [1,2] )
        print( T.name )
      
      import threading
      t1 = threading.Thread( target=F , args=() )
      t2 = threading.Thread( target=F , args=() )
      
      t1.start()
      t2.start()
      t1.join()
      t2.join()
```

2. Test Two
```
      import tensorflow as tf
      
      def F(s):
        with tf.variable_scope(s,reuse=tf.AUTO_REUSE) as scope:
          T = tf.get_variable( 'Mark' , [1,2] )
          print( T.name )
      
      import threading
      with tf.variable_scope('s',reuse=tf.AUTO_REUSE):
        s = tf.get_variable_scope()
        T = tf.get_variable( 'Mark' , [1,2] )
        t1 = threading.Thread( target=F , args=(s,) )
        t2 = threading.Thread( target=F , args=(s,) )
      
      t1.start()
      t2.start()
      t1.join()
      t2.join()
```"
54448,Problems compiling TensorFlow on M1 Mac within Rosetta,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **no**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS Monterey 12.1, MacBook Pro M1 2021**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **2.7.1 (couldn't use the command because the import fails)**
- Python version: **3.8.5 (Miniconda)**
- Bazel version (if compiling from source):
**Build label: 5.0.0
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Jan 19 14:15:55 2022 (1642601755)
Build timestamp: 1642601755
Build timestamp as int: 1642601755**
- GCC/Compiler version (if compiling from source):
**Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/4.2.1
Apple clang version 13.0.0 (clang-1300.0.29.30)
Target: x86_64-apple-darwin21.2.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin**
- CUDA/cuDNN version: **n/a**
- GPU model and memory: **n/a**

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I've been trying to compile TensorFlow from source using a Rosetta terminal.
I use Miniconda.
Here is what I end up with:
```
(sandbox) ➜  wheels python -c ""import tensorflow as tf""
Traceback (most recent call last):
  File ""/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): symbol not found in flat namespace '_PyCMethod_New'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 79, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): symbol not found in flat namespace '_PyCMethod_New'


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```

**Describe the expected behavior**

The TensorFlow library to be imported without issue.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): **no**
- Briefly describe your candidate solution(if contributing): **n/a**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
(base) ➜  wheels arch
i386
(base) ➜  wheels git clone https://github.com/tensorflow/tensorflow.git
(base) ➜  wheels cd tensorflow
(base) ➜  tensorflow git:(master) git checkout v2.7.1
(base) ➜  tensorflow git:(2a0f59ecfe6) ./configure
(base) ➜  tensorflow git:(2a0f59ecfe6) bazel build //tensorflow/tools/pip_package:build_pip_package
(base) ➜  tensorflow git:(2a0f59ecfe6) ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
(base) ➜  tensorflow git:(2a0f59ecfe6) mv /tmp/tensorflow_pkg/tensorflow-2.7.1-cp39-cp39-macosx_12_0_x86_64.whl 
 tmp/tensorflow_pkg/tensorflow-2.7.1-py3-none-any.whl
(base) ➜  tensorflow git:(2a0f59ecfe6) conda create -y --name sandbox python=3.8.5
(base) ➜  tensorflow git:(2a0f59ecfe6) conda activate sandbox
(sandbox) ➜  tensorflow git:(2a0f59ecfe6) pip install /tmp/tensorflow_pkg/tensorflow-2.7.1-py3-none-any.whl
(sandbox) ➜  tensorflow git:(2a0f59ecfe6) python -c ""import tensorflow as tf""
```

**Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Any help would be really appreciated. Thanks!"
54447,mean_squared_error accuracy,"My currenct observation with ""mean_squared_error"" loss,
If you use mean_squared_error loss and metrics= [""acc""] , 
during training you can't see any meaningfull accuracy,
I wrote my custom acc. function , it's very hard for me to make a PR, because of project complexty,

Here is my fn. 
```
def mean_squared_error_acc(y_true, y_pred):
    loss = tf.losses.mean_squared_error(y_true,y_pred)    
    return 100 * ( 1 - tf.math.sqrt(loss))

```
( by the way, I'm at very begining of my ML experience, and this issue would be meaningless , please lead me to right direction if I'm wrong) 

--Edit--
I use one of my colab experiments to reproduce issue
This is colab link
https://colab.research.google.com/drive/1hYG4RmPaUlnEcRm28NE6Y3HxIMoq1DTM?usp=sharing

These are the cells related the issue : 

https://colab.research.google.com/drive/1hYG4RmPaUlnEcRm28NE6Y3HxIMoq1DTM#scrollTo=lL3kJjI-cXLS&line=4&uniqifier=1

https://colab.research.google.com/drive/1hYG4RmPaUlnEcRm28NE6Y3HxIMoq1DTM#scrollTo=eYb8YEiRvVuK&line=3&uniqifier=1"
54446,Can't install Tensorflow 2.x.x on Jetson boards with pip,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson - Jp 4.6
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 2.x.x
- Python version: 3.6/3.8
- Installed using virtualenv? pip? conda?: Pip/virtualenv
- CUDA/cuDNN version: 11.3
- GPU model and memory: Jetson AGX/ 32GB



**Describe the problem**
Cannot install Tensorflow on Jetson boards with ease of pip.

pip install tensorflow
"
54445,Save and load a model with PReLU activation results in error,"There is very specific scenarios when PReLU activation failed during `tf.keras.models.load`

- Create and save the model via keras.Model.save
- Load the model, make modifications to the Layer that including the PReLU activation
- Save the modified Model and load it again, PReLU layer failed during `build` when initializing the `alpha` parameters

Note: this issue disappear when I subclassing the `PReLU` that is replacing all `PReLU` with `ParamReLU`
or specify the `share_axes` argument also solve the problem

```python
class ParamReLU(PReLU):
    ...
```

---

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**, **Windows 10**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.6.0, 2.6.1, 2.7.0, 2.8.0**; **v2.8.0-rc1-32-g3f878cff5b6 2.8.0**
- Python version: **3.7.10**
- CUDA/cuDNN version: **11.2/8.10**
- GPU model and memory: **RTX 3080 / 10GB**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): **no**
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, Input, PReLU

x = Input((1, 1, 1))
y = Conv2D(8, 1)(x)
y = PReLU()(y)
y = Conv2D(40, 1)(y)
model = Model(x, y)

model.save('/tmp/model_abc')
model_new = tf.keras.models.load_model('/tmp/model_abc')
layer1 = model_new.get_layer('conv2d')
layer2 = model_new.get_layer('p_re_lu')
layer3 = model_new.get_layer('conv2d_1')

# === prune the last filter of layer1
cfg = layer1.get_config()
cfg['filters'] = 7
layer1_copy = layer1.__class__.from_config(cfg)
layer1_copy.build((None, 1, 1, 1))
layer1_copy.set_weights([layer1.kernel.numpy()[..., :7],
                         layer1.bias.numpy()[:7]])
y = layer1_copy(model_new.inputs[0])

# remove the last alpha of PReLU
layer2_copy = layer2.__class__.from_config(layer2.get_config())
layer2_copy.build(y.shape)
layer2_copy.set_weights([layer2.alpha.numpy()[..., :7]])
y = layer2_copy(y)

# remove 1 input filter layer3
cfg = layer3.get_config()
layer3_copy = layer3.__class__.from_config(cfg)
layer3_copy.build(y.shape)
layer3_copy.set_weights([layer3.kernel.numpy()[..., :7, :],
                         layer3.bias.numpy()])
y = layer3_copy(y)
model_prune = Model(model_new.inputs, y)

# everything OK here
print(model_prune.summary())

# === error happening here when we save and load the model again
model_prune.save('/tmp/model_abc_prune')
model_prune_new = tf.keras.models.load_model('/tmp/model_abc_prune')
```

**Other info / logs** 
```
Traceback (most recent call last):
  File ""..."", line 44, in <module>
    model_prune_new = tf.keras.models.load_model('/tmp/model_abc_prune')
  File ""/home/trung/miniconda3/envs/denoise/lib/python3.7/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/trung/miniconda3/envs/denoise/lib/python3.7/site-packages/keras/initializers/initializers_v2.py"", line 145, in __call__
    return tf.zeros(shape, dtype)
ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
```

"
54443,I want to add support for a neuro accelerator,"Hi.
I want to add to tf support for its own neural gas pedal like CUDA, how difficult is it to implement such a thing? "
54442,Reproducible init of trainable variables (TVs) even when number of TVs changes,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS
- TensorFlow installed from (source or binary): PIP
- TensorFlow version (use command below):2.8
- Python version:3.8.12
- GPU model and memory:8×v100

For example:

Model has 3 vars, varA, varB and varC

Now, we change the model with a new varA2, insert between varA and varB. So the model has 4 vars, varA, varA2, varB and varC.

Even varA2 is not used except weight initialization, e.g. tf.keras.initializers.GlorotUniform(). The model will output different result than before, because now the varA2 may use the weights of pervious varB and so on.

It is possible to binding the weight random initialization with its name?

@duncanriach 


https://colab.research.google.com/drive/19Ui9Fp1JF06Xl440pnHZ8zis-yyTx9jq?usp=sharing"
54440,"""ValueError: Setting hub.KerasLayer.trainable = True is unsupported when loading from the TF1 Hub format."" when running Object Detection with TensorFlow Lite Model Maker tutorial on Windows 10. Tutorial ran well on Mac book","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):2.8.0
- Python version:3.9.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 11.0
- GPU model and memory: NVIDIA GeForce MX150  16GB Ram

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Encountered 

    ValueError: Setting hub.KerasLayer.trainable = True is unsupported when loading from the TF1 Hub format.

When running below tutorial for training custom model for object detection using Tensorflow Lite Model Maker

https://www.tensorflow.org/lite/tutorials/model_maker_object_detection



**Describe the expected behavior**

custom model to be trained.

Similar set up is working in Mac book. Please advise on how to get the custom model training working on Windows 10 machine.


**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

On Windows 10 machine, download the Anaconda installer. Follow default settings to install Anaconda

Open Anaconda prompt and create environment test by

conda create -name test python=3.9

activate environment by

(base) C:\Users\dev>conda activate test

(test) C:\Users\dev>

Install tflite-model-maker by 

(test) C:\Users\dev>pip install tflite-model-maker

after installation completed, enter python prompt

(test) C:\Users\dev>python
Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>>

execute the instructions adapted from 

https://www.tensorflow.org/lite/tutorials/model_maker_object_detection

(test) C:\Users\dev>python
Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> import os
>>> from tflite_model_maker.config import QuantizationConfig
>>> from tflite_model_maker.config import ExportFormat
>>> from tflite_model_maker import model_spec
>>> from tflite_model_maker import object_detector
>>> import tensorflow as tf
>>> assert tf.__version__.startswith('2')
>>> spec = model_spec.get('efficientdet_lite0')
2022-02-18 12:41:00.098002: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-02-18 12:41:01.311912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory:  -> device: 0, name: NVIDIA GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1
>>> train_data, validation_data, test_data = object_detector.DataLoader.from_csv('C:/Users/dev/Documents/achiever/tensorFlow/sub_salads.csv')
INFO:tensorflow:Cache will be stored in C:\Users\dev\AppData\Local\Temp\tmpafosi8_7 with prefix filename train_47472d894c199311b64931ed1b6dae18. Cache_prefix is C:\Users\dev\AppData\Local\Temp\tmpafosi8_7\train_47472d894c199311b64931ed1b6dae18
INFO:tensorflow:Cache will be stored in C:\Users\dev\AppData\Local\Temp\tmpv_nxxrzt with prefix filename val_47472d894c199311b64931ed1b6dae18. Cache_prefix is C:\Users\dev\AppData\Local\Temp\tmpv_nxxrzt\val_47472d894c199311b64931ed1b6dae18
INFO:tensorflow:Cache will be stored in C:\Users\dev\AppData\Local\Temp\tmpdfguu885 with prefix filename test_47472d894c199311b64931ed1b6dae18. Cache_prefix is C:\Users\dev\AppData\Local\Temp\tmpdfguu885\test_47472d894c199311b64931ed1b6dae18
INFO:tensorflow:On image 0
INFO:tensorflow:On image 0
INFO:tensorflow:On image 0
>>> model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)
INFO:tensorflow:Retraining the models...
WARNING:tensorflow:The size of the validation_data (3) is smaller than batch_size (8). Ignore the validation_data.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\dev\anaconda3\envs\test\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\object_detector.py"", line 260, in create
    object_detector.train(train_data, validation_data, epochs, batch_size)
  File ""C:\Users\dev\anaconda3\envs\test\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\object_detector.py"", line 123, in train
    return self.model_spec.train(self.model, train_ds, steps_per_epoch,
  File ""C:\Users\dev\anaconda3\envs\test\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_spec\object_detector_spec.py"", line 264, in train
    train.setup_model(model, config)
  File ""C:\Users\dev\anaconda3\envs\test\lib\site-packages\tensorflow_examples\lite\model_maker\third_party\efficientdet\keras\train.py"", line 113, in setup_model
    model.build((None, *config.image_size, 3))
  File ""C:\Users\dev\anaconda3\envs\test\lib\site-packages\keras\engine\training.py"", line 440, in build
    self.call(x, **kwargs)
  File ""C:\Users\dev\anaconda3\envs\test\lib\site-packages\tensorflow_examples\lite\model_maker\third_party\efficientdet\keras\train_lib.py"", line 885, in call
    cls_outputs, box_outputs = self.base_model(inputs, training=training)
  File ""C:\Users\dev\anaconda3\envs\test\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\dev\anaconda3\envs\test\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 692, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: Exception encountered when calling layer ""keras_layer"" (type KerasLayer).

in user code:

    File ""C:\Users\dev\anaconda3\envs\test\lib\site-packages\tensorflow_hub\keras_layer.py"", line 213, in call  *
        self._check_trainability()
    File ""C:\Users\dev\anaconda3\envs\test\lib\site-packages\tensorflow_hub\keras_layer.py"", line 272, in _check_trainability  *
        raise ValueError(

    ValueError: Setting hub.KerasLayer.trainable = True is unsupported when loading from the TF1 Hub format.


Call arguments received:
  • inputs=tf.Tensor(shape=(None, 320, 320, 3), dtype=float32)
  • training=False

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Related to issue:

https://github.com/tensorflow/hub/issues/841

I have since uninstall Python 3.8.6, Anaconda3 and reinstalled Anaconda3. I tried running the above in Anaconda environment with python 3.9 installed. However, the error persisted.

I repeat the same setup on my Mac book and it worked. Thus, this looks to be a Windows 10 specific issue.

attached is Anaconda exported environment from windows:

[winEnv.yml.txt](https://github.com/tensorflow/tensorflow/files/8094267/winEnv.yml.txt)

and the working Anaconda exported environment from Mac Book

[mac.yml.txt](https://github.com/tensorflow/tensorflow/files/8094270/mac.yml.txt)



"
54439,error with functools32,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.15
- Python version:3.6.4
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):0.26.1
- GCC/Compiler version (if compiling from source):7.3.0
- CUDA/cuDNN version:none
- GPU model and memory:cpu-only

**Describe the problem**
I compiled the file `tensorflow-1.15.5-cp36-cp36m-linux_x86_64.whl` by using the tensorflw@r1.15, when i use `pip install tensorflow-1.15.5-cp36-cp36m-linux_x86_64.whl`,  there is a problem:

Downloading functools32-3.2.3-2.tar.gz (31 kB)
    ERROR: Command errored out with exit status 1:
     command: /root/anaconda3/envs/tf_test/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-zi7j89fx/functools32_fe3235542fe548578e556e08b3efc74b/setup.py'""'""'; __file__='""'""'/tmp/pip-install-zi7j89fx/functools32_fe3235542fe548578e556e08b3efc74b/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-pip-egg-info-oymtginn
         cwd: /tmp/pip-install-zi7j89fx/functools32_fe3235542fe548578e556e08b3efc74b/
    Complete output (1 lines):
    This backport is for Python 2.7 only.

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
54437,libXNNPACK.so: internal symbol `xnn_f16_ibilinear_ukernel__neonfp16arith_c8' isn't defined,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 56e2e916929746cd6c415c2a402a46349bffa7e0 (commit id)
- Python version: N/A
- Installed using virtualenv? pip? conda?:  N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

I was trying to build TFLite with XNNPACK delegate natively on [an ARM-based board](https://www.khadas.com/vim3) using CMake. Previously I successfully built TFLite as static library, but when I tried to link it to my program, many errors of undefined references related to `ruy` happened. So I turned to compile TFLite as shared library instead, but got the following error:
```
[ 21%] Linking CXX shared library libXNNPACK.so
/usr/bin/ld: CMakeFiles/XNNPACK.dir/src/init.c.o: in function `init':
init.c:(.text+0x139c): undefined reference to `xnn_f16_ibilinear_ukernel__neonfp16arith_c8'
/usr/bin/ld: init.c:(.text+0x13a0): undefined reference to `xnn_f16_ibilinear_ukernel__neonfp16arith_c8'
/usr/bin/ld: libXNNPACK.so: internal symbol `xnn_f16_ibilinear_ukernel__neonfp16arith_c8' isn't defined
/usr/bin/ld: final link failed: bad value
collect2: error: ld returned 1 exit status
make[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:6532: _deps/xnnpack-build/libXNNPACK.so] Error 1
make[1]: *** [CMakeFiles/Makefile2:5486: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/all] Error 2
make: *** [Makefile:152: all] Error 2
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
cmake -B tflite-build -DBUILD_SHARED_LIBS=ON tensorflow/tensorflow/lite
cmake --build tflite-build -j 4
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
54436,Using `+` in custom residual Keras `Layer` does not create correct model graph,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.6
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0
- Python version: 3.7.12

Also reproducible on various hosted Jupyter environments (Kaggle, Colab) with and without GPU.

**Describe the current behavior**

In custom residual block implementation with keras APIs, `+` yields broken graph.

```python
def call(self, x, training=False):
    for block in self.blocks:
        h = x
        for conv in block:
            h = conv(h, training=training)

        x = x + h

    return x
```

This is the resulting graph:
![res-block-broken](https://user-images.githubusercontent.com/7884451/154602227-346a939a-275f-4508-84ff-8b0163e5ac13.png)

**Describe the expected behavior**

This code produces the correct graph, where each `add` is a separate instance of `keras.layers.Add`.
The `+` operator should produce the same graph.

```python
def call(self, x, training=False):
    for block, add in zip(self.blocks, self.adds):
        h = x
        for conv in block:
            h = conv(h, training=training)

        x = add([x, h])

    return x
```
![res-block-working](https://user-images.githubusercontent.com/7884451/154602398-cd61f319-cfee-4bcc-89a3-b1b50e49109d.png)

I will add that this isn't just a visualization issue. My model would not train until after I identified this problem and applied the fixed implementation described above. This was causing serious issues with my gradients and the model could not learn because it was too deep without the skip connections.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): I would consider it, but leaning towards no.
- Briefly describe your candidate solution(if contributing): N/A

**Standalone code to reproduce the issue**
```python
from typing import Optional, Tuple, Union

import tensorflow as tf
import tensorflow.nn
import tensorflow.keras.backend as K
import tensorflow.keras.layers as L
from tensorflow import keras

def plot_model(model, shape):
    inputs = keras.Input(shape[1:])
    ones = tf.ones(shape)
    model(ones)  # I think needed to properly init graph for plotting
    outputs = model.call(inputs)
    wrapped_model = keras.Model(inputs, outputs)
    return tensorflow.keras.utils.plot_model(
        wrapped_model, expand_nested=True, show_shapes=True)

class ConvBnAct(L.Layer):

    def __init__(
        self,
        out_channels: int,
        kernel_size: Union[int, Tuple[int]],
        stride: Union[int, Tuple[int]],
        activation: Optional[str] = 'swish',
        use_bias=False,
        use_batch_norm=True,
        data_format='channels_last'
            ):
        super().__init__()

        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.use_bias = use_bias
        self.data_format = data_format

        self.activation = L.Activation(activation)
        self.act_type = activation

        bn_axis = 1 if data_format == 'channels_first' else -1
        self.batch_norm = L.BatchNormalization(
            axis=bn_axis) if use_batch_norm else None

    def build(self, input_shape):
        self.conv = L.Conv2D(
            self.out_channels,
            self.kernel_size,
            input_shape=input_shape[1:],
            padding='same',
            strides=self.stride,
            activation=None,
            use_bias=self.use_bias,
            data_format=self.data_format,
            )

    def call(self, inputs, training=False):
        x = self.conv(inputs)

        if self.batch_norm:
            x = self.batch_norm(x, training=training)

        if self.activation:
            x = self.activation(x)

        return x

class ResBlock(L.Layer):

    def __init__(
        self,
        blocks: int,
        shortcut=True,
        data_format='channels_last'
    ):
        super().__init__()
        self.n_blocks = blocks
        self.shortcut = shortcut
        self.data_format = data_format

    def build(self, input_shape):
        channel_axis = 1 if self.data_format == 'channels_first' else -1
        channels = input_shape[channel_axis]

        self.blocks = []
        for i in range(self.n_blocks):
            block = [
                ConvBnAct(channels, kernel_size=1, stride=1, data_format=self.data_format),
                ConvBnAct(channels, kernel_size=3, stride=1, data_format=self.data_format)
                ]
            self.blocks.append(block)

    def call(self, x, training=False):
        for block in self.blocks:
            h = x
            for conv in block:
                h = conv(h, training=training)

            x = x + h if self.shortcut else h

        return x

if __name__ == '__main__':
    i = keras.Input((24, 24, 3))
    r = ResBlock(2, True)
    plot_model(r, (1, 24, 24, 3))
```
"
54435,Module structure not recognized by Visual Studio Code Linter,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 10.0.22000
- TensorFlow installed from (source or binary): through pip?
- TensorFlow version (use command below): 2.8.0
- Python version: 3.10.2

I am very perplexed by tensorflow's module structure.  I recently started using tensorflow in Visual Studio Code and immediately ran into a problem where imports from tensorflow.keras cannot be resolved by Pylance.  For example, the ""layers"" module is not recognized from the line `from tensorflow.keras import layers`.  Interestingly enough, the code runs fine despite this error, but the lack of support from the linter makes writing code very difficult.  (For reference, I am using the ""Probabilistic Bayesian Neural Networks"" example script.)

My attempts to fix this issue led to a number of other discoveries which still have me confused:

- Using `from tensorflow import keras` is recognized by the linter, but the linter still can't offer any helpful predictions off of the keras module.  In this instance, replacing references to `layers` with `keras.layers` still work, despite no indication from the linter that they should.  
- Importing keras directly (2.8.0) instead of through tensorflow is recognized, and `keras.layers` is still valid, but hints from linting are still not present, and other parts of the code will break unexpectedly.  For example, a call to `keras.optimizers.RMSprop`, is invalid even though `keras.optimizers` is recognized and both are recognized if keras is imported through tensor flow.
- Importing keras through tensorflow.python behaves similarly to directly importing keras.
- Even though `keras.layers` is not recognized by the linter after importing just keras, the linter does recognize `from keras import layers` and offers completions for layers after the fact.
- `from tensorflow import keras` was recognized in tensorflow 2.7.0.  This issue only started when I updated.

Admittedly, my understanding of packaging and linting in Python is somewhat limited, but I've never had issues like this with any other package I've worked with.  Am I missing something obvious here?  Is this a known issue?  If this kind of structure is intentional, what is the rationale behind it?  Are there known workarounds/resources that I could use to better understand this issue?

Thanks in advance for the help.
"
54423,tensorflow-gpu doesn't pack CUDA and cuDNN anymore through conda-forge,"**System information**
- OS Platform and Distribution : Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 2.3.0
- Python version: 3.8.12
- Installed using virtualenv? pip? conda?: conda
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: NVIDIA Quadro P400, 2Gb



**Describe the problem**

A few months ago, I installed TensorFlow on a machine through conda-forge. After running into some issues installing it at the system level, I found the simplest solution was to install the tensorflow-gpu package in a conda environment. 
That meta-package came in with cudatoolkit and cudnn, and it worked out of the box.

However, as I tried to replicate that install on another machine, I found that the list of packages provided when trying to install tensorflow-gpu through conda-forge did not contain cudatoolkit or cudnn. 

I also tried again on the machine where I had a successful install, in another environment, and the issue persisted.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`conda install tensorflow-gpu==2.3.0 -c conda-forge`

conda outputs the following : 

```
The following NEW packages will be INSTALLED:

  _tflow_select      pkgs/main/win-64::_tflow_select-2.3.0-gpu
  absl-py            conda-forge/noarch::absl-py-1.0.0-pyhd8ed1ab_0
  aiohttp            conda-forge/win-64::aiohttp-3.8.1-py38h294d835_0
  aiosignal          conda-forge/noarch::aiosignal-1.2.0-pyhd8ed1ab_0
  astor              conda-forge/noarch::astor-0.8.1-pyh9f0ad1d_0
  astunparse         conda-forge/noarch::astunparse-1.6.3-pyhd8ed1ab_0
  async-timeout      conda-forge/noarch::async-timeout-4.0.2-pyhd8ed1ab_0
  attrs              conda-forge/noarch::attrs-21.4.0-pyhd8ed1ab_0
  blinker            conda-forge/noarch::blinker-1.4-py_1
  brotlipy           conda-forge/win-64::brotlipy-0.7.0-py38h294d835_1003
  cachetools         conda-forge/noarch::cachetools-5.0.0-pyhd8ed1ab_0
  cffi               conda-forge/win-64::cffi-1.15.0-py38hd8c33c5_0
  charset-normalizer conda-forge/noarch::charset-normalizer-2.0.12-pyhd8ed1ab_0
  click              conda-forge/win-64::click-8.0.3-py38haa244fe_1
  colorama           conda-forge/noarch::colorama-0.4.4-pyh9f0ad1d_0
  cryptography       conda-forge/win-64::cryptography-36.0.1-py38hb7941b4_0
  frozenlist         conda-forge/win-64::frozenlist-1.3.0-py38h294d835_0
  gast               conda-forge/noarch::gast-0.4.0-pyh9f0ad1d_0
  google-auth        conda-forge/noarch::google-auth-2.6.0-pyh6c4a22f_1
  google-auth-oauth~ conda-forge/noarch::google-auth-oauthlib-0.4.6-pyhd8ed1ab_0
  google-pasta       conda-forge/noarch::google-pasta-0.2.0-pyh8c360ce_0
  grpcio             conda-forge/win-64::grpcio-1.43.0-py38he5377a8_0
  h5py               conda-forge/win-64::h5py-2.10.0-nompi_py38he6c2248_106
  hdf5               conda-forge/win-64::hdf5-1.10.6-nompi_h5268f04_1114
  idna               conda-forge/noarch::idna-3.3-pyhd8ed1ab_0
  importlib-metadata conda-forge/win-64::importlib-metadata-4.11.1-py38haa244fe_0
  intel-openmp       conda-forge/win-64::intel-openmp-2022.0.0-h57928b3_3663
  keras-applications conda-forge/noarch::keras-applications-1.0.8-py_1
  keras-preprocessi~ conda-forge/noarch::keras-preprocessing-1.1.2-pyhd8ed1ab_0
  krb5               conda-forge/win-64::krb5-1.19.2-h20d022d_3
  libblas            conda-forge/win-64::libblas-3.9.0-13_win64_mkl
  libcblas           conda-forge/win-64::libcblas-3.9.0-13_win64_mkl
  libcurl            conda-forge/win-64::libcurl-7.81.0-h789b8ee_0
  liblapack          conda-forge/win-64::liblapack-3.9.0-13_win64_mkl
  libprotobuf        conda-forge/win-64::libprotobuf-3.19.4-h7755175_0
  libssh2            conda-forge/win-64::libssh2-1.10.0-h680486a_2
  libzlib            conda-forge/win-64::libzlib-1.2.11-h8ffe710_1013
  m2w64-gcc-libgfor~ conda-forge/win-64::m2w64-gcc-libgfortran-5.3.0-6
  m2w64-gcc-libs     conda-forge/win-64::m2w64-gcc-libs-5.3.0-7
  m2w64-gcc-libs-co~ conda-forge/win-64::m2w64-gcc-libs-core-5.3.0-7
  m2w64-gmp          conda-forge/win-64::m2w64-gmp-6.1.0-2
  m2w64-libwinpthre~ conda-forge/win-64::m2w64-libwinpthread-git-5.0.0.4634.697f757-2
  markdown           conda-forge/noarch::markdown-3.3.6-pyhd8ed1ab_0
  mkl                conda-forge/win-64::mkl-2022.0.0-h0e2418a_796
  msys2-conda-epoch  conda-forge/win-64::msys2-conda-epoch-20160418-1
  multidict          conda-forge/win-64::multidict-6.0.2-py38h294d835_0
  numpy              conda-forge/win-64::numpy-1.22.2-py38hcf66579_0
  oauthlib           conda-forge/noarch::oauthlib-3.2.0-pyhd8ed1ab_0
  opt_einsum         conda-forge/noarch::opt_einsum-3.3.0-pyhd8ed1ab_1
  protobuf           conda-forge/win-64::protobuf-3.19.4-py38h885f38d_0
  pyasn1             conda-forge/noarch::pyasn1-0.4.8-py_0
  pyasn1-modules     conda-forge/noarch::pyasn1-modules-0.2.7-py_0
  pycparser          conda-forge/noarch::pycparser-2.21-pyhd8ed1ab_0
  pyjwt              conda-forge/noarch::pyjwt-2.3.0-pyhd8ed1ab_1
  pyopenssl          conda-forge/noarch::pyopenssl-22.0.0-pyhd8ed1ab_0
  pyreadline         conda-forge/win-64::pyreadline-2.1-py38haa244fe_1005
  pysocks            conda-forge/win-64::pysocks-1.7.1-py38haa244fe_4
  python_abi         conda-forge/win-64::python_abi-3.8-2_cp38
  pyu2f              conda-forge/noarch::pyu2f-0.1.5-pyhd8ed1ab_0
  requests           conda-forge/noarch::requests-2.27.1-pyhd8ed1ab_0
  requests-oauthlib  conda-forge/noarch::requests-oauthlib-1.3.1-pyhd8ed1ab_0
  rsa                conda-forge/noarch::rsa-4.8-pyhd8ed1ab_0
  scipy              conda-forge/win-64::scipy-1.8.0-py38ha1292f7_1
  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0
  tbb                conda-forge/win-64::tbb-2021.5.0-h2d74725_0
  tensorboard        conda-forge/noarch::tensorboard-2.8.0-pyhd8ed1ab_1
  tensorboard-data-~ conda-forge/win-64::tensorboard-data-server-0.6.0-py38haa244fe_1
  tensorboard-plugi~ conda-forge/noarch::tensorboard-plugin-wit-1.8.1-pyhd8ed1ab_0
  tensorflow         pkgs/main/win-64::tensorflow-2.3.0-mkl_py38h8557ec7_0
  tensorflow-base    pkgs/main/win-64::tensorflow-base-2.3.0-eigen_py38h75a453f_0
  tensorflow-estima~ conda-forge/noarch::tensorflow-estimator-2.5.0-pyh81a9013_1
  tensorflow-gpu     pkgs/main/win-64::tensorflow-gpu-2.3.0-he13fc11_0
  termcolor          conda-forge/noarch::termcolor-1.1.0-py_2
  tk                 conda-forge/win-64::tk-8.6.12-h8ffe710_0
  typing-extensions  conda-forge/noarch::typing-extensions-4.1.1-hd8ed1ab_0
  typing_extensions  conda-forge/noarch::typing_extensions-4.1.1-pyha770c72_0
  urllib3            conda-forge/noarch::urllib3-1.26.8-pyhd8ed1ab_1
  werkzeug           conda-forge/noarch::werkzeug-2.0.3-pyhd8ed1ab_1
  win_inet_pton      conda-forge/win-64::win_inet_pton-1.1.0-py38haa244fe_3
  wrapt              conda-forge/win-64::wrapt-1.13.3-py38h294d835_1
  yarl               conda-forge/win-64::yarl-1.7.2-py38h294d835_1
  zipp               conda-forge/noarch::zipp-3.7.0-pyhd8ed1ab_1
  zlib               conda-forge/win-64::zlib-1.2.11-h8ffe710_1013
```

Then, running those lines : 

```
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```

Outputs the following : 

```
2022-02-17 18:50:41.041229: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-02-17 18:50:41.046722: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-02-17 18:50:46.483787: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-02-17 18:50:46.490003: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found      
2022-02-17 18:50:46.496485: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found  
2022-02-17 18:50:46.506046: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found
2022-02-17 18:50:46.512642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found  
2022-02-17 18:50:46.519233: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
2022-02-17 18:50:46.524611: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.       
Skipping registering GPU devices...
Num GPUs Available:  0
```
"
54422,`tf.nn.depth_to_space` related. What is the equivalent of ONNX DepthToSpace with mode `CRB` in TF?,"**System information**
- TensorFlow version (you are using): 2.7.0
- Are you willing to contribute it (Yes/No): Yes.



**Describe the feature and the current behavior/state.**

I what to convert an [ONNX DepthToSpace](https://github.com/onnx/onnx/blob/main/docs/Operators.md#DepthToSpace) layer with mode `CRB` to an equivalent TensorFlow layer. I found [tensorflow::ops::DepthToSpace](https://www.tensorflow.org/api_docs/python/tf/nn/depth_to_space) which, based on my understanding, is equivalent to ONNX DepthToSpace layer with mode `DCR`.

I could not find an implementation of `CRB` mode in TF. Can you confirm this? If this is the case, with your guidance, I can work on this feature.

**Will this change the current api? How?**
Addition of an argument `mode` which accepts `CRB` or `DCR` (Default `DCR`).

**Code to experiment**

```
import tensorflow as tf
import numpy as np

@tf.function(experimental_compile=True)
def depth_to_space_nchw(x, block_size):
    return tf.nn.depth_to_space(x, block_size=block_size, data_format=""NCHW"")

def onnx_depth_to_space_dcr(x, block_size):
    b, c, h, w = x.shape
    tmp = np.reshape(x, [b, block_size, block_size, c // (block_size**2), h, w])
    tmp = np.transpose(tmp, [0, 3, 4, 1, 5, 2])
    y = np.reshape(tmp, [b, c // (block_size**2), h * block_size, w * block_size])
    return np.copy(y, order='C')

def onnx_depth_to_space_crd(x, block_size):
    b, c, h, w = x.shape
    tmp = np.reshape(x, [b, c // (block_size ** 2), block_size, block_size, h, w])
    tmp = np.transpose(tmp, [0, 1, 4, 2, 5, 3])
    y = np.reshape(tmp, [b, c // (block_size ** 2), h * block_size, w * block_size])
    return np.copy(y, order='C')

arr = np.array([x for x in range(12)]).reshape(1, 1, 1, 12)
block_size = 2

arr_nchw = np.copy(np.transpose(arr, [0, 3, 1, 2]),  order='C')
out = tf.nn.depth_to_space(arr, block_size=block_size, data_format='NHWC')
out_nchw = depth_to_space_nchw(arr_nchw, block_size=block_size)

onnx_out_dcr = onnx_depth_to_space_dcr(arr_nchw, block_size)
onnx_out_crb = onnx_depth_to_space_crd(arr_nchw, block_size)

>> out
<tf.Tensor: shape=(1, 2, 2, 3), dtype=int32, numpy=
array([[[[ 0,  1,  2],
         [ 3,  4,  5]],

        [[ 6,  7,  8],
         [ 9, 10, 11]]]])>

>> np.transpose(out_nchw, [0, 2, 3, 1])
array([[[[ 0,  1,  2],
         [ 3,  4,  5]],

        [[ 6,  7,  8],
         [ 9, 10, 11]]]])

>>np.transpose(onnx_out_dcr, [0, 2, 3, 1]) ## Note that CRB of ONNX is same as TF
array([[[[ 0,  1,  2],
         [ 3,  4,  5]],

        [[ 6,  7,  8],
         [ 9, 10, 11]]]])

>>np.transpose(onnx_out_crb, [0, 2, 3, 1])
array([[[[ 0,  4,  8],
         [ 1,  5,  9]],

        [[ 2,  6, 10],
         [ 3,  7, 11]]]])
```"
54420,tflite inference on single image confidence scores.,"I am fairly new to tensorflow and I have a tflite model which needs inference on a single image (ie no datasets).  When I run the inference on the quantised model, I out output confidence scores in the range of [0,255] as uint8.

However, when I do the inference on batch I get scores in the [0,1] float range.

I am a bit puzzled why this is. For example, if you see this post: https://thinkmobile.dev/testing-tensorflow-lite-image-classification-model/ you can see that the model results are in the 0-1 range, but, if you do it over a single image like so: https://towardsdev.com/custom-image-classification-model-using-tensorflow-lite-model-maker-68ee4514cd45 the results are in [0-255] int range.

Bit perplexed as to why this is."
54418,cmake with DTFLITE_KERNEL_TEST=on in ubuntu reports errors,"Platform: Ubuntu 20.04
Steps to reproduce:
1. git clone https://github.com/tensorflow/tensorflow
2. mkdir tflite_build & cd tflite_build
3. cmake ../tensorflow/tensorflow/lite -DCMAKE_BUILD_TYPE=Debug -DTFLITE_KERNEL_TEST=on -DTFLITE_ENABLE_GPU=ON
Below errors are printed:
```
CMake Error at kernels/CMakeLists.txt:145 (add_executable):
Cannot find source file:



random_uniform_test.cc



Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
.hpp .hxx .in .txx
Call Stack (most recent call first):
kernels/CMakeLists.txt:312 (add_kernel_test)




CMake Error at kernels/CMakeLists.txt:145 (add_executable):
Cannot find source file:



random_standard_normal_test.cc



Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
.hpp .hxx .in .txx
Call Stack (most recent call first):
kernels/CMakeLists.txt:312 (add_kernel_test)




CMake Error at kernels/CMakeLists.txt:145 (add_executable):
No SOURCES given to target: random_uniform_test
Call Stack (most recent call first):
kernels/CMakeLists.txt:312 (add_kernel_test)




CMake Error at kernels/CMakeLists.txt:145 (add_executable):
No SOURCES given to target: random_standard_normal_test
Call Stack (most recent call first):
kernels/CMakeLists.txt:312 (add_kernel_test)




CMake Generate step failed. Build files cannot be regenerated correctly.
```
If I do below changes, it works:
```
diff --git a/tensorflow/lite/kernels/CMakeLists.txt b/tensorflow/lite/kernels/CMakeLists.txt
index daa390fa93c..67ccb896896 100644
--- a/tensorflow/lite/kernels/CMakeLists.txt
+++ b/tensorflow/lite/kernels/CMakeLists.txt
@@ -252,8 +252,6 @@ set(TEST_WITH_EXTERNAL_MAIN_LIST
   pow_test.cc
   quant_basic_lstm_test.cc
   quantize_test.cc
-  random_standard_normal_test.cc
-  random_uniform_test.cc
   range_test.cc
   rank_test.cc
   reduce_test.cc
@@ -323,4 +321,4 @@ if(${CMAKE_CROSSCOMPILING})
     ${CMAKE_CURRENT_BINARY_DIR}/run-tests.cmake
     COPYONLY
   )
```
However, in the next build step `cmake --build . -j8`. I will meet many build failures.

And I am a newbie for tflite gpu. I want to be familiar with tflite gpu gl backend. So could you help me how to run the kernel tests on ubuntu with gpu-gl backend? Which target should I build? "
54417,```tf.GradientTape``` returns None gradient in fitting RNN,"Define h_t as the hidden states of SimpleRNN layer at step t. h_0 the initial hidden states. In a model with an embedding layer and SimpleRNN layer, I would like to compute the partial derivative dh_t/dh_0 for each step t. 

The structure of my model:
```
# For each h_t, compute gradient:
batch_size = 100; input_length = 1403
inp= Input(batch_shape= (batch_size, input_length), name= 'input') 
emb_out= Embedding(input_dim, output_dim, input_length= input_length, 
                         weights= [Emat], trainable= False, name= 'embedding')(inp)
rnn= SimpleRNN(200, return_sequences= True, return_state= False, stateful= True, name= 'simpleRNN')

h0 = tf.convert_to_tensor(np.random.uniform(size= (batch_size, 200)).astype(np.float32))
rnn_allstates= rnn(emb_out, initial_state=h0) 
model_rnn = Model(inputs=inp, outputs= rnn_allstates, name= 'model_rnn')
model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

ds = tf.data.Dataset.from_tensor_slices((xtr_pad[:100], ytr[:100])).batch(100)
embedding_layer = model_rnn.layers[1]
rnn_layer = model_rnn.layers[2]

grads_allsteps= []
for b, (x_batch_train, y_batch_train) in enumerate(ds):
    for t in range(input_length):
        with tf.GradientTape() as tape:
            tape.watch(h0)
            et = embedding_layer(x_batch_train)
            states = rnn_layer(et, initial_state= h0)   # (100, 1403, 200)
            ht = states[:,t,:]  # (100, 200)

        grad_t= tape.gradient(ht, h0)  # (100, 200)
        print('Computed gradient dht/dh0 at step ', t+1, 'in batch', b+1)
        grads_allsteps.append(grad_t)
``` 
At each step t, h_t has shape (100,200), h_0 has shape (100,200). However ```tape.gradient(ht, h0)``` returns None for every t. Below is the result of the first step:
```
for t in range(1):
    with tf.GradientTape() as tape:
        tape.watch(h0)
        et = embedding_layer(x_batch_train)
        #tape.watch(et)
        states = rnn_layer(et, initial_state= h0)   # (100, 1403, 200)
        ht = states[:,t,:] 
        print(ht)
        print(h0)
    grad_t = tape.gradient(ht, h0)
    tf.print(grad_t)

>>
# h_t:
tf.Tensor(
[[ 0.25634336  0.5259362   0.60045886 ... -0.4978792   0.62755316
   0.09803997]
 [ 0.58387524  0.26037565  0.5646103  ...  0.31233114  0.4853201
   0.10877549]
 [ 0.17190906  0.68681747 -0.32054633 ... -0.6139967   0.48944488
   0.06301598]
 ...
 [ 0.1985917  -0.11821499 -0.47709295 ... -0.05718012  0.16089934
   0.20585683]
 [ 0.73872745  0.503326    0.25224414 ... -0.5771631   0.03748894
   0.09212588]
 [-0.6597108  -0.43926442 -0.23546427 ...  0.26760277  0.28221437
  -0.4039318 ]], shape=(100, 200), dtype=float32)

# h_0:
tf.Tensor(
[[0.51580787 0.51664346 0.70773274 ... 0.45973232 0.7760376  0.48297063]
 [0.61048764 0.26038417 0.60392565 ... 0.7426153  0.15507504 0.57494944]
 [0.11859739 0.33591187 0.68375146 ... 0.59409297 0.5302879  0.28876984]
 ...
 [0.12401487 0.39376178 0.9850304  ... 0.21582918 0.9592233  0.5257605 ]
 [0.9401199  0.2157638  0.6445949  ... 0.36316434 0.5799403  0.3749675 ]
 [0.37230062 0.18162128 0.0739954  ... 0.21624395 0.66291    0.7807376 ]], shape=(100, 200), dtype=float32)

# dh_t/dh_0:
None
```
I have successfully used GradientTape watch the inputs e_t to the RNN layer, and computed the gradients dh_t/de_t, but this does not really provide much information about the quality of model fitting. I would like to base the gradient on a quantity only at the start of the sequence.

There seems to be some difficulty for Gradient tape to watch this fixed-time quantity h_0, and perform gradient computation for dh_t/dh_0. Is there a way I can obtain this gradient shaped (100,200)? Thanks in advance for any help.

"
54416,Tensorflow not detecting GPU RTX 2070,"I am trying to get tensorflow to detect my RTX 2070. I am using Ubuntu with the nvidia-510 drivers. Pytorch is detecting the GPU but tensorflow is not detecting it. I tried reinstalling my drivers and trying the nightly version. I also followed the GPU instructions on the website and tried to install it wtih conda.

I am getting those errors when starting Tensorflow:

```
2022-02-16 22:23:08.039986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:987] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-16 22:23:08.042291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2022-02-16 22:23:08.042658: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```

This is my nvidia-smi output:
![image](https://user-images.githubusercontent.com/68305035/154398647-3fdee35a-5758-40f4-8724-d68b5efd1075.png)

I am using this code to detect the GPU:
```
import tensorflow as tf
tf.config.list_physical_devices()
```

This is what my usr/local looks like:
![image](https://user-images.githubusercontent.com/68305035/154398885-eb990fdf-1327-4251-9754-08e977f48418.png)



"
54415,`tf.histogram_fixed_width_bins` lack checking for `nbins`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
nbins = -16
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
indices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=nbins)
indices.numpy()
```
Outputs:
```
array([0, 0, 0, 0, 0, 0], dtype=int32)
```

**Describe the current behavior**
`tf.histogram_fixed_width_bins` has an argument `nbins` which should be a **positive** integer. However, it does not perform any validity checking and can accept a **negative** value like `-16`.  `tf.histogram_fixed_width` (another API with similar functionality) can detect this error and raise an `InvalidArgumentError`:
```
import tensorflow as tf
nbins = -16
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
indices = tf.histogram_fixed_width(new_values, value_range, nbins=nbins)
indices.numpy()
# InvalidArgumentError: nbins should be a positive number, but got '-16' [Op:HistogramFixedWidth]
```


**Describe the expected behavior**
`tf.histogram_fixed_width_bins` should have better input checking.
"
54414,`tf.clip_by_norm` gives WRONG results when given negative `norm`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf

x = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32)
clip_norm = -6.0
x_clipped = tf.clip_by_norm(x, clip_norm, )
print(x)
print(x_clipped)

```
Outputs:
```
tf.Tensor([[1. 2. 3. 4. 5.]], shape=(1, 5), dtype=float32)
tf.Tensor([[-0.80903983 -1.6180797  -2.4271195  -3.2361593  -4.0451994 ]], shape=(1, 5), dtype=float32)
```

**Describe the current behavior**
`tf.clip_by_norm` has an argument `clip_norm` which should be a **positive** floating point. However, it does not perform any validity checking and can accept a negative value like `-6.0`.  When applied to a tensor, it produces wrong output silently.


**Describe the expected behavior**
`tf.clip_by_norm` should check the value of `clip_norm`.
"
54413,`tf.compat.as_bytes` does not check the encoding string,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
bytes_or_text = ""hello""
encoding = ""valid""
t1 = tf.compat.as_text(bytes_or_text, encoding=encoding)
print(t1) # hello
t2 = tf.compat.as_bytes(bytes_or_text,encoding=encoding)
# LookupError: unknown encoding: valid
```

**Describe the current behavior**
`""valid""` is not valid value for `encoding`, as we can see that `tf.compat.as_bytes` would throw an `LoopupError`. However, `tf.compat.as_text` does not perform any validity checking and can accept it and even give an output.


**Describe the expected behavior**
`tf.compat.as_text` should check the validity of `encoding`.
"
54412,`tf.boolean_mask` lack checking for bool arguments,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
tensor = [0,1,2,3]
mask = tf.random.uniform([4], dtype=tf.float64)
tf.boolean_mask(tensor, mask) 
# Outputs: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)>
```

**Describe the current behavior**
`tf.boolean_mask` has an argument `mask` which should be a `bool` tensor. However, it does not perform any validity checking and can accept a `float64` value. 


**Describe the expected behavior**
`tf.boolean_mask` should check the dtype of input tensor `mask`.

For example, `tf.math.reduce_any` would check the first argument and throw an `InvalidArgumentError` for non-boolean inputs.
```
import tensorflow as tf
input_tensor = tf.random.uniform([4], dtype=tf.float64)
tf.math.reduce_any(input_tensor) # InvalidArgumentError: cannot compute Any as input #0(zero-based) was expected to be a bool tensor but is a double tensor [Op:Any]
```
"
54411,`tf.math.atan` lack support for `complex64`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
x = tf.complex(tf.random.uniform([8, 8], dtype=tf.float32),tf.random.uniform([8, 8], dtype=tf.float32))
print(x.dtype) # <dtype: 'complex64'>
tf.math.atan(x)
```

**Describe the current behavior**
`tf.math.atan` cannot accept a tensor of type `complex64`. However, according to the [document](https://www.tensorflow.org/api_docs/python/tf/math/atan?hl=en) it should support `complex64` and `complex128`.
For the above code snippet, the error message is:
```
NotFoundError: Could not find device for node: {{node Atan}} = Atan[T=DT_COMPLEX64]
All kernels registered for op Atan:
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_HALF]
 [Op:Atan]
```

"
54410,`tf.math.cumsum` lack checking for bool arguments,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
x = [0.5, 1.0, 2.0, 4.0]
axis = 0
exclusive = -1
reverse = -1
res_1 = tf.math.cumsum(x, axis=axis, exclusive=exclusive, reverse=reverse)
print(res_1) # tf.Tensor([7. 6. 4. 0.], shape=(4,), dtype=float32)
res_2 = tf.raw_ops.Cumsum(x=x, axis=axis, exclusive=exclusive, reverse=reverse)
print(res_2) # TypeError: Expected bool for argument 'exclusive' not -1.
```

**Describe the current behavior**
`tf.math.cumsum` has an argument `exclusive` which should be a `bool`. However, it does not perform any validity checking and can accept a non-bool value like `-1`.  `tf.raw_ops.Cumsum` (another API which does the same job) can detect this error and raise an `TypeError`.


**Describe the expected behavior**
`tf.math.cumsum` should check the type of input values for `exclusive` and `reverse`.
"
54407,bazel build tensorflow with CUDA and TennsorRT on windows 10 failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): master
- TensorFlow version: 2.9.0
- Python version: 3.8.10
- Installed using virtualenv? pip? conda?: conda create env -n myenvs
- Bazel version (if compiling from source): bazel 5.0.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 11.3 / cuDNN 8
- GPU model and memory:



**Describe the problem**

I want to build TensorFlow C++ API on Windows 10 with CUDA and TensorRT support.
I downloaded the master branch (https://github.com/tensorflow/tensorflow/ - date: 16.02.22)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
The configuration:

python ./configure.py
You have bazel 5.0.0 installed.
Please specify the location of python. [Default is C:\DevTools\Python\Python_3.8.10\python.exe]:


Found possible Python library paths:
  C:\DevTools\Python\Python_3.8.10\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\DevTools\Python\Python_3.8.10\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

WARNING: TensorRT support on Windows is experimental

Found CUDA 11.3 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/include
Found cuDNN 8 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/include
Found TensorRT 8 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.

Then the baze build command:
bazel build --config=opt tensorflow:tensorflow.dll

This is the error:

Starting local Bazel server and connecting to it...
WARNING: Option 'java_toolchain' is deprecated
WARNING: Option 'host_java_toolchain' is deprecated
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=237
INFO: Reading rc options for 'build' from c:\devtools\tensorflow_master\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/DevTools/Python/Python_3.8.10/python.exe
INFO: Reading rc options for 'build' from c:\devtools\tensorflow_master\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --host_java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from c:\devtools\tensorflow_master\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/DevTools/Python/Python_3.8.10/python.exe --action_env PYTHON_LIB_PATH=C:/DevTools/Python/Python_3.8.10/lib/site-packages --python_path=C:/DevTools/Python/Python_3.8.10/python.exe --config=tensorrt --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from c:\devtools\tensorflow_master\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file c:\devtools\tensorflow_master\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\devtools\tensorflow_master\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tensorrt in file c:\devtools\tensorflow_master\.bazelrc: --repo_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda in file c:\devtools\tensorflow_master\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file c:\devtools\tensorflow_master\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file c:\devtools\tensorflow_master\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\devtools\tensorflow_master\.bazelrc: --define framework_shared_object=false
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/0a77ba77a0e7f58932a038381d997c231947c77f.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Repository local_config_cuda instantiated at:
  C:/devtools/tensorflow_master/WORKSPACE:15:14: in <toplevel>
  C:/devtools/tensorflow_master/tensorflow/workspace2.bzl:868:19: in workspace
  C:/devtools/tensorflow_master/tensorflow/workspace2.bzl:96:19: in _tf_toolchains
Repository rule cuda_configure defined at:
  C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl:1448:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl"", line 1239, column 56, in _create_local_cuda_repository
                host_compiler_includes + _cuda_include_path(
        File ""C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl"", line 364, column 32, in _cuda_include_path
                inc_entries.append(realpath(repository_ctx, cuda_config.cuda_toolkit_path + ""/include""))
        File ""C:/devtools/tensorflow_master/third_party/remote_config/common.bzl"", line 290, column 19, in realpath
                return execute(repository_ctx, [bash_bin, ""-c"", ""realpath \""%s\"""" % path]).stdout.strip()
        File ""C:/devtools/tensorflow_master/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Repository command failed
/usr/bin/bash: line 1: realpath: command not found
ERROR: C:/devtools/tensorflow_master/WORKSPACE:15:14: fetching cuda_configure rule //external:local_config_cuda: Traceback (most recent call last):
        File ""C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl"", line 1239, column 56, in _create_local_cuda_repository
                host_compiler_includes + _cuda_include_path(
        File ""C:/devtools/tensorflow_master/third_party/gpus/cuda_configure.bzl"", line 364, column 32, in _cuda_include_path
                inc_entries.append(realpath(repository_ctx, cuda_config.cuda_toolkit_path + ""/include""))
        File ""C:/devtools/tensorflow_master/third_party/remote_config/common.bzl"", line 290, column 19, in realpath
                return execute(repository_ctx, [bash_bin, ""-c"", ""realpath \""%s\"""" % path]).stdout.strip()
        File ""C:/devtools/tensorflow_master/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Repository command failed
/usr/bin/bash: line 1: realpath: command not found
INFO: Found applicable config definition build:cuda in file c:\devtools\tensorflow_master\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
WARNING: Option 'java_toolchain' is deprecated
WARNING: Option 'host_java_toolchain' is deprecated
ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: Repository command failed
/usr/bin/bash: line 1: realpath: command not found

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
54406,Typo in https://blog.tensorflow.org/2022/02/boost-your-models-accuracy.html?linkId=8033010,...layer with a on top...
54405,The model size after Lite conversion is much larger than the original Tensorflow model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installation (pip package or built from source): pip install
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0

### 2. Code
```
import tensorflow as tf
import tensorflow_text as text
converter = tf.lite.TFLiteConverter.from_saved_model(
    saved_model_dir='chatter_engine',
    signature_keys=['serving_default']
)  # path to the SavedModel directory
print('after load')
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.target_spec.supported_types = [tf.float16]

print('before conversion')
tflite_model = converter.convert()
print('after conversion')

# Save the model.
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
print('after write')

```

```
- Tensorflow model : https://drive.google.com/drive/folders/1WrNca1DQ9xpbH00JAVQKE8PsxFzB60qj?usp=sharing
- Tensorflow lite model: https://drive.google.com/file/d/1nqpkM3ShOTit37BHZ1A5DYEDCjaaac6J/view?usp=sharing
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong: the converted model size is more than 400 MB where the original model is only 4 MB"
54404,Help! Didnt change anything in code and was working fine but today tensorflow gives me a bug.,"Hi guys, till yesterday i was running without a problem my GAN in colab pro + and today i stopped it and tried to begin it again and i am taking an error  ""NotImplementedError: Cannot convert a symbolic Tensor (frame/Size:0) to a numpy array."" I literally didnt change a single thing and i am doing this process the last month without a problem. Does anyone know what is happening?
"
54403,high GPU memory usage with tf.data.Dataset.from_tensor_slices,"System information
- OS Platform and Distribution: Linux 5.13.0-28-generic
- TensorFlow installed from: source
- TensorFlow version: 2.8.0
- Python version: 3.8.10
- Installed using: conda
- CUDA version: 11.5
- GPU model and memory: NVIDIA GeForce RTX 3090 24 GB
- CPU model: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz

When I run the following code on Jupyter Notebook, it uses 22651 MB GPU memory when viewed with `nvtop`:
```
import tensorflow as tf
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
```

It also prints the following message:
```
2022-02-16 11:55:26.100430: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-02-16 11:55:26.576039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22232 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6
```

I found other issues with similar messages, but they did not have the same memory issue. 
"
54400,ValueError: Could not find matching function to call loaded from the SavedModel. Got:,"from bert.tokenization import FullTokenizer
import pandas as pd
import tensorflow_hub as hub

bert_path = ""https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4""

sess = tf.Session()

def create_tokenizer_from_hub_module():
    """"""Get the vocab file and casing info from the Hub module.""""""
    bert_module =  hub.load(bert_path)
    tokenization_info = bert_module(signature=""tokenization_info"", as_dict=True)
    vocab_file, do_lower_case = sess.run(
        [
            tokenization_info[""vocab_file""],
            tokenization_info[""do_lower_case""],
        ]
    )

    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)
  
tokenizer = create_tokenizer_from_hub_module()
)

 but in this line : tokenization_info = bert_module(signature=""tokenization_info"", as_dict=True) 
I am getting  error: ValueError: Could not find matching function to call loaded from the SavedModel. Got:


  Positional arguments (2 total):
    * False
    * None
  Keyword arguments: {'do_lower_case': False, 'as_dict': True, 'signature': 'tokenization_info'}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_type_ids')}
    * False
    * None
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_type_ids')}
    * False
    * None
  Keyword arguments: {}

Option 3:
  Positional arguments (3 total):
    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_type_ids')}
    * True
    * None
  Keyword arguments: {}

Option 4:
  Positional arguments (3 total):
    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_type_ids')}
    * True
    * None
  Keyword arguments: {}

"
54398,"Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.","can't set run_eagerly,but running is wrong:
`model.compile(
        loss=CRFLoss(model.crf, model.dtype),
        optimizer=tf.keras.optimizers.Adam(params[""lr""]),
        metrics=[model.crf.viterbi_accuracy, IOBESF1(id2tag)],
        run_eagerly = None)
    model.build((None, train_text.shape[-1]))
    model.summary()

    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath=config[""ckpt_path""],
            save_weights_only=True,
            save_best_only=True,
            monitor=""val_f1"",
            mode=""max""),
    ]
    model.fit(
        train_dataset,
        epochs=params[""epochs""],
        callbacks=callbacks,
        validation_data=dev_dataset)`

"
54397,Eigen bug - build failed with MSVC when enable F16C,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.19043.1526
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.8.0
- Python version: Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 4.2.1
- GCC/Compiler version (if compiling from source): Microsoft Visual Studio 2019 v16.11.0 / Microsoft (R) C/C++ Optimizing Compiler Version 19.29.30133 for x64
- CUDA/cuDNN version: cuda_11.5.2_496.13_windows / cudnn_8.3.2.44_windows
- GPU model and memory: NVIDIA GeForce 2080Ti 11G

**Describe the problem**

https://docs.microsoft.com/en-us/cpp/intrinsics/x64-amd64-intrinsics-list
https://gitlab.com/libeigen/eigen/-/issues/2395

MSVC supports F16C instruction set but gitlab#2395 only fix for gcc.
Please help to report this issue to upstream repo and update eigen lib version.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /MP /cgthreads8 /Qpar /fp:fast /D__F16C__ /arch:AVX2 -Ob2
bazel build --config=opt --copt=-nvcc_options=use_fast_math --incompatible_strict_action_env=false --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
ERROR: C:/users/user/source/repos/tensorflow/tensorflow/tools/proto_text/BUILD:31:10: Compiling tensorflow/tools/proto_text/gen_proto_text_functions.cc failed: (Exit 2): python.exe failed: error executing command
  cd C:/users/user/_bazel_user/jcmowys7/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.5
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/User/anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Users/User/anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\User\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CUDA_COMPUTE_CAPABILITIES=compute_35,sm_50,sm_52,sm_61,sm_70,sm_75,compute_86
    SET TMP=C:\Users\User\AppData\Local\Temp
  C:/Users/User/anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /MP /cgthreads8 /Qpar /fp:fast /D__F16C__ /arch:AVX2 -Ob2 -nvcc_options=use_fast_math /std:c++14 /Fobazel-out/x64_windows-opt/bin/tensorflow/tools/proto_text/_objs/gen_proto_text_functions/gen_proto_text_functions.obj /c tensorflow/tools/proto_text/gen_proto_text_functions.cc
Execution platform: @local_execution_config_platform//:platform
C:\users\user\_bazel_user\jcmowys7\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/arch/Default/Half.h(538): error C3861: '_cvtss_sh': 找不到識別項
C:\users\user\_bazel_user\jcmowys7\execroot\org_tensorflow\external\eigen_archive\Eigen\src/Core/arch/Default/Half.h(599): error C3861: '_cvtsh_ss': 找不到識別項
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: C:/users/user/source/repos/tensorflow/tensorflow/core/BUILD:1718:23 Middleman _middlemen/_S_Stensorflow_Score_Cframework_Uheaders_Ulib-BazelCppSemantics_build_arch_x64_windows-opt failed: (Exit 2): python.exe failed: error executing command
  cd C:/users/user/_bazel_user/jcmowys7/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.5
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/User/anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Users/User/anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\User\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CUDA_COMPUTE_CAPABILITIES=compute_35,sm_50,sm_52,sm_61,sm_70,sm_75,compute_86
    SET TMP=C:\Users\User\AppData\Local\Temp
  C:/Users/User/anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /MP /cgthreads8 /Qpar /fp:fast /D__F16C__ /arch:AVX2 -Ob2 -nvcc_options=use_fast_math /std:c++14 /Fobazel-out/x64_windows-opt/bin/tensorflow/tools/proto_text/_objs/gen_proto_text_functions/gen_proto_text_functions.obj /c tensorflow/tools/proto_text/gen_proto_text_functions.cc
Execution platform: @local_execution_config_platform//:platform
```"
54396,Floating point exception (core dumped),"**System information**
GPU : 3060, 12G
Linux Ubuntu 16.04
gcc6.3
bazel3.1
python3.8
cuda11.1
cudnn8.1
Tensorflow2.4.1

I use tensorflow by python api, which compiled by the source code， Got the right answer.

I use tensorflow by c++ ,  When the code executes to "" session->Run(inputs, {""ssd_pc_loc:0"",""ssd_pc_score:0"",""ssd_pc_height:0"",""ssd_pc_classes:0""}, {}, &outputs);""  , the error occurred! 

2022-02-15 20:27:17.160612: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
/home/yj/auto_introduce/perception/v2/tf/main.cpp 5
/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 21
/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 61
/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 75
/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 80
/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 85
2022-02-15 20:27:17.237107: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-02-15 20:27:17.271412: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3600000000 Hz
2022-02-15 20:27:17.272708: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f6a920 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-15 20:27:17.272721: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-15 20:27:17.274320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-02-15 20:27:17.373277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.378295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.378866: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f69b60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-15 20:27:17.378881: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6
2022-02-15 20:27:17.378885: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3060, Compute Capability 8.6
2022-02-15 20:27:17.379032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.379463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6
coreClock: 1.777GHz coreCount: 28 deviceMemorySize: 11.75GiB deviceMemoryBandwidth: 335.32GiB/s
2022-02-15 20:27:17.379486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.379893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:02:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6
coreClock: 1.777GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s
2022-02-15 20:27:17.379928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-02-15 20:27:17.381703: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-02-15 20:27:17.381747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-02-15 20:27:17.382384: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-02-15 20:27:17.382658: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-02-15 20:27:17.384218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2022-02-15 20:27:17.384762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-02-15 20:27:17.384927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-02-15 20:27:17.384982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.385427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.385869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.386493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.386967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
2022-02-15 20:27:17.387013: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-02-15 20:27:17.814844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-15 20:27:17.814870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 
2022-02-15 20:27:17.814874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N 
2022-02-15 20:27:17.814876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N 
2022-02-15 20:27:17.814992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.815439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.815868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.816278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10631 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)
2022-02-15 20:27:17.816574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-15 20:27:17.817033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11090 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:02:00.0, compute capability: 8.6)
Session created successfully
/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 108
Load graph protobuf successfully
/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 120
Add graph to session successfully
/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 130
/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 141
2022-02-15 20:27:21.165762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-02-15 20:27:21.494311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-02-15 20:27:21.501505: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2022-02-15 20:27:21.502071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
Floating point exception (core dumped)


"
54393,WARNING: 404 Not Found,"Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from d:\vcpkg\buildtrees\tensorflow-cc\x64-windows-dbg\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/bin/python.exe
INFO: Reading rc options for 'build' from d:\vcpkg\buildtrees\tensorflow-cc\x64-windows-dbg\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from d:\vcpkg\buildtrees\tensorflow-cc\x64-windows-dbg\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/bin/python3.exe --action_env PYTHON_LIB_PATH=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/lib/python3.8/site-packages --python_path=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/bin/python3.exe --define=with_xla_support=false --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from d:\vcpkg\buildtrees\tensorflow-cc\x64-windows-dbg\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file d:\vcpkg\buildtrees\tensorflow-cc\x64-windows-dbg\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\vcpkg\buildtrees\tensorflow-cc\x64-windows-dbg\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file d:\vcpkg\buildtrees\tensorflow-cc\x64-windows-dbg\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\vcpkg\buildtrees\tensorflow-cc\x64-windows-dbg\.bazelrc: --define framework_shared_object=false
Loading: 
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
Loading: 0 packages loaded
Analyzing: 2 targets (1 packages loaded, 0 targets configured)
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  D:/vcpkg/buildtrees/tensorflow-cc/x64-windows-dbg/WORKSPACE:23:14: in <toplevel>
  D:/vcpkg/buildtrees/tensorflow-cc/x64-windows-dbg/tensorflow/workspace0.bzl:108:34: in workspace
  D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Repository local_config_cc instantiated at:
  D:/vcpkg/buildtrees/tensorflow-cc/x64-windows-dbg/WORKSPACE:23:14: in <toplevel>
  D:/vcpkg/buildtrees/tensorflow-cc/x64-windows-dbg/tensorflow/workspace0.bzl:106:24: in workspace
  D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/repositories.bzl:28:17: in rules_cc_toolchains
  D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/cc_configure.bzl:180:16: in cc_configure
Repository rule cc_autoconf defined at:
  D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/cc_configure.bzl:143:30: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cc':
   Traceback (most recent call last):
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/cc_configure.bzl"", line 120, column 36, in cc_autoconf_impl
		configure_windows_toolchain(repository_ctx)
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl"", line 694, column 31, in configure_windows_toolchain
		msvc_vars = _get_msvc_vars(repository_ctx, paths)
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl"", line 535, column 28, in _get_msvc_vars
		env = setup_vc_env_vars(repository_ctx, vc_path)
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl"", line 318, column 24, in setup_vc_env_vars
		_check_env_vars(env_map, cmd, expected = envvars)
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl"", line 324, column 32, in _check_env_vars
		auto_configure_fail(
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/lib_cc_configure.bzl"", line 112, column 9, in auto_configure_fail
		fail(""\n%sAuto-Configuration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: 
Auto-Configuration Error: Setting up VC environment variables failed, WINDOWSSDKDIR is not set by the following command:
    ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build\VCVARSALL.BAT"" amd64  -vcvars_ver=14.29.30133
ERROR: Error fetching repository: Traceback (most recent call last):
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/cc_configure.bzl"", line 120, column 36, in cc_autoconf_impl
		configure_windows_toolchain(repository_ctx)
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl"", line 694, column 31, in configure_windows_toolchain
		msvc_vars = _get_msvc_vars(repository_ctx, paths)
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl"", line 535, column 28, in _get_msvc_vars
		env = setup_vc_env_vars(repository_ctx, vc_path)
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl"", line 318, column 24, in setup_vc_env_vars
		_check_env_vars(env_map, cmd, expected = envvars)
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl"", line 324, column 32, in _check_env_vars
		auto_configure_fail(
	File ""D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/rules_cc/cc/private/toolchain/lib_cc_configure.bzl"", line 112, column 9, in auto_configure_fail
		fail(""\n%sAuto-Configuration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: 
Auto-Configuration Error: Setting up VC environment variables failed, WINDOWSSDKDIR is not set by the following command:
    ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build\VCVARSALL.BAT"" amd64  -vcvars_ver=14.29.30133
INFO: Repository rules_proto instantiated at:
  D:/vcpkg/buildtrees/tensorflow-cc/x64-windows-dbg/WORKSPACE:23:14: in <toplevel>
  D:/vcpkg/buildtrees/tensorflow-cc/x64-windows-dbg/tensorflow/workspace0.bzl:120:20: in workspace
  D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps
  D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/com_google_protobuf/protobuf_deps.bzl:42:21: in protobuf_deps
Repository rule http_archive defined at:
  D:/vcpkg/buildtrees/tensorflow-cc/.bzl/hwfhzki5/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
ERROR: D:/vcpkg/buildtrees/tensorflow-cc/x64-windows-dbg/tensorflow/cc/BUILD:125:11: //tensorflow/cc:gradient_checker depends on @local_config_cc//:cc-compiler-x64_windows in repository @local_config_cc which failed to fetch. no such package '@local_config_cc//': 
Auto-Configuration Error: Setting up VC environment variables failed, WINDOWSSDKDIR is not set by the following command:
    ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build\VCVARSALL.BAT"" amd64  -vcvars_ver=14.29.30133
ERROR: Analysis of target '//tensorflow:install_headers' failed; build aborted: Analysis failed
INFO: Elapsed time: 120.017s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (60 packages loaded, 167 targets configured)
FAILED: Build did NOT complete successfully (60 packages loaded, 167 targets configured)
"
54392,Some tests misusing assertTrue for comparisons,"`assertTrue` is not for comparing arguments, should use `assertEqual` for that.

The developer's intent of the test was to compare argument 1 with argument 2, which is not happening. Really what is happening is the test is passing because first argument is truthy. The correct method to use is assertEqual. [more details](https://codereview.doctor/features/python/best-practice/avoid-misusing-unittest-assert-true)

https://github.com/tensorflow/tensorflow/blob/d9e5d3c634c2ab1221e7d30b562c48b0b0e110b7/tensorflow/python/autograph/pyct/cfg_test.py#L85
https://github.com/tensorflow/tensorflow/blob/d9e5d3c634c2ab1221e7d30b562c48b0b0e110b7/tensorflow/python/distribute/distributed_variable_test.py#L383
https://github.com/tensorflow/tensorflow/blob/d9e5d3c634c2ab1221e7d30b562c48b0b0e110b7/tensorflow/python/eager/def_function_xla_jit_test.py#L1182
https://github.com/tensorflow/tensorflow/blob/d9e5d3c634c2ab1221e7d30b562c48b0b0e110b7/tensorflow/python/keras/mixed_precision/loss_scale_optimizer_test.py#L1007
https://github.com/tensorflow/tensorflow/blob/d9e5d3c634c2ab1221e7d30b562c48b0b0e110b7/tensorflow/python/ops/parallel_for/control_flow_ops_test.py#L2549

I found this issue automatically, see other issues [here](https://codereview.doctor/tensorflow/tensorflow)"
54391,Consider supporting png images for modelmaker object detector,"**System information**
- TensorFlow version (you are using): 2.7.0



**Describe the feature and the current behavior/state.**
Currently the `object_detector` class only supports jpg images. This can be a problem, especially if you have already created label annotations for the images in another image format. I suggest that we consider adding support for png files as well.
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
Everyone using png images instead of jpg for training data.
**Any Other info.**
As far as I can see then there is a check for file format here on line 133 of the `dict_to_tf_example` function: https://github.com/tensorflow/examples/blob/035fb73d5ff8b74958c9e3f83f44fc20dfc39119/tensorflow_examples/lite/model_maker/third_party/efficientdet/dataset/create_pascal_tfrecord.py#L133"
54389,BlazePose model support in iOS sample,"Hi Team, I was checking https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/ios example it supports
Posenet, Movenet Lightning, Movenet Thunder models. I just wanted to know, Is it possible to add googles BlazePose model  support ? If yes how can I do that ?

Thank you"
54388,Building unit tests on non-GPU build is broken,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.8.10
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 5.0.0
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

Build errors out with

`ERROR: /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/compiler/xla/service/gpu/BUILD:653:11: Compiling tensorflow/compiler/xla/service/gpu/triangular_solve_thunk.cc failed: undeclared inclusion(s) in rule '//tensorflow/compiler/xla/service/gpu:gpu_executable':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/xla/service/gpu/triangular_solve_thunk.cc':
  'tensorflow/compiler/xla/service/gpu/precompiled_kernels.h'
INFO: Elapsed time: 4632.439s, Critical Path: 957.38s
INFO: 40192 processes: 12988 internal, 27204 local.
FAILED: Build did NOT complete successfully
`

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures -- //tensorflow/... -//tensorflow/python/integration_testing/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/compiler/xrt/... -//tensorflow/core/tpu/... -//tensorflow/java/... -//tensorflow/go/... -//tensorflow/lite/... -//tensorflow/python/tools/... -//tensorflow/compiler/mlir/lite/tests:const-fold.mlir.test -//tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test -//tensorflow/python/eager:function_test -//tensorflow/python/kernel_tests/linalg:linear_operator_circulant_test -//tensorflow/python/kernel_tests/linalg:self_adjoint_eig_op_test

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The commit https://github.com/tensorflow/tensorflow/commit/3e3d3cb87d1f747928d26d832e9bbc231b201c43 is involved but that seems to have been partially addressing another problem, so not sure where the root cause is.
"
54387,ImportError: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): Fails, tested with 2.7.1 and 2.8.0
- Python version: Python 3.9.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 11.6.0-1, cudnn 8.3.1.22-1
- GPU model and memory: GTX 1050ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When `import tensorflow` in a Python 3.9.10, failed to load:

```
>>> import tensorflow
Traceback (most recent call last):
  File ""/home/david/git/project/venv/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/david/git/project/venv/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/david/git/project/venv/lib/python3.9/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/david/git/project/venv/lib/python3.9/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/david/git/project/venv/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 79, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/home/david/git/project/venv/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/david/git/project/venv/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```

**Describe the expected behavior**

```
>>> import tensorflow
```

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
>>> import tensorflow
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

When importing tensorflow in Python 3.10 it works well"
54386,tf.function with jit_compile runs on CPU when there is GPU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
pip install
- TensorFlow version (use command below):
v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: Python 3.9.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0 (ptxas cherrypicked from CUDA 11.3)
- GPU model and memory: RTX A6000

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`@tf.function(jit_compile=True)` runs on both CPU and GPU, and mostly CPU
**Describe the expected behavior**
Should mainly use GPU

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf

@tf.function(jit_compile=True)
def func(a, b):
    for i in tf.range(100):
        c = a + b
        a = b
        b = c 
    return c
concrete_fn = func.get_concrete_function(
    tf.TensorSpec(shape=[3], dtype=tf.float32, name='a'),
    tf.TensorSpec(shape=[3], dtype=tf.float32, name='b')
    )
a = tf.random.normal([3])
b = tf.random.normal([3])
tf.profiler.experimental.start('profile_results_simple')
for step in range(300):
    with tf.profiler.experimental.Trace(""Train"", step_num=step, _r=1):
        concrete_fn(a, b)
tf.profiler.experimental.stop()
```

![Screen Shot 2022-02-15 at 5 30 14 PM](https://user-images.githubusercontent.com/20623744/154033461-2cb94471-666d-4fba-87f0-aac898e2d08e.png)

The full tensorboard file can be downloaded from [here](https://drive.google.com/file/d/1IYNdjCkko4u2ktaxRe1vdjbqCMjrkgtg/view?usp=sharing).

Above is a short code snippt to reproduce the problem. In my actual project, this problem is more severe. I can see the GPU usage drops to 0% and stays there for a few seconds before rise to ~70% again and this pattern repeats for every step. This GPU drop cannot be explained by disk IO because I am working on a toy example and input is some randomly generated tensors.

Moreover, adding `with tf.device('/gpu:0'):` doesn't solve this issue.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54385,Load tflite model meet The model allocation is null/empty,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: armv7
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): tensorflow-2.8.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): arm-linux-gnueabihf-gcc
- CUDA/cuDNN version:
- GPU model and memory:


I built a libtensorflowlite_c.so with cmake
```
ARMCC_FLAGS=""-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations""
ARMCC_PREFIX=/tools/toolchain/gcc-x86_64_arm-linux-gnueabihf/bin/arm-linux-gnueabihf-
cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \
  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \
  -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
  -DCMAKE_SYSTEM_NAME=Linux \
  -DCMAKE_SYSTEM_PROCESSOR=armv7 \
  ../tensorflow/lite/c
  
cmake --build . -j  
```

however, when loading model by
```
TfLiteModel *model;
model = TfLiteModelCreateFromFile(""model.tflite);
```

i meets the following problem
```
ERROR: Mmap of '5' at offset '0' failed with error '22'.  
ERROR: The model allocation is null/empty
```

here is my model:
[model.zip](https://github.com/tensorflow/tensorflow/files/8067674/model.zip)

"
54384,does tensorflow support cuda 11.3?,"
Hi,

I noticed that tensorflow support cuda 11.2 but you do not mention if tensorflow supports cuda 11.3.

When I use cuda 11.3 with tensorflow 2.4, it shows that 
`failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error
libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
`

For the details, please refer to[ this issue ](https://github.com/open-mmlab/mmdetection3d/issues/1233#issue-1133840656)"
54383,README.md useless,"This is the most time and nerve-consuming library in the whole dev eco!!! To install it you have to spend hours and hours, just because you assume that all of us have prior knowledge that you have."
54380,Text generation isn't predicting correctly,"hello, 

I am building a text generation model in Chinese and trying to pull out the probabilities between words for another task. This is the model I follow : https://www.tensorflow.org/text/tutorials/text_generation.

```
import tensorflow as tf
from tensorflow.keras import models, layers, preprocessing, Model, callbacks, activations

inputShape = (X_converted_final.shape[1],)
input = layers.Input(shape=inputShape)
x=layers.Embedding(vocab_size2, embedding_dim, input_length=50, trainable=True, embeddings_initializer=""uniform"")(input)
x = layers.GRU(rnn_units, return_sequences=True, bias_initializer=""zeros"")(x)
output= layers.Dense(vocab_size2, activation=activations.softmax)(x)

gru_fpi_model = models.Model(input, output)
gru_fpi_model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),
              optimizer='adam', metrics=[""accuracy""])
callback = callbacks.EarlyStopping(monitor='accuracy', patience=1)
gru_fpi_model.summary()
```


So this model should be able to give me '天' as the highest possible next word if ’昨’ is provided in the sequence, since '昨天' means 'yesterday' in Chinese and we have a lot of examples in the training data. This model always takes a sequence so I tried to give the model a sequence and regard the highest probabilities of next words as the best candidate, given a previous word.  Here is what I did.


`prediction=np.argmax(loaded_model(sentence)[:,2,:], axis=-1)`

With this line, if I give the model a sentence '我昨天晚上去上学', the model should give me the word that has the highest probabilities after '昨' (in the second position, including the beginning marker of sentence), which should be '天' or other words, but it instead gives me '昨'. I couldn't find any sequence '昨昨' in the training data so there must be something wrong. For some words, it predicts well, e.g. '我', it gives me '們'and it means 'we' in English. So I think this model still works for some words, but it just doesn't work for some.

Could anyone give me some hints on this? Thank you so much"
54378,Trying to convert faster_rcnn_resnet101_coco model to a tflite model,"**System information**
Mac OS Big Sur (Version 11.6)
TF 2.2.3
Python 3.7

**Command used to run the converter or code if you’re using the Python API
If possible, please share a link to Colab/Jupyter/any notebook.**

https://colab.research.google.com/drive/1F-KqnVYFq4UEEcwKSt2hxWb161QNS668?usp=sharing

import tensorflow as tf

  converter =  tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file = 'faster_rcnn_resnet101_coco_2018_01_28/frozen_inference_graph.pb', 
    input_arrays = ['image_tensor'],
    input_shapes={'image_tensor': [1,300,300,3]},
    output_arrays = ['detection_boxes', 'detection_scores', 'detection_classes'] 
  )

  converter.use_experimental_new_converter = True
  converter.allow_custom_ops = True
  converter.target_spec.supported_types = [tf.float16]
  tflite_model = converter.convert()

  with open('custom_model.tflite', 'wb') as f:
    f.write(tflite_model)

**The output from the converter invocation**

**Model obtained from Model Zoo**
http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz

**Any other info / logs**

I'm trying to convert the faster_rcnn_resnet101_coco pre-trained model from the TF Model Zoo to TFLite. 

Is there anything wrong with my process?"
54375,tflite with CoreML delegate errors with mediapipe models,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: IPAD pro M1
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.8.0 C api
- Python version: 3.9
- Bazel version (if compiling from source): 4.1
- GCC/Compiler version (if compiling from source): apple clang
- CUDA/cuDNN version: no
- GPU model and memory: no

**Describe the current behavior**
I've built a tflite 2.8.0 with coreML delegate to run mediapipe models on ipad M1,
for the build i edit the BUILD.apple according to one of the issues here, changing the coreml dependencies to the c api build:

`# bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteC_framework

tflite_ios_framework(
    name = ""TensorFlowLiteC_framework"",
    hdrs = [
        "":builtin_ops.h"",
        "":c_api.h"",
        "":c_api_experimental.h"",
        "":common.h"",
        "":coreml_delegate.h"",
        "":xnnpack_delegate.h"",
        ""//tensorflow/lite/c:c_api_types.h"",
    ],
    allowlist_symbols_file = "":allowlist_TensorFlowLiteC.txt"",
    bundle_name = ""TensorFlowLiteC"",
    minimum_os_version = TFL_MINIMUM_OS_VERSION,
    deps = [
        "":tensorflow_lite_c"",
        ""//tensorflow/lite/delegates/coreml:coreml_delegate"",
    ],
)`

it faster than the XNNPACK delegate but i get the following errors:

`2022-02-14 18:13:53.989664+0200 Blackbox[17707:4340397] [espresso] [Espresso::ANERuntimeEngine::__forward_segment 0] evaluate[RealTime]WithModel returned 0; code=5 err=Error Domain=com.apple.appleneuralengine Code=5 ""processRequest:model:qos:qIndex:modelStringID:options:error:: 0x2: Program Inference overflow"" UserInfo={NSLocalizedDescription=processRequest:model:qos:qIndex:modelStringID:options:error:: 0x2: Program Inference overflow}

2022-02-14 18:13:53.989716+0200 Blackbox[17707:4340397] [espresso] [Espresso::overflow_error] /private/var/mobile/Containers/Data/Application/D959913C-E34A-44C9-8706-3FD4944AF1ED/tmp/B14B4C52-2C7C-4C96-A4CC-DECBF54971E5-17707-0000111529177368.mlmodelc/model.espresso.net:0`

Im using Xcode 13.1 
with XNNPACK everything is working perfect

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

tflite code:
`TfLiteModel* m_model{ nullptr };
TfLiteInterpreter* m_interpreter{ nullptr };
TfLiteDelegate* m_delegateXNNPACK{ nullptr };
TfLiteDelegate* m_delegateCoreML{ nullptr };

m_model = TfLiteModelCreateFromFile(modelFile.c_str());
if (m_model == nullptr)
	return false;

TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
TfLiteCoreMlDelegateOptions optionsCoreML;
optionsCoreML.enabled_devices = TfLiteCoreMlDelegateAllDevices;
m_delegateCoreML = TfLiteCoreMlDelegateCreate(&optionsCoreML);
TfLiteInterpreterOptionsAddDelegate(options, m_delegateCoreML);

if(!m_delegateCoreML)
{
	TfLiteXNNPackDelegateOptions optionsXNNPACK = TfLiteXNNPackDelegateOptionsDefault();
	m_delegateXNNPACK = TfLiteXNNPackDelegateCreate(&optionsXNNPACK);
	TfLiteInterpreterOptionsAddDelegate(options, m_delegateXNNPACK);
}

m_interpreter = (TfLiteInterpreter*)TfLiteInterpreterCreate(m_model, options);
if (m_interpreter == nullptr)
	return false;

// Allocate tensor buffers
if (TfLiteInterpreterAllocateTensors(m_interpreter) != kTfLiteOk)
	return false;

if (TfLiteInterpreterInvoke(m_interpreter) != kTfLiteOk)
	return false;	`

models:
https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_detection/pose_detection.tflite
https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_lite.tflite

init the model and interpreter yeilds:
`2022-02-14 19:51:21.711179+0200 Blackbox[17727:4362469] coreml_version must be 2 or 3. Setting to 3.
2022-02-14 19:51:21.722082+0200 Blackbox[17727:4362469] Initialized TensorFlow Lite runtime.
INFO: Initialized TensorFlow Lite runtime.
2022-02-14 19:51:21.727229+0200 Blackbox[17727:4362469] CoreML delegate: 37 nodes delegated out of 291 nodes, with 33 partitions.
INFO: CoreML delegate: 37 nodes delegated out of 291 nodes, with 33 partitions.
2022-02-14 19:52:12.292307+0200 Blackbox[17727:4362469] coreml_version must be 2 or 3. Setting to 3.
2022-02-14 19:52:12.293704+0200 Blackbox[17727:4362469] CoreML delegate: 112 nodes delegated out of 283 nodes, with 6 partitions.
INFO: CoreML delegate: 112 nodes delegated out of 283 nodes, with 6 partitions.`

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54372,try and except do not work with tensorflow exceptions + impossible to debug shapes issue,"You'll this [notebook][1] to reproduce the issue which downloads the files below and runs the exact same code following the description.

 - `labels.csv`: each row contains `x0`, `y0`, `x1`, `y1` text coordinates, and other columns not affecting the outcome.
 - `yolo-train-0.tfrecord`: Contains 90% of the examples found in `labels.csv`. Each example contains all labels/rows corresponding to the image in the example.

I'm experiencing a recurring error that happens when iterating over a tfrecord dataset.
After 2000-4000 iterations that successfully read batches from the dataset, I get the following error:

    iteration: 3240 2022-02-14 04:25:15.376625: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at scatter_nd_op.cc:219 : INVALID_ARGUMENT: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6]
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 800, in __next__
        return self._next_internal()
      File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 786, in _next_internal
        output_shapes=self._flat_output_shapes)
      File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2845, in iterator_get_next
        _ops.raise_from_not_ok_status(e, name)
      File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status
        raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6]
    	 [[{{function_node __inference_transform_targets_for_output_1051}}{{node TensorScatterUpdate}}]] [Op:IteratorGetNext]

It is near impossible to tell which exact inputs that are causing the issue thanks to tensorflow's brilliant graph execution. I tried using `pdb`, `tf.print` statements and many other desperate measures trying to identify which examples in `labels.csv` that cause the problem and need to be excluded, and nothing looks particularly suspicious.

Here's what the notebook runs and eventually results in the error mentioned.

    import numpy as np
    import pandas as pd
    import tensorflow as tf
    
    
    def transform_images(x, image_shape):
        x = tf.image.resize(x, image_shape)
        return x / 255
    
    
    @tf.function
    def transform_targets_for_output(y_true, grid_size, anchor_indices):
        n = tf.shape(y_true)[0]
        y_true_out = tf.zeros((n, grid_size, grid_size, tf.shape(anchor_indices)[0], 6))
        anchor_indices = tf.cast(anchor_indices, tf.int32)
        indexes = tf.TensorArray(tf.int32, 1, dynamic_size=True)
        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)
        idx = 0
        for i in tf.range(n):
            for j in tf.range(tf.shape(y_true)[1]):
                if tf.equal(y_true[i][j][2], 0):
                    continue
                anchor_eq = tf.equal(anchor_indices, tf.cast(y_true[i][j][5], tf.int32))
                if tf.reduce_any(anchor_eq):
                    box = y_true[i][j][0:4]
                    box_xy = (y_true[i][j][0:2] + y_true[i][j][2:4]) / 2
                    anchor_idx = tf.cast(tf.where(anchor_eq), tf.int32)
                    grid_xy = tf.cast(box_xy // (1 / grid_size), tf.int32)
                    indexes = indexes.write(
                        idx, [i, grid_xy[1], grid_xy[0], anchor_idx[0][0]]
                    )
                    updates = updates.write(
                        idx, [box[0], box[1], box[2], box[3], 1, y_true[i][j][4]]
                    )
                    idx += 1
        return tf.tensor_scatter_nd_update(y_true_out, indexes.stack(), updates.stack())
    
    
    def transform_targets(y, anchors, anchor_masks, size):
        y_outs = []
        grid_size = size // 32
        anchors = tf.cast(anchors, tf.float32)
        anchor_area = anchors[..., 0] * anchors[..., 1]
        box_wh = y[..., 2:4] - y[..., 0:2]
        box_wh = tf.tile(tf.expand_dims(box_wh, -2), (1, 1, tf.shape(anchors)[0], 1))
        box_area = box_wh[..., 0] * box_wh[..., 1]
        intersection = tf.minimum(box_wh[..., 0], anchors[..., 0]) * tf.minimum(
            box_wh[..., 1], anchors[..., 1]
        )
        iou = intersection / (box_area + anchor_area - intersection)
        anchor_idx = tf.cast(tf.argmax(iou, axis=-1), tf.float32)
        anchor_idx = tf.expand_dims(anchor_idx, axis=-1)
        y = tf.concat([y, anchor_idx], axis=-1)
        for anchor_indices in anchor_masks:
            y_outs.append(transform_targets_for_output(y, grid_size, anchor_indices))
            grid_size *= 2
        return tuple(y_outs)
    
    
    def read_example(
        example,
        feature_map,
        class_table,
        max_boxes,
        image_shape,
    ):
        features = tf.io.parse_single_example(example, feature_map)
        image = tf.image.decode_png(features['image'], channels=3)
        image = tf.image.resize(image, image_shape)
        object_name = tf.sparse.to_dense(features['object_name'])
        label = tf.cast(class_table.lookup(object_name), tf.float32)
        label = tf.stack(
            [tf.sparse.to_dense(features[feature]) for feature in ['x0', 'y0', 'x1', 'y1']]
            + [label],
            1,
        )
        padding = [[0, max_boxes - tf.shape(label)[0]], [0, 0]]
        label = tf.pad(label, padding)
        return image, label
    
    
    def read_tfrecord(
        fp,
        classes_file,
        image_shape,
        max_boxes,
        shuffle_buffer_size,
        batch_size,
        anchors,
        masks,
        classes_delimiter='\n',
    ):
        text_initializer = tf.lookup.TextFileInitializer(
            classes_file, tf.string, 0, tf.int64, -1, delimiter=classes_delimiter
        )
        class_table = tf.lookup.StaticHashTable(text_initializer, -1)
        files = tf.data.Dataset.list_files(fp)
        dataset = files.flat_map(tf.data.TFRecordDataset)
        feature_map = {
            'image': tf.io.FixedLenFeature([], tf.string),
            'x0': tf.io.VarLenFeature(tf.float32),
            'y0': tf.io.VarLenFeature(tf.float32),
            'x1': tf.io.VarLenFeature(tf.float32),
            'y1': tf.io.VarLenFeature(tf.float32),
            'object_name': tf.io.VarLenFeature(tf.string),
            'object_index': tf.io.VarLenFeature(tf.int64),
        }
        return (
            dataset.map(
                lambda x: read_example(x, feature_map, class_table, max_boxes, image_shape),
                tf.data.experimental.AUTOTUNE,
            )
            .batch(batch_size)
            .shuffle(shuffle_buffer_size)
            .map(
                lambda x, y: (
                    transform_images(x, image_shape),
                    transform_targets(y, anchors, masks, image_shape[0]),
                )
            )
            .prefetch(tf.data.experimental.AUTOTUNE)
        )


    if __name__ == '__main__':
        input_shape = (608, 608, 3)
        labels = pd.read_csv('labels.csv')
        classes_file = 'classes.txt'
        max_boxes = max([g[1].shape[0] for g in labels.groupby('image')])
        shuffle_buffer_size = 256
        batch_size = 8
        anchors = np.array(
                [
                    (10, 13),
                    (16, 30),
                    (33, 23),
                    (30, 61),
                    (62, 45),
                    (59, 119),
                    (116, 90),
                    (156, 198),
                    (373, 326),
                ]
            ) / np.array(input_shape[:-1])
        masks = np.array([[6, 7, 8], [3, 4, 5], [0, 1, 2]])
        train_dataset = read_tfrecord(
                    '/content/yolo-train-0.tfrecord',
                    classes_file,
                    input_shape[:-1],
                    max_boxes,
                    shuffle_buffer_size,
                    batch_size,
                    anchors,
                    masks,
                )
        for i, _ in enumerate(train_dataset, 1):  # There should be around 11000 iterations
            print(f'\riteration: {i}', end='')

Is there a way to filter out the problematic examples? How am i exactly supposed to figure out what's wrong?

I tried the following using try and except blocks and it doesn't work and gives the same exception

    dataset = iter(dataset)
    while True:
        try: 
            yield next(dataset)
        except InvalidArgumentError:
            pass


  [1]: https://colab.research.google.com/drive/1PsPyJNCwOen5RVI3wQ0g2VrHLrrytaE3?usp=sharing"
54371,AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike',"**System information**

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 professional 19042.1466
TensorFlow installed from (source or binary):pip install
TensorFlow version (use command below):v2.7.0-rc1-69-gc256c071bb2 , 2.7.0
Python version:3.7
CUDA/cuDNN version: CUDA:11.4.0_471.11,cuDNN:8.3.2.44
GPU model and memory:GTX 970,4G memory

**Describe the current behavior**
I have a problem, when running the code, it prompts:
```
  File ""D:/Tony/Documents/yunpan/invest/2022/quant/gym/study PG/pg.py"", line 79, in __init__
    self.model = tl.models.Model(inputs=input_layer, outputs=all_act)
  File ""D:\Anaconda3\envs\gym_env\lib\site-packages\tensorlayer\models\core.py"", line 213, in __init__
    if isinstance(check_argu, tf_ops._TensorLike) or tf_ops.is_dense_tensor_like(check_argu):
AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'
```

**Standalone code to reproduce the issue**
[tutorial_PG.py](https://github.com/tensorlayer/tensorlayer/blob/master/examples/reinforcement_learning/tutorial_PG.py)"
54370,`MirroredVariable`s not recorded on the gradient tape,"I am unsure whether this is the right place to post this, so feel free to remove if this is the case.

I’m currently trying to understand the internal functioning of the `MirroredStrategy` and the recording of gradients of `MirroredVariable`s in particular and doing so running into an issue where the gradient tape does not record any mirrored variables at all.

I understand the concept of the `MirroredVariable` but it’s unclear to me how a correct gradient tape is recorded over these variables in `_call_for_each_replica` in `mirrored_strategy`. As this implementation seems mostly covered by `mirrored_run` I tried to mainly focus on this file instead. Say we have 1 `MirroredVariable` with the following signature:

```
MirroredVariable {
  0: <tf.Variable 'w:0' shape=() dtype=float32>,
  1: <tf.Variable 'w/replica_1:0' shape=() dtype=float32>
}
```

I’ve tried to understand this behavior by altering the `_call_for_each_replica` implementation so it runs every function sequentially on the defined device (just removing the replica threads). This works for variable creation, computation and reduction, but breaks when recording gradients. Say I have the following function:

```
@def_function.function
def step(x):
    with backprop.GradientTape() as tape:
        loss = w * x

    optimizer.minimize(loss, var_list=[w], tape=tape)

strategy.run(step, args=(2.0,))
```

This yields:
```
No gradients provided for any variable: ['w:0']
```

Adding `tape.watch(w)` doesn’t change anything and my guess is that it’s due to the function wrapping happening in `call_for_each_replica` in `mirrored_run`. I would have expected this to record on a per gradient basis. Could anyone shine some light on how these gradients are recorded here?"
54369,Cannot install tensorflow_text in virtual environment of python 3.8 mac m1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac M1
- TensorFlow version: 2.7.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: virtualenv (see https://developer.apple.com/metal/tensorflow-plugin/)




**Describe the problem**
To use the [`xx_use_md` model](https://pypi.org/project/spacy-universal-sentence-encoder/0.2.1/), I started a virtualenv. 

I tried to load the model by 

```
import spacy_universal_sentence_encoder
nlp = spacy_universal_sentence_encoder.load_model('xx_use_md')
```

Then, I encountered the error:

```
FileNotFoundError: Op type not registered 'SentencepieceOp' in binary running on MacBook-Air. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
 You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.
```

Guided by the discussion here: https://github.com/tensorflow/tensorflow/issues/38597, I tried to install `tensorflow_text` in the virtualenv via pip3, but this raised another error:

```
ERROR: Could not find a version that satisfies the requirement tensorflow_text (from versions: none)
ERROR: No matching distribution found for tensorflow_text
```

Additionally, I tried to load the model by

```
import spacy
nlp = spacy.load('xx_use_md')
```

which raised the error

```
OSError: [E050] Can't find model 'xx_use_md'. It doesn't seem to be a Python package or a valid path to a data directory.
```

Then, I used the command `pip3 install https://github.com/MartinoMensio/spacy-universal-sentence-encoder-tfhub/releases/download/xx_use_md-0.2.1/xx_use_md-0.2.1.tar.gz#xx_use_md-0.2.1` on https://pypi.org/project/spacy-universal-sentence-encoder/0.2.1/ . Another error happened:

```
ERROR: Could not find a version that satisfies the requirement tensorflow==2.1.0 (from spacy-universal-sentence-encoder) (from versions: none)
ERROR: No matching distribution found for tensorflow==2.1.0
```

It is impossible to use the 'xx_use_md' model by `import spacy` or `import spacy_universal_sentence_encoder` in the virtualenv. However, when I tried with the same code on Colab, similar errors occurred and were resolved by installing and importing `tensowflow_text`."
54367,"InvalidArgumentError: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6]","You'll this [notebook][1] to reproduce the issue which downloads the files below and runs the exact same code following the description.

 - `labels.csv`: each row contains `x0`, `y0`, `x1`, `y1` text coordinates, and other columns not affecting the outcome.
 - `yolo-train-0.tfrecord`: Contains 90% of the examples found in `labels.csv`. Each example contains all labels/rows corresponding to the image in the example.

I'm experiencing a recurring error that happens when iterating over a tfrecord dataset.
After 2000-4000 iterations that successfully read batches from the dataset, I get the following error:

    iteration: 3240 2022-02-14 04:25:15.376625: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at scatter_nd_op.cc:219 : INVALID_ARGUMENT: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6]
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 800, in __next__
        return self._next_internal()
      File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 786, in _next_internal
        output_shapes=self._flat_output_shapes)
      File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2845, in iterator_get_next
        _ops.raise_from_not_ok_status(e, name)
      File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status
        raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6]
    	 [[{{function_node __inference_transform_targets_for_output_1051}}{{node TensorScatterUpdate}}]] [Op:IteratorGetNext]

It is near impossible to tell which exact inputs that are causing the issue thanks to tensorflow's brilliant graph execution. I tried using `pdb`, `tf.print` statements and many other desperate measures trying to identify which examples in `labels.csv` that cause the problem and need to be excluded, and nothing looks particularly suspicious.

Here's what the notebook runs and eventually results in the error mentioned.

    import numpy as np
    import pandas as pd
    import tensorflow as tf
    
    
    def transform_images(x, image_shape):
        x = tf.image.resize(x, image_shape)
        return x / 255
    
    
    @tf.function
    def transform_targets_for_output(y_true, grid_size, anchor_indices):
        n = tf.shape(y_true)[0]
        y_true_out = tf.zeros((n, grid_size, grid_size, tf.shape(anchor_indices)[0], 6))
        anchor_indices = tf.cast(anchor_indices, tf.int32)
        indexes = tf.TensorArray(tf.int32, 1, dynamic_size=True)
        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)
        idx = 0
        for i in tf.range(n):
            for j in tf.range(tf.shape(y_true)[1]):
                if tf.equal(y_true[i][j][2], 0):
                    continue
                anchor_eq = tf.equal(anchor_indices, tf.cast(y_true[i][j][5], tf.int32))
                if tf.reduce_any(anchor_eq):
                    box = y_true[i][j][0:4]
                    box_xy = (y_true[i][j][0:2] + y_true[i][j][2:4]) / 2
                    anchor_idx = tf.cast(tf.where(anchor_eq), tf.int32)
                    grid_xy = tf.cast(box_xy // (1 / grid_size), tf.int32)
                    indexes = indexes.write(
                        idx, [i, grid_xy[1], grid_xy[0], anchor_idx[0][0]]
                    )
                    updates = updates.write(
                        idx, [box[0], box[1], box[2], box[3], 1, y_true[i][j][4]]
                    )
                    idx += 1
        return tf.tensor_scatter_nd_update(y_true_out, indexes.stack(), updates.stack())
    
    
    def transform_targets(y, anchors, anchor_masks, size):
        y_outs = []
        grid_size = size // 32
        anchors = tf.cast(anchors, tf.float32)
        anchor_area = anchors[..., 0] * anchors[..., 1]
        box_wh = y[..., 2:4] - y[..., 0:2]
        box_wh = tf.tile(tf.expand_dims(box_wh, -2), (1, 1, tf.shape(anchors)[0], 1))
        box_area = box_wh[..., 0] * box_wh[..., 1]
        intersection = tf.minimum(box_wh[..., 0], anchors[..., 0]) * tf.minimum(
            box_wh[..., 1], anchors[..., 1]
        )
        iou = intersection / (box_area + anchor_area - intersection)
        anchor_idx = tf.cast(tf.argmax(iou, axis=-1), tf.float32)
        anchor_idx = tf.expand_dims(anchor_idx, axis=-1)
        y = tf.concat([y, anchor_idx], axis=-1)
        for anchor_indices in anchor_masks:
            y_outs.append(transform_targets_for_output(y, grid_size, anchor_indices))
            grid_size *= 2
        return tuple(y_outs)
    
    
    def read_example(
        example,
        feature_map,
        class_table,
        max_boxes,
        image_shape,
    ):
        features = tf.io.parse_single_example(example, feature_map)
        image = tf.image.decode_png(features['image'], channels=3)
        image = tf.image.resize(image, image_shape)
        object_name = tf.sparse.to_dense(features['object_name'])
        label = tf.cast(class_table.lookup(object_name), tf.float32)
        label = tf.stack(
            [tf.sparse.to_dense(features[feature]) for feature in ['x0', 'y0', 'x1', 'y1']]
            + [label],
            1,
        )
        padding = [[0, max_boxes - tf.shape(label)[0]], [0, 0]]
        label = tf.pad(label, padding)
        return image, label
    
    
    def read_tfrecord(
        fp,
        classes_file,
        image_shape,
        max_boxes,
        shuffle_buffer_size,
        batch_size,
        anchors,
        masks,
        classes_delimiter='\n',
    ):
        text_initializer = tf.lookup.TextFileInitializer(
            classes_file, tf.string, 0, tf.int64, -1, delimiter=classes_delimiter
        )
        class_table = tf.lookup.StaticHashTable(text_initializer, -1)
        files = tf.data.Dataset.list_files(fp)
        dataset = files.flat_map(tf.data.TFRecordDataset)
        feature_map = {
            'image': tf.io.FixedLenFeature([], tf.string),
            'x0': tf.io.VarLenFeature(tf.float32),
            'y0': tf.io.VarLenFeature(tf.float32),
            'x1': tf.io.VarLenFeature(tf.float32),
            'y1': tf.io.VarLenFeature(tf.float32),
            'object_name': tf.io.VarLenFeature(tf.string),
            'object_index': tf.io.VarLenFeature(tf.int64),
        }
        return (
            dataset.map(
                lambda x: read_example(x, feature_map, class_table, max_boxes, image_shape),
                tf.data.experimental.AUTOTUNE,
            )
            .batch(batch_size)
            .shuffle(shuffle_buffer_size)
            .map(
                lambda x, y: (
                    transform_images(x, image_shape),
                    transform_targets(y, anchors, masks, image_shape[0]),
                )
            )
            .prefetch(tf.data.experimental.AUTOTUNE)
        )


    if __name__ == '__main__':
        input_shape = (608, 608, 3)
        labels = pd.read_csv('labels.csv')
        classes_file = 'classes.txt'
        max_boxes = max([g[1].shape[0] for g in labels.groupby('image')])
        shuffle_buffer_size = 256
        batch_size = 8
        anchors = np.array(
                [
                    (10, 13),
                    (16, 30),
                    (33, 23),
                    (30, 61),
                    (62, 45),
                    (59, 119),
                    (116, 90),
                    (156, 198),
                    (373, 326),
                ]
            ) / np.array(input_shape[:-1])
        masks = np.array([[6, 7, 8], [3, 4, 5], [0, 1, 2]])
        train_dataset = read_tfrecord(
                    '/content/yolo-train-0.tfrecord',
                    classes_file,
                    input_shape[:-1],
                    max_boxes,
                    shuffle_buffer_size,
                    batch_size,
                    anchors,
                    masks,
                )
        for i, _ in enumerate(train_dataset, 1):  # There should be around 11000 iterations
            print(f'\riteration: {i}', end='')

Is there a way to filter out the problematic examples?


  [1]: https://colab.research.google.com/drive/1PsPyJNCwOen5RVI3wQ0g2VrHLrrytaE3?usp=sharing"
54364,"convert .meta,and chkpoint to .pb gives error ""maximum protobuf size of 2GB""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.15
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**

I am converting .meta and chkpoint file to protobuf type, but getting error-


Traceback (most recent call last):
  File ""converttofrozen.py"", line 28, in <module>
    pickle.dump(frozen_graph, frozen_graph_file)
  File ""/home/marlin/anaconda/envs/hific-compress1/lib/python3.7/site-packages/google/protobuf/message.py"", line 408, in __reduce__
    return type(self), (), self.__getstate__()
  File ""/home/marlin/anaconda/envs/hific-compress1/lib/python3.7/site-packages/google/protobuf/message.py"", line 393, in __getstate__
    return dict(serialized=self.SerializePartialToString())
ValueError: Message tensorflow.GraphDef exceeds maximum protobuf size of 2GB: 2237555411


I followed the suggestion [here ](https://stackoverflow.com/questions/45864363/tensorflow-how-to-convert-meta-data-and-index-model-files-into-one-graph-pb/45868106#45868106) for conversion.

The chkpt file size is bigger than 2 GB.
![image](https://user-images.githubusercontent.com/20053548/153773801-826d6bd9-d0d5-48ed-906d-829e2e5d013f.png)


Is there a way to get rid of the hard limit of 2GB in conversion?

"
54363,graph - AttributeError: can't set attribute,"OS: windows
tenosrflow  '2.8.0' (usingas  import tensorflow._api.v2.compat.v1 as tf)

Hi 
I am not able to do update operations for tensorflow graph nodes (graph -> node -> node.node_def) [https://github.com/davidsandberg/facenet/issues/161](https://github.com/davidsandberg/facenet/issues/161) I have explored few repos, none of them seems to be working.

code 

```
model_path = '../deepmind-research/meshgraphnets/chk/flag_simple/chk/model.ckpt-29872'
sess = tf.Session()

saver = tf.train.import_meta_graph(model_path + "".meta"")
saver.restore(sess, model_path)


init_op =  tf.global_variables_initializer()    
with tf.Session() as session:
    session.run(init_op)
    gd = sess.graph.as_graph_def()
    output_graph_def = graph_util.convert_variables_to_constants(sess, gd, output)
    graph_io.write_graph(graphdef_frozen, save_pb_dir, 'frozen_model1.pb', as_text=save_pb_as_text)
        


node = graph.get_operations()
print(node.node_def)
```
name: ""Model/output_normalizer/Variable/Assign""
op: ""AssignVariableOp""
input: ""Model/output_normalizer/Variable""
input: ""Model/output_normalizer/Variable/Initializer/initial_value""
attr {
  key: ""dtype""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""validate_shape""
  value {
    b: false
  }
}

```
node.node_def.op = 'VariableOp'
(runs without error but, not reflecting in node_def)
print(node.node_def)
```
name: ""Model/output_normalizer/Variable/Assign""
op: ""AssignVariableOp""
input: ""Model/output_normalizer/Variable""
input: ""Model/output_normalizer/Variable/Initializer/initial_value""
attr {
  key: ""dtype""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""validate_shape""
  value {
    b: false
  }
}
```
a = node.node_def
a.op = node.node_def.op[6:]
node.node_def = a

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Input In [111], in <module>
      1 a = node.node_def
      2 a.op = node.node_def.op[6:]
----> 3 node.node_def = a

AttributeError: can't set attribute

```

Would you please show me some relevant resources to explore in this problem, I am exhausted from searching :-| 


Here I added link to frozen inference graph [https://github.com/Nagakiran1/data/blob/main/frozen_model.pb](url)
"
54362,GPU keeps deactivating while running model.fit,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 2.7
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda: 11.2, cudnn: 8.1
- GPU model and memory: NVidia 2.50Ti 4Gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
GPU keeps disappearing/disabling after using tensorflow. I've used the following steps to create an environment.
- ```conda create -n tf python==3.8```
- ```conda activate tf```
- ```conda install cudatoolkit=11.2 cudnn=8.1 -c=conda-forge```
- ```pip install --upgrade tensorflow-gpu==2.7.0```

If I run 
``` 
import tensorflow as tf
tf.test.is_gpu_available() 
```
in the interactive python, I can see my GPU with its model.
```
2022-02-13 10:30:58.539032: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-02-13 10:31:14.682633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 2776 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1
True
```
But when I try to run the ```mode.fit()``` code in jupyter notebook, exactly when its supposed to use the GPU, GPU somehow disables itself and I can't view it in the interactive python terminal either and I have to restart my computer to view it again. And it keeps repeating no matter how many environments I recreate.

```
2022-02-13 10:26:52.177899: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-02-13 10:26:52.556208: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2022-02-13 10:26:52.575207: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pecmun
2022-02-13 10:26:52.575920: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pecmun
False
```

**Describe the expected behavior**
It should work
**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54361,[BUG] Random initializer produces different values inside/outside a gradient tape.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary):  Binary
- TensorFlow version (use command below): 2.8
- Python version: 3.8

**Describe the current behavior**

As the script below shows, no matter API version of random intializer is v1 or v2, values produced are different between inside a gradient tape or not. 

```python
import numpy as np
import tensorflow as tf

w1_init = tf.compat.v1.random_normal_initializer(mean=0., stddev=1., seed=3)
w1_val = w1_init(shape=[4,5], dtype=tf.float64)

w2_init = tf.initializers.RandomNormal(mean=0., stddev=1., seed=3)
w2_val = w2_init(shape=[4,5], dtype=tf.float64)

# assert np.allclose(w1_val, w2_val), 'W1 == W2?'

with tf.GradientTape() as tape:
    w3_init = tf.compat.v1.random_normal_initializer(mean=0., stddev=1., seed=3)
    w3_val = w3_init(shape=[4,5], dtype=tf.float64)
    w3_plus_one = w3_val + 1.

# assert np.allclose(w1_val, w3_val), 'W1 == W3?'

with tf.GradientTape() as tape:
    w4_init = tf.initializers.RandomNormal(mean=0., stddev=1., seed=3)
    w4_val = w4_init(shape=[4,5], dtype=tf.float64)
    w4_plus_one = w4_val + 1.

# assert np.allclose(w2_val, w4_val), 'W2 == W4?'
```

**Describe the expected behavior**

We want `w1_val == w3_val` and `w2_val == w4_val`, then  reproducity can be ensured.

Due to API version change, `w1_val != w2_val` can be accepted.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no"
54360,Document for tensorflow>=2.8 in wsl2 ?,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/releases

## Description of issue (what needs changing):

### Clear description
Is there any document for ```get start in wsl2``` since I notice the section
```TensorFlow has been validated on Windows Subsystem for Linux 2 (aka WSL 2) for both GPUs and CPUs.``` ?

"
54355,How to select GPU device via NNAPI？,"How to choose gpu1 and gpu2 device if using embedded platform and two gpus deployed via tflite's nnapi？
e.g. Dual GPU for embedded platform NXP i.MX8 QuadMax"
54353,[v2.8.0]: Cross compiling libtensorflow.so for ARM64 fails,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.8.0
- Python version: 3.8.8
- Installed using virtualenv? pip? conda?: Building from source, cross-compiling for Raspberry Pi
- Bazel version (if compiling from source): Automatically done by build script via docker container
- GCC/Compiler version (if compiling from source): Automatically selected by build script via docker container
- CUDA/cuDNN version: None
- GPU model and memory: None



**Describe the problem**
I am trying to cross-compile `libtensorflow.so` to use on ARM64, but the build is failing at several steps. More info below

**Provide the exact sequence of commands / steps that you executed before running into the problem**
After downloading code and checking out `v2.8.0`, I ran following command:
```
tensorflow/tools/ci_build/ci_build.sh PI \
    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
```
This command fails due to python3.6 not being available in the docker container. To fix this issue, I installed python3.6 via following entries in the `Dockerfile` and a few other modifications as shown below. Without these modification the build would fail.
```
└─ $ ▶ git diff
diff --git a/tensorflow/tools/ci_build/Dockerfile.pi b/tensorflow/tools/ci_build/Dockerfile.pi
index 53589774291..8a021f3434e 100644
--- a/tensorflow/tools/ci_build/Dockerfile.pi
+++ b/tensorflow/tools/ci_build/Dockerfile.pi
@@ -2,6 +2,20 @@ FROM ubuntu:14.04
 
 LABEL maintainer=""Jan Prach <jendap@google.com>""
 
+# install python 3.6
+RUN apt-get update
+RUN apt-get install build-essential libpq-dev libssl-dev openssl libffi-dev zlib1g-dev -y
+RUN apt-get install python3-pip python3-dev -y
+RUN apt install software-properties-common -y
+RUN apt-get install wget -y
+RUN wget https://www.python.org/ftp/python/3.6.3/Python-3.6.3.tgz
+RUN tar -xvf Python-3.6.3.tgz
+WORKDIR /Python-3.6.3
+RUN ./configure --enable-optimizations
+RUN make -j8
+RUN make install
+WORKDIR /
+
 # Copy and run the install scripts.
 COPY install/*.sh /install/
 RUN /install/install_bootstrap_deb_packages.sh
diff --git a/tensorflow/tools/ci_build/install/install_auditwheel.sh b/tensorflow/tools/ci_build/install/install_auditwheel.sh
index c84bdf4e2ec..55e9f297f06 100755
--- a/tensorflow/tools/ci_build/install/install_auditwheel.sh
+++ b/tensorflow/tools/ci_build/install/install_auditwheel.sh
@@ -27,7 +27,7 @@ patchelf_location=$(which patchelf)
 if [[ -z ""$patchelf_location"" ]]; then
   set -e
   # Install patchelf from source (it does not come with trusty package)
-  wget https://nixos.org/releases/patchelf/patchelf-0.9/patchelf-0.9.tar.bz2
+  wget --no-check-certificate https://nixos.org/releases/patchelf/patchelf-0.9/patchelf-0.9.tar.bz2
   tar xfa patchelf-0.9.tar.bz2
   cd patchelf-0.9
   ./configure --prefix=/usr/local
diff --git a/tensorflow/tools/ci_build/install/install_pip_packages.sh b/tensorflow/tools/ci_build/install/install_pip_packages.sh
index 18e0bf73d3a..305a8c28760 100755
--- a/tensorflow/tools/ci_build/install/install_pip_packages.sh
+++ b/tensorflow/tools/ci_build/install/install_pip_packages.sh
@@ -17,7 +17,8 @@
 set -e
 
 # Get the latest version of pip so it recognize manylinux2010
-wget https://bootstrap.pypa.io/get-pip.py
+#wget https://bootstrap.pypa.io/get-pip.py
+wget https://bootstrap.pypa.io/pip/3.6/get-pip.py
 python3.6 get-pip.py
 rm -f get-pip.py
```

At this point the build proceeds up to a point where it starts to compile tensorflow code. However, I ran into following issues:
```
INFO: Repository aarch64_compiler instantiated at:
  /workspace/WORKSPACE:15:14: in <toplevel>
  /workspace/tensorflow/workspace2.bzl:888:21: in workspace
  /workspace/tensorflow/workspace2.bzl:208:20: in _tf_repositories
  /workspace/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /workspace/third_party/repo.bzl:81:35: in <toplevel>
ERROR: /workspace/tensorflow/core/util/BUILD:379:24: //tensorflow/core/util:version_info_gen depends on @local_config_git//:gen/spec.json in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Git Configuration Error: Traceback (most recent call last):
  File ""/home/sdeoras/Downloads/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_sdeoras/eab0d61a99b6696edb3d2aff87b585e8/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py"", line 28, in <module>
    from builtins import bytes  # pylint: disable=redefined-builtin
ImportError: No module named builtins

ERROR: /workspace/tensorflow/core/util/BUILD:379:24: //tensorflow/core/util:version_info_gen depends on @local_config_git//:gen/head in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Git Configuration Error: Traceback (most recent call last):
  File ""/home/sdeoras/Downloads/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_sdeoras/eab0d61a99b6696edb3d2aff87b585e8/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py"", line 28, in <module>
    from builtins import bytes  # pylint: disable=redefined-builtin
ImportError: No module named builtins

ERROR: /workspace/tensorflow/core/util/BUILD:379:24: //tensorflow/core/util:version_info_gen depends on @local_config_git//:gen/branch_ref in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Git Configuration Error: Traceback (most recent call last):
  File ""/home/sdeoras/Downloads/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_sdeoras/eab0d61a99b6696edb3d2aff87b585e8/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py"", line 28, in <module>
    from builtins import bytes  # pylint: disable=redefined-builtin
ImportError: No module named builtins

ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 74.009s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (375 packages loaded, 10502 targets configured)
FAILED: Build did NOT complete successfully (375 packages loaded, 10502 targets configured)
```

Any recommendations on what to do to be able to build `libtensorflow.so` for ARM64? Alternatively, if a pre-compiled library file exists what is a download location link? Also should I be building a previous version, if so, which one?


"
54349,"Error with ""tf.keras.layers.Normalization"" when using multiple GPU devices","**System information**
Tensorflow 2.7.0 (gpu)
Python 3.8
Linux ubuntu.
### Context:
I am trying to use a normalization layer as demonstrated in load model code below:
The code works normally as expected when removing ```the tf.keras.layers.Normalization``` layer and keeping the Rescaling layer. Therefore i would discard any problem related to input pipeline, tfrecord parsing, incorrect label format, etc...  ***The error occurs at the end of the epoch at validation inference.***

# Edit: 
I found out that the problem is related to running in multiple gpu devices. 
I created this [gist](https://colab.research.google.com/drive/1-6POGaRSpMhTu5gPO2XC_mtLil7eYXa6?usp=sharing) in order to test tensorflow/python versions and everything ran normally. I have even installed the exact same python version (3.8.10) to verify whether it would be the case but worked with no errors. Then, back in the original enviroment I've limited the number of gpu devices to 1 and the code ran normally.

[tfrecords](https://drive.google.com/drive/folders/1iwmkIV93KAwTh3ML0ghcy0tn8u3Ny1wH?usp=sharing) for reproducing.

### Code: 
```Python
def load_and_configure_model(optimizer, loss, metrics, path):
  model = ResNet50V2(include_top=True, weights='imagenet')
  transfer_layer = model.get_layer('avg_pool')
  resnet_submodel = Model(inputs=model.input,outputs=transfer_layer.output)
  model_config = resnet_submodel.get_config()
  
  submodel = model_config['layers']
  submodel.remove(submodel[0]) # Remove the previous input layer
  
  input_layer = keras.Input(shape=(224, 224, 3), dtype='float32',name=""input"") # Create a new input layer
  normalization = tf.keras.layers.Normalization(mean=[118.662, 119.194, 96.877], variance=[2769.232, 2633.742, 2702.492], axis=-1, dtype='float32')(input_layer)
  rescaling = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset=-1, dtype='float32')(normalization)  
  
  new_model = Model(inputs=input_layer,outputs=rescaling) # Declare pre-processing model to be merged with the ResNet model.
  new_model_cfg = new_model.get_config() 
      
  new_model_cfg['layers'].extend(submodel) # Merge two models.

  # Replace the previous input layer with the output from the preprocessing model
  # (Connect the preprocessing model to the resnet) 
  output_name = new_model_cfg['layers'][2]['name'] # Get the output layer name (rescaling).
  
  new_model_cfg['layers'][3]['inbound_nodes'] = [[[output_name, 0, 0, {}]]] # Replace last inbound node name with the preprocessing model layer name.  
  
  new_model = new_model.__class__.from_config(new_model_cfg, custom_objects={})  # change custom objects if necessary

  # Set back pre-trained weights on new model
  weights = [layer.get_weights() for layer in resnet_submodel.layers[1:]] # For each layer (after the input_layer) in the original resnet50:
  for layer, weight in zip(new_model.layers[3:], weights): # Set imagenet weights on each new model layer.
      layer.set_weights(weight)

  for layer in new_model.layers[:]:
    layer.trainable = False
  for layer in new_model.layers[:]:      
    trainable = True
    layer.trainable = trainable # Train everything. 

  transfer_layer = new_model.get_layer('avg_pool')
  #dropout = tf.keras.layers.Dropout(rate=0.3)(transfer_layer.output)
  species = Dense(1000, activation='softmax', dtype='float32',name='species')(transfer_layer.output) # Specify dtype for handling mixed precision specifications.

  model = keras.Model(
      inputs=[new_model.inputs],
      outputs=[species],
  )
  if not path == None :
    model.load_weights(path)
  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
  return model
```



obs: the same normalization/scaling pipeline works just fine and as expected when creating a model cosisting of these two layers only as exemplified below:

```Python
normalization = tf.keras.layers.Normalization(mean=[118.662, 119.194, 96.877], variance=[2769.232, 2633.742, 2702.492], axis=-1)(input_layer)

rescaling = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset=-1)(normalization)
model_2 = Model(inputs=input_layer,outputs=rescaling)

pred = model_2.predict(img)
pred = tf.cast(pred, tf.uint8)
pred = tf.squeeze(pred,axis=0)
pred = tf.io.encode_jpeg(pred)

fname = tf.constant('norm_then_scale.jpg')
fwrite = tf.io.write_file(fname, pred)


```

### Traceback:
```
Epoch 1/70
2022-02-11 17:12:35.280711: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302
2022-02-11 17:12:35.671694: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302
2022-02-11 17:12:37.630704: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
 95/100 [===========================>..] - ETA: 2s - loss: 6.9175 - categori2022-02-11 17:13:46.316950: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: required broadcastable shapes
Traceback (most recent call last):
  File ""flat_resnet50.py"", line 263, in <module>
    history = train_model(train_path, validation_path, epochs, steps_per_epoch, resnet_50V2)
  File ""flat_resnet50.py"", line 82, in train_model
    history = model.fit(x=train_dataset,
  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.
  (0) INVALID_ARGUMENT:  required broadcastable shapes
	 [[node replica_1/model_2/normalization/sub
 (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)
]]
	 [[div_no_nan/ReadVariableOp_1/_62]]
  (1) INVALID_ARGUMENT:  required broadcastable shapes
	 [[node replica_1/model_2/normalization/sub
 (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)
]]
	 [[div_no_nan/AddN/_76]]
  (2) INVALID_ARGUMENT:  required broadcastable shapes
	 [[node replica_1/model_2/normalization/sub
 (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)
]]
0 successful operations.
0 derived errors ignored. [Op:__inference_test_function_65650]

Errors may have originated from an input operation.
Input Source operations connected to node replica_1/model_2/normalization/sub:
In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	
In[1] model_2/normalization/sub/y:

Operation defined at: (most recent call last)
>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
>>>     self._bootstrap_inner()
>>> 
>>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
>>>     self.run()
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step
>>>     outputs = model.test_step(data)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step
>>>     y_pred = self(x, training=False)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call
>>>     return self._run_internal_graph(
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph
>>>     outputs = node.layer(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call
>>>     return ((inputs - self.mean) /
>>> 

Input Source operations connected to node replica_1/model_2/normalization/sub:
In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	
In[1] model_2/normalization/sub/y:

Operation defined at: (most recent call last)
>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
>>>     self._bootstrap_inner()
>>> 
>>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
>>>     self.run()
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step
>>>     outputs = model.test_step(data)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step
>>>     y_pred = self(x, training=False)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call
>>>     return self._run_internal_graph(
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph
>>>     outputs = node.layer(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call
>>>     return ((inputs - self.mean) /
>>> 

Input Source operations connected to node replica_1/model_2/normalization/sub:
In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	
In[1] model_2/normalization/sub/y:

Operation defined at: (most recent call last)
>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
>>>     self._bootstrap_inner()
>>> 
>>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
>>>     self.run()
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step
>>>     outputs = model.test_step(data)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step
>>>     y_pred = self(x, training=False)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call
>>>     return self._run_internal_graph(
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph
>>>     outputs = node.layer(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call
>>>     return ((inputs - self.mean) /
>>> 

Function call stack:
test_function -> test_function -> test_function

Error in sys.excepthook:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 72, in apport_excepthook
    from apport.fileutils import likely_packaged, get_recent_crashes
  File ""/usr/lib/python3/dist-packages/apport/__init__.py"", line 5, in <module>
    from apport.report import Report
  File ""/usr/lib/python3/dist-packages/apport/report.py"", line 32, in <module>
    import apport.fileutils
  File ""/usr/lib/python3/dist-packages/apport/fileutils.py"", line 27, in <module>
    from apport.packaging_impl import impl as packaging
  File ""/usr/lib/python3/dist-packages/apport/packaging_impl.py"", line 23, in <module>
    import apt
  File ""/usr/lib/python3/dist-packages/apt/__init__.py"", line 35, in <module>
    apt_pkg.init_config()
apt_pkg.Error: E:Syntax error /etc/apt/apt.conf.d/20auto-upgrades:6: Extra junk at end of file

Original exception was:
Traceback (most recent call last):
  File ""flat_resnet50.py"", line 263, in <module>
    history = train_model(train_path, validation_path, epochs, steps_per_epoch, resnet_50V2)
  File ""flat_resnet50.py"", line 82, in train_model
    history = model.fit(x=train_dataset,
  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.
  (0) INVALID_ARGUMENT:  required broadcastable shapes
	 [[node replica_1/model_2/normalization/sub
 (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)
]]
	 [[div_no_nan/ReadVariableOp_1/_62]]
  (1) INVALID_ARGUMENT:  required broadcastable shapes
	 [[node replica_1/model_2/normalization/sub
 (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)
]]
	 [[div_no_nan/AddN/_76]]
  (2) INVALID_ARGUMENT:  required broadcastable shapes
	 [[node replica_1/model_2/normalization/sub
 (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py:257)
]]
0 successful operations.
0 derived errors ignored. [Op:__inference_test_function_65650]

Errors may have originated from an input operation.
Input Source operations connected to node replica_1/model_2/normalization/sub:
In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	
In[1] model_2/normalization/sub/y:

Operation defined at: (most recent call last)
>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
>>>     self._bootstrap_inner()
>>> 
>>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
>>>     self.run()
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step
>>>     outputs = model.test_step(data)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step
>>>     y_pred = self(x, training=False)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call
>>>     return self._run_internal_graph(
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph
>>>     outputs = node.layer(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call
>>>     return ((inputs - self.mean) /
>>> 

Input Source operations connected to node replica_1/model_2/normalization/sub:
In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	
In[1] model_2/normalization/sub/y:

Operation defined at: (most recent call last)
>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
>>>     self._bootstrap_inner()
>>> 
>>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
>>>     self.run()
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step
>>>     outputs = model.test_step(data)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step
>>>     y_pred = self(x, training=False)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call
>>>     return self._run_internal_graph(
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph
>>>     outputs = node.layer(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call
>>>     return ((inputs - self.mean) /
>>> 

Input Source operations connected to node replica_1/model_2/normalization/sub:
In[0] cond/Identity_1 (defined at /usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1355)	
In[1] model_2/normalization/sub/y:

Operation defined at: (most recent call last)
>>>   File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
>>>     self._bootstrap_inner()
>>> 
>>>   File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
>>>     self.run()
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1349, in run_step
>>>     outputs = model.test_step(data)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1303, in test_step
>>>     y_pred = self(x, training=False)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call
>>>     return self._run_internal_graph(
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph
>>>     outputs = node.layer(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/normalization.py"", line 257, in call
>>>     return ((inputs - self.mean) /
>>> 

Function call stack:
test_function -> test_function -> test_function

100/100 [==============================] - ETA: 0s - loss: 6.9095 - categorical_accuracy: 0.0055
```"
54348,TFTRT and Ragged operations,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18 (using docker image from nvidia: [nvcr.io/nvidia/tensorflow:21.12-tf2-py3](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow)
- TensorFlow installed from (source or binary): 2.6.2, but the problem persists on older versions too (all versions in 2021). Newer versions are not yet stable for tensorrt. 
- TensorFlow version (use command below): via docker image. 
- Python version: Python 3.8.10
- CUDA/cuDNN version: CUDA 11.5.0
- GPU model and memory: Tesla T4 16 GB, but also on GTX 1080 TI. 

**Describe the current behaviour**
When trying to optimize the model with ragged operations via TFTRT, optimization fails with a following error:
```
2022-02-09 14:56:57.575556: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at segment_reduction_ops_impl.h:422 : Invalid argument: data.shape = [48,256] does not start with segment_ids.shape = [168]
Traceback (most recent call last):
  File ""optimize.py"", line 41, in <module>
    converter.build(input_fn=my_input_fn)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 1320, in build
    self._converted_func(*map(ops.convert_to_tensor, first_input))
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1707, in __call__
    return self._call_impl(args, kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py"", line 246, in _call_impl
    return super(WrappedFunction, self)._call_impl(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1725, in _call_impl
    return self._call_with_flat_signature(args, kwargs, cancellation_manager)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1774, in _call_with_flat_signature
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 591, in call
    outputs = execute.execute(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError:  data.shape = [48,256] does not start with segment_ids.shape = [168]
	 [[node StatefulPartitionedCall/ragged_net/lambda/RaggedReduceMax/RaggedReduce_1/RaggedReduce_1/UnsortedSegmentMax (defined at optimize.py:25) ]] [Op:__inference_pruned_2056]
```
 After some investigation, I have found out that the problem is happening in ``row_splits_to_segment_ids``, invoked within RaggedReduceMax( actually under the hood it is invoked in ``_ragged_segment_aggregate()``,  invoked by ``ragged_reduce_aggregate`` which is invoked by ``reduce_max`` in ``tensorflow.python.ops.ragged.ragged_math_ops``). The problem is with correctly calculating ``row_lenghts`` which in most cases are done correctly (while training and first sweeps of optimization, but in the second part it produces wrong results). The current version is: 
```python
    row_lengths = splits[1:] - splits[:-1]
```
The proposed version is:
```python
    s = splits[::-1][1:][::-1]
    row_lengths = splits[1:] - s
```
Which essentially does the very same thing, but at least it does not result in the presented crash. On the other hand, it produces a soft crash (non-breaking one but indicating that no significant optimization was run):
```
2022-01-25 13:24:56.322210: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger 3: [executionContext.cpp::enqueueInternal::328] Error Code 3: API Usage Error (Parameter check failed at: runtime/api/executionContext.cpp::enqueueInternal::328, condition: bindings[x] != nullptr
```

**Describe the expected behaviour**
I would like to get a model which is TFTRT optimized with smaller ``min_segment_size``.  My current workaround is to use a larger value for the parameter ``min_segment_size`` but this gives me a suboptimal solution in terms of performance.
  
**Standalone code to reproduce the issue and logs**
Gist with required code and logs: [https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24). 
Explanation of how to use gist:
- to train a model **without custom changes** in ``row_splits_to_segment_ids()``: use [train_round1.py](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24#file-train_round1-py)
- to train a model **with custom changes** in ``row_splits_to_segment_ids()``: use [train_round2.py](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24#file-train_round2-py)
- to optimize a trained model: use [optimize.py](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24#file-optimize-py)
- my logs for optimizing model trained with no custom changes: [log_after_optimize_for_round_1.txt](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24#file-log_after_optimize_for_round_1-txt)
- my logs for optimizing model trained with custom changes: [log_after_optmize_for_round_2.txt](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24#file-log_after_optmize_for_round_2-txt)

Due to a nightmare with configuring TensorRT in Google Colab, I provide only gists. 
"
54347,"Specify what axes are independent in `batch_jacobian`, instead of it being always the first axis only. ","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using): 2.7.0
- Are you willing to contribute it (Yes/No): Yes

Hi everyone,

**Describe the feature and the current behavior/state.**

Currently, `GradientTape`'s `batch_jacobian` method assumes that `target[i,...]` is independent of `source[j,...]` for `j != i`.
See: https://www.tensorflow.org/api_docs/python/tf/GradientTape#batch_jacobian

Also from the docs: The function is logically equivalent to `tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`.
The first dimension gets a special role, it ""contains"" the independent tensors whose Jacobian should be calculated.

However, the shape of independent tensors doesn't have to be of rank one, it may be multidimensional.
It may be nice to be able to specify where the line passes, between the independent tensors' shape and tensors whose jacobian we want to find.

This can be done today using `reshape` before and after the `batch_jacobian` call: merging the independent dimensions into the first dimension, applying `batch_jacobian`, and then reshaping back.
This is not so nice though :(

Here is an example of both a use case and the reshaping trick: https://colab.research.google.com/drive/1oXsmJl9GuQihJsxq0t3PusQ_-xBSiAyk

**Will this change the current api? How?**

Maybe the function can have an `axis` parameter to specify what dimensions to calculate the jacobian over, while the other dimensions will have the same ""special"" role the first dimension of `target` has now.

**Who will benefit with this feature?**

One use case may be calculating the batch_jacobian over the result of `meshgrid`, as I did to calculate the normals of a parametric surface (See colab notebook link above).

**Any Other info.**
"
54346,"""iterating over `tf.Tensor` is not allowed"" when training object detection model with pyinstaller","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2/8
- GPU model and memory: Nvidia GTX 1080 Ti

I am training a detection model with a Python script using the object_detection module (with TF2). The script works perfectly fine when executed with Python (python TrainSuspendedElementsDetection.py ...), apart from a load of deprecation warnings. Problems arise when I try to convert the Python script to an executable with pyinstaller (last version 4.9). When launching the executable an exception occurs at the beginning of the training loop:

Traceback (most recent call last):
  File ""TrainSuspendedElementsDetection.py"", line 256, in <module>
  File ""tensorflow\python\platform\app.py"", line 40, in run
  File ""absl\app.py"", line 303, in run
  File ""absl\app.py"", line 251, in _run_main
  File ""TrainSuspendedElementsDetection.py"", line 181, in main
  File ""object_detection\model_lib_v2.py"", line 678, in train_loop
  File ""tensorflow\python\util\traceback_utils.py"", line 153, in error_handler
  File ""tensorflow\python\util\traceback_utils.py"", line 150, in error_handler
  File ""tensorflow\python\eager\def_function.py"", line 910, in __call__
  File ""tensorflow\python\eager\def_function.py"", line 958, in _call
  File ""tensorflow\python\eager\def_function.py"", line 780, in _initialize
  File ""tensorflow\python\eager\function.py"", line 3157, in _get_concrete_function_internal_garbage_collected
  File ""tensorflow\python\eager\function.py"", line 3557, in _maybe_define_function
  File ""tensorflow\python\eager\function.py"", line 3392, in _create_graph_function
  File ""tensorflow\python\framework\func_graph.py"", line 1143, in func_graph_from_py_func
  File ""tensorflow\python\eager\def_function.py"", line 672, in wrapped_fn
  File ""tensorflow\python\framework\func_graph.py"", line 1118, in autograph_handler
  File ""tensorflow\python\autograph\impl\api.py"", line 440, in converted_call
  File ""tensorflow\python\autograph\impl\api.py"", line 490, in _fall_back_unconverted
  File ""tensorflow\python\autograph\impl\api.py"", line 464, in _call_unconverted
  File ""object_detection\model_lib_v2.py"", line 659, in _dist_train_step
  File ""tensorflow\python\framework\ops.py"", line 572, in __iter__
  File ""tensorflow\python\framework\ops.py"", line 561, in _disallow_iteration
  File ""tensorflow\python\framework\ops.py"", line 525, in _disallow_when_autograph_unavailable
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph is unavailable in this runtime. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information. 

Attached are:
- TrainSuspendedElementsDetection.py: The source code of the Python script (the problem occurs at line 181 when calling the training loop of the object_detection module)
- pipeline.config: The configuration file of the training process
- TrainSuspendedElementsDetection.spec: the spec file used by pyinstaller
- log.txt: the log file of the pyinstaller output. I saw numerous errors relative to modules not found, but it does not seem to have any impact in the execution of the progra
[pyinstaller.zip](https://github.com/tensorflow/tensorflow/files/8047968/pyinstaller.zip)
m"
54345,Halts after: I tensorflow/stream_executor/platform/default/dso_loader.cc:49]Successfully opened dynamic library libcudart.so.10.1,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- No. Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Ubuntu 20.04. OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- No. Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- Tensorflow and gpu related binaries installed using: conda install -c anaconda tensorflow-gpu. TensorFlow installed from (source or binary):
- unknown 2.4.1. TensorFlow version (use command below):
- Python 3.9.7. Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- 4 x NVIDIA GeForce RTX 3090, 24M. GPU model and memory:

- CUDA/cuDNN version:
2022-02-11 09:45:08.686783: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2022-02-11 09:45:08.686810: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2022-02-11 09:45:08.686818: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2022-02-11 09:45:08.686823: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-02-11 09:45:08.686833: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-02-11 09:45:08.686839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-02-11 09:45:08.686846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2022-02-11 09:45:08.686853: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7


You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Executing the following cmd from the conda environment:
$ python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""

This halts at:
2022-02-11 09:45:08.695095: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1

But aftter 10mins it outputs:
2022-02-11 09:55:33.285562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-11 09:55:33.285591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2 3 
2022-02-11 09:55:33.285596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N N N 
2022-02-11 09:55:33.285599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N N N 
2022-02-11 09:55:33.285601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   N N N N 
2022-02-11 09:55:33.285604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   N N N N 
...
tf.Tensor(-120.64484, shape=(), dtype=float32)
For full output, see faulty_behavior.txt.

**Describe the expected behavior**
I executed the same command in the docker:
$ docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu python -c ""import tensorflow as tf;
...
no halts and it ends with
tf.Tensor(-680.4232, shape=(), dtype=float32) 
for full output, see expected_behavior.txt.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
- install miniconda
- install python 3.9 environment using conda

- install: conda install -c anaconda tensorflow-gpu
- execute: python -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""

**Other info / logs** Include any logs or source code that would be helpful to

diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

[faulty_behavior.txt](https://github.com/tensorflow/tensorflow/files/8048225/faulty_behavior.txt)
[expected_behavior.txt](https://github.com/tensorflow/tensorflow/files/8048223/expected_behavior.txt)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/8047786/tf_env.txt)
"
54344,How to update in get_config() a dictionary variable of Custom layer?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): Tf 2.3
- Python version:3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior** Not able to save or serialize a dictionary variable (vmap) of type Tensor in the get_config method while saving the model

**Describe the expected behavior** Model successfully saved with my vmap dictionary

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

I am trying to save my model with Custom layer , using model.save(). In my custom layer i have a dictionary variable (vmap), which i need to update in the get_config(). Since i have the custom layer i am defining in the get_config() method all the variables explicitly which i want to be saved and properly reloaded whenever i load the model. I keep getting a list of errors. Not sure if its the correct way of updating the get_config(). Any help is appreciated. 
    model.save('D:\Thesis\ma_sayli-deshmukh\qnn\qnn\saved_model\my_model')
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1978, in save
    save.save_model(self, filepath, overwrite, include_optimizer, save_format,
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\saving\save.py"", line 133, in save_model
    saved_model_save.save(model, filepath, overwrite, include_optimizer,
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\saving\saved_model\save.py"", line 80, in save
    save_lib.save(model, filepath, signatures, options)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\saved_model\save.py"", line 975, in save
    _, exported_graph, object_saver, asset_info = _build_meta_graph(
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1075, in _build_meta_graph
    object_graph_proto = _serialize_object_graph(saveable_view,
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\saved_model\save.py"", line 720, in _serialize_object_graph
    _write_object_proto(obj, obj_proto, asset_file_def_index,
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\saved_model\save.py"", line 761, in _write_object_proto
    metadata=obj._tracking_metadata)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 3011, in _tracking_metadata
    return self._trackable_saved_model_saver.tracking_metadata
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\saving\saved_model\base_serialization.py"", line 54, in tracking_metadata
    return json_utils.Encoder().encode(self.python_properties)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 41, in python_properties
    return self._python_properties_internal()
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\saving\saved_model\model_serialization.py"", line 35, in _python_properties_internal
    metadata = super(ModelSavedModelSaver, self)._python_properties_internal()
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 59, in _python_properties_internal
    metadata.update(get_config(self.obj))
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 118, in get_config
    config = generic_utils.serialize_keras_object(obj)['config']
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py"", line 245, in serialize_keras_object
    config = instance.get_config()
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\keras\engine\sequential.py"", line 468, in get_config
    'layers': copy.deepcopy(layer_configs)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 205, in _deepcopy_list
    append(deepcopy(a, memo))
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 153, in deepcopy
    y = copier(memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\training\tracking\data_structures.py"", line 465, in __deepcopy__
    copied = super(ListWrapper, self).__deepcopy__(memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\site-packages\tensorflow\python\training\tracking\data_structures.py"", line 324, in __deepcopy__
    return type(self)(copy.deepcopy(self._storage, memo))
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 205, in _deepcopy_list
    append(deepcopy(a, memo))
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 270, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 270, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 270, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Users\deshm\.conda\envs\qnn\lib\copy.py"", line 161, in deepcopy
    rv = reductor(4)
TypeError: cannot pickle '_thread.RLock' object



Following is the custom layer code: 
class valuemaplayer(keras.layers.Layer):
    def __init__(self,**kwargs):
        kwargs[""dynamic""] = True
        super(valuemaplayer,self).__init__(**kwargs)

        self._vmap = {}
        self._data = []
        self._compression = False

      

    def build(self, input_shape):
        pass


    def enable_compression(self):
        value = list(self.get_values())
        vmap = self._vmap
        cnt = 0
        # FIXME right now only 2D input data is supported
        for v0 in value:
            for v1 in v0:
                for v2 in v1:
                    for v3 in v2:
                        v = tuple(v3)
                        if v not in vmap:
                            vmap[v]=cnt
                            cnt+=1
        self._compression = True


    @tf.function
    def do_mapping(self,pixel):
        if self._compression :
            pixel = tuple(pixel)
        # to do convert pixel(of channel axis) from tensor to a tuple
            enumerated_value=self._vmap.get(pixel)
            print(enumerated_value)
        # print(tf.shape(pixel))
            exit()
            return enumerated_value


    @tf.function
    def call(self, inputs, training=True):#use eager execution or decorate with @tf.function
        if self._compression:

            elems = []

            for b in inputs:
                for h in b :
                    elems.append(h)

            # TODO check if channel axis gets mapped by tf.map_fn
            #resize inputs to 2 axis, 1 for each pixel and other channel , work on each pixel
            changed_inputs = tf.map_fn(self.do_mapping, elems)
            return changed_inputs

         # else compression is disabled
         # in case we're training, we do not want to observe values
        if not training:
            self._data.append(inputs)
        return inputs

        # get values of the output of value map layer
    def get_values(self):
        for d in self._data:
            try:
                d = d.numpy()
            except AttributeError:
                continue
            yield d

    def get_config(self):
        config = super(valuemaplayer,self).get_config().copy()
        config.update({'vmap': self._vmap,
                       'data': self._data,
                       })
        return config

    @classmethod
    def from_config(cls, config):
        return cls(**config)

    def compute_output_shape(self, input_shape):
        print(""input shape of value map layer:"", input_shape)
        if self._compression:
            # TODO did I set the channel axis? and does this work?
            if input_shape[-1] == 1:
                print(""channel axis is set"")
                input_shape[-1] = 1
        return input_shape

"
54343,Tensorflow gradient tape returns exploding gradient model.trainable_variables,"Dear Tensorflow-Team,

I currently facing problems when using ```gradientTape``` facing exploding gradients. I have no problem when using tensorflows inbuild ```fit``` function. Based on the MNIST dataset my code looks the following:

```
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.metrics import sparse_categorical_crossentropy

#(x_train, labels), (x_test, y_test) = keras.datasets.mnist.load_data()
#(mnist_train, mnist_test), (x_test, y_test) = keras.datasets.cifar10.load_data()

(mnist_train, mnist_test), mnist_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)

def prepare(ds, batch_size=128):
  ds = ds.cache()
  ds = ds.batch(batch_size)
  ds = ds.prefetch(tf.data.experimental.AUTOTUNE)
  return ds

def split_tasks(ds, predicate):
  return ds.filter(predicate), ds.filter(lambda img, label: not predicate(img, label))

task_A_train, task_B_train = split_tasks(mnist_train, lambda img, label: label % 2 == 0)
task_A_train, task_B_train = prepare(task_A_train), prepare(task_B_train)
task_A_test, task_B_test = split_tasks(mnist_test, lambda img, label: label % 2 == 0)
task_A_test, task_B_test = prepare(task_A_test), prepare(task_B_test)

def evaluate(model, test_set):
    acc = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')
    for i, (imgs, labels) in enumerate(test_set):
        preds = model.predict_on_batch(imgs)
        acc.update_state(labels, preds)
    return acc.result().numpy()

multi_task_model = tf.keras.Sequential([
   tf.keras.layers.Flatten(input_shape=(28, 28, 1)),
   tf.keras.layers.Dense(128, activation='relu'),
   tf.keras.layers.Dense(10)
])

multi_task_model.compile(optimizer='adam', loss=sparse_categorical_crossentropy, metrics='accuracy')


def l2_penalty(model, theta_A):
  penalty = 0
  for i, theta_i in enumerate(model.trainable_variables):
    _penalty = tf.norm(theta_i - theta_A[i])
    penalty += _penalty
  return 0.5*penalty


def train_with_l2(model, task_A_train, task_B_train, task_A_test, task_B_test, epochs=6):
  # First we're going to fit to task A and retain a copy of parameters trained on Task A
  model.fit(task_A_train, epochs=epochs)
  theta_A = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}
 
  print(""Task A accuracy after training on Task A: {}"".format(evaluate(model, task_A_test)))
   
  # Metrics for the custom training loop
  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
  loss = tf.keras.metrics.SparseCategoricalCrossentropy('loss')
 
  for epoch in range(epochs):
    accuracy.reset_states()
    loss.reset_states()
    for batch, (imgs, labels) in enumerate(task_B_train):
      with tf.GradientTape() as tape:
        preds = model(imgs)
        # Loss is crossentropy loss with regularization term for each parameter
        total_loss = model.loss(labels, preds) + l2_penalty(model, theta_A)
      grads = tape.gradient(total_loss, model.trainable_variables)
      model.optimizer.apply_gradients(zip(grads, model.trainable_variables))
       
      accuracy.update_state(labels, preds)
      loss.update_state(labels, preds)
      print(""\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f}"".format(
          epoch+1, batch+1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end=''
         )
    print("""")
   
  print(""Task B accuracy after training trained model on Task B: {}"".format(evaluate(model, task_B_test)))
  print(""Task A accuracy after training trained model on Task B: {}"".format(evaluate(model, task_A_test)))
  

train_with_l2(multi_task_model, task_A_train, task_B_train, task_A_test, task_B_test)
```

This problem also persists when using another trained model.

Here are my system specifications:
- Windows
- Tensorflow installed from source  (version 2.7.0)
- Python version 3.9.7


"
54342,Random test failures in tflite/xnnpack due to precision error,"**System information**
- OS Platform and Distribution: WSL2 `tensorflow/tensorflow:devel-gpu`
- TensorFlow installed from source
- TensorFlow version: master

Tests `tensorflow/lite/delegates/xnnpack/...` can randomly fail due to precision error.

This issue caused my PR to be blocked from merging: https://github.com/tensorflow/tensorflow/pull/53864

The reason why it can fail is because the test data is generated randomly with seeding directly from `std::random_device`. The seeds are not logged when tests fail, so it is hard to reproduce a failure when it happens.

After the tests failed during pre-merge checks in my PR, I tried to reproduce it in my local environment inside `tensorflow/tensorflow:devel-gpu` docker. But on my machine the tests always passed.

However, I did find a way to get a consistent failure on `tensorflow/lite/delegates/xnnpack/softmax_test` (a different one from what failed in my PR). When compiled in `tensorflow/tensorflow:devel-gpu` with `copt` flags set to `-march=haswell -mtune=haswell -ffast-math` it always fails.

I think the tests should be improved to avoid random failures in the future:

- make the tests deterministic
- use gtest's `random_seed` for seeding
- log seeds when tests fail
- reconsider precision threshold"
54341,ValueError: No gradients provided for any variable:,"I am using TensorFlow 2.6.0 and Python 3.9. I am attempting to implement a Variational Autoencoder toy example using MNIST dataset with Convolutional Neural Network as encoder and decoder. You can refer to the complete Jupyter notebook [here](https://github.com/arjun-majumdar/Autoencoders_Experiments/blob/master/Testing-VAE_TF2.ipynb).

For some reason, on using GradientTape for training - in this case, computing the gradients with respect to the trainable parameters of the defined model, it keeps giving **ValueError: No gradients provided for any variable:** error message.

The exact lines of code are:

```
with tf.GradientTape() as tape:
    total_loss = compute_total_loss(
        data = X, reconstruction = X_recon,
        mu = mu, log_var = log_var,
        alpha = 1
    )
    
grads = tape.gradient(total_loss, model.trainable_weights)

type(grads), len(grads)
# (list, 16)


# No gradients are computed!
for x in grads:
    print(x)
'''
None
None
None
None
None
None
None
None
None
None
None
None
None
None
None
None
'''


optimizer.apply_gradients(zip(grads, model.trainable_weights))
""""""
ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_232/111942921.py in <module>
----> 1 optimizer.apply_gradients(zip(grads, model.trainable_weights))

~\anaconda3\envs\tf-cpu\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)
    639       RuntimeError: If called in a cross-replica context.
    640     """"""
--> 641     grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    642     var_list = [v for (_, v) in grads_and_vars]
    643 

~\anaconda3\envs\tf-cpu\lib\site-packages\tensorflow\python\keras\optimizer_v2\utils.py in filter_empty_gradients(grads_and_vars)
     73 
     74   if not filtered:
---> 75     raise ValueError(""No gradients provided for any variable: %s."" %
     76                      ([v.name for _, v in grads_and_vars],))
     77   if vars_with_empty_grads:

ValueError: No gradients provided for any variable: ['vae_1/encoder_4/conv2d_8/kernel:0', 'vae_1/encoder_4/conv2d_8/bias:0', 'vae_1/encoder_4/conv2d_9/kernel:0', 'vae_1/encoder_4/conv2d_9/bias:0', 'vae_1/decoder_3/dense_9/kernel:0', 'vae_1/decoder_3/dense_9/bias:0', 'vae_1/decoder_3/conv2d_transpose_9/kernel:0', 'vae_1/decoder_3/conv2d_transpose_9/bias:0', 'vae_1/decoder_3/conv2d_transpose_10/kernel:0', 'vae_1/decoder_3/conv2d_transpose_10/bias:0', 'vae_1/decoder_3/conv2d_transpose_11/kernel:0', 'vae_1/decoder_3/conv2d_transpose_11/bias:0', 'vae_1/dense_10/kernel:0', 'vae_1/dense_10/bias:0', 'vae_1/dense_11/kernel:0', 'vae_1/dense_11/bias:0'].
""""""

```

Is this a bug? Is it the case that tf.GradientTape() API is somehow not computing the gradients?
"
54340,Keep getting this error while training my model,"ResourceExhaustedError:  OOM when allocating tensor with shape[5,64,4890,4890] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node sequential/conv2d/Relu
 (defined at /usr/local/lib/python3.7/dist-packages/keras/backend.py:4867)
]]"
54339,RuntimeError: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.,"2022-02-11 11:05:30.792376: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""01_GenQuadProposals.py"", line 82, in <module>
    CNNModel.restoreCNNSess()
  File ""/home/dms/SupplementaryMaterials/CodeAndData/Code/QuadProposals/CNNQuadDetector.py"", line 92, in restoreCNNSess
    saver = tf.train.import_meta_graph(self.cfg.cornerdet_sess + '.meta', import_scope=""cornerdet"")
  File ""/home/dms/anaconda3/envs/SupplementaryMaterials/lib/python3.8/site-packages/tensorflow/python/training/saver.py"", line 1460, in import_meta_graph
    return _import_meta_graph_with_return_elements(meta_graph_or_file,
  File ""/home/dms/anaconda3/envs/SupplementaryMaterials/lib/python3.8/site-packages/tensorflow/python/training/saver.py"", line 1472, in _import_meta_graph_with_return_elements
    raise RuntimeError(""Exporting/importing meta graphs is not supported when ""
RuntimeError: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.

**System information**

tensorboard 2.8.0
tensorboard-data-server 0.6.1
tensorboard-plugin-wit 1.8.1
tensorflow 2.3.1
tensorflow-estimator 2.3.0
tensorflow-hub 0.12.0
tensorflow-io-gcs-filesystem 0.24.0
tensorflowjs 3.13.0
cuda 10.1
cudnn 7.6
python 3.8

has update import tensorflow. as tf  to import tensorflow.compat.v1 as tf
"
54338,RuntimeError: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>
2022-02-11 11:05:30.792376: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""01_GenQuadProposals.py"", line 82, in <module>
    CNNModel.restoreCNNSess()
  File ""/home/dms/SupplementaryMaterials/CodeAndData/Code/QuadProposals/CNNQuadDetector.py"", line 92, in restoreCNNSess
    saver = tf.train.import_meta_graph(self.cfg.cornerdet_sess + '.meta', import_scope=""cornerdet"")
  File ""/home/dms/anaconda3/envs/SupplementaryMaterials/lib/python3.8/site-packages/tensorflow/python/training/saver.py"", line 1460, in import_meta_graph
    return _import_meta_graph_with_return_elements(meta_graph_or_file,
  File ""/home/dms/anaconda3/envs/SupplementaryMaterials/lib/python3.8/site-packages/tensorflow/python/training/saver.py"", line 1472, in _import_meta_graph_with_return_elements
    raise RuntimeError(""Exporting/importing meta graphs is not supported when ""
RuntimeError: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.

**System information**
- TensorFlow version (you are using):
tensorboard                  2.8.0
tensorboard-data-server      0.6.1
tensorboard-plugin-wit       1.8.1
tensorflow                   2.3.1
tensorflow-estimator         2.3.0
tensorflow-hub               0.12.0
tensorflow-io-gcs-filesystem 0.24.0
tensorflowjs                 3.13.0
cuda                           10.1
cudnn                          7.6
python                         3.8
- Are you willing to contribute it (Yes/No):
Yes


**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
54337,TensorFlow Lite 2.8 ARM cross-compilation failed when XNNPACK=ON: unknown type name 'float16x8_t',"**System information**
* Linux Ubuntu 20.04
* TensorFlow 2.8
* CMake 3.16.3
* gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf ([here](https://www.tensorflow.org/lite/guide/build_cmake_arm#download_toolchain_2))

**Describe the problem**

I trying to cross-compile TensorFlow Lite 2.8 with XNNPACK=ON for ARM using CMake. I got error ""unknown type name 'float16x8_t'"":

```
...
[ 60%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qc8-gemm/gen/2x8c2s4-minmax-fp32-neonv8-mlal.c.o
make[2]: Entering directory '/home/pi/tflite_build'
/home/pi/tflite_build/xnnpack/src/f16-f32-vcvt/gen/vcvt-neonfp16-x16.c: In function ‘xnn_f16_f32_vcvt_ukernel__neonfp16_x16’:
cd /home/pi/tflite_build/_deps/xnnpack-build && /home/pi/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/home/pi/tflite_build/xnnpack/include -I/home/pi/tflite_build/xnnpack/src -I/home/pi/tflite_build/clog/deps/clog/include -I/home/pi/tflite_build/cpuinfo/include -I/home/pi/tflite_build/pthreadpool-source/include -I/home/pi/tflite_build/FXdiv-source/include -I/home/pi/tflite_build/FP16-source/include  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -pthread -std=gnu99  -marm  -march=armv8-a -mfpu=neon-fp-armv8  -O2  -o CMakeFiles/XNNPACK.dir/src/qc8-gemm/gen/2x8c2s4-minmax-fp32-neonv8-mlal.c.o   -c /home/pi/tflite_build/xnnpack/src/qc8-gemm/gen/2x8c2s4-minmax-fp32-neonv8-mlal.c
/home/pi/tflite_build/xnnpack/src/f16-f32-vcvt/gen/vcvt-neonfp16-x16.c:31:11: error: unknown type name ‘float16x8_t’
     const float16x8_t vh0 = vreinterpretq_f16_u16(vld1q_u16(i)); i += 8;
           ^~~~~~~~~~~
/home/pi/tflite_build/xnnpack/src/f16-f32-vcvt/gen/vcvt-neonfp16-x16.c:31:29: warning: implicit declaration of function ‘vreinterpretq_f16_u16’; did you mean ‘vreinterpretq_s16_u16’? [-Wimplicit-function-declaration]
     const float16x8_t vh0 = vreinterpretq_f16_u16(vld1q_u16(i)); i += 8;
                             ^~~~~~~~~~~~~~~~~~~~~
                             vreinterpretq_s16_u16
/home/pi/tflite_build/xnnpack/src/f16-f32-vcvt/gen/vcvt-neonfp16-x16.c:32:11: error: unknown type name ‘float16x8_t’
     const float16x8_t vh1 = vreinterpretq_f16_u16(vld1q_u16(i)); i += 8;
           ^~~~~~~~~~~
```

I able to compile TensorFlow Lite 2.7 with XNNPACK=ON for ARM using CMake.

I using build instructions provided [here](https://www.tensorflow.org/lite/guide/build_cmake_arm#build_for_armv7_neon_enabled)"
54336,tf.TensorArray as a FIFO ???,"Hello,

  [here](https://github.com/keras-team/keras/issues/16015) I was pointed to use `tf.TensorArray` instead of `tf.Variable` or `tf.queue.FIFOQueue` for making FIFO contained in custom layer. Is it an effective way? Exist any alternative here?

If it's the most effective method how can I replace `self.queue.assign(tf.concat([self.queue[timesteps:, :], inputs], axis=0))` with methods of `tf.TensorArray`?

## Code
```python
class FIFOLayer(Layer):
    def __init__(self, window_size, **kwargs):
        super(FIFOLayer, self).__init__(**kwargs)

        self.window_size = window_size
        self.count = 0

    def build(self, input_shape):
        super(FIFOLayer, self).build(input_shape)

        self.queue = self.add_weight(
            name=""queue"",
            shape=(self.window_size, input_shape[-1]),
            initializer=tf.initializers.Constant(value=np.nan),
            trainable=False,
        )

    def call(self, inputs, training):
        timesteps = tf.shape(inputs)[0]

        # check if batch_size is more than queue capacity
        if timesteps > self.window_size:
            raise ValueError()

        # 1. append new state to queue
        self.queue.assign(tf.concat([self.queue[timesteps:, :], inputs], axis=0))
        self.count += timesteps

        # 2. feed-forward
        if self.count < self.window_size:
            # generate mask
            attention_mask = tf.cast(
                tf.math.reduce_all(
                    tf.math.logical_not(tf.math.is_nan(self.queue)), axis=-1
                ),
                dtype=tf.float32,
            )
            attention_mask = tf.matmul(
                attention_mask[..., tf.newaxis],
                attention_mask[..., tf.newaxis],
                transpose_b=True,
            )
            return self.queue[tf.newaxis, ...], attention_mask
        # !!! check overflow
        elif self.count > self.window_size:
            self.count = self.window_size

        return self.queue[tf.newaxis, ...], None

    @property
    def is_full(self):
        return self.count == self.window_size

    def clear(self):
        self.count = 0
        self.queue.assign(tf.fill(self.queue.shape, np.nan))


l = FIFOLayer(window_size=10)
for i in range(6):
    x = tf.random.normal((2, 12))
    y = l(x)
    print(y)

print(l.is_full, ""\n\n"")

l.clear()

print(l(x))
print(l.is_full, ""\n\n"")
```

Thanks a lot for your time.
Have a nice day."
54335,Inference overhead when resizing input image,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): PIP Binary
- TensorFlow version (use command below): 2.6
- Python version: 3.7

**Describe the current behavior**
Inference time increases by a lot when changing input image resolution in a dynamic tensor shape model. 
This only happens if the input shape changes, consecutive runs for different images performs well as long as the input size is the same. Once it changes, even for lower resolutions (1024x768 -> 512x512), the inference time increases for the first inference and then goes back to normal for the next ones.

"
54334,Unable to Quantize Model with Custom Op Transpose,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: 
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
-   **Python version**: 3.8.2
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-  **Command to reproduce**: python bug_demo_01.py

### Describe the problem
When attempting to quantize a Saved Model that contains a Custom Transpose op, it fails as it cannot initialize that op for quantization. The source code for bug_demo_01.py is listed below. The model folder is listed here. [BisNet-512x512x3-rgb-model.zip](https://github.com/tensorflow/tensorflow/files/8044758/BisNet-512x512x3-rgb-model.zip)

### Source code / logs
```
import tensorflow as tf
import numpy as np

def representative_dataset_gen():
    for _ in range(10):
        yield [np.random.random([1,512,512,3]).astype(np.float32)]

converter = tf.lite.TFLiteConverter.from_saved_model('BisNet-512x512x3-rgb-model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.experimental_new_converter = True
converter.allow_custom_ops = True

tflite_model = converter.convert()
```

The ending error is as follows
2022-02-10 15:12:03.161809: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1786] The following operation(s) need TFLite custom op implementation(s):
Custom ops: Transpose
Details:
        tf.Transpose {device = """"}
Traceback (most recent call last):
  File ""bug_demo_01.py"", line 14, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\venv\lib\site-packages\tensorflow\lite\python\lite.py"", line 921, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""C:\Users\venv\lib\site-packages\tensorflow\lite\python\lite.py"", line 521, in _calibrate_quantize_model
    calibrated = calibrate_quantize.calibrate(
  File ""C:\Users\venv\lib\site-packages\tensorflow\lite\python\optimize\calibrator.py"", line 172, in calibrate
    self._calibrator.Prepare([list(s.shape) for s in sample])
RuntimeError: Failed to initialize op resolver for calibration:
There are unresolved custom ops: [Transpose]tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (512 != 3)Node number 2 (CONV_2D) failed to prepare.
"
54331,Batch processing for tflite_runtime,"**System information**
- TensorFlow version (you are using): 2.7.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Batch processing of images in object detection models that are running in a tflite_runtime environment would be a great feature to add. Right now, it is possible to resize the input tensor to something like [x, h, w, c] where x is the number of images in a batch, and run it through the interpreter. However, the predictions come out looking like an image classification model, where there are probabilities but no bounding boxes.

**Will this change the current api? How?**
Yes. It will allow for developers to leverage the savings associated with batch processing.

**Who will benefit with this feature?**
Anyone who wants to break large images into x number of tiles and run inference on them all at once. This will greatly improve object detection capabilities on EdgeTPU devices.

**Any Other info.**
[Here](https://discuss.tensorflow.org/t/tflite-batch-inference-bug/7495) is a post I made on batch processing with an EfficientDet model.
"
54328,"Getting error data cardinality is ambigous on running in functional api , while running it line by line doesn't","<em>Please make sure that this is an issue related to keras.
tag:keras_template</em>

**Important Notice**

Please note that `tf.keras` code was moved entirely to
[keras-team/keras](https://github.com/keras-team/keras) repository

You can open any code/doc bugs, performance issues, and feature requests
 in [keras-team/keras](https://github.com/keras-team/keras/issues) repository

`tf.keras` related issues opened in
[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) repository may
not get attention as [keras-team/keras](https://github.com/keras-team/keras)
repository is dedicated for the development of `keras` code
"
54327,Cant install TF2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version: any
- Python version: 3.7 (and same with 3.8)
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: i dont know this
- GPU model and memory: 64gb ram. No gpu i guess? (Because its VPS)



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow
pip3 install tensorflow
( Any of those 2 )
```bash
Same when trying install 2.3.1
ERROR: Could not find a version that satisfies the requirement tensorflow>=2.3.1 (from -r requirements.txt (line 5)) (from versions: none)
ERROR: No matching distribution found for tensorflow>=2.3.1 (from -r requirements.txt (line 5))
```

**Error**
```bash
root@vm:~/bot/DolboNet# pip install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```
"
54326,"NotFoundError：dlopen(.../roi_pooling.so, 6): image not found","I try to run Faster_RCNN_TF from this source：
https://github.com/smallcorgi/Faster-RCNN_TF#installation-sufficient-for-the-demo
and I get this error can't solve.
--------------------------------------------------------------------------------
...line 60, in load_op_library 
lib_handle = py_tf.TF_LoadLibrary(library_filename)

tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/anthony/Faster-RCNN_TF/tools/../lib/roi_pooling_layer/roi_pooling.so, 6): image not found

--------------------------------------------------------------------------------
Environment：
Run on the Mac m1  chip
macOS：11.6.1
Python：3.7.11
TensorFlow：2.0.0
TensorFlow install form anaconda
GPU model and memory：M1 GPU,16 GB RAM
Bazel version：N/A
CUDA/cuDNN：N/A 

--------------------------------------------------------------------------------
I have solved most of error so far but I can't find a solution for this error.
I would appreciate any ideas."
54325,Multi-GPU training not starting or freezing. GPUs at 100%,"Very same script runs without issues on 1 GPU (setting NVIDIA_VISIBLE_DEVICES to one GPU) but freezes with 2 or 3 GPUs.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Server 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): 2.7 (Anaconda), 2.8 (official docker), 2.8 (self compiled docker)
- TensorFlow version (use command below): 2.8 & 2,7
- Python version: 3.9
- Bazel version (if compiling from source): 4.2.1
- GCC/Compiler version (if compiling from source): 9
- CUDA/cuDNN version: 11.4
- GPU model and memory: 3x RTX A6000 / 3x 48GB

**Describe the current behavior**
Training script hangs at executing `model.fit()`. Terminal shows `Loaded cuDNN version 8201` thrice (once for each GPU) and displays ""TensorFloat-32 will be used..."" once. GPUs turn to 100% but nothing gets calculated, CPU usage is nearly 0%, ""Epoch 1/50"" is displayed by `model.fit()` but no progress bar and nothing happens. Script is also unresponsive, even to Ctrl+C => process has to be killed manually.

**Describe the expected behavior**
After showing ""Loaded cuDNN 8201"" training should start and progress bar of `model.fit()` call should show up.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): -

**Standalone code to reproduce the issue**
My code is a more complex scenario but even this simple code here shows the same issue (executes right away on 1 GPU but freezes with 2 or 3 GPUs):
```
import tensorflow as tf
import numpy as np
 
 
X = np.random.random((1000, 128, 128, 3)).astype(np.float32)
Y = np.random.random((1000, 10)).astype(np.float32)
 
dat_x = tf.data.Dataset.from_tensor_slices(X)
dat_y = tf.data.Dataset.from_tensor_slices(Y)
ds = tf.data.Dataset.zip((dat_x, dat_y))
ds = ds.batch(96)
ds = ds.repeat(50)
 
strategy = tf.distribute.MirroredStrategy()
 
with strategy.scope():
    inputs = tf.keras.Input(shape=(128, 128, 3))
    x = tf.keras.layers.Conv2D(32, (3, 3))(inputs)
    x = tf.keras.layers.MaxPool2D()(x)
    x = tf.keras.layers.Conv2D(32, (3, 3))(x)
    x = tf.keras.layers.MaxPool2D()(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(10)(x)
    model = tf.keras.Model(inputs=inputs, outputs=x)
    optim = tf.optimizers.Adam()
 
    model.compile(optim, loss=tf.keras.losses.CategoricalCrossentropy())
 
 
model.fit(ds, epochs=50)

```


"
54323,"TensorFlow Lite 2.8.0, benchmark_model build, error C2065: 'M_PI': undeclared identifier","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 21H1 19043.1526
- TensorFlow installed from (source or binary): None
- TensorFlow version: 2.8.0
- Python version: Python 3.7.6
- Installed using virtualenv? pip? conda?: None
- Bazel version (if compiling from source): None, using CMake version 3.19.2
- GCC/Compiler version (if compiling from source): Microsoft Visual Studio Professional 2019, Version 16.11.6
- CUDA/cuDNN version: None
- GPU model and memory: None



**Describe the problem**

During benchmark_model build I've received following error:
```
D:\Git_repos\tensorflow\tensorflow/core/lib/random/random_distributions_utils.h(83,27): error C2065: 'M_PI': undeclared identifier [D:\Git_repos\tensorflow\tensorflow\lite\build_win_2_8\tensorflow-lite.vcxpro
j]
D:\Git_repos\tensorflow\tensorflow/core/lib/random/random_distributions_utils.h(83,15): error C2737: 'v1': const object must be initialized [D:\Git_repos\tensorflow\tensorflow\lite\build_win_2_8\tensorflow-li
te.vcxproj]
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**


I've tried to build benchmark_model (tag v2.8.0) on Windows with following steps:
```
cd tensorflow\tensorflow\lite
mkdir build_win_2_8
cd build_win_2_8
cmake ..
cmake --build . -j -t benchmark_model --config Release
```

**Any other info / logs**

I've managed to fix this with following code change:

```patch
index f09b32f63d3..65c8d90b919 100644
--- a/tensorflow/core/lib/random/random_distributions_utils.h
+++ b/tensorflow/core/lib/random/random_distributions_utils.h
@@ -25,6 +25,10 @@ limitations under the License.
 namespace tensorflow {
 namespace random {

+#ifndef M_PI
+  #define M_PI 3.14159265358979323846
+#endif
+
 // Helper function to convert an 32-bit integer to a float between [0..1).
 PHILOX_DEVICE_INLINE float Uint32ToFloat(uint32_t x) {
   // IEEE754 floats are formatted as follows (MSB first):
```

Is this a correct way of solving this issue or am I missing something in the build process?
"
54322,TFlite on Raspberry 4B: cannot find VerifyField<int8_t>,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 2021-10-30-raspios-bullseye-arm64
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): TensorFlow Lite 2.6.0

Hi, i follow the steps from  https://qengineering.eu/install-tensorflow-2-lite-on-raspberry-64-os.html
for installing TensorFlow Lite on RPi4B with Bullseye 64-bits OS

Dependencies was fully installed and the C++ installation was successful, after exchanging the old version of flatbuffers and successful compilation, i got two libraries and two folders with header files.
![2022-02-08-030833_1920x1080_scrot](https://user-images.githubusercontent.com/33203044/153391879-c8926b6b-9ce9-47c8-8e3d-eeeb42b2d5ad.png)

but when i run TestTensorFlow_Lite.cpb from https://github.com/Qengineering/TensorFlow_Lite_Pose_RPi_64-bits/issues/3 with Code::Blocks, comes out the following errors. That VerifyField<int8_t> is missing. could you please tell me, how can i solve this problem, thx? 
![2022-02-08-030032_1920x1080_scrot](https://user-images.githubusercontent.com/33203044/153392022-a764ca94-24d3-4efe-9640-3b848a5f43f0.png)





![2022-02-08-030319_1920x1080_scrot](https://user-images.githubusercontent.com/33203044/153392028-a09f1d11-a76f-4d1e-ad50-d194275af42a.png)
"
54321,How to apply/link the Flex delegate before inference.,"**System information**
- I was working for porting tflite models on to raspberry pi, so for that TensorFlow library does not work in raspberry pi so I need to somehow use tensorflow_runtime library, but while using it in colab I faced the error for the following code.


**Code**

```
# 
#import tensorflow.lite as tflite
import tflite_runtime.interpreter as tt
#from interpreter import Interpreter 
import numpy as np
import IPython

# Test the model on random input data.
def fastspeech_infer(tflite_model_path, input_text):
  # Load the TFLite model and allocate tensors.
  interpreter = tt.Interpreter(model_path=tflite_model_path)
  print(interpreter)
  #print(interpreter.get_tensor_details())

  # Get input and output tensors.
  input_details = interpreter.get_input_details()
  output_details = interpreter.get_output_details()

  a = {""pad"": 0, ""-"": 1, ""!"": 2, ""'"": 3, ""("": 4, "")"": 5, "","": 6, ""."": 7, "":"": 8, "";"": 9, ""?"": 10, "" "": 11, ""A"": 12, ""B"": 13, ""C"": 14, ""D"": 15, ""E"": 16, ""F"": 17, ""G"": 18, ""H"": 19, ""I"": 20, ""J"": 21, ""K"": 22, ""L"": 23, ""M"": 24, ""N"": 25, ""O"": 26, ""P"": 27, ""Q"": 28, ""R"": 29, ""S"": 30, ""T"": 31, ""U"": 32, ""V"": 33, ""W"": 34, ""X"": 35, ""Y"": 36, ""Z"": 37, ""a"": 38, ""b"": 39, ""c"": 40, ""d"": 41, ""e"": 42, ""f"": 43, ""g"": 44, ""h"": 45, ""i"": 46, ""j"": 47, ""k"": 48, ""l"": 49, ""m"": 50, ""n"": 51, ""o"": 52, ""p"": 53, ""q"": 54, ""r"": 55, ""s"": 56, ""t"": 57, ""u"": 58, ""v"": 59, ""w"": 60, ""x"": 61, ""y"": 62, ""z"": 63, ""@AA"": 64, ""@AA0"": 65, ""@AA1"": 66, ""@AA2"": 67, ""@AE"": 68, ""@AE0"": 69, ""@AE1"": 70, ""@AE2"": 71, ""@AH"": 72, ""@AH0"": 73, ""@AH1"": 74, ""@AH2"": 75, ""@AO"": 76, ""@AO0"": 77, ""@AO1"": 78, ""@AO2"": 79, ""@AW"": 80, ""@AW0"": 81, ""@AW1"": 82, ""@AW2"": 83, ""@AY"": 84, ""@AY0"": 85, ""@AY1"": 86, ""@AY2"": 87, ""@B"": 88, ""@CH"": 89, ""@D"": 90, ""@DH"": 91, ""@EH"": 92, ""@EH0"": 93, ""@EH1"": 94, ""@EH2"": 95, ""@ER"": 96, ""@ER0"": 97, ""@ER1"": 98, ""@ER2"": 99, ""@EY"": 100, ""@EY0"": 101, ""@EY1"": 102, ""@EY2"": 103, ""@F"": 104, ""@G"": 105, ""@HH"": 106, ""@IH"": 107, ""@IH0"": 108, ""@IH1"": 109, ""@IH2"": 110, ""@IY"": 111, ""@IY0"": 112, ""@IY1"": 113, ""@IY2"": 114, ""@JH"": 115, ""@K"": 116, ""@L"": 117, ""@M"": 118, ""@N"": 119, ""@NG"": 120, ""@OW"": 121, ""@OW0"": 122, ""@OW1"": 123, ""@OW2"": 124, ""@OY"": 125, ""@OY0"": 126, ""@OY1"": 127, ""@OY2"": 128, ""@P"": 129, ""@R"": 130, ""@S"": 131, ""@SH"": 132, ""@T"": 133, ""@TH"": 134, ""@UH"": 135, ""@UH0"": 136, ""@UH1"": 137, ""@UH2"": 138, ""@UW"": 139, ""@UW0"": 140, ""@UW1"": 141, ""@UW2"": 142, ""@V"": 143, ""@W"": 144, ""@Y"": 145, ""@Z"": 146, ""@ZH"": 147, ""eos"": 148}
  l=[]
  temp=''
  for i in input_text.lower():
    if i== ' ' :
      if temp!=' ':
        l.append(a[i])
    else:
      l.append(a[i])
    temp=i

  l.append(148) 
  

  interpreter.resize_tensor_input(input_details[0]['index'], 
                                  [1, len(l)])
  interpreter.resize_tensor_input(input_details[1]['index'], 
                                  [1])
  interpreter.resize_tensor_input(input_details[2]['index'], 
                                  [1])
  interpreter.resize_tensor_input(input_details[3]['index'], 
                                  [1])
  interpreter.resize_tensor_input(input_details[4]['index'], 
                                  [1])
  interpreter.allocate_tensors()
  #input_data = fastspeech_prepare_input(l)
  
  #input_data = fastspeech_prepare_input(input_ids)

  x=np.array(l, dtype=np.int32)
  x.resize(1,len(x))
  y=np.array([0],dtype=np.int32)
  z=np.array([1.0],dtype=np.float32)
  w=np.array([1.0],dtype=np.float32)
  v=np.array([1.0],dtype=np.float32)

  interpreter.set_tensor(0, x.astype(np.int32))
  interpreter.set_tensor(1, y)
  interpreter.set_tensor(2, z)
  interpreter.set_tensor(3, w)
  interpreter.set_tensor(4,v)




  interpreter.invoke()

  # The function `get_tensor()` returns a copy of the tensor data.
  # Use `tensor()` in order to get a pointer to the tensor.
  return (interpreter.get_tensor(output_details[0]['index']),
          interpreter.get_tensor(output_details[1]['index']))
  
def run_melgan(mel_spec, quantization):
    model_name = f'melgan_{quantization}.tflite'
    
    feats = np.expand_dims(mel_spec, 0)
    interpreter = tt.Interpreter(model_path=model_name)

    input_details = interpreter.get_input_details()

    output_details = interpreter.get_output_details()

    interpreter.resize_tensor_input(input_details[0]['index'],  [1, feats.shape[1], feats.shape[2]], strict=True)
    interpreter.allocate_tensors()

    interpreter.set_tensor(input_details[0]['index'], feats)

    interpreter.invoke()

    output = interpreter.get_tensor(output_details[0]['index'])
    
    return output

text =  ""Hello how are you my name is text to speech ""    
_, tac_output = fastspeech_infer('fastspeech_quant.tflite', text)
tac_output = np.squeeze(tac_output)
sample_rate = 22050

waveform = run_melgan(tac_output, 'float16')
waveform = np.squeeze(waveform)

IPython.display.display(IPython.display.Audio(waveform, rate=sample_rate))



```

**Output**

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
[<ipython-input-3-6c23683cd482>](https://localhost:8080/#) in <module>()
     91 
     92 text =  ""Hello how are you my name is text to speech ""
---> 93 _, tac_output = fastspeech_infer('fastspeech_quant.tflite', text)
     94 tac_output = np.squeeze(tac_output)
     95 sample_rate = 22050

1 frames
[<ipython-input-3-6c23683cd482>](https://localhost:8080/#) in fastspeech_infer(tflite_model_path, input_text)
     62 
     63 
---> 64   interpreter.invoke()
     65 
     66   # The function `get_tensor()` returns a copy of the tensor data.

[/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py](https://localhost:8080/#) in invoke(self)
    921     """"""
    922     self._ensure_safe()
--> 923     self._interpreter.Invoke()
    924 
    925   def reset_all_variables(self):

RuntimeError: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 456 (FlexRandomUniform) failed to prepare.
```

Please do help me out to solve this error.
"
54318,`tf.compat.v1.layers.AveragePooling3D` lack support for float64,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf

pool_size = [3,3,3]
strides = 2
padding = ""valid""
x = tf.random.uniform([1, 11, 12, 10, 4], dtype=tf.float64)
print(tf.compat.v1.layers.AveragePooling3D(pool_size,strides,padding=padding,)(x))
```
throws 
```
NotFoundError: Exception encountered when calling layer ""average_pooling3d"" (type AveragePooling3D).

Could not find device for node: {{node AvgPool3D}} = AvgPool3D[T=DT_DOUBLE, data_format=""NDHWC"", ksize=[1, 3, 3, 3, 1], padding=""VALID"", strides=[1, 2, 2, 2, 1]]
All kernels registered for op AvgPool3D:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_FLOAT]
 [Op:AvgPool3D]
```

**Expected output**
`tf.compat.v1.layers.AveragePooling3D` should be able to accept a `float64` input."
54317,`tf.math.asin` lack support for complex,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf

x = tf.complex(tf.random.uniform([4], dtype=tf.float64),tf.random.uniform([4], dtype=tf.float64))
print(tf.math.asin(x))
# Could not find device for node: {{node Asin}} = Asin[T=DT_COMPLEX128]
```

**Expected output**
According to the document [tf.math.asin](https://www.tensorflow.org/api_docs/python/tf/math/asin), it should be able to accept a complex input."
54314,Issue created for Rollback of PR #54191: Check for negative `nbins` in histogram ops,"Merged PR #54191 is rolled back in 04f83c818eb4b9d89b5b3e1f97f42213a8ac416c.
    Please follow up with the reviewer and close this issue once its resolved."
54310,TFTRT is not optimizing Resize Bilinear,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18 (using docker image from nvidia: [nvcr.io/nvidia/tensorflow:21.12-tf2-py3](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow)
- TensorFlow installed from (source or binary): 2.6.2, but the problem persists on older versions too (all versions in 2021). Newer versions are not yet stable for tensorrt. 
- TensorFlow version (use command below): via docker image. 
- Python version: Python 3.8.10
- CUDA/cuDNN version: CUDA 11.5.0
- GPU model and memory: Telsla T4 16 GB, but also on GTX 1080 TI. 

**Describe the current behaviour**
When training and optimizing the model, logs indicate that an operation called ResizeBilinear is not optimized. Initially, I thought that's because I am using dynamic_shapes = True (and this operation does not support dynamic shapes), but when I disabled that, the log persists. 

**My question is also, what kind of improvement should I expect if this operation is supported?**

**Describe the expected behaviour**
I think a lot of applications are using Resizing on the input, and from my research [ONNX](https://github.com/onnx/onnx/blob/main/docs/Operators.md#resize)  and [Tesnorrt](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_resize_layer.html) is supporting this operation. I also checked that ``tf.resize`` via nearest neighbour interpolation is supported which gives me no indication why it is not working. 

**Standalone code to reproduce the issue**
I was trying to create a collab, but it is a nightmare to set up TensorRT. Here are my tries and standalone code for script which trains a simple model, saves it and then optimizes it via TFTRT. 
- Google Colab: [link](https://colab.research.google.com/drive/1K-6Qoeo-3GxINUJw6Xi3N_6V0y0SHBON?usp=sharing)
- Python script:  [link](https://gist.github.com/rpytel1/de52d7e3116c6d3da5c7bd3f9e65a738)

**Other info / logs**:
Unsupported comment looks like this: 
<img width=""1016"" alt=""Screenshot 2022-02-09 at 11 57 59"" src=""https://user-images.githubusercontent.com/12116282/153184823-51f0835b-7db1-4f70-a656-1309d723e879.png"">

"
54309,TensorflowLite model can't run on Hexagon DSP device. Failed to apply delegate: Failed: Failed to prepare graph.,"### 1. System information

- Linux Ubuntu 16.04 / Android 10.0
- pip package
- tensorflow==2.6.2 

### 2. Code
**export tflite code:**
```python3
import numpy as np
import tensorflow as tf

# Generate tf.keras model.
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.DepthwiseConv2D(kernel_size=(1, 9), strides=(1, 1), padding='same', data_format='channels_last'))
# model.add(tf.keras.layers.Dense(2, input_shape=(3,)))
model.compile(loss=tf.keras.losses.MSE,
              optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),
              metrics=[tf.keras.metrics.categorical_accuracy],
              sample_weight_mode='temporal')


x = np.random.random((1, 1, 9, 64))
y = np.random.random((1, 1, 9, 64))
model.train_on_batch(x, y)
model.predict(x)

# Save tf.keras model in H5 format.
keras_file = 'keras_model.h5'
tf.keras.models.save_model(model, keras_file)

# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)

def representative_dataset_gen():
    for _ in range(100):
        yield [np.ones([1,1,9,64]).astype(np.float32)]
# converter.quantized_input_stats = {'input' : (0., 1.)}
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
# converter.inference_type = tf.uint8
converter.representative_dataset = representative_dataset_gen

tflite_model = converter.convert()

# Save the model.
with open('model2.tflite', 'wb') as f:
  f.write(tflite_model)
```

**android project code:**
```java
package com.elevoc.demo.app.tflitetest;

import android.content.Context;
import android.content.res.AssetFileDescriptor;
import android.content.res.AssetManager;
import android.os.SystemClock;
import android.util.Log;
import org.tensorflow.lite.DataType;
import org.tensorflow.lite.HexagonDelegate;
import org.tensorflow.lite.Interpreter;
import org.tensorflow.lite.support.tensorbuffer.TensorBuffer;

import java.io.FileInputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;
import java.util.concurrent.locks.ReentrantLock;


public class TfInter {
    private final String MODEL_NAME = ""model2.tflite"";
    private final ReentrantLock tfLiteLock = new ReentrantLock();

    private Context context;
    private Interpreter tfLite = null;
    private HexagonDelegate hexagonDelegate = null;

    public TfInter(Context context){
        this.context = context;
        init(context);
    }

    public void init(Context context){
        Interpreter.Options options = (new Interpreter.Options());

//        options.setNumThreads(4);

        //TODO Hexagon delegate
        // Create the Delegate instance.
        try {
            hexagonDelegate = new HexagonDelegate(context);
            options.addDelegate(hexagonDelegate);
        } catch (UnsupportedOperationException e) {
            // Hexagon delegate is not supported on this device.
            options.setNumThreads(4);
        }

        MappedByteBuffer buf = null;
        try {
            buf = loadModelFile(context.getAssets(), MODEL_NAME);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }

        try {
            tfLite = new Interpreter(buf, options);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
        // create the input tensor
        int inTensorIndex = 0;
        int[] inShape = tfLite.getInputTensor(inTensorIndex).shape(); // {1, height, width, 3}
        DataType inDataType = tfLite.getInputTensor(inTensorIndex).dataType();

        // create the output tensor
        int outTensorIndex = 0;
        int[] outShape = tfLite.getOutputTensor(outTensorIndex).shape(); // {1, max_lane_count, y_anchors, x_anchors}
        DataType outDataType = tfLite.getOutputTensor(outTensorIndex).dataType();
    }

    private MappedByteBuffer loadModelFile(AssetManager assetManager, String model_name) throws IOException {
        try (AssetFileDescriptor fileDescriptor = assetManager.openFd(model_name);
             FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor())) {
            FileChannel fileChannel = inputStream.getChannel();
            long startOffset = fileDescriptor.getStartOffset();
            long declaredLength = fileDescriptor.getDeclaredLength();
            return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
        }
    }

    public void run(){
        byte[] input_t = new byte[1*1*9*64*4];
        ByteBuffer input1 = ByteBuffer.wrap(input_t);
        DataType probabilityDataType = tfLite.getOutputTensor(0).dataType();
        TensorBuffer output1 = TensorBuffer.createFixedSize(new int[]{1, 1, 9, 64 * 4}, probabilityDataType);

        long startTimeForReference = SystemClock.uptimeMillis();

        for(int i=0;i<1000;i++){
            tfLite.run(input1, output1.getBuffer().rewind());
        }

        long endTimeForReference = SystemClock.uptimeMillis();
        Log.d(""tryTimeCost"", ""Timecost: "" + (endTimeForReference - startTimeForReference));
    }
}

```


### 5. (optional) Any other info / logs
We used a test vehicle hardware platform to execute the above java code.
We have obtained ROOT permission for this device.
The following command also executed successfully
```
adb shell setenforce 0
```
We ran the [example tflite model](https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz) and it worked on the Hexagon DSP device, but our own exported model didn't work correctly. 

**our own exported model file can run on cpu/gpu/nnapi, but can't run on Hexagon DSP device.**
The logs from logcat are as follows:
```
Process: com.elevoc.demo.app.tflitetest, PID: 14450
    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.elevoc.demo.app.tflitetest/com.elevoc.demo.app.tflitetest.MainActivity}: java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Failed: Failed to prepare graph.
    .
    Node number 3 (TfLiteHexagonDelegate) failed to prepare.
    
    Restored original execution plan after delegate application failure.
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3274)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3413)
        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:83)
        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2020)
        at android.os.Handler.dispatchMessage(Handler.java:107)
        at android.os.Looper.loop(Looper.java:214)
        at android.app.ActivityThread.main(ActivityThread.java:7395)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)
     Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Failed: Failed to prepare graph.
    .
    Node number 3 (TfLiteHexagonDelegate) failed to prepare.
    
    Restored original execution plan after delegate application failure.
        at com.elevoc.demo.app.tflitetest.TfInter.init(TfInter.java:59)
        at com.elevoc.demo.app.tflitetest.TfInter.<init>(TfInter.java:31)
        at com.elevoc.demo.app.tflitetest.MainActivity.onCreate(MainActivity.java:15)
        at android.app.Activity.performCreate(Activity.java:7825)
        at android.app.Activity.performCreate(Activity.java:7814)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1309)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3249)
        	... 11 more
     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Failed: Failed to prepare graph.
    .
    Node number 3 (TfLiteHexagonDelegate) failed to prepare.
    
    Restored original execution plan after delegate application failure.
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:530)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:93)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:66)
        at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:44)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:226)
        at com.elevoc.demo.app.tflitetest.TfInter.init(TfInter.java:57)
        	... 17 more
```"
54308,Getting ```Check failed: IsAligned() ptr = 0x306a33ff0``` on M1 Mac while training with tensorflow-metal,"**System information**
- Macbook Air M1 2021 8GB RAM (MacOS Monterey 12.2)
- TensorFlow version: 2.7.0
- Python version: Python 3.9.10

**Describe the current behavior**
I am trying to train a model and what happens is that it stops executing at the beginning of the training and throws the following error:
```
Metal device set to: Apple M1

systemMemory: 8.00 GB
maxCacheSize: 2.67 GB

2022-02-09 13:55:17.221197: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-02-09 13:55:17.221301: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2022-02-09 13:55:17.925933: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
2022-02-09 13:55:17.937419: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
Epoch 1/15
2022-02-09 13:55:24.784714: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2022-02-09 13:55:25.313823: F tensorflow/core/framework/tensor.cc:681] Check failed: IsAligned() ptr = 0x306a33ff0
zsh: abort      python3 main.py
``` 
This only happens if ```tensorflow-metal``` is installed. (I am using tensorflow-metal 0.3.0 BTW). If I uninstall tensorflow-metal, it runs normally but each epochs take 1 hour to train, because it's running on cpu instead of gpu as there is no tensorflow-metal installed
. 
**Describe the expected behavior**

The training should work even if tensorflow-metal is installed.

**Standalone code to reproduce the issue**

This is my code:
```
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import os
import sys
import inspect
currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
parentdir = os.path.dirname(currentdir)
sys.path.insert(0, parentdir) 
from MLUtils import MLUtils
utils = MLUtils()

ds = pd.read_csv('resources/tweets.csv')
ds.drop('Unnamed: 0', axis=1, inplace=True)
label_encoder = LabelEncoder()
ds['label'] = label_encoder.fit_transform(ds['label'])
le_name_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))
num_labels = len(le_name_mapping)
X_train, X_test, Y_train, Y_test = train_test_split(ds['tweet'], ds['label'], test_size = 0.2, stratify=ds['label'], random_state=42)

bert_preprocess = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')
bert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')

# Bert layers
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=""text"")
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)

# Neural Network Layers
l = tf.keras.layers.Dropout(0.1, name='dropout')(outputs['pooled_output'])
l = tf.keras.layers.Dense(num_labels, name='output', activation='softmax')(l)
model = tf.keras.Model(inputs=[text_input], outputs=[l])

epochs = 15
batch_size = 32
steps_per_epoch = X_train.shape[0] // batch_size
metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]
loss = tf.keras.losses.SparseCategoricalCrossentropy()

# Model training
model.compile(optimizer='adam', loss=loss, metrics=metrics)
model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)
model.evaluate(X_test, Y_test)
model.save('text_classifier')
```
"
54304,Can't convert model with IF op to quantized TF-Lite model,"## System information
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.7.0

## Provide the text output from tflite_convert

<details>
  <summary>console output of tflite converter</summary>

  ```py
  ---------------------------------------------------------------------------
  
  RuntimeError                              Traceback (most recent call last)
  
  [<ipython-input-12-1e0173995c78>](https://localhost:8080/#) in <module>()
       15 ]
       16 
  ---> 17 tflite_model = converter.convert()
       18 
       19 # Save the model.
  
  12 frames
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in convert(self)
     1669         Invalid quantization parameters.
     1670     """"""
  -> 1671     return super(TFLiteConverterV2, self).convert()
     1672 
     1673 
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in wrapper(self, *args, **kwargs)
      773   def wrapper(self, *args, **kwargs):
      774     # pylint: disable=protected-access
  --> 775     return self._convert_and_export_metrics(convert_func, *args, **kwargs)
      776     # pylint: enable=protected-access
      777 
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _convert_and_export_metrics(self, convert_func, *args, **kwargs)
      759     self._save_conversion_params_metric()
      760     start_time = time.process_time()
  --> 761     result = convert_func(self, *args, **kwargs)
      762     elapsed_time_ms = (time.process_time() - start_time) * 1000
      763     if result:
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in convert(self)
     1339 
     1340     return super(TFLiteFrozenGraphConverterV2,
  -> 1341                  self).convert(graph_def, input_tensors, output_tensors)
     1342 
     1343 
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in convert(self, graph_def, input_tensors, output_tensors)
      950 
      951     return self._optimize_tflite_model(
  --> 952         result, self._quant_mode, quant_io=self.experimental_new_quantizer)
      953 
      954 
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
      224       except Exception as error:
      225         report_error_message(str(error))
  --> 226         raise error from None  # Re-throws the exception.
      227 
      228     return wrapper
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
      214     def wrapper(*args, **kwargs):
      215       try:
  --> 216         return func(*args, **kwargs)
      217       except ConverterError as converter_error:
      218         if converter_error.errors:
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _optimize_tflite_model(self, model, quant_mode, quant_io)
      720         q_allow_float = quant_mode.is_allow_float()
      721         model = self._quantize(
  --> 722             model, q_in_type, q_out_type, q_activations_type, q_allow_float)
      723 
      724       m_in_type = in_type if in_type else _dtypes.float32
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _quantize(self, result, input_type, output_type, activations_type, allow_float)
      540           self._experimental_disable_per_channel,
      541           input_data_type=input_type,
  --> 542           output_data_type=output_type)
      543     else:
      544       return calibrate_quantize.calibrate_and_quantize(
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
      224       except Exception as error:
      225         report_error_message(str(error))
  --> 226         raise error from None  # Re-throws the exception.
      227 
      228     return wrapper
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
      214     def wrapper(*args, **kwargs):
      215       try:
  --> 216         return func(*args, **kwargs)
      217       except ConverterError as converter_error:
      218         if converter_error.errors:
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py](https://localhost:8080/#) in mlir_quantize(input_data_str, disable_per_channel, fully_quantize, inference_type, input_data_type, output_data_type, enable_numeric_verify, enable_whole_model_verify, denylisted_ops, denylisted_nodes)
      240       convert_tensor_tf_type_to_tflite_type(output_data_type),
      241       enable_numeric_verify, enable_whole_model_verify, denylisted_ops,
  --> 242       denylisted_nodes)
      243 
      244 
  
  [/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/wrap_toco.py](https://localhost:8080/#) in wrapped_experimental_mlir_quantize(input_data_str, disable_per_channel, fully_quantize, inference_type, input_data_type, output_data_type, enable_numeric_verify, enable_whole_model_verify, denylisted_ops, denylisted_nodes)
       48       input_data_str, disable_per_channel, fully_quantize, inference_type,
       49       input_data_type, output_data_type, enable_numeric_verify,
  ---> 50       enable_whole_model_verify, denylisted_ops, denylisted_nodes)
       51 
       52 
  
  RuntimeError: Failed to quantize: <unknown>:0: error: loc(""Identity""): 'tf.If' op result #0 must be tensor of tf.dtype values, but got 'tensor<1x!quant.uniform<i8:f32, 0.0039215688593685627:-128>>'
  <unknown>:0: note: loc(""Identity""): see current operation: %8 = ""tf.If""(%6) {else_branch = @cond_false_100, is_stateless = false, then_branch = @cond_true_90} : (tensor<*xi1>) -> tensor<1x!quant.uniform<i8:f32, 0.0039215688593685627:-128>>
  ```
</details>

## Standalone code to reproduce the issue
[Colab notebook](https://colab.research.google.com/drive/1R3bIfsVqB1z6CuqJKLRj0oe_iCzOTP5t?usp=sharing)  


## TLDR
It is currently not possible to get a full `int8` quantized model with the `tfl.if` (`tf.cond`) control flow operation in it. The TFLiteConverter fails to convert. Dynamic quantization is working, however not helpful in my case as I want to run the model with tensorflow lite micro which doesn't support hybrid models.
"
54303,Linking of rule '//tensorflow/java:libtensorflow_jni.dylib' failed (Exit 1),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monterey
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.1.4
- Python version: 3.8.12(installed with brew, python@3.8)
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): How can I detect?
- CUDA/cuDNN version:
- GPU model and memory: Not using GPU, 16g memory

I'm using macbookpro m1.
I executed bazel like this.
`bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni //tensorflow/java:pom`

Error message was like this
`
ERROR: ~/tensorflow/tensorflow/java/BUILD:416:1: Linking of rule '//tensorflow/java:libtensorflow_jni.dylib' failed (Exit 1)
ld: warning: pointer not aligned at address 0x91C12B6 (_grpc_lb_v1_ClientStats_fields + 166 from bazel-out/darwin-opt/bin/external/grpc/libgrpclb_proto.a(load_balancer.pb.o))
ld: warning: pointer not aligned at address 0x91C1225 (_grpc_lb_v1_ClientStats_fields + 21 from bazel-out/darwin-opt/bin/external/grpc/libgrpclb_proto.a(load_balancer.pb.o))
ld: warning: pointer not aligned at address 0x91C1312 (_grpc_lb_v1_LoadBalanceRequest_fields + 50 from bazel-out/darwin-opt/bin/external/grpc/libgrpclb_proto.a(load_balancer.pb.o))
ld: warning: pointer not aligned at address 0x91C12F5 (_grpc_lb_v1_LoadBalanceRequest_fields + 21 from bazel-out/darwin-opt/bin/external/grpc/libgrpclb_proto.a(load_balancer.pb.o))
ld: warning: pointer not aligned at address 0x91C1372 (_grpc_lb_v1_InitialLoadBalanceResponse_fields + 50 from bazel-out/darwin-opt/bin/external/grpc/libgrpclb_proto.a(load_balancer.pb.o))
ld: warning: pointer not aligned at address 0x91C13B5 (_grpc_lb_v1_ServerList_fields + 21 from bazel-out/darwin-opt/bin/external/grpc/libgrpclb_proto.a(load_balancer.pb.o))
ld: warning: pointer not aligned at address 0x91C1412 (_grpc_lb_v1_LoadBalanceResponse_fields + 50 from bazel-out/darwin-opt/bin/external/grpc/libgrpclb_proto.a(load_balancer.pb.o))
ld: warning: pointer not aligned at address 0x91C13F5 (_grpc_lb_v1_LoadBalanceResponse_fields + 21 from bazel-out/darwin-opt/bin/external/grpc/libgrpclb_proto.a(load_balancer.pb.o))
ld: warning: pointer not aligned at address 0x91C2886 (_grpc_gcp_AltsContext_fields + 166 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(altscontext.pb.o))
ld: warning: pointer not aligned at address 0x91C29AD (_grpc_gcp_StartClientHandshakeReq_fields + 253 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2973 (_grpc_gcp_StartClientHandshakeReq_fields + 195 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2956 (_grpc_gcp_StartClientHandshakeReq_fields + 166 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2939 (_grpc_gcp_StartClientHandshakeReq_fields + 137 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2A12 (_grpc_gcp_ServerHandshakeParameters_fields + 50 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2A72 (_grpc_gcp_StartServerHandshakeReq_HandshakeParametersEntry_fields + 50 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2B46 (_grpc_gcp_StartServerHandshakeReq_fields + 166 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2B29 (_grpc_gcp_StartServerHandshakeReq_fields + 137 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2AD2 (_grpc_gcp_StartServerHandshakeReq_fields + 50 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2BBF (_grpc_gcp_HandshakerReq_fields + 79 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2BA2 (_grpc_gcp_HandshakerReq_fields + 50 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2B85 (_grpc_gcp_HandshakerReq_fields + 21 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2CB3 (_grpc_gcp_HandshakerResult_fields + 195 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2C79 (_grpc_gcp_HandshakerResult_fields + 137 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2D2F (_grpc_gcp_HandshakerResp_fields + 79 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(handshaker.pb.o))
ld: warning: pointer not aligned at address 0x91C2DB2 (_grpc_gcp_RpcProtocolVersions_fields + 50 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(transport_security_common.pb.o))
ld: warning: pointer not aligned at address 0x91C2D95 (_grpc_gcp_RpcProtocolVersions_fields + 21 from bazel-out/darwin-opt/bin/external/grpc/libalts_proto.a(transport_security_common.pb.o))
ld: unaligned pointer(s) for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
INFO: Elapsed time: 5379.928s, Critical Path: 327.92s
INFO: 11618 processes: 11618 local.
FAILED: Build did NOT complete successfully
`

I think `ld: unaligned pointer(s) for architecture x86_64` is the main problem.
How can I build tensorflow with jni?

Thanks.


**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
54300,Suggested changes in tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb,"Hi,

I ran the notebook tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb and I encountered some issues that I believe it would good to change:

1) In cell 2, tensorflow is installed and in cell 3 a couple of packages are imported that would not be installed in a fresh environment: pandas and matplotlib. I believe it would be good to install these two together with tensorflow.

2) In cell 33, the size of the models is plotted in a table. I got 160 bytes for size_tf while the total size of the model folder for me is 107573. The following code does give me this number:

```
def get_dir_size(path='.'):
    total = 0
    with os.scandir(path) as it:
        for entry in it:
            if entry.is_file():
                total += entry.stat().st_size
            elif entry.is_dir():
                total += get_dir_size(entry.path)
    return total
size_tf = get_dir_size(MODEL_TF)
```

3) If we do not want to commit the models generated when running the examples, it may be good to add the following line to the .gitignore file?

```
tensorflow/lite/micro/examples/*/train/models
```

I can create a PR with these changes if they make sense. 

Thanks!"
54299,`tf.data` shuffle uses 2.5x more memory than necessary during `model.fit`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.8.0 and 2.9.0.dev20220208
- Python version: 3.9

**Describe the current behavior**

When training a Keras model with a `tf.data.Dataset` that has not been cached in memory before applying `tf.data.Dataset.shuffle(buffer)`, it seems like the shuffle buffer is never properly freed thus memory usage increases far beyond the memory expected from the shuffle buffer.

When running the code below using `dataset.cache().shuffle(num_examples)` the entire dataset will be loaded into memory and shuffled. This is expected and the code will require **~145 GB** of memory which is equivalent to the size of the dataset.

However using `dataset.shuffle(num_examples // 2)` without prior in memory caching (like what would be require on smaller machines) the code requires **~160 GB** of memory which is more than the entire size of the dataset, making `.shuffle()` unusable with large datasets and buffers. The same behaviour can be observed when setting `reshuffle_each_iteration=False`. It seems like after the first epoch the memory usage just continues to go up rather than staying at roughly the size that is required to store the shuffle buffer.

**Describe the expected behavior**
I would expect that `tf.data` and `model.fit` do not use memory beyond what's set required by the shuffle buffer, so in this example around **~73 GB**. Ideally the buffer would even be shared across epochs so that it doesn't need to be filled again before every epoch. A bit of memory overhead is expected to do this efficiently but the overhead should not be twice the size of the shuffle buffer and should certainly not exceed the total size of the dataset since that defeats the purpose of using a shuffle buffer that is smaller than the dataset.

I don't think I am doing anything non-standard in the code below. In fact I follow strickly the recommended usage of both `tf.data` and Keras so I am very surprised about the larger than expected memory usage.

**Standalone code to reproduce the issue**

Consider the following simple example that trains a no-op model on a large dataset (in this case ImageNet):
```python
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow import keras

batch_size = 1024

dataset, info = tfds.load(
    ""imagenet2012:5.0.*"",
    decoders={""image"": tfds.decode.SkipDecoding()},
    split=""train"",
    with_info=True,
    data_dir=""gs://my_data_bucket"",
)
num_examples = info.splits[""train""].num_examples


def _decode_and_center_crop(image_bytes):
    """"""Crops to center of image with padding then scales image_size.""""""
    shape = tf.image.extract_jpeg_shape(image_bytes)
    image_height, image_width = shape[0], shape[1]
    image_size = 224

    padded_center_crop_size = tf.cast(
        (
            (image_size / (image_size + 32))
            * tf.cast(tf.minimum(image_height, image_width), tf.float32)
        ),
        tf.int32,
    )

    offset_height = ((image_height - padded_center_crop_size) + 1) // 2
    offset_width = ((image_width - padded_center_crop_size) + 1) // 2
    crop_window = tf.stack(
        [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size]
    )
    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)
    image = tf.image.resize(image, [image_size, image_size])
    return tf.cast(image, dtype=tf.float32)


def preprocessing(data):
    return _decode_and_center_crop(data[""image""]), data[""label""]


dataset = (
    dataset.shuffle(num_examples // 2)
    .map(preprocessing, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(batch_size)
    .prefetch(1)
)

model = keras.models.Sequential(
    [
        keras.layers.GlobalMaxPool2D(input_shape=(224, 224, 3)),
        keras.layers.Dense(1000, activation=""softmax""),
    ]
)

model.compile(optimizer=""sgd"", loss=""sparse_categorical_crossentropy"")

model.fit(dataset, epochs=3)
```

Unfortunately this cannot be reproduced on Colab since not enough memory is available there. This issue is not related the the exact dataset used. I am just using ImageNet streamed from a GCS bucket, since this should be a pretty standard task. However this can be reproduced with any significantly large dataset.

The underlying cause might similar to what caused #36240, but I haven't been able to look into the `tf.data` or Keras code to debug what's going on.
"
54298,tflite with hexagon delegate get poor performance while running mobilenet_quant_v1_224.tflite on Snapdragon 845,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MeiZu 16th, Snapdragon 845
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master branch, commit hash : 851b83d0ef9659bb83b4f8c56708fb045fe276dc
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): ndk 20

After push all libhexagon_*.so to phone, I run benchmark_model with option ""--use_hexagon=true --hexagon_profiling=true"", then get results below
![image](https://user-images.githubusercontent.com/19945545/152995700-7ecb5c09-a39a-483e-bd54-6870c3fb471e.png)

It seem that symbol of remote_handle_control not found, and the performance just so poor which even almost reach 60ms

"
54296,saved_model_aot_compile.py removes unwanted tensors from signature,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.9
- CUDA/cuDNN version:  11.1
- GPU model and memory: 12 GB


**Describe the current behavior**

When I load the frozen graph using `tf.saved_model.load(""saved_model.pb"")`, the signature shows all the tensor names, but when `.local/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_aot_compile.py` does the `_prune_removed_feed_nodes(signature_def, graph_def)`, it prunes multiple tensors that it does not find in `graph_dev.node` and as a result my final deployed model is incorrect:

```
def _prune_removed_feed_nodes(signature_def, graph_def):
  """"""Identify the inputs in the signature no longer in graph_def, prune them.

  Args:
    signature_def: A `SignatureDef` instance.
    graph_def: A `GraphDef` instance.

  Returns:
    A new pruned `SignatureDef`.
  """"""
  node_names = set([n.name for n in graph_def.node])
  new_signature_def = meta_graph_pb2.SignatureDef()
  new_signature_def.CopyFrom(signature_def)
  for (k, v) in signature_def.inputs.items():
    tensor_name, _ = _parse_tensor_name(v.name)
    if tensor_name not in node_names:
    ¦ logging.warn(
    ¦   ¦ 'Signature input key \'{}\', tensor name \'{}\', has been pruned '
    ¦   ¦ 'while freezing the graph.  Removing it from the compiled signatures.'
    ¦   ¦ .format(k, tensor_name))
    ¦ del new_signature_def.inputs[k]
  return new_signature_def
```

 Here are my other TF related packages:

```
tensorboard             2.6.0
tensorboard-data-server 0.6.1
tensorboard-plugin-wit  1.7.0
tensorflow              2.4.1
tensorflow-addons       0.11.2
tensorflow-estimator    2.4.0
tensorflow-probability  0.12.2
tf-agents               0.7.1
tf-estimator-nightly    2.4.0.dev2020102201
```

Also, `2.4.1` was used to create the model: 

```
>>> imported
<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7f6bcc4f7a58>
>>> imported.tensorflow_version
'2.4.1'
```

Here is the debug output:
```
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 511.41GiB/s
2022-02-06 21:56:34.286796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-02-06 21:56:34.290840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-02-06 21:56:34.290891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-02-06 21:56:34.293434: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-02-06 21:56:34.293855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-02-06 21:56:34.296799: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-02-06 21:56:34.297732: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-02-06 21:56:34.297943: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-02-06 21:56:34.299759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-02-06 21:56:34.299802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-02-06 21:56:35.075324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-06 21:56:35.075387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2022-02-06 21:56:35.075400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2022-02-06 21:56:35.078385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11119 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: $
000:82:00.0, compute capability: 6.0)
2022-02-06 21:56:35.096700: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2194810000 Hz
2022-02-06 21:56:35.236522: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] model_pruner failed: Invalid argument: Graph does not contain terminal node StatefulPartitionedCall_2.
2022-02-06 21:56:35.247615: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize
  model_pruner: Graph size after: 38 nodes (-2), 48 edges (0), time = 0.987ms.
  implementation_selector: Graph size after: 38 nodes (0), 48 edges (0), time = 0.562ms.
  function_optimizer: Graph size after: 343 nodes (305), 581 edges (533), time = 22.39ms.
  common_subgraph_elimination: Graph size after: 303 nodes (-40), 541 edges (-40), time = 3.508ms.
  constant_folding: Graph size after: 227 nodes (-76), 387 edges (-154), time = 55.232ms.
  shape_optimizer: shape_optimizer did nothing. time = 0.41ms.
  arithmetic_optimizer: Graph size after: 238 nodes (11), 398 edges (11), time = 4.174ms.
  layout: Graph size after: 238 nodes (0), 398 edges (0), time = 5.838ms.
  remapper: Graph size after: 238 nodes (0), 398 edges (0), time = 1.459ms.
  loop_optimizer: Graph size after: 238 nodes (0), 397 edges (-1), time = 1.714ms.
  dependency_optimizer: Graph size after: 156 nodes (-82), 221 edges (-176), time = 3.391ms.
  memory_optimizer: Graph size after: 156 nodes (0), 221 edges (0), time = 6.835ms.
  model_pruner: Invalid argument: Graph does not contain terminal node StatefulPartitionedCall_2.
  implementation_selector: Graph size after: 156 nodes (0), 221 edges (0), time = 0.468ms.
  function_optimizer: function_optimizer did nothing. time = 0.127ms.
  common_subgraph_elimination: Graph size after: 146 nodes (-10), 211 edges (-10), time = 1.021ms.
  constant_folding: Graph size after: 146 nodes (0), 211 edges (0), time = 3.151ms.
  shape_optimizer: shape_optimizer did nothing. time = 0.133ms.
  arithmetic_optimizer: Graph size after: 146 nodes (0), 211 edges (0), time = 2.64ms.
  remapper: Graph size after: 146 nodes (0), 211 edges (0), time = 0.8ms.
  dependency_optimizer: Graph size after: 146 nodes (0), 211 edges (0), time = 1.752ms.

2022-02-06 21:56:35.281080: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-02-06 21:56:35.282047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:82:00.0 name: Tesla P100-PCIE-12GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 511.41GiB/s
2022-02-06 21:56:35.282083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-02-06 21:56:35.282137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-02-06 21:56:35.282155: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-02-06 21:56:35.282172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-02-06 21:56:35.282190: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-02-06 21:56:35.282208: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-02-06 21:56:35.282226: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-02-06 21:56:35.282243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-02-06 21:56:35.283971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-02-06 21:56:35.284299: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-02-06 21:56:35.285214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:82:00.0 name: Tesla P100-PCIE-12GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 511.41GiB/s
2022-02-06 21:56:35.285237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-02-06 21:56:35.285258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-02-06 21:56:35.285277: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-02-06 21:56:35.285295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-02-06 21:56:35.285311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-02-06 21:56:35.285328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-02-06 21:56:35.285345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-02-06 21:56:35.285363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-02-06 21:56:35.287113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-02-06 21:56:35.287143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-06 21:56:35.287153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2022-02-06 21:56:35.287161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2022-02-06 21:56:35.288959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11119 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:82:00.0, compute capability: 6.0)
INFO:tensorflow:Restoring parameters from /home/llvm-project/llvm/lib/Analysis/models/inliner/variables/variables
2022-02-06 21:56:35.354948: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)
WARNING:tensorflow:From /home/.local/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_aot_compile.py:332: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /home/.local/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py:856: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
WARNING:tensorflow:Signature input key 'XXX', tensor name 'action_XXX', has been pruned while freezing the graph.  Removing it from the compiled signatures.
WARNING:tensorflow:Signature input key 'discount', tensor name 'action_discount', has been pruned while freezing the graph.  Removing it from the compiled signatures.
WARNING:tensorflow:Signature input key 'XXX', tensor name 'action_XXX', has been pruned while freezing the graph.  Removing it from the compiled signatures.
WARNING:tensorflow:Signature input key 'reward', tensor name 'action_reward', has been pruned while freezing the graph.  Removing it from the compiled signatures.
WARNING:tensorflow:Signature input key 'step_type', tensor name 'action_step_type', has been pruned while freezing the graph.  Removing it from the compiled signatures.
WARNING:tensorflow:Signature input key 'inlining_default', tensor name 'action_inlining_default', has been pruned while freezing the graph.  Removing it from the compiled signatures.
INFO:tensorflow:Writing graph def to: /tmp/saved_model_clilxj7nh6h/frozen_graph.pb
INFO:tensorflow:Writing config_pbtxt to: /tmp/saved_model_clilxj7nh6h/config.pbtxt
INFO:tensorflow:Generating XLA AOT artifacts in: /home/llvm-project/build/lib/Analysis

```

"
54295,Do you have plan to remove –cxxopt=”-D_GLIBCXX_USE_CXX11_ABI=0 in build?,"Hi,

I found the latest TensorFlow release binary is build with “–cxxopt=”-D_GLIBCXX_USE_CXX11_ABI=0"" for compatibility with the older ABI, do you have plain to remove this one? that mean build with --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=1"""
54294,Do you have plan to build with C++17 by default?,"Hi,

I found TensorFlow has build option to build with C++17, but seems the release version binary did not use this one.
Do you have plan to build with C++17 in release binary?"
54290,Target //tensorflow/tools/pip_package:build_pip_package failed to build,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
Linux Ubuntu 16.04
CPU:5220R,memory 128G,disk:14T
use these command:


docker pull tensorflow/tensorflow:devel
docker run -it -w /tensorflow_src -v $PWD:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" \
tensorflow/tensorflow:devel bash
git pull

use proxy and:

pip install --upgrade pip setuptools       
pip install --upgrade pip

./configure is N with all question,and build with this command:
bazel build -c opt --copt=-march=native --copt=-mfpmath=both --config=v2   -k //tensorflow/tools/pip_package:build_pip_package --verbose_failures



the error info is:

ERROR: /tensorflow_src/tensorflow/stream_executor/tpu/BUILD:43:11: Compiling tensorflow/stream_executor/tpu/c_api_conversions.cc failed: (Exit 1): gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \
  exec env - \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.0.0-linux-x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/k8-opt/bin/tensorflow/stream_executor/tpu/_objs/c_api_conversions/c_api_conversions.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/stream_executor/tpu/_objs/c_api_conversions/c_api_conversions.pic.o' -fPIC -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_GETSPECIFIC=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_LSEEK64=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=""X86""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=""x86_64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64-unknown-linux-gnu""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -iquote. -iquotebazel-out/k8-opt/bin -iquoteexternal/com_google_absl -iquotebazel-out/k8-opt/bin/external/com_google_absl -iquoteexternal/nsync -iquotebazel-out/k8-opt/bin/external/nsync -iquoteexternal/eigen_archive -iquotebazel-out/k8-opt/bin/external/eigen_archive -iquoteexternal/gif -iquotebazel-out/k8-opt/bin/external/gif -iquoteexternal/libjpeg_turbo -iquotebazel-out/k8-opt/bin/external/libjpeg_turbo -iquoteexternal/com_google_protobuf -iquotebazel-out/k8-opt/bin/external/com_google_protobuf -iquoteexternal/com_googlesource_code_re2 -iquotebazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquoteexternal/farmhash_archive -iquotebazel-out/k8-opt/bin/external/farmhash_archive -iquoteexternal/fft2d -iquotebazel-out/k8-opt/bin/external/fft2d -iquoteexternal/highwayhash -iquotebazel-out/k8-opt/bin/external/highwayhash -iquoteexternal/zlib -iquotebazel-out/k8-opt/bin/external/zlib -iquoteexternal/double_conversion -iquotebazel-out/k8-opt/bin/external/double_conversion -iquoteexternal/local_config_cuda -iquotebazel-out/k8-opt/bin/external/local_config_cuda -iquoteexternal/local_config_rocm -iquotebazel-out/k8-opt/bin/external/local_config_rocm -iquoteexternal/local_config_tensorrt -iquotebazel-out/k8-opt/bin/external/local_config_tensorrt -iquoteexternal/llvm-project -iquotebazel-out/k8-opt/bin/external/llvm-project -iquoteexternal/llvm_terminfo -iquotebazel-out/k8-opt/bin/external/llvm_terminfo -iquoteexternal/llvm_zlib -iquotebazel-out/k8-opt/bin/external/llvm_zlib -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SubElementInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithmeticBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithmeticCanonicalizationIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithmeticOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CopyOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ComplexBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ComplexOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TilingInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AllocationOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BufferizableOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BufferizationBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BufferizationOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MathBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MathOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/mlir/include -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=native' '-mfpmath=both' '-std=c++14' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/stream_executor/tpu/c_api_conversions.cc -o bazel-out/k8-opt/bin/tensorflow/stream_executor/tpu/_objs/c_api_conversions/c_api_conversions.pic.o)
# Configuration: cb47f8d13e3125d32e10218a9865bded6972cd74ed0740dd8e7d96b1926d8feb
# Execution platform: @local_execution_config_platform//:platform
tensorflow/stream_executor/tpu/c_api_conversions.cc: In instantiation of 'void ApiConverter::CreateVectorBase(absl::lts_20211102::Span<T>, DstList*) [with Src = const float; Dst = float; DstList = FloatList]':
tensorflow/stream_executor/tpu/c_api_conversions.cc:175:66:   required from here
tensorflow/stream_executor/tpu/c_api_conversions.cc:164:15: error: cannot convert 'float*' to 'float_t*' {aka 'long double*'} in assignment
  164 |     dst->heap = new Dst[dst->size];
      |     ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~
tensorflow/stream_executor/tpu/c_api_conversions.cc: In instantiation of 'absl::lts_20211102::Span<const T> ApiConverter::MakeSpanBase(const SrcList&) [with Dst = float; Src = float; SrcList = FloatList]':
tensorflow/stream_executor/tpu/c_api_conversions.cc:214:56:   required from here
tensorflow/stream_executor/tpu/c_api_conversions.cc:203:58: error: cannot convert 'const float_t*' {aka 'const long double*'} to 'const float*' in initialization
  203 |   const Src* src = src_list.size > TPU_C_API_MAX_INLINED ? src_list.heap
      |                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
      |                                                          |
      |                                                          const float_t* {aka const long double*}
  204 |                                                          : &src_list.inlined[0];
      |                                                          ~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 5.973s, Critical Path: 4.90s
INFO: 2 processes: 2 internal.
FAILED: Build did NOT complete successfully

thanks"
54289,TFLite 2.8.0 iOS framework cplusplus build not c,"

**System information**
- iOS framework generation
- build from source
- tflite 2.8.0

**How to build for iOS c++ not c?**

I have common tflite code for mac, win, android and iOS which was developed with c++. 

eg : 
```
#include ""tensorflow/lite/model.h""
...
tflite::FlatBufferModel::BuildFromBuffer(.., ..);
```
But above does not compile with TFLite framework I built with following. My guess is it's only c not c++.
`bazel build --config=ios_fat -c opt tensorflow/lite/ios:TensorFlowLiteC_framework`

How to build tflite iOS framework for c++?

Thanks


"
54288,What is 'will be cast to bool' mean?,"Hello, I am using `tf.keras.metrics` and confuse with the annotation for y_true in the source code:
```
def update_state(self, y_true, y_pred, sample_weight=None):
    """"""Accumulates true positive and false positive statistics.
    Args:
      y_true: The ground truth values, with the same dimensions as `y_pred`.
        Will be cast to `bool`.
      y_pred: The predicted values. Each element must be in the range `[0, 1]`.
      sample_weight: Optional weighting of each example. Defaults to 1. Can be a
        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must
        be broadcastable to `y_true`.
```
What is the meaning of  Will be cast to `bool`?  I want to see the precision of each step, does it means that I can use it on a float model output(0~1) and it will  change it to 0 or 1 by threshold both for y_true and y_pred?  But the demo below shows a different case. 
```
import tensorflow as tf

y_true = [0, 1, 0.6, 0]
y_pre =  [1, 1, 0.3, 1]

ms_name = ['TruePositives', 'TrueNegative', 'FalsePositives', 'FalseNegative']
ms = [   
    tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(),
    tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()
]
for m_name, m in zip(ms_name, ms):
    m.update_state(y_true, y_pre)
    res = m.result().numpy()
    del m
    print('\n', m_name, res)


metric_name = ['precision']
metrics = [tf.keras.metrics.Precision(0.1)]

for m_name, m in zip(metric_name, metrics):
    m.update_state(y_true, y_pre)
    res = m.result().numpy()
    del m
    print('\n', m_name, res)
```
"
54287,TFlite model shows different output shape than TF for simple MLP,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04**
- TensorFlow installation (pip package or built from source): **pip**
- TensorFlow library (version, if pip package or github SHA, if built from source): **v2.8.0-rc1-32-g3f878cff5b6 2.8.0**

### 2. Code
Standalone code to reproduce the issue can be found in the Colab gist [here](https://colab.research.google.com/gist/moberweger/aaadb484f3829dfab4e3e82b83212c02/untitled1.ipynb#scrollTo=clDg3HuITEJj)

### 3. Failure after conversion
The model conversion is successful, but the output shape of the generated model seems wrong. Note that the shape of the inference is correct, but the printed shapes are different.
There are two identical TF models exported (modelling an MLP as a conv and as a fully-connected layer). Both models have an output shape of `(None, 128, 2)` on the TF side.
The TFlite models show two different shapes (see `shape` and `shape_signature`) `[  1, 128,   2]` vs. `[1, 1, 2]`. Since the models are the same on the TF side, they should also match on the TFlite side.
The issue could be caused by some broadcasting/reshaping in the `Dense` layer (`tensordot`), since it looks like the second dimension is changed to ""dynamic"" after that layer.

"
54286,module 'tensorflow.compat.v2.__internal__' has no attribute 'monitoring',"Hello,

when following the tensor flow tutorial (https://www.youtube.com/watch?v=tPYj3fFJGjk at 1:49:00) I get the following error:
AttributeError: module 'tensorflow.compat.v2.__internal__' has no attribute 'monitoring'.
So far I have only found one hint online, suggesting to uninstall tensorflow and reinstall tensorflow-cpu, yet this did not solve the problem.
Can anyone come up with a working solutoin? Would you need more information?

Here my system specs:
- Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.4.1
- Numpy: 1.21.2
- Python version: 3.8.12
- IPython 7.29.0

"
54285,Memory leak after model.fit is called in tf 2.7 and 2.8 and training does not start,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: custom code
- OS Platform and Distribution: Ubuntu 18 (google colab)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7, 2.8, 2.9.0-dev20220203
- Python version: 3.7, 3.9, 3.10
- CUDA/cuDNN version: Build cuda_11.1.TC455_06.29190527_0
- GPU model and memory: varies

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I'm using my yolo [implementation](https://github.com/alternativebug/yolo-tf2) which used to work fine on tensorflow versions prior to 2.5. I tried recently training yolo3 on a small dataset (which uses `tf.keras.Model.fit`). Here's a colab [notebook](https://colab.research.google.com/drive/18jCTQajjgBO2bmKekaI5frrluJ2CX4NN?usp=sharing) which you can use to reproduce the issue. Shortly after `model.fit` is called, the following 2 messages keep repeating in no particular order:

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)

and

    INFO:tensorflow:Assets written to: ram://eefa3127-ad7d-4445-a186-75fd8f0b81e1/assets

They keep repeating, memory usage keeps growing and eventually a memory crash occurs. (which doesn't happen in earlier tensorflow versions <= 2.5). You can verify so using this other [notebook](https://colab.research.google.com/drive/1a3RAhVA3pCTQj2FyGY2df7vum27pj69a?usp=sharing) which uses tensorflow 2.5 instead, things should go perfectly fine and training goes as expected. I also tried installing tensorflow 2.8 instead of colab's default version (2.7) and the issue persists.

**Describe the expected behavior**

I'm expecting to see the following immediately after `model.fit` is called:

    Epoch 1/100

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes with enough info
- Briefly describe your candidate solution(if contributing): will describe one when I have one.

**Standalone code to reproduce the issue**
* [Notebook](https://colab.research.google.com/drive/18jCTQajjgBO2bmKekaI5frrluJ2CX4NN?usp=sharing) with error
* [Notebook](https://colab.research.google.com/drive/1a3RAhVA3pCTQj2FyGY2df7vum27pj69a?usp=sharing) without error

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Here's the output containing problems (notebook 1):

    2022-02-07 05:52:00,476 yolo_tf2.utils.common.activate_gpu +325: INFO     [260] GPU activated
    2022-02-07 05:52:00,477 yolo_tf2.utils.common.train +468: INFO     [260] Starting training ...
    2022-02-07 05:52:04,293 yolo_tf2.utils.common.create_models +447: INFO     [260] Training and inference models created
    2022-02-07 05:52:04,295 yolo_tf2.utils.common.wrapper +64: INFO     [260] create_models execution time: 3.8118433569999866 seconds
    2022-02-07 05:52:04,301 yolo_tf2.utils.common.create_new_dataset +366: INFO     [260] Generating new dataset ...
    2022-02-07 05:52:07,014 yolo_tf2.utils.common.adjust_non_voc_csv +184: INFO     [260] Adjustment from existing received 10107 labels containing 16 classes
    2022-02-07 05:52:07,022 yolo_tf2.utils.common.adjust_non_voc_csv +187: INFO     [260] Added prefix to images: /content/yolo-data/images
    Parsed labels:
    Car               3153
    Pedestrian        1418
    Palm Tree         1379
    Traffic Lights    1269
    Street Sign       1109
    Street Lamp        995
    Road Block         363
    Flag               124
    Trash Can           90
    Minivan             68
    Fire Hydrant        52
    Bus                 43
    Pickup Truck        20
    Bicycle             17
    Delivery Truck       4
    Motorcycle           3
    Name: object_name, dtype: int64
    2022-02-07 05:52:09,513 yolo_tf2.utils.common.save_fig +33: INFO     [260] Saved figure /content/output/plots/Relative width and height for 10107 boxes..png
    /usr/local/lib/python3.7/dist-packages/yolo_tf2/utils/dataset_handlers.py:209: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
      groups = np.array(data.groupby('image_path'))
    Processing beverly_hills_train.tfrecord
    Building example: 406/411 ... Beverly_hills184.jpg 99% completed2022-02-07 05:52:12,922 yolo_tf2.utils.common.save_tfr +227: INFO     [260] Saved training TFRecord: /content/data/tfrecords/beverly_hills_train.tfrecord
    Building example: 411/411 ... Beverly_hills365.jpg 100% completed
    Processing beverly_hills_test.tfrecord
    Building example: 31/46 ... Beverly_hills335.jpg 67% completed2022-02-07 05:52:13,175 yolo_tf2.utils.common.save_tfr +229: INFO     [260] Saved validation TFRecord: /content/data/tfrecords/beverly_hills_test.tfrecord
    2022-02-07 05:52:13,271 yolo_tf2.utils.common.read_tfr +263: INFO     [260] Read TFRecord: /content/data/tfrecords/beverly_hills_train.tfrecord
    Building example: 46/46 ... Beverly_hills186.jpg 100% completed
    2022-02-07 05:52:18,892 yolo_tf2.utils.common.read_tfr +263: INFO     [260] Read TFRecord: /content/data/tfrecords/beverly_hills_test.tfrecord
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    2022-02-07 05:52:50.575910: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
    INFO:tensorflow:Assets written to: ram://eefa3127-ad7d-4445-a186-75fd8f0b81e1/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://cbe6d5a4-5322-494b-ba91-3fd34131cdd9/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://f15f3f25-9adb-4eb0-aa0d-83fa874bc74e/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://86dd6f5f-4416-4465-99c0-928fd88e8a93/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://ca08220f-cabc-4017-96d3-383557342388/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://0f634207-e822-4d6c-a805-3cfeab37532f/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://a971d021-3da4-402a-a004-4ae4aa67148a/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://31d72fdf-1ce6-4131-a7e6-f6444747e9c9/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://dac323b6-591a-481c-bbe6-85bb82bef38c/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://99b029f7-11d1-40f2-b459-fd1d8dca5ba1/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://210489fb-0895-4769-8be3-effd01d92695/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)

Here's the output without the problem (notebook 2):

    2022-02-07 06:09:53.125735: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
    2022-02-07 06:09:55.370728: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
    2022-02-07 06:09:55,387 yolo_tf2.utils.common.train +468: INFO     [269] Starting training ...
    2022-02-07 06:09:55.387211: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
    2022-02-07 06:09:55.387252: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (de0312867ce7): /proc/driver/nvidia/version does not exist
    2022-02-07 06:09:55.427963: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
    2022-02-07 06:10:00,078 yolo_tf2.utils.common.create_models +447: INFO     [269] Training and inference models created
    2022-02-07 06:10:00,080 yolo_tf2.utils.common.wrapper +64: INFO     [269] create_models execution time: 4.689235652999997 seconds
    2022-02-07 06:10:00,081 yolo_tf2.utils.common.create_new_dataset +366: INFO     [269] Generating new dataset ...
    2022-02-07 06:10:02,572 yolo_tf2.utils.common.adjust_non_voc_csv +184: INFO     [269] Adjustment from existing received 10107 labels containing 16 classes
    2022-02-07 06:10:02,574 yolo_tf2.utils.common.adjust_non_voc_csv +187: INFO     [269] Added prefix to images: /content/yolo-data/images
    Parsed labels:
    Car               3153
    Pedestrian        1418
    Palm Tree         1379
    Traffic Lights    1269
    Street Sign       1109
    Street Lamp        995
    Road Block         363
    Flag               124
    Trash Can           90
    Minivan             68
    Fire Hydrant        52
    Bus                 43
    Pickup Truck        20
    Bicycle             17
    Delivery Truck       4
    Motorcycle           3
    Name: object_name, dtype: int64
    2022-02-07 06:10:04,900 yolo_tf2.utils.common.save_fig +33: INFO     [269] Saved figure /content/output/plots/Relative width and height for 10107 boxes..png
    /usr/local/lib/python3.7/dist-packages/yolo_tf2/utils/dataset_handlers.py:209: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
      groups = np.array(data.groupby('image_path'))
    Processing beverly_hills_train.tfrecord
    Building example: 392/411 ... Beverly_hills294.jpg 95% completed2022-02-07 06:10:10,341 yolo_tf2.utils.common.save_tfr +227: INFO     [269] Saved training TFRecord: /content/data/tfrecords/beverly_hills_train.tfrecord
    Building example: 411/411 ... Beverly_hills94.jpg 100% completed
    Processing beverly_hills_test.tfrecord
    Building example: 25/46 ... Beverly_hills334.jpg 54% completed2022-02-07 06:10:10,730 yolo_tf2.utils.common.save_tfr +229: INFO     [269] Saved validation TFRecord: /content/data/tfrecords/beverly_hills_test.tfrecord
    Building example: 46/46 ... Beverly_hills251.jpg 100% completed
    2022-02-07 06:10:10,843 yolo_tf2.utils.common.read_tfr +263: INFO     [269] Read TFRecord: /content/data/tfrecords/beverly_hills_train.tfrecord
    2022-02-07 06:10:15,264 yolo_tf2.utils.common.read_tfr +263: INFO     [269] Read TFRecord: /content/data/tfrecords/beverly_hills_test.tfrecord
    2022-02-07 06:10:15.676352: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.
    2022-02-07 06:10:15.676423: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.
    2022-02-07 06:10:15.701051: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      category=CustomMaskWarning)
    2022-02-07 06:10:17.064324: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
    2022-02-07 06:10:17.081408: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz
    Epoch 1/100
          1/Unknown - 40s 40s/step - loss: 7333.2617 - layer_205_lambda_loss: 403.8862 - layer_230_lambda_loss: 1509.9465 - layer_255_lambda_loss: 5407.68902022-02-07 06:10:59.974130: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.
    2022-02-07 06:10:59.974196: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.
          2/Unknown - 50s 11s/step - loss: 7819.7124 - layer_205_lambda_loss: 697.4546 - layer_230_lambda_loss: 1647.7856 - layer_255_lambda_loss: 5462.71582022-02-07 06:11:10.059899: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.
    2022-02-07 06:11:10.088821: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.
    2022-02-07 06:11:10.133747: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10
    2022-02-07 06:11:10.157875: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.trace.json.gz
    2022-02-07 06:11:10.189438: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10
    2022-02-07 06:11:10.189678: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.memory_profile.json.gz
    2022-02-07 06:11:10.192796: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10Dumped tool data for xplane.pb to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.xplane.pb
    Dumped tool data for overview_page.pb to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.overview_page.pb
    Dumped tool data for input_pipeline.pb to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.input_pipeline.pb
    Dumped tool data for tensorflow_stats.pb to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.tensorflow_stats.pb
    Dumped tool data for kernel_stats.pb to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.kernel_stats.pb
    
         15/Unknown - 181s 10s/step - loss: 3493.4009 - layer_205_lambda_loss: 232.7220 - layer_230_lambda_loss: 629.6332 - layer_255_lambda_loss: 2618.9722
"
54284,Tensorflow unsupported within MSYS2?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): wanted binary .whl via `python3 -m pip install tensorflow`
- TensorFlow version: any would be fine, was trying for 2.6.3
- Python version: 3.9
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: no GPU

**Describe the problem**

On MSYS2, using pip via the mingw64-compiled Python, the message is
```
# python3 -m pip install --upgrade https://files.pythonhosted.org/packages/f2/ff/32e0f1e039fb6cabc88a5e03a0a2b2f01cd50b69cc1b915daf80ca0345c5/tensorflow-2.6.3-cp39-cp39-win_amd64.whl
ERROR: tensorflow-2.6.3-cp39-cp39-win_amd64.whl is not a supported wheel on this platform.
```

It's the same if I just use the name eg `python3 -m pip install tensorflow`.

I have an extensive codebase that currently depends on MSYS2 and gcc in the Windows context, and we have a module that successfully uses Tensorflow on our Linux version of the code. As above, however, I can't use Tensorflow via Python under MSYS2 using the version Python that MSYS2 provides.

I wonder if I could get some clarity on what the options are for Windows/MSYS2 therefore:

- can tensorflow be built on Windows using gcc in MSYS2? The instructions, even if they use MSYS2, seem to be referring to MSVC as the compilier. What are the reasons why 'full' MSYS2 support is not possible? *
- should I instead switch, if possible, to building my software to link against Windows Python instead of MSYS2 Python. This would be less convenient for users, but might work.
- would it be possible for the situation regarding MSYS2 support to be clarified in the MSYS2 documentation somehow?

[*] I note that https://www.tensorflow.org/install/source_windows says that there is some issue with path support in Windows by Bazel, but obviously Tensorflow builds find on Linux, where such paths exist. Is this a Bazel configuration issue?


"
54283,UnknownError: Failed to get convolution algorithm,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): *Linux Debian 10 @ GCP Vertex AI*
- TensorFlow installed from (source or binary): *Vertex AI Environment TensorFlow Enterprise 2.7 (Intel® MKL-DNN/MKL)*
- TensorFlow version (use command below): *TensorFlow Enterprise 2.7 (Intel® MKL-DNN/MKL)*
- Python version: *Python 3.7.12*
- Bazel version (if compiling from source): *N/A*
- GCC/Compiler version (if compiling from source): *N/A*
- CUDA/cuDNN version: *CUDA 11.2, cuDNN 8.3.0 (default from the Environment)*

- GPU model and memory: tested at both NVidia Tesla K80 and NVidia Tesla P3

**Describe the current behavior**

We get the following error (completely by random during training pipeline). Can appear during 1st batch or 200 batch of 1st epoch.

```
UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
[[node spatio_ temporal _autoencoder/sequential/chong_tay_encoder/conv3d/Conv3D
(defined at /opt/conda/lib/python3.7/site-packages/keras/layers/convolutional.py:238)
11 [Op: inference predict function 4511]
```

**Describe the expected behavior**

Everything working?

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

To be honest, I have no idea... The issue is only visible on the Vertex AI, on Colab and our machines everything is okay... I am unable to share code, as we are using confidential datasets, that cannot be shared or easily reproduced.

**Other info / logs**

What we tried to do:
- remove `$HOME/.nv` folder
- reinstalled whole environment
- changed parameters of model to ensure that there is no memory shortage

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/8009147/tf_env.txt)
"
54282,TFLite model working in multi-processing but not multi-threading,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur (M1)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary (pip package)
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.12
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: M1 Chip

**Describe the current behavior**
Tensorflow lite model doesn't work when using multi-threading using `multiprocessing.dummy.Pool` in Python, while it does work in multi-processing.

**Describe the expected behavior**
Tensorflow Lite model should have worked in both.

**Standalone code to reproduce the issue**
(Sample TFLite model attached here: [model.tflite.zip](https://github.com/tensorflow/tensorflow/files/8008907/model.tflite.zip))

```
import math
import numpy as np
import os
import tensorflow as tf
import time

#use below one for multi-threading
from multiprocessing.dummy import Pool 

#use below one for multi-processing
#from multiprocessing import Pool 

from multiprocessing import cpu_count
from tqdm import tqdm

interpreter = None
input_index = None
output_index = None

def init_interpreter(model_path):
    global interpreter
    global input_index
    global output_index
    interpreter = tf.lite.Interpreter(model_path=model_path)
    input_index = interpreter.get_input_details()
    output_index = interpreter.get_output_details()
    interpreter.allocate_tensors()
    print('done init')

def get_prediction(img_name):
    img = np.random.randn(1, 256, 256, 3).astype(np.float32)
    interpreter.set_tensor(input_index[0][""index""], img)
    interpreter.invoke()
    pred = interpreter.tensor(output_index[0][""index""])()[0][0]
    return pred

def main():
    samples = list(range(100))
    model_path = 'model.tflite'
    with Pool(processes=2, initializer=init_interpreter, initargs=(model_path,)) as pool:
        preds = list(tqdm(pool.imap(get_prediction, samples)))

if __name__ == '__main__':
    main()

```

**Other info / logs**  It gives the following error when using TFLite model with multi-threading
```
RuntimeError: There is at least 1 reference to internal data
      in the interpreter in the form of a numpy array or slice. Be sure to
      only hold the function returned from tensor() if you are using raw
      data access.
```

The problem that we are working on is memory sensitive and requires many models to be loaded into the memory and thus, we can't use multi-processing as it leads to the same model being loaded as many times as the number of processes. Is there any way to use TFLite models in multi-threading? Any help would be much appreciated. Thanks!"
54281,"train_step method of custom model is not called in graph execution (i.e., non eager) mode","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, but I modified only a minor portion from the stock example.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA.
- TensorFlow installed from (source or binary): via `pip install tensorflow tensorflow-gpu keras`, probably from binary.
- TensorFlow version (use command below): `v2.8.0-rc1-32-g3f878cff5b6 2.8.0`
- Python version: `Python 3.9.7`
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA 11.5
- GPU model and memory: NVIDIA GeForce GTX 1060 6GB.

**Describe the current behavior**
Custom model's `train_step` is not being used in non-eager execution mode.

**Describe the expected behavior**
Custom model's `train_step` is used regardless of whether eager execution is enabled or not.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

```python
import keras
import numpy as np
import tensorflow as tf


class CustomModel(keras.Model):
	def train_step(self, data):
		return {m.name: m.result() for m in self.metrics}


if __name__ == '__main__':
	# config
	# tf.compat.v1.enable_eager_execution()
	tf.compat.v1.disable_eager_execution()

	print(""TensorFlow version: {}"".format(tf.__version__))
	print(""Eager execution: {}"".format(tf.executing_eagerly()))

	# Construct and compile an instance of CustomModel
	inputs = keras.Input(shape=(32,))
	outputs = keras.layers.Dense(1)(inputs)
	model = CustomModel(inputs, outputs)
	model.compile(optimizer=""adam"", loss=""mse"", metrics=[""mae""])

	# Just use `fit` as usual
	x = np.random.random((1000, 32))
	y = np.random.random((1000, 1))

	print(model.evaluate(x, y))
	model.fit(x, y, epochs=3)
	print(model.evaluate(x, y))
```

**Other info / logs**

When eager execution is enabled, `train_step` gets called, which means the model **isn't** trained as **expected**.

```
32/32 [==============================] - 0s 1ms/step - loss: 0.3040 - mae: 0.4428
[0.3039644658565521, 0.442813515663147]
Epoch 1/3
32/32 [==============================] - 0s 874us/step - loss: 0.0000e+00 - mae: 0.0000e+00
Epoch 2/3
32/32 [==============================] - 0s 810us/step - loss: 0.0000e+00 - mae: 0.0000e+00
Epoch 3/3
32/32 [==============================] - 0s 762us/step - loss: 0.0000e+00 - mae: 0.0000e+00
32/32 [==============================] - 0s 1ms/step - loss: 0.3040 - mae: 0.4428
[0.3039644658565521, 0.442813515663147]
```

When eager execution is disabled, `train_step` is ignored, and the model is trained normally and `train_step` is ignored.
This is **not expected**.

```
[0.26861959040164946, 0.41127136]
Train on 1000 samples
Epoch 1/3
1000/1000 [==============================] - 0s 71us/sample - loss: 0.2598 - mae: 0.4037
Epoch 2/3
1000/1000 [==============================] - 0s 40us/sample - loss: 0.2432 - mae: 0.3912
Epoch 3/3
1000/1000 [==============================] - 0s 37us/sample - loss: 0.2296 - mae: 0.3805
[0.2220638926625252, 0.3743403]
```

related issues: #45922 #40880

the snippet is modified from stock example [here](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/#a_first_simple_example).
"
54280,Unintuitive error in learning rate type when saving then loading weights after further fitting,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: Ubuntu 20.04.3 LTS (or the colab-env)
- TensorFlow installed from (source or binary): binary (through pip) (or the colab-env)
- TensorFlow version: 2.7.0
- Python version: 3.8.10 (or the colab-env)
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

The following procedure causes a fairly unintuitive `tensorflow.python.framework.errors_impl.InvalidArgumentError`:

1. Setup model
2. Compile model
3. Fit model with e.g. learning-rate scheduler callback (saving the history)
4. Save weights of model
5. Compile model using `learning_rate` from last epoch from previous fit (using saved history)
6. Fit model
7. Load the saved weights, which triggers `InvalidArgumentError `

see linked colab notebook for details.

What happens is that the type for the learning-rate changes when compiling the model the second time, causing a mismatch and the saved learning-rate fails to load. This appears unintuitive since both types for the learning-rate are apparently valid (can be used to perform a fit), but you cannot load if you accidentally changed the type.

**Describe the expected behavior**

Step 7. in the above should not produce an error.

**Standalone code to reproduce the issue**

This example code:

```python
import tensorflow as tf

X, y = tf.random.uniform((50,)), tf.random.uniform((50,))
X_val, y_val = tf.random.uniform((5,)), tf.random.uniform((5,))

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule=lambda epoch,lr: lr*0.995, verbose=1)

model = tf.keras.models.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])

model.compile(optimizer=tf.keras.optimizers.SGD(), loss='mse')

history = model.fit(X, y, epochs=1, validation_data=(X_val,y_val), callbacks=[lr_scheduler])

model.save_weights('tmp_model')

model.compile(
    optimizer=tf.keras.optimizers.SGD(learning_rate=history.history['lr'][-1]*0.1),
    #optimizer=tf.keras.optimizers.SGD(learning_rate=float(history.history['lr'][-1]*0.1)), # work-around
    loss='mse'
)

model.fit(X, y, epochs=1, validation_data=(X_val,y_val), callbacks=[lr_scheduler])
model.load_weights('tmp_model')
```
outputs:
```
Epoch 00001: LearningRateScheduler setting learning rate to 0.00994999977760017.
2/2 [==============================] - 0s 126ms/step - loss: 0.6959 - val_loss: 0.9985 - lr: 0.0099

Epoch 00001: LearningRateScheduler setting learning rate to 0.0009900249862112105.
2/2 [==============================] - 0s 157ms/step - loss: 0.6430 - val_loss: 0.9910 - lr: 9.9002e-04
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-3d1de671fc08> in <module>()
     21 
     22 model.fit(X, y, epochs=1, validation_data=(X_val,y_val), callbacks=[lr_scheduler])
---> 23 model.load_weights(f'tmp_model')

1 frames
/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57     ctx.ensure_initialized()
     58     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 59                                         inputs, attrs, num_outputs)
     60   except core._NotOkStatusException as e:
     61     if name is not None:

InvalidArgumentError: tensor_name = optimizer/learning_rate/.ATTRIBUTES/VARIABLE_VALUE; expected dtype double does not equal original dtype float [Op:RestoreV2]
```

notebook with code:

https://colab.research.google.com/drive/1FxwBUjUcw0U0FiL1LDGFbfCY7ocGQ8ij?usp=sharing

"
54279,Extremely slow grads computation in dilatied convlution with a large dilation rate,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux & Windows & colab
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below): 2.7, 2.8rc, 2.8
- Python version: 3.8
- GPU model and memory: Titan RTX

https://colab.research.google.com/drive/1-64VeSW-3SB3rS4mrzUzU1FnubX5TvnD?usp=sharing

Suggest to try both CPU and GPU.

In CPU mode, large dilation rate can run 100x slower than smaller rate.
"
54277,Option to avoid caching with bijectors.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.7.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
The current behaviour for `tensorflow_probability.bijectors.Bijector` is to cache the input to be used if the inverse function is called. I would very much like a keyword being able to turn this caching off to ensure the inverse function of the bijector is always called. Whilst setting the property of `_is_injective` can accomplish this, you lose other features.

**Will this change the current api? How?**
Yes, a keyword in the definition of the Bijector class.

**Who will benefit with this feature?**
Anyone who wishes to have bijectors where the inverse function is always called rather than the input that's been cached.

**Any Other info.**
"
54276,Deterministic GPU implementation of unsorted segment reduction op not available on Windows,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 21H2
- TensorFlow installed from (source or binary): from PyPI
- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0
- Python version: 3.10.2
- CUDA/cuDNN version: 11.2, 8.1.1
- GPU model and memory: GeForce RTX 2060

**Describe the current behavior**
The code below works on Linux, but not on Windows where I am seeing

> tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:
> Detected at node 'UnsortedSegmentSum_1' defined at (most recent call last):
> Node: 'UnsortedSegmentSum_1'
> Deterministic GPU implementation of unsorted segment reduction op not available.
>          [[{{node UnsortedSegmentSum_1}}]] [Op:__inference_train_function_517]

**Describe the expected behavior**
It works on both OSs.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
tf.random.set_seed(0)
tf.config.experimental.enable_op_determinism()
data = tf.ones((1, 1))
layer = tf.keras.layers.Input(shape=[1])
model = tf.keras.models.Model(inputs=layer, outputs=layer)
model.compile(loss=""categorical_crossentropy"", metrics=""AUC"")
model.fit(x=data, y=data)
```

This is due to the `AUC` metric as discussed in https://github.com/tensorflow/tensorflow/issues/51978. It was resolved for Linux, but not Windows in https://github.com/tensorflow/tensorflow/pull/51861. A workaround is given by `set TF_DISABLE_SEGMENT_REDUCTION_OP_DETERMINISM_EXCEPTIONS=True`. I am posting a new issue here as recommended in https://github.com/tensorflow/tensorflow/issues/39751#issuecomment-982919265."
54274,AutoGraph could not transform <function ...>,"```
INFO:tensorflow:<function train_main.<locals>.train at 0x2976dd700> is not cached for subkey ConversionOptions[{}]
INFO:tensorflow:Source code of <function train_main.<locals>.train at 0x2976dd700>:

@tf.function
def train(batch):
    s    = batch[:, start_s:    end_s]
    a    = batch[:, start_a:    end_a]
    s_   = batch[:, start_s_:   end_s_]
    r    = batch[:, start_r:    end_r]
    done = batch[:, start_done: end_done]

    noise = tf.random.normal([args.batch_size, env.action_dim])
    a_, log_π_ = actor([s, noise])
    y = r + args.gamma * (1-done) * (
    tf.minimum(critic_1_target([s_, a_]),
    critic_2_target([s_, a_]))
    - args.alpha * log_π_)

    with tf.GradientTape() as tape:
        MSBE_1 = (1/args.batch_size) * tf.reduce_sum((critic_1([s, a]) - y)**2)
    MSBE_1_grads = tape.gradient(MSBE_1, critic_1.trainable_weights)
    critic_1_optimizer.apply_gradients(zip(MSBE_1_grads, critic_1.trainable_weights))

    with tf.GradientTape() as tape:
        MSBE_2 = (1/args.batch_size) * tf.reduce_sum((critic_2([s, a]) - y)**2)
    MSBE_2_grads = tape.gradient(MSBE_2, critic_2.trainable_weights)
    critic_2_optimizer.apply_gradients(zip(MSBE_2_grads, critic_2.trainable_weights))

    noise = tf.random.normal([args.batch_size, env.action_dim])
    with tf.GradientTape() as tape:
        log_π_θ: tf.Tensor
        a_θ , log_π_θ = actor([s, noise])
        expected_reward = (1/args.batch_size) *                 tf.reduce_sum(critic_1([s, a_θ]) - args.alpha * log_π_θ)
        neg_expected_reward = -expected_reward
    expected_reward_grad = tape.gradient(neg_expected_reward, actor.trainable_weights)
    actor_optimizer.apply_gradients(zip(expected_reward_grad, actor.trainable_weights))

    polyak_average(critic_1_target.variables, critic_1.variables)
    polyak_average(critic_2_target.variables, critic_2.variables)

    if args.model_name:
        MSBE_1_log(MSBE_1)
        MSBE_2_log(MSBE_2)
        expected_reward_log(expected_reward)


INFO:tensorflow:Error transforming entity <function train_main.<locals>.train at 0x2976dd700>
Traceback (most recent call last):
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 433, in converted_call
    converted_f = _convert_actual(target_entity, program_ctx)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 275, in _convert_actual
    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 286, in transform
    return self.transform_function(obj, user_context)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 470, in transform_function
    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 363, in transform_function
    result = self.transform_ast(node, context)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 243, in transform_ast
    node = self.initial_analysis(node, ctx)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 231, in initial_analysis
    node = activity.resolve(node, ctx, None)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 709, in resolve
    return ActivityAnalyzer(context, parent_scope).visit(node)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 445, in visit
    result = super(Base, self).visit(node)
  File ""/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 601, in visit_FunctionDef
    node.body = self.visit_block(node.body)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 340, in visit_block
    replacement = self.visit(node)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 445, in visit
    result = super(Base, self).visit(node)
  File ""/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 651, in visit_With
    node = self.generic_visit(node)
  File ""/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py"", line 483, in generic_visit
    value = self.visit(value)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 445, in visit
    result = super(Base, self).visit(node)
  File ""/opt/homebrew/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 394, in visit_AnnAssign
    node.value = self.visit(node.value)
  File ""/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 431, in visit
    raise ValueError(msg)
ValueError: invalid value for ""node"": expected ""ast.AST"", got ""<class 'NoneType'>""; to visit lists of nodes, use ""visit_block"" instead
WARNING:tensorflow:AutoGraph could not transform <function train_main.<locals>.train at 0x2976dd700> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: invalid value for ""node"": expected ""ast.AST"", got ""<class 'NoneType'>""; to visit lists of nodes, use ""visit_block"" instead
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (`pip3 install tensorflow-macos`)
- TensorFlow version (use command below): `unknown 2.7.0`
- Python version: 3.9.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Sorry, but I can't provide the full source code.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54273,Build broken on AARCH64 by XNNPACK update,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version:3.8.10
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 5.0.0
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

build_pip_package fails with

ERROR: /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/XNNPACK/BUILD.bazel:9264:19: Compiling src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc failed: (Exit 1): gcc failed: error executing command 
  (cd /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/lib64:/usr/local/lib \
    PATH=/home/builder/.cache/bazelisk/downloads/bazelbuild/bazel-5.0.0-linux-arm64/bin:/home/builder/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
  /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-opt/bin/external/XNNPACK/_objs/jit/1/1x8-aarch64-neonfma-cortex-a75.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/XNNPACK/_objs/jit/1/1x8-aarch64-neonfma-cortex-a75.pic.o' -fPIC '-DXNN_LOG_LEVEL=0' -DPTHREADPOOL_NO_DEPRECATED_API -iquoteexternal/XNNPACK -iquotebazel-out/aarch64-opt/bin/external/XNNPACK -iquoteexternal/FP16 -iquotebazel-out/aarch64-opt/bin/external/FP16 -iquoteexternal/clog -iquotebazel-out/aarch64-opt/bin/external/clog -iquoteexternal/pthreadpool -iquotebazel-out/aarch64-opt/bin/external/pthreadpool -iquoteexternal/FXdiv -iquotebazel-out/aarch64-opt/bin/external/FXdiv -iquoteexternal/cpuinfo -iquotebazel-out/aarch64-opt/bin/external/cpuinfo -Ibazel-out/aarch64-opt/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/aarch64-opt/bin/external/clog/_virtual_includes/clog -Ibazel-out/aarch64-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/aarch64-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/aarch64-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/XNNPACK/include -isystem bazel-out/aarch64-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/aarch64-opt/bin/external/XNNPACK/src -isystem external/FP16/include -isystem bazel-out/aarch64-opt/bin/external/FP16/include -isystem external/pthreadpool/include -isystem bazel-out/aarch64-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/aarch64-opt/bin/external/FXdiv/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -Iinclude -Isrc -O2 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc -o bazel-out/aarch64-opt/bin/external/XNNPACK/_objs/jit/1/1x8-aarch64-neonfma-cortex-a75.pic.o)
# Configuration: ce8f4d34c0d44ddff120a1f5ad2cdfaaab653410aefcb8fe64f7a37d66c62619
# Execution platform: @local_execution_config_platform//:platform
external/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc: In member function 'void xnnpack::aarch64::{anonymous}::Generator::generate(bool, size_t, size_t, size_t, float, float)':
external/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc:48:39: error: 'numeric_limits' is not a member of 'std'
   48 |   const bool clamp_min = min != -std::numeric_limits<float>::infinity();
      |                                       ^~~~~~~~~~~~~~
external/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc:48:54: error: expected primary-expression before 'float'
   48 |   const bool clamp_min = min != -std::numeric_limits<float>::infinity();
      |                                                      ^~~~~
external/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc:49:39: error: 'numeric_limits' is not a member of 'std'
   49 |   const bool clamp_max = max != +std::numeric_limits<float>::infinity();
      |                                       ^~~~~~~~~~~~~~
external/XNNPACK/src/f32-igemm/1x8-aarch64-neonfma-cortex-a75.cc:49:54: error: expected primary-expression before 'float'
   49 |   const bool clamp_max = max != +std::numeric_limits<float>::infinity();
      |                                                      ^~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 387.892s, Critical Path: 129.24s
INFO: 2857 processes: 206 internal, 2651 local.
FAILED: Build did NOT complete successfully


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Seems to be caused by 
https://github.com/tensorflow/tensorflow/commit/b9f4c111ce9301240bd3d3b97449bc485ca52b15"
54271,model.fit() bug when using a zipped Dataset as input for a multiple-input model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.9.0.dev20220202
- Python version: 3.10.2

**Describe the current behavior**
I have a custom model which takes 3 images as input
I have 3 separate (currently unbatched as I debug this error) datasets, classes encoded as categorical, meaning each input tensor has shape ((x, y, z), (c,))
Trying to input the 3 datasets separately fails, either by inputting them as a dict mapping each ds to a named input `{""Input1"": ds1, {""Input2"": ds2, {""Input3"": ds3}`, or using a list `[ds, ds2, ds3]`

I zip the three datasets. Testing the resulting dataset with [(using the docs as guidance)](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip):
```
for element in zipped_ds.as_numpy_iterator():
print(""element"", element)
```
Outputs:
```
element [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] 
element [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] 
...

```
Seems to work, right? Every call to the iterator returns 3 elements.
Well, when I use the zipped dataset as input of model_fit(), the first element in the tuple returned by the dataset object is treated as the input for the whole model, meaning that instead of using [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] as the input to the model, it uses [[x1, y1, z1], [c1,]], and the training fails.

I've tried many approaches, like using `zipped_ds.as_numpy_iterator()` or `([ds1, ds2, ds3] for idx, (ds1, ds2, ds3) in enumerate(zipped_ds))`, but both fail as the returned item is empty

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
# %%
import os

import tensorflow as tf # tensorflow nightly, version>=2.5
from tensorflow import keras
from tensorflow.image import crop_to_bounding_box as tfimgcrop
from tensorflow.keras.preprocessing import image_dataset_from_directory

BATCH_SIZE=32 # Adjust?

IMG_SIZE=(224, 224)
IMG_SHAPE = IMG_SIZE + (3,)

# %%
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'
path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

train_dataset = tf.keras.preprocessing.image_dataset_from_directory(train_dir,
                                             shuffle=False,
                                             label_mode='categorical',
                                             batch_size=32,
                                             image_size=IMG_SIZE)
validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(validation_dir,
                                             shuffle=False,
                                             label_mode='categorical',
                                             batch_size=32,
                                             image_size=IMG_SIZE)

# %%
base_model1 = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet',
                                               minimalistic=False,
                                               pooling=max,
                                               dropout_rate=0.2)
base_model2 = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet',
                                               minimalistic=False,
                                               pooling=max,
                                               dropout_rate=0.2)
base_model3 = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet',
                                               minimalistic=False,
                                               pooling=max,
                                               dropout_rate=0.2)

# %%
pre_concat_layer1 = tf.keras.layers.Dense(64, 
                                        activation='relu', 
                                        kernel_initializer='random_uniform', 
                                        bias_initializer='zeros')
pre_concat_layer2 = tf.keras.layers.Dense(64, 
                                        activation='relu', 
                                        kernel_initializer='random_uniform', 
                                        bias_initializer='zeros')
pre_concat_layer3 = tf.keras.layers.Dense(64, 
                                        activation='relu', 
                                        kernel_initializer='random_uniform', 
                                        bias_initializer='zeros')

post_concat_layer = tf.keras.layers.Dense(128, 
                                        activation='relu', 
                                        kernel_initializer='random_uniform', 
                                        bias_initializer='zeros')
prediction_layer = tf.keras.layers.Dense(2, 
                                        activation='softmax', 
                                        kernel_initializer='random_uniform', 
                                        bias_initializer='zeros')

# %%
input1 = tf.keras.Input(shape=(64, 64, 3), name=""First"")
input2 = tf.keras.Input(shape=(64, 64, 3), name=""Second"")
input3 = tf.keras.Input(shape=(64, 64, 3), name=""Third"")

x = base_model1(input1, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = pre_concat_layer1(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = tf.keras.layers.BatchNormalization()(x)
body1 = tf.keras.Model(input1, outputs)

x = base_model2(input2, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = pre_concat_layer2(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = tf.keras.layers.BatchNormalization()(x)
body2 = tf.keras.Model(input2, outputs)

x = base_model3(input3, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = pre_concat_layer3(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = tf.keras.layers.BatchNormalization()(x)
body3 = tf.keras.Model(input3, outputs)

# %%
body1.get_layer(""MobilenetV3large"")._name = ""MobilenetV3large1""
body2.get_layer(""MobilenetV3large"")._name = ""MobilenetV3large2""
body3.get_layer(""MobilenetV3large"")._name = ""MobilenetV3large3""

# %%
combinedInput = tf.keras.layers.concatenate([body1.output, body2.output, body3.output])
x = post_concat_layer(combinedInput)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.BatchNormalization()(x)
foutput = prediction_layer(x)
final_model = tf.keras.Model(inputs=[body1.input, body2.input, body3.input], outputs=foutput)

# %%
def resize_data1(images, classes):
    return (tfimgcrop(images,
                        offset_height=0,
                        offset_width=0,
                        target_height=64,
                        target_width=64),
                    classes)
def resize_data2(images, classes):
    return (tfimgcrop(images,
                        offset_height=0,
                        offset_width=64,
                        target_height=64,
                        target_width=64),
                    classes)
def resize_data3(images, classes):
    return (tfimgcrop(images,
                        offset_height=0,
                        offset_width=128,
                        target_height=64,
                        target_width=64),
                    classes)

# %%
train_dataset_unb = train_dataset.unbatch()
train_dataset1 = train_dataset_unb.map(resize_data1)
train_dataset2 = train_dataset_unb.map(resize_data2)
train_dataset3 = train_dataset_unb.map(resize_data3)
train_dataset_zip = tf.data.Dataset.zip((train_dataset1, train_dataset2, train_dataset3))

validation_dataset_unb = validation_dataset.unbatch()
validation_dataset1 = validation_dataset_unb.map(resize_data1)
validation_dataset2 = validation_dataset_unb.map(resize_data2)
validation_dataset3 = validation_dataset_unb.map(resize_data3)
validation_dataset_zip = tf.data.Dataset.zip((validation_dataset1, validation_dataset2, validation_dataset3))

# %%
final_model.compile()

# %%
history = final_model.fit(train_dataset_zip,
                        epochs=999, 
                        validation_data=validation_dataset_zip,
                        validation_steps=32
                        )
```"
54270,Getting incorrect value of lengthOutputTensor on iOS,"**System information**
- Using a stock example script provided in TensorFlow
- iOS
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.6.0
- Python version: 3.9.10
- Bazel version (if compiling from source):3.7.2

**Current behavior**
I've used the stock example code snippet as mentioned below.

```
std::unique_ptr<tflite::FlatBufferModel> tflModel = nullptr;
tflite::ops::builtin::BuiltinOpResolver tflResolver;
std::unique_ptr<tflite::Interpreter> tflInterpreter;
tflModel = tflite::FlatBufferModel::BuildFromFile(pakgFilePath.c_str());
tflite::InterpreterBuilder(*tflModel, tflResolver)(&tflInterpreter);
tflInterpreter->AllocateTensors();

int32_t lengthInputTensor = tflInterpreter->input_tensor(0)->dims->data[1];
int32_t lengthOutputTensor = tflInterpreter->output_tensor(0)->dims->data[1];

tflInterpreter->Invoke();
```

The above code is executed on Android and iOS both but getting an incorrect value of **lengthOutputTensor** in iOS for the same file.

**Expected behavior**
The value of **lengthOutputTensor** on iOS should be the same as it's on Android for the same file.

Also, while going through the documentation, I've found below details. So is the above mentioned issue due to this work in progress for iOS?

<img width=""1344"" alt=""Screenshot 2022-02-04 at 5 57 12 PM"" src=""https://user-images.githubusercontent.com/98508996/152529171-e67e17f3-9e2b-4a84-b564-e340d4386648.png"">

"
54269,GPU delegate for tflite not finding libOpenCL when deployed on linux,"**System information**
- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow):* yes
- *OS Platform and Distribution (e.g., Linux Ubuntu 16.04):* Built on and deployed to Ubuntu 18.04 x86_64 
- *TensorFlow installed from (source or binary):* Package of tflite built from source, deployed as deb
- *TensorFlow version (use command below):* tflite v2.5.0 (2.6 was slower and we've not yet had time to assess 2.7)
- *Python version:* n/a
- *Bazel version (if compiling from source):* 4.2.1
- *GCC/Compiler version (if compiling from source):* 7.5.0-3ubuntu1~18.04
- *CUDA/cuDNN version:* n/a
- *GPU model and memory:* Deployed to systems with nvidia GTX 1070

**Describe the current behavior**
On a linux system the GPU delegate attempts to dynamically load libOpenCL but uses the ""dev"" name of the library. 
When deploying a solution on an Ubuntu system the filename for the installed OpenCL library from package ""ocl-icd-libopencl1"" is `libOpenCL.so.1`. The symlink `libOpenCL.so` would only be present via the ""ocl-icd-opencl-dev"" package which would usually be installed when building not deploying. 

**Describe the expected behavior**
When deployed it should load OpenCL from the library package and not the dev package.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/cl/opencl_wrapper.cc#L92

"
54268,Can't build libtensorflowlite.so with `--config=elinux_armhf` - external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:36:28: error: incompatible types when initializing type 'uint16x8_t' using type 'int',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.8, e994fb9c3ad250d38fd07511aaa445eda728f9af
- Python version: Python 3.8.10
- Bazel version (if compiling from source): 4.2.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0

**Describe the problem**

```
$ git clone --branch r2.8 https://github.com/tensorflow/tensorflow
$ git checkout e994fb9c3ad250d38fd07511aaa445eda728f9af
$  bazel build --verbose_failures --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=158
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:elinux_armhf in file /root/tensorflow/.bazelrc: --config=elinux --cpu=armhf --distinct_host_configuration=true
INFO: Found applicable config definition build:elinux in file /root/tensorflow/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
INFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /root/tensorflow/WORKSPACE:23:14: in <toplevel>
  /root/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/XNNPACK/BUILD.bazel:6811:19: Compiling src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c failed: (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \
  exec env - \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-4.2.1-linux-x86_64/bin:/root/bin:/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neonfp16_prod_microkernels/1/vcvt-neonfp16-x16.pic.d '-frandom-seed=bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neonfp16_prod_microkernels/1/vcvt-neonfp16-x16.pic.o' -fPIC -DPTHREADPOOL_NO_DEPRECATED_API -iquote external/XNNPACK -iquote bazel-out/armhf-opt/bin/external/XNNPACK -iquote external/FP16 -iquote bazel-out/armhf-opt/bin/external/FP16 -iquote external/pthreadpool -iquote bazel-out/armhf-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/armhf-opt/bin/external/FXdiv -iquote external/cpuinfo -iquote bazel-out/armhf-opt/bin/external/cpuinfo -iquote external/clog -iquote bazel-out/armhf-opt/bin/external/clog -Ibazel-out/armhf-opt/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/armhf-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/armhf-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/armhf-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/armhf-opt/bin/external/clog/_virtual_includes/clog -isystem external/XNNPACK/include -isystem bazel-out/armhf-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/armhf-opt/bin/external/XNNPACK/src -isystem external/FP16/include -isystem bazel-out/armhf-opt/bin/external/FP16/include -isystem external/pthreadpool/include -isystem bazel-out/armhf-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/armhf-opt/bin/external/FXdiv/include -w -DAUTOLOAD_DYNAMIC_KERNELS -Iinclude -Isrc -marm '-march=armv7-a' '-mfpu=neon-fp16' '-std=c99' -O2 -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c -o bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neonfp16_prod_microkernels/1/vcvt-neonfp16-x16.pic.o)
Execution platform: @local_execution_config_platform//:platform
external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c: In function 'xnn_f32_f16_vcvt_ukernel__neonfp16_x16':
external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:36:28: error: incompatible types when initializing type 'uint16x8_t' using type 'int'
     const uint16x8_t vh0 = vreinterpretq_u16_f16(vcombine_f16(vcvt_f16_f32(vf0), vcvt_f16_f32(vf1)));
                            ^~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:37:28: error: incompatible types when initializing type 'uint16x8_t' using type 'int'
     const uint16x8_t vh1 = vreinterpretq_u16_f16(vcombine_f16(vcvt_f16_f32(vf2), vcvt_f16_f32(vf3)));
                            ^~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:45:27: error: incompatible types when initializing type 'uint16x4_t' using type 'int'
     const uint16x4_t vh = vreinterpret_u16_f16(vcvt_f16_f32(vf));
                           ^~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:55:21: error: incompatible types when initializing type 'uint16x4_t' using type 'int'
     uint16x4_t vh = vreinterpret_u16_f16(vcvt_f16_f32(vf));
                     ^~~~~~~~~~~~~~~~~~~~
Target //tensorflow/lite:libtensorflowlite.so failed to build
INFO: Elapsed time: 2.279s, Critical Path: 1.96s
INFO: 293 processes: 236 internal, 57 local.
FAILED: Build did NOT complete successfully
```"
54267,asd,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54266,The gif encoding and decoding is not lossless in tf and tfio,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7.11
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: running on CPU
- GPU model and memory: running on CPU

**Describe the current behavior**
The gif encoding and decoding is not lossless. The input after gif encoding and decoding is not equivalent to the original input.

**Describe the expected behavior**
The image encoded by gif and then decoded should be lossless.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
need to install tensorflow=2.7.0 and tensorflow_io=0.22.0
```
import tensorflow as tf
import tensorflow_io as tfio
import numpy as np

input = np.array(
    [[[[13, 54, 87,],
    [56, 210, 195,],
    [230, 135, 61,],],
    [[13, 54, 87,],
    [56, 210, 195,],
    [230, 135, 61,],],
    [[13, 54, 87,],
    [56, 210, 195,],
    [230, 135, 61,]]]]
)

input_uint8 = tf.cast(input, tf.uint8)
encoded_file = tfio.image.encode_gif(input_uint8)
input_decoded = tf.io.decode_gif(encoded_file) 
print(np.allclose(input_uint8, input_decoded))  # False
```
The input after gif encoding and decoding is not equivalent to the original input. But gif itself should be lossless.
"
54265,Reduce axis produce wrong results in tf.function mode when there are duplicate dimensions,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7.11
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: running on CPU
- GPU model and memory: running on CPU

**Describe the current behavior**
The two API: tf.math.reduce_sum, tf.math.reduce_mean will have wrong output in tf.function when there are duplicate dimension in axis.

**Describe the expected behavior**
It should either raise an exception or clear the duplicate dimension.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
```
import traceback
import tensorflow as tf
import numpy as np

input = np.array([[  79, -126,   95,   39,  -50,  -59,   -6,  -82,   95,  124,  -94,
        -126,  -45,   46,   33,  123, -126],
       [-108,   68,   91,   -5,   53,  122,  -56,  106, -107,  126,   64,
         -25, -118,  -17,  -59,  -99,   98],
       [ -25, -119,  -28,   44,   13,  -80,   30,  105,   42,  119,   79,
          38,   34,  -89,  -23, -110,    1],
       [ -44, -121,   -7,  -62,   -8,  120,   24,  110,  -68,  -22,  -67,
         -10, -118,   21,  -47,  -94,  108],
       [ 108,   82,  -14,   26,  120,  -11,   19,  -77,  -47,  -35,  109,
          -3,   67,   20,  -90,   42,   95],
       [-125,  -80,  -18,   72,  -90,   55,   39,   -2,  -81,   -6,  -84,
          18,   96, -100,   68,   73,  -57],
       [  67,   46,  -49,   47,  -11,  -66,  -14,  107,   43, -105,  -71,
        -108,   23,  -21,   77,  -63, -107],
       [-115,  -14,  -92,   68, -118,   73,   92,   27,  -21,  -99,  -99,
         124,   47,  -70,  -93,  122,   69],
       [ -96,  -47,  -23,   43,   12,   12,  -80,   65,   39, -110,    2,
          -4,  110,  -23,   94,   67,   39]], dtype=np.int8)
axis = np.array([0, 0], dtype=np.int32)

fun_list = [tf.math.reduce_sum, tf.math.reduce_mean]
fun = tf.math.reduce_sum

for fun in fun_list:
    try:
        output1 = fun(input, axis)
    except:
        print(traceback.format_exc())  # the eager mode will raise an execption

    @tf.function
    def fun_wrapper(x, y):
        return fun(x, y)

    output2 = fun_wrapper(input, axis)
    print(output2)  # the results of tf function is wrong
```
The tf function mode should either raise an Exception like the eager mode or clear the duplicate dimension and then do the execution.
"
54264,tf.image.non_max_suppression produces wrong results in tf.function when the input is float16.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7.11
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: running on CPU
- GPU model and memory: running on CPU

**Describe the current behavior**
tf.image.non_max_suppression will produce wrong results in tf.function when the input is float16.

**Describe the expected behavior**
The results should be equivalent to eager mode.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

argument = {'max_output_size': 3673497876354746723,
            'scores': np.array([61920., 31020., 2234., 41280., -25490.], dtype=np.float16),
            'boxes': np.array([[-40500., -59940., -106.3, 41800.],
                               [-37700., 37500., -53630., 49920.],
                               [-44900., -5356., 28080., 24940.],
                               [-44770., -18110., 28940., 7900.],
                               [-32380., 36200., -40640., 22850.]], dtype=np.float16)}

fun = tf.image.non_max_suppression

output1 = fun(**argument)
print(output1)  # the results of eager mode is correct

@tf.function
def fun_wrapper(arg):
    return fun(**arg)

output2 = fun_wrapper(argument) 
print(output2)  # the results of tf function is wrong
```

The results are:

tf.Tensor([0 3 1 2 4], shape=(5,), dtype=int32)
tf.Tensor([0], shape=(1,), dtype=int32)"
54263,Why is inference time so slow when using tensorflow.compat.v1?,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source(pip install)
- TensorFlow version (use command below): TFv1(1.13.1 / 1.14.0 /1.15.4), TFv2(2.7.0)
- Python version: Python 3.6(TFv1), Python 3.7(TFv2)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 11.3 / cuDNN 8.2.1
- GPU model and memory: GTX 1080ti 11G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: 1.13.1 / 1.14.0 / 1.15.4
2. TF 2.0: 2.7.0

**Describe the current behavior**
Inference time is slow when loading and inferring models using tf.compat.v1.
I tested 4 cases about one model.
the model is one of tensorflow object detection model zoo and trained using own dataset(fine tuning). 

1 case) inference using TF 1.13.1 -> inference time : 0.05
2 case) inference using TF 1.14.0 -> inference time : 0.05
3 case) inference using TF 1.15.4 -> inference time : 0.27
4 case) inference using TF 2.7.0 -> inference time : 0.27

Looking at the above results, I think version of tf.compat.v1 is 1.15.4.
Can I change version of tf.compat.v1 from 1.15 -> 1.14?

Our company solution is using tensorflow 1.13 to load and infer models. And I am migrating to tf2 to support RTX 3000 series.

**Describe the expected behavior**

Expected result,
1 case) inference using TF 1.13.1 -> inference time : 0.05
2 case) inference using TF 1.14.0 -> inference time : 0.05
3 case) inference using TF 1.15.4 -> inference time : 0.05
4 case) inference using TF 2.7.0 -> inference time : 0.05


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54262,q,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54261,keras.layers.BatchNormalization/Reshape produces dynamic tensors when converted to tflite,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (converter), Linux 5.10.17 (tflite)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 Model B Rev 1.4
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8 (converter) 3.7 (tflite)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: 

**Describe the current behavior**
Converting a model containing a `BatchNormalization` & `Reshape` layers (allowed via `SELECT_TF_OPS`) produces tensors which have dynamic size. When running this using a tflite delegate which only supports static tensors the following error occurs:

```
RuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#420 is a dynamic-sized tensor).
```

where `tensor#420` is:

```
{
    'name': 'model_3/batch_normalization_4/FusedBatchNormV3', 
    'index': 420, 
    'shape': array([   1,    5,    1,    1, 1024]), 
    'shape_signature': array([   1,    5,    1,    1, 1024]), 
    'dtype': <class 'numpy.float32'>, 
    'quantization': (0.0, 0), 
    'quantization_parameters': {
      'scales': array([], dtype=float32), 
      'zero_points': array([], dtype=int32), 
      'quantized_dimension': 0
    }, 
    'sparsity_parameters': {}
}
```



**Describe the expected behavior**

The converted model runs successfully on a static delegate.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**
This is reproducible with:

```python
import tensorflow as tf
from tensorflow.keras import Sequential, Input, Model
from tensorflow.keras.layers import BatchNormalization, Dense, Reshape

inputs = Input(shape=(10,))
MLP = Sequential()
MLP.add(Dense(1024*5))
MLP.add(Reshape((5,1,1,1024)))
MLP.add(BatchNormalization(axis=1))
MLP.add(Dense(1024*5))

output = MLP(inputs)

model = Model(inputs=inputs, outputs=output)
model.save('reproduce_bug')
converter = tf.lite.TFLiteConverter.from_saved_model('reproduce_bug')

converter._experimental_lower_tensor_list_ops = False
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]

tflite_model = converter.convert()

with open(""reproduce_bug.tflite"", ""wb"") as f:
    f.write(tflite_model)
```

and then on the tflite device:

```python
import tflite_runtime.interpreter as tflite

interpreter = tflite.Interpreter(model_path='reproduce_bug.tflite')

for tdetails in interpreter.get_tensor_details():
    if tdetails['index'] == 19: # or your failing index
        print(tdetails)

interpreter.allocate_tensors()
```

Note that unlike our real failing case this fails on a tensor in reshape rather than in `BatchNormalization`.

**Other info / logs** 
Full traceback:

```python-traceback
Traceback (most recent call last):
  File ""inference.py"", line 20, in <module>
    interpreter.allocate_tensors()
  File ""/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 521, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#420 is a dynamic-sized tensor).Ignoring failed application of the default TensorFlow Lite delegate indexed at 0.
```
"
54260,huhuuh,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54259,inference/prediction batch is non deterministic and overlap data,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7/2.2
- Python version: 3.10,3.8

**Describe the current behavior**
we create tensor dataset from parquet files using make_batch_reader() & make_petastorm_dataset().
Strategy - Multiworker mirror strategy
we use tf native functions like unbatch, batch, autosharding etc 
tf.data.experimental.AutoShardPolicy.DATA

ds = dataset.
        .unbatch()
        .batch(xcc_batch_size)
        .with_options(options)
         .prefetch(AUTO)

strategy.experimental_distribute_dataset(ds)

tensor data have overlap between batches and we expect the data shouldn't overlap between batches. 

**Describe the expected behavior**

data shouldn't overlap between batches. if we batch 100 records in 10 batch then i would expect all 10 batches should have unique data. 
"
54258,"Build with exception ""no such package '@llvm-raw//utils/bazel':"" .","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS monterey 12.0.1
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
TensorFlow installed from (source or binary):source
TensorFlow version: r2.8
Python version: python3.9
Installed using virtualenv? pip? conda?:pip
Bazel version (if compiling from source): bazel 3.7.2
GCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang-1300.0.29.3)
CUDA/cuDNN version: no enabled
GPU model and memory:: no enabled(Just CPU model)



**Describe the problem**
The ""bazel build --config=dbg --strip=never -c dbg --copt='-g' --cxxopt='-g' //tensorflow/tools/pip_package:build_pip_package"" always failed at the step pull and install the package from url ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz"" with msg ""no such package '@llvm-raw//utils/bazel':"" , maybe  it is too large , can it be solved by download the package  to local and install from local ? If so ,  I want to download it to local firstly and install from local , but how to install it  from local ? any file need to be modified ?


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
ERROR: An error occurred during the fetch of repository 'llvm-raw':
   Traceback (most recent call last):
	File ""/Users/xxx/ai/tf/tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl
		ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_shenyufeng/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF
ERROR: Error fetching repository: Traceback (most recent call last):
	File ""/Users/xxx/ai/tf/tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl
		ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_xxxx/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF
ERROR: no such package '@llvm-raw//utils/bazel': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_shenyufeng/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF
INFO: Elapsed time: 1738.882s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)"
54257,How to install llvm-project from local ? ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS monterey 12.0.1
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
TensorFlow installed from (source or binary):source
TensorFlow version: r2.7
Python version: python3.9
Installed using virtualenv? pip? conda?:pip
Bazel version (if compiling from source): bazel 3.7.2
GCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang-1300.0.29.3)
CUDA/cuDNN version: no enabled
GPU model and memory:: no enabled(Just CPU model)


**Describe the problem**
When I run the command "" bazel build --config=dbg --strip=never -c dbg --copt='-g' --cxxopt='-g' //tensorflow/tools/pip_package:build_pip_package"", it would pull package from ""https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz"" ,maybe it is too large,  the bazel build  command always failed at this step, so how to install this package from local ? As I can download this package  to local successfully.

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

ERROR: An error occurred during the fetch of repository 'llvm-raw':
   Traceback (most recent call last):
	File ""/Users/xxx/ai/tf/tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl
		ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_shenyufeng/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF
ERROR: Error fetching repository: Traceback (most recent call last):
	File ""/Users/xxxx/ai/tf/tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl
		ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_shenyufeng/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF
ERROR: no such package '@llvm-raw//utils/bazel': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz, https://github.com/llvm/llvm-project/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz] to /private/var/tmp/_bazel_shenyufeng/a4eaa5fcc3d32c786eb9bb79fa0163a2/external/llvm-raw/temp10925281720111290723/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz: Premature EOF
INFO: Elapsed time: 1738.882s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
"
54256,Error: Shape error While Using tf.image.multiscale_SSIM,"```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-21-37ba827d3ac9> in <module>()
    11 gan_model = define_gan(g_model, d_model, image_shape)
    12 # train model
---> 13 train(d_model, g_model, gan_model, dataset)

 2 frames
 /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)
 1169     """"""
 1170     if not self.is_compatible_with(other):
 -> 1171       raise ValueError(""Shapes %s and %s are incompatible"" % (self,other))
 1172 
 1173   def most_specific_compatible_shape(self, other):

 ValueError: Shapes (256, 256, 3) and (16, 16, 1) are incompatible
```"
54255,"InvalidArgumentError:  Size 1 must be non-negative, not -13 	 [[{{node gradient_tape/mean_squared_error/sub-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]] [Op:__inference_train_function_7208]","I am trying to train my neuronal Network but get this error everytime and i dont know what it means.

I am using tenserfow 2.7.0 in google colab.
Here is my NN architecture (i am training a CNN to output two informations - steering and speed.
Is something wrong with the mean_squared_error? 
`class v2Model():
    def __init__(self):
        self.logdir = os.path.join(TRAIN_LOGS_PATH, datetime.datetime.now().strftime(""run-V2_%d-%b-%Y__%H-%M-%S_%f""))
        self.epochs = 10
        self.batchSizeTrain = 100
        self.batchSizeVal = 25

    def createModel(self) -> None:
        inputs = Input(shape=(80, 320, 1))
        self.model = Rescaling(scale=1./127.5, offset=-1.)(inputs)
        self.model = Conv2D(filters=16, kernel_size=(5, 5), padding='VALID', activation='elu')(self.model)
        self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)
        self.model = Conv2D(filters=32, kernel_size=(3, 3), padding='VALID', activation='elu')(self.model)
        self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)
        self.model = Conv2D(filters=64, kernel_size=(3, 3), padding='VALID', activation='elu')(self.model)
        self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)
        self.model = Dense(19456, use_bias=True)(self.model)
        self.model = Dropout(rate=0.2)(self.model)
        self.model = Dense(500, use_bias=True)(self.model)
        dropV2 = Dropout(rate=0.2)(self.model)

        headSteeringV2 = Dense(1, activation=""linear"", name=""output_ster"")(dropV2)
        headSpeedV2 = Dense(1, activation=""linear"", name=""output_acc"")(dropV2)

        self.model = Model(inputs=inputs, outputs=[headSteeringV2, headSpeedV2])
        #print(self.model.summary())
      
        lossMse = MeanSquaredError()
        optimizer = Adam()

        self.model.compile(optimizer=optimizer, loss=""mean_squared_error"", metrics=['accuracy'])
    
    def trainModel(self, x_train, y_train_ster, y_train_acc, x_val, y_val_ster, y_val_acc) -> None:

        tensorboardCallback = tf.keras.callbacks.TensorBoard(self.logdir, histogram_freq=1)
        earlyStopCallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=2, mode='auto')
        # bei zittern 1e-2, 1e-3

        stepsPerEpoch = len(x_train) // self.batchSizeTrain
        stepsVal = len(x_val) // self.batchSizeVal

        self.history = self.model.fit(genData(x_train, {""output_ster"":y_train_ster, ""output_acc"":y_train_acc}, self.batchSizeTrain, True, ""v2""),
                                  steps_per_epoch=stepsPerEpoch,
                                  epochs=self.epochs ,
                                  validation_data=genData(x_val, {""output_ster"": y_val_ster, ""output_acc"": y_val_acc}, self.batchSizeVal, False, ""v2""),
                                  validation_steps=stepsVal,
                                  callbacks=[tensorboardCallback, earlyStopCallback])

    def uploadTraining(self):
      !tensorboard dev upload --logdir {self.logdir} \
        --name f""Training Results from CNN V2 at {datetime.datetime.now().strftime('%d-%b-%Y__%H-%M-%S')}"" \
        --description ""Accuracy and loss graphs"" \
        --one_shot
    
    def saveModel(self):
      self.model.save(""/content/drive/MyDrive/model-v2.h5"")`

`
Full Error message.
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
[<ipython-input-22-33a8561955b9>](https://localhost:8080/#) in <module>()
     74 v2CNN = v2Model()
     75 v2CNN.createModel()
---> 76 v2CNN.trainModel(x_train, y_train_ster, y_train_acc, x_val, y_val_ster, y_val_acc)
     77 v2CNN.uploadTraining()
     78 v2CNN.saveModel()

2 frames
[/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py](https://localhost:8080/#) in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57     ctx.ensure_initialized()
     58     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 59                                         inputs, attrs, num_outputs)
     60   except core._NotOkStatusException as e:
     61     if name is not None:

InvalidArgumentError:  Size 1 must be non-negative, not -12
	 [[{{node gradient_tape/mean_squared_error_1/sub-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]] [Op:__inference_train_function_5310]
`"
54254,Dataset error when using with XLA device (No unary variant device copy function found),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.3 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below):  v2.8.0-rc1-32-g3f878cff5b6 2.8.0
- tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl
- Python version: 3.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
error in script: 
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:XLA_CPU:0 in order to run __inference_f_27: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper [Op:__inference_f_27]

**Describe the expected behavior**
script runs with no errors and produces values as expected 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no - internal error 
- Briefly describe your candidate solution(if contributing): no - internal error 

**Standalone code to reproduce the issue**
```
import os
os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices' 

import tensorflow as tf

dataset = tf.data.Dataset.range(10)
    
@tf.function
def f():
    for x in dataset:
        tf.print(x)

with tf.device('/device:XLA_CPU:0'):
    f()
```"
54250,Isn't tf.compat.v1.get_variable the default trainable variable?,"I am implementing deep learning model using tensorflow keras where I need γ learnable parameter. For that I am utilizing

`gamma = tf.compat.v1.get_variable(""gamma"", [1], initializer=tf.constant_initializer(0.0))`

I read the [Official Documentation of tf.compat.v1.get_variable](https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable) but could not get the complete idea. In the official documentation it seems that by default this variable is non trainable. am I right? If yes then should I need to true this function in order to make trainable, like:
` gamma = tf.compat.v1.get_variable(""gamma"", [1], trainable=True, initializer=tf.constant_initializer(0.0))`

Could someone please clear it?"
54249,Segmentation Fault After Canonicalization Pass,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source. 
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):GCC 8.3
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Segment Fault.

**Describe the expected behavior**
Run canonicalization pass successfully.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): I have some findings but I can't solve it independently.
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
- First create the `reproduce.mlir` as below.
```
module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 0 : i32}} {
  func @main(%arg0: tensor<1xf32>, %arg1: tensor<1xf32>) -> tensor<1xf32> {
    %0 = mhlo.constant dense<9.99999997E-7> : tensor<f32>
    %1 = mhlo.constant dense<0.000000e+00> : tensor<1xf32>
    %2 = shape.shape_of %1 : tensor<1xf32> -> tensor<1xindex>
    %3 = shape.shape_of %0 : tensor<f32> -> tensor<0xindex>
    %4 = shape.cstr_broadcastable %2, %3 : tensor<1xindex>, tensor<0xindex>
    %5 = shape.assuming %4 -> (tensor<1xi1>) {
      %8 = shape.const_shape [1] : tensor<1xindex>
      %9 = ""mhlo.dynamic_broadcast_in_dim""(%1, %8) {broadcast_dimensions = dense<0> : tensor<1xi64>} : (tensor<1xf32>, tensor<1xindex>) -> tensor<1xf32>
      %10 = ""mhlo.dynamic_broadcast_in_dim""(%0, %8) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<1xindex>) -> tensor<1xf32>
      %11 = ""mhlo.compare""(%9, %10) {comparison_direction = ""LT""} : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xi1>
      shape.assuming_yield %11 : tensor<1xi1>
    }
    %6 = shape.const_witness true
    %7 = shape.assuming %6 -> (tensor<1xf32>) {
      %8 = ""mhlo.select""(%5, %arg0, %arg1) : (tensor<1xi1>, tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
      shape.assuming_yield %8 : tensor<1xf32>
    }
    return %7 : tensor<1xf32>
  }
}
```
- `bazel  build //tensorflow/compiler/mlir/hlo:mlir-hlo-opt`
- `./bazel-bin/tensorflow/compiler/mlir/hlo/mlir-hlo-opt -canonicalize reproduce.mlir`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
- backtrace: `readBits (rawData=0x0, bitPos=0, bitWidth=1)`
- If `mlir-hlo-opt` is built by CMake, the command could run successfully.
- If `mlir-hlo-opt` is built by Bazel with debug mode (`-c dbg`), the command could run successfully.
- The bug start to appear from tensorflow commit `51ab810dd80114a463d6703f`.
"
54248,Tensorflow accesses .hdf5 weights file after it's been loaded,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux - Redhat 7.9
- TensorFlow installed from (source or binary): binary (conda)
- TensorFlow version (use command below): On all versions from 2.2 to 2.7 (2.7.0 for the latest)
- Python version: 3.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.2.0

**Describe the current behavior**

When loading a keras model from a temporary `.hdf5` weights file, the weights are read successfully read into the model, but Tensorflow outputs a DATA_LOSS warning after the `load_weights` method has finished executing indicating that Tensorflow is still trying to read the file.

**Describe the expected behavior**

I expect Tensorflow to not touch the weights file once the `load_weights` function as returned.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**

```
import tensorflow as tf
import tensorflow_datasets as tfds
import tempfile
import os
import shutil

# Load MNIST Datasets
print(""Load MNIST Dataset"")
mnist_datasets, ds_info = tfds.load('mnist', as_supervised=True, with_info=True)
train_ds = mnist_datasets['train']
test_ds = mnist_datasets['test']

batch_size = 32

def normalize_img(image, label):
  """"""Normalizes images: `uint8` -> `float32`.""""""
  return tf.cast(image, tf.float32) / 255., label

try:
    autotune_opt = tf.data.AUTOTUNE
except:
    autotune_opt = tf.data.experimental.AUTOTUNE

train_ds = train_ds.map(
    normalize_img, num_parallel_calls=autotune_opt)
train_ds = train_ds.cache()
train_ds = train_ds.shuffle(ds_info.splits['train'].num_examples)
train_ds = train_ds.batch(batch_size)

test_ds = test_ds.map(
    normalize_img, num_parallel_calls=autotune_opt)
test_ds = test_ds.batch(batch_size)
test_ds = test_ds.cache()
test_ds = test_ds.prefetch(autotune_opt)

# Define function to create functional keras model
def create_keras_model(layer_dims):
    inp = tf.keras.layers.Input((28,28,1), name=""Input_Name"")
    last_layer = inp
    last_layer = tf.keras.layers.Reshape((28*28*1,), name=""Input_Reshape"")(last_layer)
    for i in range(len(layer_dims)):
        dim = layer_dims[i]
        last_layer = tf.keras.layers.Dense(dim, name=f""Layer_{i}"")(last_layer)
    outp = tf.keras.layers.Dense(10, name=""Output_Layer"", activation='sigmoid')(last_layer)

    return tf.keras.Model(inputs=inp, outputs=outp)

# Create and train initial model:

print(""Create and train initial keras model"")
mdl_1 = create_keras_model([32,32,32])

mdl_1.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=tf.keras.metrics.SparseCategoricalAccuracy())

mdl_1.fit(train_ds, epochs=1)

# Create function to test models
def eval_on_test(mdl):
    num_matches = 0
    total_examples = 0

    for X, Y in test_ds:
        # Eval model
        Y_eval = tf.argmax(mdl(X),axis=1).numpy()

        # Count
        total_examples += len(Y_eval)
        num_matches += (Y.numpy() == Y_eval).sum()

    return num_matches/total_examples

print(""Initial model performance"")
print(eval_on_test(mdl_1))

# Save weights to disk
mdl_weight_file = 'weights.hdf5'
mdl_1.save_weights(mdl_weight_file)

# Create new model and load from a temporary file
mdl_2 = create_keras_model([32,32,32])

with tempfile.NamedTemporaryFile('w+b') as temp_f:
    # Copy content to temporary file
    print(""Copy weights to temp file"")
    with open(mdl_weight_file, 'rb') as f:
        temp_f.write(f.read())

    # Flush to disk
    temp_f.flush()

    print(""Load weights from temp file"")
    # Load weights from temp file
    mdl_2.load_weights(temp_f.name)

print(""temp file gone"")

print(""Loaded model performance"")
print(eval_on_test(mdl_2))
```

**Other info / logs**

This the output I get by running this code locally:

```
Load MNIST Dataset
2022-02-02 15:45:23.299505: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2022-02-02 15:45:23.299702: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (iforge123): /proc/driver/nvidia/version does not exist
Create and train initial keras model
1875/1875 [==============================] - 6s 2ms/step - loss: 0.3753 - sparse_categorical_accuracy: 0.8929 
Initial model performance
0.9172
Copy weights to temp file
Load weights from temp file
2022-02-02 15:45:32.765376: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /tmp/tmpfpsuz_25: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
temp file gone
Loaded model performance
0.9172
```

I tried running this code on colab, however I can't see any of the tensorflow logging lines I see on my local machine."
54246,CONV_ADD and CONV_ADDV2 Fusion,"Why do we need to check the broadcast compatibility before CONV_ADD and CONV_ADDV2 Fusion?

Refer line 818 of [remapper.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc)

`if (!IsAddN(*node_def) && !IsAddWithNoBroadcast(ctx, *node_def)) return false;`

Why do we need _**IsAddWithNoBroadcast(ctx, *node_def)**_ check? 

Refer line 795 to  805 of [remapper.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc) for the definition of  _**IsAddWithNoBroadcast**_

`bool IsAddWithNoBroadcast(const RemapperContext& ctx, const NodeDef& node) {`
`  if (!IsAdd(node)) return false;`

  `// Check if this is case of broadcasting - Add node supports broadcasting.`
  `const auto& props = ctx.graph_properties.GetInputProperties(node.name())`;
  `if (props.size() == 2 &&`
      `ShapesSymbolicallyEqual(props[0].shape(), props[1].shape())) {`
   ` return true;`
 ` }`
  `return false;`
`}`

If we have Add op defined in our network, then both of its inputs are either same or broadcast compatible otherwise we can not perform Add.

Consider the third ADD node of [resnet50_v1.pb](https://zenodo.org/record/2535873/files/resnet50_v1.pb) in the attached figure. 

<img width=""421"" alt=""Resnet50_v1_Add3"" src=""https://user-images.githubusercontent.com/40749307/152197427-69dd56ca-16f9-4bc0-844a-0757ca2c62cd.PNG"">

Please clarify why we need a Broadcast compatibility check. If inputs to Add are not the same or broadcast compatible, then How is it possible to have an Add op in the network? "
54245,ResourceExhaustedError,"Hey everyone. I am training resnet50v2 on my dataset, I have 18746 images in the training set and 4000+ images in the validation set. I am using a batch size of 32. The first time I tried to train my model with this batch size, it worked. When I try to train my model it gave me the following error this time.
![error](https://user-images.githubusercontent.com/61932757/152167745-4ce0449f-6530-41d7-b4c3-b1afd00d82e7.png)

I tried a lot, but nothing worked for me."
54244,On-device-training fails using Tflite_runtime: Node number 54 (FlexReluGrad) failed to prepare.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Custom embedded Linux distribution (Kernel 5.15)**
- Mobile device: **Raspberry Pi 4B**
- TensorFlow installed from (source or binary): **Source (Building on Tflite_runtime using pip_package scripts)**
- TensorFlow version (use command below): **2.7.0**
- Python version: **3.8**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **9.3 (OpenEmbedded GNU Toolchain)**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
I am following the tutorial on how to do [on-device-training](https://www.tensorflow.org/lite/examples/on_device_training/overview). The first step was to create and train the Fashion_mnist model on google Colab which was successful since I managed to download as an output the tflite model after converting (I made sure to mention all the necessary signatures while saving the model). The documentation is providing an example only on Java for android Apps, but I'm trying to explore whether this is feasible with tflite_runtime wheel.
I sent then the tflite model to the target (raspberry pi) where the Tflite_runtime has been installed using [this article](https://www.tensorflow.org/lite/guide/build_cmake_pip) and I'm trying to run the training function by feeding my neural network with arrays of zeros just to prove that it is working. This is the code snippet I'm running on my target. 

```
import numpy as np
import struct as st
import tflite_runtime.interpreter as tflr
nImg = 10000
nR = 28
nC =28
images_array = np.zeros((1, 28, 28), dtype=""float32"") 
labels_array = np.zeros((1,10), dtype=""float32"")
modelpath=""fashion_mnist_model.tflite""
interpreter = tflr.Interpreter(modelpath)
signatures = interpreter.get_signature_list()
```

```
print('Signature:', signatures)
>>> Signature: {'infer': {'inputs': ['x'], 'outputs': ['logits', 'output']}, 'restore': {'inputs': ['checkpoint_path'], 'outputs': ['dense_1/bias:0', 'dense_1/kernel:0', 'dense_2/bias:0', 'dense_2/kernel:0']}, 'save': {'inputs': ['checkpoint_path'], 'outputs': ['checkpoint_path']}, 'train': {'inputs': ['x', 'y'], 'outputs': ['loss']}}
```
```
train = interpreter.get_signature_runner('train')
infer = interpreter.get_signature_runner('infer')
train(x=images_array, y=labels_array)
```
The script fails at the train function call and return the following traces:
```
`RuntimeError: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. 
Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. 
See instructions: https://www.tensorflow.org/lite/guide/ops_select
Node number 54 (FlexReluGrad) failed to prepare.`
```
**Describe the expected behavior**
The expected behavior is to get a complete and functional training on target. 

Any help would be appreciated, thank you in advance."
54243,Tenserflow recommenders : tfrs.metrics.FactorizedTopK,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
**2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`**

**Describe the current behavior**

![image](https://user-images.githubusercontent.com/57206771/152147536-8bedf93a-5594-4c46-af94-55ba8b88e812.png)

```

!pip install -q tensorflow-recommenders
!pip install -q --upgrade tensorflow_addons
!pip install -q --upgrade tensorflow-datasets
!pip install -q scann

import os
import pprint
import tempfile
from typing import Dict, Text

import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_recommenders as tfrs

import tensorflow_addons as tfa
from tqdm.keras import TqdmCallback
from tensorflow.keras import callbacks as tf_callbacks

# === 연습용 Rawdata 로딩 ===
# 해당 예제에서는 ratings 테이블에 있는 데이터를 '유저가 해당 영화를 보았다'는 것으로 간주하고
# Ratings data.
ratings = tfds.load(""movielens/100k-ratings"", split=""train"")
# Features of all the available movies.
movies = tfds.load(""movielens/100k-movies"", split=""train"")

ratings = ratings.map(lambda x: {
    ""movie_title"": x[""movie_title""],
    ""user_id"": x[""user_id""],
})
movies = movies.map(lambda x: x[""movie_title""])

# embedding 시, vocabulary size 설정을 위해
# user 수와 movie 수 파악
movie_titles = movies
user_ids = ratings.map(lambda x: x[""user_id""])
# movie_titles = movies
# user_ids = ratings.map(lambda x: x[""user_id""])

unique_movie_titles = np.unique(np.concatenate(list(movie_titles.batch(1))))
unique_user_ids = np.unique(np.concatenate(list(user_ids.batch(1))))

embedding_dimension = 64

user_model = tf.keras.Sequential([
  # embedding layer에 대한 input 용 이므로 레이블인코딩 수행
  tf.keras.layers.StringLookup(vocabulary=unique_user_ids, output_mode=""int"", mask_token=None),
  # We add an additional embedding to account for unknown tokens.
  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension),
  tf.keras.layers.Dense(32, activation=""relu"")
])

movie_model = tf.keras.Sequential([
  # embedding layer에 대한 input 용 이므로 레이블인코딩 수행
  tf.keras.layers.StringLookup(vocabulary=unique_movie_titles, mask_token=None),
  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension),
  tf.keras.layers.Dense(32, activation=""relu"")
])

# learning parameter setting
batch_size = 1024
eta = 1e-3
weight_decay = 1e-5
# model_save_flag = False
# checkpoint_filepath = folder_path + 'models/tmp_checkpoint/'

task = tfrs.tasks.Retrieval(
  loss=tf.keras.losses.CategoricalCrossentropy(),
  metrics=tfrs.metrics.FactorizedTopK(candidates=movies.batch(batch_size).map(movie_model), k=200, name=""top_k_acc"")
)

cb_reduceLR = tf_callbacks.ReduceLROnPlateau(patience=1, factor=0.6, min_lr=1e-7)
cb_earlyStopping = tf_callbacks.EarlyStopping(patience=10, monitor='val_top_k_acc/top_100_categorical_accuracy', mode='max')

tf.random.set_seed(42)
shuffled = ratings.shuffle(batch_size, seed=42, reshuffle_each_iteration=True)

train = shuffled.take(80000)
test = shuffled.skip(80000).take(2000)

cached_train = train.shuffle(batch_size, seed=42, reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.AUTOTUNE)
cached_test = test.batch(batch_size).prefetch(tf.data.AUTOTUNE)


# tfrs.Model 클래스를 상속받아 모델 빌드
class MovielensModel(tfrs.Model):

  def __init__(self, user_model, movie_model):
    super().__init__()
    self.movie_model: tf.keras.Model = movie_model
    self.user_model: tf.keras.Model = user_model
    self.task: tf.keras.layers.Layer = task

  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:
    # We pick out the user features and pass them into the user model.
    user_embeddings = self.user_model(features[""user_id""])
    # And pick out the movie features and pass them into the movie model,
    # getting embeddings back.
    positive_movie_embeddings = self.movie_model(features[""movie_title""])

    # The task computes the loss and the metrics.
    return self.task(user_embeddings, positive_movie_embeddings)

model = MovielensModel(user_model, movie_model)

model.compile(
    optimizer=tfa.optimizers.AdamW(learning_rate=eta, weight_decay=weight_decay),
    # loss=tf.keras.losses.CategoricalCrossentropy(),
    # metrics=tfrs.metrics.FactorizedTopK(candidates=movies.batch(batch_size).map(movie_model), k=200, name=""top_k_acc"")
)
history = model.fit(
  cached_train, validation_data=cached_test, epochs=30, verbose=0,
  callbacks=[cb_reduceLR, cb_earlyStopping, TqdmCallback(verbose=0)]
)

```





















**I wannted to use the tfrs.metrics.FactorizedTopK(**k=200**).
but like above, the output shows the result only with k=1,5,10,100 (always 4 cases).**

**Describe the expected behavior**

**I want to see the result with k=200**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
54242, symbol not found in flat namespace '__ZN4mlir2TF6detail25ResourceAliasAnalysisInfo18kMaxResourceTypeIdE',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS monterey 12.0.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):source
- TensorFlow version: r2.7
- Python version: python3.9
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source): bazel 3.7.2
- GCC/Compiler version (if compiling from source):  Apple clang version 13.0.0 (clang-1300.0.29.3)
- CUDA/cuDNN version: no enabled
- GPU model and memory:: no enabled(Just CPU model)



**Describe the problem**
when running the ""import tensorflow as tf"" , I got the exception below:
""Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/usr/local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): symbol not found in flat namespace '__ZN4mlir2TF6detail25ResourceAliasAnalysisInfo18kMaxResourceTypeIdE'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/xxxxx/PycharmProjects/pythonProject1/tensorflow/testdebug.py"", line 1, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python3.9/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/usr/local/lib/python3.9/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/usr/local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 79, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/usr/local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): symbol not found in flat namespace '__ZN4mlir2TF6detail25ResourceAliasAnalysisInfo18kMaxResourceTypeIdE'


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.

Process finished with exit code 1
""


**Provide the exact sequence of commands / steps that you executed before running into the problem**
 steps:
 
0) download source code and "" git checkout r2.7""
1)  ./configure  (choose ""N"" for all of the questions)
2)  bazel build --config=dbg --strip=never -c dbg --copt='-g' --cxxopt='-g'   //tensorflow/tools/pip_package:build_pip_package
3) ./bazel-bin/tensorflow/tools/pip_package/build_pip_package pkt2/tensorflow_pkg
4) pip install /Users/xxxxx/ai/tf/tensorflow/pkt2/tensorflow_pkg/tensorflow-2.7.0-cp39-cp39-macosx_12_0_x86_64.whl --force-reinstall
5) try to run ""import tensorflow as tf"" and got the exception:""ImportError: dlopen(/usr/local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): symbol not found in flat namespace '__ZN4mlir2TF6detail25ResourceAliasAnalysisInfo18kMaxResourceTypeIdE'
""


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


"
54241,FIFOQueue as a part of custom layer,"Hello,

   I need a queue as a part of my custom input layer in Transformer for Short-term memory purposes. I need to store the coming features ""in first in first out"" manner and remove the oldies features when it's full and clean all content if it's needed. I see [here](https://www.tensorflow.org/api_docs/python/tf/queue/FIFOQueue) something like that for working with Tensor, but is it a good choice for me and contains all that I need? Is the FIFOQueue faster than using Python's deque or Numpy based queue directly in a custom layer?

Compare Python's deque collection with TF's FIFOQueue:
| Python | TensorFlow |
|--------|-------------|
| deque(maxlen=max_size) | tf.queue.FIFOQueue(capacity, dtypes, shapes=None, names=None, shared_name=None)
| append(x)<br># if it's full discard from the left end | enqueue(vals) |
| clear() | ??? |
| len(queue) | size() |
| pop() | dequeue() |
| queue[i] | ??? |

Or alternatively do it with Numpy like:
```python
queue = np.zeros((maxlen, features)) # my queue
x = np.random.normal(size=features) # new features

# append()
queue = np.concatenate([queue[:-1], x[np.newaxis, :]])

# predict
y = model(queue[np.newaxis, :, :])
```

With Python's timeit I get a benchmark on 100000 cycles with maxlen=1000 and features=6:
| Method | Time |
|--------|-------------|
| Numpy | 0.1765849579999994  s |
| deque | 12.360834958000002  s |

Thanks, have a nice day."
54239,tensorflow.python.keras.api._v1.keras.layers' has no attribute 'experimental',"
**System information**
- Linux Ubuntu 16.04
- TensorFlow version: 1.15.5
- Python version: 3.7
- Installed using virtualenv

**Describe the problem**

Command 

import tensorflow_model_optimization

Error that I get
File ""test.py"", line 2, in <module>
    import tensorflow_model_optimization
  File ""/home/marlin/.local/lib/python3.7/site-packages/tensorflow_model_optimization/__init__.py"", line 86, in <module>
    from tensorflow_model_optimization.python.core.api import clustering
  File ""/home/marlin/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/api/__init__.py"", line 19, in <module>
    from tensorflow_model_optimization.python.core.api import sparsity
  File ""/home/marlin/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/api/sparsity/__init__.py"", line 16, in <module>
    from tensorflow_model_optimization.python.core.api.sparsity import keras
  File ""/home/marlin/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/api/sparsity/keras/__init__.py"", line 18, in <module>
    from tensorflow_model_optimization.python.core.sparsity.keras.prune import prune_low_magnitude
  File ""/home/marlin/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/prune.py"", line 22, in <module>
    from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper
  File ""/home/marlin/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py"", line 33, in <module>
    from tensorflow_model_optimization.python.core.sparsity.keras import prune_registry
  File ""/home/marlin/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/prune_registry.py"", line 26, in <module>
    class PruneRegistry(object):
  File ""/home/marlin/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/prune_registry.py"", line 96, in PruneRegistry
    layers.experimental.preprocessing.Rescaling.__class__: [],
  File ""/home/marlin/anaconda/envs/hific-compress1/lib/python3.7/site-packages/tensorflow_core/python/util/module_wrapper.py"", line 193, in __getattr__
    attr = getattr(self._tfmw_wrapped_module, name)
AttributeError: module 'tensorflow.python.keras.api._v1.keras.layers' has no attribute 'experimental'





**I can not upgrade to tensorflow 2.x as I have other dependencies on tensorflow 1.15.5 and trying to quantize my model. Am I doing anything wrong here?**"
54237,"Github still lists 2.6.2 as the release, but pip installs 2.7.0",What is the current version in such a case?
54235,Cannot build tflite wheels for Windows,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows (github actions)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): cloned from github
- TensorFlow version: from github
- Python version: 3.8
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): using cmake
- GCC/Compiler version (if compiling from source): Visual Studio 17.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**

I am trying to build tensorflow lite wheels for Windows on the github actions runner. I am using the `build_pip_package_with_cmake.sh` I can get compiler to run, but in the end it fails with 1738 errors. Here is a small snippet of these:

```
(const flatbuffers::Verifier &,flatbuffers::voffset_t,size_t) const': expects 3 arguments - 2 provided [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
         D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow/lite/schema/schema_generated.h(4977,12): error C2672: 'flatbuffers::Table::VerifyField': no matching overloaded function found [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
         D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow/lite/schema/schema_generated.h(4977,1): error C2780: 'bool flatbuffers::Table::VerifyField(const flatbuffers::Verifier &,flatbuffers::voffset_t,size_t) const': expects 3 arguments - 2 provided [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
         D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow/lite/schema/schema_generated.h(4978,12): error C2672: 'flatbuffers::Table::VerifyField': no matching overloaded function found [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
         D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow/lite/schema/schema_generated.h(4978,1): error C2780: 'bool flatbuffers::Table::VerifyField(const flatbuffers::Verifier &,flatbuffers::voffset_t,size_t) const': expects 3 arguments - 2 provided [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
         D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow/lite/schema/schema_generated.h(4979,12): error C2672: 'flatbuffers::Table::VerifyField': no matching overloaded function found [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
         D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow/lite/schema/schema_generated.h(4979,1): error C2780: 'bool flatbuffers::Table::VerifyField(const flatbuffers::Verifier &,flatbuffers::voffset_t,size_t) const': expects 3 arguments - 2 provided [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
         D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow/lite/schema/schema_generated.h(4980,12): error C2672: 'flatbuffers::Table::VerifyField': no matching overloaded function found [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
         D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow/lite/schema/schema_generated.h(4980,1): error C2780: 'bool flatbuffers::Table::VerifyField(const flatbuffers::Verifier &,flatbuffers::voffset_t,size_t) const': expects 3 arguments - 2 provided [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
         D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow/lite/schema/schema_generated.h(5067,12): error C2672: 'flatbuffers::Table::VerifyField': no matching overloaded function found [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
         D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow/lite/schema/schema_generated.h(5067,1): fatal error C1003: error count exceeds 100; stopping compilation [D:\a\tflite-runtime-wheels\tflite-runtime-wheels\tensorflow\tensorflow\lite\tools\pip_package\gen\tflite_pip\python3\cmake_build\tensorflow-lite.vcxproj]
```

Building wheels for MacOS and Linux works fine. I would be open for any suggestions. The goal is to make it possible to get wheels for Windows. I know that there are wheels available from coral, but not for Python 3.10.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

``` 
tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh windows
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
54234,How to debug custom loss function?,"
I know about 2 things needed to debug tensorflow in eager mode:
- add `run_eagerly=True` when compiling model
- add `tf.config.run_functions_eagerly(True)` line

Neither is working when it comes to debugging custom loss function. I used [simple MNIST example](https://www.tensorflow.org/datasets/keras_example) to recreate the issue:

```
import tensorflow as tf
import tensorflow_datasets as tfds

tf.config.run_functions_eagerly(True)  # does not help

(ds_train, ds_test), ds_info = tfds.load(
    'mnist',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

def normalize_img(image, label):
  """"""Normalizes images: `uint8` -> `float32`.""""""
  return tf.cast(image, tf.float32) / 255., label


def custom_loss(y_true, y_pred):
    stop = 1   # BREAKPOINT HERE NOT WORKING!


ds_train = ds_train.map(
    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)
ds_train = ds_train.batch(128)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)

ds_test = ds_test.map(
    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.AUTOTUNE)

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10)
])
model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    loss=custom_loss,
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
    run_eagerly=True,       # does not help either
)

model.fit(
    ds_train,
    epochs=6,
    validation_data=ds_test,
)
```
"
54233,"I got an error on trainingstart = model.fit(x=x_train, y=y_train, epochs=10, batch_size=64)","I don,t know what type of error i'm facing right now, need help ASAP

Epoch 1/10
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
C:\Users\Public\Documents\Wondershare\CreatorTemp/ipykernel_976/3417244454.py in <module>
----> 1 trainingstart = model.fit(x=x_train, y=y_train, epochs=10, batch_size=64)

C:\ProgramData\Anaconda3\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     56   try:
     57     ctx.ensure_initialized()
---> 58     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     59                                         inputs, attrs, num_outputs)
     60   except core._NotOkStatusException as e:

InvalidArgumentError:  Received a label value of 32 which is outside the valid range of [0, 11).  Label values: 16 7 17 1 13 4 8 3 24 1 8 12 11 30 16 0 18 15 0 32 16 30 10 31 21 12 6 9 0 23 19 5 21 3 6 8 11 31 14 31 19 5 27 0 32 32 0 19 4 7 16 3 31 27 18 31 20 19 1 22 22 26 6 8
	 [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits
 (defined at C:\ProgramData\Anaconda3\lib\site-packages\keras\backend.py:5113)
]] [Op:__inference_train_function_703]

Errors may have originated from an input operation.
Input Source operations connected to node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:
In[0] sparse_categorical_crossentropy/Reshape_1 (defined at C:\ProgramData\Anaconda3\lib\site-packages\keras\backend.py:5109)	
In[1] sparse_categorical_crossentropy/Reshape (defined at C:\ProgramData\Anaconda3\lib\site-packages\keras\backend.py:3561)

Operation defined at: (most recent call last)
>>>   File ""C:\ProgramData\Anaconda3\lib\runpy.py"", line 197, in _run_module_as_main
>>>     return _run_code(code, main_globals, None,
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\runpy.py"", line 87, in _run_code
>>>     exec(code, run_globals)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py"", line 16, in <module>
>>>     app.launch_new_instance()
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\traitlets\config\application.py"", line 846, in launch_instance
>>>     app.start()
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelapp.py"", line 677, in start
>>>     self.io_loop.start()
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\tornado\platform\asyncio.py"", line 199, in start
>>>     self.asyncio_loop.run_forever()
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\asyncio\base_events.py"", line 596, in run_forever
>>>     self._run_once()
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\asyncio\base_events.py"", line 1890, in _run_once
>>>     handle._run()
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\asyncio\events.py"", line 80, in _run
>>>     self._context.run(self._callback, *self._args)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 457, in dispatch_queue
>>>     await self.process_one()
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 446, in process_one
>>>     await dispatch(*args)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 353, in dispatch_shell
>>>     await result
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 648, in execute_request
>>>     reply_content = await reply_content
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\ipkernel.py"", line 353, in do_execute
>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2901, in run_cell
>>>     result = self._run_cell(
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2947, in _run_cell
>>>     return runner(coro)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\async_helpers.py"", line 68, in _pseudo_sync_runner
>>>     coro.send(None)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3172, in run_cell_async
>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3364, in run_ast_nodes
>>>     if (await self.run_code(code, result,  async_=asy)):
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3444, in run_code
>>>     exec(code_obj, self.user_global_ns, self.user_ns)
>>> 
>>>   File ""C:\Users\Public\Documents\Wondershare\CreatorTemp/ipykernel_976/117376915.py"", line 1, in <module>
>>>     trainingstart = model.fit(x=x_train, y=y_train, epochs=32, batch_size=64)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training.py"", line 1216, in fit
>>>     tmp_logs = self.train_function(iterator)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training.py"", line 878, in train_function
>>>     return step_function(self, iterator)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training.py"", line 867, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training.py"", line 860, in run_step
>>>     outputs = model.train_step(data)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training.py"", line 809, in train_step
>>>     loss = self.compiled_loss(
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\compile_utils.py"", line 201, in __call__
>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\losses.py"", line 141, in __call__
>>>     losses = call_fn(y_true, y_pred)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\losses.py"", line 245, in call
>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\losses.py"", line 1737, in sparse_categorical_crossentropy
>>>     return backend.sparse_categorical_crossentropy(
>>> 
>>>   File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\backend.py"", line 5113, in sparse_categorical_crossentropy
>>>     res = tf.nn.sparse_softmax_cross_entropy_with_logits(
>>> 

"
54232,tf.data.Dataset.from_generator() does not work with tf.data.experimental.enable_debug_mode(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow version (use command below): `v2.7.0-0-gc256c071bb2 2.7.0`
- Python version: 3.7.12

**Describe the current behavior**



`tf.data.Dataset.from_generator()` does not work when `tf.data.experimental.enable_debug_mode()` is called in advance.

```python
import tensorflow as tf

tf.data.experimental.enable_debug_mode()

def gen():
    yield from iter(range(10))

dataset = tf.data.Dataset.from_generator(gen, output_signature=tf.TensorSpec(shape=(), dtype=tf.int32))

for item in dataset:
    print(item)
```

Results in

```
  ...

  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 810, in get_iterator
    return self._iterators[iterator_id]

TypeError: unhashable type: 'numpy.ndarray'


	 [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]
```

From a small investigation it seems that the ""obvious"" solution — changing https://github.com/tensorflow/tensorflow/blob/ffe6f62b7f8e57177c26ca3b38c0929d5f64b43f/tensorflow/python/data/ops/dataset_ops.py#L833 to

```python
return np.int64(ret)
```

does not fix the issue, even though `np.int64(0)` is hashable, while `np.array(0, dtype=np.int64)` is not.

Additionally, I tried a workaround and added to `_GeneratorState` the following method:

```python
@staticmethod
def _fix_iterator_id(iterator_id):
    assert isinstance(iterator_id, np.ndarray)
    assert iterator_id.ndim == 0
    assert iterator_id.dtype == np.int64
    return np.int64(iterator_id)
```

and added calls to it from `get_iterator()` and `iterator_completed()`. However, this triggered yet another issue: in `tf.python.ops.script_ops`, `FuncRegistry` started throwing an error `ValueError('callback pyfunc_63 is not found')`.

**Describe the expected behavior**

No crash.

**Standalone code to reproduce the issue**

https://colab.research.google.com/gist/dniku/80456bc9d30fbaaadca4a22469c2c1df/tf_dataset_from_generator_with_debug_mode_crash.ipynb"
54225,build_pip_package is broken,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 8.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

build fails with

ERROR: /home/andrew/src/tensorflow/WORKSPACE:23:14: error loading package '@com_github_grpc_grpc//src/compiler': in /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/external/com_github_grpc_grpc/bazel/grpc_build_system.bzl: in /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/external/build_bazel_rules_apple/apple/ios.bzl: in /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/external/build_bazel_rules_apple/apple/internal/ios_rules.bzl: in /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/external/build_bazel_rules_swift/swift/swift.bzl: in /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/external/build_bazel_rules_swift/swift/internal/swift_proto_library.bzl: Extension 'swift/internal/swift_protoc_gen_aspect.bzl' has errors and referenced by '//external:grpc_python_plugin'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 60.334s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (194 packages loaded, 3623 targets configured)


**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
54224,Issue with load_model and custom_object_scope,"So as it seems like there is an issue with load_model for tensorflow 2.7.0, 
This was not happening for me on tensorflow 2.4.1 and everything was running smoothly.

Let's get to the topic. 

When i try to load my model instance i am getting an error the first time i try to load it i'll highlight my code here.
   ```
 with K.utils.custom_object_scope(
        {
            ""r2"": r2,
            ""r2_keras"": r2,
            ""AttentionAugmentation2D"": AttentionAugmentation2D,
            ""Functional"": tf.keras.models.Model,
        }
    ):
        with GzipFile(filename, ""rb"") as fs:
            estimator = pickle.load(fs)
            keras_model = pickle.load(fs)
            model_h5_instance = h5py.File(BytesIO(keras_model), ""r"")
            estimator.estimator = K.models.load_model(
                model_h5_instance, compile=False
            )
```
it is also noted that the instance that i am using for saving is obviously a tf.keras.models.Model() instance

so the first time this line `estimator.estimator = K.models.load_model(model_h5_instance, compile=False)`  runs i am getting this error:
`AttirbuteError: module 'keras.api._v2.keras.models' has no attribute ""Functional""`
the second time i run this line it loads the model regularly.
also noting that i have already tried to pass the custom objects on load_model.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- CUDA/cuDNN version: 11.6
- GPU model and memory: GTX 1050 TI"
54222,Node 'training/Adam/gradients/gradients/batch_normalization_8/cond_grad/StatelessIf': Connecting to invalid output 3 of source node batch_normalization_8/cond which has 3 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).,"<em>Please make sure that this is an issue related to keras.
tag:keras_template</em>

**Important Notice**

Please note that `tf.keras` code was moved entirely to
[keras-team/keras](https://github.com/keras-team/keras) repository

You can open any code/doc bugs, performance issues, and feature requests
 in [keras-team/keras](https://github.com/keras-team/keras/issues) repository

`tf.keras` related issues opened in
[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) repository may
not get attention as [keras-team/keras](https://github.com/keras-team/keras)
repository is dedicated for the development of `keras` code
"
54221,Allow to change thread timeout in collective_ops with an ENV variable,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.6.2
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
This issue happens with XLA when using JAX in a multi-host system, where I get `This thread has been waiting for 5000ms for and may be stuck` when the pmap function is getting compiled and rapidly followed by `Thread is unstuck!  Warning above was a false-positive.`. This gets annoying when I know that it is not really stuck and I get it on most of the devices (so the log gets repeated `num_devices` times). 



**Will this change the current api? How?**
Looking at the code, the 5000ms are hardcoded. Because I understand the need to log these cases, I would propose adding an ENV var where you could change the 5000ms to be another value, and if the variable is not set, then keep the 5000ms which works on most cases. 

It would only require to modify this function at `tensorflow/tensorflow/compiler/xla/service/collective_ops_utils.h`
```
template <typename DescFn>
void WaitAndLogIfStuck(tensorflow::BlockingCounter* counter,
                       const DescFn& desc_fn) {
  VLOG(3) << ""Begin: "" << desc_fn();
  const std::chrono::milliseconds timeout(5000);
  bool ok = counter->WaitFor(timeout);
  if (ok) {
    VLOG(3) << ""Finished: "" << desc_fn();
    return;
  }
  LOG(ERROR) << ""This thread has been waiting for "" << timeout.count()
             << ""ms for and may be stuck: "" << desc_fn();
  counter->Wait();
  LOG(ERROR) << ""Thread is unstuck!  Warning above was a false-positive.  ""
                ""Perhaps the timeout is too short: ""
             << desc_fn();
}
```
**Who will benefit with this feature?**

Cleaner logs and avoid known false positive.


**Any Other info.**

Also, there's a `for` repeated on the log which could be fixed in the same PR :)
"
54219,clang: error: no such file or directory: '/d2ReducedOptimizeHugeFunctions',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  windows11
- Bazel version (if compiling from source): 4.2.2


**Describe the problem**
    I want to build tensorflowLite from source, I use this command: `bazel build -c opt --fat_apk_cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tensorflow/lite/java:tensorflow-lite`,  and it occurs this error:

`ERROR: D:/desktop/ky/tensorflow-master/tensorflow/lite/kernels/internal/BUILD:838:11: Compiling tensorflow/lite/kernels/internal/mfcc.cc failed: (Exit 1): clang failed: error executing command
  cd C:/users/25486/_bazel_25486/v4xxzvgh/execroot/org_tensorflow
  SET ANDROID_BUILD_TOOLS_VERSION=30.0.3
    SET ANDROID_NDK_API_LEVEL=21
    SET ANDROID_NDK_HOME=D:/SoftWare/Andriod/ndk/21.4.7075529
    SET ANDROID_SDK_API_LEVEL=29
    SET ANDROID_SDK_HOME=D:/SoftWare/Andriod
    SET PATH=D:\SoftWare\Msys2\usr\bin;D:\SoftWare\Msys2\bin;C:\Windows;C:\Windows\System32;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;D:\SoftWare\Python\Scripts\;D:\SoftWare\Python\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;D:\SoftWare\Xshell\;D:\SoftWare\WinSCP\WinSCP\;D:\SoftWare\mingw64\bin;D:\SoftWare\Cmake\bin;D:\SoftWare\Ninja;D:\SoftWare\Andriod\platform-tools;D:\SoftWare\Git\Git\cmd;D:\SoftWare\Gradle\gradle-6.8.3\bin;D:\SoftWare\bazel;D:\SoftWare\Java\jdk1.8.0_202\bin;C:\Users\25486\AppData\Local\Microsoft\WindowsApps;D:\SoftWare\Bandizip\;D:\SoftWare\VSCode\Microsoft VS Code\bin;D:\SoftWare\mingw64\bin;D:\SoftWare\Java\jdk1.8.0_202\bin;
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/SoftWare/Python/python.exe
    SET PYTHON_LIB_PATH=D:/SoftWare/Python/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -D__ANDROID_API__=21 -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -target armv7-none-linux-androideabi -march=armv7-a -mfloat-abi=softfp -mfpu=vfpv3-d16 -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/windows-x86_64 -fpic -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig -Werror=return-type -Werror=int-to-pointer-cast -Werror=pointer-to-int-cast -Werror=implicit-function-declaration -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/_objs/audio_utils/mfcc.pic.d -frandom-seed=bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/_objs/audio_utils/mfcc.pic.o -fPIC -iquote . -iquote bazel-out/android-armeabi-v7a-opt/bin -iquote external/fft2d -iquote bazel-out/android-armeabi-v7a-opt/bin/external/fft2d /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /std:c++14 -DFARMHASH_NO_CXX_STRING -mfpu=neon -O3 -ffunction-sections -fdata-sections -fno-exceptions --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/lite/kernels/internal/mfcc.cc -o bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/_objs/audio_utils/mfcc.pic.o
Execution platform: @local_execution_config_platform//:platform
clang: error: no such file or directory: '/W0'

clang: error: no such file or directory: '/D_USE_MATH_DEFINES'

clang: error: no such file or directory: '/experimental:preprocessor'

clang: error: no such file or directory: '/d2ReducedOptimizeHugeFunctions'

clang: error: no such file or directory: '/std:c++14'

Target //tensorflow/lite/java:tensorflow-lite failed to build
INFO: Elapsed time: 21.998s, Critical Path: 17.47s
INFO: 95 processes: 73 internal, 22 local.
FAILED: Build did NOT complete successfully`

I have installed visual studio 2019, but in the error info above, I gusess the reason may be it can't find the visual studio??  please help...

"
54218,OpenMPI issue causes import tensorflow to hang intermittently?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: n/a
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**:tf.version.VERSION = 2.7.0
-   **Python version**: (3, 8, 10, 'final', 0)
-   **Bazel version (if compiling from source)**: n/a
-   **GCC/Compiler version (if compiling from source)**: n/a
-   **CUDA/cuDNN version**:CUDA Version: 11.4 
-   **GPU model and memory**: 4 x A6000 48GB
-   **Exact command to reproduce**:  import tensorflow as tf

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Importing tensorflow into a jupyter notebook hangs pseudorandomly.  After five months we've be unable to isolate a cause or devise a 100% effective workaround.  Sometimes the line ""import tensorflow as tf"" works, other times it hangs.  Often we're forced to do a full power off to clear this condition but even that isn't 100% effective.  We're not trying to do distributed (multi-node) training.  We're just working with the four GPUs on this machine.  Thanks for your help!


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

This command produces the output below.
strace -ttt python -c 'import tensorflow as tf; print(tf.__version__)' | tee -a strace-python.txt

1643302791.282122 openat(AT_FDCWD, ""/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi3/mca_ess_singleton.so"", O_RDONLY|O_CLOEXEC) = 7
1643302791.282146 read(7, ""\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0\340'\0\0\0\0\0\0""..., 832) = 832
1643302791.282169 fstat(7, {st_mode=S_IFREG|0644, st_size=27704, ...}) = 0
1643302791.282194 mmap(NULL, 29544, PROT_READ, MAP_PRIVATE|MAP_DENYWRITE, 7, 0) = 0x7f3d5f678000
1643302791.282216 mmap(0x7f3d5f67a000, 12288, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 7, 0x2000) = 0x7f3d5f67a000
1643302791.282243 mmap(0x7f3d5f67d000, 4096, PROT_READ, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 7, 0x5000) = 0x7f3d5f67d000
1643302791.282267 mmap(0x7f3d5f67e000, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 7, 0x5000) = 0x7f3d5f67e000
1643302791.282295 close(7)              = 0
1643302791.282371 mprotect(0x7f3d5f67e000, 4096, PROT_READ) = 0
1643302791.282524 pipe([7, 8])          = 0
1643302791.282551 pipe([9, 10])         = 0
1643302791.282575 stat(""/usr/bin/orted"", {st_mode=S_IFREG|0755, st_size=14648, ...}) = 0
1643302791.282611 clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7f3dba183a10) = 1251309
1643302791.285529 close(8)              = 0
1643302791.285554 close(9)              = 0
1643302791.285581 read(7, 


^Cstrace: Process 1251181 detached

(command hung and had to be terminated with ctrl-c)

*****

Here is an ifconfig.

eno1: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        ether 3c:ec:ef:7f:e4:3a  txqueuelen 1000  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
        device memory 0xf3d00000-f3d7ffff  

enxb03af2b6059f: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        ether b0:3a:f2:b6:05:9f  txqueuelen 1000  (Ethernet)
        RX packets 60  bytes 3904 (3.9 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 92  bytes 16432 (16.4 KB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eth1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.23.188  netmask 255.255.255.0  broadcast 192.168.23.255
        inet6 fe80::27f0:4e0d:bcfe:3279  prefixlen 64  scopeid 0x20<link>
        ether 3c:ec:ef:7f:e5:d6  txqueuelen 1000  (Ethernet)
        RX packets 243180  bytes 83830328 (83.8 MB)
        RX errors 19963008343767  dropped 58  overruns 0  frame 0
        TX packets 273889  bytes 161266394 (161.2 MB)
        TX errors 19997367730176  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 5303219  bytes 11465259037 (11.4 GB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 5303219  bytes 11465259037 (11.4 GB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0


******

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.94       Driver Version: 470.94       CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A6000    On   | 00000000:21:00.0 Off |                  Off |
| 30%   32C    P8    22W / 300W |      8MiB / 48685MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A6000    On   | 00000000:22:00.0 Off |                  Off |
| 30%   41C    P8    26W / 300W |    451MiB / 48685MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A6000    On   | 00000000:41:00.0  On |                  Off |
| 30%   44C    P8    33W / 300W |   1065MiB / 48682MiB |      8%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A6000    On   | 00000000:43:00.0 Off |                  Off |
| 30%   38C    P8    21W / 300W |      8MiB / 48685MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2829      G   /usr/lib/xorg/Xorg                  4MiB |
|    1   N/A  N/A      2829      G   /usr/lib/xorg/Xorg                  4MiB |
|    1   N/A  N/A   1669191      C   /usr/bin/python3                  443MiB |
|    2   N/A  N/A      2829      G   /usr/lib/xorg/Xorg                230MiB |
|    2   N/A  N/A      3143      G   /usr/bin/gnome-shell               62MiB |
|    2   N/A  N/A      4284      G   /usr/lib/firefox/firefox          285MiB |
|    2   N/A  N/A      4518      G   /usr/lib/firefox/firefox            4MiB |
|    2   N/A  N/A      5140      G   gnome-control-center               28MiB |
|    2   N/A  N/A     13899      G   /usr/bin/nvidia-settings            0MiB |
|    2   N/A  N/A   1668753      C   /usr/bin/python3                  443MiB |
|    2   N/A  N/A   1691476      G   /usr/lib/firefox/firefox            4MiB |
|    3   N/A  N/A      2829      G   /usr/lib/xorg/Xorg                  4MiB |
+-----------------------------------------------------------------------------+


"
54215,Tensorflow Object detection AI,"0


I am using Tensorflow model zoo object detection. SSD MobileNet V2 FPNLite 320x320 is the model I am using to train my model. Everything goes well my model starts training but I receive some weird msgs. I don't why this msg is showing up.

I think half of my model is training on GPU and then it is switching to CPU but I am not sure.

Here are the msgs that are showing up.

2022-01-30 19:30:21.237816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9971 MB memory: -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6 INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',) I0130 19:30:21.241063 140126199379776 mirrored_strategy.py:376] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)

After this it is showing me the following msgs.

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',). I0130 19:30:43.470607 140126199379776 cross_device_ops.py:619] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

Here is my GPU information.

![image](https://user-images.githubusercontent.com/43025113/151710543-0dcf2f72-94ec-465f-bb08-f12dfa2815fb.png)

Please someone help me with this. I have been struggling for weeks.
"
54214,Gradient for tf.sparse.reduce_max,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>



**Describe the feature and the current behavior/state.**
tf.sparse.reduce_max do not define grad. So it cannot be used in training step.

**Will this change the current api? How?**
Won't change current api. Just add grad in tf.sparse.reduce_max

**Who will benefit with this feature?**
People using sparse.reduce_max in training. If they have huge sparse matrix and cannot turn to dense array.

**Any Other info.**
I am working with pointnet-like model. And it will take a huge sparse matrix. If I turn this sparse matrix to dense, it will take ~60G memory. So I need do global maxpooling. Using tf sparse reduce_max. Unfortunately, it cannot be used in training right now.
Thanks"
54211,Tensorboard: This site can’t be reached localhost refused to connect.,"Hey everyone! I am training my model, when I use !tensorborad logdir=runs, it shows a link( http://localhost:6006/). When I click on this link it shows this site cant be reached. I don't what's wrong? I tried some solutions suggested at this site. But nothing worked for me."
54208,partially initialized module 'tensorflow' has no attribute 'Tensor' (most likely due to a circular import),"**System information**
- Running code from: https://github.com/jpiedrafita/ai_number_read/blob/main/numbers.py
❯pyhton3 numbers.py

- OS Platform and Distribution: OSX 10.15.7

- TensorFlow installed from: pip install tensorflow
As described in other ""has no attribute"" issues I tried other tf versions using
pip install tensorflow==x.x.x --ignore-installed

- TensorFlow version (use command below):

❯ pip show tensorflow
Name: tensorflow
Version: 2.7.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages
Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, protobuf, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wheel, wrapt
Required-by:

❯ pip show tensorflow_datasets
Name: tensorflow-datasets
Version: 4.5.0
Summary: tensorflow/datasets is a library of datasets ready to use with TensorFlow.
Home-page: https://github.com/tensorflow/datasets
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages
Requires: absl-py, dill, numpy, promise, protobuf, requests, six, tensorflow-metadata, termcolor, tqdm
Required-by:

- Python version 3.9.6

**Describe the current behavior**
Console error: 
https://github.com/jpiedrafita/ai_number_read/blob/main/traceback.txt
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow_datasets/core/utils/type_utils.py"", line 49, in <module>
    Tensor = Union[tf.Tensor, tf.SparseTensor, tf.RaggedTensor]
AttributeError: partially initialized module 'tensorflow' has no attribute 'Tensor' (most likely due to a circular import)

In my code ther isn't any Tensor reference that could make a circular import.

**Describe the expected behavior**
Simple console output similar to:
>Resultado: 0.974

**Standalone code to reproduce the issue**
https://github.com/jpiedrafita/ai_number_read/blob/main/numbers.py)

**Other info / logs**
"
54207,"No details on the CPU chip AVX requirement, pls give details up front. Not all folks have ivory tower HW.  ","Your documentation tells of the Nvidia GPU/CUDA requirement but not the CPU chip AVX requirement.   I have spent three days swapping video cards, now to find my dual Xeon(R) CPU  X5450 Dell cannot run your 2.0 version.  There should be a clear grid/listing of requirements and a lookup function.  It will take me a week and $ to build out of stored Dell t5600 with the Sandbridge CPUs.  Why are there no switches to disable advance features to use latest SW.  No all folks have ivory tower HW.  I run multiple old Dells with multiple configurations. I do not care if a job takes 2 days for a POC/hobby projects, just needs to run.  I find it very odd to think/expect a person would load of of this SW onto their main PC/Laptop.  Also, a large amount sugar coating and theory on the YouTube Tensorflow guides  (Josh G. nice detail, grateful you have no tin cup out).  But, there is a disservice in hiding technical details and steps. Do you think non-tech folks going to pick this up, not likely.  FYI..if you do not know structured tables vs non, do not tell them to jump into the pool.

Question- Can you advise if I build from source, will the CPU chip AVX be accounted for?  I am running Tensorflow 1.5 successfully , but not sure is Sahre prject will support that version

Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs
------------
Thanks in advance.

TH"
54206,Is there any way to decrease tensorflow lite invoke time on windows 10 ?  ,"**System information**
- windows 10, x64
- TensorFlow version: 2.7.0
- Python version: 3.9.5
- Bazel version : 3.7.2
- GCC/Compiler version : 11.2.0
- xnnpack : enable

I run a custom CNN tflite model on tensorflow lite using c++ where  i got invoke time 2.5 second and same tensorflow model version i run on python where i got 1.19 second . As i know c++ is more faster than python then what is the issue ?
Note : because of commercial reason i can't give the code snapshot.



"
54198,"NameError: Exception encountered when calling layer ""lambda_1"" (type Lambda).","`if __name__ == ""__main__"":
    (x_train, y_train, groups_train), (x_test, y_test, groups_test) = load_data()
    
    model = load_model(os.path.join(""/content/drive/MyDrive/models"", ""model.h5""))
    model.summary()

    print(""training:"")
    y_true, y_pred = y_train, np.argmax(model.predict(x_train, batch_size=1024, verbose=1), axis=-1)

    C = confusion_matrix(y_true, y_pred, labels=(1, 0))
    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]
    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)
    print(""acc: {}, sn: {}, sp: {}"".format(acc, sn, sp))

    print(""testing:"")
    y_true, y_pred = y_test, np.argmax(model.predict(x_test, batch_size=1024, verbose=1), axis=-1)

    C = confusion_matrix(y_true, y_pred, labels=(1, 0))
    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]
    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)
    print(""acc: {}, sn: {}, sp: {}"".format(acc, sn, sp))`
![image](https://user-images.githubusercontent.com/86849708/151673124-81fbe818-87c8-4285-b1db-e944da943eca.png)
"
54197,Extra GPU-CPU memory transfer when broadcasting operations between integer tensors,"Hello,

<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): v2.6.1-9-gc2363d6d025 2.6.2
- Python version: 3.6.9
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version: NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5
- GPU model and memory: GeForce 1080Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

The code runs fine but when we look in detail in Tensorboard we observe that there are silent memory transfers between CPU and GPU, when doing operation that involve integer tensors that need broadcasting.

In the picture below (The green block ""MemCpyH2D"" in the Stream23, that has been clicked to show the name of the culprit op, should not exist) : 

![2022-01-29-175200_1920x1080_scrot](https://user-images.githubusercontent.com/11304248/151670751-2498d7a0-a26a-415e-9f2d-0c6f92312d12.png)

**Describe the expected behavior**
I expect 0 memory transfer for ops as simple as additions, bitwise element operation like bitwise_and with a mask that is a constant, integer reduction along various axis, expand_dims, reshape and squeeze, gather_nd and scatter_nd should also not result in data transfer between CPU and GPU

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Create a log folder if necessary
And install tensorboard profiler (See https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras ) : 
`pip install -U tensorboard_plugin_profile`

```
import tensorflow as tf
from datetime import datetime

class CustomLayer(tf.keras.layers.Layer):
    def __init__(self):
        #N must be divisible by 4
        super(CustomLayer, self).__init__()
        self.dense1 = tf.keras.layers.Dense(100,use_bias=False)

    @tf.function
    def call(self, query):
        c = tf.cast(query,tf.int32,name=""castQueryToInt32"")
        d = tf.expand_dims( c, axis=1,name=""expandDimsAxis1"")
        e = tf.expand_dims( c, axis=2,name=""expandDimsAxis2"")
        g = tf.add(d , e ,name=""additionBroadcasted"")
        #h = tf.reduce_sum(g, axis=1) #This also create extra memory transfer between gpu and cpu
        f = tf.cast(g,tf.float32, name=""castGToFloat"")
        f = tf.reduce_sum(f,axis=1,name=""reduceSumF"")
        rem = (query - f)* (query - f)
        out = self.dense1(rem)
        return out



inputs = tf.keras.Input(shape=(100,))
out = CustomLayer()(inputs)


model = tf.keras.Model(inputs=inputs, outputs=out)
model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),loss=""mse"")


xtrain = tf.random.uniform((600000,100),dtype=tf.float32)
ytrain = tf.random.uniform((600000,100),dtype=tf.float32)

logs = ""logs/"" + datetime.now().strftime(""%Y%m%d-%H%M%S"")

#We investigate the slowness due the extra memory transfer between GPU and CPU because of broadcasting behavior of integer tensors
tboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,
                                                 histogram_freq=1,
                                                 profile_batch=""10,20"")

model.fit( xtrain,ytrain, batch_size=100,epochs=1,callbacks=[tboard_callback])
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

I had simple code (additions and reductions between integer tensors of broadcastable shapes)  that had very low GPU utilization (25% and abysmal performance 0.1x), so I investigate with Tensorboard, observed plenty of non-necessary GPU-CPU transfer and extracted this simple example. (Once you fix this example, I can provide additional test cases).

Maybe some additional ops need to be registered/created to handle the integer tensors properly.

The ugly work-around is to cast every int32 to float64 do the operations that need broadcasting there and cast back to int32, it almost works (except for the backward pass of a gather_nd that I haven't managed to suppress yet).
I have tried using repeat with integer tensor but it result in extra transfer too.

Thank you"
54195,TensorFlowLite CocoaPod v1.13.1 throwing error EXC_BAD_ACCESS on iOS ,"** System Information

- iOS App built using React Native v0.61.2 (iOS v15.2)
- Installed TensorFlowLite POD v1.13.1
- Getting this error while inputing tensor model to interpreter

int32_t lengthInputTensor = tflInterpreter->input_tensor(0)->dims->data[1];

<img width=""837"" alt=""Screenshot 2022-01-29 at 12 44 34 PM"" src=""https://user-images.githubusercontent.com/98508996/151651883-c0ad5965-ed1b-489a-b050-d77fbb498ac1.png"">

<img width=""888"" alt=""Screenshot 2022-01-29 at 12 44 02 PM"" src=""https://user-images.githubusercontent.com/98508996/151651891-50d13594-aa52-4a46-883a-a0ddd5d53113.png"">
"
54187,"AttributeError: 'float' object has no attribute 'dtype',  When executing tfp.optimizer.differential_evolution_one_step","This error is shown whenever tf.optimizer.differential_evolution_one_step is called.

This error is due to line 125 in the tensorflow optimizer file differential_evolution.py, which is,
crossover_prob=0.9

In this line, the variable crossover_prob is given as a float whereas it is required to be a tensorflow variable later in the file on line 652, which is,
dtype=crossover_prob.dtype.base_dtype,

changing crossover_prob to a tf.Variable before line 652 will solve this problem.
Also, setting the default value of crossover_prob in line 125 from
crossover_prob=0.9, to
crossover_prob=tf.Variable(0.9)
will also solve this problem and allow for this function to run.

Thank you"
54180,ImportError: cannot import name 'Bidirectional' from 'tensorflow.python.keras.layers' (tf-nightly),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip
- TensorFlow version: tf-nightly 2.9.0.dev20220127
- Python version: 3.10.2
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the problem**
I'm using the following import statement:

`from tensorflow.python.keras.layers import LSTM, Dense, Dropout, Bidirectional`

which ends up with the following error when I run the script

ImportError: cannot import name 'Bidirectional' from 'tensorflow.python.keras.layers' (C:\Python310\lib\site-packages\tensorflow\python\keras\layers\__init__.py)

I'm using VS Code and as such the import resolves just fine. I had to change the import line from
`tensorflow.keras.layers` to `tensorflow.python.keras.layers` though.

Am I doing something wrong here or is it an issue with tf-nightly and win10/python 3.10?
"
54172,Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Monterey 12.1 on M1 Chip
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.5.4
- Python version: 3.6.12
- Installed using virtualenv? pip? conda?: Yes pipenv
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang-1300.0.29.30) aka 4.2.1
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**
Build fails with linker error - pointer not aligned at address
`tensorflow/tensorflow/python/BUILD:4936:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)`

```
ld: warning: cannot export hidden symbol unsigned int
std::__1::__sort5<std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> >
tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,
tensorflow::tfprof::Options const&)::'lambda'
(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode
const*)&, tensorflow::tfprof::CodeNode**>
(std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> >
tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,
tensorflow::tfprof::Options const&)::'lambda'
(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode
const*)&, std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> >
tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,
tensorflow::tfprof::Options const&)::'lambda'
(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode
const*)&, std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> >
tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,
tensorflow::tfprof::Options const&)::'lambda'
(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode
const*)&, std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> >
tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,
tensorflow::tfprof::Options const&)::'lambda'
(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode
const*)&, std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> >
tensorflow::tfprof::TFMultiShow::SortNodes<tensorflow::tfprof::CodeNode>(std::__1::vector<tensorflow::tfprof::CodeNode*,
std::__1::allocator<tensorflow::tfprof::CodeNode*> > const&,
tensorflow::tfprof::Options const&)::'lambda'
(tensorflow::tfprof::CodeNode const*, tensorflow::tfprof::CodeNode
const*)&, tensorflow::tfprof::CodeNode) from
bazel-out/host/bin/tensorflow/core/profiler/internal/libtfprof_code.a
(tfprof_code.o)
```
The same error repeats for `tfprof_graph` and `tfprof_op`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow && git checkout tags/v1.5.4
./configure
bazel build //tensorflow/tools/pip_package:build_pip_package
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

**Please see attachment**
[build_fail.txt](https://github.com/tensorflow/tensorflow/files/7960509/build_fail.txt)"
54171,TensorArray.stack should accept axis as argument,"**Describe the feature and the current behavior/state.**

Currently there is:
```python
TensorArray.stack(name=None) -> tf.Tensor
```

If we want to have stacking along a different axis, we must manually transpose using:
```python
myTensorArray = tf.TensorArray(dtype=tf.float32, size=6, element_shape=(2, 3))
# [6, 2, 3]
res = myTensorArray.stack()
axis = 1
*perm, = range(len(res.shape))
perm[0], perm[axis] = perm[axis], perm[0]
# [2, 6, 3]
res_correct = tf.transpose(res, perm)
```

NOTE: I could have directly written out the `perm` parameter, but what if you're writing a library where you want to be able to stack along user-defined axis?

**Will this change the current api? How?**

New definition could directly do this:
```ptyhon
TensorArray.stack(axis=0, name=None) -> tf.Tensor
```

**Who will benefit with this feature?**

Anyone who uses `TensorArray.stack` and doesn't want to stack on 0th dimension.

**Any Other info.**

Performance implication of stacking along non-0 axis should be considered and clearly explained in docs if significant.

EDIT: Other APIs in TensorArray should also be considered to be consistent. For example
```python
TensorArray.concat(name=None) ->tf.Tensor
TensorArray.unstack(value, name=None) ->tf.Tensor
```
"
54165,Build did NOT complete successfully ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install 
- TensorFlow version: 2.6.0
- Python version:3.8.5
- Installed using virtualenv? pip? conda?: conda,  maybe I do not use conda env
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source):9.3.0
- CUDA/cuDNN version: 11.5
- GPU model and memory: Quadro P4000   8G



**Describe the problem**
I couldn't build successfully as the instructions.   

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
sudo proxychains bash apollo.sh build

# OR
sudo  bash apollo.sh build
```
![image](https://user-images.githubusercontent.com/30117686/151467138-48a0f20d-24ef-4f67-b501-37d9bebe64c7.png)


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
root@in-dev-docker:/apollo# sudo proxychains bash apollo.sh build
ProxyChains-3.1 (http://proxychains.sf.net)
[INFO] Apollo Environment Settings:
[INFO]     APOLLO_ROOT_DIR: /apollo
[INFO]     APOLLO_CACHE_DIR: /apollo/.cache
[INFO]     APOLLO_IN_DOCKER: true
[INFO]     APOLLO_VERSION: predtr-2021-12-28-463fb82f9e
[INFO]     DOCKER_IMG: 
[INFO]     APOLLO_ENV:  STAGE=dev USE_ESD_CAN=false
[INFO]     USE_GPU: USE_GPU_HOST= USE_GPU_TARGET=1
[ OK ] Running GPU build on x86_64 platform.
[WARNING] ESD CAN library supplied by ESD Electronics doesn't exist.
[WARNING] If you need ESD CAN, please refer to:
[WARNING]   third_party/can_card_library/esd_can/README.md
[INFO] Build Overview: 
[INFO]     USE_GPU: 1  [ 0 for CPU, 1 for GPU ]
[INFO]     Bazel Options: --config=gpu
[INFO]     Build Targets: //modules/... union //cyber/...
[INFO]     Disabled:      except //modules/drivers/canbus/can_client/esd/...
Starting local Bazel server and connecting to it...
WARNING: ignoring LD_PRELOAD in environment.
(00:29:33) INFO: Invocation ID: 40695d7b-c7dd-416c-b7ae-de72a1612dec
(00:29:33) INFO: Current date is 2022-01-28
(00:30:32) INFO: Repository build_bazel_rules_swift instantiated at:
  /apollo/WORKSPACE:68:16: in <toplevel>
  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:38:29: in grpc_extra_deps
  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/build_bazel_rules_apple/apple/repositories.bzl:117:11: in apple_rules_dependencies
  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/build_bazel_rules_apple/apple/repositories.bzl:84:14: in _maybe
Repository rule http_archive defined at:
  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
(00:30:32) INFO: Repository rules_java instantiated at:
  /apollo/WORKSPACE:68:16: in <toplevel>
  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps
  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/com_google_protobuf/protobuf_deps.bzl:44:21: in protobuf_deps
Repository rule http_archive defined at:
  /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
(00:30:32) WARNING: Download from https://github.com/bazelbuild/rules_java/archive/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz failed: class java.io.IOException connect timed out
(00:30:32) ERROR: An error occurred during the fetch of repository 'rules_java':
   Traceback (most recent call last):
	File ""/apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/bazel_tools/tools/build_defs/repo/http.bzl"", line 111, column 45, in _http_archive_impl
		download_info = ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_java/archive/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz] to /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/rules_java/temp14011213055655260428/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz: connect timed out
(00:30:32) ERROR: While resolving toolchains for target //modules/v2x/v2x_proxy/os_interface:os_interface_cpplint: invalid registered toolchain '@bazel_tools//tools/jdk:all': while parsing '@bazel_tools//tools/jdk:all': no such package '@rules_java//java': java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_java/archive/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz] to /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/rules_java/temp14011213055655260428/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz: connect timed out
(00:30:32) ERROR: Analysis of target '//modules/v2x/v2x_proxy/os_interface:os_interface_cpplint' failed; build aborted: invalid registered toolchain '@bazel_tools//tools/jdk:all': while parsing '@bazel_tools//tools/jdk:all': no such package '@rules_java//java': java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_java/archive/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz] to /apollo/.cache/bazel/540135163923dd7d5820f3ee4b306b32/external/rules_java/temp14011213055655260428/981f06c3d2bd10225e85209904090eb7b5fb26bd.tar.gz: connect timed out
(00:30:32) INFO: Elapsed time: 59.517s
(00:30:32) INFO: 0 processes.
(00:30:32) FAILED: Build did NOT complete successfully (595 packages loaded, 6242 targets configured)
    currently loading: @bazel_tools//tools/jdk ... (3 packages)
root@in-dev-docker:/apollo# 
```"
54164,RuntimeError: Encountered unresolved custom op: UnboundedIndexRangeEncode. number 3 (UnboundedIndexRangeEncode) failed to prepare. Node number 108 (WHILE) failed to invoke.,"**### 1. System information**

Linux
TF 2.7.0
### 2. Code

**Command used to run the converter or code if you’re using the Python API**

Converter:

  import argparse
  import io
  import os
  import sys
  import urllib
  from absl import app
  from absl.flags import argparse_flags
  import cv2
  import numpy as np
  import tensorflow.compat.v1 as tf
  
  #import tensorflow as tf
  from tensorflow.python import pywrap_tensorflow
  import tensorflow_compression as tfc  # pylint:disable=unused-import
  
  
  
  
  with tf.Session() as sess:
          saver = tf.train.import_meta_graph('/mnt/6t_hdd/Priyanka/hific-lo.metagraph')
          #saver.restore(sess, latest_checkpoint_path)
          inputs=None
          outputs=None
          signature='sender'
          request = urllib.request.urlopen('file:///mnt/6t_hdd/Priyanka/hific-lo.metagraph') # replace it with your local path and model
          try:
                  string = request.read()
          finally:
                  request.close()
  
          metagraph = tf.compat.v1.MetaGraphDef()
          loaded = metagraph.ParseFromString(string)
  
          wrapped_import = tf.compat.v1.wrap_function(lambda: tf.compat.v1.train.import_meta_graph('/mnt/6t_hdd/Priyanka/hific-lo.metagraph'), [])
          graph = wrapped_import.graph
          print(""*************************************"")
          inputs = metagraph.signature_def['sender'].inputs
          print(inputs)
          concrete_function =  metagraph.signature_def['sender']
          print(""$$$$$$$$$$"")
          inputs = [graph.as_graph_element(inputs[k].name) for k in sorted(inputs)]
          print(inputs)
          outputs = metagraph.signature_def[signature].outputs
          print(outputs)
          outputs = [graph.as_graph_element(outputs[k].name) for k in sorted(outputs)]
          print(""*************************************"")
          converter = tf.lite.TFLiteConverter.from_session(sess, inputs, outputs)
          converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
          converter.allow_custom_ops = True
          converter.optimizations = [tf.lite.Optimize.DEFAULT]
          tflite_model = converter.convert()
          open(""/mnt/6t_hdd/Priyanka/converted_model_fp16.tflite"", ""wb"").write(tflite_model) # replace the path with your local path


Here, i get the warning for custom ops as below - 
Custom ops: UnboundedIndexRangeDecode, UnboundedIndexRangeEncode
Details:
        tf.UnboundedIndexRangeDecode(tensor<*x!tf_type.string>, tensor<?x?x320xi32>, tensor<320x48xi32>, tensor<320xi32>, tensor<320xi32>) -> (tensor<?x?x320xi32>) : {debug_level = 0 : i64, device = """", overflow_width = 4 : i64, precision = 16 : i64}
        tf.UnboundedIndexRangeEncode(tensor<*xi32>, tensor<*xi32>, tensor<64x1481xi32>, tensor<64xi32>, tensor<64xi32>) -> (tensor<!tf_type.string>) : {debug_level = 0 : i64, device = """", overflow_width = 4 : i64, precision = 16 : i64}
        tf.UnboundedIndexRangeEncode(tensor<*xi32>, tensor<?x?x320xi32>, tensor<320x48xi32>, tensor<320xi32>, tensor<320xi32>) -> (tensor<!tf_type.string>) : {debug_level = 0 : i64, device = """", overflow_width = 4 : i64, precision = 16 : i64}
See instructions: https://www.tensorflow.org/lite/guide/ops_custom





Inference:

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""./model.tflite"")
interpreter.allocate_tensors()
print(""all ok"")

# # Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# # Test the model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# # The function `get_tensor()` returns a copy of the tensor data.
# # Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)


*The output from the converter invocation*

Traceback (most recent call last):
  File ""load_tflite.py"", line 31, in <module>
    interpreter.invoke()
  File ""/home/marlin/anaconda/envs/prune/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py"", line 923, in invoke
    self._interpreter.Invoke()
RuntimeError: Encountered unresolved custom op: UnboundedIndexRangeEncode.
See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 3 (UnboundedIndexRangeEncode) failed to prepare.
Node number 108 (WHILE) failed to invoke.


I'm wondering why the flex delegate couldn't prepare the UnboundedIndexRangeEncode function, even though I have the SELECT_TF_OPS flag enabled?

"
54163,Why the parameter size of mobilenetv3 in 2.7.0 is different from nightly?,"<em>
</em>
The number of parameters of MobileNetv3Small(alpha=1) in tf=2.7.0 is 1.5M, while it is 0.9M in tf-nightly. 

The same code was used to load the MobileNetV3Small model.

`
base_model =keras.applications.MobileNetV3Small(input_shape=(384, 512, 3),include_top=False,weights='imagenet',)
base_model.summary()
`

I saw that some layers were deleted in tf-nightly. Why is that?

![image](https://user-images.githubusercontent.com/41880345/151437092-63762b58-5bca-4ca7-9409-2146ed80631e.png)
"
54157,Ability to clear cache on CacheDataset,"**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**

If a dataset is cached, there is currently no public API for explicitly clearing the cache (I think?).

**Will this change the current api? How?**

Add `clear()` or `clear_cache()` method (perhaps on DatasetV2 class?). After calling this function, the cache has been evicted.

**Who will benefit with this feature?**

People using data augmentations who would like to create a fresh set of augmentations after each epoch (or say, every N epochs). For example, [here is an unanswered stack overflow question](https://stackoverflow.com/q/65037119/1430829) where a number of people have upvoted indicating the desire for this capability.

**Any Other info.**

For file-based caches, I think simply deleting the the cache file would suffice as a workaround for now. I'm not sure if there's a workaround for in-memory caches.
"
54156,MacOS build compilation error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra (10.13.6)
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6
- Python version: 3.9.10
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.10.44.4)

**Describe the problem**

Hi.

The problem i'm trying to solve is to build and install TF on early2009 mac mini with core2duo and 4gb ram inside.

Firstly I tried to install TF through PIP (`pip install tensorflow/pip install tensorflow-cpu`). But after running `python -c 'import keras'` i got `illegal instruction 4` error.

As far as I understand the pip package is build with AVX support which not supported on my workstation. So i'm trying to create package manually from the 'r2.6' branch but stucked on some wired c++ compilation errors.



The build commands i'm trying:
```
bazel build --config libc++ //tensorflow/tool/...
bazel build //tensorflow/tool/...
```


Produces such kind of output
```
Starting local Bazel server and connecting to it... 
INFO: Options provided by the client: 
  Inherited 'common' options: --isatty=1 --terminal_columns=109 
INFO: Reading rc options for 'build' from /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: 
  Inherited 'common' options: --experimental_repo_remote_exec 
INFO: Reading rc options for 'build' from /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: 
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true 
INFO: Reading rc options for 'build' from /Users/macmini/Documents/extra_money/tensorflow/.tf_configure.bazelrc: 
  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python@3.9/bin/python3.9 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages --python_path=/usr/local/opt/python@3.9/bin/python3.9 
INFO: Found applicable config definition build:short_logs in file /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING 
INFO: Found applicable config definition build:v2 in file /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1 
INFO: Found applicable config definition build:libc++ in file /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: --action_env=CC --action_env=CXX --action_env=CXXFLAGS=-stdlib=libc++ --action_env=PATH --define force_libcpp=enabled --linkopt -fuse-ld=lld 
INFO: Found applicable config definition build:macos in file /Users/macmini/Documents/extra_money/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 
DEBUG: /private/var/tmp/_bazel_macmini/09f55674c85a83ddba7431ff9a1fa9d3/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software. 
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400"" 
DEBUG: Repository io_bazel_rules_docker instantiated at: 
  /Users/macmini/Documents/extra_money/tensorflow/WORKSPACE:23:14: in <toplevel> 
  /Users/macmini/Documents/extra_money/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace 
  /private/var/tmp/_bazel_macmini/09f55674c85a83ddba7431ff9a1fa9d3/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories 
Repository rule git_repository defined at: 
  /private/var/tmp/_bazel_macmini/09f55674c85a83ddba7431ff9a1fa9d3/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel> 
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (428 packages loaded, 33200 targets configured). 
INFO: Found 1 target... 
[109 / 551] 2 actions running

....
```

and after a couple of hours waiting i get  huge list of  errors like

```
In file included from ./tensorflow/core/util/example_proto_helper.h:32: 
./tensorflow/core/util/sparse/sparse_tensor.h:181:10: error: no template named 'StatusOr' 
  static StatusOr<SparseTensor> Slice(const SparseTensor& tensor, 
         ^ 

or 


/tensorflow/core/util/sparse/sparse_tensor.h:599:7: error: no viable conversion from returned value of type '::tensorflow::Status' to function return type 'int'
TF_RETURN_IF_ERROR( 
      ^~~~~~~ 

```


Also I've tried with actual 'master' branch and also got 

```
ERROR: /Users/macmini/Documents/extra_money/tensorflow/tensorflow/compiler/xla/BUILD:226:11: Compiling tensorflow/compiler/xla/util.cc failed: (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 98 argument(s) skipped) 
tensorflow/compiler/xla/util.cc:142:7: error: no matching function for call to 'isnan' 
  if (std::isnan(value) && kPayloadBits > 0) { 
      ^~~~~~ 
tensorflow/compiler/xla/util.cc:153:3: note: in instantiation of function template specialization 'xla::RoundTripNanPayload<unsigned short, Eigen::bfloat16>' requested here 
  RoundTripNanPayload<uint16_t>(value, &result); 
  ^ 
/Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:505:1: note: candidate template ignored: requirement 'std::is_floating_point<bfloat16>::value' was not satisfied [with _A1 = Eigen::bfloat16] 
isnan(_A1 __lcpp_x) _NOEXCEPT 
^ 
/Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:513:1: note: candidate template ignored: requirement 'std::is_integral<bfloat16>::value' was not satisfied [with _A1 = Eigen::bfloat16] 
isnan(_A1) _NOEXCEPT 
^ 
tensorflow/compiler/xla/util.cc:142:7: error: no matching function for call to 'isnan' 
  if (std::isnan(value) && kPayloadBits > 0) { 
      ^~~~~~ 
tensorflow/compiler/xla/util.cc:159:3: note: in instantiation of function template specialization 'xla::RoundTripNanPayload<unsigned short, Eigen::half>' requested here 
  RoundTripNanPayload<uint16_t>(value, &result); 
  ^ 
/Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:505:1: note: candidate template ignored: requirement 'std::is_floating_point<half>::value' was not satisfied [with _A1 = Eigen::half] 
isnan(_A1 __lcpp_x) _NOEXCEPT 
^ 
/Library/Developer/CommandLineTools/usr/include/c++/v1/math.h:513:1: note: candidate template ignored: requirement 'std::is_integral<half>::value' was not satisfied [with _A1 = Eigen::half] 
isnan(_A1) _NOEXCEPT 
^ 
2 errors generated. 
Target //tensorflow/tools/pip_package:build_pip_package failed to build 
Use --verbose_failures to see the command lines of failed build steps. 
INFO: Elapsed time: 6493.890s, Critical Path: 218.62s 
INFO: 2739 processes: 7 internal, 2732 local. 
FAILED: Build did NOT complete successfully 
```

Honestly I'm not good in c++ templates and I don't understand how to solve this compilation error. 
Maybe you could help me with this?"
54155,TFLITE conversion Failed - Custom Model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): Pip
- TensorFlow installation (pip package or built from source): tensorflow==2.8.0-rc0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
Reference [TensorFlow Lite Model Colab]: https://colab.research.google.com/drive/1crWPg__nYgt5IUFOA_OzgU0qgvgzmsZc?usp=sharing


### 3. Failure after conversion
conversion Fails when adding the ""update_control_variate"" function.


### 4. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

2022-01-27 11:53:08.600343: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.
2022-01-27 11:53:08.600385: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.
2022-01-27 11:53:08.600602: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model
2022-01-27 11:53:08.647834: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }
2022-01-27 11:53:08.647877: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: saved_model
2022-01-27 11:53:08.787707: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.
2022-01-27 11:53:09.230312: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: saved_model
2022-01-27 11:53:09.631676: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 1031073 microseconds.
2022-01-27 11:53:10.288551: E tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate.cc:207] SavedModel V1 import failed: FAILED_PRECONDITION: Graph does not contain node: 

---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
/tmp/ipykernel_5933/617434638.py in <module>
     10 converter.experimental_new_converter = True
     11 converter.allow_custom_ops = True
---> 12 tflite_model = converter.convert()

~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in convert(self)
   1712         Invalid quantization parameters.
   1713     """"""
-> 1714     return super(TFLiteConverterV2, self).convert()
   1715 
   1716 

~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in wrapper(self, *args, **kwargs)
    801   def wrapper(self, *args, **kwargs):
    802     # pylint: disable=protected-access
--> 803     return self._convert_and_export_metrics(convert_func, *args, **kwargs)
    804     # pylint: enable=protected-access
    805 

~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in _convert_and_export_metrics(self, convert_func, *args, **kwargs)
    787     self._save_conversion_params_metric()
    788     start_time = time.process_time()
--> 789     result = convert_func(self, *args, **kwargs)
    790     elapsed_time_ms = (time.process_time() - start_time) * 1000
    791     if result:

~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in convert(self)
   1371     """"""
   1372     if self.experimental_lower_to_saved_model:
-> 1373       saved_model_convert_result = self._convert_as_saved_model()
   1374       if saved_model_convert_result:
   1375         return saved_model_convert_result

~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in _convert_as_saved_model(self)
   1351       if self.saved_model_dir:
   1352         self._validate_inputs(graph_def, input_tensors)
-> 1353         return self._convert_from_saved_model(graph_def)
   1354     finally:
   1355       shutil.rmtree(temp_dir, True)

~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in _convert_from_saved_model(self, graph_def)
    965     converter_kwargs.update(quant_mode.converter_flags())
    966 
--> 967     result = _convert_saved_model(**converter_kwargs)
    968     return self._optimize_tflite_model(
    969         result, quant_mode, quant_io=self.experimental_new_quantizer)

~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py in wrapper(*args, **kwargs)
    211         else:
    212           report_error_message(str(converter_error))
--> 213         raise converter_error from None  # Re-throws the exception.
    214       except Exception as error:
    215         report_error_message(str(error))

~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py in wrapper(*args, **kwargs)
    204     def wrapper(*args, **kwargs):
    205       try:
--> 206         return func(*args, **kwargs)
    207       except ConverterError as converter_error:
    208         if converter_error.errors:

~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/convert.py in convert_saved_model(**kwargs)
    787   model_flags = build_model_flags(**kwargs)
    788   conversion_flags = build_conversion_flags(**kwargs)
--> 789   data = convert(
    790       model_flags.SerializeToString(),
    791       conversion_flags.SerializeToString(),

~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/convert.py in convert(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    304       for error_data in _metrics_wrapper.retrieve_collected_errors():
    305         converter_error.append_error(error_data)
--> 306       raise converter_error
    307 
    308   return _run_deprecated_conversion_binary(

ConverterError: Graph does not contain node: "
54154,tensorflow-cpu 2.7.0 missing from conda-forge,"`tensorflow-cpu` 2.7.0 is [present on PyPI](https://pypi.org/project/tensorflow-cpu/) as of now but [missing from conda-forge](https://anaconda.org/conda-forge/tensorflow-cpu), where latest is 2.6.2."
54147,tf.math.angle not working in tflite concrete function,"Hi,
When i try to use tf.math.angle function inside concrete function and save as tflite model it gives an error like below:

Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select

tf version 2.7 

Can you help me with this?

Thanks"
54143,"Warning Tensorflow: ""Invalid GPU Compute Capability""","
Hi...i have  this warning Tensorflow: 

""Invalid GPU Compute Capability""

Att. 
Jcnew07"
54134,build tensorflow 2.9 using mingw-w64 on windows 10 ,"<em>i am trying to build the tensorflow 2.9 using mingw-w64 on windows 10 .
 i need the tensorflow c++ lib which i wanted to use it in mingw-w64 based projects.
how to do ?  has any tutorial ？ </em>

**System information**
- OS Platform and Distribution: Window 10 x64
- TensorFlow installed from: source 
- TensorFlow version: 2.9
- Python version: 3.9
- Bazel version : 4.2.2
- Compiler : mingw-w64
- CUDA/cuDNN version: 11.5
- GPU model and memory: RTX3070

"
54133,TensorFLow Lite version for iOS through POD,"Hi folks,

Can anyone please let us know what is the latest TensorFlow lite version available for iOS through PODs. In the example, it is mentioned that the latest version for TensorFLow lite is 1.13.1. 

What is the latest TensorFlow lite version that we can use with TensorFlow version 2.6.0?

<img width=""1340"" alt=""Screenshot 2022-01-27 at 10 52 12 AM"" src=""https://user-images.githubusercontent.com/98508996/151297127-95b22550-8b43-474a-835c-026cc22bd482.png"">

"
54125,tf.histogram_fixed_width_bins miss input check for `nbins`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
indices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=-5)
indices.numpy()
```
Outputs:
```
array([0, 0, 0, 0, 0, 0], dtype=int32)
```
**Expected output**
Expected nbins to be a positive interger, and raise an error if it's negative."
54122,Docker images for Tensorflow 2.6.2 are missing,"TensorFlow 2.6.2 was released, but I cannot find docker images for TF 2.6.2 in docker hub.

https://hub.docker.com/r/tensorflow/tensorflow/tags?page=1&name=2.6.2

<img width=""1262"" alt=""스크린샷 2022-01-27 오전 11 30 59"" src=""https://user-images.githubusercontent.com/8815362/151281339-8b1cad3d-fae3-4526-888e-a3536e151dc9.png"">

"
54117,tf.keras.metrics.get raise error for None,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
print(tf.keras.losses.get(None))
print(tf.keras.metrics.get(None)) # Fail
```
Outputs:
```
None
ValueError: Could not interpret metric identifier: None
```

**Expected output**
According to the document [tf.keras.metrics.get](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/get?hl=en), the `identifier` is ""One of None or string name of a metric function/class or metric configuration dictionary or a metric function or a metric class instance"". So do we expect the output to be `None` when `identifier` is `None` like `tf.keras.losses.get`?"
54116,tf.image.adjust_brightness and random_brightness miss input check,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
print(tf.image.adjust_brightness(x, delta=1.1))
print(tf.image.random_brightness(x, max_delta=1.1))

```

**Expected output**
According to the document [tf.image.adjust_brightness](https://www.tensorflow.org/api_docs/python/tf/image/adjust_brightness?hl=en), the delta should be in range `(-1, 1)`. And random_brightness is  quivalent to adjust_brightness() using a delta randomly picked in the interval [-max_delta, max_delta), so valid input check is also missing here."
54115,tf.image.adjust_hue miss input check,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
image = [[[1, 2, 3], [4, 5, 6]],
         [[7, 8, 9], [10, 11, 12]],
         [[13, 14, 15], [16, 17, 18]]]
image = tf.constant(image)
tf.image.adjust_hue(image, -2)
tf.image.adjust_hue(image, 2)

```

**Expected output**
According to the document [tf.image.adjust_hue](https://www.tensorflow.org/api_docs/python/tf/image/adjust_hue?hl=en), the delta should be in range `[-1, 1]`. Valid input check is missing here."
54107,Tensorflow Buildkite (CI) Broken,"Example failure: https://buildkite.com/bazel/tensorflow/builds/7681#4bedc5fc-2efe-401b-9aa4-f967df5f05dd

Tensorflow Buildkite seems to be broken with release Bazel."
54105,Copy-on-read Limits Usable GPU Memory,"When sparsely accessed individual variables occupy a large portion of available memory, the necessary copy during dense reads, e.g. while saving a checkpoint, can substantially increase the total memory usage. In the limit of a single variable (such as an embedding table) occupying all allocated memory, this behavior causes the memory usage to double when the model is saved. Therefore, a single sparsely accessed variable can at most occupy half the available memory to avoid OOM while saving the model.

Possible solutions could include for instance allowing a conversion from copy-on-read to copy-on-write, or dense access without copy using an exclusive lock in some situations.
[](url)




**System information**
Reproducer: [copy_on_read_oom.zip](https://github.com/tensorflow/tensorflow/files/7946149/copy_on_read_oom.zip)

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.6
- Python version: 3.8
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
"
54094,Model.fit() batch_size breaks for some batch_sizes,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and (RHEL-like (custom distro ran by organization))
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): Tensorflow 2.5.0 on Windows 10 , Tensorflow 2.5.0 AND 2.7.0 on RHEL
- Python version:  Windows 10 -> python 3.7.6    :   RHEL -> python 3.7.2+Tensorflow 2.5.0 AND python 3.8.2+Tensorflow 2.7.0
- CUDA/cuDNN version: CUDA:11.2.0 cuDNN:8.1.1
- GPU model and memory: Windows -> nVidia Quadro RTX 3000 : RHEL -> nVidia P100

**Describe the current behavior**
When utilizing a custom loss function that has been tested to work, it will break with certain batch sizes by claiming that a (None) datatype was received, or rather, the `None` could not be converted to tensor. This is despite the fact that a numpy array of dtype `'float32'` was passed to it.

This was first noted in a script that trained two separate autoencoders on two separate datasets, 1- 12800x400 and 1-8000x400, with batch sizes of 128 and 80 respectively. The first would train, but the second would break despite the code for the second autoencoder being a duplicate of the first. The first one in that situation utilized the same custom loss function `ExplainedVar().` This does not occur for the built-in loss functions. The `batch_size` in all cases was smaller than the training set.

It should be noted that when it fails, `true` and `pred` reach the loss function as `None`s . This is not a result of the math inside the loss function.

Below are the two different errors produced by Tensorflow 2.5.0 and 2.7.0 respectively.

**Tensorflow 2.5.0**
```
Traceback (most recent call last):
  File ""divided_models.py"", line 218, in <module>
    a_sae.fit(rnd_dat,rnd_dat,epochs=10,batch_size=32,verbose=2)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/keras/engine/training.py"", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 760, in _initialize
    *args, **kwds))
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3066, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3463, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3308, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 1007, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 668, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 994, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/keras/engine/training.py:853 train_function  *
        return step_function(self, iterator)
    ./custom_loss_funcs.py:117 call  *
        pred_mean = tf.reshape(tf.math.reduce_mean(pred,axis=1),(pred.shape[0],1))
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **
        return target(*args, **kwargs)
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:196 reshape
        result = gen_array_ops.reshape(tensor, shape, name)
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:8404 reshape
        ""Reshape"", tensor=tensor, shape=shape, name=name)
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:525 _apply_op_helper
        raise err
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:515 _apply_op_helper
        preferred_dtype=default_dtype)
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py:163 wrapped
        return func(*args, **kwargs)
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor
        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:346 _constant_tensor_conversion_function
        return constant(v, dtype=dtype, name=name)
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:272 constant
        allow_broadcast=True)
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:290 _constant_impl
        allow_broadcast=allow_broadcast))
    /usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:553 make_tensor_proto
        ""supported type."" % (type(values), values))

    TypeError: Failed to convert object of type <class 'tuple'> to Tensor. Contents: (None, 1). Consider casting elements to a supported type.
```

**Tensorflow 2.7.0**
```
Traceback (most recent call last):
  File ""divided_models3.py"", line 111, in <module>
    a_ae.fit(scaled_aData,scaled_aData,epochs=n_epochs,
  File ""/usr/WS2/mvander/py3_8/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/WS2/mvander/py3_8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1129, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    File ""/usr/WS2/mvander/py3_8/lib/python3.8/site-packages/keras/engine/training.py"", line 878, in train_function  *
        return step_function(self, iterator)
    File ""divided_models3.py"", line 23, in call  *
        pred_mean = tf.reshape(tf.math.reduce_mean(pred,axis=1),(pred.shape[0],1))

    TypeError: Failed to convert elements of (None, 1) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.

```

**Describe the expected behavior**
`Model.fit()` should work for any batch_size > 0 and <= dataset size. Alternatively, it should frankly work for any positive dataset size where a batch_size larger than the dataset size would default to the size of the dataset.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No

**Standalone code to reproduce the issue**
```
import os
import sys
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow.keras as krs

class ExplainedVar(krs.losses.Loss):
    def __init__(self, reduction = krs.losses.Reduction.AUTO, **kwargs):
        super( ExplainedVar, self).__init__(reduction = reduction, **kwargs)
        # self.counter = 0

    def call(self,true,pred):
        #print(pred.shape)
        #print(true.shape)
        #if true.shape[0] is None:
        #    raise Exception()
        # self.counter += 1
        # print(self.counter)
        pred = tf.convert_to_tensor(pred)
        true = tf.cast(true, pred.dtype)

        res = tf.math.reduce_sum( tf.math.square( pred - true ), axis=1 )
        pred_mean = tf.reshape(tf.math.reduce_mean(pred,axis=1),(pred.shape[0],1))
        tot = tf.math.reduce_sum( tf.math.square( pred - pred_mean ),axis=1)

        return res/tot


def make_auto(in_dim,string):

    enc_in = krs.Input((in_dim,))
    x = enc_in


    x0_0 = krs.layers.Dense(96,activation='elu',kernel_initializer='lecun_normal')(x)
    x1_0 = krs.layers.Dense(48,activation='elu',kernel_initializer='lecun_normal')(x0_0)
    x2_0 = krs.layers.Dense(25,activation='elu',kernel_initializer='lecun_normal')(x1_0)
    x3_0 = krs.layers.Dense(16,activation='elu',kernel_initializer='lecun_normal')(x2_0)

    x0_1 = krs.layers.Dense(12,activation='elu',kernel_initializer='lecun_normal')(x)
    x1_1 = krs.layers.Dense(10,activation='elu',kernel_initializer='lecun_normal')(x0_0)
    x2_1 = krs.layers.Dense(8,activation='elu',kernel_initializer='lecun_normal')(x1_0)
    x3_1 = krs.layers.Dense(6,activation='elu',kernel_initializer='lecun_normal')(x2_0)
    x4_1 = krs.layers.Dense(4,activation='elu',kernel_initializer='lecun_normal')(x3_0)

    enc_out = krs.layers.Concatenate(axis=1)([x0_1,x1_1,x2_1,x3_1,x4_1])

    a_enc = krs.Model( inputs = enc_in, outputs = enc_out, name = ""%s_enc""%string)

    dec_in = krs.Input((40,))
    x = dec_in
    x0 = x[:,:4]
    x1 = x[:,4:10]
    x2 = x[:,10:18]
    x3 = x[:,18:28]
    x4 = x[:,28:40]

    x4 = krs.layers.Dense(16,activation='elu',kernel_initializer='lecun_normal')(x4)

    x3 = krs.layers.Concatenate()([x3,x4])
    x3 = krs.layers.Dense(25,activation='elu',kernel_initializer='lecun_normal')(x3)

    x2 = krs.layers.Concatenate()([x2,x3])
    x2 = krs.layers.Dense(48,activation='elu',kernel_initializer='lecun_normal')(x2)

    x1 = krs.layers.Concatenate()([x1,x2])
    x1 = krs.layers.Dense(96,activation='elu',kernel_initializer='lecun_normal')(x1)

    x0 = krs.layers.Concatenate()([x0,x1])
    dec_out = krs.layers.Dense(400,activation='elu',kernel_initializer='lecun_normal')(x0)

    a_dec = krs.Model( inputs = dec_in, outputs = dec_out, name = ""%s_dec""%string)

    a_sae = krs.Model( inputs = enc_in, outputs = a_dec( a_enc(enc_in)), name = ""%s_sae""%string)

    return a_sae, a_enc, a_dec


def samp_minmax_transform(xmin,xmax,data):
    data = ((data.T - xmin)/(xmax-xmin)).T
    return data

def samp_minmax_inverse_transform(xmin,xmax,data):
    data = (data.T*(xmax-xmin)+xmin).T
    return data

def scale(x):
    return np.power(x,1./30.)

def descale(x):
    return np.power(x,30.)



aData = np.random.uniform(0,10.,(5000,400))
bData = np.random.uniform(0,10.,(5000,400))


scaled_aData = scale(aData)
scaled_bData = scale(bData)

print(""aData DTYPE :"",aData.dtype)
print(""bData DTYPE :"",bData.dtype)
###############################################


a_ae, a_enc, a_dec = make_auto(400,'a')

loss = ExplainedVar()
# loss = krs.losses.MSLE #### This works with all batch sizes
opt = krs.optimizers.Adam(.0001)
a_ae.compile(optimizer=opt,loss=loss)


n_epochs = 10

batch_size = int(scaled_aData.shape[0]*.8) ### This will break everything with custom loss function
# batch_size = int(scaled_aData.shape[0]*.05) #### This make it work with custom loss function

a_ae.fit(scaled_aData,scaled_aData,epochs=n_epochs,
        batch_size=batch_size,verbose=2)

###############################################
## THIS SECTION IS TO VERIFY THAT IT ISN'T THE ARCHITECTURE ITSELF

# enc_in = krs.Input((400,))
# x = krs.layers.Dense(50,activation='elu')(enc_in)
# enc_out = krs.layers.Dense(10,activation='elu')(x)
# enc = krs.Model(inputs=enc_in,outputs=enc_out,name=""enc"")

# dec_in = krs.Input((10,))
# x = krs.layers.Dense(50,activation='elu')(dec_in)
# dec_out = krs.layers.Dense(400,activation='elu')(x)
# dec = krs.Model(inputs=dec_in,outputs=dec_out,name=""dec"")

# n_epochs = 10
# batch_size = int(scaled_aData.shape[0]*.8)
# ae = krs.Model(inputs=enc_in, outputs = dec( enc(enc_in)),name='ae')
# loss = ExplainedVar()
# opt = krs.optimizers.Adam(.0001)
# ae.compile(optimizer=opt,loss=loss)
# ae.fit(scaled_aData,scaled_aData,epochs=n_epochs,
#         batch_size=batch_size,verbose=2)

```
"
54091,TFLite SignatureRunner support for the C API,"**System information**
- TensorFlow version (you are using): 2.7 / tf-nightly
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
The current TFLite interpreter C API allows using TFLite in situations where a stable ABI is needed and the C++ API is not an option. For example, in cases where TFLite needs to be used in bigger projects that cannot use bazel to build and might use different toolchains or incompatible C++ compiler settings.

However, while the C++ API already has support for multiple signatures [through the use of SignatureRunner](https://www.tensorflow.org/lite/guide/signatures#c), the C API has not been updated accordingly. As such, the use of multiple signatures in TFLite models is not possible if ABI stability requirements prevent you from using the C++ API directly.

**Will this change the current api? How?**
New APIs that allow using signature runners would need to be added [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api.h), or in a separate header if appropriate.

**Who will benefit with this feature?**
Anyone who needs stable ABIs and wants to run TFLite models with multiple signatures.

**Any Other info.**
As mentioned [here](https://www.tensorflow.org/lite/guide/signatures#known_limitations) this feature is not available _yet_, suggesting it's planned. I'm opening this feature request so that it's easier to track its status, as well as any kind of information on when it might be implemented."
54090,"LSTM model save warning, Tensorflow 2.7.0","**System information**
- OS Platform and Distribution: Windows10
- TensorFlow installed from: binary
- TensorFlow version: v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.9.10


**Describe the current behavior**
When model.save called, the below warning message occurred:
```
WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.
WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000021419A6A820> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.
```
I just upgraded python from 3.6 to 3.9, reinstall Tensorflow under Python 3.9.10. I found the LSTM model hard to train to reduce loss value and the previous warning information occurred.

![image](https://user-images.githubusercontent.com/61686583/151123541-210c3046-b23a-4a09-812d-34bb2dde9ad3.png)
"
54087,no attribute '_register_wrapper_optimizer_cls' in python 3.8.12,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 2.7.0
- Python version: 3.8.12
- Installed using virtualenv? pip? conda?: conda virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2/8.1.0
- GPU model and memory: RTX 3090 / 24GB



**Describe the problem**
I installed tensorflow 2.7.0 using pip with python 3.8.12. It imported fine in ubuntu 18.04.
However, as a result of doing the same in ubuntu 20.04, the import was not performed with an error such as ""AttributeError: module 'tensorflow.python.training.experimental.mixed_precision' has no attribute '_register_wrapper_optimizer_cls'"".
I tried the same method in python 3.9.7 version just in case, but this time it was imported.
Therefore, I wonder why python 3.8.12 version can be done in ubuntu 18.04, but not in ubuntu 20.04.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
environment.yaml file
```
name: tf27
channels:
  - anaconda
  - conda-forge
  - defaults
dependencies:
  - python==3.8.12
  - pip
  - pip:
    - lxml>=4.6.1
    - cython>=0.29.13 
    - absl-py>-0.10.0
    - matplotlib>=3.0.3
    - numpy>=1.19.4
    - Pillow>=6.0.0
    - pycocotools>=2.0
    - PyYAML>=5.1
    - six>=1.15.0
    - tensorflow>=2.7.0
    - tensorflow-addons>=0.15
    - tensorflow-hub>=0.11
    - neural-structured-learning>=1.3.1
    - opencv-python
    - tqdm
```
Then try 
```
import tensorflow
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Full error log below.

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/zeron/anaconda3/envs/tf27/lib/python3.8/site-packages/tensorflow/__init__.py"", line 479, in <module>
    keras._load()
  File ""/home/zeron/anaconda3/envs/tf27/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/home/zeron/anaconda3/envs/tf27/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/zeron/.local/lib/python3.8/site-packages/keras/__init__.py"", line 25, in <module>
    from keras import models
  File ""/home/zeron/.local/lib/python3.8/site-packages/keras/models.py"", line 20, in <module>
    from keras import metrics as metrics_module
  File ""/home/zeron/.local/lib/python3.8/site-packages/keras/metrics.py"", line 27, in <module>
    from keras import activations
  File ""/home/zeron/.local/lib/python3.8/site-packages/keras/activations.py"", line 20, in <module>
    from keras.layers import advanced_activations
  File ""/home/zeron/.local/lib/python3.8/site-packages/keras/layers/__init__.py"", line 24, in <module>
    from keras.engine.input_layer import Input
  File ""/home/zeron/.local/lib/python3.8/site-packages/keras/engine/input_layer.py"", line 21, in <module>
    from keras.engine import base_layer
  File ""/home/zeron/.local/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 41, in <module>
    from keras.mixed_precision import loss_scale_optimizer
  File ""/home/zeron/.local/lib/python3.8/site-packages/keras/mixed_precision/loss_scale_optimizer.py"", line 1180, in <module>
    mixed_precision._register_wrapper_optimizer_cls(optimizer_v2.OptimizerV2,
AttributeError: module 'tensorflow.python.training.experimental.mixed_precision' has no attribute '_register_wrapper_optimizer_cls'
"
54086,Tensorflow restore memory leak,"**System information**

- OS Platform and Distribution: MacOS Monterey 12.1
- TensorFlow installed: from binary
- TensorFlow version: The issue could be reproduced by v2.6.0
- Python version: 3.8.3

**Describe the current behavior**
I notice that Tensorflow (TF) leaks memory when I restore TF models. Specifically if I iteratively load model checkpoint from disk with TF restore API, the memory usage keeps growing no matter what I do `tf.reset_default_graph()` or `tf.keras.backend.clear_session()` or both (But it seems `tf.reset_default_graph()` and `tf.keras.backend.clear_session()` slow down the memory leak speed rate).

** Standalone code to reproduce the issue **
The issue could be reproduced as below. Run `tf_save_model.py` to create and save a model in the folder `./test_model_repro/` with the model name `foo`. Then run `tf_restore_model.py` to load the model iteratively. You would see after each iteration calling `restore()`, even if `tf.reset_default_graph()` and `tf.keras.backend.clear_session()` are being called, the log shows the memory usage keeps increasing.

tf_save_model.py
```
from typing import Tuple

import tensorflow.compat.v1 as tf
from tensorflow.keras import models
from tensorflow.keras import layers

tf.disable_v2_behavior()


def create_model() -> Tuple[tf.Tensor, tf.Tensor]:
    """"""
    Create a model with Neural-Net: 1024 x 1024
    """"""
    graph = tf.get_default_graph()
    with graph.as_default():
        # The Neural Net Size: 200 -> 1024 x 1024 -> 6
        model = models.Sequential()
        input_shape, hidden_shape, output_shape = 200, 1024, 6
        model.add(
            layers.Dense(
                hidden_shape,
                activation='tanh',
                input_shape=(input_shape,),
                name=""layer0""
            )
        )
        model.add(layers.Dense(hidden_shape, activation='tanh', name=""layer1""))
        model.add(layers.Dense(output_shape, activation='relu', name=""final_layer""))
        input_tensor = tf.placeholder(dtype=tf.float32, shape=[None, input_shape])
        output_tensor = model(input_tensor)
    return input_tensor, output_tensor


def save_model(sess: tf.Session, path: str) -> None:
    """"""
    Save the TF model according to the path.
    """"""
    saver = tf.train.Saver()
    sess.run(tf.global_variables_initializer())
    save_path = saver.save(sess, path)
    print(f""Saved the model in {save_path}"")


def main() -> None:
    input_tensor, output_tensor = create_model()
    print(
        f""Create the model with Input tensor {input_tensor} ""
        f""and output tensor {output_tensor}.""
    )
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    save_model(sess, ""./test_model_repro/foo"")


main()
```

tf.restore_model.py
```
import os
import psutil

import tensorflow.compat.v1 as tf

tf.disable_v2_behavior()


def _get_memory_rss() -> float:
    """"""
    Get the RSS memory value in GB.
    """"""
    return psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3


def restore(path: str) -> tf.train.Saver:
    """"""
    Restores the TF graph from the path and returns the saver.
    """"""
    sess = tf.Session()
    saver = tf.train.import_meta_graph(f""{path}.meta"")
    saver.restore(sess, path)


def main() -> None:
    init_prev = _get_memory_rss()
    for _ in range(10000):
        restore(""./test_model_repro/foo"")
        mem_after = _get_memory_rss() - init_prev
        print(
            f""The memory increased after restoring the TF model: {mem_after}""
        )
        tf.reset_default_graph()
        tf.keras.backend.clear_session()


main()
```
The issue could be 100% repro-d by the above Python scripts.
"
54085,LSTM slow to calibrate with large observation period,"I am reporting an issue in the calibration speed of LSTM models with large observation period even though none of my resource is saturated (CPU, GPU, RAM, SSD)

**System information**
- OS Platform and Distribution: Win11
- TensorFlow installed from: `pip install tensorflow`
- TensorFlow version: 2.7 / GPU
- Python version: 3.9
- CUDA/cuDNN version: cuDNN 8201 (installed via `pip install tensorflow`), CUDA 11.4
- GPU model and memory: Geforce GTX 3060 4 Go / RAM 32 Go / CPU AMD Ryzen 5 / SSD

**Describe the current behavior**
None of the machine resource is saturated following LSTM calibration
`print(tf.config.list_physical_devices(""GPU""))` shows that the calibration runs on the GPU as expected.

**Describe the expected behavior**
Any of the machine resource should be saturated

**Standalone code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

print(tf.config.list_physical_devices(""GPU""))

if __name__ == ""__main__"":

    epoch = 10
    batch_size = 2000
    number_output = 3
    number_features = 5
    backward = 992
    number_node = 40

    def train_gen():
        for i in range(1000):
            yield np.random.random((batch_size, backward, number_features)),\
                  np.random.random((batch_size, number_output))

    train_dataset = tf.data.Dataset.from_generator(train_gen, output_types=(tf.float64, tf.float64))
    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

    drop_ratio = 0.2
    model = Sequential([layers.Input(shape=(backward, number_features))])
    model.add(layers.LSTM(number_node, return_sequences=False))
    model.add(layers.Dropout(drop_ratio))
    model.add(layers.Dense(number_output, activation='sigmoid'))
    model.compile(optimizer='adam', loss='mse')
    model.summary()

    history = model.fit(train_dataset, epochs=epoch)
```


**Other info / logs**
![image](https://user-images.githubusercontent.com/25941523/151064911-8f1ba15f-16d5-4b63-ad9b-ffb19eb9302e.png)

"
54083,iOS app size increasing,"Hey dear TensorFlow team.
I am using  following lib on my iOS application  `pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['Metal','CoreML']`
Every time when I am trying to read .tflite, my app size is increasing.
I discovered and found some interesting think inside the app container AppData->tmp 
<img width=""1215"" alt=""Screen Shot 2022-01-25 at 22 45 39"" src=""https://user-images.githubusercontent.com/34054539/151039294-2bd9033f-f13f-4f39-93e1-ddc5da661d31.png"">

Why does it create a Core ML Compiled Model each time and not be deleted after being freed?
Please  help me , this is a really blocker 🙏"
54069,Cross compile  libtensorflow-lite.a is ok but many undefined reference when link it,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: ARM V7
- TensorFlow installed from (source or binary): source
- TensorFlow version:  2.6.2
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I can cross-compile the libtensorflow-lite.a successfully for armv7,  but when I link it with an application, I meet the similar problem like #50149. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
The following is how I build the libtensorflow-lite.a
```
ARMCC_FLAGS=""-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations""
ARMCC_PREFIX=/tools/toolchain/gcc-9.1.0-2020.07-x86_64_arm-linux-gnueabihf/bin/arm-linux-gnueabihf-
cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \
  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \
  -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
  -DCMAKE_SYSTEM_NAME=Linux \
  -DCMAKE_SYSTEM_PROCESSOR=armv7 \
  ../tensorflow/lite/
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
54068,Build failure due to cuda include being required on non-cuda platform,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.7
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**
Build fails with
ERROR: /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/compiler/xla/service/gpu/BUILD:468:16: Compiling tensorflow/compiler/xla/service/gpu/nccl_all_to_all_thunk.cc failed: (Exit 1): gcc failed: error executing command 
  (cd /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/builder/.cache/bazelisk/downloads/bazelbuild/bazel-4.2.2-linux-arm64/bin:/home/builder/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
  /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/service/gpu/_objs/nccl_collective_thunks/nccl_all_to_all_thunk.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/service/gpu/_objs/nccl_collective_thunks/nccl_all_to_all_thunk.pic.o' -fPIC '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_GETSPECIFIC=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_LSEEK64=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=""AArch64""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeAArch64AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeAArch64AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeAArch64Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeAArch64Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeAArch64TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeAArch64TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeAArch64TargetMCA' '-DLLVM_HOST_TRIPLE=""aarch64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""aarch64-unknown-linux-gnu""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote. -iquotebazel-out/aarch64-opt/bin -iquoteexternal/eigen_archive -iquotebazel-out/aarch64-opt/bin/external/eigen_archive -iquoteexternal/com_google_absl -iquotebazel-out/aarch64-opt/bin/external/com_google_absl -iquoteexternal/nsync -iquotebazel-out/aarch64-opt/bin/external/nsync -iquoteexternal/gif -iquotebazel-out/aarch64-opt/bin/external/gif -iquoteexternal/libjpeg_turbo -iquotebazel-out/aarch64-opt/bin/external/libjpeg_turbo -iquoteexternal/com_google_protobuf -iquotebazel-out/aarch64-opt/bin/external/com_google_protobuf -iquoteexternal/com_googlesource_code_re2 -iquotebazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -iquoteexternal/farmhash_archive -iquotebazel-out/aarch64-opt/bin/external/farmhash_archive -iquoteexternal/fft2d -iquotebazel-out/aarch64-opt/bin/external/fft2d -iquoteexternal/highwayhash -iquotebazel-out/aarch64-opt/bin/external/highwayhash -iquoteexternal/zlib -iquotebazel-out/aarch64-opt/bin/external/zlib -iquoteexternal/double_conversion -iquotebazel-out/aarch64-opt/bin/external/double_conversion -iquoteexternal/local_config_cuda -iquotebazel-out/aarch64-opt/bin/external/local_config_cuda -iquoteexternal/local_config_rocm -iquotebazel-out/aarch64-opt/bin/external/local_config_rocm -iquoteexternal/local_config_tensorrt -iquotebazel-out/aarch64-opt/bin/external/local_config_tensorrt -iquoteexternal/llvm-project -iquotebazel-out/aarch64-opt/bin/external/llvm-project -iquoteexternal/llvm_terminfo -iquotebazel-out/aarch64-opt/bin/external/llvm_terminfo -iquoteexternal/llvm_zlib -iquotebazel-out/aarch64-opt/bin/external/llvm_zlib -Ibazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/aarch64-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/SubElementInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithmeticBaseIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithmeticCanonicalizationIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithmeticOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/CopyOpInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefBaseIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/TilingInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/AllocationOpInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BufferizableOpInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BufferizationBaseIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BufferizationOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/MathBaseIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/MathOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/canonicalize_inc_gen -Ibazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_ops_inc_gen -Ibazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_base_inc_gen -Ibazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_inc_gen -Ibazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_pattern_gen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen -Ibazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_structs_inc_gen -Ibazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_gpu_ops_enums_inc_gen -Ibazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_gpu_ops_inc_gen -Ibazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_gpu_ops_structs_inc_gen -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/gif -isystem bazel-out/aarch64-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/aarch64-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/aarch64-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/llvm-project/llvm/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/mlir/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/mlir/include -isystem tensorflow/compiler/mlir/hlo/include -isystem bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/hlo/include -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/xla/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-ffp-contract=off' '-std=c++14' '-ffp-contract=off' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -pthread '-DTENSORFLOW_USE_XLA=1' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/compiler/xla/service/gpu/nccl_all_to_all_thunk.cc -o bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/service/gpu/_objs/nccl_collective_thunks/nccl_all_to_all_thunk.pic.o)
Execution platform: @local_execution_config_platform//:platform
In file included from ./tensorflow/stream_executor/gpu/gpu_driver.h:24,
                 from ./tensorflow/stream_executor/gpu/gpu_stream.h:23,
                 from tensorflow/compiler/xla/service/gpu/nccl_all_to_all_thunk.cc:33:
./tensorflow/stream_executor/gpu/gpu_types.h:31:10: fatal error: third_party/gpus/cuda/include/cuComplex.h: No such file or directory
   31 | #include ""third_party/gpus/cuda/include/cuComplex.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
INFO: Elapsed time: 3857.297s, Critical Path: 465.15s
INFO: 30786 processes: 8824 internal, 21962 local.
FAILED: Build did NOT complete successfully



**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --config=nonccl \
>                --copt=-ffp-contract=off --cxxopt=-ffp-contract=off \
>                --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64 \
>                --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64 \
>                -- ${DEFAULT_BAZEL_TARGETS} \
>                -//tensorflow/lite/... -//tensorflow/python/tools/... -//tensorflow/compiler/mlir/lite/tests:const-fold.mlir.test -//tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test -//tensorflow/python/eager:function_test


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Seems to be due to https://github.com/tensorflow/tensorflow/commit/ba1eab4dba24bd9a994941f0ba973fdecfcd187d
"
54067,Cannot reproducibly save a model to disk,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OpenSUSE Leap 15.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): from pip
- TensorFlow version (use command below): 2.8.0-rc1
- Python version: 3.9.10

**Describe the current behavior**

When training and saving the same model twice, I get different outputs stored to disk, even without any training: when saving the exact same model twice in the same Python process (same ""run""), `saved_model.pb` files differ while all other files are identical. Interestingly, when saving the model in two separate runes, `saved_model.pb` files are identical.

**Describe the expected behavior**

Save the exact same files four times (two in each of two runs).

**Standalone code to reproduce the issue**
```python
""""""Demonstrate bug.""""""
import filecmp
import sys

import tensorflow as tf

RUN = 1 if len(sys.argv) > 1 else 0

tf.keras.utils.set_random_seed(0)
tf.config.experimental.enable_op_determinism()
model = tf.keras.applications.vgg16.VGG16(weights=None, classes=1)
model.compile(optimizer=tf.keras.optimizers.Adam(), loss=""categorical_crossentropy"")
model.save(f""tmp_save0_run{RUN}.tf"")
model.save(f""tmp_save1_run{RUN}.tf"")

if not filecmp.cmp(
    f""tmp_save0_run{RUN}.tf/saved_model.pb"",
    f""tmp_save1_run{RUN}.tf/saved_model.pb"",
):
    print(""Files are different in the same run!"")

if (RUN == 1) and filecmp.cmp(
    ""tmp_save0_run0.tf/saved_model.pb"",
    ""tmp_save0_run1.tf/saved_model.pb"",
):
    print(""Files are identical across runs!"")
```

To compare between runs, run twice like this:
```shell
python bug.py && python bug.py X
```

Then compare:

```shell
# compare between two saves within the same run:
diff -r tmp_save0_run0.tf tmp_save1_run0.tf
# compare between two saves of different runs:
diff -r tmp_save0_run0.tf tmp_save0_run1.tf
```

Output:

```
Files are different in the same run!
Files are different in the same run!
Files are identical across runs!
Binary files tmp_save0_run0.tf/saved_model.pb and tmp_save1_run0.tf/saved_model.pb differ
```"
54066,Building Tensorflow lite from source: cannot fix unresolved external symbols in Visual Studio project,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Tensorflow lite (source)
- TensorFlow version: 2.7.0
- GCC/Compiler version (if compiling from source): MSVC 19.26.28806.0

**Describe the problem**

Building Tensorflow lite from source appears to work, in so far as I get the static libraries. However, when I create a Visual Studio project I get several linker errors (unresolved externals - see end of issue for error messages) and whatever libs I try to link, I cannot seem to get anything to build. I have followed the instructions for building the minimal example CMake project and **this appears to work**. However, we need to be able to link tensorflow lite in our application code which isn't managed with CMake, so using CMake isn't an option for us.

In a nutshell: the build process appears to work, but I can't seem to get a Visual Studio project set up. More details below.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Used this link as a guide: https://www.tensorflow.org/lite/guide/build_cmake
2. Cloned the tensorflow repo
3. Created a directory called ""mybuild"" at the top level of the repo
4. Opened a command prompt in mybuild and ran: `cmake ../tensorflow_src/tensorflow/lite`
5. Then ran: `cmake --build . -j`
6. Created a visual studio project console application (see hello world program below)
7. Set up the include directory to point to the tensorflow repo
8. Added tensorflow-lite.lib to the linker input options
9. Added the path to the lib in the additional linker directories
10. Built in x64, and tried both Debug and Release* - both resulted in the same linker error.
11. As a desperate measure, included all the libs in the _deps/ folder in the linker input options - still the same linker error

*I wasn't totally clear whether the lib I'd built was for Debug or Release. The instructions say ""It generates an optimized release binary by default"", but then the built lib ends up in mybuild/Debug which was confusing. Is this a minor bug in the build or are the instructions wrong in saying this?

Hope this is clear enough and I've posted in the right place. Any more info I can provide let me know, thanks.

**Sample code**
```
#include <iostream>
#include <tensorflow/lite/c/c_api.h>

int main()
{
	std::cout << ""Hello from TensorFlow C library version "" << TfLiteVersion() << std::endl;
	return 0;
}
```

**Build from sample project**
```
1>------ Build started: Project: ConsoleApplication2, Configuration: Release x64 ------
1>ConsoleApplication2.cpp
1>ConsoleApplication2.obj : error LNK2001: unresolved external symbol __imp_TfLiteVersion
1>C:\Users\4Sight\source\repos\ConsoleApplication2\x64\Release\ConsoleApplication2.exe : fatal error LNK1120: 1 unresolved externals
1>Done building project ""ConsoleApplication2.vcxproj"" -- FAILED.
========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========
```
"
54065,Compute gradients across two layers using gradients calculated from a previous layer using tf.gradients or tf.GradientTape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): NA
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7.12
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
## I want to use the gradients of one layer to calculate the gradients of the layer that comes before it. 

My motivation for doing this is, when I tried to use model parallelism using tf.device, I found out that backpropagation has been running on CPU. The entire Backprop started running on a chosen tf.device only after I wrapped the call to GradientTape(when it computes the gradient) within the tf.device context manager. Since the model is split, I want the backprop of each partition to execute on the device where that partition is placed.

Ideally, I would like to find out a method with which this oversimplified pseudocode is possible.
```
with tf.device(device_3):
   grad_3 = tf.gradients(loss, trainable_vars_of_partition_3)
 
with tf.device(device_2):
   grad_2 = tf.gradients(grad_3, trainable_vars_of_partition_2)

with tf.device(device_1):
   grad_1 = tf.gradients(grad_2, trainable_vars_of_partition_1)

grads = concat(grad_1, grad_2, grad_3)
```
If something like this exists then I would be overjoyed if you could point me in the right direction. 

Unfortunately, I could not find something as simple as this. The next best approach that I could think of was using the gradients of one layer to find the gradients of a layer that comes before it. Using chain rule and backpropagation, I feel that this should be possible.

I created this toy example, solving which is the first step towards the final goal.


Let's say we have a model with 3 dense layers without activations functions. X, Y as defined as follows:

```
x = tf.concat([tf.random.uniform([1, 10], minval=0, maxval=0.25),
               tf.random.uniform([1, 10], minval=0.25, maxval=0.5),
               tf.random.uniform([1, 10], minval=0.5, maxval=0.75),
               tf.random.uniform([1, 10], minval=0.75, maxval=1.),
                ], axis = 0)

y = tf.constant(0., shape=[4, 1])

d1 = tf.keras.layers.Dense(5, name='d1') 
d2 = tf.keras.layers.Dense(2, name='d2') 
d3 = tf.keras.layers.Dense(1, name='d3') 

```
I am using a tf.function in this toy example but an answer with eager mode enabled, using GradientTape will also be appreciated. 

```
@tf.function
def tf_func(x, y, d1, d2, d3):
    # Using shortforms of these function helped the code look neater and more readable to me. 
    g = tf.gradients
    rs = tf.reduce_sum
    rm = tf.reduce_mean

    o1 = d1(x)
    o2 = d2(o1)
    o3 = d3(o2)

    l = tf.reduce_mean(tf.square(o3 - y))
    
    w3, w2, w1 = d3.trainable_variables, d2.trainable_variables, d1.trainable_variables

    tf.print('actual grads' + '=' * 80)

    dl_dw3 = g(l, w3)
    
    dl_dw2 = g(l, w2)
    tf.print('dl_dw2: \n', dl_dw2)

    dl_dw1 = g(l, w1)   

    tf.print()
    tf.print()
    
    tf.print('reference grads' + '=' * 80)
    dl_do1 = g(l, o1)
    dl_do2 = g(l, o2)
    tf.print('dl_do2: \n', dl_do2)
    dl_do3 = g(l, o3)

    dl_dw1 = g(l, w1)
    dl_dw2 = g(l, w2)
    dl_dw3 = g(l, w3)

    do3_o2 = g(o3, o2)
    do2_do1 = g(o2, o1)

    do3_w3 = g(o3, w3)
    do2_w2 = g(o2, w2)
    do1_w1 = g(o1, w1)


    tf.print('testing chain_rule method' + '=' * 80)
    
    # Added a 't' before derivatives to differentiate between ref_grads and grads obtained using chain rule

    tdl_do3 = g(l, o3) # same as ref_grads

    tdo3_dw3 = g(o3, w3) # same as ref_grads
    tdl_dw3 = [rm(tdl_do3) * tdo3_dw3[0], rm(tdl_do3) * tdo3_dw3[1]] # same as actual grads

    tdo3_do2 = g(o3, o2) # same as ref_grads

    tdl_do2 = tdo3_do2 * rm(tdl_do3, axis=0)  # same as ref_grads
    tf.print('tdl_do2: \n', tdl_do2)

    tdo2_dw2 = g(o2, w2) 
    tf.print('tdo2_dw2: \n', tdo2_dw2)
    
    tdl_dw2 = [tdo2_dw2[0] * rm(tdl_do2, axis=[1]), tdo2_dw2[1] * rm(tdl_do2, axis=[1])]
    tf.print('tdl_dw2: \n', tdl_dw2)

    return None 


tf_func(x, y, d1, d2, d3)
```
The output was:

```
actual grads================================================================================
dl_dw2: 
 [[[-3.04819393 -1.30051827]
 [5.02123785 2.14232159]
 [-0.260933906 -0.111328]
 [5.87596226 2.50699162]
 [1.9655633 0.838611722]], [-4.69162369 -2.0016911]]


reference grads================================================================================
dl_do2: 
 [[[-0.43842113 -0.187053293]
 [-0.889310718 -0.379426271]
 [-1.41650343 -0.604354143]
 [-1.94738865 -0.830857456]]]


testing chain_rule method================================================================================
tdl_do2: 
 [[[-0.43842113 -0.187053293]
  [-0.889310718 -0.379426271]
  [-1.41650343 -0.604354143]
  [-1.94738865 -0.830857456]]]
tdo2_dw2: 
 [[[2.10966444 2.10966444]
 [-3.48670244 -3.48670244]
 [0.22972326 0.22972326]
 [-3.95618558 -3.95618558]
 [-1.3790133 -1.3790133]], [4 4]]
tdl_dw2: 
 [[[-2.47443795 -1.05572414]
 [4.08957386 1.74482536]
 [-0.26944378 -0.114958748]
 [4.64023352 1.97976542]
 [1.61745286 0.690089643]], [[-4.69162369 -2.0016911]]]

```
**For some reason, gradients wrt weights in tdl_dw2 and dl_dw2 differ slightly. Every value in tdl_dw2 is slightly less than dl_dw2 even though the gradients wrt biases are the same. I cannot figure out why.**

The gradient of loss wrt to w3 is as expected. 


I used tf.reduce_mean to replicate what tf.gradients was doing internally as far as I understand. Please correct me if I am wrong.

From Tensorflow's documentations: 

> gradients() adds ops to the graph to output the derivatives of ys with respect to xs. It returns a list of Tensor of length len(xs) where each tensor is the sum(dy/dx) for y in ys and for x in xs.

> tf.gradients constructs symbolic derivatives of sum of ys w.r.t. x in xs.


Any guidance or help will be greatly appreciated, thank you.


Some Similar StackOverflow questions(there are many more):

 1. https://stackoverflow.com/q/69545716/13629760
 2. https://stackoverflow.com/questions/66120442/is-it-possible-to-acquire-an-intermediate-gradient-tensorflow
 3. https://stackoverflow.com/questions/50075961/breaking-tensorflow-gradient-calculation-into-two-or-more-parts

Here is a colab notebook with the code:
https://colab.research.google.com/drive/1034hu6Zo766-spKu5qfeG4c2Yv2v-DGM?usp=sharing
"
54059,allocate temp tensor would be free before the end of the aysnc gpu kernel  execution ,https://github.com/tensorflow/tensorflow/blob/a9e4e55bcfaa95c40b36ef9c85991e3534532b2c/tensorflow/core/kernels/gpu_device_array.h#L83
54001,Some operations didn't work in tflite train procedure.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS
- TensorFlow version (use command below): tensorflow 2.7.0
- Python version: python 3.9
- CUDA/cuDNN version: Cuda 11.2/ cudnn 8.2
- GPU model and memory: rtx A6000

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
 I want to generate fully train able tflite model. I found mapped ops in ['tfl' Dialect](https://www.tensorflow.org/mlir/tfl_ops?hl=en). And then, I use these functions in my model.  All operation can convert to graph model use converter.convert(), but it dosen't work. Some operation can use in inference, but can't use in train without any error just stop.(ex. Avgpool2d). 

**Describe the expected behavior**

I have three questions in upper problem.

 First, Is this operation(avgpool) can't use in  tflite?

 Second, If first question is true, can i get trainable operation documentation in tflite?

 Third, When can i use fully trainable tflite model in mobile?


**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**


** custom source code
```
`import tensorflow as tf
import numpy as np

import h5py
import json

import os
os.environ[""CUDA_VISIBLE_DEVICES""]=""3""
print(tf.__version__)
print(""here"")


class Deepphys(tf.keras.Model):
    def __init__(self):
        super(Deepphys,self).__init__()
        self.a_1 = tf.keras.layers.ZeroPadding2D(1)
        self.a_2 = tf.keras.layers.Conv2D(filters=32,kernel_size=3,name='a_2')
        # self.a_3 = tf.keras.layers.BatchNormalization()

        self.a_4 = tf.keras.layers.ZeroPadding2D(1)
        self.a_5 = tf.keras.layers.Conv2D(filters=32, kernel_size=3,name='a_5')
        # self.a_6 = tf.keras.layers.BatchNormalization()

        self.a_7 = tf.keras.layers.AveragePooling2D(pool_size =2,strides=2)

        self.a_8 = tf.keras.layers.ZeroPadding2D(1)
        self.a_9 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,name='a_7')
        # self.a_10 = tf.keras.layers.BatchNormalization()

        self.a_11 = tf.keras.layers.ZeroPadding2D(1)
        self.a_12 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,name='a_12')
        # self.a_13 = tf.keras.layers.BatchNormalization()

        self.att_conv_1 = tf.keras.layers.Conv2D(filters=1,kernel_size=1,activation='sigmoid',name='att_1')
        self.att_conv_2 = tf.keras.layers.Conv2D(filters=1, kernel_size=1,activation='sigmoid',name='att_2')

        self.m_1 = tf.keras.layers.ZeroPadding2D(1)
        self.m_2 = tf.keras.layers.Conv2D(filters=32,kernel_size=3,padding='valid',name='m_2')
        # self.m_3 = tf.keras.layers.BatchNormalization()

        self.m_4 = tf.keras.layers.ZeroPadding2D(1)
        self.m_5 = tf.keras.layers.Conv2D(filters=32,kernel_size=3,name='m_5')
        # self.m_6 = tf.keras.layers.BatchNormalization()

        # self.m_7 = tf.keras.layers.AveragePooling2D(pool_size=2, strides=2)
        self.m_7 = tf.keras.layers.Conv2D(filters = 32, kernel_size=2, strides=2)


        self.m_8 = tf.keras.layers.ZeroPadding2D(1)
        self.m_9 = tf.keras.layers.Conv2D(filters=64,kernel_size=3,strides=1,name='m_9')
        # self.m_10 = tf.keras.layers.BatchNormalization()

        self.m_11 = tf.keras.layers.ZeroPadding2D(1)
        self.m_12 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,name='m_12')
        # self.m_13 = tf.keras.layers.BatchNormalization()

        # self.m_14 = tf.keras.layers.AveragePooling2D(pool_size=2,strides=2)
        self.m_14 = tf.keras.layers.Conv2D(filters=64, kernel_size=2, strides=2)

        self.f_1 = tf.keras.layers.Flatten()
        self.f_2 = tf.keras.layers.Dense(256)
        self.f_3 = tf.keras.layers.Dense(1)


    @tf.function(input_signature=[
        tf.TensorSpec([None,36,36,6], tf.float32),
    ])
    def call(self,inputs):
        _in = tf.split(inputs,2,axis=3)
        A = self.a_1(_in[1])
        A = self.a_2(A)
        # A = self.a_3(A)
        A = tf.keras.activations.tanh(A)

        A = self.a_4(A)
        A = self.a_5(A)
        # A = self.a_6(A)
        A = tf.keras.activations.tanh(A)

        M1 = self.att_conv_1(A)
        # M1 = tf.keras.activations.sigmoid(M1)
        # B,_,H,W = tf.shape(M1)
        norm = tf.abs(M1)
        norm = tf.math.reduce_sum(norm)
        norm = 2 * norm
        norm = tf.reshape(norm,[1,1,1,1])
        M1 = tf.divide(M1,norm)

        A = self.a_7(A)
        A = self.a_8(A)
        A = self.a_9(A)
        # A = self.a_10(A)
        A = tf.keras.activations.tanh(A)

        A = self.a_11(A)
        A = self.a_12(A)
        # A = self.a_13(A)
        A = tf.keras.activations.tanh(A)

        M2 = self.att_conv_2(A)
        # M2 = tf.keras.activations.sigmoid(M2)
        # B, _, H, W = tf.shape(M2)
        norm = tf.abs(M2)
        norm = tf.math.reduce_sum(norm)
        norm = 2 * norm
        norm = tf.reshape(norm, [1, 1, 1, 1])
        M2 = tf.divide(M2, norm)

        M = self.m_1(_in[0])
        M = self.m_2(M)
        # M = self.m_3(M)
        M = tf.keras.activations.tanh(M)

        M = self.m_4(M)
        M = self.m_5(M)
        # M = self.m_6(M)

        ones_1 = tf.ones([1,36,36,36])
        mat_1 = ones_1 @ M1 #tf.matmul(ones_1,M1)
        g1 = mat_1*M
        M = tf.keras.activations.tanh(g1)

        M = self.m_7(M)

        M = self.m_8(M)
        M = self.m_9(M)
        # M = self.m_10(M)
        M = tf.keras.activations.tanh(M)

        M = self.m_11(M)
        M = self.m_12(M)
        # M = self.m_13(M)

        ones_2 = tf.ones([1,18,18,18])
        mat_2 = ones_2 @ M
        g2 = mat_2 * M
        M = tf.keras.activations.tanh(g2)

        M = tf.keras.activations.tanh(M)

        M = self.m_14(M)

        F = self.f_1(M)
        F = self.f_2(F)
        F = self.f_3(F)

        return F

class Model(tf.Module):
    def __init__(self):
        self.model = Deepphys()
        self.model.compile(
            optimizer='adam',
            loss=tf.keras.losses.mean_squared_error)


    @tf.function(input_signature=[
        tf.TensorSpec([1, 36,36,6], tf.float32),
        tf.TensorSpec([1, ], tf.float32),
    ])
    def train(self, x, y):
        with tf.GradientTape() as tape:
            predictions = self.model(x)
            loss = self.model.loss(predictions, y)
        gradients = tape.gradient(loss, self.model.trainable_variables)
        self.model.optimizer.apply_gradients(
            zip(gradients, self.model.trainable_variables))
        result = {""loss"":loss}
        return result

    @tf.function(input_signature=[
        tf.TensorSpec([None,36,36,6], tf.float32),
    ])
    def infer(self, x):

        output = self.model(x)
        return {
            ""output"" : output
        }

    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
    def save(self, checkpoint_path):
        tensor_names = [weight.name for weight in self.model.weights]
        tensors_to_save = [weight.read_value() for weight in self.model.weights]
        tf.raw_ops.Save(
            filename=checkpoint_path, tensor_names=tensor_names,
            data=tensors_to_save, name='save')
        return {
            ""checkpoint_path"": checkpoint_path
        }

    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
    def restore(self, checkpoint_path):
        restored_tensors = {}
        for var in self.model.weights:
            restored = tf.raw_ops.Restore(
                file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,
                name='restore')
            var.assign(restored)
            restored_tensors[var.name] = restored
        return restored_tensors

with open('../../params.json') as f:
    jsonObject = json.load(f)
    params = jsonObject.get(""params"")
    model_params = jsonObject.get(""model_params"")
    save_root_path = params[""save_root_path""]
    model_name = model_params[""name""]
    dataset_name = params[""dataset_name""]
    option=""train""

hpy_file = h5py.File(save_root_path + model_name + ""_"" + dataset_name + ""_"" + option + "".hdf5"", ""r"")
video_data = []
label_data = []
for key in hpy_file.keys():
     video_data.extend(hpy_file[key]['preprocessed_video'])
     label_data.extend(hpy_file[key]['preprocessed_label'])
hpy_file.close()

# train_loader = DataLoader(video_data,label_data,1)

# train_loader = DataLoader(video_data[:(int)(video_data.__len__()*0.8)],label_data[:(int)(label_data.__len__()*0.8)],1)
# valid_loader = DataLoader(video_data[(int)(video_data.__len__()*0.8):],label_data[(int)(label_data.__len__()*0.8):],1)
# test_loader = DataLoader(video_data,label_data,32)

NUM_EPOCHS = 10
BATCH_SIZE = 1
epochs = np.arange(1, NUM_EPOCHS + 1, 1)
losses = np.zeros([NUM_EPOCHS])
m = Model()
# m.model.build(input_shape=(1,36,36,6))
# m.model.summary()
video_data = np.asarray(video_data)
label_data = np.asarray(label_data)
train_ds = tf.data.Dataset.from_tensor_slices((video_data[:2],label_data[:2]))
train_ds = train_ds.batch(BATCH_SIZE)

# trian
for i in range(NUM_EPOCHS):
  for x,y in train_ds:
    # x = tf.reshape(x, [-1, 2, 36, 36, 3])
    x = tf.cast(x,tf.float32)
    # y = tf.cast(y,tf.float32)
    result = m.train(x, y)
  losses[i] = result['loss']
  if (i + 1) % 1 == 0:
      # print(result['loss'])
   print(f""Finished {i+1} epochs"")
   print(f""  loss: {losses[i]:.3f}"")

m.save('/tmp/trained_model')

SAVED_MODEL_DIR = ""./tmp""

tf.saved_model.save(
    m,
    SAVED_MODEL_DIR,
    signatures={
        'train':
            m.train.get_concrete_function(),
        'infer':
            m.infer.get_concrete_function(),
        # 'save':
        #     m.save.get_concrete_function(),
        # 'restore':
        #     m.restore.get_concrete_function(),
    })

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)
# converter = tf.lite.TFLiteConverter.from_concrete_functions()
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]

converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()
f = open('test.tflite','wb')
f.write(tflite_model)
f.close()

interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

infer = interpreter.get_signature_runner(""infer"")

for x, _ in train_ds:
    x = tf.cast(x, tf.float32)
    logits_original = m.infer(x=x)['output'][0]
    logits_lite = infer(x=x)['output'][0]

    # print(logits_original, logits_lite)

print(""try"")
train = interpreter.get_signature_runner(""train"")

# train_loader = DataLoader(video_data[:(int)(video_data.__len__()*0.8)],label_data[:(int)(label_data.__len__()*0.8)],1)
print(""tried"")
for i in range(NUM_EPOCHS):
  for x,y in train_ds:
    x = tf.cast(x,tf.float32)
    y = tf.cast(y,tf.float32)
    result = train(x=x,y=y)
    print(result)



# # EncoderBlock
            #
            # # ConvBlock1
            # tf.keras.layers.ZeroPadding3D(padding=(0,2,2),input_shape=(32,128,128,3)),
            # tf.keras.layers.Conv3D(filters=16,kernel_size=(1,5,5),strides=(1,1,1)),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ReLU(),
            # #MaxPool3d1r
            # # tf.keras.layers.MaxPool3D(pool_size=(1,2,2), strides=(1,2,2)),
            # tf.keras.layers.Conv3D(filters=16, kernel_size=(1, 2, 2), strides=(1, 2, 2)),
            # # #
            # # # ConvBlock2
            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),
            # tf.keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), strides=(1, 1, 1)),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ReLU(),
            # # ConvBlock3
            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),
            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ReLU(),
            #
            # # # MaxPool3d2
            # tf.keras.layers.Conv3D(filters=64, kernel_size=(2, 2, 2), strides=(2, 2, 2)),
            # # tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2)),
            # #
            # # ConvBlock4
            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),
            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ReLU(),
            # # ConvBlock5
            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),
            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ReLU(),
            # #
            # # # MaxPool3d3
            # # tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2)),
            # tf.keras.layers.Conv3D(filters=64, kernel_size=(2, 2, 2), strides=(2, 2, 2)),
            # #
            # # # ConvBlock6
            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),
            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ReLU(),
            # # ConvBlock7
            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),
            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ReLU(),
            # #
            # # # MaxPool3d4
            # # tf.keras.layers.MaxPool3D(pool_size=(1, 2, 2), strides=(1, 2, 2)),
            # tf.keras.layers.Conv3D(filters=64, kernel_size=(1, 2, 2), strides=(1, 2, 2)),
            # # #
            # # # ConvBlock8
            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),
            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ReLU(),
            # # ConvBlock9
            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),
            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ReLU(),
            #
            # # # #
            # # # # DecoderBlock
            # # # #
            # # DeConvBlock1
            # tf.keras.layers.Convolution3DTranspose(filters=64,kernel_size=(4,1,1),strides=(2,1,1),padding='same'),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ELU(),
            #
            # # DeConvBlock2
            # tf.keras.layers.Convolution3DTranspose(filters=64, kernel_size=(4, 1, 1), strides=(2, 1, 1),padding='same'),
            # tf.keras.layers.BatchNormalization(),
            # tf.keras.layers.ELU(),
            #
            # #
            # # AdaptivePooling
            # #
            # AdaptivePooling3D(tf.reduce_max,(32,1,1)),
            # # #
            # #Conv3D
            # #
            # # tf.keras.layers.Conv3D(filters=32,kernel_size=(8,8,1)),
            #
            # tf.keras.layers.Conv3D(1,kernel_size=(1,64,64),strides=(1,1,1),padding='same'),
            # tf.keras.layers.Reshape((-1,)),`


```"
53951,Tensorflow Lite: Using output index causes segfault. ,"I am using Tensorflow Lite C++ interface for running inference, commit hash `040585c0f25681b399c9087b53c982959bcca44f` (HEAD of master branch at the time of posting this). 
I am using a model with 2 inputs, and a single output (array). 

I am trying to run the following code as a sanity check: 

```
#include <iostream>
#include <fstream>
#include <vector>
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""

#include <opencv2/opencv.hpp>

using namespace tflite;

int main() {
    std::ifstream modelFile (""../models/liveness.tflite"", std::ios::binary | std::ios::ate);
    std::streamsize size = modelFile.tellg();
    modelFile.seekg(0, std::ios::beg);

    std::vector<char> buffer(size);

    if (!modelFile.read(buffer.data(), size)) {
        throw std::runtime_error(""There was a issue reading the model file into memory"");
    }

    StderrReporter errorReporter;
    auto model = FlatBufferModel::BuildFromBuffer(buffer.data(), size, &errorReporter);
    if (model == nullptr) {
        throw std::runtime_error(""Error creating builder from model file"");
    }

    tflite::ops::builtin::BuiltinOpResolver resolver;

    std::unique_ptr<Interpreter> interpreter;

    if (InterpreterBuilder(*model, resolver)(&interpreter) != kTfLiteOk) {
        throw std::runtime_error(""Unable to create interpreter"");
        // Return failure.
    }

    auto inputs = interpreter->inputs();
    for (const auto input: inputs) {
        std::cout << ""Input: "" << input << std::endl;
        std::cout << ""Input name: "" << interpreter->GetInputName(input) << std::endl;
    }

    auto outputs = interpreter->outputs();
    for (const auto output: outputs) {
        std::cout << ""Output: "" << output << std::endl;
        std::cout << ""Output name: "" << interpreter->GetOutputName(output) << std::endl;
    }

    return 0;
}
```

When I run the above, it prints:
```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Input: 199
Input name: batch_normalization/batchnorm/add_1
Input: 200
Input name: batch_normalization/batchnorm/add_1
Output: 193
Segmentation fault (core dumped)

```
As can be seen, the call to ` std::cout << ""Output name: "" << interpreter->GetOutputName(output) << std::endl;` causes a segfault. 

If I instead change it to ` std::cout << ""Output name: "" << interpreter->GetOutputName(0) << std::endl;` it prints `Output name: dense/Softmax` 

Why is the first version calling a segfault? 

As a sanity check, I use python to run this snippet of code with the same model:
```
    output_details = interpreter.get_output_details()

    print(output_details)

```

This prints:

```
[{'name': 'dense/Softmax', 'index': 193, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]

```

The reason I even ask this question is because I expect to be able to obtain the inference results by calling:
```
float* out = interpreter->typed_output_tensor<float>(interpreter->outputs()[0]);
```

However, this too causes a segmentation fault. 

Running `float* out = interpreter->typed_output_tensor<float>(0);` does give me the correct output.
"
53939,backporting setup.py relaxation of requirements to 2.7,"Hi folks wondering if it would be possible to backport https://github.com/tensorflow/tensorflow/commit/41e66148999080da5b2b80a59740a8f2a59f17da to 2.7 since that would make 2.7 way more flexible to install on environments.


"
53936,TFRT saved_model_test registers type names in variant_op_registry multiple times,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.9 (stretch)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: master
- Python version: 3.8
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source): 8.2.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
The target `//tensorflow/core/tfrt/saved_model/tests:saved_model_test` somehow caused multiple call to register actions in variant_op_registry.cc.

E.g. This line [`REGISTER_UNARY_VARIANT_DECODE_FUNCTION(Tensor, ""tensorflow::Tensor"");`](https://github.com/tensorflow/tensorflow/blob/ae9e5b2aa9cad5da4e614dfb40766c8e64bfc2d6/tensorflow/core/framework/tensor.cc#L71) is called two times and fail check:
> 2022-01-24 19:21:42.384846: F tensorflow/core/framework/variant_op_registry.cc:77] Check failed: existing == nullptr (0x6d2e2e0 vs. nullptr)Unary VariantDecodeFn for type_name: tensorflow::Tensor already registered



**Provide the exact sequence of commands / steps that you executed before running into the problem**
I did some preprocessing steps to get this test to run:
1. Set visibility to public for `//tensorflow/core/tfrt/callback:op_kernel_runner` (because build fails on visibility)
2. Add `""@com_google_absl//absl/flags:flag""` dependency to `//tensorflow/core/tfrt/saved_model:saved_model_testutil`; Add
    ```
    #include ""absl/flags/declare.h""
    #include ""absl/flags/flag.h""
    ```
    to `//tensorflow/core/tfrt/saved_model/saved_model_testutil.h`
 (Otherwise `ABSL_DECLARE_FLAG` is not defined)
3. Remove the `no_oss` tag in`//tensorflow/core/tfrt/saved_model/tests:saved_model_test` (Otherwise it won't run)

And then run
```
bazel test --config=tfrt tensorflow/core/tfrt/saved_model/tests:saved_model_test
```




**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53911,iterating over `tf.Tensor` is not allowed,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.3
- Python version:3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I have a dictionary that I build by accessing only the channel dimensions of an output layer of a convolutional neural network (it has a shape (100,24,24,6) )
Therefore keys of this dictionary are tuples of tensor shape (6, ) . I want to map these keys to the input of the next layer using the tf.map_fn(). However, i am incapable of doing it because the keys of my dictionary are of type tensor and i cannot iterate over them .
Looking for some help. Thank you.
**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53910,iterating over `tf.Tensor` is not allowed,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
I have a dictionary that I build by accessing only the channel dimensions of an output layer of a convolutional neural network (it has a shape (100,24,24,6) )
Therefore keys of this dictionary are tuples of tensor shape (6, ) . I want to map these keys to the input of the next layer using the tf.map_fn(). However, i am incapable of doing it because the keys of my dictionary are of type tensor and i cannot iterate over them .
Looking for some help. Thank you.

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
53908,Not getting consistent results with .h5 and .tflite models on different machines,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tested on Windows 11, Intel Mac, Mac M1 v11.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary (tensorflow for windows and tensorflow-macos for Mac)
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.12
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: 11.0 (for training only)
- GPU model and memory: Mac M1

**Describe the current behavior**
The same h5 and/or TFLite model gives different outputs on different machines. For an image regression model where ground truth labels range between [0, 3] the results differ by ~0.2-0.3 on different machines, which causes thresholding issues.

**Describe the expected behavior**
The same model should theoretically perform the same on all machines or at least they should match upto more precision.

**Standalone code to reproduce the issue**
The current model arch and the compilation info:
```
base_model = tf.keras.applications.MobileNetV3Small(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),
                                               include_top=False,
                                               weights='imagenet',
                                               minimalistic=True)

base_model.trainable = True
    
model = tf.keras.Sequential([
        base_model,
        #pretrained_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        #tf.keras.layers.Dense(256,activation = 'relu',dtype=tf.float32),
        tf.keras.layers.Dense(2, dtype=tf.float32)
    ])

model.compile(
        loss = tf.keras.losses.MeanAbsoluteError(),
        optimizer = tf.keras.optimizers.Adam(),
        metrics = [tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.MeanSquaredError()],
        steps_per_execution=64
    )

model.summary()
```
```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
MobilenetV3small (Functional (None, 4, 4, 1024)        1031848   
_________________________________________________________________
global_average_pooling2d (Gl (None, 1024)              0         
_________________________________________________________________
dense (Dense)                (None, 2)                 2050      
=================================================================
Total params: 1,033,898
Trainable params: 1,021,786
Non-trainable params: 12,112
```
The model was converted to the usual TFLite format as well, but the issue persisted.
```
# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

The model, thus obtained, can be used to reproduce the issue on different machines. Any ideas on why it might be happening and how to deal with it would mean a lot. Thanks!"
53907,[tensorflow/examples] TensorFlow Lite Example for Android: Insecure protocols error during build,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 Entreprise 21H1**
- Android Studio Version: **Android Studio Arctic Fox 2020.3.1 Patch 4**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Any android device**
- tensorflow/examples Repo Version: [**Commit eb925e4**](https://github.com/tensorflow/examples/tree/eb925e4)
- TensorFlow version: **tensorflow-lite:2.5.0**



**Describe the problem**
When I build any Android example, I get this error :
```
> Using insecure protocols with repositories, without explicit opt-in, is unsupported. Switch Maven repository 'ossrh-snapshot(http://oss.sonatype.org/content/repositories/snapshots)' to redirect to a secure protocol (like HTTPS) or allow insecure protocols. See https://docs.gradle.org/7.3.2/dsl/org.gradle.api.artifacts.repositories.UrlArtifactRepository.html#org.gradle.api.artifacts.repositories.UrlArtifactRepository:allowInsecureProtocol for more details. 
```

We need to set `allowInsecureProtocol` to `true` in the build.gradle for the example to compile. 
```
maven {
    name 'ossrh-snapshot'
    url 'http://oss.sonatype.org/content/repositories/snapshots'
    allowInsecureProtocol = true
    }
```
This happens on this example and all other android examples: 
https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android  

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Get the tensorflow/examples repo with  `git clone https://github.com/tensorflow/examples`  
1. Open `examples\lite\examples\object_detection\android` in Android Studio  
1. Run `Build > Rebuild Project`  
"
53906,"This program requires version 3.9.0 of the Protocol Buffer runtime library, but the installed version is 3.8.0. Please update your library","env:
16.04.1-Ubuntu X86_64, GPU Driver 470.94, Cuda 11.2, Cudnn 8.1

i am tring to build the tensorflow 2.6.2 version with bazel 3.7.2 and Gcc 7.5.0. After about two hours, it actually generate the following shared libs:
libtensorflow_cc.so.2.6.2, libtensorflow_framework.so.2.6.2

however, when i excute myself program which link to above shared libraries, it tips following errors:

[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/stubs/common.cc:75] This program requires version 3.9.0 of the Protocol Buffer runtime library, but the installed version is 3.8.0. Please update your library. If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.

Actually, i check that the environment protobuf version with ""protoc --version"", it tells 3.9.2 version.

i also remove all the protobuf installed before, downloading the 3.9.2 protobuf version and install. it still tells the same errors.

Did the same question occure in your programe? can you give me some advices or a solution?

Thanks
"
53901,DeepXDE for 1D Time-dependent Wave Equation,I have published a paper about modeling a 1D wave equation with a single time-dependent source. It would be great to see it in the list of related publications.
53879,Cause: 'arguments' object has no attribute 'posonlyargs',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 professional 19042.1466
- TensorFlow installed from (source or binary):pip install 
- TensorFlow version (use command below):v2.7.0-rc1-69-gc256c071bb2 , 2.7.0
- Python version:3.7
- CUDA/cuDNN version: CUDA:11.4.0_471.11,cuDNN:8.3.2.44
- GPU model and memory:GTX 970,4G memory

**Describe the current behavior**

I use SARSA Lambda mode in CPU mode, 10 rounds per second, and in GPU DQN mode, 1 round 100 seconds, GPU utilization is less than 1%， and the prompt:
```
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001FD428A85E8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```
I don't know if the slow performance is related to this hint.

**Describe the expected behavior**
I expect performance not lower than SARSA Lambda mode.

**Standalone code to reproduce the issue**
[https://github.com/jaried/Tensorflow/blob/main/MountainCar.py](https://github.com/jaried/Tensorflow/blob/main/MountainCar.py)

**Other info / logs** Include any logs or source code that would be helpful to
prof file:
[https://github.com/jaried/Tensorflow/blob/main/question.prof](https://github.com/jaried/Tensorflow/blob/main/question.prof)
"
53871,Count number of leaves in the ensemble gradient BoostedTree,"This is an issue related to the performance of [TensorFlow.BoostedTree](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier)




**System information**
- TensorFlow version (you are using): 2.7.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Is there any method that returns the number of entire leaves in the built ensemble?
If the feature already exists, would you please introduce it a bit more here, if not could we develop it?

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Users of the tensorflow

"
53870,Generated debug info can not be read by TensorBoard under distributed training environment,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, a distributed training example
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Train on CentOS 7; Read the dump on Macbook M1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Does not apply
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v2.8.0-rc0-24-g671aa8d 2.8.0-rc0
- Python version: 3.8.3
- Bazel version (if compiling from source): bazel 4.2.2
- GCC/Compiler version (if compiling from source): GCC 9.3.0
- CUDA/cuDNN version: Does not apply
- GPU model and memory: Does not apply

**Describe the current behavior**

The dump files can not be read by TensorBoard under the distributed training environment

**Describe the expected behavior**

It can be read from TensorBoard.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
- Run the following code and generate the dump files:

```python
import tensorflow as tf
import tensorflow_datasets as tfds
import os
import json
import sys
import time
from absl import logging

num_epochs = 1
batch_size_per_replica = 64
learning_rate = 0.001

worker_idx = int(sys.argv[1])
num_workers = 2
os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [""10.3.1.101:20000"", ""10.3.1.102:20000""]
    },
    'task': {'type': 'worker', 'index': worker_idx}
})

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'

logging.set_verbosity(logging.DEBUG)

################ HERE ######################
tf.debugging.experimental.enable_dump_debug_info('/home/geng.161/my-tfdbg-dumps', tensor_debug_mode=""FULL_HEALTH"", circular_buffer_size=-1)
################ HERE ######################

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
batch_size = batch_size_per_replica * num_workers

def resize(image, label):
    image = tf.image.resize(image, [224, 224]) / 255.0
    return image, label

dataset = tfds.load(""cats_vs_dogs"", split=tfds.Split.TRAIN, as_supervised=True)
dataset = dataset.map(resize).shuffle(1024).batch(batch_size)

with strategy.scope():
    model = tf.keras.applications.MobileNetV2(weights=None, classes=2)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=[tf.keras.metrics.sparse_categorical_accuracy]
    )

begin = time.time()
model.fit(dataset, epochs=num_epochs)
end = time.time()
print(""Train Time: %s"" % (end - begin))
```

- Copy the dumps on my laptop, and use TensorFlow to read the dumps.
```bash
tensorboard --logdir ~/Downloads/my-tfdbg-dumps
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
TensorBoard 2.8.0 at http://localhost:6006/ (Press CTRL+C to quit)
```

But I got nothing on the TensorBoard,
![image](https://user-images.githubusercontent.com/14839342/150647620-6b9923c6-d0c2-49d6-a507-925929a59f28.png)

I've posted this issue on the TensorBoard repo, but they think this is a bug stem from TensorFlow since the dumps can be read without the distributed strategy. They suggested that I should post an issue on the TensorFlow repo.

Refer: https://github.com/tensorflow/tensorboard/issues/5518
"
53869,Installing tensorflow with pip,"**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**

When installing tensorflow on windows, after `pip install tensorflow`, it further needs to install Cuda and cudann and also needs to add their paths to windows environmental utility.  I'm not sure why we need to face this hassle whereas, in pytorch, we don't see such things. 

**Will this change the current api? How?** dunno. 

**Who will benefit from this feature?** all of the painful souls who use tensorflow

**Any Other info.**
"
53866,Node is not unique when frozen graph using convert_variables_to_constants_v2(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Windows 10/11 and Ubutun 20
- TensorFlow installed from pip
- tf 2.5.0/2.7.0/2.9nightly
- Python 3.6.9/3.8
- NA
- NA:
- NA:

**Describe the current behavior**

From google/deeplab2/export_model.py, add the following line:
```
  signatures = module.__call__.get_concrete_function(module.get_input_spec())
  #following is the added code.
  frozen_func = convert_variables_to_constants_v2(signatures)
```
This issue also appears in tf2onnx tool which uses this convert_variables_to_constants_v2() call.

Full command to run the above script.
```
python export.py
--experiment_option_path
""./configs/cityscapes/axial_deeplab/max_deeplab_l_backbone_os16.textproto""
--checkpoint_path
""C:/develop/max_deeplab_l_backbone_os16_axial_deeplab_cityscapes_trainfine_saved_model/""
```


- Do you want to contribute a PR? (yes/no):
no. I have no profound knowledge in this conversion, and it may take too much time for me to look into it.

**Standalone code to reproduce the issue**
[export.py](https://github.com/google-research/deeplab2/blob/main/export_model.py).
Can get the save model from the following page, I used MaX-DeepLab-L-Backbone
https://github.com/google-research/deeplab2/blob/main/g3doc/projects/axial_deeplab.md

**Other info / logs**

> 
> 2022-01-21 17:28:22.472844: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
> 2022-01-21 17:28:22.473118: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
> Skipping registering GPU devices...
> 2022-01-21 17:28:33.104143: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> I0121 17:28:33.102456 21356 deeplab.py:57] Synchronized Batchnorm is used.
> I0121 17:28:33.110966 21356 axial_resnet_instances.py:144] Axial-ResNet final config: {'num_blocks': [3, 6, 3, 3], 'backbone_layer_multiplier': 1.0, 'width_multiplier': 1.0, 'stem_width_multiplier': 1.0, 'output_stride': 16, 'classification_mode': True, 'backbone_type': 'wider_resnet', 'use_axial_beyond_stride': 16, 'backbone_use_transformer_beyond_stride': 0, 'extra_decoder_use_transformer_beyond_stride': 16, 'backbone_decoder_num_stacks': 1, 'backbone_decoder_blocks_per_stage': 1, 'extra_decoder_num_stacks': 1, 'extra_decoder_blocks_per_stage': 3, 'max_num_mask_slots': 128, 'num_mask_slots': 128, 'memory_channels': 512, 'base_transformer_expansion': 2.0, 'global_feed_forward_network_channels': 512, 'high_resolution_output_stride': 4, 'activation': 'relu', 'block_group_config': {'attention_bottleneck_expansion': 4, 'drop_path_keep_prob': 1.0, 'drop_path_beyond_stride': 4, 'drop_path_schedule': 'linear', 'positional_encoding_type': None, 'use_global_beyond_stride': 0, 'use_sac_beyond_stride': -1, 'use_squeeze_and_excite': False, 'conv_use_recompute_grad': False, 'axial_use_recompute_grad': True, 'recompute_within_stride': 0, 'transformer_use_recompute_grad': False, 'axial_layer_config': {'query_shape': (129, 129), 'key_expansion': 2, 'value_expansion': 4, 'memory_flange': (32, 32), 'double_global_attention': False, 'num_heads': 8, 'use_query_rpe_similarity': True, 'use_key_rpe_similarity': True, 'use_content_similarity': True, 'retrieve_value_rpe': True, 'retrieve_value_content': True, 'initialization_std_for_query_key_rpe': 1.0, 'initialization_std_for_value_rpe': 1.0, 'self_attention_activation': 'softmax'}, 'dual_path_transformer_layer_config': {'num_heads': 8, 'bottleneck_expansion': 2, 'key_expansion': 1, 'value_expansion': 2, 'feed_forward_network_channels': 2048, 'use_memory_self_attention': True, 'use_pixel2memory_feedback_attention': True, 'transformer_activation': 'softmax'}}, 'bn_layer': functools.partial(<class 'keras.layers.normalization.batch_normalization.SyncBatchNormalization'>, momentum=0.9900000095367432, epsilon=0.0010000000474974513), 'conv_kernel_weight_decay': 0.0}
> I0121 17:28:33.613099 21356 deeplab.py:96] Setting pooling size to (65, 129)
> I0121 17:28:36.232671 21356 api.py:446] Eval with scales ListWrapper([1.0])
> I0121 17:28:39.102459 21356 api.py:446] Eval scale 1.0; setting pooling size to [None, None]
> I0121 17:28:58.836912 21356 api.py:446] Eval with scales ListWrapper([1.0])
> I0121 17:28:58.883746 21356 api.py:446] Eval scale 1.0; setting pooling size to [None, None]
> 2022-01-21 17:40:38.838944: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
> 2022-01-21 17:40:38.847874: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
> 2022-01-21 17:40:38.883561: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
> Skipping registering GPU devices...
> 2022-01-21 17:40:40.300834: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
>   function_optimizer: Graph size after: 4654 nodes (1206), 6100 edges (1886), time = 239.541ms.
>   function_optimizer: function_optimizer did nothing. time = 2.73ms.
> Traceback (most recent call last):
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\framework\importer.py"", line 497, in _import_graph_def_internal
>     graph._c_graph, serialized, options)  # pylint: disable=protected-access
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'DeepLab/max_deeplab_l_backbone/stage4/block2/attention/height_axis/query_rpe/Gather/axis' is not unique
> During handling of the above exception, another exception occurred:
> Traceback (most recent call last):
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\absl\app.py"", line 303, in run
>     _run_main(main, args)
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\absl\app.py"", line 251, in _run_main
>     sys.exit(main(argv))
>   File ""C:/develop/deeplab2/export_model.py"", line 158, in main
>     frozen_func = convert_variables_to_constants_v2(signatures)
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\framework\convert_to_constants.py"", line 1154, in convert_variables_to_constants_v2
>     converted_input_indices)
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\framework\convert_to_constants.py"", line 1080, in _construct_concrete_function
>     new_output_names)
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\eager\wrap_function.py"", line 650, in function_from_graph_def
>     wrapped_import = wrap_function(_imports_graph_def, [])
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\eager\wrap_function.py"", line 628, in wrap_function
>     collections={}),
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 1007, in func_graph_from_py_func
>     func_outputs = python_func(*func_args, **func_kwargs)
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\eager\wrap_function.py"", line 87, in __call__
>     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\eager\wrap_function.py"", line 93, in wrapped
>     return fn(*args, **kwargs)
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\eager\wrap_function.py"", line 648, in _imports_graph_def
>     importer.import_graph_def(graph_def, name="""")
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\util\deprecation.py"", line 549, in new_func
>     return func(*args, **kwargs)
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\framework\importer.py"", line 405, in import_graph_def
>     producer_op_list=producer_op_list)
>   File ""C:\Users\tfan\.conda\envs\XNNC\lib\site-packages\tensorflow\python\framework\importer.py"", line 501, in _import_graph_def_internal
>     raise ValueError(str(e))
> ValueError: Node 'DeepLab/max_deeplab_l_backbone/stage4/block2/attention/height_axis/query_rpe/Gather/axis' is not unique
"
53865,Tensorflow Dataset: Image too large,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary):  **binary**
- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**
- Python version: **3.8.9**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **11.2 / 8.1**
- GPU model and memory: **Tested on RTX 3080 10 GB & V100 16 GB**

**Describe the current behavior**

Hello, I am creating a TF Dataset from Gigapixel WSIs (whole slide images). The dimensions of them are 51968 x 37632 x 1 (grayscale) yielding a size of 1955659776. 

```python
WSI_SHAPE = (51968, 37632)
train_ds = keras.preprocessing.image_dataset_from_directory(wsi_dataset_path, validation_split=0.2, color_mode='grayscale',
                                                                labels=None, shuffle=True, subset='training', image_size=WSI_SHAPE,
                                                                batch_size=32, seed=42)

val_ds = keras.preprocessing.image_dataset_from_directory(wsi_dataset_path, validation_split=0.2, color_mode='grayscale',
                                                              labels=None, shuffle=True, subset='validation', image_size=WSI_SHAPE,
                                                              batch_size=32, seed=42)

for image in train_ds.take(1):
    print('Image input shape:', image.input_shape)
```

This gives an error:
```
2022-01-21 20:30:25.456609: E tensorflow/core/lib/jpeg/jpeg_mem.cc:183] Image too large: 1955659776
```

**Describe the expected behavior**

Expected behavior is for tensorflow to load in the large images into the Dataset.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): **No**
- Briefly describe your candidate solution(if contributing): **N/A**

**Other info / logs**

I looked into the location of this error: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/jpeg/jpeg_mem.cc#L182-L186
```cpp
if (total_size >= (1LL << 29)) {
  LOG(ERROR) << ""Image too large: "" << total_size;
  jpeg_destroy_decompress(&cinfo);
  return nullptr;
}
```

The size of my images (1955659776) is 3.6 times larger than the maximum size (536870912). I'm curious as to why it is this specifically this number as the maximum size since I have access to 256 GB of memory. For large images like this would I have to create the dataset manually instead of `image_dataset_from_directory`?"
53862,`tf.math.accumulate_n` cannot handle a single tensor as input,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
inputs = tf.constant([0, 0, 0, 0, 0], dtype=tf.int32)  
add_n_res = tf.math.add_n(inputs)
print(""add_n_res: "", add_n_res) # add_n_res:  tf.Tensor(0, shape=(), dtype=int32)
accumulate_n_res = tf.math.accumulate_n(inputs,) # ERROR:The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
print(""accumulate_n_res: "", accumulate_n_res) # Won't get here
```

**Expected output**
According to the document of [`tf.math.accumulate_n`](https://www.tensorflow.org/api_docs/python/tf/math/accumulate_n?hl=en), `accumulate_n` performs the same operation as `tf.math.add_n`. However, `accumulate_n` throws an error when `inputs` is one single tensor, while `tf.math.add_n` supports such inputs."
53858,normalization should be done using moving averages?,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/structured_data/time_series#normalize_the_data

## Description of issue (what needs changing):

Hello this is a small feature request. In the Time series tutorial at the normalization section in the 3rd paragraph there is a remark saying:
""and that this normalization should be done using moving averages.""

I am very curious on how a moving average would be used here. 
Should  a ""simple moving average"" for the whole dataset be computed and then on that new ""dataset-MA"" just use the normalization technique that is described in the tutorial?

The feature I am requesting is something like a remark on how this would be done or maybe a pointer to different tutorial if one exists or some further references/research users could look into."
53856,Expose half_pixel_center or anti-aliasing parameter in the keras resizing layer.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 2.6
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
The feature that I am proposing is basically a fix for the current resizing keras layer which sets the `half_pixel_center` to true by default which causes problem on nnapi (android) based platforms and also on snapdragon based platforms. There are 2 possible solution to this problem - 

1. You expose the parameters `antialias` or `half_pixel_center` via the keras Resizing layer and let the user explicitly set either of the 2 properties. Better would be to expose `antialias` which can be defaulted to `None` or `True` since resizing is used mostly in later layers and not for downsampling it should be fine.
2.  Add an automated check within the resize base function to check if the half_pixel_center is actually needed. From my understanding when you are resizing just check if the (inputsize -1)/(outputsize-1) when downsampling or vice versa when upsampling, is fractional or int. If fractional set half_pixel_center to true else set it to false. 


**Will this change the current api? How?**
Yes this will change the current api by either exposing certain optional parameters or by adding an automated check.

**Who will benefit with this feature?**
Everyone running their NN on somekind of DSP.

**Any Other info.**
"
53852,no interface provided to set 'max_num_snapshots' when profile memory,"When using the tensorflow profiler for memory footprint analysis, the profiler keeps up to 1000 snapshots. This default value limits us to more detailed memory analysis. I would like to know why this default value is set to 1000.

The relevant code is as follows：
https://github.com/tensorflow/tensorflow/blob/f0df570aa30f962f58aedf6d111e017c7345431b/tensorflow/core/profiler/convert/xplane_to_memory_profile.h#L30
https://github.com/tensorflow/tensorflow/blob/f0df570aa30f962f58aedf6d111e017c7345431b/tensorflow/core/profiler/convert/xplane_to_memory_profile.cc#L550

TF version：TF 2.7
How to reproduce: Strictly speaking, this is not a bug.

Looking forward to reply."
53851,[Metric] get shape of a tensor for custom metric,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): conda-forge
- TensorFlow version (use command below): 2.6.2
- Python version: 3.7
- CUDA/cuDNN version: 10.2 / 7.6.5.32
- GPU model and memory: GTX 1080Ti

**Describe the current behavior**

I am implementing a custom metric for `tf.keras` which need to extract the batch dimension of input tensor `x.shape[0]` during `model.fit(...)` but it raises:

```
(None, None)
Traceback (most recent call last):
  File ""train.py"", line 81, in <module>
    main()
  File ""train.py"", line 71, in main
    use_multiprocessing=True)
  File ""/opt/conda/lib/python3.7/site-packages/keras/engine/training.py"", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 760, in _initialize
    *args, **kwds))
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3066, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3463, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3308, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 1007, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 668, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 994, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /opt/conda/lib/python3.7/site-packages/keras/engine/training.py:853 train_function  *
        return step_function(self, iterator)
    /AgeGenderMask/metrics/age_mae.py:36 update_state  *
        self.count.assign_add(y_true.shape[0])
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:857 assign_add  **
        ops.convert_to_tensor(delta, dtype=self.dtype),
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py:163 wrapped
        return func(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor
        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:346 _constant_tensor_conversion_function
        return constant(v, dtype=dtype, name=name)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:272 constant
        allow_broadcast=True)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py:290 _constant_impl
        allow_broadcast=allow_broadcast))
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:445 make_tensor_proto
        raise ValueError(""None values not supported."")
```

**Describe the expected behavior**

As mentioned above.

**Standalone code to reproduce the issue**

```python
class CustomMetric(keras.metrics.Metric):
    def __init__(self, name: Optional[str] = None) -> None:
        super().__init__(name=name)
        self.total = self.add_weight(""total"", initializer=""zero"")
        self.count = self.add_weight(""count"", initializer=""zero"")

    def update_state(self, y_true, y_pred, sample_weight=None) -> None:
        self.total.assign_add(tf.reduce_sum(y_pred))
        print(y_true.shape[0]) # Cannot extract tensor's shape
        self.count.assign_add(y_true.shape[0])

    def result(self) -> tf.Tensor:
        return tf.math.divide_no_nan(self.total, self.count)

    def reset_state(self) -> None:
        self.total.assign(0)
        self.count.assign(0)
```
"
53850,Update bleach to 4.1.0,
53849,Is there a direct way to add Flex delegate to options in Android Tensorflow Lite 2.5 on C API ?,"I'm trying to use tflite 2.5 and  a tflite model converted from tensorflow model and I need to use flex delegate  in android on a native level C api.

There is a method in c header lite\c\c_api.h

TFL_CAPI_EXPORT extern void TfLiteInterpreterOptionsAddDelegate(
    TfLiteInterpreterOptions* options, TfLiteDelegate* delegate);

But: there is no api for creating TfLiteDelegate object.

Is it possible to use this TfLiteInterpreterOptionsAddDelegate method or  is this method just a prank?

I need an api for creating flex TfLiteDelegate  object and If such a method exitsts, where is it?
"
53846,tf.data.experimental.sample_from_datasets non-deterministic in multi-gpu. ,"See https://github.com/NVIDIA/framework-determinism/issues/39

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Does not apply
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2
- GPU model and memory:"
53845,XLA docs for `Map` are unclear and incomplete,"## URL(s) with the issue:

https://www.tensorflow.org/xla/operation_semantics#map

## Description of issue (what needs changing):

### Clear description

No. The section
```
The mapped function is an arbitrary computation with the restriction that it has N inputs of scalar type T and a single output with type S. The output has the same dimensions as the operands except that the element type T is replaced with S.
```
is not clear. Does it mean the output of the mapped function or the output of `Map`? I think it means the latter but it's not worded like that.

### Correct links

Yes

### Parameters defined

No. It's not clear what `dimensions` is for, and the argument `static_operands` is missing, though it it possibly alluded to in the definition of `computation` (see mention of `M` indices)

### Returns defined

Yes

### Raises listed and defined

As much as any other function in these docs.

### Usage example

No. An example would be amazing, should the devs feel like adding one.

### Request visuals, if applicable

n/a

### Submit a pull request?

No."
53844,Loading optimizer weights on on TPUs,"**System information**
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GCP
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.5
Python version: 3
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Running many training sessions on TPUs using GCP (typically v2-32). Instead of using module.fit in keras, we use optimizer.apply_gradients.  Basically, we would like to be able to stop and restart training sessions by loading model weights and optimizer weights. The optimizer weights are really important because we would like to tune.

In keras, there are options to save optimizer weights using np.save(PATH_TO_OPT, optimizer.get_weights())

On GPUs we can re-load them using:
            def _loads_opt_weights():
               opt_weights = np.load(PATH_TO_OPT, allow_pickle=True)
               grad_vars = model.trainable_weights
               zero_grads = [tf.zeros_like(w) for w in grad_vars]
               optimizer.apply_gradients(zip(zero_grads, grad_vars))
               optimizer.set_weights(opt_weights)
            strategy.run(_loads_opt_weights)

For each GPU, but on the TPUs this doesn't work. We get an error of:

NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into strategy.run is a tf.function or strategy.run is called inside a tf.function if eager behavior is enabled

Turning eager mode off using tf.compat.v1.disable_eager_execution(): 
Results in InaccessibleTensorError: Operation 'LogicalAnd_30' has been marked as not fetchable. Typically this happens when it is defined in another function or code block. Use return values,explicit Python locals or TensorFlow collections to access it. When calling optimizer.set_weights(..)

How do we get this to load on distributed TPUs?

Do you want to contribute a PR? (yes/no): no
Briefly describe your candidate solution(if contributing): N/A

Standalone code to reproduce the issue: See above
"
53842,Shape issues in tf.keras.metrics.SparseTopKCategoricalAccuracy with multiple dimensions (previously #36985),"Might not be a bug could be my application. But if that is the case I feel the documentation should be edited as from reading those functionality seems to be similar.

When I run this:
```
target = tf.constant([[1,1,2,0,0,0],[1,1,2,2,0,0]])
predict = tf.constant([[[0,1,0],
                       [0,1,0],
                       [0,0,1],
                       [0,1,0],
                       [1,0,0],
                       [1,0,0]],
                       [[0,1,0],
                        [0,1,0],
                        [0,0,1],
                        [0,0,1],
                        [0,1,0],
                        [0,1,1000]]], dtype=tf.float32)


print(tf.shape(target))
print(tf.shape(predict))

mask = tf.math.logical_not(tf.math.equal(target, 0))

mask = tf.cast(mask, dtype=tf.int64)


loss = tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=True)
loss(target, predict, sample_weight=mask)
```
```
tf.Tensor([2 6], shape=(2,), dtype=int32)
tf.Tensor([2 6 3], shape=(3,), dtype=int32)
<tf.Tensor: shape=(), dtype=float32, numpy=0.3216761>
```
Works as expected. But when I try it for TopKAccuracy I get this:
```
m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)
m.update_state(target, predict, sample_weight=mask)
m.result.numpy()
InvalidArgumentError: Can not squeeze dim[1], expected a dimension of 1, got 6 [Op:Squeeze]
```

In the documentation, both of them have the same definitions for y_true, y_pred and sample_weight. So not sure why the applications seem different here.

This may be my misuse, but not clear regardless. Any help would be appreciated."
53837,Post Training Quantization doesn't work for tf.divide and tf.negative in TF 2.7,"### 1. System information

- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installation: from pip
- TensorFlow library: 2.7.0 (and 2.6.2 for reference)

### 2. Code

```
import tensorflow as tf
import numpy as np
from pathlib import Path

run_negative = True
if run_negative:
    model_file = '/tmp/debug_layers/neg_model_' + tf.__version__.replace('.', '_')
else:
    model_file = '/tmp/debug_layers/divide_2inputs_model_' + tf.__version__.replace('.', '_')

_in = tf.keras.layers.Input((4, 4))
out = tf.negative(_in) if run_negative else tf.divide(tf.keras.layers.ReLU()(_in), _in)
m = tf.keras.models.Model(inputs=_in, outputs=out)

_input = np.random.normal(size=(1, 4, 4))
output = m(_input)


def representative_dataset():
    for _ in range(100):
        d = np.random.normal(size=_input.shape).astype(np.float32)
        yield [d]


converter = tf.lite.TFLiteConverter.from_keras_model(m)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
tflite_quant_model = converter.convert()

interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)
interpreter.allocate_tensors()
for t in interpreter.get_tensor_details():
    print(t['name'])

# Path(model_file + '.tflite').write_bytes(tflite_quant_model)
# m.save(model_file + '.h5')

```

### 3. Failure after conversion
Conversion passes succesfuly.
In TF 2.7 the converted network isn't quantized, but in TF 2.6.2 the network is quantized correctly.

output for TF 2.6.2:
```
input_1
tfl.quantize
tfl.dequantize
Identity1
tfl.quantize1
Identity
```

output for TF 2.7.0:
```
serving_default_input_1:0
PartitionedCall:0
```

The output of TF 2.7 is missing the Quantize-Dequantize nodes"
53836,ValueError: Failed to find data adapter that can handle input,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):2.5.0
- Python version:3.7.11
- CUDA/cuDNN version:11.2
- GPU model and memory: rtx3060 laptop 6g

problem:
```
Traceback (most recent call last):
  File ""E:/python/cloud_remove_keras/train_models/train_resnet.py"", line 42, in <module>
    callbacks=[cp_callback])  # Pass callback to training
  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\training.py"", line 1122, in fit
    steps_per_execution=self._steps_per_execution)
  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\data_adapter.py"", line 1348, in get_data_handler
    return DataHandler(*args, **kwargs)
  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\data_adapter.py"", line 1136, in __init__
    adapter_cls = select_data_adapter(x, y)
  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\data_adapter.py"", line 978, in select_data_adapter
    _type_name(x), _type_name(y)))
ValueError: Failed to find data adapter that can handle input: <class 'data_generator.DataGenerator'>, <class 'NoneType'>
```
networks:
```
import keras.backend as K
from keras.layers import Conv2D, Activation, Lambda, Add
from keras.models import Model, Input
from tensorflow.keras.utils import plot_model

K.set_image_data_format('channels_first')


def resBlock(input_l, feature_size, kernel_size, scale=0.1):
    """"""Definition of Residual Block to be repeated in body of network.""""""
    tmp = Conv2D(feature_size, kernel_size, kernel_initializer='he_uniform', padding='same')(input_l)
    tmp = Activation('relu')(tmp)
    tmp = Conv2D(feature_size, kernel_size, kernel_initializer='he_uniform', padding='same')(tmp)

    tmp = Lambda(lambda x: x * scale)(tmp)

    return Add()([input_l, tmp])


def res_net(input_shape=(3, 512, 512),
                  num_layers=32,
                  feature_size=256):
    """"""Definition of network structure. """"""

    # define dimensions
    input = Input(shape=input_shape)

    x = input

    # Treat the concatenation
    x = Conv2D(feature_size, (3, 3), kernel_initializer='he_uniform', padding='same')(x)
    x = Activation('relu')(x)

    # main body of network as succession of resblocks
    for i in range(num_layers):
        x = resBlock(x, feature_size, kernel_size=[3, 3])

    # One more convolution
    x = Conv2D(input_shape[0], (3, 3), kernel_initializer='he_uniform', padding='same')(x)

    # Add first layer (long skip connection)
    x = Add()([x, input])

    model = Model(inputs=input, outputs=x)

    return model
```
data generator:
```
import numpy as np
import os
from tensorflow.keras.utils import Sequence
from tensorflow.keras.preprocessing.image import load_img, img_to_array


class DataGenerator(Sequence):
    def __init__(self, infile_path, batch_size, ratio, train_available):
        super(DataGenerator, self).__init__()

        self.infile_path = infile_path
        self.batch_size = batch_size
        self.ratio = ratio
        self.train_available = train_available

        self.cloud_path = os.path.join(infile_path, 'cloudy_image')
        self.ground_path = os.path.join(infile_path, 'ground_truth')
        # 所有文件列表，在 cloudy 和 ground truth 文件夹中文件名称是一样的
        self.file_list = os.listdir(self.ground_path)
        # 打乱顺序
        np.random.shuffle(self.file_list)
        # 分割训练数据和测试数据
        n_train = int(ratio * len(self.file_list))

        self.train_file_list = self.file_list[:n_train]
        self.test_file_list = self.file_list[n_train:]

        # 将训练和测试文件保存到文件中
        # np.savetxt(os.path.join(infile_path, 'train_file_list.txt'), np.array(self.train_file_list), fmt='%s')
        # np.savetxt(os.path.join(infile_path, 'test_file_list.txt'), np.array(self.test_file_list), fmt='%s')

    def __len__(self):
        if self.train_available:
            return int(np.floor(len(self.train_file_list) / self.batch_size))
        else:
            return int(np.floor(len(self.test_file_list) / self.batch_size))

    def __getitem__(self, index):
        if self.train_available:
            file_list = self.train_file_list[index * self.batch_size:(index + 1) * self.batch_size]
            batch_x = [np.transpose(img_to_array(load_img(os.path.join(self.cloud_path, file_name)))/255.0, [2, 0, 1]) for file_name in file_list]
            batch_y = [np.transpose(img_to_array(load_img(os.path.join(self.ground_path, file_name)))/255.0, [2, 0, 1]) for file_name in file_list]

            return np.array(batch_x), np.array(batch_y)

        else:
            file_list = self.test_file_list[index * self.batch_size:(index + 1) * self.batch_size]
            batch_x = [np.transpose(img_to_array(load_img(os.path.join(self.cloud_path, file_name)))/255.0, [2, 0, 1]) for file_name in file_list]
            batch_y = [np.transpose(img_to_array(load_img(os.path.join(self.ground_path, file_name))) / 255.0, [2, 0, 1]) for file_name in file_list]
            return np.array(batch_x), np.array(batch_y)
```
train code:
```
import tensorflow as tf
import os

from data_generator import DataGenerator
from networks.resnet import res_net


params = {
    'infile_path': r""E:\python\cloud_removal\data\RICE_DATASET\RICE2"",
    'batch_size': 32,
    'ratio': 0.8,
    'train_available': True
}
train_Gen = DataGenerator(**params)

params = {
    'infile_path': r""E:\python\cloud_removal\data\RICE_DATASET\RICE2"",
    'batch_size': 32,
    'ratio': 0.8,
    'train_available': False
}
val_Gen = DataGenerator(**params)

model = res_net()

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

checkpoint_path = ""train_models/resnet/cp.ckpt""
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

# Train the model with the new callback
model.fit(x=train_Gen,
          validation_data=val_Gen,
          epochs=10,
          callbacks=[cp_callback])  # Pass callback to training

model.save('train_models/resnet')
```"
53835,Failed to find data adapter that can handle input: ,"Hi, I am getting the error while writing the code:
```
Traceback (most recent call last):
  File ""E:/python/cloud_remove_keras/train_models/train_resnet.py"", line 42, in <module>
    callbacks=[cp_callback])  # Pass callback to training
  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\training.py"", line 1122, in fit
    steps_per_execution=self._steps_per_execution)
  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\data_adapter.py"", line 1348, in get_data_handler
    return DataHandler(*args, **kwargs)
  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\data_adapter.py"", line 1136, in __init__
    adapter_cls = select_data_adapter(x, y)
  File ""D:\anaconda3\envs\PyGIS\lib\site-packages\keras\engine\data_adapter.py"", line 978, in select_data_adapter
    _type_name(x), _type_name(y)))
ValueError: Failed to find data adapter that can handle input: <class 'data_generator.DataGenerator'>, <class 'NoneType'>
```
Below is the data generator I have defined：
```
import numpy as np
import os
from tensorflow.keras.utils import Sequence
from tensorflow.keras.preprocessing.image import load_img, img_to_array


class DataGenerator(Sequence):
    def __init__(self, infile_path, batch_size, ratio, train_available):
        super(DataGenerator, self).__init__()

        self.infile_path = infile_path
        self.batch_size = batch_size
        self.ratio = ratio
        self.train_available = train_available

        self.cloud_path = os.path.join(infile_path, 'cloudy_image')
        self.ground_path = os.path.join(infile_path, 'ground_truth')
        # 所有文件列表，在 cloudy 和 ground truth 文件夹中文件名称是一样的
        self.file_list = os.listdir(self.ground_path)
        # 打乱顺序
        np.random.shuffle(self.file_list)
        # 分割训练数据和测试数据
        n_train = int(ratio * len(self.file_list))

        self.train_file_list = self.file_list[:n_train]
        self.test_file_list = self.file_list[n_train:]

        # 将训练和测试文件保存到文件中
        # np.savetxt(os.path.join(infile_path, 'train_file_list.txt'), np.array(self.train_file_list), fmt='%s')
        # np.savetxt(os.path.join(infile_path, 'test_file_list.txt'), np.array(self.test_file_list), fmt='%s')

    def __len__(self):
        if self.train_available:
            return int(np.floor(len(self.train_file_list) / self.batch_size))
        else:
            return int(np.floor(len(self.test_file_list) / self.batch_size))

    def __getitem__(self, index):
        if self.train_available:
            file_list = self.train_file_list[index * self.batch_size:(index + 1) * self.batch_size]
            batch_x = [np.transpose(img_to_array(load_img(os.path.join(self.cloud_path, file_name)))/255.0, [2, 0, 1]) for file_name in file_list]
            batch_y = [np.transpose(img_to_array(load_img(os.path.join(self.ground_path, file_name)))/255.0, [2, 0, 1]) for file_name in file_list]

            return np.array(batch_x), np.array(batch_y)

        else:
            file_list = self.test_file_list[index * self.batch_size:(index + 1) * self.batch_size]
            batch_x = [np.transpose(img_to_array(load_img(os.path.join(self.cloud_path, file_name)))/255.0, [2, 0, 1]) for file_name in file_list]
            batch_y = [np.transpose(img_to_array(load_img(os.path.join(self.ground_path, file_name))) / 255.0, [2, 0, 1]) for file_name in file_list]
            return np.array(batch_x), np.array(batch_y)
```
The code when training the model is as follows：
```
import tensorflow as tf
import os

from data_generator import DataGenerator
from networks.resnet import res_net


params = {
    'infile_path': r""E:\python\cloud_removal\data\RICE_DATASET\RICE2"",
    'batch_size': 32,
    'ratio': 0.8,
    'train_available': True
}
train_Gen = DataGenerator(**params)

params = {
    'infile_path': r""E:\python\cloud_removal\data\RICE_DATASET\RICE2"",
    'batch_size': 32,
    'ratio': 0.8,
    'train_available': False
}
val_Gen = DataGenerator(**params)

model = res_net()

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

checkpoint_path = ""train_models/resnet/cp.ckpt""
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

# Train the model with the new callback
model.fit(x=train_Gen,
          validation_data=val_Gen,
          epochs=10,
          callbacks=[cp_callback])  # Pass callback to training

model.save('train_models/resnet')
```
The answer I found on stackoverflow didn't work for me, the data type returned by the data generator was already numpy.
the version of my tensorflow is 2.5.0."
53834,wierd behavior while running the inference on 2 different deep learning models(pb graphs) at the same,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.0
- Python version:3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0.130/ 7.6.5
- GPU model and memory: Nvidia GeForce GTX 1080 Ti - 11GB


**Describe the current behavior**
I am trying to run 2 tensorflow models by loading the 2 different pb graphs. 

However I also see that when I a load the 2 models together, the 2nd session is getting the values of the first session. 
I am suspecting that as the both models have input_1 layer as their first layer in their graph, this could be somehow creating a 
problem when they are loaded at the same time. 

I have included 2 experiments to prove the same. 

Experiment 1 -> When I am running the `run_SS_model.py` file, below is the output for my first and last layer
![image](https://user-images.githubusercontent.com/26414662/150339116-86d34a10-05c5-409d-9488-92b869f4850c.png)

Experiment 2 -> When I am running the `run_two_models.py` file, note that the dimensions of the first layer is overwritten
to the dimensions of the 2nd pb graph.
![image](https://user-images.githubusercontent.com/26414662/150339999-ef45a12e-a057-410d-8bd3-61ed0bcb6f4e.png)


**Describe the expected behavior**
2 Models should be executed without any problem.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Gist -> https://github.com/sachinkmohan/run_obj_det_sem_seg
Please also find the conda environment YAML file in the same gist.


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


```
2022-01-20 13:35:05.120904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-01-20 13:35:05.120917: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-01-20 13:35:05.120928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-01-20 13:35:05.120939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-01-20 13:35:05.120950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-01-20 13:35:05.120961: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-01-20 13:35:05.120972: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-01-20 13:35:05.121015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-20 13:35:05.121129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-20 13:35:05.121209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2022-01-20 13:35:05.121229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-01-20 13:35:05.121237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2022-01-20 13:35:05.121243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2022-01-20 13:35:05.121302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-20 13:35:05.121419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-20 13:35:05.121505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9927 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Tensor(""input_1:0"", shape=(?, 300, 480, 3), dtype=float32)
Tensor(""predictions/concat:0"", shape=(?, ?, 18), dtype=float32)
Tensor-2 Tensor(""input_1:0"", shape=(?, 300, 480, 3), dtype=float32)
Tensor(""sigmoid/Sigmoid:0"", shape=(?, ?, ?, 1), dtype=float32)
2022-01-20 13:35:06.200969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-01-20 13:35:06.690120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Error in model_SS
None
Traceback (most recent call last):
  File ""/home/mohan/git/run_obj_det_sem_seg/run_two_models.py"", line 17, in <module>
    cv2.imshow('prediction mask',b)
cv2.error: OpenCV(4.5.5) /io/opencv/modules/highgui/src/window.cpp:1000: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'


Process finished with exit code 1

```
"
53833,Using learning-rate decay schedule with Keras (transfer learning with RESNET) generates unsupported operand error 'ExponentialDecay',"
** System information **

- custom code
- Ubuntu 20.4
- TF installed from binary
- TF / Keras 2.7.0
- Python 3.8.12
- no GPU

** Current Behavior **

Training a Network to classify images, The network is based on RESNET152V2 + some new layers. We train last 19 layers of RESNET + the new added layers
(data is XRAY dataset from Kaggle)
Using a learning-rate decay schedule, the training crashes with an error 'unsupported operand error'

TypeError: unsupported operand type(s) for *: 'ExponentialDecay' and 'int'

** Standalone code to reproduce the issue **

[link to notebook](https://github.com/castorgit/AI-course-2021/blob/main/Group_Project/ResNet152V%20error.ipynb)


"
53831,`AutoGraph could not transform <function <lambda> at ...> ... Please report this to the TensorFlow team.`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): script to reproduce below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 5.10.91, NixOS, 21.11 (Porcupine)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): nixpkgs
- TensorFlow version (use command below): 2.7.0
- Python version: 3.9.9
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
Consider the following quick script:
```py
import tensorflow as tf
import tensorflow_datasets as tfds

print(tf.__version__)
print(tfds.__version__)
ds = tfds.load(""mnist"", split=""train"", as_supervised=True)
# Normalize 0-255 pixel values to 0.0-1.0
ds = ds.map(lambda image, label:
            (tf.cast(image, tf.float32) / 255.0, tf.one_hot(label, depth=10)))
```
based off of the example in the tensorflow-datasets docs [here](https://www.tensorflow.org/datasets/performances).

Running this code, I get the following:
```
[nix-shell:~/dev/research/lottery]$ AUTOGRAPH_VERBOSITY=10 python mnist_basic.py 
2.7.0
4.4.0+nightly
WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f63dfc5e310> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Constant constructor takes either 0 or 2 positional arguments
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f63dfc5e310> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Constant constructor takes either 0 or 2 positional arguments
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```

Here's my shell.nix to reproduce the exact same setup:
```nix
let
  # Last updated: 1/17/2022. Check for new commits at status.nixos.org.
  pkgs = import (fetchTarball (""https://github.com/NixOS/nixpkgs/archive/bc59ba15b64d0a0ee1d1764f18b4f3480d2c3e5a.tar.gz"")) { };
in
pkgs.mkShell {
  buildInputs = with pkgs; [
    python3
    python3Packages.flax
    python3Packages.ipython
    python3Packages.jax
    (python3Packages.jaxlib.override { cudaSupport = true; })
    python3Packages.matplotlib
    python3Packages.tensorflow
    python3Packages.tensorflow-datasets
    python3Packages.tqdm
    python3Packages.wandb
    yapf
  ];
}
```
**Describe the expected behavior**
The example code to run as expected without any warnings.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): n/a

**Standalone code to reproduce the issue**
See above.

**Other info / logs** 
My entire setup should be reproducible from the shell.nix file above. I'm running on an m5.large EC2 instance (x86_64-linux)."
53830,Is there plan to support rust?,
53829,`tf.sparse.sparse_dense_matmul` lack support for mixed bfloat16 and float32,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
a = tf.random.uniform([207, 768], dtype=tf.bfloat16)
b = tf.random.uniform([768, 3072], dtype=tf.float32)

c = tf.linalg.matmul(a,b)
print(c.shape) # (207, 3072)
a_sp = tf.sparse.from_dense(a)
b_sp = tf.sparse.from_dense(b)
c_sp = tf.sparse.sparse_dense_matmul(a_sp, b) # InvalidArgumentError
```

**Describe the current behavior**
`tf.sparse.sparse_dense_matmul` cannot accept a tensor of type `bfloat16` and a tensor of type `float32`. However, `tf.linalg.matmul` do support this. 
For the above code snippet, the error message is:
```
InvalidArgumentError: cannot compute SparseTensorDenseMatMul as input #3(zero-based) was expected to be a bfloat16 tensor but is a float tensor [Op:SparseTensorDenseMatMul]
```

**Describe the expected behavior**
According to the document for `tf.sparse.sparse_dense_matmul`, it is equivalent to `tf.linalg.matmul` (but for sparse tensors), so `tf.sparse.sparse_dense_matmul` should also support such mixed precision inputs.
"
53828,Descriptions of `tf.RaggedTensor`'s methods '__xx__' are unclear,"Documentation bug for: https://www.tensorflow.org/api_docs/python/tf/RaggedTensor?hl=en#__and__

## Description of issue:

### Usage examples are unmatched
`tf.RaggedTensor` has a number of class methods, for example, `__abs__`, `__add__`, ... The usage examples for these methods are unmatched. For instance, the example code for `__add__` does not contain any **ragged tensors**, instead the examples are all normal tensors (of type `tf.Tensor`). 

The example code is:
```
a = tf.constant([True])
b = tf.constant([False])
tf.math.logical_and(a, b)
```
That is not expected for a `tf.RaggedTensor.__add__()` method."
53827,ImportError: cannot import name 'StructureCoder' from 'tensorflow.python.saved_model.nested_structure_coder',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 Enterprise
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary via pip
- TensorFlow version: 2.7.0 (have tried with several versions, 2.6.0, 2.6.2, 2.8.0
- Python version: 3.8.8
- Installed using virtualenv? pip? conda?: yes, venv + pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Using jupyter notebook,  `import tensorflow as tf`  results in the following error: 

ImportError: cannot import name 'StructureCoder' from 'tensorflow.python.saved_model.nested_structure_coder' (C:\path\to\folder)

I've tried various iterations of uninstalling and re-installing tensorflow, restarting kernels, etc.   Same error. 



**Provide the exact sequence of commands / steps that you executed before running into the problem**
`import tensorflow as tf`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53822,tf.config.LogicalDeviceConfiguration memory_limit does not really work ?,"**System information**
- Custom code 
- Linux Ubuntu 20.04 (tested also 18.04)
- Computer (not mobile device) 
- TensorFlow installed from pip install:
- TensorFlow version 2.7 (tested also 2.4,2.5,2.6,2.8)
- Python version: 3.7 (tested also 3.8)
- Bazel version (if compiling from source): unbuntu 20.04 no self compil
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2.2/8.2.0.1 
- GPU model and memory: NVIDIA RTX3090 24GB (tested also RTX2060 anX GTX1660) Driver 460.x

Hello I successfully ran the profiler tool on ma classification model to profile the maximum memory usage. Because I want to use different CNN on a same GPU. But I'm really baffled by the results of the profiler. Let me explain  

I have a NVIDIA RTX3090 with 24GB memory so for my small CNN I set 512 memory limit in my code before all use with this code :  
` tf.config.set_logical_device_configuration(gpus[0],[tf.config.LogicalDeviceConfiguration(memory_limit=512)])`


It seems to work because of the tensorflow logs
`2022-01-19 16:24:13.615890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with **512 MB memory:**  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:2d:00.0, compute capability: 8.6`

Nvidia-smi shows that GPU use 419MiB are used
![smi first](https://user-images.githubusercontent.com/35316806/150172707-7ff0eef5-6feb-4915-9662-724b16356254.png)

Then I start a batch to make the inference on the classification model with batch size = 1
and tensorboard shows that the model use about 100MiB
![tensorboard](https://user-images.githubusercontent.com/35316806/150173173-823b1989-6174-4b99-a9aa-5fe272ba9b6e.png)

so theoretically I could have set a small memory limit (under 512) but .. here is the real use of the memory given by Nvidia-smi is 1869MiB !
![nvidiaamemoryused](https://user-images.githubusercontent.com/35316806/150173725-d1d8943d-9518-43b2-9607-ef0acfcbc3ec.png)

I tested the code in Tensorflow 2.4,2.5,2.6,2.7 and 2.8. With different CUDA CUDNN but it is the same. Memory limit seems to be applied at Tensorflow level but not at the real GPU memory ( Nvidia level ).
Did I miss something ? It would be very usefull to be able to manage the memory of a model !"
53819,Training a model : Error ,"# Train the Model
model.fit(x=train_batches,
          steps_per_epoch=len(train_batches),
          validation_data=valid_batches,
          validation_steps=len(valid_batches),
          epochs=10,
          verbose=2
)


........................................................................................................................................

ValueError                                Traceback (most recent call last)
Input In [12], in <module>
      1 # Train the Model
----> 2 model.fit(x=train_batches,
      3           steps_per_epoch=len(train_batches),
      4           validation_data=valid_batches,
      5           validation_steps=len(valid_batches),
      6           epochs=10,
      7           verbose=2
      8 )

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\utils\traceback_utils.py:67, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\func_graph.py:1129, in func_graph_from_py_func.<locals>.autograph_handler(*args, **kwargs)
   1127 except Exception as e:  # pylint:disable=broad-except
   1128   if hasattr(e, ""ag_error_metadata""):
-> 1129     raise e.ag_error_metadata.to_exception(e)
   1130   else:
   1131     raise

ValueError: in user code:

    File ""C:\Users\leena juliet\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\training.py"", line 878, in train_function  *
        return step_function(self, iterator)
    File ""C:\Users\leena juliet\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\training.py"", line 867, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\leena juliet\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\training.py"", line 860, in run_step  **
        outputs = model.train_step(data)
    File ""C:\Users\leena juliet\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\training.py"", line 809, in train_step
        loss = self.compiled_loss(
    File ""C:\Users\leena juliet\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\compile_utils.py"", line 201, in __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    File ""C:\Users\leena juliet\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\losses.py"", line 141, in __call__
        losses = call_fn(y_true, y_pred)
    File ""C:\Users\leena juliet\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\losses.py"", line 245, in call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    File ""C:\Users\leena juliet\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\losses.py"", line 1664, in categorical_crossentropy
        return backend.categorical_crossentropy(
    File ""C:\Users\leena juliet\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\backend.py"", line 4994, in categorical_crossentropy
        target.shape.assert_is_compatible_with(output.shape)

    ValueError: Shapes (None, None) and (None, 7, 7, 2) are incompatible
"
53818,File system scheme 's3' not implemented,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below):  tensorflow==2.7.0
- TensorFlow-IO version: tensorflow-io==0.23.1
- Python version: Python 3.8.12
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
`tensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 's3' not implemented`

**Describe the expected behavior**
`Should be able to connect the s3 filesystem.`

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): NA
- Briefly describe your candidate solution(if contributing):
 Rollback to `tensorflow-io==0.17` and `tensorflow==2.4.4` seems to resolve issue

**Standalone code to reproduce the issue**
```
from tensorflow.python.lib.io import file_io
print(file_io.stat('s3://bucketname/path/'))
```

```
from tensorflow.io import gfile
print(gfile.exists(`s3://bucketname/path/`))

```

**Other info / logs**  Similar older issue [here- 40302](https://github.com/tensorflow/tensorflow/issues/40302)"
53817,How to add monolithic flag when compile tensorflow-lite with cmake tools,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

OS Platform and Distribution: 
Linux centos 7.6
TensorFlow installed from source
TensorFlow version: 2.7.0
Python version: 3.9.6
Bazel version: 4.2.2
GCC/Compiler version (if compiling from source): 7.3.1

**Describe the problem**
![image](https://user-images.githubusercontent.com/5045116/150095111-da33a434-1e1b-4db8-884b-6f124862f6e0.png)
The article teaches how to add monolithic flag with bazel, but I know how to add options with cmake tools. Thank you!
"
53816,`tf.norm` not accurate in Tensorflow 2.7.0,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): pypl
- TensorFlow version (use command below): v2.7.0
- Python version: 3.8.10
- Bazel version (if compiling from source): not required
- GCC/Compiler version (if compiling from source): not required
- CUDA/cuDNN version: 11.0 (from docker image of TF 2.7.0)
- GPU model and memory: 2080ti

**Describe the current behavior**

We are using `tf.norm` to make a vector be unit vector.

This is quite simple and we use the following code in TF 2.2.0.

```python
vec = np.array([4.6463e-03, -4.8086258e+01], dtype=""float32"")
amount = tf.norm(vec, ord=2, axis=-1)
```

The amount is `<tf.Tensor: shape=(), dtype=float32, numpy=48.086258>` in TF 2.2.0.
Thus the normalizing is correct and we get an unit vector by `vec / amount`.
But in TF 2.7.0, the amount will be `<tf.Tensor: shape=(), dtype=float32, numpy=48.086254>`, which is incorrect.

We also found that this situation only happened when the tensor is computed on GPU.
In other word, if we set `os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""`, we could get correct value in TF 2.7.

**Describe the expected behavior**

The norm value should be `<tf.Tensor: shape=(), dtype=float32, numpy=48.086258>` in both CPU or GPU.
"
53815,tensorflow.datasets.load() throws an exception,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not a mobile device
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v2.8.0-rc0-28-g24a4b3b5e58 2.8.0-rc1
- Python version: 3.8
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 11.2 / 8.1.1.33
- GPU model and memory: NVIDIA GeForce 940MX 1629 MB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
After running the script

`import tensorflow as tf
import tensorflow_datasets as tfds

print(tf.__version__)
datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)`

I am getting the following output:

2.8.0-rc1
terminate called after throwing an instance of 'std::system_error'
  what():  Invalid argument

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)

**Describe the expected behavior**
No exception is expected

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

`import tensorflow as tf
import tensorflow_datasets as tfds

print(tf.__version__)
datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53814,bazel compile errors,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

OS Platform and Distribution: 
Linux centos 7.6
TensorFlow installed from source
TensorFlow version: 2.7.0
Python version: 3.9.6
Bazel version: 4.2.2
GCC/Compiler version (if compiling from source): 7.3.1

**Describe the problem**
Hello, I want to get libtensorflowlite.so with bazel, but I met a problem.  The llvm tar file is wrong, it can't be exacted.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build -c opt --cxxopt=-std=c++11 --config=mkl --config=numa --config=monolithic //tensorflow/lite:libtensorflowlite.so

**Any other info / logs**
Following is the error logs:
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz, https://github.com/llvm/llvm-project/archive/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz] to /home/cjh/.cache/bazel/_bazel_cjh/904c1e9488234a0ff2cc21c6287b3e45/external/llvm-raw/temp9533793750467514112/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz: Premature EOF
I try to extract the temp9533793750467514112/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz file, then a same problem occurs. The downloading file is wrong.
[cjh tensorflow-master]$ tar -xf llvm-project-2325f363010d3176e96579628cbb96b8fca003a1.tar.gz
gzip: stdin: unexpected end of file
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now"
53813,undefined symbol: _ZN4llvm7APFloat15getAllOnesValueERKNS_12fltSemanticsEj Target //tensorflow:libtensorflow_cc.so,"**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from source
- TensorFlow version: 2.7.0
- Python version: 3.9.6
- Bazel version: 3.7.2
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 11.4
- GPU model and memory: GTX3060

**Describe the problem**
Build tensorFlow 2.7.0 from source error:
ERROR: /root/Tensorflow/tensorflow-2.7.0/tensorflow/core/kernels/mlir_generated/BUILD:1146:23: Generating kernel '%{label}' failed (Exit 127): tf_to_kernel failed: error executing command bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel '--unroll_factors=4' '--tile_sizes=1024' '--max-supported-rank=5' '--arch=compute_86' ... (remaining 5 argument(s) skipped)
bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel: symbol lookup error: bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel: undefined symbol: _ZN4llvm7APFloat15getAllOnesValueERKNS_12fltSemanticsEj
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 17520.571s, Critical Path: 494.07s
INFO: 19740 processes: 6816 internal, 12924 local.
FAILED: Build did NOT complete successfully

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure
set CUDA support Yes
bazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so

**Any other info / logs**
the same error at TensorFlow 2.8
"
53811,AttributeError: 'Tensor' object has no attribute '_keras_mask' TF 2.7,"AttributeError: 'Tensor' object has no attribute '_keras_mask'
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield                   
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py"", line 346, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 699, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in user code:

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.7
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior** currently we use tf 2.2 and the code runs fine. but any version from 2.3 its failing

"
53810,Installing TF without Google OAuthLib,"**System information**
- OS Platform and Distribution: Docker deployment with RedHat 8 base image
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.7
- Python version: 3.8.8
- Installed using virtualenv? pip? conda?: virtualenv
- CUDA/cuDNN version: TBA (Will get info, but don't think it's relevant to issue)
- GPU model and memory: TBA (Will get info, but don't think it's relevant to issue)


**Problem**
Hi,

Where I work, we use Twistlock, a container security analysis tool, in our current workflow.

Twistlock reports this error with our current container:

`There is no support for PKCE implementation in the oauthlib client. Client-side PKCE for OAuth2 RFC 7636 is required for applications to have secure communication with the authorization server. OAuth 2.0 public clients utilizing the Authorization Code Grant are susceptible to the authorization code interception attack.`

We don't have the luxury to use another version of TF; we effectively need to use as close to the latest version as we can. That said, we need to be able to install TF but without this lib. Is this possible?

I should mention it seems this lib is needed to train models on Google Cloud (or so I read), but we don't need that functionality.

It would be helpful if this could be an optional feature instead of a required one. Currently, this is preventing us from deploying our container, as we can't deploy if this error yields.

I also apologize if this is improper use of this ticket (or ticketing system), I was asked to create one in the mean time, so the people managing Twistlock can make an exception if they can refer back to this ticket, but I also am seeking information about this issue we ran into.

Thanks for your time in advance.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Create docker container with Redhat 8 and Python 3.8.8, and startup container
2. create virtualenv
3. pip install tensorflow
4. Run Twistlock on container (I'm greatly simplifying this step because Twistlock is part of service that's not directly in our hands; we submit the container to a service that automatically runs Twistlock 3rd party for us.)

**Any other info / logs**

Twistlock:
`There is no support for PKCE implementation in the oauthlib client. Client-side PKCE for OAuth2 RFC 7636 is required for applications to have secure communication with the authorization server. OAuth 2.0 public clients utilizing the Authorization Code Grant are susceptible to the authorization code interception attack.`
"
53806,Deprecation message for tf.compat.v1.batch_gather suggests invalid migration,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow version (use command below): `v2.7.0-0-gc256c071bb2 2.7.0`
- Python version: `3.7.12`

**Describe the current behavior**

Using `tf.compat.v1.batch_gather` triggers the following warning:

```python
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.
Instructions for updating:
`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.
```

This warning was last edited in [this commit](https://github.com/tensorflow/tensorflow/commit/50abb98b7db07adc78df883ffa7b6c6dd4273ffc).

However, the suggested migration from `tf.compat.v1.batch_gather(...)` to `tf.gather(..., batch_dims=-1)` is invalid — e.g., see [this Colab notebook](https://colab.research.google.com/gist/dniku/7510249f61b9b00881af6c0bcfdac7a3/tf_migrate_batch_gather_to_gather.ipynb) for an example. From the [implementation](https://github.com/tensorflow/tensorflow/blob/c256c071bb26e1e13b4666d1b3e229e110bc914a/tensorflow/python/ops/array_ops.py#L5166) of `batch_gather` it seems that the correct migration is from

```python
tf.compat.v1.batch_gather(data, indices)
```
to
```python
tf.gather(data, indices, batch_dims=tf.rank(indices) - 1)
```

**Describe the expected behavior**

The warning should be amended."
53805,Using scipy fsolve or other solvers (e.g. GEKKO) in combination with backpropagation models,"I was wondering if there was any progress or existing methodologies to use existing solvers such as GEKKO to find/update certain values in your network.
E.g: When using PINN networks on a electrical circuit, one might use a variable resistance dependent on one of your network input parameters. When this dependency is given by an explicit equation it would be nice to, given the inputs, be able to use existing solvers as scipy's fsolve to get this value and use it further on in the network.

From my experience in order to be able to use solvers, tensors should be converted to numpy elements and used in combination with tf.numpy_function, this way I can use solvers as such, however it breaks the chain of gradients and TF can no longer find gradients for certain values and thus no longer train the network.

**System information**
- Tensorflow 2.3.0 (but using some TF1 functionalities from tf.compat.v1:
- No possibility to share existing code, but see below for small (hypothetical) case example. 



### start of code
from scipy.optimize import fsolve
Class PPINlayer(tf.keras.layers.Layer)
       def __init__(self):
               super(PPINlayer, self).__init__(**kwargs)

       def call(self, input, NN *args):
               NNoutput = NN(input)                # this is a NN already initialized somewhere else with trainable parameters
               output = self.f(NNoutput, input)
               return output                                # the output is then used later on to compare with labeled data as loss function for training
       def f(self, NNoutput, input)
             R0 = input[0]
             R1 = input[1]
             V0 = input[2]
             def R2_eq(R2, R0, R1):
                    return np.exp(R2/R1) * R1/R2 + R0**2
             R2 = fsolve(R2_eq, 0, args=(R0, R1))
             return (V0/R0 * R2)


Thanks in advance!
Cedric
"
53804,DeeplabV3+ Training on custom dataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- Platform and Distribution: Colab
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- CUDA/cuDNN version: 11.2
- GPU model and memory: Tesla P100, 16 GB memory

**Current situation**
 Worked on Semantics segmentation using TensorFlow's DeepLabv3+ with the Resnet versions backbone architecture. We've trained it for our custom dataset which includes satellite images of Golf Courses extracted from the Google Map API. Our Dataset includes 109 samples for training and we have trained it and got a decent amount of accuracy (~93%) but the results are not as expected.

**Current behavior**
Currently, we've referenced the test set which includes similar types of images but the results are not as good as expected. For the sake of curiosity, we have checked by inferencing on Train images also but that also not promising.

**Expected behavior**
Expected results with this accuracy should be descent as trained on the other custom dataset.
"
53802,Segmentation model C++ API's based inference output different from Python tensors,"**System information**
- Have I written custom code:No
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):  Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v2.3.0
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 10.1-cudnn7
- GPU model and memory:

**Describe the current behavior**
We've trained a Segmentation model using Keras with Tensorflow backend. Now, We are trying to infer by using the TF C++ interface.
I used the below code snippet to convert the .h5 model to a .pb file.

```python
keras_model_path=""keras_model.h5""
model = tf.keras.models.load_model(keras_model_path,  compile=False)
model.summary()
tf.saved_model.save(model, ""~/tf2_cpp/"")
```

Our python pipeline,
```python
model = tf.keras.models.load_model(keras_model_path,  compile=False)
img = cv2.imread(""test.jpg"")
resized_img = cv2.resize(img,(224, 224)).reshape([1, 224, 224,3]).astype(np.float32)
output = model.predict(resized_img) #7, 224, 244, 2
mask = tf.argmax(output[0], axis=3) #1, 244, 244
```
our C++ pipeline,
```c++
// Pre Processing
cv::Mat img = cv::imread(""test.jpg"")
cv::Size s(224,224);
cv::resize(img, img, s, cv::INTER_LINEAR);
tensorflow::Tensor input_tensor(tensorflow::DT_FLOAT, tensorflow::TensorShape({1, height, width, depth}));
float *p = input_tensor.flat<float>().data();
cv::Mat fakeMat(224, 224, CV_32FC3, p);
img.convertTo(fakeMat, CV_32FC3);

// Prediction
const string input_node = ""serving_default_input_4:0"";
// std::vector<std::pair<string, tensorflow::Tensor>> inputs_data  = {{input_node, input_tensor_mapped}};
std::vector<string> output_nodes = {{""StatefulPartitionedCall:3"",
    ""StatefulPartitionedCall:2"", 
    ""StatefulPartitionedCall:5"", 
    ""StatefulPartitionedCall:1"",
    ""StatefulPartitionedCall:4"",             
    ""StatefulPartitionedCall:0"",
    ""StatefulPartitionedCall:6""}};

std::vector<Tensor> predictions;

this->bundle.GetSession()->Run({{input_node, input_tensor}}, output_nodes, {}, &predictions);

//Post-processing
auto argOps = tensorflow::ops::ArgMax(transposeScope, predictions[1], 3);
std::vector<Tensor> temOutputs;
TF_CHECK_OK(sess.Run({argOps}, &temOutputs));
```

The tensor value is different when I compare the result between Python and C++. And, C++ argmax API returning zero for all the tensor as well.

**Describe the expected behavior**
The output expected to be same in both Python and C++. Maybe with some precision changes. 
"
53800,TFLite GPU delegate crash,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly
- Python version: N/A
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source): 8.4
- CUDA/cuDNN version: 11.1
- GPU model and memory: GTX 1650, 4GB

There is a segfault in the destructor of `tflite::gpu::cl::Buffer::Release()`, the stack trace is as follows:

```
==5463== Process terminating with default action of signal 11 (SIGSEGV)
==5463==  Access not within mapped region at address 0x8
==5463==    at 0xC0B4C20: ??? (in /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.460.106.00)
==5463==    by 0xBFA452B: ??? (in /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.460.106.00)
==5463==    by 0xBFA1AD7: ??? (in /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.460.106.00)
==5463==    by 0xBFA3BF1: ??? (in /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.460.106.00)
==5463==    by 0x1756C1: tflite::gpu::cl::Buffer::Release() (buffer.cc:82)
==5463==    by 0x1636DD: tflite::gpu::cl::Buffer::~Buffer() (buffer.h:46)
==5463==    by 0x171213: void std::_Destroy<tflite::gpu::cl::Buffer>(tflite::gpu::cl::Buffer*) (stl_construct.h:98)
==5463==    by 0x16F706: void std::_Destroy_aux<false>::__destroy<tflite::gpu::cl::Buffer*>(tflite::gpu::cl::Buffer*, tflite::gpu::cl::Buffer*) (stl_construct.h:108)
==5463==    by 0x16D497: void std::_Destroy<tflite::gpu::cl::Buffer*>(tflite::gpu::cl::Buffer*, tflite::gpu::cl::Buffer*) (stl_construct.h:137)
==5463==    by 0x16A046: void std::_Destroy<tflite::gpu::cl::Buffer*, tflite::gpu::cl::Buffer>(tflite::gpu::cl::Buffer*, tflite::gpu::cl::Buffer*, std::allocator<tflite::gpu::cl::Buffer>&) (stl_construct.h:206)
==5463==    by 0x16684C: std::vector<tflite::gpu::cl::Buffer, std::allocator<tflite::gpu::cl::Buffer> >::~vector() (stl_vector.h:567)
==5463==    by 0x164A8B: tflite::gpu::cl::InferenceContext::~InferenceContext() (inference_context.h:64)
==5463==  If you believe this happened as a result of a stack
==5463==  overflow in your program's main thread (unlikely but
==5463==  possible), you can try to increase the size of the
==5463==  main thread stack using the --main-stacksize= flag.
==5463==  The main thread stack size used in this run was 8388608.
==5463==  
```

This problem is only producible with NVidia's OpenCL. However, other implementations of OpenCL (e.g. Qualcomm) is known to let undefined behaviors pass silently. 
I believe the problem is related to OpenCL sub buffers. If I hack the tensorflow/lite/delegates/gpu/cl/inference_context.cc file and set `use_offset_assignment` to `false` right before: 
```
  if (use_offset_assignment) {
    shared_buffers_.resize(offset_assignment.offsets.size());
    RETURN_IF_ERROR(CreateReadWriteBuffer(offset_assignment.total_size, context,
                                          &shared_buffers_parent_));
    for (int i = 0; i < offset_assignment.offsets.size(); ++i) {
      RETURN_IF_ERROR(CreateReadWriteSubBuffer(
          shared_buffers_parent_, offset_assignment.offsets[i],
          buffer_usage_records[i].tensor_size, context, &shared_buffers_[i]));
    }
  } ...
```
thus preventing `CreateReadWriteSubBuffer` from being ever called, then this problem goes away.
Incidentally, I also noticed that the model I am trying to infer runs faster without using sub buffers (20ms with my hack disabling sub buffers, 30ms if I allow sub buffers, on my GTX 1650). "
53798,"Why is my CPU Performance of Float64 tf.matmul in TensorFlow2 significantly lower than the NumPy matmul, even in the graph mode?","

I'm comparing the single thread performance of the matrix-matrix products in **TensorFlow 2** and **NumPy**. I compare separately for single precision (float32) and double precision (float64). I find that the **NumPy** performance is almost equivalent to the Intel MKL C++ implementation (used as a benchmark for matrix multiplication) for both single and double precision (DGEMM and SGEMM). But in **TensorFlow**, only the single precision (float32) performance is equivalent to the MKL, and the double precision (float64) performance is significantly slower. **Why is Tensorflow slower when used with double precision data?**

**Sample Scripts:**

I consider the following instance to reproduce my observation. Consider the matrix multiplication:     

>  **C = AB** where A and B are of size 3000x3000

The TensorFlow2 and NumPy code are given below:

***Tensorflow2 code***
```python
import tensorflow as tf
import os
import time


#Check if MKL is enabled
import tensorflow.python.framework as tff
print(""MKL Enabled : "", tff.test_util.IsMklEnabled())


#Set threads
tf.config.threading.set_inter_op_parallelism_threads(1)
tf.config.threading.set_intra_op_parallelism_threads(1)

#Problem size
N = 3000
REPS = 20
DTYPE = tf.float64
#DTYPE = tf.float32


@tf.function
def gemm_implicit_noup(A, B):
    #C = A @ B
    start = tf.timestamp()
    with tf.control_dependencies([start]):
        C = tf.matmul(A,B)
    with tf.control_dependencies([C]):
        end = tf.timestamp()
    tf.print(end-start)
    return C

tf.config.run_functions_eagerly(False)

A = tf.random.normal([N, N], dtype=DTYPE)
B = tf.random.normal([N, N], dtype=DTYPE)


#Building Trace
C = gemm_implicit_noup(A,B)

for i in range(REPS):
   C = gemm_implicit_noup(A,B)
```

***Numpy code***

```python
import os
os.environ[""OMP_NUM_THREADS""] = ""1""
import numpy as np
import time

N = 3000
REPS = 20
DTYPE = np.float64
#DTYPE = np.float32

def gemm_implicit_noup(A, B):
    #C = A @ B
    C = np.matmul(A,B)
    return C



A = np.random.randn(N,N).astype(DTYPE)
B = np.random.randn(N,N).astype(DTYPE)

for i in range(REPS):
   start = time.perf_counter()
   C = gemm_implicit_noup(A,B)
   end = time.perf_counter()
   print(end-start)
```

**System and Installation settings:**

The performance was compared on Intel Xeon Skylake 2.1 GHz with CentOS 7 and also on MacBook Pro 2018 with BigSur. The performance was compared on both **Tensorflow 2.7** and **2.8**, which were built with Intel MKL. **Python 3.9.7** and **3.7.4** were checked. I compare the single thread performance so that the results can be reliably reproduced. I observe similar performance numbers in all the settings:

Single precision performance is as expected:

 - Intel MKL C++ SGEMM ~ **0.5s**
 - NumPy float32 ~ **0.5s**
 - TensorFlow float32 ~ **0.5s**

But Double precision performance:

 - Intel MKL C++ DGEMM ~ **0.9s**
 - NumPy float64 ~ **1s**
 - TensorFlow float64 > **2.5s** (Much Slower!!)

"
53795,"extracting features using BERT , it will do for only 96% why not upto 100% ","WARNING:tensorflow:From /home/dr/anaconda3/envs/nlp/lib/python2.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0117 15:32:11.388233 140388266755904 deprecation.py:323] From /home/dr/anaconda3/envs/nlp/lib/python2.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
INFO:tensorflow:Graph was finalized.
I0117 15:32:11.432744 140388266755904 monitored_session.py:240] Graph was finalized.
2022-01-17 15:32:11.432964: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2022-01-17 15:32:11.454749: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz
2022-01-17 15:32:11.455164: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556fa5428b30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-01-17 15:32:11.455180: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-01-17 15:32:11.456689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-01-17 15:32:11.550424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-17 15:32:11.550724: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556fa5539e70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-01-17 15:32:11.550740: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5
2022-01-17 15:32:11.550854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-17 15:32:11.551053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:01:00.0
2022-01-17 15:32:11.551208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-01-17 15:32:11.551998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-01-17 15:32:11.552706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-01-17 15:32:11.552874: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-01-17 15:32:11.553788: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-01-17 15:32:11.554510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-01-17 15:32:11.556731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-01-17 15:32:11.556814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-17 15:32:11.557061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-17 15:32:11.557242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2022-01-17 15:32:11.557267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-01-17 15:32:11.557561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-01-17 15:32:11.557573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2022-01-17 15:32:11.557578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2022-01-17 15:32:11.557643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-17 15:32:11.557860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-17 15:32:11.558063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3404 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)
INFO:tensorflow:Running local_init_op.
I0117 15:32:12.078099 140388266755904 session_manager.py:500] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I0117 15:32:12.114805 140388266755904 session_manager.py:502] Done running local_init_op.
2022-01-17 15:32:12.539938: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
 96%|███████████████████████████████████████████████████████████████████████████████████▌   | 77324/80553 [1:24:14<04:11, 12.83it/s]INFO:tensorflow:prediction_loop marked as finished
I0117 16:56:37.640084 140388266755904 error_handling.py:101] prediction_loop marked as finished
INFO:tensorflow:prediction_loop marked as finished
I0117 16:56:37.640234 140388266755904 error_handling.py:101] prediction_loop marked as finished
 96%|███████████████████████████████████████████████████████████████████████████████████▌   | 77324/80553 [1:24:27<03:31, 15.26it/s]

"
53794,Executing genrule //tensorflow/cc:string_ops_genrule failed (Exit 127): bash failed,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from source
- TensorFlow version: 1.14
- Python version: 2.7
- Installed using virtualenv? pip? conda?:
- Bazel version : 0.24.1
- GCC/Compiler version: 4.8
- CUDA/cuDNN version: 10.0
- I'm using an EC2 instance
   - Instance type: p2xlarge
   - AMI: Deep Learning Base AMI (Ubuntu 18.04) Version 47.0


**Describe the problem**
- I was following installation steps from here: https://github.com/cjweeks/tensorflow-cmake/blob/master/README.md

#### Step 1 => Clone tensorflow repo
```
sudo apt-get install autoconf automake libtool curl make g++ unzip  # Protobuf Dependencies
sudo apt-get install python-numpy swig python-dev python-wheel      # TensorFlow Dependencies
git clone https://github.com/tensorflow/tensorflow
git checkout v1.14.0
```
Enter the cloned repository, and append the following to the tensorflow/BUILD file:

#### Step 2 => Add build rule
```
cc_binary(
    name = ""libtensorflow_all.so"",
    linkshared = 1,
    linkopts = [""-Wl,--version-script=tensorflow/tf_version_script.lds""], # Remove this line if you are using MacOS
    deps = [
        ""//tensorflow/core:framework_internal"",
        ""//tensorflow/core:tensorflow"",
        ""//tensorflow/cc:cc_ops"",
        ""//tensorflow/cc:client_session"",
        ""//tensorflow/cc:scope"",
        ""//tensorflow/c:c_api"",
    ],
)
```
#### Step 3 =>
```
./configure      # Note that this requires user input
bazel build tensorflow:libtensorflow_all.so --verbose_failures
```

## Error
- I'm getting some error on the last command `bazel build tensorflow:libtensorflow_all.so --verbose_failures` that the build failed
- The error log is stated in the next section

**Any other info / logs**
```
INFO: Analysed target //tensorflow:libtensorflow_all.so (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/ubuntu/downl/tensorflow/tensorflow/cc/BUILD:492:1: Executing genrule //tensorflow/cc:string_ops_genrule failed (Exit 127): bash failed: error executing command 
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/364fbdb85a069ae1952a655335bcdb22/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib: \
    PATH=/home/ubuntu/.local/bin:/opt/amazon/openmpi/bin/:/opt/amazon/efa/bin/:/usr/local/cuda/bin:/opt/aws/neuron/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.7 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
    TF_NEED_TENSORRT=0 \
  /bin/bash bazel-out/k8-opt/genfiles/tensorflow/cc/string_ops_genrule.genrule_script.sh)
Execution platform: @bazel_tools//platforms:host_platform
bazel-out/host/bin/tensorflow/cc/ops/string_ops_gen_cc: symbol lookup error: bazel-out/host/bin/tensorflow/cc/ops/string_ops_gen_cc: undefined symbol: _ZN10tensorflow15shape_inference36BroadcastBinaryOpOutputShapeFnHelperEPNS0_16InferenceContextENS0_11ShapeHandleES3_PS3_
Target //tensorflow:libtensorflow_all.so failed to build
INFO: Elapsed time: 2.181s, Critical Path: 1.24s
INFO: 3 processes: 3 local.
FAILED: Build did NOT complete successfully
```
"
53793,[Feature Request] Support for convolutional layers for `tf.autodiff.ForwardAccumulator`,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently, the implementation of [`tf.autodiff.ForwardAccumulator`](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) only officially supports Dense layers.

**Will this change the current api? How?**
Yes. The new API will be able to calculate the JVP also for convolutional layers.

**Who will benefit from this feature?**
Anyone who wants to implement a neural network with convolutional layers and needs forward-mode autodiff.

**Any Other info.**
Trying to use the current API results in a shape mismatch:

```python
import tensorflow as tf

batch_size = 1
image_width = 4
image_height = 4
channels = 3

x = tf.random.uniform((batch_size, image_width, image_height, channels))
model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(filters=6, kernel_size=2)])

model.build(x.shape)

model.summary()

with tf.autodiff.ForwardAccumulator(model.trainable_weights[0], tf.constant([[[[1., 0., 0., 0., 0., 0.]]]])) as acc:
  out = model(x) # <----- ValueError

  print(acc.jvp(out))
```

Summary:

```
Model: ""sequential""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (1, 3, 3, 6)              78        
                                                                 
=================================================================
Total params: 78
Trainable params: 78
Non-trainable params: 0
_________________________________________________________________
```

Error:
```
ValueError: Exception encountered when calling layer ""conv2d"" (type Conv2D).

in user code:


    ValueError: Dimension 1 in both shapes must be equal, but are 4 and 3. Shapes are [1,4,4,6] and [1,3,3,6].
    	From merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](gradient_tape/gradient_tape/Conv2D, gradient_tape/gradient_tape/Conv2D_1)' with input shapes: [1,4,4,6], [1,3,3,6].


Call arguments received by layer ""conv2d"" (type Conv2D):
  • inputs=tf.Tensor(shape=(1, 4, 4, 3), dtype=float32)
```
"
53792,Use enable_op_determinism + Fixed seed + same hardware (partial DGX) still get different results in 2.8. ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.8rc0
- Python version: 3.8.12
- GPU model and memory:8*V100

After I use enable_op_determinism in TensorFlow 2.8, my model still gets random results (around 0.3 mIOU) after each run. Note that, I set PYTHONHASHEED to a fixed value before start python, and also set a fixed value as the random seed for TensorFlow and numpy etc.

I wonder why the result is still random in same hardware, since the UnimplementedError should be thrown if nondeterministic op is used."
53791,BUG Fail to save model when tf.summary is used inside keras model,"**System information**
- OS: Linux
- Python Version: 3.6.8
- Tensorflow version: Git version v2.6.0-rc2-32-g919f693420e  tf_version 2.6.0

**Describe the current behavior**
`tf.keras.Model.save` raises error when `tf.summary` is used inside of `call` function. A short script for reproducing the problem: 
```python
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np

class Mymodel(tf.keras.Model):
    
    def __init__(self):
        super().__init__()
        self.l = tf.keras.models.Sequential([
            layers.Dense(64) for _ in range(3)
        ] + [layers.Dense(1)])
    
    def call(self, inputs, training=None):
        tf.summary.scalar(name='avg_1', data=tf.reduce_sum(inputs))
        return self.l(inputs)

m = Mymodel()
m.compile(loss='mse', optimizer='adam')
m.fit(np.random.randn(1000, 20), np.random.randn(1000), epochs=5)

m.save('/tmp', save_format='tf')
```
this raises the following error:
```
AssertionError: Tried to export a function which references untracked resource Tensor(""72061:0"", shape=(), dtype=resource). TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
Trackable Python objects referring to this tensor (from gc.get_referrers, limited to two hops)
```
everything works fine if I comment out the `tf.summary.scalar` line.

"
53788,How to build MLIR tools on the latest master branch?,"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir#using-local-llvm-repo
Several files mentioned in it no longer exist. How to compile the new version?"
53787,Tensorflow Lite Android Example : Insecure protocols,"## URL(s) with the issue:

https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android and all other examples.

## Description of issue (what needs changing):

When I build any example, I get this error :
```
> Using insecure protocols with repositories, without explicit opt-in, is unsupported. Switch Maven repository 'ossrh-snapshot(http://oss.sonatype.org/content/repositories/snapshots)' to redirect to a secure protocol (like HTTPS) or allow insecure protocols. See https://docs.gradle.org/7.3.2/dsl/org.gradle.api.artifacts.repositories.UrlArtifactRepository.html#org.gradle.api.artifacts.repositories.UrlArtifactRepository:allowInsecureProtocol for more details. 
```

We need to set `allowInsecureProtocol` to `true` in the build.gradle. 
```
maven {
    name 'ossrh-snapshot'
    url 'http://oss.sonatype.org/content/repositories/snapshots'
    allowInsecureProtocol = true
    }
```"
53786,Android Tensorflow ObjectDetection lib_task_api default number of threads?,"Hello,
We are working on implementing Tensorflow object detection based on the example provided here:
https://github.com/tensorflow/examples/tree/eb925e460f761f5ed643d17f0c449e040ac2ac45/lite/examples/object_detection/android
It seems that there are 2 implementations for that one:
```
(1) lib_task_api that leverages the out-of-box API from the TensorFlow Lite Task Library;
(2) lib_interpreter that creates the custom inference pipleline using the TensorFlow Lite Interpreter Java API.
```
We are using `lib_task_api` and we don't see the way how we can set the number of threads with it? 
For the `lib_interpreter` - there is a variable that is being set up for that.
https://github.com/tensorflow/examples/blob/a228a3460f3fdd8edee9e8b061a08ffc92629907/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L138
On the other hand - for `lib_task_api` i see that this method is deprecated and it is not set up
https://github.com/tensorflow/examples/blob/eb925e460f761f5ed643d17f0c449e040ac2ac45/lite/examples/object_detection/android/lib_task_api/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L87

My question being - what is the default number of threads for `lib_task_api`? 
I am asking this one because i would like to tune it up for different set of devices.

Best Regards"
53785,Providing different results of FLOPS calculation when using tf.compat.v1.profiler.profile on Mac M1 chip and on NVIDIA 1650 ,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When use the following function for FLOPS calculation on mac M1 
`def get_flops(model: Union[Model, Sequential], batch_size: Optional[int] = None) -> int:
    """"""
    Calculate FLOPS for tf.keras.Model or tf.keras.Sequential .
    Ignore operations used in only training mode such as Initialization.
    Use tf.profiler of tensorflow v1 api.
    """"""
    if not isinstance(model, (Sequential, Model)):
        raise KeyError(
            ""model arguments must be tf.keras.Model or tf.keras.Sequential instanse""
        )

    if batch_size is None:
        batch_size = 1
        
    inputs = [tf.TensorSpec([batch_size] + inp.shape[1:], inp.dtype) for inp in model.inputs]
    
    # print(""@@@@@@@@@@@@@"",inputs)
    real_model = tf.function(model).get_concrete_function(inputs)
    # frozen_func, _ = convert_variables_to_constants_v2_as_graph(real_model)

    # Calculate FLOPS with tf.profiler
    run_meta = tf.compat.v1.RunMetadata()
    opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()
    flops = tf.compat.v1.profiler.profile(
        graph=real_model.graph, run_meta=run_meta, cmd=""scope"", options=opts
    )
    
    return flops.total_float_ops`

give wrong FLOPS as output: **2632160**

**Describe the expected behavior**

While the same code gives the correct calculation when tried on NVIDIA enabled machine. **VGGNET-16 FLOPS: 4587547104**
Expecting the same result to be calculated when done on Mac M1 machine.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53784,Conv 16x8 incorrect reference kernel usage,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):NA
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary):NA
- TensorFlow version (use command below):NA
- Python version:NA
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:NA
- GPU model and memory:NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Introduced by PR: https://github.com/tensorflow/tensorflow/commit/0d8705c82c64dfb39c49e346de1a66182e5eabd1

Code in 16x8 conv.cc file expects input/kernel/output offsets to be present if 16x8 conv has a 32 bit bias. But the reference kernel that is invoked does not use offsets as that is the default behavior for 16x8 conv.

**Describe the expected behavior**
It is one of the two
1. Do not expect offsets for 16x8 conv in case of 32 bit bias. In which case the correction is in conv.cc else
2. Add offset handling in the reference kernel.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):no
- Briefly describe your candidate solution(if contributing):NA

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
NA
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53783,`tf.scatter_nd` document refers to deprecated apis,"## URL(s) with the issue:

https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/scatter_nd

## Description of issue (what needs changing):

### Clear description
 Here is a paragraph describing the relationship between `tf.scatter_nd` and `tf.tensor_scatter_add`:

> This operation is similar to tf.tensor_scatter_add, except that the tensor is zero-initialized. Calling tf.scatter_nd(indices, values, shape) is identical to calling tf.tensor_scatter_add(tf.zeros(shape, values.dtype), indices, values).

However,  `tf.tensor_scatter_add` does not exist in TensorFlow 2.7, `tf.tensor_scatter_add` should be replaced with [`tf.tensor_scatter_nd_add`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add). 
"
53781,[TFLite] An multi-headers model output problem on GPU Delegates.,"Hi TFers,
I just found the problem that I was getting wrong data output from my model having multi-headers. For example, if the graph my model is like 0 -> 1 -> 2 -> 3 and the model inputs is [0] outputs is [2, 3], I will only get correct answer from 3 but 2 is not, because our GraphFloat32::outputs() only believe the value with no consumer, the 3 above, is an output value. However, our tflite::Interpreter still tell us the outputs is [2, 3] and it's right. So, I try to fix with this [PR](https://github.com/tensorflow/tensorflow/pull/53780), please take a check.

Following is my code snippet with [this model](https://github.com/tensorflow/tensorflow/files/7875588/small_test.zip) for test :
```
#define PRINT(format, ...)                              \
  do {                                                  \
    TFLITE_LOG(TFLITE_LOG_INFO, format, ##__VA_ARGS__); \
  } while (false);

bool SmallTest(Json::Value& config) {
  auto RunInterpreter =
      [](std::unique_ptr<tflite::Interpreter>* interpreter) -> bool {
    assert((*interpreter)->inputs().size() == 1);
    assert((*interpreter)->outputs().size() == 2);

    float* input_buffer = (*interpreter)->typed_input_tensor<float>(0);
    input_buffer[0] = 1.;
    auto input_index = (*interpreter)->inputs()[0];
    auto size_inputs = (*interpreter)->inputs().size();
    PRINT(""(SmallTest) Input: [%f] to input tensors: [%d] with size %d."",
          input_buffer[0], input_index, size_inputs);

    bool is_ok = (*interpreter)->Invoke() == kTfLiteOk;
    if (!is_ok) {
      return false;
    }

    float* first_output = (*interpreter)->typed_output_tensor<float>(0);
    float* second_output = (*interpreter)->typed_output_tensor<float>(1);
    PRINT(""(SmallTest) Output: [%f, %f] from output tensors: [%d, %d]"",
          first_output[0], second_output[0], (*interpreter)->outputs()[0],
          (*interpreter)->outputs()[1]);

    return true;
  };

  {
    auto model = tflite::FlatBufferModel::BuildFromFile(
        config[""model_path""].asCString());
    if (!model) return false;

    tflite::ops::builtin::BuiltinOpResolver op_resolver;
    std::unique_ptr<tflite::Interpreter> interpreter;
    tflite::InterpreterBuilder(*model, op_resolver)(&interpreter);
    interpreter->AllocateTensors();
    PRINT(""(SmallTest) CPU:"");
    RunInterpreter(&interpreter);
  }

  {
    auto model = tflite::FlatBufferModel::BuildFromFile(
        config[""model_path""].asCString());
    if (!model) return false;
    tflite::ops::builtin::BuiltinOpResolver op_resolver;
    std::unique_ptr<tflite::Interpreter> interpreter;
    tflite::InterpreterBuilder(*model, op_resolver)(&interpreter);

    // NEW: Prepare GPU delegate.
    auto options = TfLiteGpuDelegateOptionsV2Default();
    auto* delegate = TfLiteGpuDelegateV2Create(&options);
    if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk)
      assert(false);

    interpreter->AllocateTensors();
    PRINT(""(SmallTest) GPU:"");
    RunInterpreter(&interpreter);

    // NEW: Clean up.
    TfLiteGpuDelegateV2Delete(delegate);
  }

  return true;
}
#undef PRINT
```
"
53779,"keras.layers.SimpleRNN/LSTM output NaN when setting ""activation"" to ""exponential""","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7.12
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
`keras.layers.SimpleRNN` and `keras.layers.LSTM` output `NaN` when setting the `activation` parameter to `exponential` if the shape of input is larger than 3.

**Describe the expected behavior**
`keras.layers.SimpleRNN` and `keras.layer.LSTM` should not output `NaN` under this setting.

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
```
from tensorflow import keras
x = keras.layers.Input((6,3))
buggy_layer = keras.layers.SimpleRNN(50, activation=""exponential"")
y = buggy_layer(x)
model = keras.Model(x, y)
import numpy as np
input = np.random.rand(10,6,3)
res = model.predict(input)
model_path = ""simple_rnn_exponential.h5""
model.save(model_path)
print(res)
```
You may also access the code here:
https://colab.research.google.com/drive/1mVBIPF79kiIYggZUUkvs1IEB1GdxeoJC?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
To help you identify this issue, I have tried several ways to construct the buggy layer.

Trial 1:
Use `linear` as the activation while adding an additional exponential activation after the SimpleRNN. In this way, the `res` will not output `NaN`
```
from tensorflow import keras
x = keras.layers.Input((6,3))
buggy_layer = keras.layers.SimpleRNN(50, activation=""linear"")
y = buggy_layer(x)
z = keras.activations.exponential(y)
model = keras.Model(x, z)
import numpy as np
input = np.random.rand(10,6,3)
res = model.predict(input)
print(res)
```

Trial 2:
I tried converting the generated model which will output `NaN` to PyTorch using ONNX, I find that: after converting to PyTorch, the output will contain `Inf` and `0` but no `NaN`. Please see this link for more details about this trial:
https://colab.research.google.com/drive/1mVBIPF79kiIYggZUUkvs1IEB1GdxeoJC?usp=sharing
"
53778,The speed of training is reduced using a custom method in tensorflow.keras.layers,"I'm using `tensorflow.data` and `custom layers` to solve the bottleneck of data augmentation, but I found that using `tensorflow.data` alone is faster than mixing, I don't know what's going on in the `custom layers`, can someone please tell me?

Thanks in advance!

This is my data augmentation code, mainly to do standardization and resize.
```
def random_normalization(data, mean, std):
    mean = tf.multiply(mean, tf.random.uniform(shape=(), minval=0.5,maxval=0.9, dtype=tf.float64))
    std = tf.multiply(std, tf.random.uniform(shape=(), minval=0.5,maxval=0.9, dtype=tf.float64))
    return tf.divide((tf.subtract(data, mean)), std)

def random_resize(data):
    def resizing(index, data, choice, enable, new_data, number, overlap):        
        FrontEnd = tf.cond(tf.math.greater_equal(tf.subtract(index, overlap), tf.constant(0)),
                           lambda: tf.subtract(index, overlap),
                           lambda: index)
        
        BackEnd = tf.cond(tf.math.less(tf.add(tf.add(index, 10),overlap),tf.constant(2000)),
                          lambda: tf.add(tf.add(index, 10),overlap),
                          lambda: index)
        
        z1 = tf.gather(data, indices=[0], axis=1)
        z1 = tf.gather(z1, indices=tf.range(FrontEnd, BackEnd), axis=0)
        
        z2 = tf.gather(data, indices=[1], axis=1)
        z2 = tf.gather(z2, indices=tf.range(FrontEnd, BackEnd), axis=0)
        
        z3 = tf.gather(data, indices=[2], axis=1)
        z3 = tf.gather(z3, indices=tf.range(FrontEnd, BackEnd), axis=0)
        
        z4 = tf.gather(data, indices=[3], axis=1)
        z4 = tf.gather(z4, indices=tf.range(FrontEnd, BackEnd), axis=0)
        
        z5 = tf.gather(data, indices=[4], axis=1)
        z5 = tf.gather(z5, indices=tf.range(FrontEnd, BackEnd), axis=0)
        
        z6 = tf.gather(data, indices=[5], axis=1)
        z6 = tf.gather(z6, indices=tf.range(FrontEnd, BackEnd), axis=0)
        
        
        new_data = tf.tensor_scatter_nd_update(new_data, [[number, 0], [number, 1], [number, 2],
                                                          [number, 3], [number, 4], [number, 5]], 
                                               [tf.math.reduce_mean(z1), tf.math.reduce_mean(z2),
                                                tf.math.reduce_mean(z3), tf.math.reduce_mean(z4),
                                                tf.math.reduce_mean(z5), tf.math.reduce_mean(z6)])
        
        
        return tf.add(index, 10), data, choice, enable, new_data, tf.add(number, 1), overlap
    
    choice = tf.random.uniform(shape=(), minval=0,maxval=4,dtype=tf.int32)
    enable = tf.random.uniform(shape=(), minval=0,maxval=1,dtype=tf.float64)
    overlap = tf.random.uniform(shape=(), minval=5,maxval=21,dtype=tf.int32)
    
    new_data = tf.zeros((200,6), dtype=tf.float64)
    index = tf.constant(0)
    number = tf.constant(0)
    condition = lambda index, data, choice, enable, new_data, number, overlap: tf.less(index, 2000)
    r = tf.while_loop(condition, resizing, loop_vars=(index, data, choice, enable, new_data, number, overlap))
    return r[4]

def normal_resize(data):
    data = tf.reshape(data, (2000,6,1))
    data = tf.image.resize(data, size=[200,6])
    return tf.cast(tf.reshape(data, (200,6)),dtype=tf.float64)

def augmentation(data, labels):
    mean = tf.math.reduce_mean(data,axis=0)
    std = tf.math.reduce_std(data,axis=0)
    data = tf.cond(tf.random.uniform(shape=(), minval=0, maxval=1,dtype=tf.float64) < tf.constant(0.8,dtype=tf.float64), 
                   lambda: random_normalization(data, mean, std), 
                   lambda: tf.divide((tf.subtract(data, mean)), std))
    
    # 2000 resize to 200
    data = tf.cond(tf.random.uniform(shape=(), minval=0, maxval=1,dtype=tf.float64) < tf.constant(0.8,dtype=tf.float64), 
                   lambda: random_resize(data), 
                   lambda: normal_resize(data))

    return data, labels
```

Main code, including `tf.data` and model
```
if __name__ == '__main__':
    trainDS = tf.data.Dataset.from_tensor_slices((np.random.rand(3000,2000,6),
                                                  np.concatenate((np.zeros((1500)),np.ones((1500))))))
    trainDS = (
        trainDS
        .cache()
        .shuffle(1000, reshuffle_each_iteration=False)
        .map(augmentation, num_parallel_calls=tf.data.AUTOTUNE)
        .batch(128, drop_remainder=True)
        .prefetch(tf.data.AUTOTUNE))
    
    input = Input((200,6))
    x = LSTM(64, return_sequences=True)(input)
    output = Dense(1,activation='sigmoid')(x)
    model = Model(input, output)
    model.compile(optimizer='adam', loss='BinaryCrossentropy')
    model.fit(trainDS, epochs=3)
```
Then this is the code of my custom layer, although it is a bit cumbersome, it still achieves the result I want.
```
import tensorflow as tf
from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer
import numpy as np

class CustomLayer(Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def execute(self, data, batch_size, new_data, _type):
        def _fun(index, data, _type, new_data):
            resized = tf.cond(_type,
                              lambda:augmentation(tf.reshape(tf.gather(data,[index]), (2000,6))),
                              lambda:normal_resize(tf.reshape(tf.gather(data,[index]), (2000,6))))
            values = tf.reshape(resized, (1,-1))[0]
            _Indices = self.createIndices(index)
            new_data = tf.tensor_scatter_nd_update(new_data, _Indices, values)
            return tf.add(index,1), data, _type, new_data
        
        index = tf.constant(0)
        condition = lambda index, data, _type, new_data: tf.less(index, batch_size)
        r = tf.while_loop(condition, _fun, loop_vars=(index, data, _type, new_data))
        return r[-1]
    
    def createIndices(self, BatchSizeIndex):
        def loop1(_i, BatchSizeIndex, col_num, _Indices):
            def loop2(_i, _j, BatchSizeIndex, col_num, _Indices):
                _Indices = tf.tensor_scatter_nd_update(_Indices, [[col_num, 0], [col_num, 1], [col_num, 2]], 
                                                        [BatchSizeIndex, _i, _j])
                return _i, tf.add(_j,1), BatchSizeIndex, tf.add(col_num,1), _Indices
            
            _j = tf.constant(0)
            condition_loop2 = lambda _i, _j, BatchSizeIndex, col_num, _Indices: tf.less(_j, 6)
            r_loop2 = tf.while_loop(condition_loop2, loop2, loop_vars=(_i, _j, BatchSizeIndex, col_num, _Indices))  
            return tf.add(_i,1), BatchSizeIndex, r_loop2[3], r_loop2[4]

        _Indices = tf.zeros((1200,3), dtype=tf.int32)
        col_num = tf.constant(0)
        _i = tf.constant(0)
        condition_loop1 = lambda _i, BatchSizeIndex, col_num, _Indices: tf.less(_i, 200)
        r_loop1 = tf.while_loop(condition_loop1, loop1, loop_vars=(_i, BatchSizeIndex, col_num, _Indices))
        return r_loop1[-1]
    
    def call(self, images, training):
        batch_size = tf.shape(images)[0]
        new_data = tf.zeros((batch_size, 200, 6), dtype=tf.float64)
        images = tf.cast(images, dtype=tf.float64)
        if training:
            data = self.execute(images, batch_size, new_data, tf.constant(True))
        else:
            data = self.execute(images, batch_size, new_data, tf.constant(False))
        
        return data
```

The final code can be modified to execute like this.
```
def augmentation(data):
    .....
    return data

if __name__ == '__main__':
    trainDS = tf.data.Dataset.from_tensor_slices((np.random.rand(3000,2000,6),
                                                  np.concatenate((np.zeros((1500)),np.ones((1500))))))
    trainDS = (
        trainDS
        .cache()
        .shuffle(1000, reshuffle_each_iteration=False)
        .batch(128, drop_remainder=True)
        .prefetch(tf.data.AUTOTUNE))
    
    input = Input((2000,6))
    x = CustomLayer()(input)
    x = LSTM(64, return_sequences=True)(x)
    output = Dense(1,activation='sigmoid')(x)
    model = Model(input, output)
    model.compile(optimizer='adam', loss='BinaryCrossentropy')
    model.fit(trainDS, epochs=3)
```

Results: Alone `tf.data` spend about `18s`, `tf.data`+`CustomLayer` spend about `38s`.

The thing I want to clarify is that the use of `map` in `tf.data` to run augmentation is on the `CPU`, but if I write augmentation in the `Layer`, it should theoretically run on the `GPU`. Why is there such a big gap between the two?

Environment: python3.6, tensorflow2.4.0"
53777,[TFLite] Optional Debug Tools can only support log in Desktop Computer,"Hi TensorFlowers,
The `tflite::PrintInterpreterState(interpreter.get())` in [minimal.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc#L62) can only be used on computer system with desktop. Because it use `printf` style to output info in [optional_debug_tools.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/optional_debug_tools.cc#L278), not the `TFLITE_LOG` in [minimal_logging](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/minimal_logging.h) the better way supporting more systems. Besides, why not use something like `std::stringstream` to format info, that seems more general?"
53776,[TFLite] Code Snippet in official GPU Advances Guide Doc Get Crash.,"Hi TensorFlowers,
The following codes in [official guide](https://tensorflow.google.cn/lite/performance/gpu_advanced#android_cc) seem to be crash until my [PR](https://github.com/tensorflow/tensorflow/pull/53734) applied, please take a check, thx.
```
// Set up interpreter.
auto model = FlatBufferModel::BuildFromFile(model_path);
if (!model) return false;
ops::builtin::BuiltinOpResolver op_resolver;
std::unique_ptr<Interpreter> interpreter;
InterpreterBuilder(*model, op_resolver)(&interpreter);

// NEW: Prepare GPU delegate.
auto* delegate = TfLiteGpuDelegateV2Create(/*default options=*/nullptr);
if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;

// Run inference.
WriteToInputTensor(interpreter->typed_input_tensor<float>(0));
if (interpreter->Invoke() != kTfLiteOk) return false;
ReadFromOutputTensor(interpreter->typed_output_tensor<float>(0));

// NEW: Clean up.
TfLiteGpuDelegateV2Delete(delegate);
```"
53775,A spelling mistake in gpu_delegate_serialization doc.,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://tensorflow.google.cn/lite/performance/gpu_advanced#gpu_delegate_serialization

## Description of issue (what needs changing):
 ""ThThis improvement is is achieved by exchanging disk space for time savings."" -> ""This improvement is is achieved by exchanging disk space for time savings.""


"
53774,JIT Compile too slow ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): master
- Python version: 3.7
- Bazel version (if compiling from source): 4.4.4
- GCC/Compiler version (if compiling from source): 7.3.1
- CUDA/cuDNN version: cuda 11
- GPU model and memory: yes 80GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
![image](https://user-images.githubusercontent.com/33950866/149608623-f2e6ed39-b587-4015-afbd-954bc491ba06.png)
I run 1+1=2, TF cost 30mins...
It really compile 30 mins every times.

**Describe the expected behavior**
I don't want to wait 30mins ...

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):
turn off ```TensorFlow was not built with CUDA kernel binaries compatible with compute capability 8.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.```

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

#@tf.function(experimental_compile=False,autograph=True)
#def zz_sm():
#  # I don't want to use xla .
#  input_a = tf.random.uniform([1024000], dtype=tf.int32, maxval=1024000)
#  uniq_a, uniq_idx = tf.unique(input_a)
#  uniq_a_shape = tf.shape(uniq_a)
#  #uniq_a_shape_print = tf.Print(uniq_a_shape, [uniq_a_shape], ""zz:"", 10, 10)
#  source_a = tf.gather(uniq_a, uniq_idx)
#  source_a = source_a + source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  return source_a
#
#source_a = zz_sm()
with tf.xla.experimental.jit_scope(compile_ops=False):
  input_a = tf.random.uniform([1024000], dtype=tf.int32, maxval=1024000)
  uniq_a, uniq_idx = tf.unique(input_a)
  uniq_a_shape = tf.shape(uniq_a)
  #uniq_a_shape_print = tf.Print(uniq_a_shape, [uniq_a_shape], ""zz:"", 10, 10)
  source_a = tf.gather(uniq_a, uniq_idx)
  source_a = source_a + source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a

#  I want to use xla .
res_a_1 = source_a * source_a
res_a_2 = res_a_1 + res_a_1
res_a_3 = res_a_2 + res_a_2
res_a_4 = res_a_3 + res_a_3
res_a_5 = res_a_4 + res_a_4
res_a_6 = res_a_5 + res_a_5


session_config = tf.compat.v1.ConfigProto(allow_soft_placement=True,
                             log_device_placement=True)
#session_config.graph_options.rewrite_options.disable_meta_optimizer=True
session_config.graph_options.optimizer_options.global_jit_level = tf.compat.v1.OptimizerOptions.ON_1


with tf.compat.v1.Session(config = session_config) as sess:
    for i in range(16384) :
      _ = sess.run([res_a_6]);
      #print(res)
```
"
53772,issue with building manylinux wheel,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**

The system is the image build from [this dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.rbe.cuda11.4-cudnn8.2-ubuntu18.04-manylinux2010-multipython)

**Describe the problem**
When fixing the wheel built in the above environment into a manylinux wheel, auditwheel complains 
```
auditwheel: error: cannot repair ""/tmp/tensorflow-2.8.0rc0-cp39-cp39-linux_x86_64.whl"" to ""manylinux2010_x86_64"" ABI because of the presence of too-recent versioned symbols. You'll need to compile the wheel on an older toolchain.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Build commands:
```
cd <tensorflow source>
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=11.4
export TF_CUDNN_VERSION=8

export LD_LIBRARY_PATH=""/usr/local/cuda-${CUDA}/targets/x86_64-linux/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH:/usr/local/cuda/lib64/stubs:/usr/local/cuda-${CUDA}/lib64:/usr/local/cuda-${CUDA}/lib64""

export TF_CUDA_COMPUTE_CAPABILITIES=3.7,5.2,7.0,7.5,8.0
  
export TF_BUILD_FLAGS=""--config=opt --config=v2 --config=cuda \
    --distinct_host_configuration=false \
    --action_env=TF_CUDA_VERSION --action_env=TF_CUDNN_VERSION \
    --action_env=LD_LIBRARY_PATH --action_env=TF_CUDA_COMPUTE_CAPABILITIES""

yes """" | python3.9 configure.py
bazel build --color=yes --curses=yes $TF_BUILD_FLAGS --verbose_failures \
 //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
before 2.6 there used to be a toolchain to build manylinux wheel 
```
https://github.com/tensorflow/tensorflow/blob/r2.6/third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010-nvcc-cuda11.2/BUILD
```
since 2.7 this toolchain has been removed, I wonder how one should build manylinux wheels without those toolchains

"
53771,Deterministic selection of deterministic cuDNN convolution algorithms removed in TF 2.5,"Summary: deterministic selection of deterministic cuDNN convolution algorithms (added in TensorFlow version 1.14) has been broken since version 2.5. This issue explains how it was broken, provides a work-around, and discusses next-steps.

Pull request [34951](https://github.com/tensorflow/tensorflow/pull/34951), opened by @duncanriach (merged on 2020-01-27 for TF 2.2), added multi-algorithm deterministic cuDNN convolutions, which included deterministically selecting (via the auto-tuning code), for each path through the convolution op, the first working algorithm in a list of deterministic algorithms.

This refactoring [commit](https://github.com/tensorflow/tensorflow/commit/c865c2d8822ecc663f95a01e08b2f465d478f4e9), part of pull request [46965](https://github.com/tensorflow/tensorflow/pull/46965) opened by @kaixih (merged on 2021-03-19 for TF 2.5), accidentally removed the code from `tensorflow/core/kernels/gpu_utils.cc` that caused the first working algorithm to be deterministically selected when op determinism is enabled. Instead, the auto-tuner code runs as normal on the list of deterministic algorithms. This lead to a functional regression in deterministic operation that still exists up to and including TF version 2.8.

Although I have heard @reedwm mention that he thought there might be a regression, this issue was confirmed by [this comment](https://github.com/tensorflow/tensorflow/pull/34951#issuecomment-974714116), from @leiwuzheng, in the original pull request 34951 (which added the functionality) for TF version 2.5. @leiwuzheng, please add some simple reproducer code here.

This regression was not prevented, and not discovered sooner, because there were and are no tests for this functionality. At the time I implemented it, it seemed very difficult or impossible to test.

Thankfully, the XLA-equivalent code (to deterministically select the first working algorithm from a list of deterministic algorithms) appears to have been retained in a similar refactoring by @cheshire in [this commit](https://github.com/tensorflow/tensorflow/commit/eed38bdd4d7bfa7d81efb99d8bd7d68e2288adc5) (committed on 2021-05-20).

The current work-around, as confirmed by @leiwuzheng, is to set the environment variable `TF_CUDNN_USE_FRONTEND` to `1` (from TF version 2.5 through at least 2.8, the default is `0`). This is an effective work-around because the code that uses the new cuDNN 8 frontend/backend API correctly selects the first algorithm from the list of deterministic algorithms, as the legacy code did prior to the regression.

When `TF_CUDNN_USE_FRONTEND` defaults to `1`, the severity and urgency of this issue will be reduced, but the issue will persist until `TF_CUDNN_USE_FRONTEND` is removed along with the associated legacy code.

The intention of this current issue is to drive the reinstatement of the functionality in the legacy code, and to ensure that the functionality is tested in both the legacy code and the new code (with `TF_CUDNN_USE_FRONTEND` set to both `0` and `1`). Testing may require the addition of a mechanism to allow auto-tuning to be reset dynamically (in the running python process). I believe that @reedwm may have further thoughts on how to test."
53770,Inference from frozen_graph yields all black result despite deeplab/vis.py works as intended (frozen_graph uses the same checkpoint),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Stock code
- Both on Windows 10 and Google colab
- TensorFlow 1.x on google colab.
- TensorFlow 2.6.2
- TensorFlow version (use command below):


You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

I used the deeplabibrary to train my models. This is the script i used to create the checkpoint
` %tensorflow_version 1.x
!pip install tf_slim
from google.colab import drive
drive.mount('/content/drive')
!python /content/drive/MyDrive/models/research/deeplab/train.py --logtostderr \
   --training_number_of_steps=30000 \
   --train_split=""train"" \
   --model_variant=""xception_65"" \
   --atrous_rates=6 \
   --atrous_rates=12 \
   --atrous_rates=18 \
   --output_stride=16 \
   --decoder_output_stride=4 \
  --train_crop_size=""513,513"" \
   --train_batch_size=1 \
   --dataset=""pascal_voc_seg"" \
   --tf_initial_checkpoint=""/content/drive/MyDrive/models/deeplabv3_pascal_train_aug/model.ckpt"" \
   --train_logdir=""/content/drive/MyDrive/models/checkpoint_exc_all2"" \
   --dataset_dir=""/content/drive/MyDrive/models/tfrecord_all"" \
   --fine_tune_batch_norm=false \
   --initialize_last_layer=true \
   --last_layers_contain_logits_only=false`

Then, this is the working vis.py:

`%tensorflow_version 1.x
!pip install tf_slim
from google.colab import drive
drive.mount('/content/drive')
!python /content/drive/MyDrive/models/research/deeplab/vis.py --logtostderr \
  --vis_split=""val"" \
  --model_variant=""xception_65"" \
  --output_stride=16 \
    --atrous_rates=6 \
   --atrous_rates=12 \
   --atrous_rates=18 \
   --output_stride=16 \
  --decoder_output_stride=4 \
  --vis_crop_size=""513,513"" \
  --min_resize_value=513 \
  --max_resize_value=513 \
  --dataset=""pascal_voc_seg"" \
  --checkpoint_dir=""/content/drive/MyDrive/models/checkpoint_exc_all2"" \
  --vis_logdir=""/content/drive/MyDrive/models/Result_img_exc_final"" \
  --dataset_dir=""/content/drive/MyDrive/models/tfrecord_all"" \
  --train_crop_size=513 \
  --max_number_of_iterations=1 --eval_interval_secs=0`


As stated i visualize correctly the segmentated output with vis.py.

I tried to export the checkpoint using this:
`%tensorflow_version 1.x
!pip install tf_slim
from google.colab import drive
drive.mount('/content/drive',force_remount=True)
!python /content/drive/MyDrive/models/research/deeplab/export_model.py --logtostderr \
  --model_variant=""xception_65"" \
   --atrous_rates=6 \
   --atrous_rates=12 \
   --atrous_rates=18 \
   --output_stride=16 \
   --crop_size=513 \
--crop_size=513\
   --num_classes=21\
   --export_path=""/content/drive/MyDrive/models/frozen_graph/prova21.pb""\
   --checkpoint_path=""/content/drive/MyDrive/models/checkpoint_exc_all2/model.ckpt-30000"" \`
But when loading and using the model for inference i get an all blank(0) result. This is the script i use:
`import os

from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
from PIL import Image
import time
import cv2
from tqdm import tqdm

import tensorflow as tf

# Needed to show segmentation colormap labels

flags = tf.compat.v1.app.flags

FLAGS = flags.FLAGS

flags.DEFINE_string('model_dir', r""C:\Users\User\PycharmProjects\test_model_seg\models\exc65.pb"", 'Where the model is')
flags.DEFINE_string('image_dir', ""0000631.jpg"", 'Where the image is')
flags.DEFINE_string('save_dir', None, 'Dir for saving results')
flags.DEFINE_string('image_name', None, 'Image name')
class DeepLabModel(object):
    """"""Class to load deeplab model and run inference.""""""

    INPUT_TENSOR_NAME = 'ImageTensor:0'
    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'
    INPUT_SIZE = 513

    def __init__(self, model_dir):
        """"""Creates and loads pretrained deeplab model.""""""
        self.graph = tf.Graph()

        graph_def = None
        # Extract frozen graph from tar archive.
        model_filename = FLAGS.model_dir
        with tf.compat.v1.gfile.FastGFile(model_filename, 'rb') as f:
            graph_def = tf.compat.v1.GraphDef()
            graph_def.ParseFromString(open(""models\exc65.pb"", ""rb"").read())

        if graph_def is None:
            raise RuntimeError('Cannot find inference graph in tar archive.')

        with self.graph.as_default():
            tf.import_graph_def(graph_def, name='')

        self.sess = tf.compat.v1.Session(graph=self.graph)

    def run(self, image):
        """"""Runs inference on a single image.

        Args:
            image: A PIL.Image object, raw input image.

        Returns:
            resized_image: RGB image resized from original input image.
            seg_map: Segmentation map of `resized_image`.
        """"""
        width, height = image.size
        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)
        target_size = (int(resize_ratio * width), int(resize_ratio * height))
        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)
        print('Image resized')
        start_time = time.time()
        batch_seg_map = self.sess.run(
            self.OUTPUT_TENSOR_NAME,
            feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})
        print('Image processing finished')
        print('Elapsed time : ' + str(time.time() - start_time))
        seg_map = batch_seg_map[0]
        return resized_image, seg_map


model = DeepLabModel(FLAGS.model_dir)
print('Model created successfully')

img = Image.open(""0000631.jpg"")

or_img, seg = model.run(img)


for x in seg:
    print(x)`
Some side notes that may be usefull:

I used transfer training to segment the skin. My dataset is composed of nearly 32k images/segmented mask. I only have 2 classes in the segmented mask.(skin and background). I don't really know whats wrong with my code, perhaps is the way i import it?

Thanks in advance



"
53767,TFLite Converter segfaults when trying to convert per-channel quantized transposed convolutions,"When converting transposed convolutions using per-channel weight quantization the converter segfaults and crashes the Python process. Per-channel quantization is supported by TFLite Transposed convolutions:
https://github.com/tensorflow/tensorflow/blob/f87be6c7de847017c48520649e3d771e5d6b81b6/tensorflow/lite/kernels/transpose_conv.cc#L371-L380
so the converter shouldn't segfault when trying to convert such a model.

It looks like this issue has been introduced in TensorFlow 2.6 since the same model code produced a valid TFLite file in TensorFlow 2.5. This issue might also be related to #53766, but in any case the converter should never segfault.

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS / Ubuntu
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6, 2.7, 2.8rc0 and 2.9.0-dev20220114

### 2. Code

A minimal reproduction of the issue and a workaround is available in [this notebook](https://colab.research.google.com/drive/1IXri5HeDc9qTAtDOp-LqZyQTL8CcemGq?usp=sharing).

```python
import tensorflow as tf


class QuantConv2DTransposed(tf.keras.layers.Layer):
    def build(self, input_shape):
        self.kernel = self.add_weight(""kernel"", [3, 3, input_shape[-1], 24])

    def call(self, inputs):
        filters = tf.quantization.fake_quant_with_min_max_vars_per_channel(
            self.kernel, -3.0 * tf.ones([24]), 3.0 * tf.ones([24]), narrow_range=True
        )
        filters = tf.transpose(filters, (0, 1, 3, 2))
        return tf.nn.conv2d_transpose(inputs, filters, [*inputs.shape[:-1], 24], 1)


inp = tf.keras.Input(shape=(6, 8, 48), batch_size=1)
x = tf.quantization.fake_quant_with_min_max_vars(inp, -3.0, 3.0, narrow_range=True)
x = QuantConv2DTransposed()(x)
x = tf.quantization.fake_quant_with_min_max_vars(x, -3.0, 3.0, narrow_range=True)

model = tf.keras.Model(inp, x)

model.save(""/tmp/testing"")
converter = tf.lite.TFLiteConverter.from_saved_model(""/tmp/testing"")
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# terminated by signal SIGSEGV (Address boundary error)
tflite_model = converter.convert()
```"
53766,Constant folding fails when converting int8 transposed convolutions,"When converting a model that used int8 quantization aware training conversion of transposed convolutions fails.

The converter isn't able to correctly constant fold the fake quantized weights and keeps an unnecessary `tfl.transpose` operation in the graph which leads to problems when executing the TFLite model. Note that this issue is independent of TensorFlow Model Optimization and can be reproduced using plain TensorFlow as well (see linked notebook).

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS / Ubuntu
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5, 2.6, 2.7, 2.8rc0 and 2.9.0-dev20220114

### 2. Code

A minimal reproduction of the issue is available in [this notebook](https://colab.research.google.com/drive/1bfI_Dx3zyoHkabJlHpNFnbDA9r9G5ffE?usp=sharing). Re-run the notebook to show `netron` visualisations showing the conversion problem."
53763,[TFLite] Accumulator and bias types coherence for int16x8 FC operator,"Hello,

We noticed that int32 accumulator and int32 bias support was recently added for the int16x8 FULLY_CONNECTED operator (along with CONV_2D and TRANSPOSE_CONV_2D operators) through the `_experimental_full_integer_quantization_bias_type` option of the TFLiteConverter :
* [Add option to store bias as 32 bit in 16x8 Quant.](https://github.com/tensorflow/tensorflow/commit/ea33c1e7a25d8025e8ee405ad8ab7be261798d76)
* [Update TFLite kernel to use Ruy 16x8 Gemm instead of reference kernel.](https://github.com/tensorflow/tensorflow/commit/5de18207d9568ee25bdd57db8eaf6268fe0b913d) 

It seems though that these changes are leading to some incoherences with potential unexpected overflow and also created a bug. Before an int16x8 FC would always use an int64 accumulator to avoid any overflow and an int64 bias. The current status now seems to be:
* converter._experimental_full_integer_quantization_bias_type = tf.int64 or None:
    * OpResolverType.BUILTIN_REF
        * use_bias = True
            * int64 accumulator and int64 bias
        * use_bias = False
            * int64 accumulator
    * OpResolverType.BUILTIN
        * use_bias = True
            * int64 accumulator and int64 bias
        * use_bias = False
            * int32 accumulator (would have expected an int64 accumulator)
 * converter._experimental_full_integer_quantization_bias_type = tf.int32:
    * OpResolverType.BUILTIN_REF
        * use_bias = True
            * int64 accumulator and int64 bias (read an int32 bias tensor as an int64 tensor => bug, need to be an int32 bias and would expect an int32 accumulator)
        * use_bias = False
            * int64 accumulator (would have expected an int32 accumulator)
    * OpResolverType.BUILTIN
        * use_bias = True
            * int32 accumulator and int32 bias
        * use_bias = False
            * int32 accumulator

Ideally we would have expected that an int64 accumulator (no matter if the layer has no bias) and int64 bias would be used when `converter._experimental_full_integer_quantization_bias_type = tf.int64 or None` (both with the reference and optimized kernels) and an int32 accumulator (no matter if the layer has no bias) and int32 bias when `converter._experimental_full_integer_quantization_bias_type = tf.int32` (both with the reference and optimized kernels).

These incoherences also affect the CONV_2D and TRANPOSE_CONV_2D operators and we were wondering if it was intentional (and if so the relation between the bias type and the accumulator type) or just an oversight?

Code to illustrate the bug with `converter._experimental_full_integer_quantization_bias_type = tf.int32`, `OpResolverType.BUILTIN_REF` and `use_bias=True`. It's due to the https://github.com/tensorflow/tensorflow/blob/ea33c1e7a25d8025e8ee405ad8ab7be261798d76/tensorflow/lite/kernels/fully_connected.cc#L835 condition that will be true if `kernel_type == kReference` no matter the type of the bias while `FullyConnectedInt16` will always assume an int64 bias and thus read the int32 bias tensor as an int64 tensor. It fails to provide the expected results with tf_nightly-2.9.0.dev20220114.

```Python
import tensorflow as tf

input = tf.keras.Input(shape=[1], batch_size=1)
output = tf.keras.layers.Dense(
    10,
    kernel_initializer=tf.keras.initializers.Constant(value=0),
    bias_initializer=tf.keras.initializers.Constant(value=1),
)(input)
model = tf.keras.Model(inputs=[input], outputs=output)
model.summary()



def representative_dataset():
    yield [tf.constant([1.0])]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8
]
converter._experimental_full_integer_quantization_bias_type = tf.int32 # Will work if commented

tflite_model = converter.convert()
with open(""test.tflite"", ""wb"") as fp:
    fp.write(tflite_model)



interpreter = tf.lite.Interpreter(
    model_path=""test.tflite"",
    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.BUILTIN_REF, # Will work if commented
)
interpreter.allocate_tensors()

input = [tf.constant([1.0])]
interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input)
interpreter.invoke()
output = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])

# [1.]
print(input)
# Expected: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
print(output)
```

FYI @dayeongl"
53762,TF2MLIR  tf Dialect -> hlo Dialect,"
**System information**
- Linux ubuntu 18.04
- TensorFlow installed from source
- TensorFlow version: r2.6
- Python version: 3.8
- Bazel version: 4.2.2
- GCC/Compiler version: 9.4.0

I am ready to lowering the Tensorflow model to linalg Dialect

First, I use the bazel build the tf-opt and tf-mlir-translate.


Tensorflow model ssd is from Tensorflow Hub
And use the tools:
tf-mlir-translate :  savedmodel -> mlir    --ok
`./tf-mlir-translate --savedmodel-signaturedefs-to-mlir ./ssd  -o ssd.mlir`

tf-opt :  tf_executor Dialect -> tf Dialect   --ok
`./tf-opt -tf-executor-to-functional-conversion ssd.mlir -o ssd-func.mlir `

tf-opt :  tf Dialect -> hlo Dialect  -- error
`$ ./tf-opt --tf-to-hlo-pipeline ssd-func.mlir -o ssd-mhlo.mlir`
I got the error 

```
remark: lowering requires static shaped tensor operands
    %1136 = ""tf.ResizeBilinear""(%1135, %cst_3579) {align_corners = false, device = """", half_pixel_centers = false} : (tensor<*xf32>, tensor<2xi32>) -> tensor<*xf32>

```
The TF model is ssd-mobilenet, does it not support dynamic shape? or i used it wrong.
tensorflow version r2.6

the full  output log:

```
2022-01-17 08:31:23.118405: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
ssd-func.mlir:5099:13: remark: lowering requires static shaped tensor operands
    %1136 = ""tf.ResizeBilinear""(%1135, %cst_3579) {align_corners = false, device = """", half_pixel_centers = false} : (tensor<*xf32>, tensor<2xi32>) -> tensor<*xf32>
            ^
ssd-func.mlir:356:12: note: called from
    %0:8 = ""tf.StatefulPartitionedCall""(%arg0, %arg173, %arg175, %arg174, %arg176, %arg177, %arg178, %arg200, %arg199, %arg201, %arg202, %arg243, %arg265, %arg264, %arg266, %arg267, %arg203, %arg205, %arg204, %arg206, %arg207, %arg268, %arg270, %arg269, %arg271, %arg272, %arg208, %arg210, %arg209, %arg211, %arg212, %arg273, %arg275, %arg274, %arg276, %arg277, %arg213, %arg215, %arg214, %arg216, %arg217, %arg278, %arg280, %arg279, %arg281, %arg282, %arg218, %arg220, %arg219, %arg221, %arg222, %arg283, %arg285, %arg284, %arg286, %arg287, %arg223, %arg225, %arg224, %arg226, %arg227, %arg288, %arg290, %arg289, %arg291, %arg292, %arg228, %arg230, %arg229, %arg231, %arg232, %arg293, %arg295, %arg294, %arg296, %arg297, %arg233, %arg235, %arg234, %arg236, %arg237, %arg298, %arg300, %arg299, %arg301, %arg302, %arg238, %arg240, %arg239, %arg241, %arg242, %arg303, %arg305, %arg304, %arg306, %arg307, %arg179, %arg181, %arg180, %arg182, %arg183, %arg244, %arg246, %arg245, %arg247, %arg248, %arg184, %arg186, %arg185, %arg187, %arg188, %arg249, %arg251, %arg250, %arg252, %arg253, %arg189, %arg191, %arg190, %arg192, %arg193, %arg254, %arg256, %arg255, %arg257, %arg258, %arg194, %arg196, %arg195, %arg197, %arg198, %arg259, %arg261, %arg260, %arg262, %arg263, %arg313, %arg312, %arg311, %arg310, %arg323, %arg320, %arg319, %arg321, %arg322, %arg309, %arg308, %arg318, %arg315, %arg314, %arg316, %arg317, %arg328, %arg325, %arg324, %arg326, %arg327, %arg333, %arg330, %arg329, %arg331, %arg332, %cst, %cst_0, %cst_6, %cst_7, %cst_8, %cst_9, %cst_10, %cst_11, %cst_12, %cst_13, %cst_1, %cst_2, %cst_3, %cst_4, %cst_5, %arg21, %arg2, %arg1, %arg3, %arg4, %arg42, %arg23, %arg22, %arg24, %arg25, %arg63, %arg44, %arg43, %arg45, %arg46, %arg84, %arg65, %arg64, %arg66, %arg67, %arg170, %arg169, %arg105, %arg86, %arg85, %arg87, %arg88, %arg126, %arg107, %arg106, %arg108, %arg109, %arg147, %arg128, %arg127, %arg129, %arg130, %arg168, %arg149, %arg148, %arg150, %arg151, %arg172, %arg171, %arg6, %arg5, %arg7, %arg8, %arg27, %arg26, %arg28, %arg29, %arg48, %arg47, %arg49, %arg50, %arg69, %arg68, %arg70, %arg71, %arg90, %arg89, %arg91, %arg92, %arg111, %arg110, %arg112, %arg113, %arg132, %arg131, %arg133, %arg134, %arg153, %arg152, %arg154, %arg155, %arg10, %arg9, %arg11, %arg12, %arg31, %arg30, %arg32, %arg33, %arg52, %arg51, %arg53, %arg54, %arg73, %arg72, %arg74, %arg75, %arg94, %arg93, %arg95, %arg96, %arg115, %arg114, %arg116, %arg117, %arg136, %arg135, %arg137, %arg138, %arg157, %arg156, %arg158, %arg159, %arg14, %arg13, %arg15, %arg16, %arg35, %arg34, %arg36, %arg37, %arg56, %arg55, %arg57, %arg58, %arg77, %arg76, %arg78, %arg79, %arg98, %arg97, %arg99, %arg100, %arg119, %arg118, %arg120, %arg121, %arg140, %arg139, %arg141, %arg142, %arg161, %arg160, %arg162, %arg163, %arg18, %arg17, %arg19, %arg20, %arg39, %arg38, %arg40, %arg41, %arg60, %arg59, %arg61, %arg62, %arg81, %arg80, %arg82, %arg83, %arg102, %arg101, %arg103, %arg104, %arg123, %arg122, %arg124, %arg125, %arg144, %arg143, %arg145, %arg146, %arg165, %arg164, %arg166, %arg167) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348], config = """", config_proto = ""\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00"", device = """", executor_type = """", f = @__inference_signature_wrapper_236900} : (tensor<1x?x?x3xui8>, tensor<!tf_type.resource<tensor<3x3x3x32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<3x3x32x1xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<1x1x32x64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<3x3x64x1xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<1x1x64x128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<3x3x128x1xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<1x1x128x128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<3x3x128x1xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<1x1x128x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x1xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x1xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x256x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<3x3x1024x1xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1x1x1024x1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1x1x1024x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x24xf32>>>, tensor<!tf_type.resource<tensor<24xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x546xf32>>>, tensor<!tf_type.resource<tensor<546xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>) -> (tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>)
           ^
ssd-func.mlir:5099:13: note: see current operation: %169 = ""tf.ResizeBilinear""(%168, %cst_118) {align_corners = false, device = """", half_pixel_centers = false} : (tensor<1x?x?x3xf32>, tensor<2xi32>) -> tensor<1x640x640x3xf32>
    %1136 = ""tf.ResizeBilinear""(%1135, %cst_3579) {align_corners = false, device = """", half_pixel_centers = false} : (tensor<*xf32>, tensor<2xi32>) -> tensor<*xf32>
            ^
ssd-func.mlir:5099:13: remark: lowering requires static shaped tensor operands
    %1136 = ""tf.ResizeBilinear""(%1135, %cst_3579) {align_corners = false, device = """", half_pixel_centers = false} : (tensor<*xf32>, tensor<2xi32>) -> tensor<*xf32>
            ^
ssd-func.mlir:356:12: note: called from
    %0:8 = ""tf.StatefulPartitionedCall""(%arg0, %arg173, %arg175, %arg174, %arg176, %arg177, %arg178, %arg200, %arg199, %arg201, %arg202, %arg243, %arg265, %arg264, %arg266, %arg267, %arg203, %arg205, %arg204, %arg206, %arg207, %arg268, %arg270, %arg269, %arg271, %arg272, %arg208, %arg210, %arg209, %arg211, %arg212, %arg273, %arg275, %arg274, %arg276, %arg277, %arg213, %arg215, %arg214, %arg216, %arg217, %arg278, %arg280, %arg279, %arg281, %arg282, %arg218, %arg220, %arg219, %arg221, %arg222, %arg283, %arg285, %arg284, %arg286, %arg287, %arg223, %arg225, %arg224, %arg226, %arg227, %arg288, %arg290, %arg289, %arg291, %arg292, %arg228, %arg230, %arg229, %arg231, %arg232, %arg293, %arg295, %arg294, %arg296, %arg297, %arg233, %arg235, %arg234, %arg236, %arg237, %arg298, %arg300, %arg299, %arg301, %arg302, %arg238, %arg240, %arg239, %arg241, %arg242, %arg303, %arg305, %arg304, %arg306, %arg307, %arg179, %arg181, %arg180, %arg182, %arg183, %arg244, %arg246, %arg245, %arg247, %arg248, %arg184, %arg186, %arg185, %arg187, %arg188, %arg249, %arg251, %arg250, %arg252, %arg253, %arg189, %arg191, %arg190, %arg192, %arg193, %arg254, %arg256, %arg255, %arg257, %arg258, %arg194, %arg196, %arg195, %arg197, %arg198, %arg259, %arg261, %arg260, %arg262, %arg263, %arg313, %arg312, %arg311, %arg310, %arg323, %arg320, %arg319, %arg321, %arg322, %arg309, %arg308, %arg318, %arg315, %arg314, %arg316, %arg317, %arg328, %arg325, %arg324, %arg326, %arg327, %arg333, %arg330, %arg329, %arg331, %arg332, %cst, %cst_0, %cst_6, %cst_7, %cst_8, %cst_9, %cst_10, %cst_11, %cst_12, %cst_13, %cst_1, %cst_2, %cst_3, %cst_4, %cst_5, %arg21, %arg2, %arg1, %arg3, %arg4, %arg42, %arg23, %arg22, %arg24, %arg25, %arg63, %arg44, %arg43, %arg45, %arg46, %arg84, %arg65, %arg64, %arg66, %arg67, %arg170, %arg169, %arg105, %arg86, %arg85, %arg87, %arg88, %arg126, %arg107, %arg106, %arg108, %arg109, %arg147, %arg128, %arg127, %arg129, %arg130, %arg168, %arg149, %arg148, %arg150, %arg151, %arg172, %arg171, %arg6, %arg5, %arg7, %arg8, %arg27, %arg26, %arg28, %arg29, %arg48, %arg47, %arg49, %arg50, %arg69, %arg68, %arg70, %arg71, %arg90, %arg89, %arg91, %arg92, %arg111, %arg110, %arg112, %arg113, %arg132, %arg131, %arg133, %arg134, %arg153, %arg152, %arg154, %arg155, %arg10, %arg9, %arg11, %arg12, %arg31, %arg30, %arg32, %arg33, %arg52, %arg51, %arg53, %arg54, %arg73, %arg72, %arg74, %arg75, %arg94, %arg93, %arg95, %arg96, %arg115, %arg114, %arg116, %arg117, %arg136, %arg135, %arg137, %arg138, %arg157, %arg156, %arg158, %arg159, %arg14, %arg13, %arg15, %arg16, %arg35, %arg34, %arg36, %arg37, %arg56, %arg55, %arg57, %arg58, %arg77, %arg76, %arg78, %arg79, %arg98, %arg97, %arg99, %arg100, %arg119, %arg118, %arg120, %arg121, %arg140, %arg139, %arg141, %arg142, %arg161, %arg160, %arg162, %arg163, %arg18, %arg17, %arg19, %arg20, %arg39, %arg38, %arg40, %arg41, %arg60, %arg59, %arg61, %arg62, %arg81, %arg80, %arg82, %arg83, %arg102, %arg101, %arg103, %arg104, %arg123, %arg122, %arg124, %arg125, %arg144, %arg143, %arg145, %arg146, %arg165, %arg164, %arg166, %arg167) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348], config = """", config_proto = ""\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00"", device = """", executor_type = """", f = @__inference_signature_wrapper_236900} : (tensor<1x?x?x3xui8>, tensor<!tf_type.resource<tensor<3x3x3x32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<3x3x32x1xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<1x1x32x64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<3x3x64x1xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<1x1x64x128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<3x3x128x1xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<1x1x128x128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<3x3x128x1xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<1x1x128x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x1xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x1xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x256x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<3x3x1024x1xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1x1x1024x1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1x1x1024x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x24xf32>>>, tensor<!tf_type.resource<tensor<24xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x546xf32>>>, tensor<!tf_type.resource<tensor<546xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>) -> (tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>)
           ^
ssd-func.mlir:5099:13: note: see current operation: %50 = ""tf.ResizeBilinear""(%49, %20) {align_corners = false, device = """", half_pixel_centers = false} : (tensor<1x?x?x3xf32>, tensor<2xi32>) -> tensor<1x640x640x3xf32>
    %1136 = ""tf.ResizeBilinear""(%1135, %cst_3579) {align_corners = false, device = """", half_pixel_centers = false} : (tensor<*xf32>, tensor<2xi32>) -> tensor<*xf32>
            ^
ssd-func.mlir:340:3: error: The following operations cannot be legalized: tf.NonMaxSuppressionV5 (count: 90); tf.ReadVariableOp (count: 381); tf.ResizeBilinear (count: 1); tf.TopKV2 (count: 2); tf.Where (count: 1). These legalization failure(s) may be due to missing TF to HLO lowerings and/or unsupported attributes, etc.
  func @serving_default(%arg0: tensor<1x?x?x3xui8> {tf_saved_model.index_path = [""input_tensor""]}, %arg1: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/beta""}, %arg2: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/gamma""}, %arg3: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/moving_mean""}, %arg4: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/moving_variance""}, %arg5: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/beta""}, %arg6: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/gamma""}, %arg7: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/moving_mean""}, %arg8: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/moving_variance""}, %arg9: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/beta""}, %arg10: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/gamma""}, %arg11: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/moving_mean""}, %arg12: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/moving_variance""}, %arg13: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/beta""}, %arg14: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/gamma""}, %arg15: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/moving_mean""}, %arg16: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/moving_variance""}, %arg17: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/beta""}, %arg18: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/gamma""}, %arg19: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/moving_mean""}, %arg20: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/moving_variance""}, %arg21: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/kernel""}, %arg22: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/beta""}, %arg23: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/gamma""}, %arg24: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/moving_mean""}, %arg25: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/moving_variance""}, %arg26: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/beta""}, %arg27: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/gamma""}, %arg28: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/moving_mean""}, %arg29: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/moving_variance""}, %arg30: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/beta""}, %arg31: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/gamma""}, %arg32: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/moving_mean""}, %arg33: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/moving_variance""}, %arg34: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/beta""}, %arg35: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/gamma""}, %arg36: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/moving_mean""}, %arg37: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/moving_variance""}, %arg38: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/beta""}, %arg39: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/gamma""}, %arg40: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/moving_mean""}, %arg41: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/moving_variance""}, %arg42: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/kernel""}, %arg43: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/beta""}, %arg44: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/gamma""}, %arg45: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/moving_mean""}, %arg46: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/moving_variance""}, %arg47: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/beta""}, %arg48: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/gamma""}, %arg49: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/moving_mean""}, %arg50: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/moving_variance""}, %arg51: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/beta""}, %arg52: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/gamma""}, %arg53: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/moving_mean""}, %arg54: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/moving_variance""}, %arg55: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/beta""}, %arg56: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/gamma""}, %arg57: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/moving_mean""}, %arg58: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/moving_variance""}, %arg59: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/beta""}, %arg60: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/gamma""}, %arg61: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/moving_mean""}, %arg62: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/moving_variance""}, %arg63: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/kernel""}, %arg64: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/beta""}, %arg65: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/gamma""}, %arg66: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/moving_mean""}, %arg67: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/moving_variance""}, %arg68: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/beta""}, %arg69: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/gamma""}, %arg70: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/moving_mean""}, %arg71: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/moving_variance""}, %arg72: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/beta""}, %arg73: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/gamma""}, %arg74: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/moving_mean""}, %arg75: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/moving_variance""}, %arg76: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/beta""}, %arg77: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/gamma""}, %arg78: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/moving_mean""}, %arg79: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/moving_variance""}, %arg80: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/beta""}, %arg81: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/gamma""}, %arg82: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/moving_mean""}, %arg83: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/moving_variance""}, %arg84: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/kernel""}, %arg85: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/beta""}, %arg86: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/gamma""}, %arg87: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/moving_mean""}, %arg88: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/moving_variance""}, %arg89: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/beta""}, %arg90: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/gamma""}, %arg91: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/moving_mean""}, %arg92: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/moving_variance""}, %arg93: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/beta""}, %arg94: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/gamma""}, %arg95: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/moving_mean""}, %arg96: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/moving_variance""}, %arg97: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/beta""}, %arg98: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/gamma""}, %arg99: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/moving_mean""}, %arg100: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/moving_variance""}, %arg101: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/beta""}, %arg102: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/gamma""}, %arg103: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/moving_mean""}, %arg104: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/moving_variance""}, %arg105: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/kernel""}, %arg106: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/beta""}, %arg107: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/gamma""}, %arg108: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/moving_mean""}, %arg109: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/moving_variance""}, %arg110: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/beta""}, %arg111: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/gamma""}, %arg112: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/moving_mean""}, %arg113: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/moving_variance""}, %arg114: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/beta""}, %arg115: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/gamma""}, %arg116: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/moving_mean""}, %arg117: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/moving_variance""}, %arg118: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/beta""}, %arg119: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/gamma""}, %arg120: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/moving_mean""}, %arg121: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/moving_variance""}, %arg122: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/beta""}, %arg123: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/gamma""}, %arg124: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/moving_mean""}, %arg125: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/moving_variance""}, %arg126: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/kernel""}, %arg127: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/beta""}, %arg128: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/gamma""}, %arg129: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/moving_mean""}, %arg130: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/moving_variance""}, %arg131: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/beta""}, %arg132: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/gamma""}, %arg133: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/moving_mean""}, %arg134: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/moving_variance""}, %arg135: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/beta""}, %arg136: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/gamma""}, %arg137: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/moving_mean""}, %arg138: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/moving_variance""}, %arg139: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/beta""}, %arg140: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/gamma""}, %arg141: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/moving_mean""}, %arg142: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/moving_variance""}, %arg143: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/beta""}, %arg144: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/gamma""}, %arg145: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/moving_mean""}, %arg146: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/moving_variance""}, %arg147: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/kernel""}, %arg148: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/beta""}, %arg149: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/gamma""}, %arg150: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/moving_mean""}, %arg151: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/moving_variance""}, %arg152: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/beta""}, %arg153: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/gamma""}, %arg154: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/moving_mean""}, %arg155: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/moving_variance""}, %arg156: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/beta""}, %arg157: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/gamma""}, %arg158: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/moving_mean""}, %arg159: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/moving_variance""}, %arg160: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/beta""}, %arg161: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/gamma""}, %arg162: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/moving_mean""}, %arg163: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/moving_variance""}, %arg164: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/beta""}, %arg165: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/gamma""}, %arg166: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/moving_mean""}, %arg167: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/moving_variance""}, %arg168: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/kernel""}, %arg169: tensor<!tf_type.resource<tensor<24xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/bias""}, %arg170: tensor<!tf_type.resource<tensor<3x3x256x24xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/kernel""}, %arg171: tensor<!tf_type.resource<tensor<546xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/bias""}, %arg172: tensor<!tf_type.resource<tensor<3x3x256x546xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/kernel""}, %arg173: tensor<!tf_type.resource<tensor<3x3x3x32xf32>>> {tf_saved_model.bound_input = @""conv1/kernel""}, %arg174: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv1_bn/beta""}, %arg175: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv1_bn/gamma""}, %arg176: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv1_bn/moving_mean""}, %arg177: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv1_bn/moving_variance""}, %arg178: tensor<!tf_type.resource<tensor<3x3x32x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_1/depthwise_kernel""}, %arg179: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_10/depthwise_kernel""}, %arg180: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_10_bn/beta""}, %arg181: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_10_bn/gamma""}, %arg182: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_10_bn/moving_mean""}, %arg183: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_10_bn/moving_variance""}, %arg184: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_11/depthwise_kernel""}, %arg185: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_11_bn/beta""}, %arg186: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_11_bn/gamma""}, %arg187: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_11_bn/moving_mean""}, %arg188: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_11_bn/moving_variance""}, %arg189: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_12/depthwise_kernel""}, %arg190: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_12_bn/beta""}, %arg191: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_12_bn/gamma""}, %arg192: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_12_bn/moving_mean""}, %arg193: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_12_bn/moving_variance""}, %arg194: tensor<!tf_type.resource<tensor<3x3x1024x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_13/depthwise_kernel""}, %arg195: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_dw_13_bn/beta""}, %arg196: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_dw_13_bn/gamma""}, %arg197: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_dw_13_bn/moving_mean""}, %arg198: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_dw_13_bn/moving_variance""}, %arg199: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv_dw_1_bn/beta""}, %arg200: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv_dw_1_bn/gamma""}, %arg201: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv_dw_1_bn/moving_mean""}, %arg202: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv_dw_1_bn/moving_variance""}, %arg203: tensor<!tf_type.resource<tensor<3x3x64x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_2/depthwise_kernel""}, %arg204: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_dw_2_bn/beta""}, %arg205: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_dw_2_bn/gamma""}, %arg206: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_dw_2_bn/moving_mean""}, %arg207: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_dw_2_bn/moving_variance""}, %arg208: tensor<!tf_type.resource<tensor<3x3x128x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_3/depthwise_kernel""}, %arg209: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_3_bn/beta""}, %arg210: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_3_bn/gamma""}, %arg211: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_3_bn/moving_mean""}, %arg212: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_3_bn/moving_variance""}, %arg213: tensor<!tf_type.resource<tensor<3x3x128x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_4/depthwise_kernel""}, %arg214: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_4_bn/beta""}, %arg215: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_4_bn/gamma""}, %arg216: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_4_bn/moving_mean""}, %arg217: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_4_bn/moving_variance""}, %arg218: tensor<!tf_type.resource<tensor<3x3x256x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_5/depthwise_kernel""}, %arg219: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_5_bn/beta""}, %arg220: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_5_bn/gamma""}, %arg221: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_5_bn/moving_mean""}, %arg222: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_5_bn/moving_variance""}, %arg223: tensor<!tf_type.resource<tensor<3x3x256x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_6/depthwise_kernel""}, %arg224: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_6_bn/beta""}, %arg225: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_6_bn/gamma""}, %arg226: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_6_bn/moving_mean""}, %arg227: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_6_bn/moving_variance""}, %arg228: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_7/depthwise_kernel""}, %arg229: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_7_bn/beta""}, %arg230: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_7_bn/gamma""}, %arg231: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_7_bn/moving_mean""}, %arg232: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_7_bn/moving_variance""}, %arg233: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_8/depthwise_kernel""}, %arg234: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_8_bn/beta""}, %arg235: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_8_bn/gamma""}, %arg236: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_8_bn/moving_mean""}, %arg237: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_8_bn/moving_variance""}, %arg238: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_9/depthwise_kernel""}, %arg239: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_9_bn/beta""}, %arg240: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_9_bn/gamma""}, %arg241: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_9_bn/moving_mean""}, %arg242: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_9_bn/moving_variance""}, %arg243: tensor<!tf_type.resource<tensor<1x1x32x64xf32>>> {tf_saved_model.bound_input = @""conv_pw_1/kernel""}, %arg244: tensor<!tf_type.resource<tensor<1x1x512x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_10/kernel""}, %arg245: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_10_bn/beta""}, %arg246: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_10_bn/gamma""}, %arg247: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_10_bn/moving_mean""}, %arg248: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_10_bn/moving_variance""}, %arg249: tensor<!tf_type.resource<tensor<1x1x512x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_11/kernel""}, %arg250: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_11_bn/beta""}, %arg251: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_11_bn/gamma""}, %arg252: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_11_bn/moving_mean""}, %arg253: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_11_bn/moving_variance""}, %arg254: tensor<!tf_type.resource<tensor<1x1x512x1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_12/kernel""}, %arg255: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_12_bn/beta""}, %arg256: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_12_bn/gamma""}, %arg257: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_12_bn/moving_mean""}, %arg258: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_12_bn/moving_variance""}, %arg259: tensor<!tf_type.resource<tensor<1x1x1024x1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_13/kernel""}, %arg260: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_13_bn/beta""}, %arg261: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_13_bn/gamma""}, %arg262: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_13_bn/moving_mean""}, %arg263: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_13_bn/moving_variance""}, %arg264: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_pw_1_bn/beta""}, %arg265: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_pw_1_bn/gamma""}, %arg266: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_pw_1_bn/moving_mean""}, %arg267: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_pw_1_bn/moving_variance""}, %arg268: tensor<!tf_type.resource<tensor<1x1x64x128xf32>>> {tf_saved_model.bound_input = @""conv_pw_2/kernel""}, %arg269: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_2_bn/beta""}, %arg270: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_2_bn/gamma""}, %arg271: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_2_bn/moving_mean""}, %arg272: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_2_bn/moving_variance""}, %arg273: tensor<!tf_type.resource<tensor<1x1x128x128xf32>>> {tf_saved_model.bound_input = @""conv_pw_3/kernel""}, %arg274: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_3_bn/beta""}, %arg275: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_3_bn/gamma""}, %arg276: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_3_bn/moving_mean""}, %arg277: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_3_bn/moving_variance""}, %arg278: tensor<!tf_type.resource<tensor<1x1x128x256xf32>>> {tf_saved_model.bound_input = @""conv_pw_4/kernel""}, %arg279: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_4_bn/beta""}, %arg280: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_4_bn/gamma""}, %arg281: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_4_bn/moving_mean""}, %arg282: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_4_bn/moving_variance""}, %arg283: tensor<!tf_type.resource<tensor<1x1x256x256xf32>>> {tf_saved_model.bound_input = @""conv_pw_5/kernel""}, %arg284: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_5_bn/beta""}, %arg285: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_5_bn/gamma""}, %arg286: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_5_bn/moving_mean""}, %arg287: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_5_bn/moving_variance""}, %arg288: tensor<!tf_type.resource<tensor<1x1x256x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_6/kernel""}, %arg289: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_6_bn/beta""}, %arg290: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_6_bn/gamma""}, %arg291: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_6_bn/moving_mean""}, %arg292: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_6_bn/moving_variance""}, %arg293: tensor<!tf_type.resource<tensor<1x1x512x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_7/kernel""}, %arg294: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_7_bn/beta""}, %arg295: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_7_bn/gamma""}, %arg296: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_7_bn/moving_mean""}, %arg297: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_7_bn/moving_variance""}, %arg298: tensor<!tf_type.resource<tensor<1x1x512x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_8/kernel""}, %arg299: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_8_bn/beta""}, %arg300: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_8_bn/gamma""}, %arg301: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_8_bn/moving_mean""}, %arg302: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_8_bn/moving_variance""}, %arg303: tensor<!tf_type.resource<tensor<1x1x512x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_9/kernel""}, %arg304: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_9_bn/beta""}, %arg305: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_9_bn/gamma""}, %arg306: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_9_bn/moving_mean""}, %arg307: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_9_bn/moving_variance""}, %arg308: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_1/bias""}, %arg309: tensor<!tf_type.resource<tensor<1x1x256x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_1/kernel""}, %arg310: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_2/bias""}, %arg311: tensor<!tf_type.resource<tensor<1x1x512x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_2/kernel""}, %arg312: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_3/bias""}, %arg313: tensor<!tf_type.resource<tensor<1x1x1024x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_3/kernel""}, %arg314: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_1_batchnorm/beta""}, %arg315: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_1_batchnorm/gamma""}, %arg316: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_1_batchnorm/moving_mean""}, %arg317: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_1_batchnorm/moving_variance""}, %arg318: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_1_conv/kernel""}, %arg319: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_2_batchnorm/beta""}, %arg320: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_2_batchnorm/gamma""}, %arg321: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_2_batchnorm/moving_mean""}, %arg322: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_2_batchnorm/moving_variance""}, %arg323: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_2_conv/kernel""}, %arg324: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_14_batchnorm/beta""}, %arg325: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_14_batchnorm/gamma""}, %arg326: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_14_batchnorm/moving_mean""}, %arg327: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_14_batchnorm/moving_variance""}, %arg328: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_14_conv/kernel""}, %arg329: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_15_batchnorm/beta""}, %arg330: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_15_batchnorm/gamma""}, %arg331: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_15_batchnorm/moving_mean""}, %arg332: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_15_batchnorm/moving_variance""}, %arg333: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_15_conv/kernel""}) -> (tensor<*xf32> {tf_saved_model.index_path = [""raw_detection_boxes""]}, tensor<*xf32> {tf_saved_model.index_path = [""detection_anchor_indices""]}, tensor<*xf32> {tf_saved_model.index_path = [""num_detections""]}, tensor<*xf32> {tf_saved_model.index_path = [""raw_detection_scores""]}, tensor<*xf32> {tf_saved_model.index_path = [""detection_boxes""]}, tensor<*xf32> {tf_saved_model.index_path = [""detection_classes""]}, tensor<*xf32> {tf_saved_model.index_path = [""detection_scores""]}, tensor<*xf32> {tf_saved_model.index_path = [""detection_multiclass_scores""]}) attributes {tf.entry_function = {control_outputs = """", inputs = ""serving_default_input_tensor:0"", outputs = ""StatefulPartitionedCall:6,StatefulPartitionedCall:0,StatefulPartitionedCall:5,StatefulPartitionedCall:7,StatefulPartitionedCall:1,StatefulPartitionedCall:2,StatefulPartitionedCall:4,StatefulPartitionedCall:3""}, tf_saved_model.exported_names = [""serving_default""]} {
  ^
ssd-func.mlir:340:3: error: Emitting more detail about one op that failed to legalize...
  func @serving_default(%arg0: tensor<1x?x?x3xui8> {tf_saved_model.index_path = [""input_tensor""]}, %arg1: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/beta""}, %arg2: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/gamma""}, %arg3: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/moving_mean""}, %arg4: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/moving_variance""}, %arg5: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/beta""}, %arg6: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/gamma""}, %arg7: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/moving_mean""}, %arg8: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/moving_variance""}, %arg9: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/beta""}, %arg10: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/gamma""}, %arg11: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/moving_mean""}, %arg12: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/moving_variance""}, %arg13: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/beta""}, %arg14: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/gamma""}, %arg15: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/moving_mean""}, %arg16: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/moving_variance""}, %arg17: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/beta""}, %arg18: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/gamma""}, %arg19: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/moving_mean""}, %arg20: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/moving_variance""}, %arg21: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/kernel""}, %arg22: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/beta""}, %arg23: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/gamma""}, %arg24: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/moving_mean""}, %arg25: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/moving_variance""}, %arg26: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/beta""}, %arg27: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/gamma""}, %arg28: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/moving_mean""}, %arg29: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/moving_variance""}, %arg30: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/beta""}, %arg31: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/gamma""}, %arg32: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/moving_mean""}, %arg33: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/moving_variance""}, %arg34: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/beta""}, %arg35: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/gamma""}, %arg36: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/moving_mean""}, %arg37: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/moving_variance""}, %arg38: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/beta""}, %arg39: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/gamma""}, %arg40: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/moving_mean""}, %arg41: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/moving_variance""}, %arg42: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/kernel""}, %arg43: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/beta""}, %arg44: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/gamma""}, %arg45: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/moving_mean""}, %arg46: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/moving_variance""}, %arg47: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/beta""}, %arg48: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/gamma""}, %arg49: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/moving_mean""}, %arg50: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/moving_variance""}, %arg51: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/beta""}, %arg52: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/gamma""}, %arg53: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/moving_mean""}, %arg54: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/moving_variance""}, %arg55: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/beta""}, %arg56: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/gamma""}, %arg57: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/moving_mean""}, %arg58: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/moving_variance""}, %arg59: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/beta""}, %arg60: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/gamma""}, %arg61: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/moving_mean""}, %arg62: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/moving_variance""}, %arg63: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/kernel""}, %arg64: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/beta""}, %arg65: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/gamma""}, %arg66: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/moving_mean""}, %arg67: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/moving_variance""}, %arg68: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/beta""}, %arg69: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/gamma""}, %arg70: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/moving_mean""}, %arg71: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/moving_variance""}, %arg72: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/beta""}, %arg73: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/gamma""}, %arg74: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/moving_mean""}, %arg75: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/moving_variance""}, %arg76: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/beta""}, %arg77: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/gamma""}, %arg78: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/moving_mean""}, %arg79: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/moving_variance""}, %arg80: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/beta""}, %arg81: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/gamma""}, %arg82: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/moving_mean""}, %arg83: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/moving_variance""}, %arg84: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/kernel""}, %arg85: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/beta""}, %arg86: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/gamma""}, %arg87: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/moving_mean""}, %arg88: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/moving_variance""}, %arg89: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/beta""}, %arg90: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/gamma""}, %arg91: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/moving_mean""}, %arg92: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/moving_variance""}, %arg93: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/beta""}, %arg94: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/gamma""}, %arg95: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/moving_mean""}, %arg96: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/moving_variance""}, %arg97: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/beta""}, %arg98: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/gamma""}, %arg99: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/moving_mean""}, %arg100: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/moving_variance""}, %arg101: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/beta""}, %arg102: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/gamma""}, %arg103: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/moving_mean""}, %arg104: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/moving_variance""}, %arg105: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/kernel""}, %arg106: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/beta""}, %arg107: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/gamma""}, %arg108: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/moving_mean""}, %arg109: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/moving_variance""}, %arg110: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/beta""}, %arg111: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/gamma""}, %arg112: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/moving_mean""}, %arg113: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/moving_variance""}, %arg114: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/beta""}, %arg115: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/gamma""}, %arg116: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/moving_mean""}, %arg117: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/moving_variance""}, %arg118: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/beta""}, %arg119: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/gamma""}, %arg120: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/moving_mean""}, %arg121: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/moving_variance""}, %arg122: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/beta""}, %arg123: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/gamma""}, %arg124: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/moving_mean""}, %arg125: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/moving_variance""}, %arg126: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/kernel""}, %arg127: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/beta""}, %arg128: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/gamma""}, %arg129: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/moving_mean""}, %arg130: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/moving_variance""}, %arg131: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/beta""}, %arg132: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/gamma""}, %arg133: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/moving_mean""}, %arg134: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/moving_variance""}, %arg135: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/beta""}, %arg136: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/gamma""}, %arg137: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/moving_mean""}, %arg138: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/moving_variance""}, %arg139: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/beta""}, %arg140: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/gamma""}, %arg141: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/moving_mean""}, %arg142: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/moving_variance""}, %arg143: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/beta""}, %arg144: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/gamma""}, %arg145: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/moving_mean""}, %arg146: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/moving_variance""}, %arg147: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/kernel""}, %arg148: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/beta""}, %arg149: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/gamma""}, %arg150: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/moving_mean""}, %arg151: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/moving_variance""}, %arg152: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/beta""}, %arg153: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/gamma""}, %arg154: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/moving_mean""}, %arg155: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/moving_variance""}, %arg156: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/beta""}, %arg157: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/gamma""}, %arg158: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/moving_mean""}, %arg159: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/moving_variance""}, %arg160: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/beta""}, %arg161: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/gamma""}, %arg162: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/moving_mean""}, %arg163: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/moving_variance""}, %arg164: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/beta""}, %arg165: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/gamma""}, %arg166: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/moving_mean""}, %arg167: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/moving_variance""}, %arg168: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/kernel""}, %arg169: tensor<!tf_type.resource<tensor<24xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/bias""}, %arg170: tensor<!tf_type.resource<tensor<3x3x256x24xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/kernel""}, %arg171: tensor<!tf_type.resource<tensor<546xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/bias""}, %arg172: tensor<!tf_type.resource<tensor<3x3x256x546xf32>>> {tf_saved_model.bound_input = @""WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/kernel""}, %arg173: tensor<!tf_type.resource<tensor<3x3x3x32xf32>>> {tf_saved_model.bound_input = @""conv1/kernel""}, %arg174: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv1_bn/beta""}, %arg175: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv1_bn/gamma""}, %arg176: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv1_bn/moving_mean""}, %arg177: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv1_bn/moving_variance""}, %arg178: tensor<!tf_type.resource<tensor<3x3x32x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_1/depthwise_kernel""}, %arg179: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_10/depthwise_kernel""}, %arg180: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_10_bn/beta""}, %arg181: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_10_bn/gamma""}, %arg182: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_10_bn/moving_mean""}, %arg183: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_10_bn/moving_variance""}, %arg184: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_11/depthwise_kernel""}, %arg185: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_11_bn/beta""}, %arg186: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_11_bn/gamma""}, %arg187: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_11_bn/moving_mean""}, %arg188: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_11_bn/moving_variance""}, %arg189: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_12/depthwise_kernel""}, %arg190: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_12_bn/beta""}, %arg191: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_12_bn/gamma""}, %arg192: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_12_bn/moving_mean""}, %arg193: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_12_bn/moving_variance""}, %arg194: tensor<!tf_type.resource<tensor<3x3x1024x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_13/depthwise_kernel""}, %arg195: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_dw_13_bn/beta""}, %arg196: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_dw_13_bn/gamma""}, %arg197: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_dw_13_bn/moving_mean""}, %arg198: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_dw_13_bn/moving_variance""}, %arg199: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv_dw_1_bn/beta""}, %arg200: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv_dw_1_bn/gamma""}, %arg201: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv_dw_1_bn/moving_mean""}, %arg202: tensor<!tf_type.resource<tensor<32xf32>>> {tf_saved_model.bound_input = @""conv_dw_1_bn/moving_variance""}, %arg203: tensor<!tf_type.resource<tensor<3x3x64x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_2/depthwise_kernel""}, %arg204: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_dw_2_bn/beta""}, %arg205: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_dw_2_bn/gamma""}, %arg206: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_dw_2_bn/moving_mean""}, %arg207: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_dw_2_bn/moving_variance""}, %arg208: tensor<!tf_type.resource<tensor<3x3x128x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_3/depthwise_kernel""}, %arg209: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_3_bn/beta""}, %arg210: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_3_bn/gamma""}, %arg211: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_3_bn/moving_mean""}, %arg212: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_3_bn/moving_variance""}, %arg213: tensor<!tf_type.resource<tensor<3x3x128x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_4/depthwise_kernel""}, %arg214: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_4_bn/beta""}, %arg215: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_4_bn/gamma""}, %arg216: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_4_bn/moving_mean""}, %arg217: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_dw_4_bn/moving_variance""}, %arg218: tensor<!tf_type.resource<tensor<3x3x256x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_5/depthwise_kernel""}, %arg219: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_5_bn/beta""}, %arg220: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_5_bn/gamma""}, %arg221: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_5_bn/moving_mean""}, %arg222: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_5_bn/moving_variance""}, %arg223: tensor<!tf_type.resource<tensor<3x3x256x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_6/depthwise_kernel""}, %arg224: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_6_bn/beta""}, %arg225: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_6_bn/gamma""}, %arg226: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_6_bn/moving_mean""}, %arg227: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_dw_6_bn/moving_variance""}, %arg228: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_7/depthwise_kernel""}, %arg229: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_7_bn/beta""}, %arg230: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_7_bn/gamma""}, %arg231: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_7_bn/moving_mean""}, %arg232: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_7_bn/moving_variance""}, %arg233: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_8/depthwise_kernel""}, %arg234: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_8_bn/beta""}, %arg235: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_8_bn/gamma""}, %arg236: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_8_bn/moving_mean""}, %arg237: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_8_bn/moving_variance""}, %arg238: tensor<!tf_type.resource<tensor<3x3x512x1xf32>>> {tf_saved_model.bound_input = @""conv_dw_9/depthwise_kernel""}, %arg239: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_9_bn/beta""}, %arg240: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_9_bn/gamma""}, %arg241: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_9_bn/moving_mean""}, %arg242: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_dw_9_bn/moving_variance""}, %arg243: tensor<!tf_type.resource<tensor<1x1x32x64xf32>>> {tf_saved_model.bound_input = @""conv_pw_1/kernel""}, %arg244: tensor<!tf_type.resource<tensor<1x1x512x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_10/kernel""}, %arg245: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_10_bn/beta""}, %arg246: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_10_bn/gamma""}, %arg247: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_10_bn/moving_mean""}, %arg248: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_10_bn/moving_variance""}, %arg249: tensor<!tf_type.resource<tensor<1x1x512x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_11/kernel""}, %arg250: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_11_bn/beta""}, %arg251: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_11_bn/gamma""}, %arg252: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_11_bn/moving_mean""}, %arg253: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_11_bn/moving_variance""}, %arg254: tensor<!tf_type.resource<tensor<1x1x512x1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_12/kernel""}, %arg255: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_12_bn/beta""}, %arg256: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_12_bn/gamma""}, %arg257: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_12_bn/moving_mean""}, %arg258: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_12_bn/moving_variance""}, %arg259: tensor<!tf_type.resource<tensor<1x1x1024x1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_13/kernel""}, %arg260: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_13_bn/beta""}, %arg261: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_13_bn/gamma""}, %arg262: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_13_bn/moving_mean""}, %arg263: tensor<!tf_type.resource<tensor<1024xf32>>> {tf_saved_model.bound_input = @""conv_pw_13_bn/moving_variance""}, %arg264: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_pw_1_bn/beta""}, %arg265: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_pw_1_bn/gamma""}, %arg266: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_pw_1_bn/moving_mean""}, %arg267: tensor<!tf_type.resource<tensor<64xf32>>> {tf_saved_model.bound_input = @""conv_pw_1_bn/moving_variance""}, %arg268: tensor<!tf_type.resource<tensor<1x1x64x128xf32>>> {tf_saved_model.bound_input = @""conv_pw_2/kernel""}, %arg269: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_2_bn/beta""}, %arg270: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_2_bn/gamma""}, %arg271: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_2_bn/moving_mean""}, %arg272: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_2_bn/moving_variance""}, %arg273: tensor<!tf_type.resource<tensor<1x1x128x128xf32>>> {tf_saved_model.bound_input = @""conv_pw_3/kernel""}, %arg274: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_3_bn/beta""}, %arg275: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_3_bn/gamma""}, %arg276: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_3_bn/moving_mean""}, %arg277: tensor<!tf_type.resource<tensor<128xf32>>> {tf_saved_model.bound_input = @""conv_pw_3_bn/moving_variance""}, %arg278: tensor<!tf_type.resource<tensor<1x1x128x256xf32>>> {tf_saved_model.bound_input = @""conv_pw_4/kernel""}, %arg279: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_4_bn/beta""}, %arg280: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_4_bn/gamma""}, %arg281: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_4_bn/moving_mean""}, %arg282: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_4_bn/moving_variance""}, %arg283: tensor<!tf_type.resource<tensor<1x1x256x256xf32>>> {tf_saved_model.bound_input = @""conv_pw_5/kernel""}, %arg284: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_5_bn/beta""}, %arg285: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_5_bn/gamma""}, %arg286: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_5_bn/moving_mean""}, %arg287: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""conv_pw_5_bn/moving_variance""}, %arg288: tensor<!tf_type.resource<tensor<1x1x256x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_6/kernel""}, %arg289: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_6_bn/beta""}, %arg290: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_6_bn/gamma""}, %arg291: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_6_bn/moving_mean""}, %arg292: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_6_bn/moving_variance""}, %arg293: tensor<!tf_type.resource<tensor<1x1x512x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_7/kernel""}, %arg294: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_7_bn/beta""}, %arg295: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_7_bn/gamma""}, %arg296: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_7_bn/moving_mean""}, %arg297: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_7_bn/moving_variance""}, %arg298: tensor<!tf_type.resource<tensor<1x1x512x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_8/kernel""}, %arg299: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_8_bn/beta""}, %arg300: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_8_bn/gamma""}, %arg301: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_8_bn/moving_mean""}, %arg302: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_8_bn/moving_variance""}, %arg303: tensor<!tf_type.resource<tensor<1x1x512x512xf32>>> {tf_saved_model.bound_input = @""conv_pw_9/kernel""}, %arg304: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_9_bn/beta""}, %arg305: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_9_bn/gamma""}, %arg306: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_9_bn/moving_mean""}, %arg307: tensor<!tf_type.resource<tensor<512xf32>>> {tf_saved_model.bound_input = @""conv_pw_9_bn/moving_variance""}, %arg308: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_1/bias""}, %arg309: tensor<!tf_type.resource<tensor<1x1x256x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_1/kernel""}, %arg310: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_2/bias""}, %arg311: tensor<!tf_type.resource<tensor<1x1x512x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_2/kernel""}, %arg312: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_3/bias""}, %arg313: tensor<!tf_type.resource<tensor<1x1x1024x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/projection_3/kernel""}, %arg314: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_1_batchnorm/beta""}, %arg315: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_1_batchnorm/gamma""}, %arg316: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_1_batchnorm/moving_mean""}, %arg317: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_1_batchnorm/moving_variance""}, %arg318: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_1_conv/kernel""}, %arg319: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_2_batchnorm/beta""}, %arg320: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_2_batchnorm/gamma""}, %arg321: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_2_batchnorm/moving_mean""}, %arg322: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_2_batchnorm/moving_variance""}, %arg323: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/FeatureMaps/top_down/smoothing_2_conv/kernel""}, %arg324: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_14_batchnorm/beta""}, %arg325: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_14_batchnorm/gamma""}, %arg326: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_14_batchnorm/moving_mean""}, %arg327: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_14_batchnorm/moving_variance""}, %arg328: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_14_conv/kernel""}, %arg329: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_15_batchnorm/beta""}, %arg330: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_15_batchnorm/gamma""}, %arg331: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_15_batchnorm/moving_mean""}, %arg332: tensor<!tf_type.resource<tensor<256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_15_batchnorm/moving_variance""}, %arg333: tensor<!tf_type.resource<tensor<3x3x256x256xf32>>> {tf_saved_model.bound_input = @""ssd_mobile_net_v1fpn_keras_feature_extractor/bottom_up_Conv2d_15_conv/kernel""}) -> (tensor<*xf32> {tf_saved_model.index_path = [""raw_detection_boxes""]}, tensor<*xf32> {tf_saved_model.index_path = [""detection_anchor_indices""]}, tensor<*xf32> {tf_saved_model.index_path = [""num_detections""]}, tensor<*xf32> {tf_saved_model.index_path = [""raw_detection_scores""]}, tensor<*xf32> {tf_saved_model.index_path = [""detection_boxes""]}, tensor<*xf32> {tf_saved_model.index_path = [""detection_classes""]}, tensor<*xf32> {tf_saved_model.index_path = [""detection_scores""]}, tensor<*xf32> {tf_saved_model.index_path = [""detection_multiclass_scores""]}) attributes {tf.entry_function = {control_outputs = """", inputs = ""serving_default_input_tensor:0"", outputs = ""StatefulPartitionedCall:6,StatefulPartitionedCall:0,StatefulPartitionedCall:5,StatefulPartitionedCall:7,StatefulPartitionedCall:1,StatefulPartitionedCall:2,StatefulPartitionedCall:4,StatefulPartitionedCall:3""}, tf_saved_model.exported_names = [""serving_default""]} {
  ^
ssd-func.mlir:6156:74: error: 'tf.NonMaxSuppressionV5' op is not legalizable
    %selected_indices_4039, %selected_scores_4040, %valid_outputs_4041 = ""tf.NonMaxSuppressionV5""(%1931, %2111, %773, %cst_1735, %cst_1736, %cst_1737) {device = """", pad_to_max_output_size = false} : (tensor<*xf32>, tensor<*xf32>, tensor<*xi32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<*xi32>, tensor<*xf32>, tensor<*xi32>)
                                                                         ^
ssd-func.mlir:356:12: note: called from
    %0:8 = ""tf.StatefulPartitionedCall""(%arg0, %arg173, %arg175, %arg174, %arg176, %arg177, %arg178, %arg200, %arg199, %arg201, %arg202, %arg243, %arg265, %arg264, %arg266, %arg267, %arg203, %arg205, %arg204, %arg206, %arg207, %arg268, %arg270, %arg269, %arg271, %arg272, %arg208, %arg210, %arg209, %arg211, %arg212, %arg273, %arg275, %arg274, %arg276, %arg277, %arg213, %arg215, %arg214, %arg216, %arg217, %arg278, %arg280, %arg279, %arg281, %arg282, %arg218, %arg220, %arg219, %arg221, %arg222, %arg283, %arg285, %arg284, %arg286, %arg287, %arg223, %arg225, %arg224, %arg226, %arg227, %arg288, %arg290, %arg289, %arg291, %arg292, %arg228, %arg230, %arg229, %arg231, %arg232, %arg293, %arg295, %arg294, %arg296, %arg297, %arg233, %arg235, %arg234, %arg236, %arg237, %arg298, %arg300, %arg299, %arg301, %arg302, %arg238, %arg240, %arg239, %arg241, %arg242, %arg303, %arg305, %arg304, %arg306, %arg307, %arg179, %arg181, %arg180, %arg182, %arg183, %arg244, %arg246, %arg245, %arg247, %arg248, %arg184, %arg186, %arg185, %arg187, %arg188, %arg249, %arg251, %arg250, %arg252, %arg253, %arg189, %arg191, %arg190, %arg192, %arg193, %arg254, %arg256, %arg255, %arg257, %arg258, %arg194, %arg196, %arg195, %arg197, %arg198, %arg259, %arg261, %arg260, %arg262, %arg263, %arg313, %arg312, %arg311, %arg310, %arg323, %arg320, %arg319, %arg321, %arg322, %arg309, %arg308, %arg318, %arg315, %arg314, %arg316, %arg317, %arg328, %arg325, %arg324, %arg326, %arg327, %arg333, %arg330, %arg329, %arg331, %arg332, %cst, %cst_0, %cst_6, %cst_7, %cst_8, %cst_9, %cst_10, %cst_11, %cst_12, %cst_13, %cst_1, %cst_2, %cst_3, %cst_4, %cst_5, %arg21, %arg2, %arg1, %arg3, %arg4, %arg42, %arg23, %arg22, %arg24, %arg25, %arg63, %arg44, %arg43, %arg45, %arg46, %arg84, %arg65, %arg64, %arg66, %arg67, %arg170, %arg169, %arg105, %arg86, %arg85, %arg87, %arg88, %arg126, %arg107, %arg106, %arg108, %arg109, %arg147, %arg128, %arg127, %arg129, %arg130, %arg168, %arg149, %arg148, %arg150, %arg151, %arg172, %arg171, %arg6, %arg5, %arg7, %arg8, %arg27, %arg26, %arg28, %arg29, %arg48, %arg47, %arg49, %arg50, %arg69, %arg68, %arg70, %arg71, %arg90, %arg89, %arg91, %arg92, %arg111, %arg110, %arg112, %arg113, %arg132, %arg131, %arg133, %arg134, %arg153, %arg152, %arg154, %arg155, %arg10, %arg9, %arg11, %arg12, %arg31, %arg30, %arg32, %arg33, %arg52, %arg51, %arg53, %arg54, %arg73, %arg72, %arg74, %arg75, %arg94, %arg93, %arg95, %arg96, %arg115, %arg114, %arg116, %arg117, %arg136, %arg135, %arg137, %arg138, %arg157, %arg156, %arg158, %arg159, %arg14, %arg13, %arg15, %arg16, %arg35, %arg34, %arg36, %arg37, %arg56, %arg55, %arg57, %arg58, %arg77, %arg76, %arg78, %arg79, %arg98, %arg97, %arg99, %arg100, %arg119, %arg118, %arg120, %arg121, %arg140, %arg139, %arg141, %arg142, %arg161, %arg160, %arg162, %arg163, %arg18, %arg17, %arg19, %arg20, %arg39, %arg38, %arg40, %arg41, %arg60, %arg59, %arg61, %arg62, %arg81, %arg80, %arg82, %arg83, %arg102, %arg101, %arg103, %arg104, %arg123, %arg122, %arg124, %arg125, %arg144, %arg143, %arg145, %arg146, %arg165, %arg164, %arg166, %arg167) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348], config = """", config_proto = ""\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00"", device = """", executor_type = """", f = @__inference_signature_wrapper_236900} : (tensor<1x?x?x3xui8>, tensor<!tf_type.resource<tensor<3x3x3x32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<3x3x32x1xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<32xf32>>>, tensor<!tf_type.resource<tensor<1x1x32x64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<3x3x64x1xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<64xf32>>>, tensor<!tf_type.resource<tensor<1x1x64x128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<3x3x128x1xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<1x1x128x128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<3x3x128x1xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<128xf32>>>, tensor<!tf_type.resource<tensor<1x1x128x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x1xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x1xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x256x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<3x3x512x1xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<512xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<3x3x1024x1xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1x1x1024x1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<!tf_type.resource<tensor<1x1x1024x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x512x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<1x1x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x24xf32>>>, tensor<!tf_type.resource<tensor<24xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<3x3x256x546xf32>>>, tensor<!tf_type.resource<tensor<546xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>, tensor<!tf_type.resource<tensor<256xf32>>>) -> (tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>, tensor<*xf32>)
           ^
ssd-func.mlir:6156:74: note: see current operation: %986:3 = ""tf.NonMaxSuppressionV5""(%755, %985, %34, %40, %39, %3) {device = """", pad_to_max_output_size = false} : (tensor<51150x4xf32>, tensor<51150xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
    %selected_indices_4039, %selected_scores_4040, %valid_outputs_4041 = ""tf.NonMaxSuppressionV5""(%1931, %2111, %773, %cst_1735, %cst_1736, %cst_1737) {device = """", pad_to_max_output_size = false} : (tensor<*xf32>, tensor<*xf32>, tensor<*xi32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<*xi32>, tensor<*xf32>, tensor<*xi32>)
                                                                         ^

```

 
no longer exists on the new branch."
53761,print_selective_registration_header: No module named 'tensorflow.python.platform' in Offical Lastest Docker,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
- TensorFlow installation (pip package or built from source): source 
- TensorFlow library (version, if pip package or github SHA, if built from source): source

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

Hi, as this issue #53732  mention, i meet this issue again even in lastest offical docker.
Thanks!

here is the code to reproduce the errror.
```
# docker pull
docker pull tensorflow/tensorflow:latest-devel

# in the docker
cd <tensorflow-src-path>
LINE_IDX_TO_ADD=$(grep -n '"":tflite_version_script.lds"",' tensorflow/lite/BUILD | awk -F ':' '{print $1}')
sed -i ""${LINE_IDX_TO_ADD}s/^/\t\""\/\/tensorflow\/lite\/delegates\/flex:delegate\"",\n/"" ./tensorflow/lite/BUILD

bazel build --define=no_tensorflow_py_deps=true tensorflow/python/tools:print_selective_registration_header
bazel-bin/tensorflow/python/tools/print_selective_registration_header
```
here is the config.
```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""
build --python_path=""/usr/bin/python3""
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-Wno-sign-compare
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only
root@f912083c6c87:/tensorflow_src# LINE_IDX_TO_ADD=$(grep -n '"":tflite_version_script.lds"",' tensorflow/lite/BUILD | awk -F ':' '{print $1}')
root@f912083c6c87:/tensorflow_src# sed -i ""${LINE_IDX_TO_ADD}s/^/\t\""\/\/tensorflow\/lite\/delegates\/flex:delegate\"",\n/"" ./tensorflow/lite/BUILD
root@f912083c6c87:/tensorflow_src# tail ./tensorflow/lite/BUILD
    deps = [
        "":framework"",
        "":tflite_exported_symbols.lds"",
        ""//tensorflow/lite/delegates/flex:delegate"",
        "":tflite_version_script.lds"",
        ""//tensorflow/lite/kernels:builtin_ops_all_linked"",
    ],
)

tflite_portable_test_suite()
```

here is the log.
```
root@f912083c6c87:/tensorflow_src# bazel clean
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=186
INFO: Reading rc options for 'clean' from /tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'clean' from /tensorflow_src/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'clean' from /tensorflow_src/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'clean' from /tensorflow_src/.bazelrc:
  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
root@f912083c6c87:/tensorflow_src# bazel build --define=no_tensorflow_py_deps=true tensorflow/python/tools:print_selective_registration_header
root@f912083c6c87:/tensorflow_src# bazel build --define=no_tensorflow_py_deps=true tensorflow/python/tools:print_selective_registration_header
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=186
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/a2385f745b4b64729008fd892cfb4763dadf535b.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from http://mirror.tensorflow.org/github.com/bazelbuild/bazel-toolchains/archive/dfc67056200b674accd08d8f9a21e328098c07e2.tar.gz failed: class java.io.IOException Proxy address 10.71.254.21:8888 is not a valid URL
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /tensorflow_src/WORKSPACE:23:14: in <toplevel>
  /tensorflow_src/tensorflow/workspace0.bzl:108:34: in workspace
  /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow/python/tools:print_selective_registration_header (29 packages loaded, 161 targets configured).
INFO: Found 1 target...
Target //tensorflow/python/tools:print_selective_registration_header up-to-date:
  bazel-bin/tensorflow/python/tools/print_selective_registration_header
INFO: Elapsed time: 120.679s, Critical Path: 0.03s
INFO: 4 processes: 4 internal.
INFO: Build completed successfully, 4 total actions


root@f912083c6c87:/tensorflow_src# bazel-bin/tensorflow/python/tools/print_selective_registration_header
Traceback (most recent call last):
  File ""/tensorflow_src/bazel-bin/tensorflow/python/tools/print_selective_registration_header.runfiles/org_tensorflow/tensorflow/python/tools/print_selective_registration_header.py"", line 38, in <module>
    from tensorflow.python.tools import selective_registration_header_lib
ImportError: cannot import name 'selective_registration_header_lib' from 'tensorflow.python.tools' (/tensorflow_src/bazel-bin/tensorflow/python/tools/print_selective_registration_header.runfiles/org_tensorflow/tensorflow/python/tools/__init__.py)
```
"
53759,why does tensorflow use multi threads to launch gpu kernels?,"I noticed that when I launch kernel to the same GPU with multi threads, cudaLaunchKernel  api will get much slower. 
But seems it's the case in tensorflow. why not use a single thread to communicate with GPU?

Here is my test，when thread_num is larger, cuda api cost will be huge.
![image](https://user-images.githubusercontent.com/26128514/149446536-06d6117a-d348-4b0d-a2a8-8ebf81299c5a.png)

`#include <thread>
#include <vector>

using namespace std;

bool running = true;
const int thread_num = 20;
const int launch_num = 25;

__global__ void task_kernel(int x) {
    int ans = 1024;
    for(int i = 0; i < x; i++) {
        if(ans > 0) {
            ans *= (x + 2);
        }
    }
}

void task(int x) {
    while(running) {
        for(int i = 0; i < launch_num; i++) {
            task_kernel<<<1, 1>>>(x);
        }
        this_thread::sleep_for(1ms);
    }
}

void test() {
    vector<thread> threads;
    for(int i = 0; i < thread_num; i++) {
        thread t(task, i);
        threads.push_back(std::move(t));
    }
    this_thread::sleep_for(100ms);
    running = false;
    for(auto& t: threads) {
        t.join();
    }
}

int main() {
    cudaFree(0);
    test();
    return 0;
}`"
53758,movenet fp16 not executed by TFLITE NNAPI DELEGATE,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): aarch64 Android 11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.8
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I have issues while working with movenet tflite models on android 11 NNAPI 1.3
the movenet models used are sourced from tfhub:

https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4
https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4

the above two singlepose movenet lighting tflite models are float16 and INT8 respectively, and I was trying to perform benchmarking of the same using the prebuilt benchmark model for android_aarch64 sourced from tflite website (v 2.8):
https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model

When I run lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite model on NNAPI using the following command:

> ./benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite --use_nnapi=1 --nnapi_accelerator_name=nnapi-reference
STARTING!
Log parameter values verbosely: [0]
Graph: [lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite]
Use NNAPI: [1]
NNAPI accelerator name: [nnapi-reference]
NNAPI accelerators available: [nnapi-reference]
Loaded model lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
NNAPI delegate created.
INFO: Replacing 141 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 15 partitions.
Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 8 delegate kernels.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
The input model file size (MB): 2.89484
Initialized session in 59.758ms.

 the NNAPI delegate is created and the model is partially executed by NNAPI CPU.

whereas when I run float16 lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite model as follows:

> $ ./benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite --use_nnapi=1 --nnapi_accelerator_name=nnapi-reference
STARTING!
Log parameter values verbosely: [0]
Graph: [lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite]
Use NNAPI: [1]
NNAPI accelerator name: [nnapi-reference]
NNAPI accelerators available: [nnapi-reference]
Loaded model lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
NNAPI delegate created.
Though NNAPI delegate is explicitly applied, the model graph will not be executed by the delegate.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Replacing 269 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 13 partitions.
The input model file size (MB): 4.75851
Initialized session in 109.192ms.

the float16 model is not at all executed by the NNAPI delegate even though I am explicitly applying NNAPI CPU

I face the same issue if i run this model https://tfhub.dev/sayakpaul/lite-model/mobilenetv2-coco/fp16/1?lite-format=tflite, i see the same message as before for fp16 models:

> Graph: [lite-model_mobilenetv2-coco_fp16_1.tflite]
Use NNAPI: [1]
NNAPI accelerator name: [nnapi-reference]
NNAPI accelerators available: [nnapi-reference]
Loaded model lite-model_mobilenetv2-coco_fp16_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
NNAPI delegate created.
Though NNAPI delegate is explicitly applied, the model graph will not be executed by the delegate.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Replacing 211 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 43 partitions.
The input model file size (MB): 4.2551
Initialized session in 133.522ms.

I even used the --disable_nnapi_cpu=0 argument, and yet still got the same result with nightly
y
is it the same for every fp16 model that it won't be supported on NNAPI?



**Describe the expected behavior**
the model should be executed by NNAPI CPU

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53757,Issue with AdamW optimizer in tensorflow 1.15.5,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.5
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda_11.5.r11.5,  nvidia docker image
- GPU model and memory: Nvidia 2080, 8 GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

 Training object detection model with ""SSD_INCEPTION_V2"" as backbone. Updated AdamW optimizer from contrib module using following code in optimizer_buider.py.
    """"""
    config = optimizer_config.adamw_optimizer
    learning_rate = _create_learning_rate(config.learning_rate) (# learning rate type: exponential_decay_learning_rate)
    summary_vars.append(learning_rate)
    optimizer = tf.contrib.opt.AdamWOptimizer(weight_decay=4e-05, learning_rate=learning_rate)
    """"""

   PROBLEM: 
  When I use Adam optimizer, I can see progress in learning from coco metrics and it reaches 0.16 mAP for ""0.50 IOU area ALL"" 
  after 8 epochs and both training loss and validation loss decrease with small difference in values (good generalization). but when I changed to AdamW optimizer using described code, coco metric (almost 0.0) as well as loss (which is stationary) not changing at all (no generalization). 

**Describe the expected behavior**
  As per paper (https://arxiv.org/abs/1711.05101) and also from documentation, it suppose to train better and converge faster. 
 Am i missing here something? Let me know if you need any additional information.


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53756,Exploding gradient after network has converged,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Implemented a custom distribution. It is in the jupyter notebook provided at the end.
- OS Platform and Distribution: 
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
- TensorFlow installed from (source or binary): source
- TensorFlow version :  2.3.1
- Python version:  Python 3.8.5
- GCC/Compiler version : gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
- CUDA/cuDNN version:  CUDA Version: 11.0
- GPU model and memory:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM3...  Off  | 00000000:34:00.0 Off |                    0 |
| N/A   27C    P0    66W / 350W |   3358MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM3...  Off  | 00000000:36:00.0 Off |                    0 |
| N/A   26C    P0    50W / 350W |      7MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM3...  Off  | 00000000:39:00.0 Off |                    0 |
| N/A   31C    P0    49W / 350W |      7MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM3...  Off  | 00000000:3B:00.0 Off |                    0 |
| N/A   64C    P0   273W / 350W |  25960MiB / 32510MiB |    100%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-SXM3...  Off  | 00000000:57:00.0 Off |                    0 |
| N/A   23C    P0    48W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-SXM3...  Off  | 00000000:59:00.0 Off |                    0 |
| N/A   29C    P0    49W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-SXM3...  Off  | 00000000:5C:00.0 Off |                    0 |
| N/A   26C    P0    49W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-SXM3...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   28C    P0    48W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   8  Tesla V100-SXM3...  Off  | 00000000:B7:00.0 Off |                    0 |
| N/A   24C    P0    48W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   9  Tesla V100-SXM3...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   25C    P0    49W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|  10  Tesla V100-SXM3...  Off  | 00000000:BC:00.0 Off |                    0 |
| N/A   30C    P0    49W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|  11  Tesla V100-SXM3...  Off  | 00000000:BE:00.0 Off |                    0 |
| N/A   30C    P0    46W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|  12  Tesla V100-SXM3...  Off  | 00000000:E0:00.0 Off |                    0 |
| N/A   25C    P0    47W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|  13  Tesla V100-SXM3...  Off  | 00000000:E2:00.0 Off |                    0 |
| N/A   25C    P0    47W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|  14  Tesla V100-SXM3...  Off  | 00000000:E5:00.0 Off |                    0 |
| N/A   31C    P0    49W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|  15  Tesla V100-SXM3...  Off  | 00000000:E7:00.0 Off |                    0 |
| N/A   29C    P0    50W / 350W |      5MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

**Describe the current behavior**
The multinomial distribution has multiple variables where each variable takes a value that equals the probability of an unique action.
When the network converges, as some of the probabilities are very small, the resulting gradient becomes NaN.

**Describe the expected behavior**
It should provide the actual result instead of NaN.

**Standalone code to reproduce the issue**
[The jupyter notebook can produce the situation where the gradient becomes NaN](https://colab.research.google.com/drive/1IVb0s5Tt2BmibgKqLVojj3SoCi2TJctL?usp=sharing)."
53755,Tensorflow 2.5  no work with RTX3070,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu-18.04 from nvidia/cuda:11.2.2-cudnn8-devel-ubuntu18.04 dockers images
- TensorFlow installed from (source or binary):tensorflow-gpu==2.5
- TensorFlow version (use command below):2.5
- Python version: 3.6.9

- CUDA/cuDNN version:   11.2.2/8.1.1.33
- GPU model and memory: GeForce RTX 3070 7982MiB 
- Driver Version: 460.73.01    CUDA Version: 11.2

Describe the current behavior


this model (ssdlite_mobilenet_v2_coco_2018_05_09) was trained and worked perfectly on the pascal architecture (P4000, P2200,620) , tensorfflow 2.1.0

Now,  this model no works with  ampere architecture.  
do I have to train another model( more  recent )  to make it work ( objection detection) on this card? I admit being disappointed

2022-01-13 15:06:05.231427: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
DEBUG  | tensorflow           | 13.01 15:06:05.592 | MainThread | Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
DEBUG  | h5py._conv           | 13.01 15:06:05.756 | MainThread | Creating converter from 7 to 5
DEBUG  | h5py._conv           | 13.01 15:06:05.757 | MainThread | Creating converter from 5 to 7
DEBUG  | h5py._conv           | 13.01 15:06:05.757 | MainThread | Creating converter from 7 to 5
DEBUG  | h5py._conv           | 13.01 15:06:05.757 | MainThread | Creating converter from 5 to 7
2022-01-13 15:06:06.913294: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
DEBUG  | tensorflow           | 13.01 15:06:07.238 | MainThread | Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
DEBUG  | h5py._conv           | 13.01 15:06:07.384 | MainThread | Creating converter from 7 to 5
DEBUG  | h5py._conv           | 13.01 15:06:07.384 | MainThread | Creating converter from 5 to 7
DEBUG  | h5py._conv           | 13.01 15:06:07.384 | MainThread | Creating converter from 7 to 5
DEBUG  | h5py._conv           | 13.01 15:06:07.384 | MainThread | Creating converter from 5 to 7

2022-01-13 15:06:07.604639: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-01-13 15:06:07.605714: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2022-01-13 15:06:07.605794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.606156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2022-01-13 15:06:07.606178: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-01-13 15:06:07.608513: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-01-13 15:06:07.608551: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-01-13 15:06:07.609137: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2022-01-13 15:06:07.609413: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2022-01-13 15:06:07.610018: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2022-01-13 15:06:07.610494: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2022-01-13 15:06:07.610601: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-01-13 15:06:07.610717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.611128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.611518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2022-01-13 15:06:07.611541: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-01-13 15:06:07.885629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-01-13 15:06:07.885646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2022-01-13 15:06:07.885649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2022-01-13 15:06:07.885728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.886190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.886622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.887028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:0 with 6132 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)
2022-01-13 15:06:07.887347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.887722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2022-01-13 15:06:07.887751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.888140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.888499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2022-01-13 15:06:07.888510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-01-13 15:06:07.888514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2022-01-13 15:06:07.888517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2022-01-13 15:06:07.888554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.888883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.889206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:0 with 6132 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)

2022-01-13 15:06:07.902646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.902992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2022-01-13 15:06:07.903029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.903344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.903676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2022-01-13 15:06:07.903861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.904215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2022-01-13 15:06:07.904241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.904597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.904998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2022-01-13 15:06:07.905033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-01-13 15:06:07.905036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2022-01-13 15:06:07.905039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2022-01-13 15:06:07.905079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.905395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:07.905698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6132 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)
 | MainThread | Model (/home/workingsrc/model/tensorflow/1_12/frozen_inference_graph.pb) placed on GPU:0
2022-01-13 15:06:08.201813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:08.202197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2022-01-13 15:06:08.202233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:08.202661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:08.203071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2022-01-13 15:06:08.203089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-01-13 15:06:08.203093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2022-01-13 15:06:08.203096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2022-01-13 15:06:08.203139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:08.203500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-01-13 15:06:08.203909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6132 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)
2022-01-13 15:06:08.250684: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2899885000 Hz
2022-01-13 15:06:08.615925: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-01-13 15:06:08.974842: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8101
2022-01-13 15:06:09.421154: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-01-13 15:06:09.751559: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
"
53754,Master build fails on s390x arch due to FloatList struct changes in tpu/c_api_conversions.cc,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: Master
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 4.2.2- (@non-git)
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
Recently, master builds on `s390x` arch is failing due to following error:
```
tensorflow/stream_executor/tpu/c_api_conversions.cc:175:66:   required from here
tensorflow/stream_executor/tpu/c_api_conversions.cc:164:15: error: cannot convert 'float*' to 'float_t* {aka double*}' in assignment
     dst->heap = new Dst[dst->size];
     ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" build  --define=tensorflow_mkldnn_contraction_kernel=0 --define tflite_with_xnnpack=false //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
I believe the problem occurs at this line: https://github.com/tensorflow/tensorflow/blob/276e5c51c310983923b8078bcf6fcf78844c6211/tensorflow/stream_executor/tpu/c_api_decl.h#L197

I was able to successfully build by changing `float_t` occurrences in `FloatList` struct to `float`.  However, I am not sure if this change has a wider implication.

Any help appreciated.

Thanks."
53751,Keras Could not find matching function to load & run the model,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Windows 10
- TensorFlow installed from (source or binary): conda install tensorflow
- TensorFlow version:2.6.0  (CPU only)
- Python version: 3.88
- Installed using virtualenv? pip? conda?: conda




**Describe the problem**

I am try train a TF Keras model (summary below) :-

bert_en_uncased_L-12_H-768_A-12/4


**Provide the exact sequence of commands / steps that you executed before running into the problem**

I run model = build_model(bert_layer, max_len=160) but the model gives the error.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.



ValueError: in user code:

    C:\Users\h.swamy\Anaconda3\lib\site-packages\tensorflow_hub\keras_layer.py:237 call  *
        result = smart_cond.smart_cond(training,
    C:\Users\h.swamy\Anaconda3\lib\site-packages\tensorflow\python\saved_model\load.py:664 _call_attribute  **
        return instance.__call__(*args, **kwargs)
    C:\Users\h.swamy\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py:885 __call__
        result = self._call(*args, **kwds)
    C:\Users\h.swamy\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py:924 _call
        results = self._stateful_fn(*args, **kwds)
    C:\Users\h.swamy\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py:3038 __call__
        filtered_flat_args) = self._maybe_define_function(args, kwargs)
    C:\Users\h.swamy\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py:3463 _maybe_define_function
        graph_function = self._create_graph_function(args, kwargs)
    C:\Users\h.swamy\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py:3298 _create_graph_function
        func_graph_module.func_graph_from_py_func(
    C:\Users\h.swamy\Anaconda3\lib\site-packages\tensorflow\python\framework\func_graph.py:1007 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    C:\Users\h.swamy\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py:668 wrapped_fn
        out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    C:\Users\h.swamy\Anaconda3\lib\site-packages\tensorflow\python\saved_model\function_deserialization.py:288 restored_function_body
        raise ValueError(

    ValueError: Could not find matching function to call loaded from the SavedModel. Got:
      Positional arguments (3 total):
        * [<tf.Tensor 'inputs:0' shape=(None, 160) dtype=int32>, <tf.Tensor 'inputs_1:0' shape=(None, 160) dtype=int32>, <tf.Tensor 'inputs_2:0' shape=(None, 160) dtype=int32>]
        * False
        * None
      Keyword arguments: {}
    
    Expected these arguments to match one of the following 4 option(s):
    
    Option 1:
      Positional arguments (3 total):
        * {'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), 'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask')}
        * False
        * None
      Keyword arguments: {}
    
    Option 2:
      Positional arguments (3 total):
        * {'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_type_ids'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_word_ids'), 'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_mask')}
        * False
        * None
      Keyword arguments: {}
    
    Option 3:
      Positional arguments (3 total):
        * {'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_mask'), 'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_type_ids'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_word_ids')}
        * True
        * None
      Keyword arguments: {}
    
    Option 4:
      Positional arguments (3 total):
        * {'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), 'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids')}
        * True
        * None
      Keyword arguments: {}
"
53749,Convert tflite model with customerized QuantizeConfig for different layers.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source):pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): tf 2.5

### 2. Code

import tensorflow as tf
from tensorflow import keras
import tensorflow_model_optimization as tfmot
import numpy as np
import tempfile
from tensorflow.keras import layers


###################################################
LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer
MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer


class DefaultConv2DQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):
    # Configure how to quantize weights.
    def get_weights_and_quantizers(self, layer):
        return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]

    # Configure how to quantize activations.
    def get_activations_and_quantizers(self, layer):
        return [(layer.activation, MovingAverageQuantizer(num_bits=16, symmetric=False, narrow_range=False, per_axis=False))]

    def set_quantize_weights(self, layer, quantize_weights):
        # Add this line for each item returned in `get_weights_and_quantizers`
        # , in the same order
        layer.kernel = quantize_weights[0]

    def set_quantize_activations(self, layer, quantize_activations):
        # Add this line for each item returned in `get_activations_and_quantizers`
        # , in the same order.
        layer.activation = quantize_activations[0]

    # Configure how to quantize outputs (may be equivalent to activations).
    def get_output_quantizers(self, layer):
        return []

    def get_config(self):
        return {}

def apply_quantization_to_conv2d(layer):
    if isinstance(layer, tf.keras.layers.Conv2D):
        return   tfmot.quantization.keras.quantize_annotate_layer(layer, DefaultConv2DQuantizeConfig())
    return layer


if __name__ == ""__main__"":
    quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer
    quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model
    quantize_scope = tfmot.quantization.keras.quantize_scope

    input = keras.Input(shape=(28,28,1))
    x1 = keras.layers.Conv2D(filters=12, kernel_size=(3,3), activation='relu')(input)
    x2 = keras.layers.Conv2D(filters=12, kernel_size=(3,3), activation='relu')(x1)
    x3 = keras.layers.Conv2D(filters=12, kernel_size=(3,3), activation='relu')(x2)

    model = keras.Model(input, x3)
    model = tf.keras.models.clone_model(model, clone_function=apply_quantization_to_conv2d)
    with tfmot.quantization.keras.quantize_scope({'DefaultConv2DQuantizeConfig': DefaultConv2DQuantizeConfig}):
        model = tfmot.quantization.keras.quantize_apply(model)


    q_aware_model = model
    converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]

    quantized_tflite_model = converter.convert()

    #save tflite file
    with open(""testtf25.tflite"", 'wb') as f:
        f.write(quantized_tflite_model)

    interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)
    interpreter.allocate_tensors()



### 3. Failure after conversion

We're trying to quantization-aware train a model. But we're not sure how to set the QuantizeConfig with activation 16bits, weights 8bits for different layers. We try to quantize the convoluational layers as the above code. The above code can produce a tflite model, but it outputs an RuntimeError. 

""
  interpreter.allocate_tensors()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py"", line 259, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: tensorflow/lite/kernels/conv.cc:353 bias->type != kTfLiteInt64 (INT32 != INT64)Node number 1 (CONV_2D) failed to prepare.
""

We're not sure how to set the QuantizeConfig of different keras layers. Could you give us some reference or examples to quantization-aware training a model with different kinds of layers: convoluational layer, MaxPooling2D() layer, etc. 

"
53748,The custom op with nccl communication in MirroredStrategy cause hanging in the eager mode,"I write a custom op with nccl communication, and I run it under the `MirroredStrategy` in the eager mode. The expected behavior is to do nccl communication in such an op among all replicas.

As the nccl communication waits util it is aware of all peers, the custom op hangs before other op replicas run. However, the python GIL restrict other op replicas running parallelly, which cause the training step hanging in a deadlock ...

I need help to know whether my analysis above is correct or not about my unexpected hanging problem. And if it is correct, how can I fix it?

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6
- Python version: 3.9.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 11/cuDNN 8.1
- GPU model and memory: V100 with 32GB"
53747,F tensorflow/core/grappler/optimizers/constant_folding.cc:1172] Check failed: value->FromProto(raw_val),"Windows10/tf1.14.0
when i runing my project
""
Epoch 1/2
2022-01-13 10:53:49.459951: W tensorflow/core/framework/allocator.cc:107] Allocation of 19874709504 exceeds 10% of system memory.
2022-01-13 10:53:49.572648: W tensorflow/core/framework/allocator.cc:107] Allocation of 19874709504 exceeds 10% of system memory.
2022-01-13 10:53:49.611016: F tensorflow/core/grappler/optimizers/constant_folding.cc:1172] Check failed: value->FromProto(raw_val)

Process finished with exit code -1073740791 (0xC0000409)
""
what can i do ?"
53746,Problems with xla and dynamic LinearOperatorBlockDiag. Issues developing custom XLA op,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution:  Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): r2.7
- Python version: 3.6.10
- Bazel version (if compiling from source):  3.7.2
- GCC/Compiler version (if compiling from source): 7.3.1

**Describe the current behavior**
I have a 2D sparse square matrix, where the elements are organized in blocks around the diagonal, similar to [LinearOperatorBlockDiag](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorBlockDiag). 
At compilation time I know the total matrix size, but the size of each of its blocks its not known until execution time.
I need to multiply that matrix by a 1D vector to generate a 1D vector, and similar to [matvec](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorBlockDiag#matvec) method I would like to only compute the dense part of the matrix, i.e. block_0 x sub_vector_0, block_1 x sub_vector_1 ... block_n x sub_vector_n.
And I would like to have it working with XLA.

I have tried XLA with LinearOperatorBlockDiag, however it is not XLA compatible
I would like to create a custom XLA op, which could deal with the computation and offsets internally: 
it can receive as input a (NxN) matrix, a (N) vector and an array of M offsets (the size of each block), and generate the final (N) vector.
I cannot express it with the available [XLA ops](https://www.tensorflow.org/xla/operation_semantics): tried with xla::DynamicSlice + xla::Dot ,  but I do not know the size of the submatrices/subvectors, so I cannot pass the sizes to DynamicSlice.

I have created my own [Custom Call](https://www.tensorflow.org/xla/custom_call) but I am not sure how to expose it to TensorFlow. The documentation does not include how to compile the Custom Call, integrate, and how to expose it.
In any case I have register a XLA op within `tensorflow/compiler/tf2xla/kernels/mat_vec_op.cc`, compiled, but I cannot call my CustomCall (sitting in `tensorflow/compiler/xla/service/cpu/mycustomcall.cc`), it seems it was not exposed correctly, and it does not find it.


I would like to ask here:
1. Is it possible to implement that kind of op on TF-XLA?
2. If yes, can you please provide some guidance on how?
if it is with a CustomCall, can you please provide help on how to compile, expose and use? 
 


**Standalone code to reproduce the issue**
Python example of what I am trying to do:


`import tensorflow as tf`
`n_blocks = 4`

`@tf.function(jit_compile=True)`
`def myfunc(segment_ids, vector):`
`    value = tf.ones_like(segment_ids)`
`    size_blocks = tf.math.unsorted_segment_sum(value, segment_ids, num_segments=n_blocks)`
`    offsets = tf.concat([[0], tf.math.cumsum(size_blocks)], axis=-1)`
`    blocks_list = []`
`    for i in range(n_blocks):`
`        block = vector[offsets[i]:offsets[i+1]]`
`        block = tf.einsum('i,j -> ij', block, tf.transpose(block))`
`        blocks_list.append(tf.linalg.LinearOperatorFullMatrix(block))`
`    matrix = tf.linalg.LinearOperatorBlockDiag(blocks_list)`
`    return matrix.matvec(vector)`


`for i in range(3):`
`    segment_ids = tf.constant([0, 0, 1, 1, 2, 3])`
`    vector = tf.ones([6], dtype=tf.float64)`
`    print(myfunc(segment_ids, vector))`






"
53745,Small mathematical typo at https://www.tensorflow.org/guide/basics,"
## URL(s) with the issue:

https://www.tensorflow.org/guide/basics

## Description of issue (what needs changing):

### Clear description

Currently: 
At x = 1.0, y = f(x) = (1**2 + 3 - 5) = -2

Proposed:
At x = 1.0, y = f(x) = (1**2 + 2 - 5) = -2

### Submit a pull request?

Yes, I can submit a pull request if issue approved 
"
53744,Conversion to TFLite crashes without any Error,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, 10.0.19043 Build 19043
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
-   **Python version**: Python 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)] on win32
-   **CUDA/cuDNN version**: 
-   **GPU model and memory**: GeForce GTX 1050
-   **Exact command to reproduce**:
```
import tensorflow as tf

model_file = 'mobilenet_v2_1.0_224_frozen_pb'
converter = tf.lite.TFLiteConverter.from_saved_model(model_file)
converter.experimental_new_converter = True
converter.allow_custom_ops = True
tflite_model = converter.convert()

with open(model_file + '.tflite', 'wb') as f:
    f.write(tflite_model)
```
[mobilenet_v2_1.0_224_frozen_pb.zip](https://github.com/tensorflow/tensorflow/files/7858479/mobilenet_v2_1.0_224_frozen_pb.zip)
### Describe the problem
When running this command with the model provided, the script terminates without any error message or conversion to TFLite. The model is created from converting an ONNX model to Saved Model using the onnx_tf library.

Expected Behaviour: The model converts to TFLite or provides an error message for further debugging.

### Source code / logs
2022-01-12 16:02:06.422817: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-01-12 16:02:06.424495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-01-12 16:02:06.426841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]
2022-01-12 16:02:21.969931: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format.
2022-01-12 16:02:21.970194: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency.
2022-01-12 16:02:21.972195: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored change_concat_input_ranges.
2022-01-12 16:02:21.974709: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: mobilenet_v2_1.0_224_frozen_pb
2022-01-12 16:02:22.188729: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }
2022-01-12 16:02:22.189000: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: mobilenet_v2_1.0_224_frozen_pb
2022-01-12 16:02:22.190593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-01-12 16:02:22.190790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]
2022-01-12 16:02:23.052605: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.
2022-01-12 16:02:24.285677: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: mobilenet_v2_1.0_224_frozen_pb
2022-01-12 16:02:25.318756: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 3344022 microseconds.
2022-01-12 16:02:29.185145: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-01-12 16:02:31.117089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1
coreClock: 1.442GHz coreCount: 6 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 78.32GiB/s
2022-01-12 16:02:31.117492: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2022-01-12 16:02:31.558830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-01-12 16:02:31.559036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0
2022-01-12 16:02:31.559895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N

The command fully terminates at this line without anything further or debugging.

"
53743, Python API of `saved_model.load` to support `tf.saved_model.experimental.VariablePolicy`,"**System information**
- TensorFlow version (you are using): TF2.7
- CPU memory: 16Gb
- GPU memory: 10Gb

**Describe the feature and the current behavior/state.**
Some models need to store large weights/tables on CPU and smaller ones on GPU. tf.saved_model.experimental.VariablePolicy is able to store device placement but not fully supported by tf.saved_model.load. Every variable is loaded to GPU if available. There is support on C++ level, but missing in python. A repo.py is as follows. Run as python repo.py --size 11, A OOM is observed. Set the number 11 to be any number b/w the memory size of your CPU and GPU in GB.

```
import argparse
import tensorflow as tf
class LN(tf.keras.layers.Layer):
    def __init__(self, rows, cols,trainable=True):
        super(LN, self).__init__(dtype=tf.float32)
        self.rows = rows
        self.cols = cols
        self.mat = None
        self.trainable=trainable

    def build(self, input_shape):
        self.mat = self.add_weight(""mat"",shape=[self.rows, self.cols],
                                       dtype=tf.float32, trainable=self.trainable)

    def call(self, indices):
        return tf.gather(params=self.mat, indices=indices)

class TestModel(tf.keras.Model):
    def __init__(self, rows, cols):
        super().__init__()
        self.ln = LN(rows=rows, cols=cols)

    @tf.function
    def call(self, x):
        with tf.device('/cpu:0'):
            x = self.ln(x)
        with tf.device('/gpu:0'):
            x = tf.math.reduce_sum(x, axis=1)
        return x


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--size', type=int, default=100, help='Table size in GiB')
    parser.add_argument('--load_cpu', type=bool, default=False, help='Load variables to cpu')
    args = parser.parse_args()
    ncols = 128
    nrows = args.size * 2**30 // 512
    path = '/tmp/saved_model_test'
    model = TestModel(rows=nrows, cols=ncols)
    indices = tf.zeros(shape=(65536, 1), dtype=tf.int32)
    outputs = model(indices)
    print(outputs)
    model.summary()
    print('Saving the model')
    tf.saved_model.save(
            model, path,
            options = tf.saved_model.SaveOptions(
                  experimental_variable_policy=tf.saved_model.experimental.VariablePolicy.SAVE_VARIABLE_DEVICES))
    print('Saved successfully')
    if args.load_cpu:
        with tf.device('/cpu:0'):
            loaded = tf.saved_model.load(path) # Good
    else:
        loaded = tf.saved_model.load(path) # OOM
    print('Load successfully')

if __name__ == '__main__':
    main()
```"
53742,TensorFlow cublas code is not honoring NVIDIA_TF32_OVERRIDE=0 ,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6
- Python version: 3.8.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4
- GPU model and memory: A100



**Describe the current behavior**

Irrespective of NVIDIA_TF32_OVERRIDE=0/1 setting, the following is observed coming from cublas. 

> I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.


By using CUBLASLT_LOG_LEVEL=5 , only see the following kernels with both NVIDIA_TF32_OVERRIDE=0/1

[cublasLtCreate]
[cublasLtCtxInit]
[cublasLtSSSMatmulAlgoGetHeuristic]
[cublasLtSSSMatmul]

Both seem to use the following

2022-01-12 03:29:14][cublasLt][1420][Api][cublasLtSSSMatmulAlgoGetHeuristic] Adesc=[type=R_32F rows=200 cols=200 ld=200] Bdesc=[type=R_32F rows=200 cols=128 ld=200] Cdesc=[type=R_32F rows=200 cols=128 ld=200] preference=[maxWavesCount=0.0 gaussianModeMask=3M_MODE_DISALLOWED pointerModeMask=0 maxWorkspaceSizeinBytes=4194304 minBytesAlignmentA=16 minBytesAlignmentB=16 minBytesAlignmentC=16 minBytesAlignmentD=16 smCountTarget=108] computeDesc=[computeType=COMPUTE_32F_FAST_TF32 scaleType=R_32F]

**Describe the expected behavior**

Should use FP32 instead of TF32.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

Any kernel that uses cublas gemm. 

**Other info / logs** Include any logs or source code that would be helpful to
Irrespective of NVIDIA_TF32_OVERRIDE=0/1 setting, the following is observed coming from cublas. 

> I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.

"
53740,hdf5_format.py method load_weights_from_hdf5_group() fails when you exclude layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **from https://github.com/ahmedfgad/Mask-RCNN-TF2** with some other TF2 fixes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Windows Pro**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  **n/a**
- TensorFlow installed from (source or binary):  **via pip install**
- TensorFlow version (use command below):  **v2.7.0-rc1-69-gc256c071bb2 2.7.0**
- Python version:   **v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)] on win32**
- Bazel version (if compiling from source):   **n/a**
- GCC/Compiler version (if compiling from source):   **n/a**
- CUDA/cuDNN version:   **cuda_11.2.r11.2/compiler.29373293_0**
- GPU model and memory:   **Quadro 2000 32GB**

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
**No windows version**

**Describe the current behavior**
When you call `load_weights()` using the `exclude=` parameter you get errors like ""**You are trying to load a weight file containing 233 layers into a model with 229 layers.**"". Several issues have been posted about this type error but none of them addressed the fundamental problem which is in the method `load_weights_from_hdf5_group()`.
```
if len(layer_names) != len(filtered_layers):
    msg = f'You are trying to load a weight file containing {str(len(layer_names))} layers into a '\
          f'model with {str(len(filtered_layers))} layers.'
    raise ValueError(msg)
```

**Describe the expected behavior**
Depending on which way layers count goes either being greater than or less than we can either auto-correct the layers or report the actual layer that is missing when it checks for matching layer sizes:


**[Contributing](https://www.tensorflow.org/community/contribute)**
- Do you want to contribute a PR? (yes/no):  **yes**
- Briefly describe your candidate solution(if contributing):
**see https://github.com/tensorflow/tensorflow/pull/53739**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53738,How to make Keras Layer class to be immutable or is this a bug?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **from https://github.com/ahmedfgad/Mask-RCNN-TF2** with some other TF2 fixes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Windows Pro**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  **n/a**
- TensorFlow installed from (source or binary):  **via pip install**
- TensorFlow version (use command below):  **v2.7.0-rc1-69-gc256c071bb2 2.7.0**
- Python version:   **v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)] on win32**
- Bazel version (if compiling from source):   **n/a**
- GCC/Compiler version (if compiling from source):   **n/a**
- CUDA/cuDNN version:   **cuda_11.2.r11.2/compiler.29373293_0**
- GPU model and memory:   **Quadro 2000 32GB**

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
**No windows version**

**Describe the current behavior**
The exception in the log below is symptom not the cause.  The actual cause is the variable used in the `set()` call in method `_insert_layers()` of the class `Functional`
```
    # Insert layers and update other layer attrs.
    layer_set = set(self._self_tracked_trackables) 
```
The `self._self_tracked_trackables` is a list of class instances of type `keras.engine.input_layer.InputLayer`.  Set requires the instances to be immutable and since it gives an error it must not be.  So  the question then becomes why pass something that  will not pass this requirement.   

We can find that `self._self_tracked_trackables` is initialized in the code below from method `_init_graph_network` of the same class which gets called when the model is initialized. It can be  seen that self.outputs is what drives the layers output of this `_map_graph_network` call.
```
    # Keep track of the network's nodes and layers.
    nodes, nodes_by_depth, layers, _ = _map_graph_network(
        self.inputs, self.outputs)
    self._network_nodes = nodes
    self._nodes_by_depth = nodes_by_depth
    self._self_tracked_trackables = layers
```
The model is defined as:
```
            model = KM.Model([input_image, input_image_meta, input_anchors],
                             [detections, mrcnn_class, mrcnn_bbox,
                                 mrcnn_mask, rpn_rois, rpn_class, rpn_bbox],
                             name='mask_rcnn')
```
Each of the outputs are derived from `keras.layers.Layer' class.  So how do  we make Layer to  be non-mutable to be able to use that `set()` call in the keras system?

**Describe the expected behavior**
No exception

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):  **no**
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
**Please see https://github.com/ahmedfgad/Mask-RCNN-TF2**

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
Exception has occurred: TypeError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)
unhashable type: 'ListWrapper'
  File ""D:\.venvs\maskrcnn\Lib\site-packages\tensorflow\python\training\tracking\data_structures.py"", line 672, in __hash__
    raise TypeError(""unhashable type: 'ListWrapper'"")
  File ""D:\.venvs\maskrcnn\Lib\site-packages\keras\engine\functional.py"", line 851, in _insert_layers
    layer_set = set(self._self_tracked_trackables)
  File ""D:\.venvs\maskrcnn\Lib\site-packages\keras\engine\functional.py"", line 908, in _graph_network_add_loss
    self._insert_layers(new_layers, new_nodes)
  File ""D:\.venvs\maskrcnn\Lib\site-packages\keras\engine\base_layer.py"", line 1579, in add_loss
    self._graph_network_add_loss(symbolic_loss)
  File ""D:\public\Mask-RCNN-TF2\mrcnn\model.py"", line 2191, in compile
    self.keras_model.add_loss(loss)
  File ""D:\public\Mask-RCNN-TF2\mrcnn\model.py"", line 2379, in train
    self.compile(learning_rate, self.config.LEARNING_MOMENTUM)
  File ""D:\public\Mask-RCNN-TF2\samples\shapes\debug_shapes.py"", line 230, in <module>
    layers='heads')
  File ""C:\Python\python37\Lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Python\python37\Lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Python\python37\Lib\runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""C:\Python\python37\Lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Python\python37\Lib\runpy.py"", line 193, in _run_module_as_main (Current frame)
    ""__main__"", mod_spec)
```"
53735,Padding batches with high ranks (>=6) doesn't work,"### System information

-   **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: Nvidia docker (nvcr.io/nvidia/tensorflow:21.08-tf2-py3)
-   **TensorFlow version (use command below)**: 2.5 (default installation) / 2.7 (updated with pip), doesn't work for both
-   **Python version**: 3.8.10
-   **CUDA/cuDNN version**: 11.4


### Describe the problem
Padding batches with high ranks (>=6) doesn't work and raise an `tensorflow.python.framework.errors_impl.UnimplementedError: CopyElementToLargerSlice Unhandled rank: 6` error.

### Source code / logs
```python3
# Rank 5 is working
import tensorflow as tf
a = {""x"": tf.ones([1, 1, 1, 1, 1])}
b = {""x"": tf.ones([1, 1, 1, 1, 2])}
labels = [a, b]
ds = tf.data.Dataset.from_generator(lambda: iter(labels), output_types={""x"": tf.float32})
ds = ds.padded_batch(2, padded_shapes={""x"": [None, None, None, None, None]})
list(ds.take(1))
# [{'x': <tf.Tensor: shape=(2, 1, 1, 1, 1, 2), dtype=float32, 
#    numpy=array([[[[[[1., 0.]]]]], [[[[[1., 1.]]]]]], dtype=float32)>}]

# Rank 6 doesn't
import tensorflow as tf
a = {""x"": tf.ones([1, 1, 1, 1, 1, 1])}
b = {""x"": tf.ones([1, 1, 1, 1, 1, 2])}
labels = [a, b]
ds = tf.data.Dataset.from_generator(lambda: iter(labels), output_types={""x"": tf.float32})
ds = ds.padded_batch(2, padded_shapes={""x"": [None, None, None, None, None, None]})
list(ds.take(1))
# Throws below error
```

Full error message:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 800, in __next__
    return self._next_internal()
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 783, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2845, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.UnimplementedError: CopyElementToLargerSlice Unhandled rank: 6 [Op:IteratorGetNext]
```
"
53733,Selete op: analyze tools and int8 support,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): pip

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

Hi, after using tflite in runtime, i meet serval problems and they are related to selete op, e.g. model convert, int8 supprt and reduce library size. So i open this issue. 
Thanks for you reply.

1. Is the int8 only support model without selete op.
here is the doc code, and you can see that selete op model is not support, and it fail too even i add the selete op in supported_ops.
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
<b>converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]</b>
<b>converter.inference_input_type = tf.int8</b>  # or tf.uint8
<b>converter.inference_output_type = tf.int8</b>  # or tf.uint8
tflite_quant_model = converter.convert()
```

2. It there any way or tool in python i can quickly find out which keras layer depend on selete op.
i find it's confusing when i want to remove the layer with selete op or replace it. Just like the tflite convert log below, i can only know
tf.FusedBatchNormV3 and  tf.StridedSlice, but how to quickly know where i induce this op?
```
error: illegal scale: -7.049777e+305
loc(callsite(callsite(""functional_3/mask_model_input_norm/FusedBatchNormV3@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op
loc(callsite(callsite(""functional_3/dprnn_block_1/dprnn_layer_2/mask_model_rnn1_norm1/FusedBatchNormV3@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op
loc(callsite(callsite(""functional_3/tf_op_layer_strided_slice_5/strided_slice_5@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op
loc(callsite(callsite(""functional_3/tf_op_layer_strided_slice_6/strided_slice_6@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op
loc(callsite(callsite(""functional_3/dprnn_block_1/strided_slice@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op
loc(callsite(callsite(""functional_3/dprnn_block_1/strided_slice_1@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op
loc(callsite(callsite(""functional_3/dprnn_block_1/dprnn_layer_2/mask_model_rnn1_norm2/FusedBatchNormV3@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op
loc(callsite(callsite(""functional_3/dprnn_block_1/dprnn_layer_3/mask_model_rnn2_norm1/FusedBatchNormV3@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op
loc(callsite(callsite(""functional_3/dprnn_block_1/strided_slice_2@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op
loc(callsite(callsite(""functional_3/dprnn_block_1/strided_slice_3@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op
loc(callsite(callsite(""functional_3/dprnn_block_1/dprnn_layer_3/mask_model_rnn2_norm2/FusedBatchNormV3@__inference__wrapped_model_7151"" at ""StatefulPartitionedCall@__inference_signature_wrapper_17427"") at ""StatefulPartitionedCall"")): error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op
error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
        tf.FusedBatchNormV3 {data_format = ""NCHW"", device = """", epsilon = 1.000000e-03 : f32, exponential_avg_factor = 1.000000e+00 : f32, is_training = true}
        tf.StridedSlice {_cloned = true, begin_mask = 0 : i64, device = """", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}
        tf.StridedSlice {begin_mask = 0 : i64, device = """", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}
Traceback (most recent call last)
```
3. Why BatchNorm not in buildin op
I may think BN is a buildin op because it is common op in NN, but the log is ```'tf.FusedBatchNormV3' op is neither a custom op nor a flex op```.
"
53732,print_selective_registration_header: No module named 'tensorflow.python.platform',"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
- Tensorflow version: r2.3
- TensorFlow installation (pip package or built from source): source
- TensorFlow library (version, if pip package or github SHA, if built from source): source

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

Hi, i follow the tflite doc and want to reduce the library size, but after i build the print_selective_registration_header tool,  i fail to run the bin because some import error is raised, and i try two way to fix it but both fail.
Can you give me some ideas, Many thanks!

1. try to run print_selective_registration_header **not in tensorflow source dir**.
- i check the pip version has installed and then try to run print_selective_registration_header, but it fails.
```
root@3110c2e48cdb:~# python3
Python 3.8.10 (default, Nov 26 2021, 20:14:08)
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2022-01-12 16:57:13.598993: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2022-01-12 16:57:13.599037: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
>>>
root@3110c2e48cdb:~# /root/dev/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/print_selective_registration_header
Traceback (most recent call last):
  File ""/root/dev/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/print_selective_registration_header.runfiles/org_tensorflow/tensorflow/python/tools/print_selective_registration_header.py"", line 41, in <module>
    from tensorflow.python.platform import app
ModuleNotFoundError: No module named 'tensorflow.python.platform'
```

2. try to run print_selective_registration_header **in tensorflow source dir**
```
root@3110c2e48cdb:~/dev/tensorflow/tensorflow# export PYTHONPATH=`pwd`:$PYTHONPATH
root@3110c2e48cdb:~/dev/tensorflow/tensorflow# echo $PYTHONPATH
/root/dev/tensorflow/tensorflow:
root@3110c2e48cdb:~/dev/tensorflow/tensorflow# ./bazel-bin/tensorflow/python/tools/print_selective_registration_header
Traceback (most recent call last):
  File ""/root/dev/tensorflow/tensorflow/./bazel-bin/tensorflow/python/tools/print_selective_registration_header.runfiles/org_tensorflow/tensorflow/python/tools/print_selective_registration_header.py"", line 41, in <module>
    from tensorflow.python.platform import app
ModuleNotFoundError: No module named 'tensorflow.python.platform'
root@3110c2e48cdb:~/dev/tensorflow/tensorflow# cat .tf_configure.bazelrc
build --action_env ANDROID_NDK_HOME=${ANDROID_NDK_HOME}/android-ndk-r${NDK_VERSION}
build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""
build --python_path=""/usr/bin/python3""
build --config=xla
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""
```
"
53731,Flatten return null with DenseNet121,"python 3.7.9 (windows 11 x64)
tensorflow-gpu 2.7.0

```
from tensorflow.python.keras.applications.densenet import DenseNet121
from tensorflow.keras.applications.resnet_v2 import ResNet50V2
from tensorflow.keras.layers import Dense, Flatten

input_shape = (110, 110, 3)
base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)
#base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)

x = base_model.output
x = Flatten()(x)

```

Error: _(if i use ResNet50V2 not happen)_
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""...\Python37\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""...\Python37\site-packages\tensorflow\python\framework\constant_op.py"", line 106, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Exception encountered when calling layer ""flatten_1"" (type Flatten).

Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.

Call arguments received:
  • inputs=<KerasTensor: shape=(None, 3, 3, 1024) dtype=float32 (created by layer 'relu')>

"
53730,"[TF:TRT] EfficientDet D0 pre-build TRT engine failed. TF 2.7.0, TRT 7.2.3","Ubuntu 18.04
Tensorflow 2.7.0
Cuda 11.1.1
TenosrRT 7.2.3.4
CuDNN 8.1.1.33
Cuda Compute Capability 7.5
Hardware: ec2 g4dn.8xlarge (Tesla Turing T4 Tensor Core)

TF2 Model is from TensorFlow 2 Detection Model Zoo - [EfficientDet D0 512x512](http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz)

I tried to convert saved model to TRT model and then pre-build TRT engine. The conversion worked fine but pre-build TRT engine step failed.

Note: Other models such as ""Faster R-CNN ResNet50 V1 640x640"" and ""SSD MobileNet v2 320x320"" work fine. So, most probably it is not ""cudnn installation issue"" as indicated in the error message below.
```
import tensorflow as tf
from tensorflow.python.compiler.tensorrt import trt_convert as trt

conversion_params = trt.TrtConversionParams()

converter = trt.TrtGraphConverterV2(
    input_saved_model_dir=""saved_model"",
    conversion_params=conversion_params)

converter.convert()

import numpy as np
def my_input_fn():
    input_shapes = [(1,512,512,3)]
    for shape in input_shapes:
        yield [np.zeros(shape).astype(np.uint8)]

converter.build(input_fn=my_input_fn)
```
Error:
```
>>> converter.build(input_fn=my_input_fn)
2022-01-12 05:55:45.849139: I tensorflow/compiler/tf2tensorrt/common/utils.cc:58] Linked TensorRT version: 7.2.3
2022-01-12 05:55:45.849218: I tensorflow/compiler/tf2tensorrt/common/utils.cc:60] Loaded TensorRT version: 7.2.3
2022-01-12 05:57:02.621885: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.630204: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.630291: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:945] TF-TRT Warning: Engine creation for StatefulPartitionedCall/EfficientDet-D0/bifpn/TRTEngineOp_0_17 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine
2022-01-12 05:57:02.630314: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:797] TF-TRT Warning: Engine retrieval for input shapes: [[1,64,64,64], [1,32,32,64]] failed. Running native segment for StatefulPartitionedCall/EfficientDet-D0/bifpn/TRTEngineOp_0_17
2022-01-12 05:57:02.644741: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101
2022-01-12 05:57:02.647927: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-01-12 05:57:02.778160: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.778302: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.778392: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:945] TF-TRT Warning: Engine creation for StatefulPartitionedCall/TRTEngineOp_0_19 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine
2022-01-12 05:57:02.778412: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:797] TF-TRT Warning: Engine retrieval for input shapes: [[1,64,32,32], [1,64,64,64], [1,64,32,32]] failed. Running native segment for StatefulPartitionedCall/TRTEngineOp_0_19
2022-01-12 05:57:02.808375: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.808446: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.808480: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:945] TF-TRT Warning: Engine creation for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/TRTEngineOp_0_41 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine
2022-01-12 05:57:02.808495: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:797] TF-TRT Warning: Engine retrieval for input shapes: [[1,64,64,64]] failed. Running native segment for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/TRTEngineOp_0_41
2022-01-12 05:57:02.809402: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.809463: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.809507: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:945] TF-TRT Warning: Engine creation for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/TRTEngineOp_0_31 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine
2022-01-12 05:57:02.809522: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:797] TF-TRT Warning: Engine retrieval for input shapes: [[1,64,64,64]] failed. Running native segment for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/TRTEngineOp_0_31
2022-01-12 05:57:02.821888: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.821946: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.821980: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:945] TF-TRT Warning: Engine creation for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/TRTEngineOp_0_46 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine
2022-01-12 05:57:02.821995: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:797] TF-TRT Warning: Engine retrieval for input shapes: [[1,64,64,64]] failed. Running native segment for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/TRTEngineOp_0_46
2022-01-12 05:57:02.823642: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.823698: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.825524: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.825598: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)
2022-01-12 05:57:02.832321: E tensorflow/stream_executor/dnn.cc:764] CUDNN_STATUS_EXECUTION_FAILED
in tensorflow/stream_executor/cuda/cuda_dnn.cc(5553): 'cudnnPoolingForward( cudnn.handle(), pooling_desc.handle(), &alpha, src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(), output_data->opaque())'
2022-01-12 05:57:02.833347: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_op.cc:542 : INTERNAL: {{function_node StatefulPartitionedCall/TRTEngineOp_0_21_native_segment}} dnn PoolForward launch failed
	 [[{{node StatefulPartitionedCall/EfficientDet-D0/bifpn/node_23/3_up_lvl_5/input_3_up_lvl_4/downsample_max_x2/MaxPool}}]]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 1223, in build
    func(*map(ops.convert_to_tensor, inp))
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 1707, in __call__
    return self._call_impl(args, kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/wrap_function.py"", line 249, in _call_impl
    args, kwargs, cancellation_manager)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 1725, in _call_impl
    return self._call_with_flat_signature(args, kwargs, cancellation_manager)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 1774, in _call_with_flat_signature
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 1960, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 603, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) INTERNAL:  dnn PoolForward launch failed
	 [[{{node StatefulPartitionedCall/EfficientDet-D0/bifpn/node_23/3_up_lvl_5/input_3_up_lvl_4/downsample_max_x2/MaxPool}}]]
	 [[StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/TRTEngineOp_0_30/_168]]
  (1) INTERNAL:  dnn PoolForward launch failed
	 [[{{node StatefulPartitionedCall/EfficientDet-D0/bifpn/node_23/3_up_lvl_5/input_3_up_lvl_4/downsample_max_x2/MaxPool}}]]
	 [[StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/non_max_suppression_with_scores_77/NonMaxSuppressionV5/_584]]
0 successful operations.
0 derived errors ignored. [Op:__inference_pruned_135734]

Function call stack:
pruned -> pruned
```"
53729,The shell environmental variables are disabled in some actions.run as user local GCC is applied.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6.2
- Python version: 3.9
- Installed using virtualenv? pip? conda?: under conda env
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 11.0
- GPU model and memory: V100 32GiB



**Describe the problem**
I tried to compile TF 2.6.2 from the source with the GCC 7.5.0 compiled by myself under the conda env and failed. I may get messages just like the following one:
```
SUBCOMMAND: # @llvm-project//mlir:SubElementInterfacesIncGen_filegroup___gen_type_interface_defs_1075127115_genrule [action 'TdGenerate external/llvm-project/mlir/include/mlir/IR/SubElementTypeInterfaces.cpp.inc', configuration: 1c2a2a243d1505e141626f6ae2bbc7d9ae7eb8e92e00bd2e48a204719dacf979, execution platform: @local_execution_config_platform//:platform]
(cd /home/sanskrit007/.cache/bazel/_bazel_sanskrit007/5ea07439efe33ae8304ab053db0046f8/execroot/org_tensorflow && \
  exec env - \
  bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen -gen-type-interface-defs external/llvm-project/mlir/include/mlir/IR/SubElementInterfaces.td -I external/llvm-project/mlir/include -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include -I external/llvm-project/ -I bazel-out/k8-opt/bin/external/llvm-project/ -I external/llvm-project/mlir/include/mlir/IR -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include/mlir/IR -o bazel-out/k8-opt/bin/external/llvm-project/mlir/include/mlir/IR/SubElementTypeInterfaces.cpp.inc)
SUBCOMMAND: # @llvm-project//mlir:SubElementInterfacesIncGen_filegroup___gen_attr_interface_decls_-1205848285_genrule [action 'TdGenerate external/llvm-project/mlir/include/mlir/IR/SubElementAttrInterfaces.h.inc', configuration: 1c2a2a243d1505e141626f6ae2bbc7d9ae7eb8e92e00bd2e48a204719dacf979, execution platform: @local_execution_config_platform//:platform]
(cd /home/sanskrit007/.cache/bazel/_bazel_sanskrit007/5ea07439efe33ae8304ab053db0046f8/execroot/org_tensorflow && \
  exec env - \
  bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen -gen-attr-interface-decls external/llvm-project/mlir/include/mlir/IR/SubElementInterfaces.td -I external/llvm-project/mlir/include -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include -I external/llvm-project/ -I bazel-out/k8-opt/bin/external/llvm-project/ -I external/llvm-project/mlir/include/mlir/IR -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include/mlir/IR -o bazel-out/k8-opt/bin/external/llvm-project/mlir/include/mlir/IR/SubElementAttrInterfaces.h.inc)
ERROR: /home/sanskrit007/.cache/bazel/_bazel_sanskrit007/5ea07439efe33ae8304ab053db0046f8/external/llvm-project/mlir/BUILD:91:18: TdGenerate external/llvm-project/mlir/include/mlir/IR/SubElementTypeInterfaces.cpp.inc failed (Exit 1): mlir-tblgen failed: error executing command bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen -gen-type-interface-defs external/llvm-project/mlir/include/mlir/IR/SubElementInterfaces.td -I ... (remaining 13 argument(s) skipped)
bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen)
bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen)
bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen)
bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
The complied `mlir-tblgen` cannot get the library directory of my GCC for execution.
The application of static build is no help as introducing `env BAZEL_LINKOPTS=-static-libstdc++:-static-libgcc BAZEL_LINKLIBS=-l%:libstdc++.a:-lm` that is suggested in [the issue](https://github.com/tensorflow/tensorflow/issues/43311) and [the protocol](https://gist.github.com/jakublipinski/40ba68994fe0092600a05b0060e7d445).

I am not familiar with Bazel but at last, I found that the default value of `use_default_shell_env` in the `actions.run` is false. Therefore, I added `use_default_shell_env = True,` in two files to solve this problem.
File 1: third_party/mlir/tblgen.bzl
```
    ctx.actions.run(
        outputs = [ctx.outputs.out],
        inputs = trans_srcs,
        executable = ctx.executable.tblgen,
        arguments = [args],
        use_default_shell_env = True,
        mnemonic = ""TdGenerate"",  # Kythe extractor hook.
    )
```
File 2: tensorflow/core/kernels/mlir_generated/build_defs.bzl
```
    ctx.actions.run(
        inputs = [ctx.file.mlir_op, ctx.file._tfso],
        outputs = [gpu_bin],
        executable = ctx.executable._tool,
        arguments = cmd_args + [
            ""--tile_sizes=%s"" % tile_sizes,
            ""--max-supported-rank=%s"" % ctx.attr.max_supported_rank,
            ""--arch=%s"" % arch_flag,
            ""--input=%s"" % ctx.file.mlir_op.path,
            ""--output=%s"" % gpu_bin.path,
            ""--enable_ftz=%s"" % (ctx.attr.data_type == ""f32""),
            ""--cpu_codegen=%s"" % ctx.attr.cpu_codegen,
        ],
        use_default_shell_env = True,
        mnemonic = ""compile"",
    )
```
Although the compilation is successful and all environment variables are revealed after applying the above patches, I think maybe there are better ways to solve this problem."
53728,"     ValueError: Input 0 of layer ""sequential"" is incompatible with the layer: expected shape=(None, 1, 4), found shape=(None, 1, 34)","```
# Reshape 
X for model training
trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
devX = np.reshape(devX, (devX.shape[0], 1, devX.shape[1]))
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
# Running the LSTM model
model = Sequential()
model.add(LSTM(256, input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences = True))
model.add(LSTM(256))
model.add(Dropout(0.2))
model.add(Dense(look_back))
model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
history = model.fit(trainX, trainY, epochs=200, batch_size=20, validation_data=(devX, devY), verbose=0, shuffle=True)
```

ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_33336/3205999125.py in <module>
     10 model.add(Dense(look_back))
     11 model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
---> 12 history = model.fit(trainX, trainY, epochs=200, batch_size=20, validation_data=(devX, devY), verbose=0, shuffle=True)
     13 
     14 

~\trade-env\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~\trade-env\lib\site-packages\tensorflow\python\framework\func_graph.py in autograph_handler(*args, **kwargs)
   1127           except Exception as e:  # pylint:disable=broad-except
   1128             if hasattr(e, ""ag_error_metadata""):
-> 1129               raise e.ag_error_metadata.to_exception(e)
   1130             else:
   1131               raise

ValueError: in user code:

    File ""C:\Users\mforeman\trade-env\lib\site-packages\keras\engine\training.py"", line 1366, in test_function  *
        return step_function(self, iterator)
    File ""C:\Users\mforeman\trade-env\lib\site-packages\keras\engine\training.py"", line 1356, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\mforeman\trade-env\lib\site-packages\keras\engine\training.py"", line 1349, in run_step  **
        outputs = model.test_step(data)
    File ""C:\Users\mforeman\trade-env\lib\site-packages\keras\engine\training.py"", line 1303, in test_step
        y_pred = self(x, training=False)
    File ""C:\Users\mforeman\trade-env\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""C:\Users\mforeman\trade-env\lib\site-packages\keras\engine\input_spec.py"", line 263, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer ""{layer_name}"" is '

    ValueError: Input 0 of layer ""sequential"" is incompatible with the layer: expected shape=(None, 1, 4), found shape=(None, 1, 34)


I can't figure out what is causing the error.  I check the shape of the inputs and it seems to be correct.


"
53726,Func with custom gradient using `tf.numpy_function`/`tf.py_function` incompatible with `tf.vectorized_map`,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): installed via `pip`
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: -
- GPU model and memory: -

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When using `tf.vectorized_map` on a function that provides a custom gradient and the function uses either `tf.numpy_function` or `tf.py_function`, then unexpected behaviour arises:

* `tf.numpy_function`: the gradient is a vector of zeros;
* `tf.py_function`: `UnknownError:  KeyError: b'pyfunc_12'` error arises.

**Describe the expected behavior**
The gradient is computed without issues.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf
import numpy as np

@tf.function
@tf.custom_gradient
def sin(x):

    # Note: change the following line to use tf.numpy_function to
    # reproduce the other issue mentioned
    res = tf.py_function(func=np.sin, inp=(x,), Tout=tf.float64)

    def grad_fn(dy):
        j = tf.cos(x)
        return dy * j

    return res, grad_fn

inputs = tf.Variable(tf.ones((10,), dtype=tf.float64))

with tf.GradientTape() as tape:
    loss = tf.reduce_sum([sin(x) for x in inputs])

print(""loss:"", loss)
print(""gradient:"", tape.gradient(loss, inputs))

with tf.GradientTape() as tape:
    loss = tf.reduce_sum(tf.vectorized_map(sin, inputs))

print(""Vectorized loss:"", loss)
print(""Vectorized gradient:"", tape.gradient(loss, inputs))
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Output when using 
```
loss: tf.Tensor(8.414709848078965, shape=(), dtype=float64)
gradient: tf.Tensor(
[0.54030231 0.54030231 0.54030231 0.54030231 0.54030231 0.54030231
 0.54030231 0.54030231 0.54030231 0.54030231], shape=(10,), dtype=float64)
Vectorized loss: tf.Tensor(8.414709848078965, shape=(), dtype=float64)
Vectorized gradient: tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(10,), dtype=float64)
```
```
loss: tf.Tensor(8.414709848078965, shape=(), dtype=float64)
gradient: tf.Tensor(
[0.54030231 0.54030231 0.54030231 0.54030231 0.54030231 0.54030231
 0.54030231 0.54030231 0.54030231 0.54030231], shape=(10,), dtype=float64)
Vectorized loss: tf.Tensor(8.414709848078965, shape=(), dtype=float64)
```
Then getting `UnknownError:  KeyError: b'pyfunc_12'`.

For both, a `WARNING:tensorflow:Using a while_loop for converting EagerPyFunc` is also emitted."
53725,Inference time jumps for varying batch size,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes [Link below]
- OS Platform and Distribution (Ubuntu 18.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version: Tested on 2.6, 2.4
- Python version: 3.6
- CUDA/cuDNN version: Tested on 1) CUDA 10.1+cudnn 7, 2) CUDA 11.1 + cudnn 8
- GPU model and memory: Tested on 1) Tesla P100, 16GB , 2) Quadro P4000, 8GB

**Current behavior**

When running classifier inference on GPU, if an input batch size is seen for the first time, the inference time is more than expected. In subsequent runs for the same input batch size, the inference time reduces. When the inference time jump is observed, the load shifts to CPU (GPU usage drops in nvidia-smi) while on subsequent inferences the load is on GPU.

Example 1: 

![figure1_tf_expt_1](https://user-images.githubusercontent.com/3059405/148972390-3c2f86bc-bc50-44f7-ab77-a2ce78012ed2.png)

For a random batch size, the inference time on `run 2` reduces because it is seen in `run 1`. In `run 3`, all the batch sizes are seen in `run 2` and their inference time reduces.


Example 2: 

![figure2_tf_expt_2](https://user-images.githubusercontent.com/3059405/148972415-0c4a74a5-9084-46c7-8b27-b757a257869b.png)

For 10 random batch sizes, the inference time in `run 2` reduces because all these batches are seen in `run 1`. In `run 3`, all the batch sizes are seen in `run 2` and their inference time reduces.


**Why is this relevant?**

Suppose we have a video sequence with varying number of objects every few frames (i.e., the batch size=number of objects varies every few frames). Every time there are total number of objects in a frame that have not been seen before, there is a jump in inference time. For example, if there are 10 objects in the first 30 frames and then there are 8 objects in the next frame, an inference time jump is observed for this frame. In a product with real time expectations, this can have system level implementations on other modules.


**Expected behavior**

Inference time jumps should not be observed with varying batch sizes as it can have system level implications. This behavior is not observed with PyTorch.
An easy solution is to run classifier inference with all batch sizes on dummy images during initialization. For example, we can run inference for batch sizes from 1 to 64 if maximum expected objects are 64. However, that's more of a hack. I am interested in understanding the reason behind this issue. Looks like it has something to do with memory allocation - but why is it dependent on batch size? Is there a better Tensorflow configuration or inference function or memory allocation that can help resolve it?


**Standalone code to reproduce the issue**

Jupyter notebook in Colab:
https://colab.research.google.com/drive/1fHy3HcrYBskMLy-nNn12bbwBI-QIxg1c?usp=sharing

This notebook uses model() for inference. Using model.predict(), gives similar results.

**To enable GPU in Colab, select GPU**

Runtime -> Change Runtime Type -> Hardware Accelerator -> Select GPU from drop down 
"
53724,How can I uninstall tensorflow built in the v-environment of ubuntu-20.04?,"I have tried both ""sudo pip uninstall tensorflow-2.7.0"" and ""sudo pip uninstall tensorflow"" in both normal environment and the v-environment which I installed the tensorflow before, but they all just display ""WARNING: Skipping tensorflow as it is not installed."".
I check the ""pip list"" in the v-environment to see whether the tensorflow still exist, but as the picture, it just right there,safe and sound.
So how can I deal with it to uninstall it? Thanks for your help in advance
<img width=""364"" alt=""微信图片_20220111233045"" src=""https://user-images.githubusercontent.com/95178820/148972571-8e7f8b1f-7de1-4389-b983-7271125f5515.png"">
<img width=""479"" alt=""微信图片_20220111233050"" src=""https://user-images.githubusercontent.com/95178820/148972602-72f6fcb2-3976-4292-847a-80ae46456697.png"">
."
53723,Is there a way I can give a different batch size in each iteration into a fixed batch size in the feed dict?,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Windows 10 Pro
- TensorFlow installed from (source or binary)
- TensorFlow version : 2.6.0 
- Python version: 3.6.13
- CUDA/cuDNN version: 11.2
- GPU model: GeForce RTX 2080 Super

Hi, 
I am working on real time project using alexnet model. I am constantly getting frames at a constant FPS value.I need to get a prediction result for each frame. But I cut the objects whose number is not certain in each frame and give them to the trained model in a batch. Like the screenshot below;

![woring_correctly_loop_2](https://user-images.githubusercontent.com/40995578/148951684-1a84eb42-6710-4465-b82c-e528b1b2b421.png)

GPU **infrence time** works correctly when there is the same number of objects in each frame, as in the screenshot above. but when there is a different number of objects in each frame, the GPU infrence time returns with too much time. 
As far as I understand, parallel processes set themselves for different batch sizes. I can show you a screenshot of it below ;

![error_loop_log_image](https://user-images.githubusercontent.com/40995578/148952864-b120e85c-5a7e-40f0-8390-2c29f89b1e6e.png)

Between 0.005ms and 0.040 ms is a normal time for me but  values ​​other than these are not normal.

 ```
  input_batch = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 227, 227, 1], name='input__batch')
```

 ```
  test_acc_step= sess.run('import/prediction_argmax_model:0', feed_dict={'import/x:0': input_static_batch_size_images})
```
With this usage, I am sending a **static batch** into the **session feed dict**.

**Can I give this to static size instead of None and feed it with dynamic batch in each iteration?** Or is there another way to do this?




"
53722,Model subclassing: access layers saved as class variables to self.__dict__ for dynamic model definition,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: Linux Ubuntu 20.04 LTS
- TensorFlow installed from binary
- TensorFlow version: 2.5.0
- Python version: 3.7.9
- CUDA/cuDNN version: 11.2
- GPU model and memory: 2x RTX3090,, 24GB
- Gist with run-able code: https://gist.github.com/MathiesW/2640ac9cf04da4f0f471b0158437c5c6

Hi,
my goal is to define a dynamically generated FCN model, where I define the amount of filters as a list, and the model is generated with len(filters) Encoder and len(filters) Decoder blocks. I defined my blocks and model according to [this Tensorflow Guide](https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class).

Now, to achieve my dynamic model class, I have written all class variables into the `model.__dict__`:
```
class FullyConvolutionalNetworkDynamic(Model, ABC):
    def __init__(self,
                 num_filter: List[int],
                 kernel_size: List[int],
                 strides: List[int],
                 name: str = ""FullyConvolutionalNetwork"",
                 activation: str = ""relu"",
                 data_format: str = ""channels_last"",
                 *args, **kwargs):
        super(FullyConvolutionalNetworkDynamic, self).__init__(name=name, *args, **kwargs)

        # define symmetric encoder and decoder blocks
        for i, (f, k, s) in enumerate(zip(num_filter, kernel_size, strides)):
            self.__dict__[f""encoder{i}""] = EncoderBlock(num_conv=f, kernel=k, stride=s,
                                                        activation=activation,
                                                        name=f""enc{i}"", data_format=data_format)
            self.__dict__[f""decoder{i}""] = DecoderBlock(num_conv=f, kernel=k, stride=s,
                                                        activation=activation,
                                                        name=f""dec{i}"", data_format=data_format)

        # output section
        self.conv1x1 = Conv1D(1, 1, strides=1, padding='same', name=""1x1"", data_format=data_format)
        self.flatten = Flatten()
```
Now that my layers are written to `model.__dict__`, I tried to add them to my call function from here:
```
    @tf.function
    def call(self, x, training=False):
        # encoder path
        for layer in sorted([key for key in self.__dict__ if ""encoder"" in key]):
            x = self.__dict__[layer](x)

        # decoder path
        for layer in sorted([key for key in self.__dict__ if ""decoder"" in key], reverse=True):
            x = self.__dict__[layer](x)

        # output section
        x = self.conv1x1(x)
        return self.flatten(x)
```
However, when I now create an instance of my model, the model summary states that only the layers defined in the output section are added to the model:
```
>>> model = FullyConvolutionalNetwork(num_filters=[4, 8, 16], kernel_size=[3, 3, 3], strides=[1, 1, 1])
>>> model.build(input_shape=[None, 256, 10])
>>> model.summary()

Model: ""FullyConvolutionalNetwork""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
1x1 (Conv1D)                 multiple                  5         
_________________________________________________________________
flatten (Flatten)            multiple                  0         
=================================================================
Total params: 5
Trainable params: 5
Non-trainable params: 0
_________________________________________________________________

```
I already use the model but ""hard-coded"" to three layers depth on each path. When I try to recreate my hard-coded model with the dynamic approach above, the `model.__dict__` indeed contains my EncoderBlock and DecoderBlock layers, they are just not added to the model. The ""hard-coded"" version works flawlessly! Any help to solve my problem is greatly appreciated! Thank you in advance!

PS: I have allowed my self to re post my issue here, since the Tensorflow Repository seems much more active than the keras repository.
"
53721,Can not convert a tensorflow model to tflite,">>> tf.__version__
'2.9.0-dev20220110'

I have a model that I have converted to onnx and tensorflow from original pytorch. Both conversion works OK. Now I want to convert this model to tflite. But the conversion script is giving me error. 

'tf.BiasAdd' op requires channel dimension and feature dimension to match; found 1 and 1024, respectively

The model has a lstm layer

The model is attached and conversion script used is below:

[pretrained.zip](https://github.com/tensorflow/tensorflow/files/7845281/pretrained.zip)
[model.pb.zip](https://github.com/tensorflow/tensorflow/files/7845282/model.pb.zip)

```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model('/home/model.pb')
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
```


"
53713,TFLite_Detection_PostProcess produces invalid bounding box coordinates,"**System information**
- Have I written custom code: yes
```python
import tensorflow as tf                                                                                                                                                           
import numpy as np                                                                                                                                                                
import cv2                                                                                                                                                                        
                                                                                                                                                                                  
interpreter = tf.lite.Interpreter(model_path=""ssd_mobilenet_v1_1_default_1.tflite"")                                                                                               
interpreter.allocate_tensors()                                                                                                                                                    
                                                                                                                                                                                  
input_details = interpreter.get_input_details()                                                                                                                                   
output_details = interpreter.get_output_details()                                                                                                                                 
print(input_details)                                                                                                                                                              
print(output_details)                                                                                                                                                             
                                                                                                                                                                                  
input_shape = input_details[0]['shape']                                                                                                                                           
im = cv2.imread(""buggy_image_2.jpg"")  # 1920x1080                                                                                                                                 
                                                                                                                                                                                  
# RGB conversion                                                                                                                                                                  
im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)                                                                                                                                      
                                                                                                                                                                                  
# Crop center                                                                                                                                                                     
crop_im = im_rgb[0:1080, 420:1500]  # 1080x1080                                                                                                                                   
                                                                                                                                                                                  
# Resize                                                                                                                                                                          
im_rgb = cv2.resize(im_rgb, (input_shape[1], input_shape[2]))  # 300x300                                                                                                          
                                                                                                                                                                                  
# Set dimension                                                                                                                                                                   
input_data = np.expand_dims(im_rgb, axis=0)                                                                                                                                       
print(input_data.shape)                                                                                                                                                           
interpreter.set_tensor(input_details[0]['index'], input_data)                                                                                                                     
                                                                                                                                                                                  
# Inference                                                                                                                                                                       
interpreter.invoke()                                                                                                                                                              
                                                                                                                                                                                  
detection_boxes = interpreter.get_tensor(output_details[0]['index'])                                                                                                              
detection_classes = interpreter.get_tensor(output_details[1]['index'])                                                                                                            
detection_scores = interpreter.get_tensor(output_details[2]['index'])                                                                                                             
num_boxes = interpreter.get_tensor(output_details[3]['index'])                                                                                                                    
                                                                                                                                                                                  
print(""==detection_boxes=="")                                                                                                                                                      
print(detection_boxes)                                                                                                                                                            
print()                                                                                                                                                                           
print(""==detection_classes=="")                                                                                                                                                    
print(detection_classes)                                                                                                                                                          
print()                                                                                                                                                                           
print(""==detection_scores=="")                                                                                                                                                     
print(detection_scores)                                                                                                                                                           
print()                                                                                                                                                                           
print(""==num_boxes=="")                                                                                                                                                            
print(num_boxes)                                                                                                                                                                  
print()
```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
`Mac Monterey 12.0.1`

- TensorFlow version (use command below):
```bash
python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
unknown 2.7.0
```
- Python version: 
`3.9.9`


**Describe the current behavior**
Model: I got the tflite model from https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/default/1
Image: https://github.com/heeh/tflite-test/blob/main/buggy_image_2.jpg
Bounding box produces negative coordinates as well as coordinates bigger than one.
```
  [ 0.8324257   0.52732474  0.9950099   1.0011996 ]
  [ 0.83758724  0.5655538   1.0012565   0.8586073 ]
  [ 0.8129529   0.55081666  1.0012082   0.66460264]
  [-0.00595534  0.5822872   0.6086124   0.9962516 ]
  [ 0.06498116  0.5597983   0.8853406   1.0037088 ]
  [ 0.870738    0.7267949   1.0040077   0.99758047]
```
**Describe the expected behavior**
The bounding box should produce the output within `0.0` and `1.0`
According to https://www.tensorflow.org/lite/examples/object_detection/overview
```Multidimensional array of [N][4] floating point values between 0 and 1, the inner arrays representing bounding boxes in the form [top, left, bottom, right]```
![image](https://user-images.githubusercontent.com/5158322/148822427-04e924c1-71e4-40d0-9912-4a4474972db7.png)



- Briefly describe your candidate solution(if contributing):
The `TFLite_Detection_PostProcess` operator should enforce the output between zero and one.
I believe that this has something to do with the following code.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1Ouyz_BUGSvvKG2Ib_fVMye6IjAmWuuRf?usp=sharing


**Other info / logs** 
```
[{'name': 'normalized_input_image_tensor', 'index': 175, 'shape': array([  1, 300, 300,   3], dtype=int32), 'shape_signature': array([  1, 300, 300,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0078125, 128), 'quantization_parameters': {'scales': array([0.0078125], dtype=float32), 'zero_points': array([128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'TFLite_Detection_PostProcess', 'index': 167, 'shape': array([ 1, 10,  4], dtype=int32), 'shape_signature': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 168, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 169, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 170, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
(1, 300, 300, 3)
==detection_boxes==
[[[ 0.39625263  0.47477102  0.9839653   0.6032783 ]
  [ 0.44166473  0.74635595  0.95957065  0.9920321 ]
  [ 0.83758724  0.5655538   1.0012565   0.8586073 ]
  [ 0.8129529   0.55081666  1.0012082   0.66460264]
  [ 0.8254188   0.56289214  0.9910882   0.63345426]
  [ 0.8324257   0.52732474  0.9950099   1.0011996 ]
  [ 0.3868151   0.4032717   0.99340284  0.58236426]
  [-0.00595534  0.5822872   0.6086124   0.9962516 ]
  [ 0.06498116  0.5597983   0.8853406   1.0037088 ]
  [ 0.870738    0.7267949   1.0040077   0.99758047]]]

==detection_classes==
[[89. 61. 61. 61. 61. 61. 89. 61. 61. 61.]]

==detection_scores==
[[0.5234375  0.4375     0.4140625  0.3671875  0.35546875 0.34375
  0.33203125 0.30078125 0.30078125 0.28125   ]]

==num_boxes==
[10.]
```
"
53709,No matching distribution found for tensorflow==2.4.1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS Big Sur 11.6
- TensorFlow installed from (source or binary): via pip 
- TensorFlow version: trying to install 2.4.1
- Python version: 3.9.6
- Installed using virtualenv? pip? conda?: pip 


**Describe the problem**

Via jupyter notebooks on vs code, trying to run `!pip3 install tensorflow==2.4.1` but getting 
```
ERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1 (from versions: 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.5.2, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.8.0rc0)
ERROR: No matching distribution found for tensorflow==2.4.1
```
"
53708,Select TF Ops build output a large library file.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.7
- Python version: 3.9
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source): 10.1.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Try to add Select TF Ops to the libtensorflowlite.so and it outputs a large binary file about 180MB in linux gcc and 90+ MB in cross-compilation for android. Is there a way to reduce the size of library ?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I follow the guide of [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md](url) to get a BUILD file and a shared library with Select TF Ops. But it seems the size of the library is almost the same as 

> Add the TensorFlow ops delegate library dependency to the build dependencies: tensorflow/lite/delegates/flex:delegate.
It's to large for mobile deploy and is there any ways to reduce it?

The BUILD file is:
```
load(       
    ""//tensorflow/lite:build_def.bzl"",
    ""tflite_custom_cc_library"",
    ""tflite_cc_shared_object"",
)        
load(       
    ""@org_tensorflow//tensorflow/lite/delegates/flex:build_def.bzl"",
        ""tflite_flex_shared_library""
)        
         
tflite_flex_shared_library(
  name = ""tensorflowlite_flex"",
  models = [
      "":model.tflite"",
  ],        
)        
         
tflite_custom_cc_library(
    name = ""selectively_built_cc_lib"",
    models = [ 
        "":model.tflite"",
    ],   
)        
         
# Shared lib target for convenience, pulls in the core runtime and builtin ops.
# Note: This target is not yet finalized, and the exact set of exported (C/C++)
# APIs is subject to change. The output library name is platform dependent:
#   - Linux/Android: `libtensorflowlite.so`
#   - Mac: `libtensorflowlite.dylib`
#   - Windows: `tensorflowlite.dll`
tflite_cc_shared_object(
    name = ""tensorflowlite"",
    # Until we have more granular symbol export for the C++ API on Windows,
    # export all symbols.
    features = [""windows_export_all_symbols""],
    linkopts = select({
        ""//tensorflow:macos"": [
            ""-Wl,-exported_symbols_list,$(location //tensorflow/lite:tflite_exported_symbols.lds)"",
        ],
        ""//tensorflow:windows"": [], 
        ""//conditions:default"": [
            ""-Wl,-z,defs"",
            ""-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)"",
        ],
    }),  
    per_os_targets = True,
    deps = [
        "":selectively_built_cc_lib"",
        ""//tensorflow/lite:tflite_exported_symbols.lds"",                                                                                          
        ""//tensorflow/lite:tflite_version_script.lds"",
    ],   
)
```
The build command:
`bazel build -c opt --cxxopt=--std=c++14 --config=monolithic --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tensorflow/lite/tmp:libtensorflowlite_flex.so`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
53707,TFlite docker build fail using bazel.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3
- Python version: 2.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: no
- GPU model and memory: no



**Describe the problem**
I had build the tflite successully without using docker, but when i try to use docker to build it automatically. It fails.
Many Thanks for you help!

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel build --config=monolithic --define=with_select_tf_ops=true \
        --cxxopt=""-std=c++14"" \
        -c opt \
        //tensorflow/lite:libtensorflowlite.so \
        --verbose_failures
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=186
INFO: Reading rc options for 'build' from /root/dev/tensorflow/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/dev/tensorflow/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from /root/dev/tensorflow/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env ANDROID_NDK_HOME=${ANDROID_NDK_HOME}/android-ndk-r${NDK_VERSION} --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file /root/dev/tensorflow/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /root/dev/tensorflow/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:monolithic in file /root/dev/tensorflow/tensorflow/.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:linux in file /root/dev/tensorflow/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /root/dev/tensorflow/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
WARNING: /root/dev/tensorflow/tensorflow/tensorflow/core/BUILD:1749:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation
WARNING: /root/dev/tensorflow/tensorflow/tensorflow/core/BUILD:2161:1: in linkstatic attribute of cc_library rule //tensorflow/core:framework_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation
WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Repository aws-checksums instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule third_party_http_archive defined at:
  /root/dev/tensorflow/tensorflow/third_party/repo.bzl:219:28: in <toplevel>
WARNING: Download from https://mirror.tensorflow.org/github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz failed: class javax.net.ssl.SSLHandshakeException No subject alternative DNS name matching mirror.tensorflow.org found.
WARNING: Download from https://github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz failed: class java.io.IOException connect timed out
ERROR: An error occurred during the fetch of repository 'aws-checksums':
   java.io.IOException: Error downloading [https://mirror.tensorflow.org/github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz, https://github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz] to /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/aws-checksums/v0.1.5.tar.gz: connect timed out
INFO: Repository llvm-project instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule tf_http_archive defined at:
  /root/dev/tensorflow/tensorflow/third_party/repo.bzl:134:19: in <toplevel>
INFO: Repository aws-c-common instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule third_party_http_archive defined at:
  /root/dev/tensorflow/tensorflow/third_party/repo.bzl:219:28: in <toplevel>
INFO: Repository aws-c-event-stream instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule third_party_http_archive defined at:
  /root/dev/tensorflow/tensorflow/third_party/repo.bzl:219:28: in <toplevel>
ERROR: /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/aws/BUILD.bazel:12:1: @aws//:aws depends on @aws-checksums//:aws-checksums in repository @aws-checksums which failed to fetch. no such package '@aws-checksums//': java.io.IOException: Error downloading [https://mirror.tensorflow.org/github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz, https://github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz] to /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/aws-checksums/v0.1.5.tar.gz: connect timed out
ERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: no such package '@aws-checksums//': java.io.IOException: Error downloading [https://mirror.tensorflow.org/github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz, https://github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz] to /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/aws-checksums/v0.1.5.tar.gz: connect timed out
INFO: Elapsed time: 169.197s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (209 packages loaded, 12517 targets configured)
```
"
53706,Segmentation fault: 11,"pardon me for being new veteran on this. i still cannot figure this error after i try to train the model, it end up with this error. after few search, have a clue that it might be Mac issue.

My mac is macOS Catalina and version 10.15.2.

Thank you in advance ^^

![Screen Shot 2022-01-09 at 10 24 30 PM](https://user-images.githubusercontent.com/15898245/148703921-5db43328-a512-4ba5-a343-ade0aaeaee86.png)
 "
53705,I have made wrapper of Tensor ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):

Latest Github release

**Describe the feature and the current behavior/state.**

Graph

**Will this change the current api? How?**

nope

**Who will benefit with this feature?**

Everyone

**Any Other info.**


I have started wrapper of Tensorflow using cppflow. I need some help with graph. It is unclear how it works in C API. I am thinking of wrapping C++ api. It works for all swig languages. The question is does C/api suppot the graph enough to use it or should I wrap the C++ API?"
53704,Conflict between two libraries of source-built and prebuilt (conda) TensorFlow,"Hi Folks,

I installed prebuilt TensorFlow (TF) 2.6 using conda and compiled the same version from the source using bazel. I am able to call/import TF in Python but when adding the built shared libraries to the lib env variable (`LD_LIBRARY_PATH`), I got the following error about a conflict between two TF libraries.

These codes work well:

```sh
Python 3.9.0 (default, Nov 15 2020, 14:28:56)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2022-01-09 16:31:59.785672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-01-09 16:31:59.785714: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
```

But these codes yielded the error in question (suppose that shared libs are stored in `/usr/local/tensorflow/lib/`):

```
export LD_LIBRARY_PATH=/usr/local/tensorflow/lib/:$LD_LIBRARY_PATH

Python 3.9.0 (default, Nov 15 2020, 14:28:56)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: descriptor_table_tensorflow_2fcore_2fprotobuf_2fdata_5fservice_2eproto

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 79, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: descriptor_table_tensorflow_2fcore_2fprotobuf_2fdata_5fservice_2eproto

Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
>>>
```
"
53703,unicom lucy,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53702,Native support for StridedSlice in 6D and Transpose in 7D,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 804ef7223ef08fd14c274b4a4044cc4aeee68863


**Provide the text output from tflite_convert**

```
2022-01-08 12:47:32.812069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/openvino_2021/data_processing/dl_streamer/lib:/opt/intel/openvino_2021/data_processing/gstreamer/lib:/opt/intel/openvino_2021/opencv/lib:/opt/intel/openvino_2021/deployment_tools/ngraph/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/tbb/lib::/opt/intel/openvino_2021/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/omp/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/lib/intel64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2022-01-08 12:47:32.812092: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-01-08 12:47:32.812107: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
2022-01-08 12:47:32.812367: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.
2022-01-08 12:47:34.806218: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2022-01-08 12:47:34.806383: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2022-01-08 12:47:34.860258: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.008ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.

2022-01-08 12:47:37.790651: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.
2022-01-08 12:47:37.790685: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.
2022-01-08 12:47:38.104138: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1892] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexStridedSlice, FlexTranspose
Details:
	tf.StridedSlice(tensor<1x120x160x3x1x32xf32>, tensor<6xi32>, tensor<6xi32>, tensor<6xi32>) -> (tensor<1x120x160x1x32xf32>) : {begin_mask = 55 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 55 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 8 : i64}
	tf.Transpose(tensor<1x30x4x40x4x1x4xf32>, tensor<7xi32>) -> (tensor<1x30x40x1x4x4x4xf32>) : {device = """"}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
2022-01-08 12:47:38.104385: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 53.877 G  ops, equivalently 26.938 G  MACs

Estimated count of arithmetic ops: 53.877 G  ops, equivalently 26.938 G  MACs
WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded
```

**Standalone code to reproduce the issue** 

TFLite Native support for `StridedSlice` for 6D, and `Transpose` for 7D would be very beneficial. It would be nice if we could avoid Flex operations if possible.
- **`saved_model`** and converted **`tflite`** files 
[saved_model_and_float32tflite.zip](https://github.com/tensorflow/tensorflow/files/7833211/saved_model_and_float32tflite.zip)

- Conversion Script
```python
import tensorflow as tf

input_shapes = [[1,120,160,6]]

model = tf.saved_model.load(
    'flyingthings_finalpass_xl/saved_model_120x160'
)
concrete_func = model.signatures[
    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY
]
concrete_func_input_tensors = [
    tensor for tensor in concrete_func.inputs \
        if tensor.dtype != tf.resource and not 'unknown' in tensor.name
]
for conc_input, def_input in zip(concrete_func_input_tensors, input_shapes):
    conc_input.set_shape(def_input)
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS,
]
tflite_model = converter.convert()
with open('model_float32.tflite', 'wb') as w:
    w.write(tflite_model)
```

**Any other info / logs**
- Model Citation Repository - HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching
https://github.com/google-research/google-research/tree/master/hitnet
![148323208-7db28584-ce78-4398-94fa-4ce93c9f8d4d](https://user-images.githubusercontent.com/33194443/148644951-ed032ea7-f4d4-43a1-bdfb-bf368d12cb4e.gif)

- FlexStridedSlice (6D)
![Screenshot 2022-01-08 21:52:11](https://user-images.githubusercontent.com/33194443/148644890-d6400a0a-e8b7-423d-bfc8-e78e56135647.png)

- FlexTranspose (7D)
![Screenshot 2022-01-08 21:52:40](https://user-images.githubusercontent.com/33194443/148644896-b1481efb-4cd0-470a-b277-b523cfce4798.png)"
53701,The GRU/LSTM function bugs when using TF2.7,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.7
- Python version:3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.5
- GPU model and memory:NVIDIA Geforce 1070, 8GB



**Describe the current behavior**

Hello，When I used the GRU/LSTM model to accomplish the text classification with IMDB dataset，there were some problems  in model  fitting process.This is the dataset shape information.
![datasets](https://user-images.githubusercontent.com/4904955/148642698-a1fe2d86-ed76-404e-a89b-34a122a21618.png)
When I set the GRU layer with  return_sequences=True:

![GRU](https://user-images.githubusercontent.com/4904955/148642812-1fd832e2-7d8f-4bc4-94bc-443f0ed4677a.png)

![1-True](https://user-images.githubusercontent.com/4904955/148642730-781a87a5-67f1-426f-a3a3-30e70698967c.png)

Then I removed the return_sequences=False,the code was seemed to be normal,but the gradient was't decreased:

![1-false](https://user-images.githubusercontent.com/4904955/148642765-bffba379-bd36-4d8d-bcc0-1667afff676c.png)


The LSTM performance was also similar with the GRU. If I used the two layers GRU with the last GRU's return_sequences=False，the performance of model was same as the single GRU/LSTM layer.
![M-RNN](https://user-images.githubusercontent.com/4904955/148642917-8adb0da9-b3d3-4a96-8000-ac2764170bdf.png)

![M-RNN2](https://user-images.githubusercontent.com/4904955/148642905-e30f9f6b-4483-4bc5-b4a9-5b9dc396da92.png)



**Describe the expected behavior**
I asked my teammates who used **tf2.6.2**,they said all the problem I mentioned above didn't occur when they ran these codes. But some people who used  **tf2.7** found the same problem. I also checked the different systems like linux in colab or docker,it seems that the bugs could happen when using tf2.7.I has updated my code with zip attachment,


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

The Code is here:
[imdb.zip](https://github.com/tensorflow/tensorflow/files/7833147/imdb.zip)

"
53700,Why is tensorflow2.6 so much slower than pytorch1.10.0+cu113?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (Linux Ubuntu 18.04):
- TensorFlow installed from binary:
- TensorFlow version --2.6:
- Python 3.8:
- GPU model R3090 and memory 24G:

tensorflow 2.6 code：
```
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras import preprocessing
from tensorflow.keras.applications.vgg19 import VGG19

print(tf.__version__)

model = VGG19(weights='imagenet')
img = preprocessing.image.load_img('008.jpg', target_size=(224, 224))
x = preprocessing.image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x /= 255.
for _ in range(5):
    t1 = time.time()
    preds = model.predict(x)
    print(preds.shape)
    t2 = time.time()
    print(t2 - t1)
```

output:
```
2.6.0
(1, 1000)
2.8845417499542236
(1, 1000)
0.044057369232177734
(1, 1000)
0.027636051177978516
(1, 1000)
0.04877758026123047
(1, 1000)
0.027869701385498047
```

pytorch code:
```
import torch
import torchvision
import time
import thop

print(torch.__version__)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

model = torchvision.models.vgg19(pretrained=False)
model.eval().to(device)
inputs = torch.rand((1, 3, 224, 224)).to(device)
with torch.no_grad():
    for _ in range(5):
        t1 = time.time()
        outputs = model(inputs)
        print(outputs.shape)
        t2 = time.time()
        print(t2 - t1)
```

output:
```
1.10.0+cu113
torch.Size([1, 1000])
0.0020911693572998047
torch.Size([1, 1000])
0.0019617080688476562
torch.Size([1, 1000])
0.001947164535522461
torch.Size([1, 1000])
0.001924753189086914
torch.Size([1, 1000])
0.0019559860229492188
```
Is this my problem or the tensorflow problem?"
53699,"ValueError: Exception encountered when calling layer ""transformer_decoder_3"" (type TransformerDecoder).  Could not find matching concrete function to call loaded from the SavedModel","<em>I got this error, while trying to load my transformer model. </em>


Model:
>Layer (type)          ==         Output Shape    ==     Param #   ==  Connected to                     
>=============================================================================================>=====
> encoder_inputs (InputLayer)      ==  [(None, None)]  ==     0    ==       []                               
 >                                                                                                 
 >positional_embedding_6 (Positi    == (None, None, 256)==   645120  ==    ['encoder_inputs[0][0]']         
 >onalEmbedding)                                                                                   
  >                                                                                                
 >decoder_inputs (InputLayer)      ==  [(None, None)]  ==     0    ==       []                               
  >                                                                                                
 >transformer_encoder_3 (Transfo    == (None, None, 256) ==  3155456   ==  ['positional_embedding_6[0][0]'] 
 >rmerEncoder)                                                                                     
 >                                                                                                 
 >model_7 (Functional)      ==     (None, None, 2500) ==  6547140  ==   ['decoder_inputs[0][0]',         
  >                                                                                                                 'transformer_encoder_3[0][0]']  
   >                                                                                               
>==================================================================================================

Error:
>ValueError: Exception encountered when calling layer ""transformer_decoder_3"" (type TransformerDecoder).
>
>Could not find matching concrete function to call loaded from the SavedModel. Got:
 > Positional arguments (4 total):
   > * Tensor(""inputs:0"", shape=(None, None, 256), dtype=float32)
   > * Tensor(""encoder_outputs:0"", shape=(None, None, 256), dtype=float32)
   > * None
   > * False
  >Keyword arguments: {}
>
> Expected these arguments to match one of the following 2 option(s):
>
>Option 1:
 > Positional arguments (4 total):
  >  * TensorSpec(shape=(None, None, 256), dtype=tf.float32, name='inputs')
   > * TensorSpec(shape=(None, None, 256), dtype=tf.float32, name='encoder_outputs')
    >* TensorSpec(shape=(None, None), dtype=tf.bool, name='mask')
    >* False
  >Keyword arguments: {}
>
>Option 2:
 > Positional arguments (4 total):
  >  * TensorSpec(shape=(None, None, 256), dtype=tf.float32, name='inputs')
   > * TensorSpec(shape=(None, None, 256), dtype=tf.float32, name='encoder_outputs')
    >* TensorSpec(shape=(None, None), dtype=tf.bool, name='mask')
    >* True
 > Keyword arguments: {}
>
>Call arguments received:
 > • args=('tf.Tensor(shape=(None, None, 256), dtype=float32)',)
 > • kwargs={'encoder_outputs': 'tf.Tensor(shape=(None, None, 256), dtype=float32)', 'training': 'None'}

I also got this warning, while saving model.
>WARNING:absl:Found untraced functions such as embedding_12_layer_call_fn, embedding_12_layer_call_and_return_conditional_losses, embedding_13_layer_call_fn, embedding_13_layer_call_and_return_conditional_losses, multi_head_attention_9_layer_call_fn while saving (showing 5 of 150). These >functions will not be directly callable after loading.


**Important Notice**

GOOGLE COLAB
https://colab.research.google.com/drive/1vhJiMvCnxT7y4KhMv7wez_zBqwI-7yqu?usp=sharing


"
53697,[TF:TRT] Tensorflow Docker 2.7.0-gpu has incompatible TRT version 8.0.0,"**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from: binary
- TensorFlow version: 2.7.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 11.2
- GPU model and memory: NVIDIA T4, 16GB

**Steps to reproduce the issue**
Setup tensorflow/tensorflow:2.7.0-gpu container
```
$ sudo docker run -it --name tf270 --gpus all tensorflow/tensorflow:2.7.0-gpu
```
Test TF/TRT
```
$ apt list --installed | grep infer

libnvinfer-plugin8/unknown,now 8.0.0-1+cuda11.0 amd64 [installed,upgradable to: 8.2.0-1+cuda11.4]
libnvinfer8/unknown,now 8.0.0-1+cuda11.0 amd64 [installed,upgradable to: 8.2.0-1+cuda11.4]

$ python3
import tensorflow as tf
import tensorflow.compiler as tf_cc
tf_cc.tf2tensorrt._pywrap_py_utils.get_linked_tensorrt_version()

(7, 2, 2)
```
As you can see the installed TRT version is 8.0.0 but Tensorflow was built and linked with TRT 7.2.2.

@DEKHTIARJonathan  @bixia1 @sanjoy

"
53696,TensorBoard doesn't work on IOS/IpadOS. Requires apple icons. ,"
After starting the server it gives the following result:

W0107 19:43:36.780149 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404
W0107 19:43:36.798506 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404
W0107 19:43:36.832389 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404
W0107 19:43:36.849392 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404
W0107 19:43:36.900132 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404
W0107 19:43:36.915132 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404
W0107 19:43:36.948019 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404
W0107 19:43:36.965894 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404

And in afterwards you can not access tensorboard on apple devices (tested on Iphone 8, iphone 11, ipad 2018). but on android/chome(macos) works great"
53693,Will experimental_distribute_dataset or experimental_distribute_datasets_from_function distribute same data in more than one batch ?,"
------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**: 2.2
-   **Python version**: 3.8
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**: 
-   **Exact command to reproduce**:

### Describe the problem
am using tensorflow 2.2, Noticed same data being distributed in multiple batches/data shard . total batch count and no of records in each batch matched to expected count but the records present in each batch isn't unique.  Strategy - Multiworker mirror strategy

### Source code / logs
I won't be able to share the codes here. but am using multiworker mirror strategy with unbatch, batch and autoshard policy of data.
"
53692,NNAPI/Hexagon Support Through CMAKE on RB5 Arm Linux Platform,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Ubuntu 18.04 Qualcomm Linaro Release
- Qualcomm RB5
- Source
- TensorFlow version: v2.6
- Python version: 3.6
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source):  
- GCC/Compiler version (if compiling from source): gcc 7.5
- CUDA/cuDNN version: n/a
- GPU model and memory: Adreno GPU 650

Hello, I am having trouble understanding the tutorials and getting delegate support for Tensorflow lite c++/c library on the Qualcomm RB5.

I have been using ```CMAKE / GCC``` installation natively on board.

I have the following flags enabled ```-DTFLITE_ENBLE_XNN_PACK -DTFLITE_-DTFLITE_ENABLE_GPU -DTFLITE_ENABLE_NNAPI```

And I am able to compile a working(ish) library and do inferencing. However the only delegate that works is the XNN_PACK delegate. The GPU delegate causes the program to fail when specifying it (maybe not linking correct adreno library to the inferencing code)...could use some help there)

But mostly I am concerned about building NNAPI and HEXAGON delegate support. The RB5 has hexagon libraries already installed and I will also install the libhexagon_nn_skel.so libraries.

There is no documentation using cmake to enable hexagon support in the TensorFlow lite library and I see no flags in the CMakeLists.txt.

I have downloaded the Hexagon SDK and will copy the android_ndk _rc19 workspace to somewhere on board and set the path. Will this enable NNAPI correctly? Because now it does not believe it skips that compilation because it cannot find the correct headers (not installed yet)




Basically I have the following questions:


1) Will moving the android_rc19_ndk to a directory on board the RB5 and setting ANDROID_NDK_HOME successfully compile the NNAPI delegate ?

2) How can I enable hexagon support with the CMake Installation of the TfLite ? All hexagon libraries are on board.

3) What are the correct library links to use to successfully enable a GPU delegate on the Adreno GPU? I see existing support documented on the tensorflow website for this.

4) If none of the three on board are viable - Would building with Bazel on board work? Or do we need to do a cross compilation, if so how?

Thank you for taking the time to read this. I really enjoy using tensorflow lite and I know the RB5 community would really enjoy a documented process to do this correctly and unlock all of it's potential.


"
53690,Training time is varying between docker and without docker ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04. 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.4/8.2
- GPU model and memory:A6000 & 48GB

**Describe the current behavior**
If I use docker image tensorflow/tensorflow:latest-gpu container for training Convolution based neural network it is taking less time compared to Tensorflow installed using pip install tensorflow-gpu==2.7. 

**Describe the expected behavior**
If we use docker image/ installed using pip also the training should not change much

I am using tf.distribute.MirroredStrategy() to use all GPU's. How to resolve this issue?
"
53689,The absence of ruy profiler header in gather.h file,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: x
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

The `gather.h` file of reference ops uses ruy profiler but it does not include the header file.
When I include `gather.h` file alone, it occurs an error because `ruy` has not been declared.

**Any other info / logs**

```
tensorflow/lite/kernels/internal/reference/gather.h:31:3: error: ‘ruy’ has not been declared
   ruy::profiler::ScopeLabel label(""Gather"");
```

Can I add the ruy header file to gather.h file?

https://github.com/tensorflow/tensorflow/blob/f6331b7c3da432f21ce12675b15e60ec59933e47/tensorflow/lite/kernels/internal/reference/gather.h#L18-L30

In other files in the same case, it explicitly contains a header file as shown below.

https://github.com/tensorflow/tensorflow/blob/f6331b7c3da432f21ce12675b15e60ec59933e47/tensorflow/lite/kernels/internal/reference/exp.h#L20-L29"
53688,LocallyConnected layers default initializer glorot_uniform determines fanning incorrectly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7 running NGC tensorflow_21.07-tf2-py3 container
- TensorFlow installed from (source or binary): binary, Nvidia container
- TensorFlow version (use command below): 2.5.0+nv
- Python version: 3.8.10
- CUDA/cuDNN version: 11.5
- GPU model and memory: Nvidia V100, 32 GB

**Describe the current behavior**
The Keras LocallyConnectednD layers all use default kernel_initializer glorot_uniform, just like their Conv counterparts. However, with the default implementation (1), the parameter tensor for e.g. LocallyConnected1D ends up being three-dimensional input-size * kernel-size * filters. In short, this means that the initialization values for a layer are vastly different if you just switch from Conv1D to LocallyConnected1D, to the point where gradients can easily vanish.

**Describe the expected behavior**
While a locally connected layer and a convolutional layer will have different training dynamics, the underlying argument for the Xavier initialization should result in initial tensors of similar magnitude, since the number of terms in the sum for each activation is identical.

GlorotUniform is a wrapper for VarianceScaling. VarianceScaling will use _compute_fans internally, which just stacks additional input dimensions on top in a multiplicative manner (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/initializers/initializers_v2.py#L1011). This has been confirmed in the master branch, as indicated in the link.

Due to compat reasons, I guess changing the default at this point could be hard, but with a clean slate the expected behavior would be that the default init for Conv layers and LocallyConnected layers would be identical. At this point, maybe a warning and comments in the docs for both LocallyConnected layers and the initializers on how the fanning is actually determined.



**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): adding a warning mechanism in the locally connected layers if used with a default initializer. This is not ideal, as even customized code could assume another definition of fanning.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1bLUD52-syj01zjHyVXhGzdYZggLFM-kf

"
53686,API generation makes inheritance relationship confusing after r2.6,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: Python 3.6.8

**Describe the current behavior**
I'm trying to select different algorithms by different optimizer, and got following unexpected behavior:
```python
import tensorflow as tf
from tensorflow.python.keras.optimizer_v2 import adam
from tensorflow.python.keras.optimizer_v2 import optimizer_v2

opt = tf.keras.optimizers.Adam(1E-3)
cond = isinstance(opt, optimizer_v2.OptimizerV2) # It expects true.
```

Before r2.6, everything works fine. But after r2.6, it will get false instead. I noticed that there is an API-gen process when exporting the APIs. And I make other tests.

```python
opt = tf.keras.optimizers.Adam(1E-3)
cond = isinstance(opt, optimizer_v2.OptimizerV2) # 2.6: false, 2.5: true
cond = isinstance(opt, tf.keras.optimizers.Optimizer) # 2.6: true, 2.5: true

opt = adam.Adam(1E-3)
cond = isinstance(opt, optimizer_v2.OptimizerV2) # 2.6: true, 2.5: true
cond = isinstance(opt, tf.keras.optimizers.Optimizer) # 2.6: false, 2.5: true
```

It seems that the APIs have an independent type system after r2.6. It makes code hard to manage when we need to extend some functionalities.
"
53685,How to debug xla .,"I merge the features of dynamic_partition in XLA from master(TF2.8) to TF1.15, and then the features report a error:
```
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Expected instruction to have shape equal to (s64[40960]{0}, s64[40960]{0}), actual shape is (s64[<=40960]{0}, s64[<=40960]{0}):
%tuple.78 = (s64[<=40960]{0}, s64[<=40960]{0}) tuple(s64[40960]{0} %dynamic-slice.64, s64[40960]{0} %dynamic-slice.67), metadata={op_name=""XLA_Retvals""}

Failed after pipeline-start
	 [[{{node cluster_0_1/xla_compile}}]]
	 [[cluster_0_1/merge_oidx_1/_7]]
  (1) Internal: Expected instruction to have shape equal to (s64[40960]{0}, s64[40960]{0}), actual shape is (s64[<=40960]{0}, s64[<=40960]{0}):
%tuple.78 = (s64[<=40960]{0}, s64[<=40960]{0}) tuple(s64[40960]{0} %dynamic-slice.64, s64[40960]{0} %dynamic-slice.67), metadata={op_name=""XLA_Retvals""}

Failed after pipeline-start
	 [[{{node cluster_0_1/xla_compile}}]]
0 successful operations.
```

How can I debug this error ? Please give me some ideas, thanks~!"
53684,framework not found TensorFlowLite,
53683,tf.data.dataset gather element,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Tensorflow 2.7
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
I want to do something like tf.gather on tf.data.Dataset. 
For example, 
dataset = tf.data.Dataset.from_tensor_slices(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'])
choice_dataset = tf.data.Dataset.range(3).repeat(2) # Define a dataset containing [0, 1, 2, 0, 1, 2]

I want it to output a dataset having ['a', 'b', 'c', 'a', 'b', 'c']

This is somewhat similar to choose_from_datasets, but that method choose from multiple datasets not element in the dataset

**Will this change the current api? How?** Add new method to tf.data.Dataset

**Who will benefit with this feature?** Tensorflow dataset user

**Any Other info.**
 I am not sure if there is a workaround. Please correct me if I am wrong.
"
53682,NNAPI on android 11 fails with movenet fp16 and int8 tflite models,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): aarch64 Android 11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I have issues while working with movenet tflite models on android 11 NNAPI 1.3
the movenet models used are sourced from tfhub:

1. [https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4](url)
2. [https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4](url) 


the above two singlepose movenet lighting tflite models are float16 and INT8 respectively, and I was trying to perform benchmarking of the same using the prebuilt benchmark model for android_aarch64 sourced from tflite website:
[https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model](url)

I am easily able to benchmark the models on CPU and GPU, but when I try to run it on NNAPI, the benchmarking fails, which is interesting because even if the model is not supported by NNAPI delegate, it should fallback on CPU which is not happening. These models fail to execute on NNAPI CPU as well which is strange.

log for lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite:
> $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite --use_nnapi=1 --nnapi_accelerator_name=nnapi-reference
STARTING!
Log parameter values verbosely: [0]
Graph: [lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite]
Use NNAPI: [1]
NNAPI accelerator name: [nnapi-reference]
NNAPI accelerators available: [nnapi-reference]
Loaded model lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
ERROR: NN API returned error ANEURALNETWORKS_BAD_DATA at line 992 while adding operation.
ERROR: Node number 303 (TfLiteNnapiDelegate) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
Failed to apply NNAPI delegate.
Benchmarking failed.

Node 303 is a GatherNd node, but I am not sure why it fails at this node because there are two more GatherNd nodes which come before node 303.

log for lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite:

>  $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite  --use_nnapi=1 --nnapi_accelerator_name=nnapi-reference
STARTING!
Log parameter values verbosely: [0]
Graph: [lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite]
Use NNAPI: [1]
NNAPI accelerator name: [nnapi-reference]
NNAPI accelerators available: [nnapi-reference]
Loaded model lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
ERROR: NN API returned error ANEURALNETWORKS_BAD_DATA at line 992 while adding operation.
ERROR: Node number 162 (TfLiteNnapiDelegate) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
Failed to apply NNAPI delegate.
Benchmarking failed.

Node 162 for this model is a separable_conv2d/bias node.

the logcat files for both the operations are attached in logs section. 
I get the same results if i remove the '--nnapi_accelerator_name=nnapi-reference', or add '--nnapi_allow_fp16=true' parameter, I still get the same benchmarking failed issue as above.

these movenet models work well with CPU and GPU:

1. lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite CPU 4 threads:
> $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite  --num_threads=4
STARTING!
Log parameter values verbosely: [0]
Num threads: [4]
Graph: [lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite]
#threads used for CPU inference: [4]
Loaded model lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite
INFO: Initialized TensorFlow Lite runtime.
The input model file size (MB): 2.89484
Initialized session in 4.574ms.


2. lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite  GPU:

> $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite  --use_gpu=1
STARTING!
Log parameter values verbosely: [0]
Graph: [lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite]
Use gpu: [1]
Loaded model lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Following operations are not supported by GPU delegate:
ARG_MAX: Operation is not supported.
CAST: Operation is not supported.
CONCATENATION: OP is supported, but tensor type isn't matched!
FLOOR_DIV: Operation is not supported.
GATHER_ND: Operation is not supported.
MUL: OP is supported, but tensor type isn't matched!
PACK: OP is supported, but tensor type isn't matched!
RESHAPE: OP is supported, but tensor type isn't matched!
SUB: OP is supported, but tensor type isn't matched!
UNPACK: Operation is not supported.
100 operations will run on the GPU, and the remaining 57 operations will run on the CPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.
The input model file size (MB): 2.89484
Initialized session in 2957.12ms.



3. lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite CPU 4 threads:

> $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite --num_threads=4
STARTING!
Log parameter values verbosely: [0]
Num threads: [4]
Graph: [lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite]
#threads used for CPU inference: [4]
Loaded model lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite
INFO: Initialized TensorFlow Lite runtime.
The input model file size (MB): 4.75851
Initialized session in 3.211ms.



4. lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite GPU:

> $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite --use_gpu=1
STARTING!
Log parameter values verbosely: [0]
Graph: [lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite]
Use gpu: [1]
Loaded model lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Following operations are not supported by GPU delegate:
ARG_MAX: Operation is not supported.
CAST: Operation is not supported.
CONCATENATION: OP is supported, but tensor type isn't matched!
DEQUANTIZE:
FLOOR_DIV: Operation is not supported.
GATHER_ND: Operation is not supported.
MUL: OP is supported, but tensor type isn't matched!
PACK: OP is supported, but tensor type isn't matched!
RESHAPE: OP is supported, but tensor type isn't matched!
SUB: OP is supported, but tensor type isn't matched!
UNPACK: Operation is not supported.
245 operations will run on the GPU, and the remaining 52 operations will run on the CPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.
The input model file size (MB): 4.75851
Initialized session in 2013.4ms.


to make sure that NNAPI is working for other models, I used a mobilenetv2 fp16 and int8 model from tfhub
1. mobilenetv2-coco_fp16 : [https://tfhub.dev/sayakpaul/lite-model/mobilenetv2-coco/fp16/1](url)
2. mobilenetv2-coco_int8 : [https://tfhub.dev/sayakpaul/lite-model/mobilenetv2-coco/int8/1](url)
and i face no issues running NNAPI CPU.

output for mobilenetv2-coco/fp16 for NNAPI CPU:

> $ ./android_aarch64_benchmark_model --graph=lite-model_mobilenetv2-coco_fp16_1.tflite   --use_nnapi=1 --nnapi_accelerator_name=nnapi-reference
STARTING!
Log parameter values verbosely: [0]
Graph: [lite-model_mobilenetv2-coco_fp16_1.tflite]
Use NNAPI: [1]
NNAPI accelerator name: [nnapi-reference]
NNAPI accelerators available: [nnapi-reference]
Loaded model lite-model_mobilenetv2-coco_fp16_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 11 delegate kernels.
The input model file size (MB): 4.2551
Initialized session in 99.601ms.


So there is something wrong in movenet models which is failing when using NNAPi instead of falling back onto CPU. One reason i can think of, after analysing the logfile is due to tensor type not matching for CONCATENATION op, but not sure.


**Describe the expected behavior**
The movenet models, even if not entirely supported on NNAPI should fallback on CPU. If fallback is disabled, but graph is forced through NNAPI CPU, it should still give similar results as it would have when running simply on CPU, but that is not observed.


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
log files:
[lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite_android_nnapi_logcat.txt](https://github.com/tensorflow/tensorflow/files/7825438/lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite_android_nnapi_logcat.txt)
[lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite_android_nnapi_logcat.txt](https://github.com/tensorflow/tensorflow/files/7825446/lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite_android_nnapi_logcat.txt)

"
53680,error executing command crosstool_wrapper_driver_is_not_gcc,"**System information**
- OS Platform and Distribution: Slackware64 Linux (current), kernel 5.15.12
- TensorFlow installed from: source
- TensorFlow version: 2.4.0
- Python version: 3.9
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 11.2.0
- CUDA/cuDNN version: 11.5.50, 8.3.1.22
- GPU model and memory: Nvidia Quadro RTX 4000, 8GB

**the problem**

     # bazel build --workspace_status_command=""bash native_client/bazel_workspace_status_cmd.sh"" --config=monolithic -c opt --copt=-O3 --copt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --copt=-fvisibility=hidden --define=no_tensorflow_py_deps=true --define=no_nccl_support=true //native_client:libdeepspeech.so --config=cuda

gives 

```
ERROR: ~/DeepSpeech/tensorflow/tensorflow/stream_executor/cuda/BUILD:469:11: C++ compilation of rule '//tensorflow/stream_executor/cuda:cupti_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/_objs/cupti_stub/cupti_stub.pic.d ... (remaining 141 argument(s) skipped)
In file included from ./tensorflow/core/platform/status.h:28,
                 from ./tensorflow/core/platform/errors.h:22,
                 from ./tensorflow/core/platform/env.h:27,
                 from ./tensorflow/stream_executor/lib/env.h:20,
                 from tensorflow/stream_executor/cuda/cupti_stub.cc:19:
bazel-out/k8-opt/bin/tensorflow/core/protobuf/error_codes.pb.h:17:2: error: #error This file was generated by an older version of protoc which is
   17 | #error This file was generated by an older version of protoc which is
      |  ^~~~~
bazel-out/k8-opt/bin/tensorflow/core/protobuf/error_codes.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please
   18 | #error incompatible with your Protocol Buffer headers. Please
      |  ^~~~~
bazel-out/k8-opt/bin/tensorflow/core/protobuf/error_codes.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.
   19 | #error regenerate this file with a newer version of protoc.
      |  ^~~~~
In file included from ./tensorflow/core/platform/status.h:28,
                 from ./tensorflow/core/platform/errors.h:22,
                 from ./tensorflow/core/platform/env.h:27,
                 from ./tensorflow/stream_executor/lib/env.h:20,
                 from tensorflow/stream_executor/cuda/cupti_stub.cc:19:
bazel-out/k8-opt/bin/tensorflow/core/protobuf/error_codes.pb.h:47:51: error: 'AuxillaryParseTableField' in namespace 'google::protobuf::internal' does not name a type; did you mean 'AuxiliaryParseTableField'?
   47 |   static const ::PROTOBUF_NAMESPACE_ID::internal::AuxillaryParseTableField aux[]
      |                                                   ^~~~~~~~~~~~~~~~~~~~~~~~
      |                                                   AuxiliaryParseTableField
Target //native_client:libdeepspeech.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 10.015s, Critical Path: 8.56s
INFO: 123 processes: 98 internal, 25 local.
FAILED: Build did NOT complete successfully
```"
53672,Higher validation and test loss and lower accuracy using tf.data.dataset with tfrecords than using numpy,"**System information**
- OS Platform and Distribution : Kaggle and Colab gpu/tpu
- TensorFlow version (use command below): 2.4 on Kaggle, 2.7 on Colab
- GPU model and memory:tesla p100, colab tpu, kaggle tpu

Hi, I'm training and testing an LSTM network using kaggle notebooks. 
To improve time performance I have converted my numpy dataset to TFRecords, but the results are very differents respects results obtained with numpy dataset. I noticed this behavior either on GPU and TPU on Kaggle and Google Colab. 
I want to specify that the hyperparametrs and the dataset order is the same with numpy method and td.data.
I repeated the tests on differents file of the dataset many times but each time the validation and test accuracy are lower  and validation and test loss are higher using tf.data rsepect to using numpy.

With tf.data:
![accuracy](https://user-images.githubusercontent.com/48318112/148437400-86932751-279e-4ae3-96de-2a9c7c3589e3.png)
![loss](https://user-images.githubusercontent.com/48318112/148437401-a0529b6b-71f7-4d56-8604-86914565cc2f.png)

With numpy dataset:
![accuracy](https://user-images.githubusercontent.com/48318112/148437518-2eb912f8-5f37-496a-a0e6-a393b9b64d2a.png)
![loss](https://user-images.githubusercontent.com/48318112/148437522-f79ecc38-6681-42df-91ab-57fb00c0afdc.png)

Execution with tf.data:
```
Epoch 1/18
11929/11929 [==============================] - 323s 26ms/step - loss: 0.1282 - binary_accuracy: 0.9558 - auc: 0.8314 - precision: 0.7457 - recall: 0.6855 - val_loss: 1.6054 - val_binary_accuracy: 0.6283 - val_auc: 0.6912 - val_precision: 0.9549 - val_recall: 0.0453
2022-01-06 15:14:59.141498: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 146116, Output num: 3
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1641482099.138121196"",""description"":""Error received from peer ipv4:10.0.0.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 146116, Output num: 3"",""grpc_status"":3}
Epoch 2/18
11929/11929 [==============================] - 303s 25ms/step - loss: 0.0450 - binary_accuracy: 0.9862 - auc: 0.8506 - precision: 0.8013 - recall: 0.7681 - val_loss: 1.4894 - val_binary_accuracy: 0.7084 - val_auc: 0.8023 - val_precision: 0.9523 - val_recall: 0.2626
Epoch 3/18
11929/11929 [==============================] - 307s 26ms/step - loss: 0.0294 - binary_accuracy: 0.9915 - auc: 0.8516 - precision: 0.8214 - recall: 0.8078 - val_loss: 1.4176 - val_binary_accuracy: 0.7219 - val_auc: 0.8154 - val_precision: 0.9737 - val_recall: 0.2922
Epoch 4/18
11929/11929 [==============================] - 308s 26ms/step - loss: 0.0186 - binary_accuracy: 0.9947 - auc: 0.8534 - precision: 0.8326 - recall: 0.8298 - val_loss: 1.2668 - val_binary_accuracy: 0.7506 - val_auc: 0.8195 - val_precision: 0.9890 - val_recall: 0.3619
Epoch 5/18
11929/11929 [==============================] - 306s 26ms/step - loss: 0.0142 - binary_accuracy: 0.9961 - auc: 0.8542 - precision: 0.8387 - recall: 0.8378 - val_loss: 1.2077 - val_binary_accuracy: 0.7710 - val_auc: 0.8313 - val_precision: 0.9954 - val_recall: 0.4125
Epoch 6/18
11929/11929 [==============================] - 304s 25ms/step - loss: 0.0126 - binary_accuracy: 0.9965 - auc: 0.8548 - precision: 0.8432 - recall: 0.8397 - val_loss: 1.1289 - val_binary_accuracy: 0.7836 - val_auc: 0.8471 - val_precision: 0.9974 - val_recall: 0.4442
Epoch 7/18
11929/11929 [==============================] - 303s 25ms/step - loss: 0.0094 - binary_accuracy: 0.9975 - auc: 0.8555 - precision: 0.8470 - recall: 0.8455 - val_loss: 0.9592 - val_binary_accuracy: 0.8196 - val_auc: 0.8885 - val_precision: 0.9983 - val_recall: 0.5365
Epoch 8/18
11929/11929 [==============================] - 304s 26ms/step - loss: 0.0085 - binary_accuracy: 0.9978 - auc: 0.8559 - precision: 0.8478 - recall: 0.8463 - val_loss: 0.7648 - val_binary_accuracy: 0.8320 - val_auc: 0.9281 - val_precision: 0.9983 - val_recall: 0.5686
Epoch 9/18
11929/11929 [==============================] - 305s 26ms/step - loss: 0.0070 - binary_accuracy: 0.9982 - auc: 0.8561 - precision: 0.8498 - recall: 0.8481 - val_loss: 0.6531 - val_binary_accuracy: 0.8376 - val_auc: 0.9435 - val_precision: 0.9986 - val_recall: 0.5828
Epoch 10/18
11929/11929 [==============================] - 304s 25ms/step - loss: 0.0062 - binary_accuracy: 0.9984 - auc: 0.8565 - precision: 0.8506 - recall: 0.8491 - val_loss: 0.6175 - val_binary_accuracy: 0.8421 - val_auc: 0.9462 - val_precision: 0.9987 - val_recall: 0.5943
Epoch 11/18
11929/11929 [==============================] - 307s 26ms/step - loss: 0.0048 - binary_accuracy: 0.9987 - auc: 0.8567 - precision: 0.8514 - recall: 0.8501 - val_loss: 0.5576 - val_binary_accuracy: 0.8626 - val_auc: 0.9529 - val_precision: 0.9990 - val_recall: 0.6470
Epoch 12/18
11929/11929 [==============================] - 309s 26ms/step - loss: 0.0044 - binary_accuracy: 0.9988 - auc: 0.8568 - precision: 0.8520 - recall: 0.8514 - val_loss: 0.4787 - val_binary_accuracy: 0.8744 - val_auc: 0.9641 - val_precision: 0.9992 - val_recall: 0.6773
Epoch 13/18
11929/11929 [==============================] - 308s 26ms/step - loss: 0.0040 - binary_accuracy: 0.9989 - auc: 0.8568 - precision: 0.8526 - recall: 0.8521 - val_loss: 0.4774 - val_binary_accuracy: 0.8818 - val_auc: 0.9635 - val_precision: 0.9991 - val_recall: 0.6965
Epoch 14/18
11929/11929 [==============================] - 306s 26ms/step - loss: 0.0036 - binary_accuracy: 0.9990 - auc: 0.8568 - precision: 0.8528 - recall: 0.8522 - val_loss: 0.4594 - val_binary_accuracy: 0.8837 - val_auc: 0.9663 - val_precision: 0.9992 - val_recall: 0.7011
Epoch 15/18
11929/11929 [==============================] - 307s 26ms/step - loss: 0.0035 - binary_accuracy: 0.9990 - auc: 0.8568 - precision: 0.8529 - recall: 0.8517 - val_loss: 0.4588 - val_binary_accuracy: 0.8946 - val_auc: 0.9615 - val_precision: 0.9992 - val_recall: 0.7294
Epoch 16/18
11929/11929 [==============================] - 305s 26ms/step - loss: 0.0031 - binary_accuracy: 0.9991 - auc: 0.8568 - precision: 0.8534 - recall: 0.8529 - val_loss: 0.3980 - val_binary_accuracy: 0.9048 - val_auc: 0.9642 - val_precision: 0.9994 - val_recall: 0.7553
Epoch 17/18
11929/11929 [==============================] - 304s 26ms/step - loss: 0.0030 - binary_accuracy: 0.9991 - auc: 0.8569 - precision: 0.8532 - recall: 0.8528 - val_loss: 0.3670 - val_binary_accuracy: 0.9098 - val_auc: 0.9685 - val_precision: 0.9993 - val_recall: 0.7683
Epoch 18/18
11929/11929 [==============================] - 306s 26ms/step - loss: 0.0028 - binary_accuracy: 0.9992 - auc: 0.8569 - precision: 0.8533 - recall: 0.8526 - val_loss: 0.3948 - val_binary_accuracy: 0.9091 - val_auc: 0.9655 - val_precision: 0.9992 - val_recall: 0.7667
3497/3497 [==============================] - 83s 23ms/step - loss: 0.4007 - binary_accuracy: 0.9079 - auc: 0.9650 - precision: 0.9993 - recall: 0.7645
2022-01-06 16:43:10.007313: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1641487390.006754880"",""description"":""Error received from peer ipv4:10.0.0.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3"",""grpc_status"":3}
```
Only with Kaggle TPU i have the following error as you can see just after first epoch validation and during testing, but the training run correctly and the program don't crash:

```
W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1641487390.006754880"",""description"":""Error received from peer ipv4:10.0.0.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3"",""grpc_status"":3}
```
This problem occour also without this error on Kaggle GPU and Colab, so this error is specific on platform TPU Kaggle and is not related with the problem.

In the following link you can find my code with tfrecord write and read and all the process to get the dataset from tfrecord, model creation, fitting and testing:
[link code](https://colab.research.google.com/drive/1goQZL3xRU2vkBos04PZkWL2o1qHv8qul?usp=sharing)

Seems like to with tf.data the training start with lower validation accuracy value and increase more slowly compared to numpy dataset, so with a lot of epochs maybe also with tf.data can reach numpy accuracy level, is this comportament normal using tf.data or not?
"
53671,Using tf.device when creating Keras layers does not move layer computation to that device,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud AI Platform (TensorFlow Enterprise 2.7)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: Python 3.7.12
- CUDA/cuDNN version: 11.3, V11.3.109
- GPU model and memory: NVIDIA Tesla K80 (x4)


**Describe the current behavior**

When creating a Keras model using the `tf.device` context manager, the resources used by that layer are placed on the requested device, but the TensorFlow ops involved in that layer do not execute on that device.

**Describe the expected behavior**

Using the `tf.device` context manager when constructing a Keras layer should perform that layer's computation on the specified device.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
tf.debugging.set_log_device_placement(True)
        
_input = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)
x = _input
with tf.device(""/GPU:1""):
    x = tf.keras.layers.Dense(10, name=""should_be_on_gpu"")(x)
    x = tf.keras.layers.Dense(10, name=""should_be_on_gpu_2"")(x)
model = tf.keras.models.Model(inputs=[_input], outputs=[x])
model.compile('adam', 'mse')
model.summary()
model.fit([2], [4])
```

Hundreds of log lines are printed showing the placement of each op on device, but crucially, the `/GPU:1` device stores the `Dense` layers (i.e.: `ReadVariableOp`) but is _not_ where the computation (`MatMul`, in this case) happens:

```
model_3/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/device:GPU:0
model_3/Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0
model_3/should_be_on_gpu/MatMul/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:1
model_3/should_be_on_gpu/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
model_3/should_be_on_gpu/BiasAdd/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:1
model_3/should_be_on_gpu/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0
model_3/should_be_on_gpu_2/MatMul/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:1
model_3/should_be_on_gpu_2/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
model_3/should_be_on_gpu_2/BiasAdd/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:1
model_3/should_be_on_gpu_2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0
```"
53668,Compiling tensorflow/core/kernels/bincount_op.cc failed: (Exit 1): clang failed: error executing command,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.7
- Python version: 3.9
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source): 10.1.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Try to build the libtensorflowlite.so with select-tf-ops. I add `""//tensorflow/lite/delegates/flex:delegate""` to tensorflow/lite/BUILD. Then the Compilation failed.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
1. git clone https://github.com/tensorflow/tensorflow
2. cd tensorflow && ./configure
3. vim tensorflow/lite/BUILD && add ""//tensorflow/lite/delegates/flex:delegate"" to the deps of tensorflowlite
4. bazel build -c opt --cxxopt=--std=c++11 --fat_apk_cpu=arm64-v8a --config=android_arm64 //tensorflow/lite:libtensorflowlite.so --verbose_failures
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Full log:

> INFO: Options provided by the client:
>   Inherited 'common' options: --isatty=1 --terminal_columns=146
> INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.bazelrc:
>   Inherited 'common' options: --experimental_repo_remote_exec
> INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.bazelrc:
>   'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
> INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.tf_configure.bazelrc:
>   'build' options: --action_env PYTHON_BIN_PATH=/root/miniconda2/envs/py36/bin/python3 --action_env PYTHON_LIB_PATH=/root/miniconda2/envs/py36/lib/python3.6/site-packages --python_path=/root/miniconda2/envs/py36/bin/python3 --define=PREFIX=/data/offline/deps/prefix --action_env ANDROID_NDK_HOME=/data/offline/deps/android-ndk-r21e --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=30.0.2 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/opt/android-sdk
> INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.bazelrc:
>   'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
> INFO: Found applicable config definition build:short_logs in file /data/offline/deps/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
> INFO: Found applicable config definition build:v2 in file /data/offline/deps/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
> INFO: Found applicable config definition build:android_arm64 in file /data/offline/deps/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
> INFO: Found applicable config definition build:android in file /data/offline/deps/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --define=with_xla_support=false
> INFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (76 packages loaded, 4130 targets configured).
> INFO: Found 1 target...
> ERROR: /data/offline/deps/tensorflow/tensorflow/core/kernels/BUILD:6848:11: Compiling tensorflow/core/kernels/bincount_op.cc failed: (Exit 1): clang failed: error executing command 
>   (cd /root/.cache/bazel/_bazel_root/c8bef3cfd9e610c81b071433025995b0/execroot/org_tensorflow && \
>   exec env - \
>     ANDROID_BUILD_TOOLS_VERSION=30.0.2 \
>     ANDROID_NDK_API_LEVEL=21 \
>     ANDROID_NDK_HOME=/data/offline/deps/android-ndk-r21e \
>     ANDROID_SDK_API_LEVEL=30 \
>     ANDROID_SDK_HOME=/opt/android-sdk \
>     LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64 \
>     PATH=/data/offline/deps/cmdline-tools/tools:/data/offline/deps/cmdline-tools/tools/bin:/data/offline/deps/cmdline-tools/platform-tools:/root/miniconda2/envs/py36/bin:/root/miniconda2/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/cuda-11.0/bin:/usr/local/cuda/bin:/root/.ft:/root/.local/bin:/usr/local/bin:/root/bin:/usr/local/git/bin \
>     PWD=/proc/self/cwd \
>     PYTHON_BIN_PATH=/root/miniconda2/envs/py36/bin/python3 \
>     PYTHON_LIB_PATH=/root/miniconda2/envs/py36/lib/python3.6/site-packages \
>     TF2_BEHAVIOR=1 \
>   external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android '-D__ANDROID_API__=21' -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig '-Werror=return-type' '-Werror=int-to-pointer-cast' '-Werror=pointer-to-int-cast' '-Werror=implicit-function-declaration' -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/bincount_op.pic.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/bincount_op.pic.o' -fPIC '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' '-DS_IEXEC=S_IXUSR' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DSUPPORT_SELECTIVE_REGISTRATION -iquote . -iquote bazel-out/arm64-v8a-opt/bin -iquote external/gif -iquote bazel-out/arm64-v8a-opt/bin/external/gif -iquote external/eigen_archive -iquote bazel-out/arm64-v8a-opt/bin/external/eigen_archive -iquote external/com_google_absl -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/arm64-v8a-opt/bin/external/nsync -iquote external/libjpeg_turbo -iquote bazel-out/arm64-v8a-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf -iquote external/double_conversion -iquote bazel-out/arm64-v8a-opt/bin/external/double_conversion -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm64-v8a-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/bin/external/farmhash_archive -iquote external/png -iquote bazel-out/arm64-v8a-opt/bin/external/png -iquote external/zlib -iquote bazel-out/arm64-v8a-opt/bin/external/zlib -iquote external/highwayhash -iquote bazel-out/arm64-v8a-opt/bin/external/highwayhash -iquote external/icu -iquote bazel-out/arm64-v8a-opt/bin/external/icu -iquote external/fft2d -iquote bazel-out/arm64-v8a-opt/bin/external/fft2d -iquote external/gemmlowp -iquote bazel-out/arm64-v8a-opt/bin/external/gemmlowp -isystem external/gif -isystem bazel-out/arm64-v8a-opt/bin/external/gif -isystem external/eigen_archive -isystem bazel-out/arm64-v8a-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/arm64-v8a-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/farmhash_archive/src -isystem external/png -isystem bazel-out/arm64-v8a-opt/bin/external/png -isystem external/zlib -isystem bazel-out/arm64-v8a-opt/bin/external/zlib -isystem external/icu/icu4c/source/common -isystem bazel-out/arm64-v8a-opt/bin/external/icu/icu4c/source/common -w '--std=c++11' '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/core/kernels/bincount_op.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/bincount_op.pic.o)
> Execution platform: @local_execution_config_platform//:platform
> Target //tensorflow/lite:libtensorflowlite.so failed to build
> INFO: Elapsed time: 942.540s, Critical Path: 321.94s
> INFO: 1130 processes: 48 internal, 1082 local.
> FAILED: Build did NOT complete successfully
> 
> "
53665,Do we have any way to avoid memory copy in tf.convert_to_tensor ?,"**System information**
- TensorFlow version (use command below): 2.4.1

**Describe the current behavior**

Code:

```python
@tf.function
def inference(model, image):
    return model(image)


def test_model():
    model = tf.keras.Sequential([tf.keras.layers.Conv2D(filters=3, kernel_size=3)])

    np_image = np.random.randn(256, 256, 256, 3)
    model(np_image)
    # passing a python object to tf.function is not a recommend way, so this is slow.
    inference(model, np_image)

    tf_image = tf.convert_to_tensor(np_image)
    inference(model, tf_image)

    total_time = 0
    for _ in range(10):
        np_image = np.random.randn(256, 256, 256, 3)
        start_time = time.time()
        model(np_image)
        total_time += time.time() - start_time
    print(""inference by model: {0} s"".format(total_time))

    total_time = 0
    for _ in range(10):
        np_image = np.random.randn(256, 256, 256, 3)
        start_time = time.time()
        inference(model, np_image)
        total_time += time.time() - start_time
    print(""inference by np_image: {0} s"".format(total_time))

    total_time = 0
    for _ in range(10):
        np_image = np.random.randn(256, 256, 256, 3)
        tf_image = tf.convert_to_tensor(np_image)
        start_time = time.time()
        inference(model, tf_image)
        total_time += time.time() - start_time
    print(""inference by tf_image: {0} s"".format(total_time))
```

The result:

```txt
inference by model: 2.4521803855895996 s
inference by np_image: 2.472818374633789 s
inference by tf_image: 0.007699012756347656 s, convert time: 2.4546449184417725 s
```

My question is:

1. What does  `tf.convert_to_tensor` do ? Does it copy cpu data from numpy to tf.Tensor and also copy it to GPU ?
2. How can I avoid memory copy when using tf.convert_to_tensor ? In my case I will not modify the input image at all, so it's safe for me to make a memory view for the numpy data to tf.Tensor.
I can restrict the numpy data to be continuous and C order if it is needed.

From official docs I can find that it's recommended to create data from tf.data.Dataset which is already a tf.Tensor. But in my case we cannot use tf.data.Dataset API because I want to use some data which is mutable when training.

The alternative is to produce tf.Tensor by multiprocessing and push it to main process for training, but it will dump and reload the tf.Tensor object when using multiprocessing.Queue.
And I cannot run `tf.queue.FIFOQueue` in multiprocessing. Is this a good way to sharing tensor between different processes? (For numpy, we can use memmap to share numpy data.)

**Describe the expected behavior**

My expected behavior is that the convert time is very small.
"
53664,"When allocate_output 200MB tensor, op kernel will execute twice","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.15


The op code is as follows:
```
REGISTER_OP(""ZeroOut"")
    .Input(""to_zero: int32"")
    .Output(""zeroed: int32"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c->set_output(0, c->input(0));
      return Status::OK();
    });

int times = 0;

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    LOG(INFO) << ""execute times:"" << (++times);

    Tensor* output_tensor = NULL;
    //right: 
    //OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
    //                                                 &output_tensor));
    //wrong: 
    OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({200000000, 1}), &output_tensor));
  }
};
REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);

```


The test code is as follows:
```
import tensorflow as tf
import heap.tensorflow as hp

result = hp.zero_out([5, 4, 3, 2, 1])
with tf.Session() as sess:
  sess.run(result)
```

**Describe the current behavior**
![image](https://user-images.githubusercontent.com/70072713/148356663-33e93475-a92d-43a9-8c71-71e0e0b40142.png)


"
53663,Cross-compilation error by Bazel of TensorFlow Lite in 2.7,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.7
- Python version: 3.9
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source): 10.1
- CUDA/cuDNN version: no
- GPU model and memory: no



**Describe the problem**
I try to build libtensorflowlite.so for android and it return an error.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
1. git clone https://github.com/tensorflow/tensorflow
2. cd tensorflow && ./configure
3. bazel build -c opt --cxxopt=--std=c++11 --fat_apk_cpu=arm64-v8a --config=android_arm64 //tensorflow/lite:libtensorflowlite.so
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
The full log:
```
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=146
INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/root/miniconda2/envs/py36/bin/python3 --action_env PYTHON_LIB_PATH=/root/miniconda2/envs/py36/lib/python3.6/site-packages --python_path=/root/miniconda2/envs/py36/bin/python3 --define=PREFIX=/data/offline/deps/prefix --action_env ANDROID_NDK_HOME=/data/offline/deps/android-ndk-r21e --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=30.0.2 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/opt/android-sdk
INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /data/offline/deps/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /data/offline/deps/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm64 in file /data/offline/deps/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /data/offline/deps/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --define=with_xla_support=false
INFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /data/offline/deps/tensorflow/tensorflow/lite/schema/BUILD:98:22: Generating flatbuffer files for schema_fbs_srcs: //tensorflow/lite/schema:schema_fbs_srcs failed: (Exit 126): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)
/bin/bash: bazel-out/host/bin/external/flatbuffers/flatc: cannot execute binary file
Target //tensorflow/lite:libtensorflowlite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 0.242s, Critical Path: 0.01s
INFO: 2 processes: 2 internal.
FAILED: Build did NOT complete successfully
```"
53662,Resurrecting Swift bindings / how much of the core C++ API can run on iOS?,"I'm directly addressing TensorFlow's admins and maintainers of the core C/C++ code with this issue.

Before starting out, I am not using TensorFlow for _inference_ on iOS. I am doing _training_, which is orders of magnitude more difficult to support. It has never been done on a high-level, feature-complete ML framework except for [DL4S](https://github.com/palle-k/dl4s) (MPSGraph and ArrayFire don't count). Using an iPhone or iPad for training may seem strange, but I have numerous reasons for doing so (leaving out those reasons for brevity).

The existing TensorFlow training API is only compatible with macOS, Linux, and Windows. I am concerned with the C API used internally by [Swift for TensorFlow](https://github.com/tensorflow/swift-apis). iOS can't run Python, which means it certainly can't run the TensorFlow in this repository. However, Swift for TensorFlow brings an opportunity to take Python out of the equation. C and C++ can run on iOS, and I assume a lot of the code is platform-agnostic. Has anyone ever succeeded in training TensorFlow models on an Android device?

Apple released a PluggableDevice implementation of GPU acceleration for Macs. However, it's closed-source, missing several ops it could support, and shuts out low-end Intel Macs. I'm looking for universal training support just like [DL4S](https://github.com/palle-k/dl4s). Is there any chance that I could reuse the C TensorFlow API as a component of my Metal backend for iOS?

This would save time and make DirectML acceleration for Swift for TensorFlow easier. Also, there are several ops like Cholesky grad which aren't in Metal Performance Shaders or Accelerate. I don't have the slightest clue how to implement them in CPU code. I assume that TensorFlow's core C API contains CPU kernels for those obscure ops.

One more thing - if I could use PluggableDevice for my backend, it would be much more in line with TensorFlow and less likely that my S4TF branch would diverge from main and eventually die. I'm approaching this  with the objective of realistically resurrecting S4TF. Whether TensorFlow's owners can change their mind and officially support my work has a massive impact on its long-term viability. How can I contact the people who have authority over this?"
53661,OSError: SavedModel file does not exist at: final_model_weights.hdf5/{saved_model.pbtxt|saved_model.pb},"Hey a have a problem whn i try to load the module :

my code : index.py

```
def getPrediction(filename):
     model = tf.keras.models.load_model(""final_model_weights.hdf5"")
     img = load_img('static/'+filename, target_size=(180, 180))
     img = img_to_array(img)
     img = img / 255
     img = np.expand_dims(img,axis=0)
     category = model.predict_classes(img)
     answer = category[0]
     probability = model.predict(img)
     probability_results = 0

     if answer == 1:
          answer = ""Recycle""
          probability_results = probability[0][1]
     else:
          answer = ""Organic""
          probability_results = probability[0][0]

     answer = str(answer)
     probability_results=str(probability_results)

     values = [answer, probability_results, filename]
     return values[0], values[1], values[2]
```

when i upload the image to my code i get this error :

```
OSError
OSError: SavedModel file does not exist at: final_model_weights.hdf5/{saved_model.pbtxt|saved_model.pb}

Traceback (most recent call last)
File ""/home/devokba/.local/lib/python3.9/site-packages/flask/app.py"", line 2091, in __call__
return self.wsgi_app(environ, start_response)
File ""/home/devokba/.local/lib/python3.9/site-packages/flask/app.py"", line 2076, in wsgi_app
response = self.handle_exception(e)
File ""/home/devokba/.local/lib/python3.9/site-packages/flask/app.py"", line 2073, in wsgi_app
response = self.full_dispatch_request()
File ""/home/devokba/.local/lib/python3.9/site-packages/flask/app.py"", line 1518, in full_dispatch_request
rv = self.handle_user_exception(e)
File ""/home/devokba/.local/lib/python3.9/site-packages/flask/app.py"", line 1516, in full_dispatch_request
rv = self.dispatch_request()
File ""/home/devokba/.local/lib/python3.9/site-packages/flask/app.py"", line 1502, in dispatch_request
return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)
File ""/home/devokba/Documents/waste-classification-model/index.py"", line 42, in submit_image
getPrediction(filename)
File ""/home/devokba/Documents/waste-classification-model/main.py"", line 14, in getPrediction
model = tf.keras.models.load_model(""final_model_weights.hdf5"")
File ""/home/devokba/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
raise e.with_traceback(filtered_tb) from None
File ""/home/devokba/.local/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 118, in parse_saved_model
raise IOError(
OSError: SavedModel file does not exist at: final_model_weights.hdf5/{saved_model.pbtxt|saved_model.pb}
The debugger caught an exception in your WSGI application. You can now look at the traceback which led to the error.
To switch between the interactive traceback and the plaintext one, you can click on the ""Traceback"" headline. From the text traceback you can also create a paste of it. For code execution mouse-over the frame you want to debug and click on the console icon on the right side.

You can execute arbitrary Python code in the stack frames and there are some extra helpers available for introspection:

dump() shows all variables in the frame
dump(obj) dumps all that's known about the object


```

structure of my code : 
```
┣ Resources/
┃ ┣ Dataset/
┃ ┣ Images/
┃ ┗ Model/
┣ __pycache__/
┃ ┣ main.cpython-38.pyc
┃ ┗ main.cpython-39.pyc
┣ static/
┃ ┣ Empty_Coca_Cola_Bottle.jpg
┃ ┣ O_1.jpg
┃ ┣ O_2.jpg
┃ ┗ img1.jpeg
┣ templates/
┃ ┗ index.html
┣ .gitattributes
┣ Procfile.txt
┣ Project 3 Proposal.docx
┣ README.md
┣ final_model.ipynb
┣ final_model_weights.hdf5
┣ index.py
┣ main.py
┣ presentation_notebook.ipynb
┗ requirements.txt

```
any solution for that ??"
53660,`tf.sparse.split` crashes when axis is a tuple,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
data = tf.random.uniform([1, 32, 32], dtype=tf.float32)
axis = [1, 2]
x = tf.sparse.from_dense(data)
result = tf.sparse.split(x,3, axis=axis) # crash
```
Session crashes. `tf.sparse.split` failed to do proper checking for `axis`.

**Describe the expected behavior**
`tf.sparse.split` should raise `InvalidArgumentError` when `axis` is not a 0-D tensor, instead of crashing."
53658,tf.sparse.softmax randomly outputs wrong results!,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
logits = tf.random.uniform([248, 248, 1, 16], dtype=tf.float32)
r1 = tf.nn.softmax(logits,axis=-1)
logits_sp = tf.sparse.from_dense(logits)
r2 = tf.sparse.softmax(logits_sp)
r3 = tf.sparse.to_dense(r2)
assert tf.math.reduce_all(tf.math.equal(r1, r3))
```
Normally the assertion would pass because `tf.sparse.softmax` and `tf.nn.softmax` should have the same output for the same input value. However, if I run the above code for 100 times then sometimes the assertion fails! Please see the [gist](https://colab.research.google.com/gist/ArrowIntoTheSky/fed8a0babf38a699ce40ecf59f70dbb6/untitled0.ipynb?authuser=2#scrollTo=PgfFQVFeCyLF) here for reference.


"
53657,tf.sparse.softmax lack support for float16,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
logits = tf.random.uniform([16, 1, 10], dtype=tf.float16)
r1 = tf.nn.softmax(logits,axis=-1) # pass
logits_sp = tf.sparse.from_dense(logits)
r2 = tf.sparse.softmax(logits_sp) # InvalidArgumentError
```

**Describe the current behavior**
`tf.sparse.softmax` cannot accept a tensor of type `float16`. However, `tf.nn.softmax` do support `half`. 
For the above code snippet, the error message is:
```
InvalidArgumentError: Value for attr 'T' of half is not in the list of allowed values: float, double
	; NodeDef: {{node SparseSoftmax}}; Op<name=SparseSoftmax; signature=sp_indices:int64, sp_values:T, sp_shape:int64 -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]> [Op:SparseSoftmax]
```

**Describe the expected behavior**
According to the document for `tf.sparse.softmax`, it is equivalent to `tf.nn.softmax` (but for sparse tensors), so `tf.sparse.softmax` should also support `float16` inputs.
"
53655,tf.sparse.segment_sum doesn't support complex dtypes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
data = tf.complex(tf.random.uniform([3, 4], dtype=tf.float64),tf.random.uniform([3, 4], dtype=tf.float64))
segment_ids = [0,0,1]
res = tf.math.segment_sum(data=data,segment_ids=segment_ids) # pass
res_sp = tf.sparse.segment_sum(data=data,indices=tf.constant([0, 1, 2]),segment_ids=segment_ids) # InvalidArgumentError
```

**Describe the current behavior**
`tf.sparse.segment_sum` cannot accept a tensor of type `complex128`. However, `tf.math.segment_sum` do support it. 
For the above code snippet, the error message is:
```
InvalidArgumentError: Value for attr 'T' of complex128 is not in the list of allowed values: float, double, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64
	; NodeDef: {{node SparseSegmentSum}}; Op<name=SparseSegmentSum; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> [Op:SparseSegmentSum]
```

**Describe the expected behavior**
`tf.sparse.segment_sum` should also support complex dtypes. Actually I would expect the valid types of `data` of `tf.sparse.segment_sum`  to be the same as `tf.math.segment_sum`, since the document of `tf.sparse.segment_sum` does not have this information.
"
53653,tf.sparse.to_dense don't support complex dtypes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
x = tf.cast(tf.constant([1.0, 2.0]), tf.complex128)
x_sparse = tf.sparse.from_dense(x)
print(""from_dense pass:"", x_sparse) # pass
x_dense = tf.sparse.to_dense(x_sparse) # fail
print(""to_dense pass:"", x_dense) 
```
**Describe the current behavior**
`tf.sparse.from_dense` can convert a complex dense tensor to a sparse tensor, however, `tf.sparse.to_dense` fails to convert a complex sparse tensor back to a dense tensor.
For the above code snippet, the output is:
```
from_dense pass: SparseTensor(...)
NotFoundError: Could not find device for node: {{node SparseToDense}} = SparseToDense[T=DT_COMPLEX128, Tindices=DT_INT64, validate_indices=true]
All kernels registered for op SparseToDense:
  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT8]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT8]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_UINT8]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_UINT8]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT16]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT16]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_UINT16]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_UINT16]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_UINT32]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_UINT32]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_UINT64]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_UINT64]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT32]
 [Op:SparseToDense]              
```
**Describe the expected behavior**
`tf.sparse.to_dense` should also support complex dtypes."
53651,tf.bincount outputs wrong results,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
```
import tensorflow as tf
v = tf.constant([[0, 1, 2],
       [3, 4, 5]], dtype=tf.int64)
res1 = tf.math.bincount(v,axis=-1,maxlength=0, binary_output=True)
res2 = tf.sparse.bincount(v,axis=-1,maxlength=0, binary_output=True)
print(res1)
print(res2)
```

**Current output**
```
tf.Tensor([], shape=(2, 0), dtype=int32)
SparseTensor(indices=tf.Tensor(
[[0 0]
 [0 1]
 [0 2]
 [1 3]
 [1 4]
 [1 5]], shape=(6, 2), dtype=int64), values=tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int64), dense_shape=tf.Tensor([2 6], shape=(2,), dtype=int64))
```

**Expected output**
For `tf.sparse.bincount`, the output should have length at most `maxlength`, in the above code `maxlength` is `0`, so the output should be an empty tensor.
(`tf.math.bincount` is just to show that the output for `tf.sparse.bincount` should be have _same length_ as `tf.math.bincount`. )"
53650,Question on implicit transpose in HLO bitcast/copy/reshape instruction #2,"In below dumped hlo instruction:
```
%bitcast.94 = f16[1,128,2,64]{3,1,2,0} bitcast(f16[1,2,128,64]{3,2,1,0} %bitcast.50)
```

both tensor shape and memory layout are permuted between dimension 1 and dimension 2.

However in this instruction
```
%copy.6 = f16[2,128,128]{2,1,0} copy(f16[2,128,128]{1,2,0} %bitcast.70)
```

Dimension 1 and dimension 2 have same size, I cannot infer from dumped IR about whether tensor share are permuted between dim 1 and dim 2.

May I ask how can I get complete information shape/layout permutation?

"
53649,Question on implicit transpose in HLO bitcast/copy/reshape instruction,"HLO instructions like bitcast/copy/reshape often conduct implicit transpose. I am confusing on how the reshape/transpose performed exactly, as I am not aware of they are documented anywhere else including official operation semantics. 

Take tensor A[2,3]{1,0} as an example, suppose tensor A have element [[1,2,3], [4,5,6]] with memory layout [1,2,3,4,5,6], consider following two transposition performed by bitcast/copy/reshape:

1. %bitcast = f32[2,3]{0,1} bitcast(f32[2,3]{1, 0} %A)
we get tensor elements [[1,2,3], [4,5,6]] with memory layout [1,4,2,5,3,6] 
2. %bitcast = f32[3,2]{0,1} bitcast(f32[2,3]{1, 0} %A)
For this case both tensor shape and memory layout are permutes, I suppose technically tensor A first reshape to [3,2] (get [3,2]{1,0}), then permute memory layout to get the final result.
So the process would be [[1,2,3], [4,5,6]] [1,2,3,4,5,6]  --reshape--> [[1,2],[3,4],[5,6]] [1,2,3,4,5,6] --transpose--> [[1,2],[3,4],[5,6]] [1,3,5,2,4,6]

Am I understanding implicit transpose correctly?

Thanks a lot:)"
53648,Unit test builds broken by recent commit,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.8.10
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

Commit https://github.com/tensorflow/tensorflow/commit/fea79a29ad16aa3081e2c0c1c6cf8b81771d46b1 introduced an error that prevents the unit test build from completing

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64 --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64 --copt=-ffp-contract=off --verbose_failures -- //tensorflow/python/... -//tensorflow/python/tools/... -//tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test -//tensorflow/python/ops/ragged:ragged_dispatch_test -//tensorflow/python:quantized_ops_test

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64 --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64 --copt=-ffp-contract=off --verbose_failures -- //tensorflow/python/... -//tensorflow/python/tools/... -//tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test -//tensorflow/python/ops/ragged:ragged_dispatch_test -//tensorflow/python:quantized_ops_test
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=180
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:
  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:
  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium
INFO: Found applicable config definition build:short_logs in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition test:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only
INFO: Found applicable config definition build:nonccl in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:linux in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/python/ops/distributions/BUILD:6:11: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
ERROR: /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/core/profiler/lib/BUILD:286:11: //tensorflow/core/profiler/lib:profiler_disabled_test: no such attribute 'env' in 'cc_test' rule
ERROR: /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/python/BUILD:3324:10: Target '//tensorflow/core/profiler/lib:profiler_session_impl' contains an error and its package is in error and referenced by '//tensorflow/python:win_lib_files_for_exported_symbols'
INFO: Repository cython instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:15:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:888:21: in workspace
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:686:20: in _tf_repositories
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:81:35: in <toplevel>
INFO: Repository go_sdk instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:23:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace0.bzl:120:20: in workspace
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>
INFO: Repository rules_java instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:23:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace0.bzl:120:20: in workspace
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/com_google_protobuf/protobuf_deps.bzl:34:21: in protobuf_deps
Repository rule http_archive defined at:
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
INFO: Repository typing_extensions_archive instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:15:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:888:21: in workspace
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:380:20: in _tf_repositories
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:81:35: in <toplevel>
INFO: Repository rules_python instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:15:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:888:21: in workspace
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:786:20: in _tf_repositories
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:81:35: in <toplevel>
INFO: Repository remote_coverage_tools instantiated at:
  /DEFAULT.WORKSPACE.SUFFIX:11:13: in <toplevel>
Repository rule http_archive defined at:
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
INFO: Repository gast_archive instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:15:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:888:21: in workspace
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:362:20: in _tf_repositories
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:81:35: in <toplevel>
INFO: Repository termcolor_archive instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:15:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:888:21: in workspace
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:371:20: in _tf_repositories
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:81:35: in <toplevel>
INFO: Repository com_google_pprof instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:15:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:888:21: in workspace
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:668:20: in _tf_repositories
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:81:35: in <toplevel>
INFO: Repository dill_archive instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:15:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:888:21: in workspace
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:429:20: in _tf_repositories
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:81:35: in <toplevel>
INFO: Repository flatbuffers instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:15:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:881:28: in workspace
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:66:16: in _initialize_third_party
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/flatbuffers/workspace.bzl:6:20: in repo
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:81:35: in <toplevel>
INFO: Repository wrapt instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:15:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:888:21: in workspace
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:855:20: in _tf_repositories
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:81:35: in <toplevel>
INFO: Repository six_archive instantiated at:
  /home/builder/1/tensorflow_build/tensorflow-git/WORKSPACE:15:14: in <toplevel>
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:888:21: in workspace
  /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/workspace2.bzl:318:20: in _tf_repositories
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/builder/1/tensorflow_build/tensorflow-git/third_party/repo.bzl:81:35: in <toplevel>
ERROR: Analysis of target '//tensorflow/python:pybind_symbol_target_libs_file' failed; build aborted: Analysis failed
INFO: Elapsed time: 66.952s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (229 packages loaded, 3869 targets configured)
FAILED: Build did NOT complete successfully (229 packages loaded, 3869 targets configured)
    currently loading: @com_google_protobuf// ... (3 packages)
    Fetching @io_bazel_rules_docker; Cloning 9bfcd7dbf0294ed9d11a99da6363fc28df904502 of https://github.com/bazelbuild/rules_docker.git
"
53647,Switch build system,"**System information**
- TensorFlow version (you are using): 2.8
- Are you willing to contribute it (Yes/No): Only with a system change

**Describe the feature and the current behavior/state.**
Building TensorFlow on Windows is a nightmare. A big problem is that either TensorFlow's Bazel scripts or Bazel itself is quite buggy. Examples:
 - Visual Studio 2022 does not work
 - LLVM does not always work under Windows
 - Admin rights are needed to build

With Bazel, building TF is a gamble. For example, if you install VS2022 next to VS2019 it doesn't work anymore without workarounds. Another problem with Bazel is that the files are always stored in C:\Users\username\ _bazel\_username. You can't easily change the directory to e.g. move the build to a fast SSD or when C is running out of space.

TF has used CMake in the past. A switch back to CMake would be desirable.

**Who will benefit with this feature?**
Anyone who builds TensorFlow on Windows.
"
53646,Tensorflow is unable to detect my GTX 1650 GPU with CUDA 11.2,"I have CUDA drivers installed on my pc with Nvidia GTX 1650.

```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Feb_14_21:12:58_PST_2021
Cuda compilation tools, release 11.2, V11.2.152
Build cuda_11.2.r11.2/compiler.29618528_0
```
And I have Tensorflow-gpu 2.7.0

The following TensorFlow code is unable to recognize the GPU.
```
import tensorflow as tf
print(tf.test.gpu_device_name())
```
The error :-
```
2022-01-01 11:38:54.599751: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-01-01 11:38:54.601786: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2022-01-01 11:38:54.601803: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-01-01 11:38:54.601817: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (wasseypur): /proc/driver/nvidia/version does not exist
```"
53645,Conv2DTranspose have a non-deterministic behavior ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 8.1
- GPU model and memory: GeForce GTX 1080 Ti

**Describe the current behavior**
It seems that Conv2DTranspose behaves in a non-deterministic way.

**Describe the expected behavior**
I would expect to get the exact same output when I run the same model with the same input.

**Standalone code to reproduce the issue**
#*****************************************
import numpy as np
from keras.layers import Conv2DTranspose, Input
from keras.models import Model

input_layer = Input(shape=(8, 8, 3))
output = Conv2DTranspose(3, 4)(input_layer)
model = Model(input_layer, output)
data = np.random.randn(1, 8, 8, 3)

for _ in range(10):
    output_sum = np.sum(model.predict(data))
    print(f'output_sum: {output_sum}')
#*****************************************

Output I get:
output_sum: 1.6755037307739258
output_sum: 1.675503134727478
output_sum: 1.675502896308899
output_sum: 1.6755034923553467
output_sum: 1.6755017042160034
output_sum: 1.675503134727478
output_sum: 1.6755036115646362
output_sum: 1.6755034923553467
output_sum: 1.6755030155181885
output_sum: 1.6755015850067139



"
53644,How to check the total no of neurons of model?,"My question is about the Neural Network neuron's numbers.
No of neurons in input layer is equal to no of input features..say 32,32,1 if MNIST dataset 
and no of neurons in Output layer is equal to no of target variable say,,in MNIST it is 10
but how can we know about the neurons of hidden layers?
https://colab.research.google.com/github/AviatorMoser/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb#scrollTo=BL0AhUGJwT6M
in this colab notebook, there are 2 network defined.
How can we know the total no of neurons of the architecure?

Same question is for TFLite model too.
Also, How to check the performance in terms of TOPs/Power consumption/GPU-CPU usage etc?

"
53643,How to build tensorflow 1.15 with docker build？I can only compile the latest tensorflow2.9 according to the official tutorial,"![image](https://user-images.githubusercontent.com/44188056/148175006-782c6cef-fb0a-4c80-bdae-c83d2212497f.png)

I want to switch branch r1.15, but it didn't work！thankyou！

"
53642,How to build tensorflow 1.15 with docker build？I can only compile the latest tensorflow2.9 according to the official tutorial,"![image](https://user-images.githubusercontent.com/44188056/148174661-c22df66d-c624-4d87-8a7c-02ff97837443.png)

I want to switch branch r1.15, but it didn't work！thankyou！"
53641,如何通过 docker build，来build tensorflow指定版本，比如我想build  tensorflow1.15,"![image](https://user-images.githubusercontent.com/44188056/148174262-c83fd792-c10c-4e46-8a62-59c4aee815cd.png)

我按照 docker build的方法 build出来时最新的tensorflow 2.9的版本，里面我也不能git checkout 来切换分支，请问有什么好的解决方法吗？

谢谢"
53640,Failed build: Problem getting numpy include path.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monterey 12.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6.2(latest - cloned yesterday)
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I wan to build GPU Metal Delegate and get the error `Problem getting numpy include path.`. I'm pretty sure I have numpy installed for python3 and python2.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I've tried to fix the problem with this [suggestion](https://github.com/tensorflow/tensorflow/issues/53467#issuecomment-996688168), but still get the same output. Running on this command:
`bazel build --action_env PYTHON_BIN_PATH=/usr/bin/python3 --verbose_failures --config=ios_arm64 -c opt //tensorflow/lite/ios:TensorFlowLiteCMetal_framework`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

**NumPy logs on python3 and python2**
```
adityadwipamungkas@Adityas-MacBook-Pro tensorflow-2.6.2 % pip3 show numpy
Name: numpy
Version: 1.21.5
Summary: NumPy is the fundamental package for array computing with Python.
Home-page: https://www.numpy.org
Author: Travis E. Oliphant et al.
Author-email: None
License: BSD
Location: /usr/local/lib/python3.7/site-packages
Requires: 
Required-by: 
adityadwipamungkas@Adityas-MacBook-Pro tensorflow-2.6.2 % pip show numpy
DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.
Name: numpy
Version: 1.16.6
Summary: NumPy is the fundamental package for array computing with Python.
Home-page: https://www.numpy.org
Author: Travis E. Oliphant et al.
Author-email: None
License: BSD
Location: /Users/adityadwipamungkas/Library/Python/2.7/lib/python/site-packages
Requires: 
Required-by: matplotlib
```

**Configure**
```
adityadwipamungkas@Adityas-MacBook-Pro tensorflow-2.6.2 % ./configure
Found possible Python library paths:
  /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: N
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Do you wish to build TensorFlow with iOS support? [y/N]: y
iOS support will be enabled for TensorFlow.
```

**Output when run bazel build**
```
adityadwipamungkas@Adityas-MacBook-Pro tensorflow-2.6.2 % bazel build --action_env PYTHON_BIN_PATH=/usr/bin/python3 --verbose_failures --config=ios_arm64 -c opt //tensorflow/lite/ios:TensorFlowLiteCMetal_framework
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=159
INFO: Reading rc options for 'build' from /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages --python_path=/usr/bin/python3
INFO: Found applicable config definition build:short_logs in file /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:ios_arm64 in file /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/.bazelrc: --config=ios --cpu=ios_arm64
INFO: Found applicable config definition build:ios in file /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/.bazelrc: --apple_platform_type=ios --apple_bitcode=embedded --copt=-fembed-bitcode --copt=-Wno-c++11-narrowing --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --define=with_xla_support=false
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/b570a1921c9e55ac53c8972bd2bfd37cd0eb510d.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: /private/var/tmp/_bazel_adityadwipamungkas/5c73181a29c264533309cd4aec522b90/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Repository local_execution_config_python instantiated at:
  /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/WORKSPACE:15:14: in <toplevel>
  /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/tensorflow/workspace2.bzl:1088:19: in workspace
  /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/tensorflow/workspace2.bzl:85:27: in _tf_toolchains
  /private/var/tmp/_bazel_adityadwipamungkas/5c73181a29c264533309cd4aec522b90/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
  /private/var/tmp/_bazel_adityadwipamungkas/5c73181a29c264533309cd4aec522b90/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
Repository rule local_python_configure defined at:
  /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/third_party/py/python_configure.bzl:275:41: in <toplevel>
INFO: Repository local_config_python instantiated at:
  /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/WORKSPACE:15:14: in <toplevel>
  /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/tensorflow/workspace2.bzl:1088:19: in workspace
  /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/tensorflow/workspace2.bzl:95:21: in _tf_toolchains
Repository rule python_configure defined at:
  /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/third_party/py/python_configure.bzl:294:35: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
        File ""/Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/third_party/py/python_configure.bzl"", line 267, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""/Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/third_party/py/python_configure.bzl"", line 213, column 39, in _create_local_python_repository
                numpy_include = _get_numpy_include(repository_ctx, python_bin) + ""/numpy""
        File ""/Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/third_party/py/python_configure.bzl"", line 187, column 19, in _get_numpy_include
                return execute(
        File ""/Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'numpy'
Is numpy installed?
ERROR: Error fetching repository: Traceback (most recent call last):
        File ""/Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/third_party/py/python_configure.bzl"", line 267, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""/Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/third_party/py/python_configure.bzl"", line 213, column 39, in _create_local_python_repository
                numpy_include = _get_numpy_include(repository_ctx, python_bin) + ""/numpy""
        File ""/Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/third_party/py/python_configure.bzl"", line 187, column 19, in _get_numpy_include
                return execute(
        File ""/Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'numpy'
Is numpy installed?
INFO: Repository go_sdk instantiated at:
  /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/WORKSPACE:23:14: in <toplevel>
  /Users/adityadwipamungkas/Downloads/tensorflow-2.6.2/tensorflow/workspace0.bzl:120:20: in workspace
  /private/var/tmp/_bazel_adityadwipamungkas/5c73181a29c264533309cd4aec522b90/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  /private/var/tmp/_bazel_adityadwipamungkas/5c73181a29c264533309cd4aec522b90/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains
  /private/var/tmp/_bazel_adityadwipamungkas/5c73181a29c264533309cd4aec522b90/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  /private/var/tmp/_bazel_adityadwipamungkas/5c73181a29c264533309cd4aec522b90/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>
ERROR: Analysis of target '//tensorflow/lite/ios:TensorFlowLiteCMetal_framework' failed; build aborted: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'numpy'
Is numpy installed?
INFO: Elapsed time: 24.504s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (27 packages loaded, 26 targets configured)
    Fetching @local_config_xcode; fetching
```

CC: @yyoon "
53636,Missing comma in test results in unexpected string concatenation,"**System information**.
- Have I written custom code (as opposed to using a stock example script provided in Keras): NA
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NA
- TensorFlow installed from (source or binary): NA
- TensorFlow version (use command below): NA
- Python version: NA
- Bazel version (if compiling from source): NA
- GPU model and memory: NA
- Exact command to reproduce: NA

**Describe the problem**.

Absent comma results in unwatned string concatenation on line 330:
https://github.com/tensorflow/tensorflow/blob/0d8705c82c64dfb39c49e346de1a66182e5eabd1/tensorflow/python/keras/engine/training_generator_test.py#L330-L336

So `'matmul'` gets appended with 'Yaks are also quite nice' resulting in `matmulYaks are also quite nice` being used in the test.

**Describe the current behavior**.

`'matmulYaks are also quite nice'` is used in the test

**Describe the expected behavior**.

`'matmul', 'Yaks are also quite nice'` is used in the test

**[Contributing](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md)**.

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):

Add the missing comma

**Standalone code to reproduce the issue**.

NA


**Source code / logs**.

NA"
53634,"tf.keras k-fold save weights failed! RuntimeError: Can't decrement id ref count (unable to close file, errno = 5, error message = 'Input/output error')","# Background:
I want to train a two-dense-layer classification model, I use the k-fold strategy to train it and use tf.keras.callback.checkpoint utils to save the best model weights:

`RuntimeError: Can't decrement id ref count (unable to close file, errno = 5, error message = 'Input/output error')`

I tried to change the version of h5py to fix this bug, but it did not work.(2.10.0 ~ 3.6.0).
This bug will be occured `randomly`, it seems like bugs more frequently when training cost time shorter，It seems like when training a k-fold model, the model will be saveing parallel, then this bug will be occurred.

# when I track to source code of keras, the save_weights function is just like that:
```python
def save_weights(self, filepath, overwrite=True):
    ...
    with h5py.File(filepath, 'w') as f:
        saving.save_weights_to_hdf5_group(f, self.layers)
        f.flush()
saving.save_weights_to_hdf5_group means save weights into the HDF5 group
```
# This bug can be repeated by those codes:
```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.model_selection import KFold
import numpy as np

input_dim = 256
input_x = Input(input_dim,)
x = Dense(128, activation='relu')(input_x)
p = Dense(2, activation='sigmoid')(x)
model = Model(inputs=input_x, outputs=p)

model.compile(loss='mse', optimizer=Adam(1e-5), metrics = ['acc'])

sample_num = 2000
train, label = np.random.random((sample_num, input_dim)), np.random.random((sample_num, 2))
kfold = KFold(n_splits=5)
for i, (train_ind, valid_ind) in enumerate(kfold.split(list(range(sample_num)))):
    model_path = f'model-{i}.h5'
    ckpt = ModelCheckpoint(model_path, monitor='val_loss')
    model.fit([train[train_ind], label[train_ind]], label[train_ind], validation_data=[train[valid_ind], label[valid_ind]], epochs=10, batch_size=64, callbacks=[ckpt])
```
# More detailed bug information is:
```python
Traceback (most recent call last):
  File ""/home/notebook/code/personal/test_sihao/gitcode/oCTS_lsh/train_tf2.py"", line 36, in <module>
    current_train_pickle_path, n_fold=5, monitor='val_categorical_crossentropy')
  File ""/home/notebook/data/group/sihao_work/gitcode/oCTS_lsh/classifier/b4k_classifier_tf2.py"", line 186, in model_train_kfold
    self.train(model, train_D, valid_D, tmp_model_save_path, monitor=monitor)
  File ""/home/notebook/data/group/sihao_work/gitcode/oCTS_lsh/classifier/b4k_classifier_tf2.py"", line 159, in train
    callbacks=[early_stopping, plateau, checkpoint])
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1433, in fit_generator
    steps_name='steps_per_epoch')
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py"", line 331, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 311, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 969, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 998, in _save_model
    self.model.save_weights(filepath, overwrite=True)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1315, in save_weights
    self._ckpt_saved_epoch).encode('utf8')
  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py"", line 544, in __exit__
    self.close()
  File ""/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py"", line 526, in close
    self.id._close_open_objects(h5f.OBJ_LOCAL | h5f.OBJ_FILE)
  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""h5py/h5f.pyx"", line 358, in h5py.h5f.FileID._close_open_objects
RuntimeError: Can't decrement id ref count (unable to close file, errno = 5, error message = 'Input/output error')
Segmentation fault
```"
53633,Dataset.batch() change tensor shape and abort LSTM fit,"**System information**
- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1
- Platform: Kaggle TPU

Hi, my LSTM code give this error using dataset.batch():

```
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:540 run
        return self.extended.tpu_run(fn, args, kwargs, options)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1296 tpu_run
        return func(args, kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1364 tpu_function
        xla_options=tpu.XLAOptions(use_spmd_for_xla_partitioning=False))
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/tpu/tpu.py:968 replicate
        xla_options=xla_options)[1]
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/tpu/tpu.py:1439 split_compile_and_replicate
        outputs = computation(*computation_inputs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1325 replicated_fn
        result[0] = fn(*replica_args, **replica_kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step
        y_pred = self(x, training=True)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility
        str(tuple(shape)))

    ValueError: Input 0 of layer model is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, None, 3, 13)
```

Meanwhile without dataset.batch() the code works correctly.

I'm using Tensorflow dataset from tfrecord files:

```
def read_tfrecord(example_proto):
    feature_description = {
    'x': tf.io.FixedLenFeature([], tf.string),
    'y': tf.io.FixedLenFeature([], tf.string),
    }
    example = tf.io.parse_single_example(example_proto, feature_description)
    vect = tf.io.parse_tensor(example['x'], tf.float16)
    label = tf.io.parse_tensor(example['y'], tf.int8)

    vect = tf.reshape(vect, [-1, ws, num_features])
    label = tf.reshape(label, [-1, 1])

    return vect, label
```
I have to add the reshape because without i get this error:

    TypeError: can't multiply sequence by non-int of type 'NoneType'

and here suggest to add reshape: [link][1]


```
def load_dataset(filenames):
    ignore_order = tf.data.Options()
    ignore_order.experimental_deterministic = False  # disable order, increase speed
    dataset = tf.data.TFRecordDataset(filenames)  # automatically interleaves reads from multiple files
    dataset = dataset.with_options(ignore_order)  # uses data as soon as it streams in, rather than in its original order
    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)
    dataset = dataset.batch(bs)
    dataset = dataset.prefetch(buffer_size=AUTOTUNE)
    return dataset
```

```
    model = Sequential(name = ""model"")
    model.add(LSTM(units=units, input_shape=(ws, num_features), return_sequences=False, dropout = dropout))
    model.add(Dense(units=num_labels, activation = ""sigmoid""))
    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=[BinaryAccuracy(), AUC(), Precision(), Recall()])
```
Here you can see the dataset with: 
```
for parsed_record in parsed_dataset.take(10):
  print(repr(parsed_record))
```
![MicrosoftTeams-image](https://user-images.githubusercontent.com/48318112/148097080-fcb05ce4-9769-421f-9815-90ddf1506ad8.png)


  [1]: https://github.com/tensorflow/tensorflow/issues/40864
"
53632,Updating bazel causes python to abort during build on AARCH64,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.7
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

Build fails with an abort from Python when using Bazel 4.2.2. The same build works with bazel 3.7.2

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures --copt=-ffp-contract=off --cxxopt=-ffp-contract=off

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

https://ci.linaro.org/job/ldcg-python-manylinux-tensorflow-nightly/225/console
18:51:45     Execution platform: @local_execution_config_platform//:platform
18:51:45     *** Error in `/tmp/workspace/venv-cp37-cp37m/bin/python3': free(): invalid pointer: 0x0000fffef8184fa8 ***
18:51:45     ======= Backtrace: =========
18:51:45     /lib64/libc.so.6(+0x7d1ec)[0xffff9694d1ec]
18:51:45     /lib64/libstdc++.so.6(_ZNSt6locale5_Impl16_M_install_facetEPKNS_2idEPKNS_5facetE+0x138)[0xfffef80d08dc]
18:51:45     /lib64/libstdc++.so.6(_ZNSt6locale5_ImplC1Em+0x188)[0xfffef80d0cfc]
18:51:45     /lib64/libstdc++.so.6(+0x717dc)[0xfffef80d17dc]
18:51:45     /lib64/libpthread.so.0(+0x5db0)[0xffff96b85db0]
18:51:45     /lib64/libstdc++.so.6(+0x71828)[0xfffef80d1828]
18:51:45     /lib64/libstdc++.so.6(_ZNSt6localeC2Ev+0x1c)[0xfffef80d1864]
18:51:45     /lib64/libstdc++.so.6(_ZNSt8ios_base4InitC2Ev+0xbc)[0xfffef80ceb58]
18:51:45     /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/google/protobuf/pyext/_message.cpython-37m-aarch64-linux-gnu.so(+0xa89dc)[0xfffef7eb89dc]
18:51:45     /lib/ld-linux-aarch64.so.1(+0xe470)[0xffff96c5e470]
18:51:45     /lib/ld-linux-aarch64.so.1(+0x12a68)[0xffff96c62a68]
18:51:45     /lib/ld-linux-aarch64.so.1(+0xe28c)[0xffff96c5e28c]
18:51:45     /lib/ld-linux-aarch64.so.1(+0x12124)[0xffff96c62124]
18:51:45     /lib64/libdl.so.2(+0xfa4)[0xffff96b50fa4]
18:51:45     /lib/ld-linux-aarch64.so.1(+0xe28c)[0xffff96c5e28c]
18:51:45     /lib64/libdl.so.2(+0x1614)[0xffff96b51614]
18:51:45     /lib64/libdl.so.2(dlopen+0x38)[0xffff96b5104c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyImport_FindSharedFuncptr+0x1ac)[0x4ff20c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyImport_LoadDynamicModuleWithSpec+0x124)[0x4d5834]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x4d3604]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyMethodDef_RawFastCallDict+0x304)[0x436bc4]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(PyCFunction_Call+0x184)[0x436df8]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalFrameDefault+0x61c8)[0x426188]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalCodeWithName+0x82c)[0x4bb34c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyFunction_FastCallKeywords+0x7c)[0x435ea0]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41fc30]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalFrameDefault+0x5ad0)[0x425a90]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41eb4c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41fc30]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalFrameDefault+0x4a04)[0x4249c4]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41eb4c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41fc30]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalFrameDefault+0x44a4)[0x424464]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41eb4c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41fc30]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalFrameDefault+0x44a4)[0x424464]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41eb4c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41fc30]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalFrameDefault+0x44a4)[0x424464]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41eb4c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x437fc0]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyObject_CallMethodIdObjArgs+0x9c)[0x43827c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(PyImport_ImportModuleLevelObject+0x580)[0x4d48e0]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x4b6a40]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(PyCFunction_Call+0x114)[0x436d88]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalFrameDefault+0x61c8)[0x426188]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalCodeWithName+0x82c)[0x4bb34c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyFunction_FastCallKeywords+0x7c)[0x435ea0]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x41fc30]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalFrameDefault+0x44a4)[0x424464]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalCodeWithName+0x82c)[0x4bb34c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyFunction_FastCallDict+0x1d0)[0x435d10]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x437fc0]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyObject_CallMethodIdObjArgs+0x9c)[0x43827c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(PyImport_ImportModuleLevelObject+0x334)[0x4d4694]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalFrameDefault+0x7188)[0x427148]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalCodeWithName+0x82c)[0x4bb34c]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(PyEval_EvalCode+0x38)[0x4bb5fc]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3[0x4b8738]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyMethodDef_RawFastCallDict+0x304)[0x436bc4]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(PyCFunction_Call+0x184)[0x436df8]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalFrameDefault+0x61c8)[0x426188]
18:51:45     /tmp/workspace/venv-cp37-cp37m/bin/python3(_PyEval_EvalCodeWithName+0x82c)[0x4bb34c]
18:51:45     ======= Memory map: ========
18:51:45     00400000-00640000 r-xp 00000000 fd:02 405195408                          /opt/_internal/cpython-3.7.12/bin/python3.7
18:51:45     00650000-00660000 r--p 00240000 fd:02 405195408                          /opt/_internal/cpython-3.7.12/bin/python3.7
18:51:45     00660000-006d0000 rw-p 00250000 fd:02 405195408                          /opt/_internal/cpython-3.7.12/bin/python3.7
18:51:45     006d0000-006f0000 rw-p 00000000 00:00 0
18:51:45     3ca80000-3dd70000 rw-p 00000000 00:00 0                                  [heap]
18:51:45     fffef0000000-fffef0030000 rw-p 00000000 00:00 0
18:51:45     fffef0030000-fffef4000000 ---p 00000000 00:00 0
18:51:45     fffef7e10000-fffef8040000 r-xp 00000000 fd:02 405409191                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/google/protobuf/pyext/_message.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8040000-fffef8050000 r--p 00220000 fd:02 405409191                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/google/protobuf/pyext/_message.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8050000-fffef8060000 rw-p 00230000 fd:02 405409191                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/google/protobuf/pyext/_message.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8060000-fffef8150000 r-xp 00000000 fd:02 270523991                  /usr/lib64/libstdc++.so.6.0.19
18:51:45     fffef8150000-fffef8160000 ---p 000f0000 fd:02 270523991                  /usr/lib64/libstdc++.so.6.0.19
18:51:45     fffef8160000-fffef8170000 r--p 000f0000 fd:02 270523991                  /usr/lib64/libstdc++.so.6.0.19
18:51:45     fffef8170000-fffef8180000 rw-p 00100000 fd:02 270523991                  /usr/lib64/libstdc++.so.6.0.19
18:51:45     fffef8180000-fffef8210000 rw-p 00000000 00:00 0
18:51:45     fffef82f0000-fffef8300000 r-xp 00000000 fd:02 276318214                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/google/protobuf/internal/_api_implementation.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8300000-fffef8310000 rw-p 00000000 fd:02 276318214                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/google/protobuf/internal/_api_implementation.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8310000-fffef8390000 rw-p 00000000 00:00 0
18:51:45     fffef8390000-fffef8430000 r-xp 00000000 fd:02 276318271                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_generator.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8430000-fffef8440000 r--p 00090000 fd:02 276318271                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_generator.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8440000-fffef8470000 rw-p 000a0000 fd:02 276318271                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_generator.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8470000-fffef8480000 r-xp 00000000 fd:02 276318277                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_sfc64.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8480000-fffef8490000 r--p 00000000 fd:02 276318277                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_sfc64.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8490000-fffef84a0000 rw-p 00010000 fd:02 276318277                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_sfc64.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef84a0000-fffef84b0000 r-xp 00000000 fd:02 276318268                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_pcg64.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef84b0000-fffef84c0000 r--p 00000000 fd:02 276318268                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_pcg64.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef84c0000-fffef84d0000 rw-p 00010000 fd:02 276318268                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_pcg64.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef84d0000-fffef84f0000 r-xp 00000000 fd:02 276318282                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_philox.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef84f0000-fffef8500000 r--p 00010000 fd:02 276318282                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_philox.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8500000-fffef8510000 rw-p 00020000 fd:02 276318282                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_philox.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8510000-fffef8530000 r-xp 00000000 fd:02 276318285                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_mt19937.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8530000-fffef8540000 r--p 00010000 fd:02 276318285                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_mt19937.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8540000-fffef8550000 rw-p 00020000 fd:02 276318285                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_mt19937.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8550000-fffef85b0000 r-xp 00000000 fd:02 276318298                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_bounded_integers.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef85b0000-fffef85c0000 r--p 00050000 fd:02 276318298                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_bounded_integers.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef85c0000-fffef85d0000 rw-p 00060000 fd:02 276318298                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_bounded_integers.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef85d0000-fffef85e0000 r-xp 00000000 fd:02 272114734                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/binascii.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef85e0000-fffef85f0000 r--p 00000000 fd:02 272114734                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/binascii.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef85f0000-fffef8600000 rw-p 00010000 fd:02 272114734                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/binascii.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8600000-fffef8640000 r-xp 00000000 fd:02 276318288                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_common.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8640000-fffef8650000 r--p 00030000 fd:02 276318288                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_common.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8650000-fffef8660000 rw-p 00040000 fd:02 276318288                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/_common.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8660000-fffef8690000 r-xp 00000000 fd:02 276318291                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/bit_generator.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8690000-fffef86a0000 r--p 00020000 fd:02 276318291                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/bit_generator.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef86a0000-fffef86b0000 rw-p 00030000 fd:02 276318291                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/bit_generator.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef86b0000-fffef86f0000 rw-p 00000000 00:00 0
18:51:45     fffef86f0000-fffef8770000 r-xp 00000000 fd:02 276318287                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/mtrand.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8770000-fffef8780000 r--p 00070000 fd:02 276318287                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/mtrand.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8780000-fffef87b0000 rw-p 00080000 fd:02 276318287                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/random/mtrand.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef87b0000-fffef87f0000 rw-p 00000000 00:00 0
18:51:45     fffef87f0000-fffef8810000 r-xp 00000000 fd:02 276318311                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/fft/_pocketfft_internal.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8810000-fffef8820000 r--p 00010000 fd:02 276318311                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/fft/_pocketfft_internal.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8820000-fffef8830000 rw-p 00020000 fd:02 276318311                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/fft/_pocketfft_internal.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8830000-fffef8880000 r-xp 00000000 fd:02 272114701                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_decimal.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8880000-fffef8890000 r--p 00040000 fd:02 272114701                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_decimal.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8890000-fffef88a0000 rw-p 00050000 fd:02 272114701                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_decimal.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef88a0000-fffef88e0000 rw-p 00000000 00:00 0
18:51:45     fffef88e0000-fffef88f0000 r-xp 00000000 fd:02 272114737                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/grp.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef88f0000-fffef8900000 r--p 00000000 fd:02 272114737                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/grp.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8900000-fffef8910000 rw-p 00010000 fd:02 272114737                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/grp.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8910000-fffef8940000 r-xp 00000000 fd:02 270522138                  /usr/lib64/liblzma.so.5.2.2
18:51:45     fffef8940000-fffef8950000 r--p 00020000 fd:02 270522138                  /usr/lib64/liblzma.so.5.2.2
18:51:45     fffef8950000-fffef8960000 rw-p 00030000 fd:02 270522138                  /usr/lib64/liblzma.so.5.2.2
18:51:45     fffef8960000-fffef8970000 r-xp 00000000 fd:02 272114708                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_lzma.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8970000-fffef8980000 r--p 00000000 fd:02 272114708                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_lzma.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8980000-fffef8990000 rw-p 00010000 fd:02 272114708                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_lzma.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8990000-fffef89a0000 r-xp 00000000 fd:02 272114683                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_bz2.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef89a0000-fffef89b0000 r--p 00000000 fd:02 272114683                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_bz2.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef89b0000-fffef89c0000 rw-p 00010000 fd:02 272114683                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_bz2.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef89c0000-fffef89e0000 r-xp 00000000 fd:02 405192202                  /usr/lib64/libz.so.1.2.7
18:51:45     fffef89e0000-fffef89f0000 r--p 00010000 fd:02 405192202                  /usr/lib64/libz.so.1.2.7
18:51:45     fffef89f0000-fffef8a00000 rw-p 00020000 fd:02 405192202                  /usr/lib64/libz.so.1.2.7
18:51:45     fffef8a00000-fffef8a10000 r-xp 00000000 fd:02 272158592                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/zlib.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8a10000-fffef8a20000 r--p 00000000 fd:02 272158592                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/zlib.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8a20000-fffef8a30000 rw-p 00010000 fd:02 272158592                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/zlib.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8a30000-fffef8af0000 rw-p 00000000 00:00 0
18:51:45     fffef8af0000-fffef8b20000 r-xp 00000000 fd:02 7592199                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/linalg/_umath_linalg.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8b20000-fffef8b30000 r--p 00020000 fd:02 7592199                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/linalg/_umath_linalg.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8b30000-fffef8b60000 rw-p 00030000 fd:02 7592199                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/linalg/_umath_linalg.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8b60000-fffef8b70000 r-xp 00000000 fd:02 7592203                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/linalg/lapack_lite.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8b70000-fffef8b80000 r--p 00000000 fd:02 7592203                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/linalg/lapack_lite.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8b80000-fffef8bb0000 rw-p 00010000 fd:02 7592203                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/linalg/lapack_lite.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8bb0000-fffef8c30000 rw-p 00000000 00:00 0
18:51:45     fffef8c30000-fffef8c60000 r-xp 00000000 fd:02 276318408                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/core/_multiarray_tests.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8c60000-fffef8c70000 r--p 00020000 fd:02 276318408                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/core/_multiarray_tests.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8c70000-fffef8c80000 rw-p 00030000 fd:02 276318408                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/core/_multiarray_tests.cpython-37m-aarch64-linux-gnu.so
18:51:45     fffef8c80000-fffef8d00000 rw-p 00000000 00:00 0
18:51:45     fffef8d00000-fffefcd00000 rw-p 00000000 00:00 0
18:51:45     fffefcd00000-fffefcd10000 ---p 00000000 00:00 0
18:51:45     fffefcd10000-fffefd510000 rw-p 00000000 00:00 0
18:51:45     fffefd510000-fffeff510000 rw-p 00000000 00:00 0
18:51:45     fffeff510000-fffeff520000 ---p 00000000 00:00 0
18:51:45     fffeff520000-fffeffd20000 rw-p 00000000 00:00 0
18:51:45     fffeffd20000-ffff01d20000 rw-p 00000000 00:00 0
18:51:45     ffff01d20000-ffff01d30000 ---p 00000000 00:00 0
18:51:45     ffff01d30000-ffff02530000 rw-p 00000000 00:00 0
18:51:45     ffff02530000-ffff04530000 rw-p 00000000 00:00 0
18:51:45     ffff04530000-ffff04540000 ---p 00000000 00:00 0
18:51:45     ffff04540000-ffff04d40000 rw-p 00000000 00:00 0
18:51:45     ffff04d40000-ffff06d40000 rw-p 00000000 00:00 0
18:51:45     ffff06d40000-ffff06d50000 ---p 00000000 00:00 0
18:51:45     ffff06d50000-ffff07550000 rw-p 00000000 00:00 0
18:51:45     ffff07550000-ffff09550000 rw-p 00000000 00:00 0
18:51:45     ffff09550000-ffff09560000 ---p 00000000 00:00 0
18:51:45     ffff09560000-ffff09d60000 rw-p 00000000 00:00 0
18:51:45     ffff09d60000-ffff0bd60000 rw-p 00000000 00:00 0
18:51:45     ffff0bd60000-ffff0bd70000 ---p 00000000 00:00 0
18:51:45     ffff0bd70000-ffff0c570000 rw-p 00000000 00:00 0
18:51:45     ffff0c570000-ffff0e570000 rw-p 00000000 00:00 0
18:51:45     ffff0e570000-ffff0e580000 ---p 00000000 00:00 0
18:51:45     ffff0e580000-ffff0ed80000 rw-p 00000000 00:00 0
18:51:45     ffff0ed80000-ffff10d80000 rw-p 00000000 00:00 0
18:51:45     ffff10d80000-ffff10d90000 ---p 00000000 00:00 0
18:51:45     ffff10d90000-ffff11590000 rw-p 00000000 00:00 0
18:51:45     ffff11590000-ffff13590000 rw-p 00000000 00:00 0
18:51:45     ffff13590000-ffff135a0000 ---p 00000000 00:00 0
18:51:45     ffff135a0000-ffff13da0000 rw-p 00000000 00:00 0
18:51:45     ffff13da0000-ffff15da0000 rw-p 00000000 00:00 0
18:51:45     ffff15da0000-ffff15db0000 ---p 00000000 00:00 0
18:51:45     ffff15db0000-ffff165b0000 rw-p 00000000 00:00 0
18:51:45     ffff165b0000-ffff185b0000 rw-p 00000000 00:00 0
18:51:45     ffff185b0000-ffff185c0000 ---p 00000000 00:00 0
18:51:45     ffff185c0000-ffff18dc0000 rw-p 00000000 00:00 0
18:51:45     ffff18dc0000-ffff1adc0000 rw-p 00000000 00:00 0
18:51:45     ffff1adc0000-ffff1add0000 ---p 00000000 00:00 0
18:51:45     ffff1add0000-ffff1b5d0000 rw-p 00000000 00:00 0
18:51:45     ffff1b5d0000-ffff1d5d0000 rw-p 00000000 00:00 0
18:51:45     ffff1d5d0000-ffff1d5e0000 ---p 00000000 00:00 0
18:51:45     ffff1d5e0000-ffff1dde0000 rw-p 00000000 00:00 0
18:51:45     ffff1dde0000-ffff1fde0000 rw-p 00000000 00:00 0
18:51:45     ffff1fde0000-ffff1fdf0000 ---p 00000000 00:00 0
18:51:45     ffff1fdf0000-ffff205f0000 rw-p 00000000 00:00 0
18:51:45     ffff205f0000-ffff225f0000 rw-p 00000000 00:00 0
18:51:45     ffff225f0000-ffff22600000 ---p 00000000 00:00 0
18:51:45     ffff22600000-ffff22e00000 rw-p 00000000 00:00 0
18:51:45     ffff22e00000-ffff24e00000 rw-p 00000000 00:00 0
18:51:45     ffff24e00000-ffff24e10000 ---p 00000000 00:00 0
18:51:45     ffff24e10000-ffff25610000 rw-p 00000000 00:00 0
18:51:45     ffff25610000-ffff27610000 rw-p 00000000 00:00 0
18:51:45     ffff27610000-ffff27620000 ---p 00000000 00:00 0
18:51:45     ffff27620000-ffff27e20000 rw-p 00000000 00:00 0
18:51:45     ffff27e20000-ffff29e20000 rw-p 00000000 00:00 0
18:51:45     ffff29e20000-ffff29e30000 ---p 00000000 00:00 0
18:51:45     ffff29e30000-ffff2a630000 rw-p 00000000 00:00 0
18:51:45     ffff2a630000-ffff2c630000 rw-p 00000000 00:00 0
18:51:45     ffff2c630000-ffff2c640000 ---p 00000000 00:00 0
18:51:45     ffff2c640000-ffff2ce40000 rw-p 00000000 00:00 0
18:51:45     ffff2ce40000-ffff2ee40000 rw-p 00000000 00:00 0
18:51:45     ffff2ee40000-ffff2ee50000 ---p 00000000 00:00 0
18:51:45     ffff2ee50000-ffff2f650000 rw-p 00000000 00:00 0
18:51:45     ffff2f650000-ffff31650000 rw-p 00000000 00:00 0
18:51:45     ffff31650000-ffff31660000 ---p 00000000 00:00 0
18:51:45     ffff31660000-ffff31e60000 rw-p 00000000 00:00 0
18:51:45     ffff31e60000-ffff33e60000 rw-p 00000000 00:00 0
18:51:45     ffff33e60000-ffff33e70000 ---p 00000000 00:00 0
18:51:45     ffff33e70000-ffff34670000 rw-p 00000000 00:00 0
18:51:45     ffff34670000-ffff36670000 rw-p 00000000 00:00 0
18:51:45     ffff36670000-ffff36680000 ---p 00000000 00:00 0
18:51:45     ffff36680000-ffff36e80000 rw-p 00000000 00:00 0
18:51:45     ffff36e80000-ffff38e80000 rw-p 00000000 00:00 0
18:51:45     ffff38e80000-ffff38e90000 ---p 00000000 00:00 0
18:51:45     ffff38e90000-ffff39690000 rw-p 00000000 00:00 0
18:51:45     ffff39690000-ffff3b690000 rw-p 00000000 00:00 0
18:51:45     ffff3b690000-ffff3b6a0000 ---p 00000000 00:00 0
18:51:45     ffff3b6a0000-ffff3bea0000 rw-p 00000000 00:00 0
18:51:45     ffff3bea0000-ffff3dea0000 rw-p 00000000 00:00 0
18:51:45     ffff3dea0000-ffff3deb0000 ---p 00000000 00:00 0
18:51:45     ffff3deb0000-ffff3e6b0000 rw-p 00000000 00:00 0
18:51:45     ffff3e6b0000-ffff406b0000 rw-p 00000000 00:00 0
18:51:45     ffff406b0000-ffff406c0000 ---p 00000000 00:00 0
18:51:45     ffff406c0000-ffff40ec0000 rw-p 00000000 00:00 0
18:51:45     ffff40ec0000-ffff42ec0000 rw-p 00000000 00:00 0
18:51:45     ffff42ec0000-ffff42ed0000 ---p 00000000 00:00 0
18:51:45     ffff42ed0000-ffff436d0000 rw-p 00000000 00:00 0
18:51:45     ffff436d0000-ffff456d0000 rw-p 00000000 00:00 0
18:51:45     ffff456d0000-ffff456e0000 ---p 00000000 00:00 0
18:51:45     ffff456e0000-ffff45ee0000 rw-p 00000000 00:00 0
18:51:45     ffff45ee0000-ffff47ee0000 rw-p 00000000 00:00 0
18:51:45     ffff47ee0000-ffff47ef0000 ---p 00000000 00:00 0
18:51:45     ffff47ef0000-ffff486f0000 rw-p 00000000 00:00 0
18:51:45     ffff486f0000-ffff4a6f0000 rw-p 00000000 00:00 0
18:51:45     ffff4a6f0000-ffff4a700000 ---p 00000000 00:00 0
18:51:45     ffff4a700000-ffff4af00000 rw-p 00000000 00:00 0
18:51:45     ffff4af00000-ffff4cf00000 rw-p 00000000 00:00 0
18:51:45     ffff4cf00000-ffff4cf10000 ---p 00000000 00:00 0
18:51:45     ffff4cf10000-ffff4d710000 rw-p 00000000 00:00 0
18:51:45     ffff4d710000-ffff4f710000 rw-p 00000000 00:00 0
18:51:45     ffff4f710000-ffff4f720000 ---p 00000000 00:00 0
18:51:45     ffff4f720000-ffff4ff20000 rw-p 00000000 00:00 0
18:51:45     ffff4ff20000-ffff51f20000 rw-p 00000000 00:00 0
18:51:45     ffff51f20000-ffff51f30000 ---p 00000000 00:00 0
18:51:45     ffff51f30000-ffff52730000 rw-p 00000000 00:00 0
18:51:45     ffff52730000-ffff54730000 rw-p 00000000 00:00 0
18:51:45     ffff54730000-ffff54740000 ---p 00000000 00:00 0
18:51:45     ffff54740000-ffff54f40000 rw-p 00000000 00:00 0
18:51:45     ffff54f40000-ffff56f40000 rw-p 00000000 00:00 0
18:51:45     ffff56f40000-ffff56f50000 ---p 00000000 00:00 0
18:51:45     ffff56f50000-ffff57750000 rw-p 00000000 00:00 0
18:51:45     ffff57750000-ffff59750000 rw-p 00000000 00:00 0
18:51:45     ffff59750000-ffff59760000 ---p 00000000 00:00 0
18:51:45     ffff59760000-ffff59f60000 rw-p 00000000 00:00 0
18:51:45     ffff59f60000-ffff5bf60000 rw-p 00000000 00:00 0
18:51:45     ffff5bf60000-ffff5bf70000 ---p 00000000 00:00 0
18:51:45     ffff5bf70000-ffff5c770000 rw-p 00000000 00:00 0
18:51:45     ffff5c770000-ffff5e770000 rw-p 00000000 00:00 0
18:51:45     ffff5e770000-ffff5e780000 ---p 00000000 00:00 0
18:51:45     ffff5e780000-ffff5ef80000 rw-p 00000000 00:00 0
18:51:45     ffff5ef80000-ffff60f80000 rw-p 00000000 00:00 0
18:51:45     ffff60f80000-ffff60f90000 ---p 00000000 00:00 0
18:51:45     ffff60f90000-ffff61790000 rw-p 00000000 00:00 0
18:51:45     ffff61790000-ffff63790000 rw-p 00000000 00:00 0
18:51:45     ffff63790000-ffff637a0000 ---p 00000000 00:00 0
18:51:45     ffff637a0000-ffff63fa0000 rw-p 00000000 00:00 0
18:51:45     ffff63fa0000-ffff65fa0000 rw-p 00000000 00:00 0
18:51:45     ffff65fa0000-ffff65fb0000 ---p 00000000 00:00 0
18:51:45     ffff65fb0000-ffff667b0000 rw-p 00000000 00:00 0
18:51:45     ffff667b0000-ffff687b0000 rw-p 00000000 00:00 0
18:51:45     ffff687b0000-ffff687c0000 ---p 00000000 00:00 0
18:51:45     ffff687c0000-ffff68fc0000 rw-p 00000000 00:00 0
18:51:45     ffff68fc0000-ffff6afc0000 rw-p 00000000 00:00 0
18:51:45     ffff6afc0000-ffff6afd0000 ---p 00000000 00:00 0
18:51:45     ffff6afd0000-ffff6b7d0000 rw-p 00000000 00:00 0
18:51:45     ffff6b7d0000-ffff6d7d0000 rw-p 00000000 00:00 0
18:51:45     ffff6d7d0000-ffff6d7e0000 ---p 00000000 00:00 0
18:51:45     ffff6d7e0000-ffff6dfe0000 rw-p 00000000 00:00 0
18:51:45     ffff6dfe0000-ffff6ffe0000 rw-p 00000000 00:00 0
18:51:45     ffff6ffe0000-ffff6fff0000 ---p 00000000 00:00 0
18:51:45     ffff6fff0000-ffff707f0000 rw-p 00000000 00:00 0
18:51:45     ffff707f0000-ffff727f0000 rw-p 00000000 00:00 0
18:51:45     ffff727f0000-ffff72800000 ---p 00000000 00:00 0
18:51:45     ffff72800000-ffff73000000 rw-p 00000000 00:00 0
18:51:45     ffff73000000-ffff75000000 rw-p 00000000 00:00 0
18:51:45     ffff75000000-ffff75010000 ---p 00000000 00:00 0
18:51:45     ffff75010000-ffff75810000 rw-p 00000000 00:00 0
18:51:45     ffff75810000-ffff77810000 rw-p 00000000 00:00 0
18:51:45     ffff77810000-ffff77820000 ---p 00000000 00:00 0
18:51:45     ffff77820000-ffff78020000 rw-p 00000000 00:00 0
18:51:45     ffff78020000-ffff7a020000 rw-p 00000000 00:00 0
18:51:45     ffff7a020000-ffff7a030000 ---p 00000000 00:00 0
18:51:45     ffff7a030000-ffff7a830000 rw-p 00000000 00:00 0
18:51:45     ffff7a830000-ffff7c830000 rw-p 00000000 00:00 0
18:51:45     ffff7c830000-ffff7c840000 ---p 00000000 00:00 0
18:51:45     ffff7c840000-ffff7d040000 rw-p 00000000 00:00 0
18:51:45     ffff7d040000-ffff7f040000 rw-p 00000000 00:00 0
18:51:45     ffff7f040000-ffff7f050000 ---p 00000000 00:00 0
18:51:45     ffff7f050000-ffff7f850000 rw-p 00000000 00:00 0
18:51:45     ffff7f850000-ffff81850000 rw-p 00000000 00:00 0
18:51:45     ffff81850000-ffff81860000 ---p 00000000 00:00 0
18:51:45     ffff81860000-ffff82060000 rw-p 00000000 00:00 0
18:51:45     ffff82060000-ffff82070000 ---p 00000000 00:00 0
18:51:45     ffff82070000-ffff82870000 rw-p 00000000 00:00 0
18:51:45     ffff82870000-ffff829c0000 r-xp 00000000 fd:02 7592180                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libgfortran-daac5196.so.5.0.0
18:51:45     ffff829c0000-ffff829d0000 r--p 00140000 fd:02 7592180                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libgfortran-daac5196.so.5.0.0
18:51:45     ffff829d0000-ffff82a00000 rw-p 00150000 fd:02 7592180                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libgfortran-daac5196.so.5.0.0
18:51:45     ffff82a00000-ffff83d40000 r-xp 00000000 fd:02 7592178                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libopenblasp-r0-32ff4d91.3.13.so
18:51:45     ffff83d40000-ffff83d50000 ---p 01340000 fd:02 7592178                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libopenblasp-r0-32ff4d91.3.13.so
18:51:45     ffff83d50000-ffff83d60000 r--p 01340000 fd:02 7592178                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libopenblasp-r0-32ff4d91.3.13.so
18:51:45     ffff83d60000-ffff83d80000 rw-p 01350000 fd:02 7592178                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libopenblasp-r0-32ff4d91.3.13.so
18:51:45     ffff83d80000-ffff83e00000 rw-p 014e0000 fd:02 7592178                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libopenblasp-r0-32ff4d91.3.13.so
18:51:45     ffff83e00000-ffff84190000 r-xp 00000000 fd:02 276318455                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84190000-ffff841a0000 r--p 00380000 fd:02 276318455                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff841a0000-ffff841c0000 rw-p 00390000 fd:02 276318455                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff841c0000-ffff841e0000 rw-p 00000000 00:00 0
18:51:45     ffff841e0000-ffff84200000 rw-p 00400000 fd:02 276318455                  /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84200000-ffff84280000 rw-p 00000000 00:00 0
18:51:45     ffff84280000-ffff84290000 r-xp 00000000 fd:02 270252825                  /usr/lib64/libbz2.so.1.0.6
18:51:45     ffff84290000-ffff842a0000 rw-p 00010000 fd:02 270252825                  /usr/lib64/libbz2.so.1.0.6
18:51:45     ffff842a0000-ffff84380000 rw-p 00000000 00:00 0
18:51:45     ffff84380000-ffff843a0000 r-xp 00000000 fd:02 272114713                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_pickle.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff843a0000-ffff843b0000 r--p 00010000 fd:02 272114713                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_pickle.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff843b0000-ffff843c0000 rw-p 00020000 fd:02 272114713                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_pickle.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff843c0000-ffff843e0000 r-xp 00000000 fd:02 272114699                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_datetime.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff843e0000-ffff843f0000 r--p 00010000 fd:02 272114699                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_datetime.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff843f0000-ffff84400000 rw-p 00020000 fd:02 272114699                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_datetime.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84400000-ffff844c0000 rw-p 00000000 00:00 0
18:51:45     ffff844c0000-ffff844d0000 r-xp 00000000 fd:02 272114714                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_posixsubprocess.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff844d0000-ffff844e0000 r--p 00000000 fd:02 272114714                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_posixsubprocess.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff844e0000-ffff844f0000 rw-p 00010000 fd:02 272114714                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_posixsubprocess.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff844f0000-ffff84510000 r-xp 00000000 fd:02 7592179                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libz-558a5e64.so.1.2.7
18:51:45     ffff84510000-ffff84520000 r--p 00010000 fd:02 7592179                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libz-558a5e64.so.1.2.7
18:51:45     ffff84520000-ffff84540000 rw-p 00020000 fd:02 7592179                    /tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages/numpy.libs/libz-558a5e64.so.1.2.7
18:51:45     ffff84540000-ffff84550000 r-xp 00000000 fd:02 272114736                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/fcntl.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84550000-ffff84560000 r--p 00000000 fd:02 272114736                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/fcntl.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84560000-ffff84570000 rw-p 00010000 fd:02 272114736                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/fcntl.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84570000-ffff84580000 r-xp 00000000 fd:02 272114694                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_csv.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84580000-ffff84590000 r--p 00000000 fd:02 272114694                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_csv.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84590000-ffff845a0000 rw-p 00010000 fd:02 272114694                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_csv.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff845a0000-ffff845e0000 rw-p 00000000 00:00 0
18:51:45     ffff845e0000-ffff845f0000 r-xp 00000000 fd:02 272114746                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/select.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff845f0000-ffff84600000 r--p 00000000 fd:02 272114746                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/select.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84600000-ffff84610000 rw-p 00010000 fd:02 272114746                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/select.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84610000-ffff84630000 r-xp 00000000 fd:02 272114721                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_socket.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84630000-ffff84640000 r--p 00010000 fd:02 272114721                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_socket.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84640000-ffff84650000 rw-p 00020000 fd:02 272114721                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_socket.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84650000-ffff84880000 r-xp 00000000 fd:02 272114704                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_hashlib.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84880000-ffff848b0000 r--p 00220000 fd:02 272114704                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_hashlib.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff848b0000-ffff848c0000 rw-p 00250000 fd:02 272114704                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_hashlib.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff848c0000-ffff84980000 rw-p 00000000 00:00 0
18:51:45     ffff84990000-ffff849a0000 r-xp 00000000 fd:02 272114749                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/termios.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff849a0000-ffff849b0000 r--p 00000000 fd:02 272114749                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/termios.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff849b0000-ffff849c0000 rw-p 00010000 fd:02 272114749                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/termios.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff849c0000-ffff84a80000 rw-p 00000000 00:00 0
18:51:45     ffff84a90000-ffff84aa0000 r-xp 00000000 fd:02 272114716                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_random.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84aa0000-ffff84ab0000 r--p 00000000 fd:02 272114716                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_random.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84ab0000-ffff84ac0000 rw-p 00010000 fd:02 272114716                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_random.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84ac0000-ffff84ad0000 r-xp 00000000 fd:02 272114681                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_bisect.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84ad0000-ffff84ae0000 r--p 00000000 fd:02 272114681                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_bisect.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84ae0000-ffff84af0000 rw-p 00010000 fd:02 272114681                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_bisect.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84af0000-ffff84b00000 r-xp 00000000 fd:02 272114719                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_sha3.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84b00000-ffff84b10000 r--p 00000000 fd:02 272114719                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_sha3.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84b10000-ffff84b20000 rw-p 00010000 fd:02 272114719                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_sha3.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84b20000-ffff84b30000 r-xp 00000000 fd:02 272114682                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_blake2.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84b30000-ffff84b40000 r--p 00000000 fd:02 272114682                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_blake2.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84b40000-ffff84b50000 rw-p 00010000 fd:02 272114682                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_blake2.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84b50000-ffff84b60000 r-xp 00000000 fd:02 272114738                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/math.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84b60000-ffff84b70000 r--p 00000000 fd:02 272114738                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/math.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84b70000-ffff84b80000 rw-p 00010000 fd:02 272114738                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/math.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff84b80000-ffff84c40000 rw-p 00000000 00:00 0
18:51:45     ffff84c40000-ffff86650000 r-xp 00000000 fd:02 277818305                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/libtensorflow_framework.so.2.9.0
18:51:45     ffff86650000-ffff86660000 ---p 01a10000 fd:02 277818305                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/libtensorflow_framework.so.2.9.0
18:51:45     ffff86660000-ffff86710000 r--p 01a10000 fd:02 277818305                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/libtensorflow_framework.so.2.9.0
18:51:45     ffff86710000-ffff86720000 rw-p 01ac0000 fd:02 277818305                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/libtensorflow_framework.so.2.9.0
18:51:45     ffff86720000-ffff86740000 rw-p 00000000 00:00 0
18:51:45     ffff86740000-ffff959d0000 r-xp 00000000 fd:02 409165075                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so
18:51:45     ffff959d0000-ffff95f80000 r--p 0f280000 fd:02 409165075                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so
18:51:45     ffff95f80000-ffff95fc0000 rw-p 0f830000 fd:02 409165075                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so
18:51:45     ffff95fc0000-ffff96040000 rw-p 00000000 00:00 0
18:51:45     ffff96040000-ffff96060000 r-xp 00000000 fd:02 270259739                  /usr/lib64/libgcc_s-4.8.5-20150702.so.1
18:51:45     ffff96060000-ffff96070000 r--p 00010000 fd:02 270259739                  /usr/lib64/libgcc_s-4.8.5-20150702.so.1
18:51:45     ffff96070000-ffff96080000 rw-p 00020000 fd:02 270259739                  /usr/lib64/libgcc_s-4.8.5-20150702.so.1
18:51:45     ffff96080000-ffff96090000 r-xp 00000000 fd:02 405191957                  /usr/lib64/librt-2.17.so
18:51:45     ffff96090000-ffff960a0000 r--p 00000000 fd:02 405191957                  /usr/lib64/librt-2.17.so
18:51:45     ffff960a0000-ffff960b0000 rw-p 00010000 fd:02 405191957                  /usr/lib64/librt-2.17.so
18:51:45     ffff960c0000-ffff96280000 r-xp 00000000 fd:02 282369312                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/platform/_cpu_feature_guard.so
18:51:45     ffff96280000-ffff96290000 ---p 001c0000 fd:02 282369312                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/platform/_cpu_feature_guard.so
18:51:45     ffff96290000-ffff962a0000 r--p 001c0000 fd:02 282369312                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/platform/_cpu_feature_guard.so
18:51:45     ffff962a0000-ffff962b0000 rw-p 001d0000 fd:02 282369312                  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/platform/_cpu_feature_guard.so
18:51:45     ffff962b0000-ffff962f0000 rw-p 00000000 00:00 0
18:51:45     ffff962f0000-ffff96300000 r-xp 00000000 fd:02 272114724                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_struct.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff96300000-ffff96310000 r--p 00000000 fd:02 272114724                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_struct.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff96310000-ffff96320000 rw-p 00010000 fd:02 272114724                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_struct.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff96320000-ffff96330000 r-xp 00000000 fd:02 270259730                  /usr/lib64/libffi.so.6.0.1
18:51:45     ffff96330000-ffff96340000 r--p 00000000 fd:02 270259730                  /usr/lib64/libffi.so.6.0.1
18:51:45     ffff96340000-ffff96350000 rw-p 00010000 fd:02 270259730                  /usr/lib64/libffi.so.6.0.1
18:51:45     ffff96350000-ffff96370000 r-xp 00000000 fd:02 272114695                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_ctypes.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff96370000-ffff96380000 r--p 00010000 fd:02 272114695                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_ctypes.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff96380000-ffff96390000 rw-p 00020000 fd:02 272114695                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_ctypes.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff96390000-ffff964e0000 rw-p 00000000 00:00 0
18:51:45     ffff964e0000-ffff964f0000 r-xp 00000000 fd:02 405191895                  /usr/lib64/libnss_files-2.17.so
18:51:45     ffff964f0000-ffff96500000 r--p 00000000 fd:02 405191895                  /usr/lib64/libnss_files-2.17.so
18:51:45     ffff96500000-ffff96510000 rw-p 00010000 fd:02 405191895                  /usr/lib64/libnss_files-2.17.so
18:51:45     ffff96510000-ffff96520000 r-xp 00000000 fd:02 272114705                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_heapq.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff96520000-ffff96530000 r--p 00000000 fd:02 272114705                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_heapq.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff96530000-ffff96540000 rw-p 00010000 fd:02 272114705                  /opt/_internal/cpython-3.7.12/lib/python3.7/lib-dynload/_heapq.cpython-37m-aarch64-linux-gnu.so
18:51:45     ffff96540000-ffff96740000 rw-p 00000000 00:00 0
18:51:45     ffff96740000-ffff968d0000 r--p 00000000 fd:02 137409502                  /usr/lib/locale/locale-archive
18:51:45     ffff968d0000-ffff96a40000 r-xp 00000000 fd:02 405191738                  /usr/lib64/libc-2.17.so
18:51:45     ffff96a40000-ffff96a50000 r--p 00160000 fd:02 405191738                  /usr/lib64/libc-2.17.so
18:51:45     ffff96a50000-ffff96a60000 rw-p 00170000 fd:02 405191738                  /usr/lib64/libc-2.17.so
18:51:45     ffff96a60000-ffff96b00000 r-xp 00000000 fd:02 405191856                  /usr/lib64/libm-2.17.so
18:51:45     ffff96b00000-ffff96b10000 r--p 00090000 fd:02 405191856                  /usr/lib64/libm-2.17.so
18:51:45     ffff96b10000-ffff96b20000 rw-p 000a0000 fd:02 405191856                  /usr/lib64/libm-2.17.so
18:51:45     ffff96b20000-ffff96b30000 r-xp 00000000 fd:02 405191997                  /usr/lib64/libutil-2.17.so
18:51:45     ffff96b30000-ffff96b40000 r--p 00000000 fd:02 405191997                  /usr/lib64/libutil-2.17.so
18:51:45     ffff96b40000-ffff96b50000 rw-p 00010000 fd:02 405191997                  /usr/lib64/libutil-2.17.so
18:51:45     ffff96b50000-ffff96b60000 r-xp 00000000 fd:02 405191759                  /usr/lib64/libdl-2.17.so
18:51:45     ffff96b60000-ffff96b70000 r--p 00000000 fd:02 405191759                  /usr/lib64/libdl-2.17.so
18:51:45     ffff96b70000-ffff96b80000 rw-p 00010000 fd:02 405191759                  /usr/lib64/libdl-2.17.so
18:51:45     ffff96b80000-ffff96ba0000 r-xp 00000000 fd:02 405191936                  /usr/lib64/libpthread-2.17.so
18:51:45     ffff96ba0000-ffff96bb0000 r--p 00010000 fd:02 405191936                  /usr/lib64/libpthread-2.17.so
18:51:45     ffff96bb0000-ffff96bc0000 rw-p 00020000 fd:02 405191936                  /usr/lib64/libpthread-2.17.so
18:51:45     ffff96bc0000-ffff96bf0000 r-xp 00000000 fd:02 134274849                  /usr/local/lib/libcrypt.so.2
18:51:45     ffff96bf0000-ffff96c00000 r--p 00020000 fd:02 134274849                  /usr/local/lib/libcrypt.so.2
18:51:45     ffff96c00000-ffff96c10000 rw-p 00000000 00:00 0
18:51:45     ffff96c10000-ffff96c20000 rwxp 00000000 00:00 0
18:51:45     ffff96c20000-ffff96c40000 r--p 00000000 00:00 0                          [vvar]
18:51:45     ffff96c40000-ffff96c50000 r-xp 00000000 00:00 0                          [vdso]
18:51:45     ffff96c50000-ffff96c70000 r-xp 00000000 fd:02 405191638                  /usr/lib64/ld-2.17.so
18:51:45     ffff96c70000-ffff96c80000 r--p 00010000 fd:02 405191638                  /usr/lib64/ld-2.17.so
18:51:45     ffff96c80000-ffff96c90000 rw-p 00020000 fd:02 405191638                  /usr/lib64/ld-2.17.so
18:51:45     ffffc78e0000-ffffc7910000 rw-p 00000000 00:00 0                          [stack]
18:51:45     /bin/bash: line 1: 51393 Aborted 
"
53631,Request for grouped convolutions on CPU,"**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
Grouped convolutions are not currently supported on CPUs in graph mode. We get the following error when we try to do the same:
```
UnimplementedError:  Fused conv implementation does not support grouped convolutions for now.
	 [[node sequential_2/sequential/conv2d/BiasAdd
 (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py:265)
]] [Op:__inference_predict_function_1730]
```
Till date, there have been numerous issues on TF's GitHub repo regarding this. Owing to that, I am requesting the team to look into this.

**Will this change the current api? How?**
`model.predict()` with a model having grouped convolutions will not result in an error on a CPU.

**Who will benefit with this feature?**
Many architectures today use grouped convolutions as they are known to be more efficient while maintaining the accuracy. Thus, I believe many TF developers will benefit from this addition. 
"
53629,"Building with Bazel failed during fetching of repository ""go sdk""","Happy New Year 2022!

I've been trying to build TF 2.6 from source using Bazel on my Ubuntu machine. Unfortunately, I got stuck at the very beginning step of installing dependencies for TF. The error says that bazel failed to fetch (and also extract?) GO SDK package (see log below). My system details and building commands are also provided.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 21.04 // 16 cores AMD Ryzen 7 1700 processors with 16 GB of RAM.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6
- Python version: 3.9.9
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3
- CUDA/cuDNN version: no
- GPU model and memory: no

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I had cloned TF from my repo forked from the official TF repo, checkouted `r2.6`, and then configured the build (`./configure`) and left answers empty (answered No to all questions).

```
bazel build --jobs=8 --local_ram_resources=""HOST_RAM*.50"" \
  --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" -c opt \
  --config=noaws --config=nogcp --config=nohdfs --config=nonccl --config=v2 \
  //tensorflow:libtensorflow.so \
  //tensorflow:libtensorflow_cc.so  \
  //tensorflow:libtensorflow_framework.so  \
  //tensorflow:install_headers
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
WARNING: Output base '/home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a' is on NFS. This may lead to surprising failures and undetermined behavior.
WARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=83
INFO: Reading rc options for 'build' from /home/rketka/github/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/rketka/github/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /home/rketka/github/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/rketka/miniconda3/envs/tfcc/bin/python3 --action_env PYTHON_LIB_PATH=/home/rketka/miniconda3/envs/tfcc/lib/python3.9/site-packages --python_path=/home/rketka/miniconda3/envs/tfcc/bin/python3
INFO: Found applicable config definition build:short_logs in file /home/rketka/github/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/rketka/github/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:noaws in file /home/rketka/github/tensorflow/.bazelrc: --define=no_aws_support=true
INFO: Found applicable config definition build:nogcp in file /home/rketka/github/tensorflow/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:nohdfs in file /home/rketka/github/tensorflow/.bazelrc: --define=no_hdfs_support=true
INFO: Found applicable config definition build:nonccl in file /home/rketka/github/tensorflow/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:v2 in file /home/rketka/github/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/rketka/github/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false
INFO: Found applicable config definition build:dynamic_kernels in file /home/rketka/github/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/b570a1921c9e55ac53c8972bd2bfd37cd0eb510d.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /home/rketka/github/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/rketka/github/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Repository go_sdk instantiated at:
  /home/rketka/github/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/rketka/github/tensorflow/tensorflow/workspace0.bzl:120:20: in workspace
  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains
  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  /home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>
ERROR: An error occurred during the fetch of repository 'go_sdk':
   Traceback (most recent call last):
	File ""/home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/io_bazel_rules_go/go/private/sdk.bzl"", line 51, column 16, in _go_download_sdk_impl
		_remote_sdk(ctx, [url.format(filename) for url in ctx.attr.urls], ctx.attr.strip_prefix, sha256)
	File ""/home/rketka/.cache/bazel/_bazel_rketka/6f5ef4905e7610b7a736fc4fcd0da30a/external/io_bazel_rules_go/go/private/sdk.bzl"", line 128, column 17, in _remote_sdk
		fail(""error extracting Go SDK:\n"" + res.stdout + res.stderr)
Error in fail: error extracting Go SDK:
Timed out
ERROR: Analysis of target '//tensorflow:libtensorflow_framework.so' failed; build aborted: error extracting Go SDK:
Timed out
INFO: Elapsed time: 1891.172s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (23 packages loaded, 32 targets configured)
```

Any ideas to tackle this issue? Hope that the info above is sufficient, but let me know if you need further info for investigation. 

Thank you very much.
Rangsiman"
53628,TensorFlow Lite C API TfLiteInterpreter misleading documentation,"This is documentation bug in TFLite C API. I am not sure if I chose a proper issue tag.

## Description of issue (what needs changing):

### Clear description

See [the documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api.h#L146-L158) of `TfLiteInterpreterCreate`.

According to this documentation, the snippet below is valid:
```c++
std::vector<char> model_data;
{
    // Fill model data    
}
TfLiteModel *model = TfLiteModelCreate(model_data.data(), model_data.size());
// Create the interpreter.
TfLiteInterpreter *interpreter = TfLiteInterpreterCreate(model, options);
// According to documentation, I can delete the model.
// Hence, the lifetime of the model ends.
TfLiteModelDelete(model);
```

From now on, according to [the documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api.h#L98-L104) of `TfLiteModelCreate`, `model_data` 
can be modified. Like the snippet below:
```c++
for (char& e : model_data) {
  e = 0;
}
```
This is just a demo to show that current or another process may modify the memory region.

However, this is not valid, and corrupts the interpreter severely. Deleting a `TfLiteModel` object is ok, if one reads it from a file, because the interpreter relies on the file which is _assumed_ to stay unmodified during the lifetime of the interpreter. On the other hand, lifetime of an Interpreter must be bounded by the model which is bounded by the lifetime of the model data to avoid errors. Similar statement can be found in [the documentation](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter#summary) of the C++ API (see the usage snippet).

I think @jdduke is the one who can fix the documentation properly, but my suggestion is to replace `... and can destroy it immediately after creating the interpreter; the interpreter will maintain its own reference to the underlying model data` part with ` and the model data must outlive the interpreter` to avoid confusion."
53626,CUDA_ERROR_ILLEGAL_ADDRESS when enable unified memory in multi-GPUs training,"I perform data parallel training on nvidia v100 based on horovod+tensorflow. Since a single gpu cannot accommodate the size of my model, I am trying to use tensorflow's unified memory through `per_process_gpu_memory_fractio`.

When I use 4 gpus for training and enable unified memory, everything is fine, and the performance loss is acceptable.

But when I used 8 gpus for training and turned on unified memory, I encountered the following error:
```
E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x7fd97d23d970: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
terminate called after throwing an instance of 'std::logic_error'
  what():  cudaEventSynchronize failed: an illegal memory access was encounteredterminate called after throwing an instance of 
```

- Tensorflow version: 1.13.1
- CUDA 10.0
- CentOS 7.2

I don’t know the reason for this error, any troubleshooting suggestions will be very much appreciated."
53625,model.build documentation absent on wensite,"The build method for model class is not documented or/and is not present on the [page](https://www.tensorflow.org/api_docs/python/tf/keras/Model#methods_2). Can this be added?
Thank you!
"
53621,"TF 2.4, CUDA 11.5.50, cudnn 8.3.1.22: Could not find any cudnn.h","**System information**
- OS Platform and Distribution: Slackware64 Linux (current), kernel 5.15.11
- TensorFlow installed from: source
- TensorFlow version: 2.4
- Python version: 3.9
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 11.2.0
- CUDA/cuDNN version: 11.5.50, 8.3.1.22
- GPU model and memory: Nvidia Quadro RTX 4000, 8GB

**the problem**

     # ./configure
gives this
```

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 11.5


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 8


Please specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: 


Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /usr/share/cuda/bin,/usr/share/cuda/include,/usr/include,/usr/lib64,/usr/share/cuda


Could not find any cudnn.h, cudnn_version.h matching version '8' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
        '/usr/include'
        '/usr/lib64'
        '/usr/share/cuda'
        '/usr/share/cuda/bin'
        '/usr/share/cuda/include'
```
Yet it is there:
```
$ ls /usr/share/cuda/include/cudnn.h
/usr/share/cuda/include/cudnn.h
```
I don't have any `cudnn_version.h`. Should I?"
53620,py_function is slower in TF2.7 (and 2.6) compared to TF2.5,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): docker
- TensorFlow version (use command below): 2.7
- Python version: 3.8.10

**Describe the current behavior**

In my training code I'm using `tf.data.Dataset.map` with a `tf.py_function` within it. I know that using `tf.py_function` is slower than regular tensorflow, but there's a part of my dataset processing that requires the usage of plain Python code.   

However, when I tried to upgrade TF from 2.5 to 2.6, I noticed that my trainings were slower in 2.6 compared to 2.5. I postponed the upgrade at the moment. When 2.7 came out I tried again but the slower data processing was still present.

**Describe the expected behavior**

I would have expected this kind of code to be as efficient as it was in 2.5

**Standalone code to reproduce the issue**

You can find a reproducible example in [this colab](https://colab.research.google.com/drive/1AmOnuNosr7nq5p4m25jJwG-I2R9r7PTf?usp=sharing).

To reproduce my issue, I wrapped the opening of the image on disk within a `tf.py_function` and opened it using Pillow and numpy. I know that this can be done in a more efficient way as I explained directly in the colab.

Here are the results I got when executing the `%%timeit` cell with 2 different versions of TF:
- tf.__version__ = ""2.7.0"" -> 3 loops, best of 7: 1.81 s per loop (would have been the same in TF2.6)
- tf.__version__ = ""2.5.2"" -> 3 loops, best of 7: 1.44 s per loop
"
53616,Android TensorFlow GPU Delegate,"I want to apply cartoon effect to picture in android (java/kotlin or c/c++) with tensorflow lite. (tflite model / dr, fp16, int8 quantization)

I found [this tutorial.](https://blog.tensorflow.org/2020/09/how-to-create-cartoonizer-with-tf-lite.html)

[This code](https://github.com/margaretmz/Cartoonizer-with-TFLite) is also based on it.

My demo app works on CPU with 4 threads but when I want to test it with GPU delegate gives an [error.](https://i.stack.imgur.com/4whGx.jpg)

I used several devices and the result was the same in all of them.

```
// Check if device supports GPU delegate

val compatList = CompatibilityList()

val isSupported = compatList.isDelegateSupportedOnThisDevice

// If device does not support, then it will be done on 4 CPU threads

val options = Model.Options.Builder().apply {

if(isSupported) setDevice(Model.Device.GPU)

else setNumThreads(4)

}.build()

val model = WhiteboxCartoonGanFp16.newInstance(this, options)

val process = model.process(picture)
                     
val cartoon = process.cartoonizedImageAsTensorImage

val bitmap = image.bitmap
                                    
model.close()
```"
53615,SelfAdjointEigV2 GPU operation takes a lot of temporary memory.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.9.0
- Python version: 3.8.10
- CUDA/cuDNN version: 11.5
- GPU model and memory: GTX 1660 Ti

**Describe the current behavior**
A single call to `tf.linalg.eigh()` takes a linear amount of memory in batch size, despite being processed matrix by matrix. I think the ScratchSpace is only freed in the end of the call after ALL matrices in the batch are processed, instead of on the fly matrix, per matrix. This I conclude from the enormous amount of allocations reported by the allocator during the OOM, and looking at the code.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): maybe
- Briefly describe your candidate solution(if contributing):
  - **Best solution:** reuse the ScratchSpace for every matrix.  
  - **Next best solution:** free the ScratchSpace after every matrix in the batch.

**Standalone code to reproduce the issue**
```py
import tensorflow as tf
tensor = tf.random.uniform((1024 * 256, 4, 4)) # just make sure the batch size is big enough
tensor = tf.matmul(tensor, tensor, transpose_b=True)
t = tf.linalg.eigvalsh(tensor)
```

**Other info / logs**
See below the summary of the allocator:
```
2022-01-03 16:18:32.407928: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 215.4KiB (rounded to 220672)requested by op SelfAdjointEigV2

2022-01-03 16:18:32.518897: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: 
2022-01-03 16:18:32.518906: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 256 totalling 256B
2022-01-03 16:18:32.518916: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB
2022-01-03 16:18:32.518924: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 65536 totalling 64.0KiB
2022-01-03 16:18:32.518930: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 11841 Chunks of size 220672 totalling 2.43GiB
2022-01-03 16:18:32.518937: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 251648 totalling 245.8KiB
2022-01-03 16:18:32.518944: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 410880 totalling 401.2KiB
2022-01-03 16:18:32.518951: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4194304 totalling 4.00MiB
2022-01-03 16:18:32.518958: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 268435456 totalling 512.00MiB
2022-01-03 16:18:32.518964: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 2.94GiB
2022-01-03 16:18:32.518970: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 3154771968 memory_limit_: 3154771968 available bytes: 0 curr_region_allocation_bytes_: 6309543936
2022-01-03 16:18:32.518982: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: 
Limit:                      3154771968
InUse:                      3154771968
MaxInUse:                   3154771968
NumAllocs:                       11850
MaxAllocSize:                268435456
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-01-03 16:18:32.519244: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ****************************************************************************************************
2022-01-03 16:18:32.519294: F ./tensorflow/core/util/gpu_solvers.h:533] Non-OK-status: context->allocate_temp(DataTypeToEnum<Scalar>::value, shape, &scratch_tensor_, alloc_attr) status: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[55137] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
```

Especially this line:  
```
...r.cc:1074] 11841 Chunks of size 220672 totalling 2.43GiB
```
So here, I conclude that it crashes when it's trying to process matrix number 11842 out of 16384.

Pointers:
 - ScratchSpace request in the `HeevdImpl` call (at line 635): https://github.com/tensorflow/tensorflow/blob/322cba072c9313689aeb2fc3174f18fce57194d7/tensorflow/core/util/cuda_solvers.cc#L620-L643
 - Batch processing matrix by matrix: https://github.com/tensorflow/tensorflow/blob/322cba072c9313689aeb2fc3174f18fce57194d7/tensorflow/core/kernels/linalg/self_adjoint_eig_v2_op_gpu.cc#L130-L140"
53614," Error loading option @local_config_cuda//:enable_cuda: Argument 0 of execute is neither a path, label, nor string.","**System information**
- OS Platform : Windows 11
- TensorFlow version: 2.8
- Python version: 3.9.9
- Bazel version : 421
- GPU model and memory: GTX1650
- CUDA/CUdnn: 11.5/8.3.1


**Describe the problem**
Problem when build from source.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=156
INFO: Reading rc options for 'build' from c:\users\dubli\downloads\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=E:/Python3/python.exe
INFO: Reading rc options for 'build' from c:\users\dubli\downloads\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from c:\users\dubli\downloads\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=E:/Python3/python.exe --action_env PYTHON_LIB_PATH=E:/Python3/lib/site-packages --python_path=E:/Python3/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.5 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from c:\users\dubli\downloads\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file c:\users\dubli\downloads\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\dubli\downloads\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file c:\users\dubli\downloads\tensorflow\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file c:\users\dubli\downloads\tensorflow\.tf_configure.bazelrc: --copt=/arch:FMA3 --host_copt=/arch:FMA3
INFO: Found applicable config definition build:windows in file c:\users\dubli\downloads\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\dubli\downloads\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Repository local_config_cuda instantiated at:
  C:/users/dubli/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/dubli/downloads/tensorflow/tensorflow/workspace2.bzl:878:19: in workspace
  C:/users/dubli/downloads/tensorflow/tensorflow/workspace2.bzl:96:19: in _tf_toolchains
Repository rule cuda_configure defined at:
  C:/users/dubli/downloads/tensorflow/third_party/gpus/cuda_configure.bzl:1448:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""C:/users/dubli/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""C:/users/dubli/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1182, column 53, in _create_local_cuda_repository
                host_compiler_includes = get_cxx_inc_directories(
        File ""C:/users/dubli/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 306, column 49, in get_cxx_inc_directories
                includes_cpp = _get_cxx_inc_directories_impl(
        File ""C:/users/dubli/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 277, column 22, in _get_cxx_inc_directories_impl
                result = raw_exec(repository_ctx, [cc, ""-E"", ""-x"" + lang, ""-"", ""-v""] +
        File ""C:/users/dubli/downloads/tensorflow/third_party/remote_config/common.bzl"", line 252, column 34, in raw_exec
                return repository_ctx.execute(cmdline)
Error in execute: Argument 0 of execute is neither a path, label, nor string.
ERROR: Error fetching repository: Traceback (most recent call last):
        File ""C:/users/dubli/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""C:/users/dubli/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1182, column 53, in _create_local_cuda_repository
                host_compiler_includes = get_cxx_inc_directories(
        File ""C:/users/dubli/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 306, column 49, in get_cxx_inc_directories
                includes_cpp = _get_cxx_inc_directories_impl(
        File ""C:/users/dubli/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 277, column 22, in _get_cxx_inc_directories_impl
                result = raw_exec(repository_ctx, [cc, ""-E"", ""-x"" + lang, ""-"", ""-v""] +
        File ""C:/users/dubli/downloads/tensorflow/third_party/remote_config/common.bzl"", line 252, column 34, in raw_exec
                return repository_ctx.execute(cmdline)
Error in execute: Argument 0 of execute is neither a path, label, nor string.
INFO: Found applicable config definition build:cuda in file c:\users\dubli\downloads\tensorflow\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: Argument 0 of execute is neither a path, label, nor string.

**Any other info / logs**
ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: Argument 0 of execute is neither a path, label, nor string.
"
53613,Eager loading of datasets,"**System information**
- TensorFlow version (you are using): 2.6.0, 2.7.0
- Are you willing to contribute it (Yes/No): No (I don't have the required knowledge)



**Describe the feature and the current behavior/state.**

In #53292, I mentioned the inability to load datasets from temporary files and mentioned eager dataset load as a potential solution. This led to the discovery and correction of a bug in `Dataset.cache`.

I still believe that eager dataset load is desirable, since calling `cache` then consuming the dataset to achieve the same result feels at best like a makeshift hack. Hence I'm making this new issue as a more explicit feature request, because I do believe that this feature would be beneficial in some cases.

**Will this change the current api? How?**

The feature I propose is to allow callers of `tf.data.experimental.load` to specify whether they want a lazily-loaded dataset, or an eagerly-loaded dataset (that reads the files only once when `load` is called, and stores the data in memory).

The way I see it, this evolution would add a new optional parameter to `tf.data.experimental.load`. Its default value needs to correspond to lazy loading for backward compatibility.

This would not pose a big issue since 1. it will be backward-compatible and 2. only the experimental API will be modified (and the documentation explicitly says the experimental API may evolve at any time).

**Who will benefit from this feature?**

Anyone who currently needs to cache a loaded dataset to memory, be it because the dataset comes from temporary files or because it is accessed very frequently

**Any Other info.**

As in the previous issue:

The fact that the datasets are lazy-loaded is not documented, it would be nice to add this to the current documentation of `tf.data.experimental.load`.

Also on the subject of documentation, once (if) the eager load option is added, the documentation should warn the user about the potentially high memory usage when eager-loading large datasets."
53612,INVALID_ARGUMENT: transpose expects a vector of size 0 (when GPU units are more than 1),"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 20.04.2 LTS
- TensorFlow installed from (source or binary): Yes
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8.5
- CUDA/cuDNN version &  GPU model and memory:

![image](https://user-images.githubusercontent.com/9564095/147903851-129bf763-dcb2-4f47-b136-14f31ad67553.png)


I am using a pre-trained model to train an image classifier. Below Code is running fine on CPU and single unit GPU (i.e. when #GPU=1)

```
class Metrics(tf.keras.callbacks.Callback):
    def __init__(self, train_tf_data, val_tf_data, CLASSES, logs={}, **kwargs):
        super().__init__(**kwargs)
        # self.keras_metric = tf.keras.metrics.Mean(""val_f1_after_epoch"")
        self.train_tf_data = train_tf_data
        self.val_tf_data = val_tf_data
        # self.model = model
        self.CLASSES = CLASSES

    def on_epoch_end(self, epoch, logs={}):
        # self.keras_metric.reset_state()
        # for train data
        self.train_reports = test_model(model=self.model, data=self.train_tf_data, CLASSES=self.CLASSES)
        self.train_f1_after_epoch = self.train_reports['f1_score']
        self.train_recall_after_epoch = self.train_reports['recall']
        self.train_prec_after_epoch = self.train_reports['precision']

        # for val data
        self.val_reports = test_model(model=self.model, data=self.val_tf_data, CLASSES=self.CLASSES)
        self.val_f1_after_epoch = self.val_reports['f1_score']
        self.val_recall_after_epoch = self.val_reports['recall']
        self.val_prec_after_epoch = self.val_reports['precision']

        # saving train results to log dir
        logs[""f1_after_epoch""]=self.train_f1_after_epoch
        logs['precision_after_epoch'] = self.train_prec_after_epoch
        logs['recall_after_epoch'] = self.train_recall_after_epoch
        
        # saving val results to log dir
        logs['val_f1_after_epoch'] = self.val_f1_after_epoch
        logs['val_precision_after_epoch'] = self.val_prec_after_epoch
        logs['val_recall_after_epoch'] = self.val_recall_after_epoch
        # self.keras_metric.update_state(self.val_f1_after_epoch)

        print('reports_after_epoch', self.train_reports)
        print('val_reports_after_epoch', self.val_reports)
        



with strategy.scope():
    pretrained_model = tf.keras.applications.MobileNetV2(
                                                    weights='imagenet',
                                                    include_top=False,
                                                    input_shape=[*IMAGE_SIZE, IMG_CHANNELS])
    pretrained_model.trainable = True #fine tuning
    q_aware_pretrained_model = tf.keras.models.clone_model(pretrained_model,
                                                          clone_function=apply_quantization_to_dense,)
    base_model = tf.keras.Sequential([
                            tf.keras.layers.Lambda(# Convert image from int[0, 255] to the format expect by this base_model
                            lambda data:tf.keras.applications.mobilenet.preprocess_input(
                                tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3]),
                            q_aware_pretrained_model,
                            tf.keras.layers.GlobalAveragePooling2D()])
    base_model.layers[1]._name = 'custom_mnet_trainable'
    base_model.add(tf.keras.layers.Dense(64, name='object_dense',kernel_regularizer=tf.keras.regularizers.l2(l2=0.1)))
    base_model.add(tf.keras.layers.BatchNormalization(scale=False, center = False))
    base_model.add(tf.keras.layers.Activation('relu', name='relu_dense_64'))
    base_model.add(tf.keras.layers.Dropout(rate=0.5, name='dropout_dense_64'))
    base_model.add(tf.keras.layers.Dense(32, name='object_dense_2',kernel_regularizer=tf.keras.regularizers.l2(l2=0.1)))
    base_model.add(tf.keras.layers.BatchNormalization(scale=False, center = False))
    base_model.add(tf.keras.layers.Activation('relu', name='relu_dense_32'))
    base_model.add(tf.keras.layers.Dropout(rate=0.4, name='dropout_dense_32'))
    base_model.add(tf.keras.layers.Dense(16, name='object_dense_16', kernel_regularizer=tf.keras.regularizers.l2(l2=0.1)))
    base_model.add(tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax', name='object_prob'))
    m1 = tf.keras.metrics.CategoricalAccuracy()
    m2 = tf.keras.metrics.Recall()
    m3 = tf.keras.metrics.Precision()

    m4 = Metrics(train_tf_data=train_data, val_tf_data=test_data, CLASSES=CLASS_NAMES)


    optimizers = [
        tfa.optimizers.AdamW(learning_rate=lr * .001 , weight_decay=wd),
        tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)
            ]

    optimizers_and_layers = [(optimizers[0], base_model.layers[0]), (optimizers[1], base_model.layers[1:])]

    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)

    annotated_model = tf.keras.models.clone_model(
        base_model,
        clone_function=apply_quantization_to_dense,
    )


    model = tfmot.quantization.keras.quantize_apply(annotated_model)
    model.compile(
        optimizer= optimizer, loss=tfa.losses.SigmoidFocalCrossEntropy(reduction=tf.keras.losses.Reduction.AUTO),
        metrics=[m1, m2, m3],
        )

tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)

checkpoint_name = os.getcwd() + os.sep + CUSTOM_MODEL_PATH + os.sep + ""training_chkpts/cp-{epoch:04d}-{val_f1_after_epoch:.2f}.ckpt""
checkpoint_dir_path  = os.getcwd() + os.sep + CUSTOM_MODEL_PATH + os.sep+ ""training_chkpts""
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_name, 
                                                    monitor = 'val_f1_after_epoch',
                                                    save_best_only=True,
                                                    save_weights_only=True,
                                                    mode='max',
                                                    save_freq='epoch',
                                                    verbose=1)

checkpoint_cb._supports_tf_logs = False
current_dir = os.getcwd()
history = model.fit(train_data, validation_data=test_data, 
                    epochs=N_EPOCHS,
                    callbacks=[m4, checkpoint_cb, tensorboard_cb])
```
But If I use a system when the number of GPU > 1 then it is throwing the below error.

Epoch 1/2 6/Unknown - 44s 150ms/step - loss: 19.2255 - categorical_accuracy: 0.0625 - recall: 0.0000e+00 - precision: 0.0000e+00

/bwz_venv/lib/python3.8/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument. layer_config = serialize_layer_fn(layer) 288/Unknown - 84s 141ms/step - loss: 13.7873 - categorical_accuracy: 0.1788 - recall: 0.0080 - precision: 0.77082021-12-30 15:08:31.404434: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at transpose_op.cc:142 : INVALID_ARGUMENT: transpose expects a vector of size 0. But input(1) is a vector of size 4

Traceback (most recent call last): File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main return _run_code(code, main_globals, None, File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code exec(code, run_globals) File ""/ssd/custom_mnet_v2.py"", line 536, in history = model.fit(train_data, validation_data=test_data, File ""bwz_venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler raise e.with_traceback(filtered_tb) from None File ""/bwz_venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.

(0) INVALID_ARGUMENT: transpose expects a vector of size 0. But input(1) is a vector of size 4 [[{{node gradient_tape/replica_1/sequential/custom_mnet_trainable/Conv1/Conv2D/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]] [[div_no_nan_3/ReadVariableOp/_558]]

(1) INVALID_ARGUMENT: transpose expects a vector of size 0. But input(1) is a vector of size 4 [[{{node gradient_tape/replica_1/sequential/custom_mnet_trainable/Conv1/Conv2D/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]] [[assert_less_equal/Assert/AssertGuard/else/_4049/assert_less_equal/Assert/AssertGuard/Assert/data_4/_546]]

(2) INVALID_ARGUMENT: transpose expects a vector of size 0. But input(1) is a vector of size 4 [[{{node gradient_tape/replica_1/sequential/custom_mnet_trainable/Conv1/Conv2D/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]] 0 successful operations. 0 derived errors ignored. [Op:__inference_train_function_1079980]

Function call stack: train_function -> train_function -> train_function

Few things that I have already tested

1) Tried out different metrics (categorical_accuracy) to check whether the issue is related to the custom monitoring metrics or not. Working fine
2) Run the code in CPU and single GPU environment and it is working perfectly fine

[Here](https://colab.research.google.com/drive/1n9MmaLKZORa0vQvluQcY3t-VuX-yiSxf?usp=sharing) is the link to the Google Colab Notebook to reproduce the error(please set #GPU>1)"
53611,Problem with setting up CUDA or cuDNN,"**System information**
- OS Platform and Distribution: Windows 10 21H2 19044.1415
- TensorFlow installed from: pip install (binary)
- TensorFlow version: 2.7.0 (keras too)
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: nope, pip
- CUDA/cuDNN version: cudnn-windows-x86_64-8.3.1.22 cuda 11.5 
- GPU model and memory: GTX 1050Ti 4GB


I get this error in terminal 
`
Could not load library cudnn_cnn_infer64_8.dll. Error code 126
Please make sure cudnn_cnn_infer64_8.dll is in your library path!
`

![installed cudnn .dll files](https://i.ibb.co/C7qgYB5/temp.jpg)


I also tried with v11.2 with modifiaction on system env. variable, did not help...

Also adding this lines of code did not helped
`
import os
os.add_dll_directory(""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.5/bin"")
`

The problem shows on clear python env. and in jupyter notebook."
53610,Loading a Keras model which contains __ne__ or __eq__ operators fails,"I posted this on Keras first, but it didn't get any attention, so posting here as well.
https://github.com/keras-team/keras/issues/15832"
53609,[XLA] The output of const type have some redundancy H2D ops,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): master
- Python version: 3.8
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 11.2
- GPU model and memory: ?? yes

**Describe the current behavior**
xla_run will move const type output from Host to Device in every steps.

**Describe the expected behavior**
Fill output from cache instead of H2D

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):yes
- Briefly describe your candidate solution(if contributing):
1. Record xla const output: 
```
map<output_idx, output_const_tensors> cache_;
```
2. Get output from cache; 
```
for (int i = 0; i < ctx->num_outputs(); ++i) {
  ...
  if (kernel->outputs[i].is_constant) {
    ctx->allocate_output(i, const_tensor.shape(), &output_tensor));
    output_tensor = cache_[i];
  else(others) {
    ...
  }
  ...
}
```

### Background
1. GPU training data H2D:
  Training data in CPU and model in GPU, so we need to move training data from host to device.We want to Increase training speed so we use ```prefetch_to_device``` to async H2D and training.
2. XLA H2D:
![image](https://user-images.githubusercontent.com/33950866/147870122-7d7fd471-2b63-4a4c-bae6-83ba86842b05.png)
(tensorflow/compiler/jit/xla_launch_util.cc:290, Status XlaComputationLaunchContext::PopulateOutputs)
### Problem
  Background 1 and Background 2 is in two completely unrelated threads, so XLA H2D maybe wait until features H2D done, like:
![image](https://user-images.githubusercontent.com/33950866/147870952-9ac17707-6059-4b7a-9415-a3622c1e236a.png)
### Solution
  I think that the output of the constant type has the same value at each step, so we don't need to have H2D for each step. We should cache the output in the device, and then get the result in the cache.
  I try to cache device tensor, like:
![image](https://user-images.githubusercontent.com/33950866/147870539-b48bc0d2-7ac7-40a7-9f50-80e0737b18ec.png)
"
53608,Tensorflow consuming too much GPU memory?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Windows 10
- Tensorflow 2.5.0 (from pip)
- Python version: 3.8.9
- CUDA/cuDNN version: CUDA 11.2 / CuDNN 8.1
- GPU model and memory: NVIDIA RTX 3080 (10GB)

**Describe the current behavior**
I am running a basic ResNet50 model w/ 23.6M params (~.1GB). This model takes up 8.2GB of GPU memory upon being loaded (after calling `model.compile`). **However, the largest batchsize I can run is 48 before I get OOM errors**. During training w/ batchsize 48, it takes up 9.1/10GB GPU memory. This batchsize seems very low to me for a 10GB RTX 3080 GPU. Is this expected or is this indeed a performance issue?   


**Describe the expected behavior**
I would imagine I can run the model with a much larger batchsize. 


**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

print(""TF version:"", tf.__version__)
print(""GPU is"", ""available"" if tf.config.list_physical_devices('GPU') else ""NOT AVAILABLE"")

BATCH_SIZE = 48
N_CLASSES = 27
INPUT_SHAPE = (240, 320, 3)

class DataGenerator(tf.keras.utils.Sequence):
    'Generates random data'

    def __init__(self, batch_size=32, dim=INPUT_SHAPE, n_classes=10):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.n_classes = n_classes

    def __len__(self):
        'Denotes the number of batches per epoch'
        return 10000000

    def __getitem__(self, index):
        'Generate one batch of data'
        X = np.random.uniform(size=([self.batch_size] + list(self.dim)))
        y = np.random.randint(0, self.n_classes, size=(self.batch_size, 1))
        return X, y


train_ds = DataGenerator(batch_size=BATCH_SIZE, n_classes=N_CLASSES)
valid_ds = DataGenerator(batch_size=BATCH_SIZE, n_classes=N_CLASSES)

base_model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=False, pooling='avg',
                                                               weights=None, input_shape=INPUT_SHAPE)
model = tf.keras.Sequential([
    # Explicitly define the input shape so the model can be properly
    # loaded by the TFLiteConverter
    tf.keras.layers.InputLayer(input_shape=INPUT_SHAPE),
    base_model,
    tf.keras.layers.Dense(N_CLASSES, kernel_regularizer=tf.keras.regularizers.l2(0.0001))
])
model.build((None,) + INPUT_SHAPE)
model.summary()

model.compile(
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

hist = model.fit(
    train_ds,
    epochs=5, steps_per_epoch=100,
    validation_data=valid_ds,
    validation_steps=10).history
```

**Output log**
```
C:\code\ai_dev\venv\Scripts\python.exe ""C:\Program Files\JetBrains\PyCharm Community Edition 2021.3\plugins\python-ce\helpers\pydev\pydevd.py"" --multiproc --qt-support=auto --client 127.0.0.1 --port 64926 --file C:/Users/Administrator/AppData/Roaming/JetBrains/PyCharmCE2021.3/scratches/scratch.py
Connected to pydev debugger (build 213.5744.248)
2022-01-01 21:10:24.141719: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
TF version: 2.5.0
2022-01-01 21:10:28.471977: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll
2022-01-01 21:10:28.503983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s
2022-01-01 21:10:28.504378: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
2022-01-01 21:10:28.938839: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2022-01-01 21:10:28.938949: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
2022-01-01 21:10:29.194104: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll
2022-01-01 21:10:29.225316: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll
2022-01-01 21:10:29.439959: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll
2022-01-01 21:10:29.640869: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll
2022-01-01 21:10:30.322655: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2022-01-01 21:10:30.322801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
GPU is available
2022-01-01 21:10:30.357061: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-01-01 21:10:30.358533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s
2022-01-01 21:10:30.358724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2022-01-01 21:10:31.060271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-01-01 21:10:31.060365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2022-01-01 21:10:31.060412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2022-01-01 21:10:31.062137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7440 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:09:00.0, compute capability: 8.6)
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
resnet50v2 (Functional)      (None, 2048)              23564800  
_________________________________________________________________
dense (Dense)                (None, 27)                55323     
=================================================================
Total params: 23,620,123
Trainable params: 23,574,683
Non-trainable params: 45,440
_________________________________________________________________
Backend TkAgg is interactive backend. Turning interactive mode on.
2022-01-01 21:15:37.453277: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/5
2022-01-01 21:15:47.718187: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2022-01-01 21:15:48.810810: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8101
2022-01-01 21:15:50.745911: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2022-01-01 21:15:51.640704: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
2022-01-01 21:15:53.390127: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.48GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-01 21:15:53.390348: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.48GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-01 21:15:53.401685: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2022-01-01 21:15:54.460765: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.48GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-01 21:15:54.461055: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.48GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-01 21:15:54.487507: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-01 21:15:54.487759: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-01 21:15:54.583271: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.32GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-01 21:15:54.583537: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.32GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-01 21:15:54.790347: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-01 21:15:54.790599: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
100/100 [==============================] - 40s 219ms/step - loss: 3.3853 - accuracy: 0.0202 - val_loss: 3.3123 - val_accuracy: 0.0000e+00
Epoch 2/5
100/100 [==============================] - 27s 203ms/step - loss: 3.3962 - accuracy: 0.0150 - val_loss: 3.3624 - val_accuracy: 0.0000e+00
Epoch 3/5

Process finished with exit code -1
```"
53606,Unable to restore custom metric.,"I have written a code for training a neural network in tensorflow where I added a custom metric just for the sake of printing the current learning rate at each epoch (this is useful for me when I use learning schedules). I trained my model and then I tried to load it again for re-training however I got the following error:

Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.

I tried the proposed solution in many other posts in which one use the custom_objects for indicating the custom metric, however I have not been able to solve this issue, below a simple code to reproduce the issue.

```python

import numpy as np
import tensorflow as tf
from tensorflow import keras


print(tf.__version__)

def get_lr_metric(optimizer):
    def lr(y_true, y_pred):
        return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr
    return lr


_input = tf.keras.layers.Input(shape=(500), name=""fbank"") # B*T*F*c
out = tf.keras.layers.Dense(50, activation=""tanh"")(_input)
probabilities = tf.keras.layers.Dense(2, activation=""softmax"")(out)
model = tf.keras.Model(inputs=_input, outputs=probabilities)
optimizerAdam = keras.optimizers.Adam(learning_rate=1e-5)
lr_metric = get_lr_metric(optimizerAdam)

model.compile(optimizerAdam, loss=tf.keras.losses.CategoricalCrossentropy(), 
              metrics=['accuracy', lr_metric])

model.summary()

x=np.random.rand(300,500)
y=np.random.rand(300,2)
model.fit(x,y,batch_size=100, epochs=2)

path = 'saved_model/'
model.save(path, save_format='tf')

del model
model = tf.keras.models.load_model('saved_model', compile=False,custom_objects={""lr_metric"": lr_metric})
print(""Done"")


```"
53605,"What can I use instead of ""tf.contrib.layers.embed_sequence""","I try to build a chatbot for studying purpose, but it seems like the lesson is quite out-dated, they used tensorflow v1 and there are some problems with that.

What can I use instead of ""tf.contrib.layers.embed_sequence"" since ""contrib"" is no longer available and I can't find anything in [tf_slim/layers](https://github.com/google-research/tf-slim/tree/master/tf_slim/layers) and [addons](https://github.com/tensorflow/addons), too."
53604,MaxPool2D crashes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7
- Python version: 3.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: cuda 11.2
- GPU model and memory: n/a

**Standalone code to reproduce the issue**
```
import tensorflow as tf
kernel_size = [1,3]
strides = [2,2]
layer = tf.keras.layers.MaxPool2D(kernel_size,strides=strides)
input_tensor = tf.random.uniform([3, 1, 1, 64], dtype=tf.float32)
layer(input_tensor) 
# Aborted (core dumped)
```
This code would crash with GPU. 

**Describe the expected behavior**
With CPU the output of the `MaxPool2D` layer would be a tensor of shape `(3, 1, 0, 64)`. I think the output should be similar with GPU, or at least there should also be some error checking instead of crashing.
"
53600,raggedtensor: how to convert a list of numpy array with a uniform dim in a certain axis to a ragged tensor,"suppose I have three numpy array with shape (1,3) and Stack them to group with shape (2,3) and (1,3). Then I stack them with tf.ragged.stack to get a ragged tensor

```
x1 = np.asarray([1,0,0])
x2 = np.asarray([0,1,0])
x3 = np.asarray([0,0,1])

group_a = np.stack([x1,x2])
group_b = np.stack([x3])


ac = tf.ragged.stack([group_a,group_b], axis=0)
```

I expect its shape to be (2, None, 3) but (2, None, None). How to implement it?? Using tf 2.5.2"
53599,Tensor Issue. 1,https://github.com/tensorflow/tensorflow/blob/c256c071bb26e1e13b4666d1b3e229e110bc914a/tensorflow/python/ops/tensor_array_ops.py#L961-L1296
53598,"Keras: how to define custom loss function for a ragged tensor output with shape(None, 1)","Suppose the output of my model is with shape (None,1) and how can I define custom loss function such as
```
def mycrossentropy(y_true, y_pred):
    loss = K.categorical_crossentropy(K.exp(y_true)/K.sum(K.exp(y_true)), K.exp(y_pred)/K.sum(K.exp(y_pred)))
    return loss
```
This function is used for normal tensor with shape (20,1) before. Now with new model, the output shape become (None, 1) ragged tensor. So, in this function y_pred should be a ragged tensor with shape(None, 1) and I can guarantee that y_true and y_pred have the same dimensions during training.

I am using tf 2.5.2 and tf.keras API for model define"
53597,error LNK2001 when build on windows with --config=dynamic_kernels,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10 19044.1415
- TensorFlow installed from: source
- TensorFlow version: master
- Python version: 3.9.7
- Installed using: conda
- Bazel version: 4.2.2
- GCC/Compiler version: msvc 19.29.30038.1
- CUDA/cuDNN version: cuda 11.1, cudnn 8.2.1
- GPU model and memory: GTX 1080TI 11G



**Describe the problem**

I'm trying to compile tensorflow with ""--config=dynamic_kernels"" so I can use custom op. What I did:
* Following https://www.tensorflow.org/install/source_windows?hl=zh-cn except using anaconda python
* Edit .bazelrc and replace ""build:windows --config=monolithic"" with ""build:windows --config=dynamic_kernels""
* run ""bazel build //tensorflow/tools/pip_package:build_pip_package --config=avx2_win""
When building goes into link stage I got a lot of  LNK2001 error.


**Any other info / logs**
[build-log.txt](https://github.com/tensorflow/tensorflow/files/7796223/build-log.txt)

"
53594,Keras: How to use another model as a layer in current model which input is ragged tensor and  output is also ragged tensor with same shape,"Suppose I have a base model take two inputs and output single value:

```
# define two input
input1 = keras.Input(shape=(100,), dtype=tf.int8)
input2 = keras.Input(shape=(20,), dtype=tf.int8)
# DNN for onehot feature
dense1 = Dense(32, activation='relu')(input1)
dense2 = Dense(4, activation='relu')(input2 )
# output 
output = Dense(1, activation='sigmoid')(Concatenate(axis=1)([dense1 , dense2 ]))
# define base model 
item_base_model = keras.Model(inputs=[input1, input2], outputs=output, name=""base_model"")
```

Then I have a model take (None,100) and (None, 20) array as two inputs:
```
# define input list
input1_list = keras.Input(shape=(None, 100],), dtype=tf.int8)
input2_list = keras.Input(shape=(None, 20,), dtype=tf.int16)
```

I want to ask that how I can call the based model for each input in the inpu1_list and input2_list and get their output as a tensor with shape (None,).  Finally, I will train the entire model"
53593,ValueError: node 'Placeholder' in input_map does not exist in graph (input_map entry: input_image:0->Placeholder:0),"### System information

-   **Have I written custom code**:
-   **OS Platform and Distribution (Windows 10)**:
-   **TensorFlow installed from (binary)**:
-   **TensorFlow version (1.15)**:
-   **Python version(3.7.6)**:
-   **CUDA/cuDNN version(10.0)**:
-   **GPU model and memory(GTX 1660ti and 6GB)**:


### Description of problem
I was running file.py but still, it is not running, I did some changes in the source file. Still, the placeholder error is the same. The code is also updated with this question. Kindly review and Help.

### Source code / logs

#### minimal code
    with tf.io.gfile.GFile(args.model, ""rb"") as f:
        graph_def = tf.compat.v1.GraphDef()
        graph_def.ParseFromString(f.read())
    
        with tf.Graph().as_default() as graph:
            generated_image_1, generated_image_2, generated_image_3, = tf.import_graph_def(
                    graph_def, 
                    input_map={'input_image' : input_tensor, 'short_edge_1' : short_edge_1, 'short_edge_2' : short_edge_2, 'short_edge_3' : short_edge_3}, 
                    return_elements=['style_subnet/conv-block/resize_conv_1/output:0', 'enhance_subnet/resize_conv_1/output:0', 'refine_subnet/resize_conv_1/output:0'], 
                    name=None, 
                    op_dict=None, 
                    producer_op_list=None  *#line number 55*
                )
    
    short_edges = [int(e) for e in args.hierarchical_short_edges.split(',')]

#### error

```
(myenv) D:\subbu>python file.py --model=models/model.pb --input_image=test_images/image.jpg
2021-12-25 00:54:11.765889: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
input image -  test_images/image.jpg (1200, 630, 3)
WARNING:tensorflow:From file.py:55: calling import_graph_def (from tensorflow.python.framework.importer) with op_dict is deprecated and will be removed in a future version.
Instructions for updating:
Please file an issue at https://github.com/tensorflow/tensorflow/issues if you depend on this feature.
Traceback (most recent call last):
  File ""C:\Users\subbu\myenv\lib\site-packages\tensorflow_core\python\framework\importer.py"", line 501, in _import_graph_def_internal
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: node 'Placeholder' in input_map does not exist in graph (input_map entry: input_image:0->Placeholder:0)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""file.py"", line 87, in <module>
    main()
  File ""file.py"", line 55, in main
    producer_op_list=None
  File ""C:\Users\subbu\myenv\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\subbu\myenv\lib\site-packages\tensorflow_core\python\framework\importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""C:\Users\subbu\myenv\lib\site-packages\tensorflow_core\python\framework\importer.py"", line 505, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: node 'Placeholder' in input_map does not exist in graph (input_map entry: input_image:0->Placeholder:0)
```"
53590,TF > 2.5 does not work properly for keras.fit for dictionary inputs,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.2, 2.6.2, 2.7
- Python version: 3.9
- CUDA/cuDNN version: 11.3
- GPU model and memory: 1080 Ti


**Describe the current behavior**

If I use TF 2.5.2 with keras.fit and inputs that are dictionaries for my multi input model, my model will fit.  If I used TF 2.6.* or 2.7.* I get out of memory errors.  

**Describe the expected behavior**

TF should not run out of memory in one version and not run out of memory in another, this model is definitively fittable with my GPU memory.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

I did find a solution and that was to start using tf.dataset with from_generator being based on my input dictionaries instead of directly using dictionaries for my multi input model.  Why this would make any difference I do not know and this is the bug that should be addressed.

**Standalone code to reproduce the issue**

Don't have a standalone case, but this should be easily identifiable, there should be unit tests around all the different ways of passing input to keras.fit.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53588,Request for RandomIllumination layer in keras,"Something like this already is possible with `ImageDataGenerator` but with corners in it (only applies brightness scale).
What is needed is a full keras layer that randomize illumination differences spatially across the image (or even a single scaling factor to the whole image).
i think more ideas can be added to something like this if it was implemented properly."
53587,PKIX path validation failed for giflib/giflib-5.2.1.tar.gz ( CertPathValidatorException ),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Linux Ubuntu 18
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7
- Python version: 3.8.10
- Installed using virtualenv? pip? conda?:  docker pull tensorflow/tensorflow:devel
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- CUDA/cuDNN version: 
- GPU model and memory: 



**Describe the problem**

**Reproduction step:**
 bazel build //tensorflow/tools/graph_transforms:transform_graph

**ERROR:** tensorflow/tensorflow/core/platform/default/build_config/BUILD:172:11: //tensorflow/core/platform/default/build_config:gif depends on @gif//:gif in repository @gif which failed to fetch. no such package '@gif//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz, https://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz] to /root/.cache/bazel/_bazel_root/ce3ec9ad38e107ecc2ce5a4b33070bd2/external/gif/temp11457534064291798322/giflib-5.2.1.tar.gz: PKIX path validation failed: java.security.cert.CertPathValidatorException: validity check failed
ERROR: Analysis of target '//tensorflow/tools/graph_transforms:transform_graph' failed; build aborted: Analysis failed


**Logs:**

INFO: Found applicable config definition build:dynamic_kernels in file /local/mnt/workspace/sdks/tf_src/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/curl.haxx.se/download/curl-7.78.0.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
INFO: Repository gif instantiated at:
  /local/mnt/workspace/sdks/tf_src/tensorflow/WORKSPACE:15:14: in <toplevel>
  /local/mnt/workspace/sdks/tf_src/tensorflow/tensorflow/workspace2.bzl:1089:21: in workspace
  /local/mnt/workspace/sdks/tf_src/tensorflow/tensorflow/workspace2.bzl:366:20: in _tf_repositories
  /local/mnt/workspace/sdks/tf_src/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /local/mnt/workspace/sdks/tf_src/tensorflow/third_party/repo.bzl:66:35: in <toplevel>
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
WARNING: Download from https://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path validation failed: java.security.cert.CertPathValidatorException: validity check failed
ERROR: An error occurred during the fetch of repository 'gif':
   Traceback (most recent call last):
        File ""/local/mnt/workspace/sdks/tf_src/tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl
                ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz, https://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz] to /root/.cache/bazel/_bazel_root/ce3ec9ad38e107ecc2ce5a4b33070bd2/external/gif/temp11457534064291798322/giflib-5.2.1.tar.gz: PKIX path validation failed: java.security.cert.CertPathValidatorException: validity check failed
INFO: Repository icu instantiated at:
  /local/mnt/workspace/sdks/tf_src/tensorflow/WORKSPACE:15:14: in <toplevel>
  /local/mnt/workspace/sdks/tf_src/tensorflow/tensorflow/workspace2.bzl:1082:28: in workspace
  /local/mnt/workspace/sdks/tf_src/tensorflow/tensorflow/workspace2.bzl:70:8: in _initialize_third_party
  /local/mnt/workspace/sdks/tf_src/tensorflow/third_party/icu/workspace.bzl:6:20: in repo
  /local/mnt/workspace/sdks/tf_src/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /local/mnt/workspace/sdks/tf_src/tensorflow/third_party/repo.bzl:66:35: in <toplevel>
ERROR: /local/mnt/workspace/sdks/tf_src/tensorflow/tensorflow/core/platform/default/build_config/BUILD:172:11: //tensorflow/core/platform/default/build_config:gif depends on @gif//:gif in repository @gif which failed to fetch. no such package '@gif//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz, https://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz] to /root/.cache/bazel/_bazel_root/ce3ec9ad38e107ecc2ce5a4b33070bd2/external/gif/temp11457534064291798322/giflib-5.2.1.tar.gz: PKIX path validation failed: java.security.cert.CertPathValidatorException: validity check failed
ERROR: Analysis of target '//tensorflow/tools/graph_transforms:transform_graph' failed; build aborted: Analysis failed
INFO: Elapsed time: 40.378s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (186 packages loaded, 8493 targets configured)
"
53586,TypeError: slice indices must be integers or None or have an __index__ method for @tf.function and when I use tfp.mcmc.NoUTurnSampler(),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur
- TensorFlow installed from (source or binary): using homebrew
- TensorFlow version (use command below): tf-nightly==2.9.0.dev20211224
- Tensorflow probability version: tensorflow-probability==0.13.0
- Python version: Python 3.9.9


**Describe the current behavior**
I am unable to use @tf.function with indexing, for instance, the code below gives

- Auscomp.Qobs is data
t_end = len(Auscomp.Qobs)
t_obs = np.linspace(start=0.,
                    stop= t_end,
                    num=int(t_end)).tolist()
Auscomp.Prec.values is observed data values.

@tf.function(jit_compile=True)
def p(t):
    pos = tf.raw_ops.Bucketize(input=t,boundaries=t_obs)
    precip_t = Auscomp.Prec.values[pos - 1:pos]

    return precip_t[0]

- call function

p(1.5) throws the error message 

TypeError: in user code:

    File ""<ipython-input-3-f42dda5a54a8>"", line 20, in p  *
        precip_t = Auscomp.Prec.values[pos - 1:pos]

    TypeError: slice indices must be integers or None or have an __index__ method

**Describe the expected behavior**
I expect to have scaler value returned. For instance, if I add 
tf.config.run_functions_eagerly(True)  at the top of the programme the function works.
But I also have functions that solve ODEs and do Bayesian inference with TensorFlow probability. So when I run a function tfp.mcmc.NoUTurnSampler() that calls an ODE that calls the above function, I get the  message below

WARNING:tensorflow:It looks like tf.function behavior was disabled, perhaps using tf.config.run_functions_eagerly. Vectorization primitives (e.g. tf.vectorized_map) require tf.function to work. These primitives will override the disable.

and later the error

File ""<ipython-input-6-7453b3323d01>"", line 27, in precip
        precip_t = Auscomp.Prec.values[pos - 1:pos]

    TypeError: slice indices must be integers or None or have an __index__ method

**Standalone code to reproduce the issue**

import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp


t_end = 21
t_obs = np.linspace(0.0, t_end, 20)

- data set with zeros and ones 
precip = np.zeros(t_end + 1)
t_precip = np.linspace(0, t_end + 1, num=t_end + 2).tolist()

precip[0::7] = 1.0

@tf.function
def p(t):
        pos = tf.raw_ops.Bucketize(input=t,boundaries=t_precip)
    
        precip_t = precip[pos - 1:pos]

        return precip_t[0]
- call function  
p(1.5)
"
53585,Inconsistent results of K.dot,"there are two numpy matrixs：
**first** with shape:[1,39],dtype=float32, 
 **second** with shape[39,2],dtype=float32

`
import tensorflow.keras.backend as K
`
`
K.dot(tf.constant(first),tf.constant(second[:,0:1]))
`
Out[5]: <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[**0.8299226**]], dtype=float32)>

``
K.dot(tf.constant(first),tf.constant(second[:,0:2]))
``
Out[6]: <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[**0.8301826**, 1.0395387]], dtype=float32)>

the two results diff 2.66e-4 which is big enough for me

**can anybody tell me why this happens and any solutions?** 

tf v2.4.0

values of the matrix:

first
Out[7]: 
array([[-7.2378081e-01,  1.6171148e+00, -1.4751177e+00, -1.1468199e+00,
        -9.4957459e-01, -1.1004009e+00, -6.0811418e-01,  1.3347808e+00,
        -5.6221259e-01, -2.8636627e-02, -1.3238943e+00,  1.3290122e+00,
         3.3704469e-01,  4.8254672e-01, -6.0419792e-01,  7.0332962e-01,
        -8.1343579e-01, -1.7627051e+00,  3.5882825e-01, -1.4614569e+00,
        -3.9931360e-01, -9.7357178e-01, -6.1273777e-01,  4.6369970e-01,
         9.3952209e-01,  5.2290356e-01, -6.9766754e-01,  4.8909006e-01,
        -2.4669762e-01,  1.1569391e+00,  8.5423976e-01, -1.2952095e-03,
        -3.4862864e-01, -2.7243546e-01,  8.0830842e-01, -3.0422598e-01,
         2.8226626e-01, -5.6840044e-02, -7.3323792e-01]], dtype=float32)

second[:,0:2]
Out[8]: 
array([[-0.09545885,  0.0301608 ],
       [ 0.10142329, -0.09098691],
       [-0.04838413,  0.15964064],
       [ 0.16257629, -0.1777806 ],
       [-0.131538  , -0.14655067],
       [-0.085496  , -0.13280763],
       [ 0.18278474, -0.11492068],
       [ 0.03820068,  0.00508413],
       [-0.00234684, -0.10233091],
       [ 0.12245703,  0.10799986],
       [ 0.01588374,  0.05467606],
       [-0.15388036,  0.14180547],
       [-0.0121727 , -0.08367829],
       [ 0.09906906,  0.09994766],
       [-0.07855757, -0.18606487],
       [-0.02123192,  0.07851872],
       [-0.1276356 , -0.12484328],
       [-0.17431177,  0.00072895],
       [-0.02924933, -0.01532622],
       [ 0.06264558, -0.19014324],
       [ 0.07497945, -0.0103462 ],
       [ 0.00148363,  0.12542334],
       [ 0.0202539 , -0.04290475],
       [ 0.06409332,  0.17675939],
       [ 0.07451543, -0.01164982],
       [ 0.14731088, -0.16865423],
       [-0.13444461, -0.14083615],
       [ 0.17176068, -0.09960409],
       [-0.12674293, -0.17646563],
       [ 0.13016194,  0.09005257],
       [-0.19060531, -0.06375018],
       [ 0.03746909,  0.00599393],
       [ 0.18026543,  0.18963405],
       [-0.11866984, -0.11520495],
       [ 0.09463558,  0.11655363],
       [ 0.05268271, -0.1161972 ],
       [-0.1774411 ,  0.18842372],
       [ 0.02491291,  0.12098533],
       [-0.11909473,  0.03792849]], dtype=float32)"
53584,Please add `tf.random.stateless_shuffle`,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
we only have `tf.random.shuffle` which is generally annoying to work with and bug-prune when reproducible results are needed

(same reasoning for all 'tf.random.stateless_*`)

**Will this change the current api? How?**A
only API addition

**Who will benefit with this feature?**
people that want to reproduce random function rexecution


**Any Other info.**
no, thank you"
53583,Release tensorflow-macos wheel compatible with Python 3.9 and x86,"Currently in version 2.7.0 of `tensorflow-macos`, there are only 3 wheels released in pip: 2 for arm64 supporting Python 3.8-3.9 and 1 for x86 only supporting Python 3.8.
It would be great to have also a wheel for Python 3.9 for the x86 architecture.
The same thing also applies for `tensorflow-metal`.


**System information**
- TensorFlow version (you are using): 2.7.0



**Describe the feature and the current behavior/state.**
See above

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Every Intel Mac User with Python 3.9

**Any Other info.**
"
53582,Description of Build Order and bootstrapped builds,"I would like to request documentation on the ideal build order for tensorflow and both its required dependencies as well as suggested dependencies:

As described in: https://github.com/tensorflow/tensorflow/blob/v2.7.0/tensorflow/tools/pip_package/setup.py#L104
the package `tensorflow` depends on 4 dependencies that must be moved in step:
```
    'tensorboard ~= 2.6',
    'tensorflow_estimator ~= 2.7.0rc0, < 2.8',
    # Keras release is not backward compatible with old tf release, and we have
    # to make the version aligned between TF and Keras.
    'keras >= 2.7.0rc0, < 2.8',
    'tensorflow-io-gcs-filesystem >= 0.21.0',
```

However, it is my understanding that in order to build `estimator` that `tensorflow` must be importable by python.

Is that really the case?

If not, is there somewhere where we can read up on the ideal build order. Ideally we would have:
1. Build `tensorflow`. This will create an importable package, but without many extra features.
     * It might be that this package is called `tensorflow-base` or something else to your liking
2. Build `estimator`....
3. Build `keras` ....
4. Assemble all dependencies into a single dependency called `tensorflow`

Ideally this would form some directed acyclic graph so that we can bootstrap the builds without having an ""older version of tensorflow"" already compiled.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7.0
- Python version: 3.7-3.10
- Installed using virtualenv? pip? conda?: conda with conda-forge
- Bazel version (if compiling from source): 4.XX
- GCC/Compiler version (if compiling from source): 9
- CUDA/cuDNN version: 11.2
- GPU model and memory: Many



**Describe the problem**

It is best to look a the file `recipe/build.sh`  that builds the wheel. Finally the file `recipe/build_pkg.sh` installs the wheel.
https://github.com/conda-forge/tensorflow-feedstock/pull/189

**Any other info / logs**
The best logs can be found on the conda-forge recipe. Here is the PR upgrading the system to 2.7.0. You can see a few patches strip out certain dependencies.

https://github.com/conda-forge/tensorflow-feedstock/pull/189

"
53581,tf.ragged.stack_dynamic_partitions silently output wrong result with negative `partitions`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: Cuda 11.4
- GPU model and memory: n/a

**Standalone code to reproduce the issue**
```
import tensorflow as tf
data=['a', 'b', 'c', 'd', 'e']
partitions=[3, -2, 2, -1, 2]
num_partitions=5
t1 = tf.ragged.stack_dynamic_partitions(data, partitions, num_partitions) # Succeed
print(t1)
t2 = tf.ragged.stack(tf.dynamic_partition(data, partitions, num_partitions)) # Raise InvalidArgumentError
print(t2)
```
Outputs
```
<tf.RaggedTensor [[], [], [b'b', b'd'], [b'c'], []]>
InvalidArgumentError: partitions[1] = -2 is not in [0, 5) [Op:DynamicPartition]
```

**Describe the current behavior**
For `tf.ragged.stack_dynamic_partitions`, values in `partitions` should be greater or equal to zero. (as the document points out)

**Describe the expected behavior**
In the document it says ""if `num_partitions` is an int (not a Tensor), then `tf.ragged.stack_dynamic_partitions(data, partitions, num_partitions)` is equivalent to `tf.ragged.stack(tf.dynamic_partition(data, partitions, num_partitions))`"". So similar to the latter case, an `InvalidArgumentError` is expected to be raised when calling `tf.ragged.stack_dynamic_partitions` with `partitions` containing negative values.
"
53579,AttributeError: 'NoneType' object has no attribute 'outer_context' when building a token classification model,"Hi, I am training a token-classification model, and I use ELMO embeddings as one layer of my model.

I'm using elmo embeddings from tensorflow hub, and I am using them with tensorflow 2.4.1. The same model architecture worked when built in Tensorflow 1.x, but I need to migrate it to TF2. I followed the instructions on how to replace hub.Module() with hub.KerasLayer() or hub.load() in order to use elmo in TF2. The example is with hub.load(). When using hub.KerasLayer() I cannot use the ""token"" signature. 

Please note that the same model architecture worked in TF1, but I need a solution to make it work in TF2. 

This is how I'm building the model: 
```
def ElmoEmbedding(x):
    return elmo_model.signatures[""tokens""](tokens = tf.squeeze(tf.cast(x, tf.string)), sequence_len = tf.constant(batch_size * [max_len]))[""elmo""]

def build_model(max_len, n_words, n_tags): 
    word_input_layer = Input(shape=(max_len, 40, ))
    elmo_input_layer = Input(shape=(max_len,), dtype=tf.string)
    word_output_layer = Dense(n_tags, activation = 'softmax')(word_input_layer)
    elmo_output_layer = Lambda(ElmoEmbedding, output_shape=(None, 1024))(elmo_input_layer)

    output_layer = Concatenate()([word_output_layer, elmo_output_layer])
    output_layer = BatchNormalization()(output_layer)
    output_layer = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(output_layer)
    output_layer = TimeDistributed(Dense(n_tags, activation='softmax'))(output_layer)
    
    model = Model([elmo_input_layer, word_input_layer], output_layer)
    
    return model
```

And therefore want to train the model as follows:
```
elmo_model = hub.load('https://tfhub.dev/google/elmo/3')
model = build_model(max_len, n_words, n_tags)
model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""])
model.summary()

history = model.fit([np.array(X1_train), np.array(X2_train).reshape((len(X2_train), max_len, 40))], 
                    y_train, 
                    validation_data=([np.array(X1_valid), np.array(X2_valid).reshape((len(X2_valid), max_len, 40))], y_valid),
                    batch_size=32, epochs=2, verbose=1)
```

To clarify, X1_train is a list of tokenized sentences and X2_train is a list of hand-picked features for every token in every sentence. 


The error that I'm getting when calling model.fit is the following one:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-14-04a3722fa4b2> in <module>
     51                     y_train,
     52                     validation_data=([np.array(X1_valid), np.array(X2_valid).reshape((len(X2_valid), max_len, 40))], y_valid),
---> 53                     batch_size=32, epochs=2, verbose=1)
     54 hist = pd.DataFrame(history.history)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-> 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830       new_tracing_count = self.experimental_get_tracing_count()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    869       # This is the first call of __call__, so we have to initialize.
    870       initializers = []
--> 871       self._initialize(args, kwds, add_initializers_to=initializers)
    872     finally:
    873       # At this point we know that the initialization is complete (or less

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    724     self._concrete_stateful_fn = (
    725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 726             *args, **kwds))
    727 
    728     def invalid_creator_scope(*unused_args, **unused_kwds):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2967       args, kwargs = None, None
   2968     with self._lock:
-> 2969       graph_function, _ = self._maybe_define_function(args, kwargs)
   2970     return graph_function
   2971 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3359 
   3360           self._function_cache.missed.add(call_context_key)
-> 3361           graph_function = self._create_graph_function(args, kwargs)
   3362           self._function_cache.primary[cache_key] = graph_function
   3363 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3204             arg_names=arg_names,
   3205             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3206             capture_by_value=self._capture_by_value),
   3207         self._function_attributes,
   3208         function_spec=self.function_spec,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    988         _, original_func = tf_decorator.unwrap(python_func)
    989 
--> 990       func_outputs = python_func(*func_args, **func_kwargs)
    991 
    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    632             xla_context.Exit()
    633         else:
--> 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    635         return out
    636 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, ""ag_error_metadata""):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

AttributeError: in user code:

    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step
        y_pred = self(x, training=True)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:425 call
        inputs, training=training, mask=mask)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph
        outputs = node.layer(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:917 call
        result = self.function(inputs, **kwargs)
    <ipython-input-7-21ffba0b5a13>:7 ElmoEmbedding
        return elmo_model.signatures[""tokens""](tokens = tf.squeeze(tf.cast(x, tf.string)), sequence_len = tf.constant(batch_size * [max_len]))[""elmo""]
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1669 __call__
        return self._call_impl(args, kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py:247 _call_impl
        args, kwargs, cancellation_manager)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1687 _call_impl
        return self._call_with_flat_signature(args, kwargs, cancellation_manager)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1736 _call_with_flat_signature
        return self._call_flat(args, self.captured_inputs, cancellation_manager)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1924 _call_flat
        forward_function, args_with_tangents = forward_backward.forward()
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1448 forward
        self._inference_args, self._input_tangents)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1207 forward
        self._forward_and_backward_functions(inference_args, input_tangents))
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1407 _forward_and_backward_functions
        outputs, inference_args, input_tangents)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:910 _build_functions_for_outputs
        src_graph=self._func_graph)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:552 _GradientsHelper
        to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs_set)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:125 _PendingCount
        between_op_list, between_ops, colocate_gradients_with_ops)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_state.py:780 MaybeCreateControlFlowState
        loop_state.AddWhileContext(op, between_op_list, between_ops)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_state.py:577 AddWhileContext
        outer_forward_ctxt = forward_ctxt.outer_context

    AttributeError: 'NoneType' object has no attribute 'outer_context'
```

Any tips would help!"
53578,Failed to compile TensorFlow C++ API with Bazel and GCC 9.3 on WSL Ubuntu 20.04,"Hi All,

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): TensorFlow GitHub branch r2.6
- TensorFlow version: 2.6
- Python version: 3.9.0
- Installed using virtualenv? pip? conda?: conda's standard environment
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the problem**
---

I've been trying to compile TF C++ API from source using bazel and gcc on WSL Ubuntu 20.04 - AMD Ryzen 5 3600 - 12 cores with 12 GB of memory.

**Command I used to compile**
```
cd tensorflow/
git branch r2.6
./configure
bazel build --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --copt=""-O3"" -c opt \
    --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1  \
    --copt=-msse4.2 --copt=-mfpmath=both --config=nogcp --config=nonccl \
    //tensorflow:libtensorflow_cc.so
```

**Compile log and error message**
```
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=167
INFO: Reading rc options for 'build' from /home/rangsiman/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/rangsiman/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /home/rangsiman/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/rangsiman/miniconda3/envs/tf_cc/bin/python3 --action_env PYTHON_LIB_PATH=/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages --python_path=/home/rangsiman/miniconda3/envs/tf_cc/bin/python3
INFO: Found applicable config definition build:short_logs in file /home/rangsiman/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/rangsiman/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:nogcp in file /home/rangsiman/tensorflow/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:nonccl in file /home/rangsiman/tensorflow/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:linux in file /home/rangsiman/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false
INFO: Found applicable config definition build:dynamic_kernels in file /home/rangsiman/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /home/rangsiman/.cache/bazel/_bazel_rangsiman/941ebeb4a3431ad9648cd9d348263c60/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Build options --action_env and --define have changed, discarding analysis cache.
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /home/rangsiman/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/rangsiman/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  /home/rangsiman/.cache/bazel/_bazel_rangsiman/941ebeb4a3431ad9648cd9d348263c60/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  /home/rangsiman/.cache/bazel/_bazel_rangsiman/941ebeb4a3431ad9648cd9d348263c60/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow:libtensorflow_cc.so (212 packages loaded, 19185 targets configured).
INFO: Found 1 target...
ERROR: /home/rangsiman/tensorflow/tensorflow/core/kernels/BUILD:3345:18: C++ compilation of rule '//tensorflow/core/kernels:matmul_op' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 134 argument(s) skipped)
gcc: fatal error: Killed signal terminated program cc1plus
compilation terminated.
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2600.294s, Critical Path: 791.86s
INFO: 4437 processes: 416 internal, 4021 local.
FAILED: Build did NOT complete successfully
```

I had run the same bazel build commands many times and I found that the compilations failed at different .cc files. So I was not sure if it was because the memory is not enough or not. I also tried `--jobs=8` and `--jobs=4` to limit the memory consumption but it didn't help. Any ideas to solve this issue? Thanks
"
53577,model.fit excess memory usage with GPU,"I am running 4 scripts which should be equivalent in theory. However, two of those [which use GPU to train the model] are using 10 times more memory than their CPU counterparts. 

The scripts which are using acceptable levels of memory are scripts 1 and 3. The unacceptable levels are scripts 2 and 4. 

Here is the code:
script1: https://pastebin.com/2gRbjKmx
script2: https://pastebin.com/GK9iE2DC
script3: https://pastebin.com/cUniWZxj
script4: https://pastebin.com/BwkWbHjg

My python version is 3.9.7 and tensorflow version is 2.4.1.

The OS is: Red Hat Enterprise Linux
The Kernel is: Linux 3.10.0-1160.2.1.el7.x86_64
The architecture is: x86-64

The GPU is Nvidia either P100 or Nvidia V100."
53575,AttributeError: 'NoneType' object has no attribute 'outer_context' when building a token classification model,"Hi, I am training a token-classification model, and I use ELMO embeddings as one layer of my model.

I'm using elmo embeddings from tensorflow hub, and I am using them with tensorflow 2.4.1. The same model architecture worked when built in Tensorflow 1.x, but I need to migrate it to TF2. I followed the instructions on how to replace hub.Module() with hub.KerasLayer() or hub.load() in order to use elmo in TF2. The example is with hub.load(). When using hub.KerasLayer() I cannot use the ""token"" signature. 

Please note that the same model architecture worked in TF1, but I need a solution to make it work in TF2. 

This is how I'm building the model: 
```
def ElmoEmbedding(x):
    return elmo_model.signatures[""tokens""](tokens = tf.squeeze(tf.cast(x, tf.string)), sequence_len = tf.constant(batch_size * [max_len]))[""elmo""]

def build_model(max_len, n_words, n_tags): 
    word_input_layer = Input(shape=(max_len, 40, ))
    elmo_input_layer = Input(shape=(max_len,), dtype=tf.string)
    word_output_layer = Dense(n_tags, activation = 'softmax')(word_input_layer)
    elmo_output_layer = Lambda(ElmoEmbedding, output_shape=(None, 1024))(elmo_input_layer)

    output_layer = Concatenate()([word_output_layer, elmo_output_layer])
    output_layer = BatchNormalization()(output_layer)
    output_layer = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(output_layer)
    output_layer = TimeDistributed(Dense(n_tags, activation='softmax'))(output_layer)
    
    model = Model([elmo_input_layer, word_input_layer], output_layer)
    
    return model
```

And therefore want to train the model as follows:
```
elmo_model = hub.load('https://tfhub.dev/google/elmo/3')
model = build_model(max_len, n_words, n_tags)
model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""])
model.summary()

history = model.fit([np.array(X1_train), np.array(X2_train).reshape((len(X2_train), max_len, 40))], 
                    y_train, 
                    validation_data=([np.array(X1_valid), np.array(X2_valid).reshape((len(X2_valid), max_len, 40))], y_valid),
                    batch_size=32, epochs=2, verbose=1)
```

To clarify, X1_train is a list of tokenized sentences and X2_train is a list of hand-picked features for every token in every sentence. 


The error that I'm getting when calling model.fit is the following one:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-14-04a3722fa4b2> in <module>
     51                     y_train,
     52                     validation_data=([np.array(X1_valid), np.array(X2_valid).reshape((len(X2_valid), max_len, 40))], y_valid),
---> 53                     batch_size=32, epochs=2, verbose=1)
     54 hist = pd.DataFrame(history.history)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-> 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830       new_tracing_count = self.experimental_get_tracing_count()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    869       # This is the first call of __call__, so we have to initialize.
    870       initializers = []
--> 871       self._initialize(args, kwds, add_initializers_to=initializers)
    872     finally:
    873       # At this point we know that the initialization is complete (or less

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    724     self._concrete_stateful_fn = (
    725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 726             *args, **kwds))
    727 
    728     def invalid_creator_scope(*unused_args, **unused_kwds):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2967       args, kwargs = None, None
   2968     with self._lock:
-> 2969       graph_function, _ = self._maybe_define_function(args, kwargs)
   2970     return graph_function
   2971 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3359 
   3360           self._function_cache.missed.add(call_context_key)
-> 3361           graph_function = self._create_graph_function(args, kwargs)
   3362           self._function_cache.primary[cache_key] = graph_function
   3363 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3204             arg_names=arg_names,
   3205             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3206             capture_by_value=self._capture_by_value),
   3207         self._function_attributes,
   3208         function_spec=self.function_spec,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    988         _, original_func = tf_decorator.unwrap(python_func)
    989 
--> 990       func_outputs = python_func(*func_args, **func_kwargs)
    991 
    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    632             xla_context.Exit()
    633         else:
--> 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    635         return out
    636 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, ""ag_error_metadata""):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

AttributeError: in user code:

    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step
        y_pred = self(x, training=True)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:425 call
        inputs, training=training, mask=mask)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph
        outputs = node.layer(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:917 call
        result = self.function(inputs, **kwargs)
    <ipython-input-7-21ffba0b5a13>:7 ElmoEmbedding
        return elmo_model.signatures[""tokens""](tokens = tf.squeeze(tf.cast(x, tf.string)), sequence_len = tf.constant(batch_size * [max_len]))[""elmo""]
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1669 __call__
        return self._call_impl(args, kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py:247 _call_impl
        args, kwargs, cancellation_manager)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1687 _call_impl
        return self._call_with_flat_signature(args, kwargs, cancellation_manager)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1736 _call_with_flat_signature
        return self._call_flat(args, self.captured_inputs, cancellation_manager)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1924 _call_flat
        forward_function, args_with_tangents = forward_backward.forward()
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1448 forward
        self._inference_args, self._input_tangents)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1207 forward
        self._forward_and_backward_functions(inference_args, input_tangents))
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1407 _forward_and_backward_functions
        outputs, inference_args, input_tangents)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:910 _build_functions_for_outputs
        src_graph=self._func_graph)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:552 _GradientsHelper
        to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs_set)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:125 _PendingCount
        between_op_list, between_ops, colocate_gradients_with_ops)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_state.py:780 MaybeCreateControlFlowState
        loop_state.AddWhileContext(op, between_op_list, between_ops)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_state.py:577 AddWhileContext
        outer_forward_ctxt = forward_ctxt.outer_context

    AttributeError: 'NoneType' object has no attribute 'outer_context'
```

Any tips would help!"
53572,tf.data.Dataset .map().batch() pattern is not matched to use fused implementation.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.1-69264-g0cdf35562dc 2.9.0
- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.5 / 8.3
- GPU model and memory: GTX1660 Ti

**Describe the current behavior**
combining `tf.data.Dataset.map()` with `.batch()` does not use the fused BatchAndMap implementation.

**Describe the expected behavior**
It does use the fused implementation. Currently, it's only possible to use the fused implementation when using the deprecated `experimental.map_and_batch()` transformation.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

```py
import os
import datetime
from tqdm import tqdm
import numpy as np

import tensorflow as tf
print('TF version', tf.__version__)

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPUs')
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)


@tf.function
def do_stuff(wmat, tf_var):
    with tf.device(""/gpu:0""):
        S = tf.constant(0.0)
        for i in tf.range(4):
            fi = tf.cast(i, dtype=tf.float32)
            A = tf.math.lgamma(tf.tanh(tf.matmul(wmat + fi, tf.transpose(wmat - fi, [0, 2, 1]))))
            S += tf.reduce_sum(A)
        error = tf.reduce_mean(tf_var)
        return error, S

exp_uuid = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")

n_batches = 512


def gen():
    for i in range(n_batches):
        with tf.device(""/cpu:0""): # Make sure it comes from CPU
            r = tf.ones((400,800))
        yield r

option_names = ['map().batch()', 'map_and_batch()']
for option in range(2):

    with tf.device(""/cpu:0""):
        dataset = tf.data.Dataset.from_generator(gen, output_types=tf.float32)

        def my_identity(x):
            with tf.device(""/cpu:0""):
                print(""my_identity input:"", x, x.device)
                y = tf.identity(x)
                print(""my_identity output:"", y, y.device)
                return y

        if option == 0:
            ## Option 0: map().batch()
            dataset = dataset.map(my_identity).batch(16)

        elif option == 1:
            ## Option 1: deprecated map_and_batch()
            dataset = dataset.apply(tf.data.experimental.map_and_batch(my_identity, 16))

    gpu_transform = tf.data.experimental.prefetch_to_device('/gpu:0', buffer_size=4)
    dataset = dataset.apply(gpu_transform)


    tf_var = tf.Variable(tf.zeros(3))
    adam = tf.keras.optimizers.Adam(1e-4)
    logpath = os.path.join('data', 'logs', 'pa_' + exp_uuid + '_' + option_names[option])

    tf.profiler.experimental.start(logpath)
    start = datetime.datetime.now()
    for b, wmat in tqdm(enumerate(dataset)):
        with tf.GradientTape() as tape:

            if b == 0:
                print('\n\n dataset element device', wmat.device)
                print('\n')

            # Do some calculations
            result = do_stuff(wmat, tf_var)

        grads = tape.gradient(result[0], [tf_var])
        adam.apply_gradients(zip(grads, [tf_var]))
    stop = datetime.datetime.now()
    tf.profiler.experimental.stop()

    print(f'\n\nOption {option_names[option]}\n===========================\n')
    print(logpath)
    print('Time lapsed=', stop - start)
    print(""\n\n"")
```

**Other info / logs**

**Option 1**:
![image](https://user-images.githubusercontent.com/845012/147661299-a7f72017-00ff-47b6-bb71-8812bd5163d3.png)
Symptoms:
 - See the blocks `Iterator::FlapMap` and `Iterator::BatchV2` stacked on top of each other.
 - The MemcpyH2D (selected, see the details panel) is comping from pagable memory, instead of pinned memory (which is what MapAndBatch does). Because of the source being pagable memory, it can't overlap with kernel computations.
 
**Option 2**:
![image](https://user-images.githubusercontent.com/845012/147661558-9f316201-a4e5-4df1-a77b-032272537321.png)
Evidence:
 - The MapAndBatch block is used.
 - The MemcopyH2D comes from pinned memory (see details pane) and overlaps with kernel computations.

The whole deal about pinned memory is to allow parallel data upload and kernel computations. So the dataset needs to be produced into pinned host memory, which then can be uploaded asynchronously by the driver without an extra copy. See https://github.com/tensorflow/tensorflow/issues/43905#issuecomment-823675760 and https://github.com/tensorflow/tensorflow/issues/43905#issuecomment-824145184 and:
https://github.com/tensorflow/tensorflow/blob/40e9b534962989af7486bc6567ca472d71eb5049/tensorflow/core/kernels/data/experimental/map_and_batch_dataset_op.cc#L522

This is a follow up on https://github.com/tensorflow/tensorflow/issues/43905.
"
53571,AveragePooling1D wrongly accept stride parameter to be None,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.3/8
- GPU model and memory: N/A

**Describe the current behavior**
`tf.keras.layers.AveragePooling1D` can output normal value when setting `stride` parameter to `None` with no warning/exceptions. In contrast `tf.keras.layers.Conv1D` will raise an exception to forbit such usage.

**Describe the expected behavior**
This API should throw an exception or raise a warning instead of outputing a normal value.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): I haven't tried a solution yet and I am not familiar with your coding style to deal with such an issue.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
x = tf.keras.layers.Input((32,3))
layer = tf.keras.layers.AveragePooling1D(pool_size=3, strides=None)
y = layer(x)
model = tf.keras.Model(x,y)
model.predict(np.random.rand(10,32,3))
```
You may access the code here:
https://colab.research.google.com/drive/1u80B4LFEEctS5GOZj3GUd_G6u_3A9kcu?usp=sharing
"
53569,convert_variables_to_constants_v2 not supper fo embedding?,"TensorFlow not supper frozen ConcreteFunction when I convert My deepFM model,
 It's because convert_variables_to_constants_v2 not supper enbedding, I want to know the fix of this issue please?
![image](https://user-images.githubusercontent.com/22836006/147644669-d550ed28-0715-495a-86b4-8b0b4b6d28c6.png)


<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.3.4-gpu
- Python version: 3.7
- CUDA/cuDNN version: 10.1
- GPU model and memory:  16G

**Describe the current behavior**
TensorFlow not supper frozen ConcreteFunction when I convert My deepFM model,
 It's because convert_variables_to_constants_v2 not supper enbedding

**Describe the expected behavior**
Conver saved model to froze graph success

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**


**Other info / logs** Include any logs or source code that would be helpful to
"
53568,GPU sync failed and CUDA_ERROR_ILLEGAL_ADDRESS error,"**System information**
- Have I written custom code: **yes**
- OS Platform and Distribution: **ubuntu 20.04**
- Mobile device if the issue happens on mobile device: **No**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version: **Tensorflow-1.4.0**
- Python version: **Python 3.5.2**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **CUDA Version 8.0.61, cuDNN 7.0.5**
- GPU model and memory: **GeForce GTX 1080 Ti with 11G memory**

**Describe the current behavior**
The baseline model is implemented under TensorFlow 1.4.0, so I built my model based on this model. Then I made a docker and ran it in docker. When I train my model, it occasionally shows this error: 
`tensorflow.python.framework.errors_impl.InternalError: GPU sync failed`
and
`2017-11-08 12:24:52.838106: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x51f18f0: CUDA_ERROR_ILLEGAL_ADDRESS`


When using fewer training samples, it seems that there is no problem when training the model once (epoch=1). This problem usually occurs when I train the model 5 times (epoch=5).
When more training samples are used, even if the model is only trained once, this problem will occur more frequently.

I downloaded cudnn7.0.5 according to the [this](https://github.com/tensorflow/tensorflow/issues/14363), but it did not alleviate this problem.
I had tried to restart the docker but it didn't help.
The GPU memory is not exceeded. In the case of successful training, it only takes up 30% of the memory.

At the same time, I found the following error in the system log. (by `sudo journalctl -f`)
`Dec 29 15:02:52 zpp-lab kernel: NVRM: Xid (PCI:0000:17:00): 31, pid=1342831, Ch 00000012, intr 30000000. MMU Fault: ENGINE GRAPHICS GPCCLIENT_T1_9 faulted @ 0x7f96_54002000. Fault is of type FAULT_PDE ACCESS_TYPE_ATOMIC
`

Then I run my code with cuda-memcheck, [here](https://www.dropbox.com/s/9zdegkd64kzmtgk/cuda_memcheck.txt?dl=0) is the error info.


**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
[https://github.com/fakeppz/demo](https://github.com/fakeppz/demo)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53567,TripletSemiHardLoss produces only NaNs when training with a TPU,"This bug was discovered in Google Colab, but appears to be rooted in TripletSemiHardLoss.

Here's an example Colab that illustrates the problem:
https://colab.research.google.com/drive/10GuYAFDMuxXe2jqdj7hy_I7XcApJFmj7?usp=sharing

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

I combined the example Colab here:
https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/01_MNIST_TPU_Keras.ipynb

with some of the steps here:
https://www.tensorflow.org/addons/tutorials/losses_triplet

Specifically:
I deleted the line: `label = tf.one_hot(label, 10)`
I replaced the last layer of the network (Dense ReLU) with these two layers:
```
 tf.keras.layers.Dense(10, activation=None),
 tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings
```

And then changed the loss function in `make_model` to: `tfa.losses.TripletSemiHardLoss(distance_metric='angular', margin=0.2)`

When run on a CPU or GPU, the training works as expected and the loss function produces reasonable results.
When run on a TPU, the training runs (as in it does not produce an error), but the loss function produces only NaN, causing the training to effectively fail.

I understand that not all operations are available on TPUs and TripletSemiHardLoss may fundamentally not work on a TPU, but it should at least fail with an informative error message."
53566,tensorflow/lite/schema/upgrade_schema.py has a docstring arg out-of-line,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/87462bf/tensorflow/lite/schema/upgrade_schema.py#L234

## Description of issue (what needs changing):

### Clear description

The `(see :schema.fbs).` is an odd syntax, which my custom docstring parser picks up as an argument. What function does it have? - Can it be moved to the line above it?

```python
    def RemapOperatorType(operator_type):
      """"""Remap operator structs from old names to new names.
      Args:
        operator_type: String representing the builtin operator data type
          string.
        (see :schema.fbs).
      Raises:
        ValueError: When the model has consistency problems.
      Returns:
        Upgraded builtin operator data type as a string.
      """"""
```

### Parameters defined

No there is a parameter that is defined that doesn't exist, namely `(see :schema.fbs).`.

### Submit a pull request?

Yes I can send a PR"
53565, No such package '@png_archive//': Traceback (most recent call last):,"### 1. System information
Installation Package Version：
CUDA 10.0
CUDNN 7.6
Bazel  0.2
msys  X64
tensorflow 1.13.1
### 2. Code

I get the following error when compiling TENSORFLOW
F:/tensorflow-1.13.1/source/tensorflow/core/platform/default/build_config/BUILD:188:1: no such package '@png_archive//': Traceback (most recent call last):
        File ""F:/tensorflow-1.13.1/source/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""F:/tensorflow-1.13.1/source/third_party/repo.bzl"", line 73, in _apply_patch
                _execute_and_check_ret_code(ctx, cmd)
        File ""F:/tensorflow-1.13.1/source/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\msys64\usr\bin\bash.exe -l -c ""patch"" ""-p1"" ""-d"" ""C:/users/cgh/_bazel_llc/guutvuc2/external/png_archive"" ""-i"" ""F:/tensorflow-1.13.1/source/third_party/png_fix_rpi.patch""':
Stdout:
Stderr: /usr/bin/bash: patch: 鏈壘鍒板懡浠? and referenced by '//tensorflow/core/platform/default/build_config:png'

Any other info / logs

ERROR: Analysis of target '//tensorflow:libtensorflow_cc.so' failed; build aborted: Analysis failed
INFO: Elapsed time: 29.908s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (95 packages loaded, 4625 targets configured)
    Fetching @icu; fetching 7s
    Fetching @grpc; fetching
    Fetching @png_archive; fetching
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
"
53564,v.2.7: Failure to build for iOS,"When building our code for iOS against TensorFlow Lite 2.7.0, we're seeing the following errors:

```
In file included from /Users/alex/.conan/data/ooo/1.0/ci/local/source/integrations/tflite/TFLiteStage.cpp:37:
In file included from /Users/alex/.conan/data/tensorflow-lite/2.7.0.11+fb1696d4/ci/branch_release_2.7/package/3b95326f653baf403ae4d70dae74814d9c4fef8a/lib/TensorFlowLiteCMetal.framework/Headers/TensorFlowLiteCMetal.h:1:
In file included from /Users/alex/.conan/data/tensorflow-lite/2.7.0.11+fb1696d4/ci/branch_release_2.7/package/3b95326f653baf403ae4d70dae74814d9c4fef8a/lib/TensorFlowLiteCMetal.framework/Headers/metal_delegate.h:19:
In file included from /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Metal.framework/Headers/Metal.h:9:
In file included from /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Metal.framework/Headers/MTLTypes.h:8:
In file included from /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/Foundation.h:8:
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:523:1: error: expected unqualified-id
@class NSString, Protocol;
^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:525:9: error: unknown type name 'NSString'
typedef NSString * NSExceptionName NS_TYPED_EXTENSIBLE_ENUM;
        ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:526:9: error: unknown type name 'NSString'
typedef NSString * NSRunLoopMode NS_TYPED_EXTENSIBLE_ENUM;
        ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:528:19: error: unknown type name 'NSString'
FOUNDATION_EXPORT NSString *NSStringFromSelector(SEL aSelector);
                  ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:529:44: error: unknown type name 'NSString'
FOUNDATION_EXPORT SEL NSSelectorFromString(NSString *aSelectorName);
                                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:531:19: error: unknown type name 'NSString'
FOUNDATION_EXPORT NSString *NSStringFromClass(Class aClass);
                  ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:532:53: error: unknown type name 'NSString'
FOUNDATION_EXPORT Class _Nullable NSClassFromString(NSString *aClassName);
                                                    ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:534:19: error: unknown type name 'NSString'
FOUNDATION_EXPORT NSString *NSStringFromProtocol(Protocol *proto) API_AVAILABLE(macos(10.5), ios(2.0), watchos(2.0), tvos(9.0));
                  ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:534:50: error: unknown type name 'Protocol'
FOUNDATION_EXPORT NSString *NSStringFromProtocol(Protocol *proto) API_AVAILABLE(macos(10.5), ios(2.0), watchos(2.0), tvos(9.0));
                                                 ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:535:19: error: unknown type name 'Protocol'
FOUNDATION_EXPORT Protocol * _Nullable NSProtocolFromString(NSString *namestr) API_AVAILABLE(macos(10.5), ios(2.0), watchos(2.0), tvos(9.0));
                  ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:535:61: error: unknown type name 'NSString'
FOUNDATION_EXPORT Protocol * _Nullable NSProtocolFromString(NSString *namestr) API_AVAILABLE(macos(10.5), ios(2.0), watchos(2.0), tvos(9.0));
                                                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:539:30: error: unknown type name 'NSString'
FOUNDATION_EXPORT void NSLog(NSString *format, ...) NS_FORMAT_FUNCTION(1,2) NS_NO_TAIL_CALL;
                             ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:539:53: error: format argument not an NSString
FOUNDATION_EXPORT void NSLog(NSString *format, ...) NS_FORMAT_FUNCTION(1,2) NS_NO_TAIL_CALL;
                             ~~~~~~~~~~~~~~~~       ^                  ~
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:94:49: note: expanded from macro 'NS_FORMAT_FUNCTION'
        #define NS_FORMAT_FUNCTION(F,A) __attribute__((format(__NSString__, F, A)))
                                                       ^                    ~
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:540:31: error: unknown type name 'NSString'
FOUNDATION_EXPORT void NSLogv(NSString *format, va_list args) NS_FORMAT_FUNCTION(1,0) NS_NO_TAIL_CALL;
                              ^
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:540:63: error: format argument not an NSString
FOUNDATION_EXPORT void NSLogv(NSString *format, va_list args) NS_FORMAT_FUNCTION(1,0) NS_NO_TAIL_CALL;
                              ~~~~~~~~~~~~~~~~                ^                  ~
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObjCRuntime.h:94:49: note: expanded from macro 'NS_FORMAT_FUNCTION'
        #define NS_FORMAT_FUNCTION(F,A) __attribute__((format(__NSString__, F, A)))
```

The same exact code building in the same exact environment against TFLite 2.6.2 builds with no errors.
"
53558,"Is it possible to build tensorflow lite C API to .hex file if the microcontroller such as AURIX TC29xT, which doesn't have any operating system ","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): None
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: AURIX TC29xT
- TensorFlow installed from (source or binary): 
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Even if I understand we can build tensorflow lite C API into .so file for microcontroller with linux system, for the application of electrical control unit (ECU), is that possible to build C API into .hex file so as to use the library into the microcontroller without OS?

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53557,Compile selected tflite from bazel failed.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.7.0
- Python version: 3.6
- Bazel version (if compiling from source): 4.4.0
- GCC/Compiler version (if compiling from source): 5.2.0
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I followed the example here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md to build a tflite library with selected tf ops.
The BUILD file is:
```
load(  
    ""@org_tensorflow//tensorflow/lite/delegates/flex:build_def.bzl"",
    ""tflite_flex_shared_library""
)         
          
# Shared lib target for convenience, pulls in the standard set of TensorFlow
# ops and kernels. The output library name is platform dependent:
#   - Linux/Android: `libtensorflowlite_flex.so`
#   - Mac: `libtensorflowlite_flex.dylib`
#   - Windows: `libtensorflowlite_flex.dll`
tflite_flex_shared_library(
  name = ""tensorflowlite_flex"",
  models = [ 
      "":opennmt.tflite"",                                                                                                                                      
  ],   
)
```
Here is the build command:
```
bazel build -c opt --cxxopt='--std=c++14' \
      --config=monolithic \
      --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
      //tmp2:tensorflowlite_flex
```
And here is the full log:
```
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=158
INFO: Reading rc options for 'build' from /data/nmt3/tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /data/nmt3/tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /data/nmt3/tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /data/nmt3/tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:monolithic in file /data/nmt3/tensorflow_src/.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:linux in file /data/nmt3/tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /data/nmt3/tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/c3e082762b7664bbc7ffd2c39e86464928e27c0c.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_swift/releases/download/0.21.0/rules_swift.0.21.0.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_apple/releases/download/0.31.3/rules_apple.0.31.3.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/apple_support/releases/download/0.10.0/apple_support.0.10.0.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Build option --define has changed, discarding analysis cache.
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /data/nmt3/tensorflow_src/WORKSPACE:23:14: in <toplevel>
  /data/nmt3/tensorflow_src/tensorflow/workspace0.bzl:108:34: in workspace
  /root/.cache/bazel/_bazel_root/dadf5c5bfc89a01814f2e47e8a82306e/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  /root/.cache/bazel/_bazel_root/dadf5c5bfc89a01814f2e47e8a82306e/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/113092317754c7dea47bfb3cb49c4f59c3c1fa10.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/oneapi-src/oneDNN/archive/v2.5-rc.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tmp2:tensorflowlite_flex (1 packages loaded, 18165 targets configured).
INFO: Found 1 target...
ERROR: /data/nmt3/tensorflow_src/tensorflow/core/kernels/sparse/BUILD:32:18: Compiling tensorflow/core/kernels/sparse/mat_mul_op.cc failed: (Exit 1): gcc failed: error executing command /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 215 argument(s) skipped)
tensorflow/core/kernels/sparse/mat_mul_op.cc: In lambda function:
tensorflow/core/kernels/sparse/mat_mul_op.cc:308:33: internal compiler error: in gimplify_var_or_parm_decl, at gimplify.c:1801
           HandleBatchAndRowRange(
                                 ^
0x92054c gimplify_var_or_parm_decl
        ../../gcc/gimplify.c:1801
0x9243b3 gimplify_expr(tree_node**, gimple_statement_base**, gimple_statement_base**, bool (*)(tree_node*), int)
        ../../gcc/gimplify.c:8485
0x921953 gimplify_modify_expr
        ../../gcc/gimplify.c:4644
0x92310b gimplify_expr(tree_node**, gimple_statement_base**, gimple_statement_base**, bool (*)(tree_node*), int)
        ../../gcc/gimplify.c:8054
0x9258f6 gimplify_stmt(tree_node**, gimple_statement_base**)
        ../../gcc/gimplify.c:5519
0x925aaa gimplify_and_add(tree_node*, gimple_statement_base**)
        ../../gcc/gimplify.c:423
0x925aaa gimplify_init_ctor_eval
        ../../gcc/gimplify.c:3662
0x920ea6 gimplify_init_constructor
        ../../gcc/gimplify.c:4017
0x92179e gimplify_modify_expr_rhs
        ../../gcc/gimplify.c:4274
0x92187b gimplify_modify_expr
        ../../gcc/gimplify.c:4603
0x92310b gimplify_expr(tree_node**, gimple_statement_base**, gimple_statement_base**, bool (*)(tree_node*), int)
        ../../gcc/gimplify.c:8054
0x9258f6 gimplify_stmt(tree_node**, gimple_statement_base**)
        ../../gcc/gimplify.c:5519
0x9270a8 gimplify_and_add(tree_node*, gimple_statement_base**)
        ../../gcc/gimplify.c:423
0x9270a8 internal_get_tmp_var
        ../../gcc/gimplify.c:568
0x922f36 gimplify_expr(tree_node**, gimple_statement_base**, gimple_statement_base**, bool (*)(tree_node*), int)
        ../../gcc/gimplify.c:8978
0x9286a0 gimplify_call_expr
        ../../gcc/gimplify.c:2477
0x9243e7 gimplify_expr(tree_node**, gimple_statement_base**, gimple_statement_base**, bool (*)(tree_node*), int)
        ../../gcc/gimplify.c:8025
0x92492a gimplify_target_expr
        ../../gcc/gimplify.c:5445
0x92492a gimplify_expr(tree_node**, gimple_statement_base**, gimple_statement_base**, bool (*)(tree_node*), int)
        ../../gcc/gimplify.c:8421
0x923308 gimplify_addr_expr
        ../../gcc/gimplify.c:4979
Please submit a full bug report,
with preprocessed source if appropriate.
Please include the complete backtrace with any bug report.
See <http://gcc.gnu.org/bugs.html> for instructions.
Target //tmp2:tensorflowlite_flex failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2829.962s, Critical Path: 291.83s
INFO: 3978 processes: 100 internal, 3878 local.
FAILED: Build did NOT complete successfully
```
**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
